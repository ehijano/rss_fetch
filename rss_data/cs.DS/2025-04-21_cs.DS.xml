<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Apr 2025 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A New Impossibility Result for Online Bipartite Matching Problems</title>
      <link>https://arxiv.org/abs/2504.14251</link>
      <description>arXiv:2504.14251v1 Announce Type: new 
Abstract: Online Bipartite Matching with random user arrival is a fundamental problem in the online advertisement ecosystem. Over the last 30 years, many algorithms and impossibility results have been developed for this problem. In particular, the latest impossibility result was established by Manshadi, Oveis Gharan and Saberi in 2011. Since then, several algorithms have been published in an effort to narrow the gap between the upper and the lower bounds on the competitive ratio.
  In this paper we show that no algorithm can achieve a competitive ratio better than $1- \frac e{e^e} = 0.82062\ldots$, improving upon the $0.823$ upper bound presented in (Manshadi, Oveis Gharan and Saberi, SODA 2011). Our construction is simple to state, accompanied by a fully analytic proof, and yields a competitive ratio bound intriguingly similar to $1 - \frac1e$, the optimal competitive ratio for the fully adversarial Online Bipartite Matching problem.
  Although the tightness of our upper bound remains an open question, we show that our construction is extremal in a natural class of instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14251v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Flavio Chierichetti, Mirko Giacchini, Alessandro Panconesi, Andrea Vattani</dc:creator>
    </item>
    <item>
      <title>Temporal Graph Realization With Bounded Stretch</title>
      <link>https://arxiv.org/abs/2504.14258</link>
      <description>arXiv:2504.14258v1 Announce Type: new 
Abstract: A periodic temporal graph, in its simplest form, is a graph in which every edge appears exactly once in the first $\Delta$ time steps, and then it reappears recurrently every $\Delta$ time steps, where $\Delta$ is a given period length. This model offers a natural abstraction of transportation networks where each transportation link connects two destinations periodically. From a network design perspective, a crucial task is to assign the time-labels on the edges in a way that optimizes some criterion. In this paper we introduce a very natural optimality criterion that captures how the temporal distances of all vertex pairs are `stretched', compared to their physical distances, i.e. their distances in the underlying static (non-temporal) graph. Given a static graph $G$, the task is to assign to each edge one time-label between 1 and $\Delta$ such that, in the resulting periodic temporal graph with period~$\Delta$, the duration of the fastest temporal path from any vertex $u$ to any other vertex $v$ is at most $\alpha$ times the distance between $u$ and $v$ in $G$. Here, the value of $\alpha$ measures how much the shortest paths are allowed to be \emph{stretched} once we assign the periodic time-labels.
  Our results span three different directions: First, we provide a series of approximation and NP-hardness results. Second, we provide approximation and fixed-parameter algorithms. Among them, we provide a simple polynomial-time algorithm (the \textit{radius-algorithm}) which always guarantees an approximation strictly smaller than $\Delta$, and which also computes the optimum stretch in some cases. Third, we consider a parameterized local search extension of the problem where we are given the temporal labeling of the graph, but we are allowed to change the time-labels of at most $k$ edges; for this problem we prove that it is W[2]-hard but admits an XP algorithm with respect to $k$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14258v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George B. Mertzios, Hendrik Molter, Nils Morawietz, Paul G. Spirakis</dc:creator>
    </item>
    <item>
      <title>Polynomial-Time Constant-Approximation for Fair Sum-of-Radii Clustering</title>
      <link>https://arxiv.org/abs/2504.14683</link>
      <description>arXiv:2504.14683v1 Announce Type: new 
Abstract: In a seminal work, Chierichetti et al. introduced the $(t,k)$-fair clustering problem: Given a set of red points and a set of blue points in a metric space, a clustering is called fair if the number of red points in each cluster is at most $t$ times and at least $1/t$ times the number of blue points in that cluster. The goal is to compute a fair clustering with at most $k$ clusters that optimizes certain objective function. Considering this problem, they designed a polynomial-time $O(1)$- and $O(t)$-approximation for the $k$-center and the $k$-median objective, respectively. Recently, Carta et al. studied this problem with the sum-of-radii objective and obtained a $(6+\epsilon)$-approximation with running time $O((k\log_{1+\epsilon}(k/\epsilon))^kn^{O(1)})$, i.e., fixed-parameter tractable in $k$. Here $n$ is the input size. In this work, we design the first polynomial-time $O(1)$-approximation for $(t,k)$-fair clustering with the sum-of-radii objective, improving the result of Carta et al. Our result places sum-of-radii in the same group of objectives as $k$-center, that admit polynomial-time $O(1)$-approximations. This result also implies a polynomial-time $O(1)$-approximation for the Euclidean version of the problem, for which an $f(k)\cdot n^{O(1)}$-time $(1+\epsilon)$-approximation was known due to Drexler et al.. Here $f$ is an exponential function of $k$. We are also able to extend our result to any arbitrary $\ell\ge 2$ number of colors when $t=1$. This matches known results for the $k$-center and $k$-median objectives in this case. The significant disparity of sum-of-radii compared to $k$-center and $k$-median presents several complex challenges, all of which we successfully overcome in our work. Our main contribution is a novel cluster-merging-based analysis technique for sum-of-radii that helps us achieve the constant-approximation bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14683v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sina Bagheri Nezhad, Sayan Bandyapadhyay, Tianzhi Chen</dc:creator>
    </item>
    <item>
      <title>Approximate all-pairs Hamming distances and 0-1 matrix multiplication</title>
      <link>https://arxiv.org/abs/2504.14723</link>
      <description>arXiv:2504.14723v1 Announce Type: new 
Abstract: Arslan showed that computing all-pairs Hamming distances is easily
  reducible to arithmetic 0-1 matrix multiplication (IPL 2018). We
  provide a reverse, linear-time reduction of arithmetic 0-1 matrix
  multiplication to computing all-pairs distances in a Hamming space.
  On the other hand, we present a fast randomized algorithm for
  approximate all-pairs distances in a Hamming space. By combining it
  with our reduction, we obtain also a fast randomized algorithm for
  approximate 0-1 matrix multiplication. Next, we present an
  output-sensitive randomized algorithm for a minimum spanning tree of
  a set of points in a generalized Hamming space, the lower is the
  cost of the minimum spanning tree the faster is our
  algorithm. Finally, we provide $(2+\epsilon)$- approximation
  algorithms for the $\ell$-center clustering and minimum-diameter
  $\ell$-clustering problems in a Hamming space $\{0,1\}^d$ that are
  substantially faster than the known $2$-approximation ones when both
  $\ell$ and $d$ are super-logarithmic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14723v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miroslaw Kowaluk, Andrzej Lingas, Mia Persson</dc:creator>
    </item>
    <item>
      <title>The k-Center Problem of Uncertain Points on Graphs</title>
      <link>https://arxiv.org/abs/2504.14803</link>
      <description>arXiv:2504.14803v1 Announce Type: new 
Abstract: In this paper, we study the $k$-center problem of uncertain points on a graph. Given are an undirected graph $G = (V, E)$ and a set $\mathcal{P}$ of $n$ uncertain points where each uncertain point with a non-negative weight has $m$ possible locations on $G$ each associated with a probability. The problem aims to find $k$ centers (points) on $G$ so as to minimize the maximum weighted expected distance of uncertain points to their expected closest centers. No previous work exist for the $k$-center problem of uncertain points on undirected graphs. We propose exact algorithms that solve respectively the case of $k=2$ in $O(|E|^2m^2n\log |E|mn\log mn )$ time and the problem with $k\geq 3$ in $O(\min\{|E|^km^kn^{k+1}k\log |E|mn\log m, |E|^kn^\frac{k}{2}m^\frac{k^2}{2}\log |E|mn\})$ time, provided with the distance matrix of $G$. In addition, an $O(|E|mn\log mn)$-time algorithmic approach is given for the one-center case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14803v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haitao Xu, Jingru Zhang</dc:creator>
    </item>
    <item>
      <title>Weakly Approximating Knapsack in Subquadratic Time</title>
      <link>https://arxiv.org/abs/2504.15001</link>
      <description>arXiv:2504.15001v1 Announce Type: new 
Abstract: We consider the classic Knapsack problem. Let $t$ and $\mathrm{OPT}$ be the capacity and the optimal value, respectively. If one seeks a solution with total profit at least $\mathrm{OPT}/(1 + \varepsilon)$ and total weight at most $t$, then Knapsack can be solved in $\tilde{O}(n + (\frac{1}{\varepsilon})^2)$ time [Chen, Lian, Mao, and Zhang '24][Mao '24]. This running time is the best possible (up to a logarithmic factor), assuming that $(\min,+)$-convolution cannot be solved in truly subquadratic time [K\"unnemann, Paturi, and Schneider '17][Cygan, Mucha, W\k{e}grzycki, and W{\l}odarczyk '19]. The same upper and lower bounds hold if one seeks a solution with total profit at least $\mathrm{OPT}$ and total weight at most $(1 + \varepsilon)t$. Therefore, it is natural to ask the following question.
  If one seeks a solution with total profit at least $\mathrm{OPT}/(1+\varepsilon)$ and total weight at most $(1 + \varepsilon)t$, can Knsapck be solved in $\tilde{O}(n + (\frac{1}{\varepsilon})^{2-\delta})$ time for some constant $\delta &gt; 0$?
  We answer this open question affirmatively by proposing an $\tilde{O}(n + (\frac{1}{\varepsilon})^{7/4})$-time algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15001v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Chen, Jiayi Lian, Yuchen Mao, Guochuan Zhang</dc:creator>
    </item>
    <item>
      <title>Deterministic $k$-Median Clustering in Near-Optimal Time</title>
      <link>https://arxiv.org/abs/2504.15115</link>
      <description>arXiv:2504.15115v1 Announce Type: new 
Abstract: The metric $k$-median problem is a textbook clustering problem. As input, we are given a metric space $V$ of size $n$ and an integer $k$, and our task is to find a subset $S \subseteq V$ of at most $k$ `centers' that minimizes the total distance from each point in $V$ to its nearest center in $S$.
  Mettu and Plaxton [UAI'02] gave a randomized algorithm for $k$-median that computes a $O(1)$-approximation in $\tilde O(nk)$ time. They also showed that any algorithm for this problem with a bounded approximation ratio must have a running time of $\Omega(nk)$. Thus, the running time of their algorithm is optimal up to polylogarithmic factors.
  For deterministic $k$-median, Guha et al.~[FOCS'00] gave an algorithm that computes a $\text{poly}(\log (n/k))$-approximation in $\tilde O(nk)$ time, where the degree of the polynomial in the approximation is unspecified. To the best of our knowledge, this remains the state-of-the-art approximation of any deterministic $k$-median algorithm with this running time.
  This leads us to the following natural question: What is the best approximation of a deterministic $k$-median algorithm with near-optimal running time? We make progress in answering this question by giving a deterministic algorithm that computes a $O(\log(n/k))$-approximation in $\tilde O(nk)$ time. We also provide a lower bound showing that any deterministic algorithm with this running time must have an approximation ratio of $\Omega(\log n/(\log k + \log \log n))$, establishing a gap between the randomized and deterministic settings for $k$-median.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15115v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mart\'in Costa, Ermiya Farokhnejad</dc:creator>
    </item>
    <item>
      <title>Distribution Testing Meets Sum Estimation</title>
      <link>https://arxiv.org/abs/2504.15153</link>
      <description>arXiv:2504.15153v1 Announce Type: new 
Abstract: We study the problem of estimating the sum of $n$ elements, each with weight $w(i)$, in a structured universe. Our goal is to estimate $W = \sum_{i=1}^n w(i)$ within a $(1 \pm \epsilon)$ factor using a sublinear number of samples, assuming weights are non-increasing, i.e., $w(1) \geq w(2) \geq \dots \geq w(n)$. The sum estimation problem is well-studied under different access models to the universe $U$. However, to the best of our knowledge, nothing is known about the sum estimation problem using non-adaptive conditional sampling. In this work, we explore the sum estimation problem using non-adaptive conditional weighted and non-adaptive conditional uniform samples, assuming that the underlying distribution ($D(i)=w(i)/W$) is monotone. We also extend our approach to to the case where the underlying distribution of $U$ is unimodal. Additionally, we consider support size estimation when $w(i) = 0$ or $w(i) \geq W/n$, using hybrid sampling (both weighted and uniform) to access $U$. We propose an algorithm to estimate $W$ under the non-increasing weight assumption, using $O(\frac{1}{\epsilon^3} \log{n} + \frac{1}{\epsilon^6})$ non-adaptive weighted conditional samples and $O(\frac{1}{\epsilon^3} \log{n})$ uniform conditional samples. Our algorithm matches the $\Omega(\log{n})$ lower bound by \cite{ACK15}. For unimodal distributions, the sample complexity remains similar, with an additional $O(\log{n})$ evaluation queries to locate the minimum weighted point in the domain. For estimating the support size $k$ of $U$, where weights are either $0$ or at least $W/n$, our algorithm uses $O\big( \frac{\log^3(n/\epsilon)}{\epsilon^8} \cdot \log^4 \frac{\log(n/\epsilon)}{\epsilon} \big)$ uniform samples and $O\big( \frac{\log(n/\epsilon)}{\epsilon^2} \cdot \log \frac{\log(n/\epsilon)}{\epsilon} \big)$ weighted samples to output $\hat{k}$ satisfying $k - 2\epsilon n \leq \hat{k} \leq k + \epsilon n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15153v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pinki Pradhan, Sampriti Roy</dc:creator>
    </item>
    <item>
      <title>The Model Counting Competitions 2021-2023</title>
      <link>https://arxiv.org/abs/2504.13842</link>
      <description>arXiv:2504.13842v1 Announce Type: cross 
Abstract: Modern society is full of computational challenges that rely on probabilistic reasoning, statistics, and combinatorics. Interestingly, many of these questions can be formulated by encoding them into propositional formulas and then asking for its number of models. With a growing interest in practical problem-solving for tasks that involve model counting, the community established the Model Counting (MC) Competition in fall of 2019 with its first iteration in 2020. The competition aims at advancing applications, identifying challenging benchmarks, fostering new solver development, and enhancing existing solvers for model counting problems and their variants. The first iteration, brought together various researchers, identified challenges, and inspired numerous new applications. In this paper, we present a comprehensive overview of the 2021-2023 iterations of the Model Counting Competition. We detail its execution and outcomes. The competition comprised four tracks, each focusing on a different variant of the model counting problem. The first track centered on the model counting problem (MC), which seeks the count of models for a given propositional formula. The second track challenged developers to submit programs capable of solving the weighted model counting problem (WMC). The third track was dedicated to projected model counting (PMC). Finally, we initiated a track that combined projected and weighted model counting (PWMC). The competition continued with a high level of participation, with seven to nine solvers submitted in various different version and based on quite diverging techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13842v1</guid>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes K. Fichte, Markus Hecher</dc:creator>
    </item>
    <item>
      <title>Reveal-or-Obscure: A Differentially Private Sampling Algorithm for Discrete Distributions</title>
      <link>https://arxiv.org/abs/2504.14696</link>
      <description>arXiv:2504.14696v1 Announce Type: cross 
Abstract: We introduce a differentially private (DP) algorithm called reveal-or-obscure (ROO) to generate a single representative sample from a dataset of $n$ observations drawn i.i.d. from an unknown discrete distribution $P$. Unlike methods that add explicit noise to the estimated empirical distribution, ROO achieves $\epsilon$-differential privacy by randomly choosing whether to "reveal" or "obscure" the empirical distribution. While ROO is structurally identical to Algorithm 1 proposed by Cheu and Nayak (arXiv:2412.10512), we prove a strictly better bound on the sampling complexity than that established in Theorem 12 of (arXiv:2412.10512). To further improve the privacy-utility trade-off, we propose a novel generalized sampling algorithm called Data-Specific ROO (DS-ROO), where the probability of obscuring the empirical distribution of the dataset is chosen adaptively. We prove that DS-ROO satisfies $\epsilon$-DP, and provide empirical evidence that DS-ROO can achieve better utility under the same privacy budget of vanilla ROO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14696v1</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naima Tasnim, Atefeh Gilani, Lalitha Sankar, Oliver Kosut</dc:creator>
    </item>
    <item>
      <title>Explicit Lossless Vertex Expanders</title>
      <link>https://arxiv.org/abs/2504.15087</link>
      <description>arXiv:2504.15087v1 Announce Type: cross 
Abstract: We give the first construction of explicit constant-degree lossless vertex expanders. Specifically, for any $\varepsilon &gt; 0$ and sufficiently large $d$, we give an explicit construction of an infinite family of $d$-regular graphs where every small set $S$ of vertices has $(1-\varepsilon)d|S|$ neighbors (which implies $(1-2\varepsilon)d|S|$ unique-neighbors). Our results also extend naturally to construct biregular bipartite graphs of any constant imbalance, where small sets on each side have strong expansion guarantees. The graphs we construct admit a free group action, and hence realize new families of quantum LDPC codes of Lin and M. Hsieh with a linear time decoding algorithm.
  Our construction is based on taking an appropriate product of a constant-sized lossless expander with a base graph constructed from Ramanujan Cayley cubical complexes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15087v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.GR</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun-Ting Hsieh, Alexander Lubotzky, Sidhanth Mohanty, Assaf Reiner, Rachel Yun Zhang</dc:creator>
    </item>
    <item>
      <title>Faster Algorithms for Agnostically Learning Disjunctions and their Implications</title>
      <link>https://arxiv.org/abs/2504.15244</link>
      <description>arXiv:2504.15244v1 Announce Type: cross 
Abstract: We study the algorithmic task of learning Boolean disjunctions in the distribution-free agnostic PAC model. The best known agnostic learner for the class of disjunctions over $\{0, 1\}^n$ is the $L_1$-polynomial regression algorithm, achieving complexity $2^{\tilde{O}(n^{1/2})}$. This complexity bound is known to be nearly best possible within the class of Correlational Statistical Query (CSQ) algorithms. In this work, we develop an agnostic learner for this concept class with complexity $2^{\tilde{O}(n^{1/3})}$. Our algorithm can be implemented in the Statistical Query (SQ) model, providing the first separation between the SQ and CSQ models in distribution-free agnostic learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15244v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Daniel M. Kane, Lisheng Ren</dc:creator>
    </item>
    <item>
      <title>On Learning Parallel Pancakes with Mostly Uniform Weights</title>
      <link>https://arxiv.org/abs/2504.15251</link>
      <description>arXiv:2504.15251v1 Announce Type: cross 
Abstract: We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on $\mathbb{R}^d$. This task is known to have complexity $d^{\Omega(k)}$ in full generality. To circumvent this exponential lower bound on the number of components, research has focused on learning families of GMMs satisfying additional structural properties. A natural assumption posits that the component weights are not exponentially small and that the components have the same unknown covariance. Recent work gave a $d^{O(\log(1/w_{\min}))}$-time algorithm for this class of GMMs, where $w_{\min}$ is the minimum weight. Our first main result is a Statistical Query (SQ) lower bound showing that this quasi-polynomial upper bound is essentially best possible, even for the special case of uniform weights. Specifically, we show that it is SQ-hard to distinguish between such a mixture and the standard Gaussian. We further explore how the distribution of weights affects the complexity of this task. Our second main result is a quasi-polynomial upper bound for the aforementioned testing task when most of the weights are uniform while a small fraction of the weights are potentially arbitrary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15251v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Daniel M. Kane, Sushrut Karmalkar, Jasper C. H. Lee, Thanasis Pittas</dc:creator>
    </item>
    <item>
      <title>Simple Worst-Case Optimal Adaptive Prefix-Free Coding</title>
      <link>https://arxiv.org/abs/2109.02997</link>
      <description>arXiv:2109.02997v4 Announce Type: replace 
Abstract: We give a new and simple worst-case optimal algorithm for adaptive prefix-free coding that matches Gagie and Nekrich's bounds except for lower-order terms, and uses no data structures more complicated than a lookup table. Moreover, when Gagie and Nekrich's algorithm is modified for adaptive alphabetic prefix-free coding its decoding time slows down to $O (\log \log n)$ per character, but ours can be modified for this problem with no asymptotic slowdown. As far as we know, this gives the first algorithm for this alphabetic problem that is simultaneously worst-case optimal in terms of encoding and decoding time and of encoding length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.02997v4</guid>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Travis Gagie</dc:creator>
    </item>
    <item>
      <title>Clustering What Matters in Constrained Settings</title>
      <link>https://arxiv.org/abs/2305.00175</link>
      <description>arXiv:2305.00175v2 Announce Type: replace 
Abstract: Constrained clustering problems generalize classical clustering formulations, e.g., $k$-median, $k$-means, by imposing additional constraints on the feasibility of clustering. There has been significant recent progress in obtaining approximation algorithms for these problems, both in the metric and the Euclidean settings. However, the outlier version of these problems, where the solution is allowed to leave out $m$ points from the clustering, is not well understood. In this work, we give a general framework for reducing the outlier version of a constrained $k$-median or $k$-means problem to the corresponding outlier-free version with only $(1+\varepsilon)$-loss in the approximation ratio. The reduction is obtained by mapping the original instance of the problem to $f(k,m, \varepsilon)$ instances of the outlier-free version, where $f(k, m, \varepsilon) = \left( \frac{k+m}{\varepsilon}\right)^{O(m)}$. As specific applications, we get the following results:
  - First FPT (in the parameters $k$ and $m$) $(1+\varepsilon)$-approximation algorithm for the outlier version of capacitated $k$-median and $k$-means in Euclidean spaces with hard capacities.
  - First FPT (in the parameters $k$ and $m$) $(3+\varepsilon)$ and $(9+\varepsilon)$ approximation algorithms for the outlier version of capacitated $k$-median and $k$-means, respectively, in general metric spaces with hard capacities.
  - First FPT (in the parameters $k$ and $m$) $(2-\delta)$-approximation algorithm for the outlier version of the $k$-median problem under the Ulam metric. Our work generalizes the known results to a larger class of constrained clustering problems. Further, our reduction works for arbitrary metric spaces and so can extend clustering algorithms for outlier-free versions in both Euclidean and arbitrary metric spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00175v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ragesh Jaiswal, Amit Kumar</dc:creator>
    </item>
    <item>
      <title>A Simple $(1-\epsilon)$-Approximation Semi-Streaming Algorithm for Maximum (Weighted) Matching</title>
      <link>https://arxiv.org/abs/2307.02968</link>
      <description>arXiv:2307.02968v3 Announce Type: replace 
Abstract: We present a simple semi-streaming algorithm for $(1-\epsilon)$-approximation of bipartite matching in $O(\log{\!(n)}/\epsilon)$ passes. This matches the performance of state-of-the-art "$\epsilon$-efficient" algorithms -- the ones with much better dependence on $\epsilon$ albeit with some mild dependence on $n$ -- while being considerably simpler.
  The algorithm relies on a direct application of the multiplicative weight update method with a self-contained primal-dual analysis that can be of independent interest. To show case this, we use the same ideas, alongside standard tools from matching theory, to present an equally simple semi-streaming algorithm for $(1-\epsilon)$-approximation of weighted matchings in general (not necessarily bipartite) graphs, again in $O(\log{\!(n)}/\epsilon)$ passes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02968v3</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sepehr Assadi</dc:creator>
    </item>
    <item>
      <title>Optimal Clustering with Dependent Costs in Bayesian Networks</title>
      <link>https://arxiv.org/abs/2308.03970</link>
      <description>arXiv:2308.03970v5 Announce Type: replace 
Abstract: Clustering of nodes in Bayesian Networks (BNs) and related graphical models such as Dynamic BNs (DBNs) has been demonstrated to enhance computational efficiency and improve model learning. Typically, it involves the partitioning of the underlying Directed Acyclic Graph (DAG) into cliques, or optimising for some cost or criteria. Computational cost is important since BN and DBN inference, such as estimating marginal distributions given evidence or updating model parameters, is NP-hard. The challenge is exacerbated by cost dependency, where inference outcomes and hence clustering cost depends on both nodes within a cluster and the mapping of clusters that are connected by at least one arc. We propose an algorithm called Dependent Cluster MAPping (DCMAP) which is shown analytically, given an arbitrarily defined, positive cost function, to find all optimal cluster mappings, and do so with no more iterations than an equally informed algorithm. DCMAP is demonstrated on a complex systems seagrass DBN, which has 25 nodes per time-slice, and captures biological, ecological and environmental dynamics and their interactions to predict the impact of dredging stressors on resilience and their cumulative effects over time. The algorithm is employed to find clusters to optimise the computational efficiency of inferring marginal distributions given evidence. For the 25 (one time-slice) and 50-node (two time-slices) DBN, the search space size was $9.91\times10^9$ and $1.51\times10^{21}$ possible cluster mappings, respectively, but the first optimal solution was found at iteration number 856 (95\% CI 852,866), and 1569 (1566,1581) with a cost that was 4\% and 0.2\% of the naive heuristic cost, respectively. Through optimal clustering, DCMAP opens up opportunities for further research beyond improving computational efficiency, such as using clustering to minimise entropy in BN learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.03970v5</guid>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul Pao-Yen Wu, Fabrizio Ruggeri, Kerrie Mengersen</dc:creator>
    </item>
    <item>
      <title>Last Truck Scheduling for Middle-mile Next-day Delivery Coverage</title>
      <link>https://arxiv.org/abs/2310.18388</link>
      <description>arXiv:2310.18388v2 Announce Type: replace 
Abstract: We consider an e-commerce retailer operating a supply chain that consists of middle- and last-mile transportation, and study its ability to deliver products stored in warehouses within a day from customer's order time. Successful next-day delivery requires inventory availability and timely truck schedules in the middle-mile and in this paper we assume a fixed inventory position and focus on optimizing the middle-mile last truck schedule. We formulate a novel optimization problem which decides the departure of the last truck at each (potential) network connection in order to maximize the number of customer orders that are served with next-day promise. We show that the respective next-day delivery optimization is a combinatorial problem that is NP-hard to approximate within $(1-1/e)opt\approx 0.632opt$, hence every retailer that offers one-day deliveries has to deal with this complexity barrier. We study three variants of the problem motivated by operational constraints that different retailers encounter, and propose solutions schemes tailored to each problem's properties. To that end, we rely on greedy submodular maximization, pipage rounding techniques, and Lagrangian heuristics. The algorithms are scalable, offer worst-case optimality gap guarantees, and evaluated in realistic datasets and network scenarios were found to achieve even near-optimal results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18388v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Konstantinos Benidis, Georgios Paschos, Martin Gross, George Iosifidis</dc:creator>
    </item>
    <item>
      <title>Recursive lattice reduction -- A framework for finding short lattice vectors</title>
      <link>https://arxiv.org/abs/2311.15064</link>
      <description>arXiv:2311.15064v3 Announce Type: replace 
Abstract: We propose a recursive lattice reduction framework for finding short non-zero vectors or dense sublattices of a lattice. The framework works by recursively searching for dense sublattices of dense sublattices (or their duals) with progressively lower rank. When the procedure encounters a recursive call on a lattice $L$ with relatively low rank, we simply use a known algorithm to find a shortest non-zero vector in $L$.
  This new framework is complementary to basis reduction algorithms, which similarly work to reduce an $n$-dimensional lattice problem with some approximation factor $\gamma$ to a lower-dimensional exact lattice problem in some lower dimension $k$, with a tradeoff between $\gamma$, $n$, and $k$. Our framework provides an alternative and arguably simpler perspective. For example, our algorithms can be described at a high level without explicitly referencing any specific basis of the lattice, the Gram-Schmidt orthogonalization, or even projection (though, of course, concrete implementations of algorithms in this framework will likely make use of such things).
  We present a number of instantiations of our framework. Our main concrete result is an efficient reduction that matches the tradeoff achieved by the best-known basis reduction algorithms. This reduction also can be used to find dense sublattices with any rank $\ell$ satisfying $\min\{\ell,n-\ell\} \leq n-k+1$, using only an oracle for SVP in $k$ dimensions, with slightly better parameters than what was known using basis reduction.
  We also show a simple reduction with the same tradeoff for finding short vectors in quasipolynomial time, and a reduction from finding dense sublattices of a high-dimensional lattice to this problem in lower dimension. Finally, we present an automated search procedure that finds algorithms in this framework that (provably) achieve better approximations with fewer oracle calls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15064v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Divesh Aggarwal, Thomas Espitau, Spencer Peters, Noah Stephens-Davidowitz</dc:creator>
    </item>
    <item>
      <title>Realizing temporal transportation trees</title>
      <link>https://arxiv.org/abs/2403.18513</link>
      <description>arXiv:2403.18513v3 Announce Type: replace 
Abstract: In this paper, we study the complexity of the periodic temporal graph realization problem with respect to upper bounds on the fastest path durations among its vertices. This constraint with respect to upper bounds appears naturally in transportation network design applications where, for example, a road network is given, and the goal is to appropriately schedule periodic travel routes, while not exceeding some desired upper bounds on the travel times. In our work, we focus only on underlying tree topologies, which are fundamental in many transportation network applications.
  As it turns out, the periodic upper-bounded temporal tree realization problem (TTR) has a very different computational complexity behavior than both (i) the classic graph realization problem with respect to shortest path distances in static graphs and (ii) the periodic temporal graph realization problem with exact given fastest travel times (which was recently introduced). First, we prove that, surprisingly, TTR is NP-hard, even for a constant period $\Delta$ and when the input tree $G$ satisfies at least one of the following conditions: (a) $G$ is a star, or (b) $G$ has constant maximum degree. Second, we prove that TTR is fixed-parameter tractable (FPT) with respect to the number of leaves in the input tree $G$, via a novel combination of techniques for totally unimodular matrices and mixed integer linear programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18513v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George B. Mertzios, Hendrik Molter, Nils Morawietz, Paul G. Spirakis</dc:creator>
    </item>
    <item>
      <title>Prophet Inequalities with Cancellation Costs</title>
      <link>https://arxiv.org/abs/2404.00527</link>
      <description>arXiv:2404.00527v2 Announce Type: replace 
Abstract: Most of the literature on online algorithms in revenue management focuses on settings with irrevocable decisions, where once a decision is made upon the arrival of a new input, it cannot be canceled later. Motivated by modern applications -- such as cloud spot markets, selling banner ads, or online hotel booking -- we introduce and study "prophet inequalities with cancellations" under linear cancellation costs (known as the buyback model). In the classic prophet inequality problem, a sequence of independent random variables $X_1, X_2, \ldots$ with known distributions is revealed one by one, and a decision maker must decide when to stop and accept the current variable in order to maximize the expected value of their choice. In our model, after accepting $X_j$, one may later discard $X_j$ and accept another $X_i$ at a cost of $f \times X_j$, where $f\geq 0$ is a parameter. The goal is to maximize the expected net reward: the value of the final accepted variable minus the total cancellation cost. We aim to design online policies that are competitive against the optimal offline benchmark.
  Our first main result is an optimal prophet inequality for all parameters $f \ge 0$. We fully characterize the worst-case competitive ratio of the optimal online policy against the optimal offline via the solution to a certain differential equation (for which we provide a constructive solution). Our second main result is to design and analyze a simple and polynomial-time randomized adaptive policy that achieves this optimal competitive ratio. Importantly, our policy is order-agnostic (\`a la [Samuel-Cahn, 1984]), as it only needs the set of distributions and not their arrival order. These results are obtained by novel techniques related to factor-revealing LPs and generalized flow, reductions to a differential equation, and embedding of problem instances into specific Poisson point processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00527v2</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farbod Ekbatani, Rad Niazadeh, Pranav Nuti, Jan Vondrak</dc:creator>
    </item>
    <item>
      <title>Fair Set Cover</title>
      <link>https://arxiv.org/abs/2405.11639</link>
      <description>arXiv:2405.11639v3 Announce Type: replace 
Abstract: The potential harms of algorithmic decisions have ignited algorithmic fairness as a central topic in computer science. One of the fundamental problems in computer science is Set Cover, which has numerous applications with societal impacts, such as assembling a small team of individuals that collectively satisfy a range of expertise requirements. However, despite its broad application spectrum and significant potential impact, set cover has yet to be studied through the lens of fairness. Therefore, in this paper, we introduce Fair Set Cover, which aims not only to cover with a minimum-size set but also to satisfy demographic parity in its selection of sets. To this end, we develop multiple versions of fair set cover, study their hardness, and devise efficient approximation algorithms for each variant. Notably, under certain assumptions, our algorithms always guarantee zero-unfairness, with only a small increase in the approximation ratio compared to regular set cover. Furthermore, our experiments on various data sets and across different settings confirm the negligible price of fairness, as (a) the output size increases only slightly (if any) and (b) the time to compute the output does not significantly increase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11639v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3690624.3709184</arxiv:DOI>
      <dc:creator>Mohsen Dehghankar, Rahul Raychaudhury, Stavros Sintos, Abolfazl Asudeh</dc:creator>
    </item>
    <item>
      <title>Deterministic $(2/3-\varepsilon)$-Approximation of Matroid Intersection Using Nearly-Linear Independence-Oracle Queries</title>
      <link>https://arxiv.org/abs/2410.18820</link>
      <description>arXiv:2410.18820v3 Announce Type: replace 
Abstract: In the matroid intersection problem, we are given two matroids $\mathcal{M}_1 = (V, \mathcal{I}_1)$ and $\mathcal{M}_2 = (V, \mathcal{I}_2)$ defined on the same ground set $V$ of $n$ elements, and the objective is to find a common independent set $S \in \mathcal{I}_1 \cap \mathcal{I}_2$ of largest possible cardinality, denoted by $r$. In this paper, we consider a deterministic matroid intersection algorithm with only a nearly linear number of independence oracle queries. Our contribution is to present a deterministic $O(\frac{n}{\varepsilon} + r \log r)$-independence-query $(2/3-\varepsilon)$-approximation algorithm for any $\varepsilon &gt; 0$. Our idea is very simple: we apply a recent $\tilde{O}(n \sqrt{r}/\varepsilon)$-independence-query $(1 - \varepsilon)$-approximation algorithm of Blikstad [ICALP 2021], but terminate it before completion. Moreover, we also present a semi-streaming algorithm for $(2/3 -\varepsilon)$-approximation of matroid intersection in $O(1/\varepsilon)$ passes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18820v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatsuya Terao</dc:creator>
    </item>
    <item>
      <title>Parallel Contraction Hierarchies Can Be Efficient and Scalable</title>
      <link>https://arxiv.org/abs/2412.18008</link>
      <description>arXiv:2412.18008v2 Announce Type: replace 
Abstract: Contraction Hierarchies (CH) (Geisberger et al., 2008) is one of the most widely used algorithms for shortest-path queries on road networks. Compared to Dijkstra's algorithm, CH enables orders of magnitude faster query performance through a preprocessing phase, which iteratively categorizes vertices into hierarchies and adds shortcuts. However, constructing a CH is an expensive task. Existing solutions, including parallel ones, may suffer from long construction time. Especially in our experiments, we observe that existing parallel solutions demonstrate unsatisfactory scalability and have close performance to sequential algorithms.
  In this paper, we present SPoCH (Scalable Parallelization of Contraction Hierarchies), an efficient and scalable CH construction algorithm in parallel. To address the challenges in previous work, our improvements focus on both redesigning the algorithm and leveraging parallel data structures. %to maintain the original and shortcut edges dynamically. We implement our algorithm and compare it with the state-of-the-art sequential and parallel implementations on 13 graphs, including road networks, synthetic graphs, and k-NN graphs. Our experiments show that SPoCH achieves 17-131x speedups in CH construction over the best baseline, while maintaining competitive query performance and CH graph size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18008v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zijin Wan, Xiaojun Dong, Letong Wang, Enzuo Zhu, Yan Gu, Yihan Sun</dc:creator>
    </item>
    <item>
      <title>Potential-Based Greedy Matching for Dynamic Delivery Pooling</title>
      <link>https://arxiv.org/abs/2502.16862</link>
      <description>arXiv:2502.16862v2 Announce Type: replace 
Abstract: We study the pooling of multiple orders into a single trip, a strategy widely adopted by online delivery platforms. When an order has to be dispatched, the platform must determine which (if any) of the other available orders to pool it with, weighing the immediate efficiency gains against the uncertain, differential benefits of holding each order for future pooling opportunities. In this paper, we demonstrate the effectiveness of using the length of each job as its opportunity cost, via a potential-based greedy algorithm (PB). The algorithm is very simple, pooling each departing job with the available job that maximizes the savings in travel distance minus a half of its distance (i.e. the potential). On the theoretical front, we show that PB significantly improves upon a naive greedy algorithm in terms of worst-case performance: as the density of the market increases, the regret per job vanishes under PB but remains constant under naive greedy. In addition, we show that the potential approximates the marginal cost of dispatching each job in a stochastic setting with sufficient density. Moreover, we conduct extensive numerical experiments and show that despite its simplicity, PB consistently outperforms a number of benchmark algorithms, including (i) batching-based heuristics that are widely used in practice, and (ii) forecast-aware heuristics that estimate the marginal costs of dispatching different jobs using historical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16862v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongyao Ma, Will Ma, Matias Romero</dc:creator>
    </item>
    <item>
      <title>New Results on a General Class of Minimum Norm Optimization Problems</title>
      <link>https://arxiv.org/abs/2504.13489</link>
      <description>arXiv:2504.13489v2 Announce Type: replace 
Abstract: We study the general norm optimization for combinatorial problems, initiated by Chakrabarty and Swamy (STOC 2019). We propose a general formulation that captures a large class of combinatorial structures: we are given a set $U$ of $n$ weighted elements and a family of feasible subsets $F$. Each subset $S\in F$ is called a feasible solution/set of the problem. We denote the value vector by $v=\{v_i\}_{i\in [n]}$, where $v_i\geq 0$ is the value of element $i$. For any subset $S\subseteq U$, we use $v[S]$ to denote the $n$-dimensional vector $\{v_e\cdot \mathbf{1}[e\in S]\}_{e\in U}$. Let $f: \mathbb{R}^n\rightarrow\mathbb{R}_+$ be a symmetric monotone norm function. Our goal is to minimize the norm objective $f(v[S])$ over feasible subset $S\in F$.
  We present a general equivalent reduction of the norm minimization problem to a multi-criteria optimization problem with logarithmic budget constraints, up to a constant approximation factor. Leveraging this reduction, we obtain constant factor approximation algorithms for the norm minimization versions of several covering problems, such as interval cover, multi-dimensional knapsack cover, and logarithmic factor approximation for set cover. We also study the norm minimization versions for perfect matching, $s$-$t$ path and $s$-$t$ cut. We show the natural linear programming relaxations for these problems have a large integrality gap. To complement the negative result, we show that, for perfect matching, there is a bi-criteria result: for any constant $\epsilon,\delta&gt;0$, we can find in polynomial time a nearly perfect matching (i.e., a matching that matches at least $1-\epsilon$ proportion of vertices) and its cost is at most $(8+\delta)$ times of the optimum for perfect matching. Moreover, we establish the existence of a polynomial-time $O(\log\log n)$-approximation algorithm for the norm minimization variant of the $s$-$t$ path problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13489v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kuowen Chen, Jian Li, Yuval Rabani, Yiran Zhang</dc:creator>
    </item>
    <item>
      <title>Tight Complexity Bounds for Counting Generalized Dominating Sets in Bounded-Treewidth Graphs Part I: Algorithmic Results</title>
      <link>https://arxiv.org/abs/2211.04278</link>
      <description>arXiv:2211.04278v3 Announce Type: replace-cross 
Abstract: We investigate how efficiently a well-studied family of domination-type problems can be solved on bounded-treewidth graphs. For sets $\sigma,\rho$ of non-negative integers, a $(\sigma,\rho)$-set of a graph $G$ is a set $S$ of vertices such that $|N(u)\cap S|\in \sigma$ for every $u\in S$, and $|N(v)\cap S|\in \rho$ for every $v\not\in S$. The problem of finding a $(\sigma,\rho)$-set (of a certain size) unifies standard problems such as Independent Set, Dominating Set, Independent Dominating Set, and many others.
  For all pairs of finite or cofinite sets $(\sigma,\rho)$, we determine (under standard complexity assumptions) the best possible value $c_{\sigma,\rho}$ such that there is an algorithm that counts $(\sigma,\rho)$-sets in time $c_{\sigma,\rho}^{\sf tw}\cdot n^{O(1)}$ (if a tree decomposition of width ${\sf tw}$ is given in the input). For example, for the Exact Independent Dominating Set problem (also known as Perfect Code) corresponding to $\sigma=\{0\}$ and $\rho=\{1\}$, we improve the $3^{\sf tw}\cdot n^{O(1)}$ algorithm of [van Rooij, 2020] to $2^{\sf tw}\cdot n^{O(1)}$.
  Despite the unusually delicate definition of $c_{\sigma,\rho}$, an accompanying paper shows that our algorithms are most likely optimal, that is, for any pair $(\sigma, \rho)$ of finite or cofinite sets where the problem is non-trivial, and any $\varepsilon&gt;0$, a $(c_{\sigma,\rho}-\varepsilon)^{\sf tw}\cdot n^{O(1)}$-algorithm counting the number of $(\sigma,\rho)$-sets would violate the Counting Strong Exponential-Time Hypothesis (#SETH). For finite sets $\sigma$ and $\rho$, these lower bounds also extend to the decision version, and hence, our algorithms are optimal in this setting as well. In contrast, for many cofinite sets, we show that further significant improvements for the decision and optimization versions are possible using the technique of representative sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.04278v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Focke, D\'aniel Marx, Fionn Mc Inerney, Daniel Neuen, Govind S. Sankar, Philipp Schepper, Philip Wellnitz</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Neural Computation in Superposition</title>
      <link>https://arxiv.org/abs/2409.15318</link>
      <description>arXiv:2409.15318v2 Announce Type: replace-cross 
Abstract: Superposition, the ability of neural networks to represent more features than neurons, is increasingly seen as key to the efficiency of large models. This paper investigates the theoretical foundations of computing in superposition, establishing complexity bounds for explicit, provably correct algorithms.
  We present the first lower bounds for a neural network computing in superposition, showing that for a broad class of problems, including permutations and pairwise logical operations, computing $m'$ features in superposition requires at least $\Omega(\sqrt{m' \log m'})$ neurons and $\Omega(m' \log m')$ parameters. This implies the first subexponential upper bound on superposition capacity: a network with $n$ neurons can compute at most $O(n^2 / \log n)$ features. Conversely, we provide a nearly tight constructive upper bound: logical operations like pairwise AND can be computed using $O(\sqrt{m'} \log m')$ neurons and $O(m' \log^2 m')$ parameters. There is thus an exponential gap between the complexity of computing in superposition (the subject of this work) versus merely representing features, which can require as little as $O(\log m')$ neurons based on the Johnson-Lindenstrauss Lemma.
  Our hope is that our results open a path for using complexity theoretic techniques in neural network interpretability research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15318v2</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.NE</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micah Adler, Nir Shavit</dc:creator>
    </item>
    <item>
      <title>Algorithmic contiguity from low-degree conjecture and applications in correlated random graphs</title>
      <link>https://arxiv.org/abs/2502.09832</link>
      <description>arXiv:2502.09832v2 Announce Type: replace-cross 
Abstract: In this paper, assuming a natural strengthening of the low-degree conjecture, we provide evidence of computational hardness for two problems: (1) the (partial) matching recovery problem in the sparse correlated Erd\H{o}s-R\'enyi graphs $\mathcal G(n,q;\rho)$ when the edge-density $q=n^{-1+o(1)}$ and the correlation $\rho&lt;\sqrt{\alpha}$ lies below the Otter's threshold, solving a remaining problem in \cite{DDL23+}; (2) the detection problem between the correlated sparse stochastic block model $\mathcal S(n,\tfrac{\lambda}{n};k,\epsilon;s)$ and a pair of independent stochastic block models $\mathcal S(n,\tfrac{\lambda s}{n};k,\epsilon)$ when $\epsilon^2 \lambda s&lt;1$ lies below the Kesten-Stigum (KS) threshold and $s&lt;\sqrt{\alpha}$ lies below the Otter's threshold, solving a remaining problem in \cite{CDGL24+}.
  One of the main ingredient in our proof is to derive certain forms of \emph{algorithmic contiguity} between two probability measures based on bounds on their low-degree advantage. To be more precise, consider the high-dimensional hypothesis testing problem between two probability measures $\mathbb{P}$ and $\mathbb{Q}$ based on the sample $\mathsf Y$. We show that if the low-degree advantage $\mathsf{Adv}_{\leq D} \big( \frac{\mathrm{d}\mathbb{P}}{\mathrm{d}\mathbb{Q}} \big)=O(1)$, then (assuming the low-degree conjecture) there is no efficient algorithm $\mathcal A$ such that $\mathbb{Q}(\mathcal A(\mathsf Y)=0)=1-o(1)$ and $\mathbb{P}(\mathcal A(\mathsf Y)=1)=\Omega(1)$. This framework provides a useful tool for performing reductions between different inference tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09832v2</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>Generalized De Bruijn Words, Invertible Necklaces, and the Burrows-Wheeler Transform</title>
      <link>https://arxiv.org/abs/2502.12844</link>
      <description>arXiv:2502.12844v2 Announce Type: replace-cross 
Abstract: We define generalized de Bruijn words as those words having a Burrows-Wheeler transform that is a concatenation of permutations of the alphabet. We show that generalized de Bruijn words are in 1-to-1 correspondence with Hamiltonian cycles in the generalized de Bruijn graphs introduced in the early '80s in the context of network design. When the size of the alphabet is a prime $p$, we define invertible necklaces as those whose BWT-matrix is non-singular. We show that invertible necklaces of length $n$ correspond to normal bases of the finite field $F_{p^n}$, and that they form an Abelian group isomorphic to the Reutenauer group $RG_p^n$. Using known results in abstract algebra, we can make a bridge between generalized de Bruijn words and invertible necklaces. In particular, we highlight a correspondence between binary de Bruijn words of order $d+1$, binary necklaces of length $2^{d}$ having an odd number of $1$'s, invertible BWT matrices of size $2^{d}\times 2^{d}$, and normal bases of the finite field $F_{2^{2^{d}}}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12844v2</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.FL</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Fici, Est\'eban Gabory</dc:creator>
    </item>
    <item>
      <title>Improved Approximation Algorithms for Three-Dimensional Bin Packing</title>
      <link>https://arxiv.org/abs/2503.08863</link>
      <description>arXiv:2503.08863v2 Announce Type: replace-cross 
Abstract: We study three fundamental three-dimensional (3D) geometric packing problems: 3D (Geometric) Bin Packing (3D-BP), 3D Strip Packing (3D-SP), and Minimum Volume Bounding Box (3D-MVBB), where given a set of 3D (rectangular) cuboids, the goal is to find an axis-aligned nonoverlapping packing of all cuboids. In 3D-BP, we need to pack the given cuboids into the minimum number of unit cube bins. In 3D-SP, we need to pack them into a 3D cuboid with a unit square base and minimum height. Finally, in 3D-MVBB, the goal is to pack into a cuboid box of minimum volume.
  It is NP-hard to even decide whether a set of rectangles can be packed into a unit square bin -- giving an (absolute) approximation hardness of 2 for 3D-BP and 3D-SP. The previous best (absolute) approximation for all three problems is by Li and Cheng (SICOMP, 1990), who gave algorithms with approximation ratios of 13, $46/7$, and $46/7+\varepsilon$, respectively, for 3D-BP, 3D-SP, and 3D-MVBB. We provide improved approximation ratios of 6, 6, and $3+\varepsilon$, respectively, for the three problems, for any constant $\varepsilon &gt; 0$.
  For 3D-BP, in the asymptotic regime, Bansal, Correa, Kenyon, and Sviridenko (Math.~Oper.~Res., 2006) showed that there is no asymptotic polynomial-time approximation scheme (APTAS) even when all items have the same height. Caprara (Math.~Oper.~Res., 2008) gave an asymptotic approximation ratio of $T_{\infty}^2 + \varepsilon\approx 2.86$, where $T_{\infty}$ is the well-known Harmonic constant in Bin Packing. We provide an algorithm with an improved asymptotic approximation ratio of $3 T_{\infty}/2 +\varepsilon \approx 2.54$. Further, we show that unlike 3D-BP (and 3D-SP), 3D-MVBB admits an APTAS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08863v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debajyoti Kar, Arindam Khan, Malin Rau</dc:creator>
    </item>
  </channel>
</rss>
