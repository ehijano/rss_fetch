<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Jul 2024 04:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 26 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Semiparametric Piecewise Accelerated Failure Time Model for the Analysis of Immune-Oncology Clinical Trials</title>
      <link>https://arxiv.org/abs/2407.17658</link>
      <description>arXiv:2407.17658v1 Announce Type: new 
Abstract: Effectiveness of immune-oncology chemotherapies has been presented in recent clinical trials. The Kaplan-Meier estimates of the survival functions of the immune therapy and the control often suggested the presence of the lag-time until the immune therapy began to act. It implies the use of hazard ratio under the proportional hazards assumption would not be appealing, and many alternatives have been investigated such as the restricted mean survival time. In addition to such overall summary of the treatment contrast, the lag-time is also an important feature of the treatment effect. Identical survival functions up to the lag-time implies patients who are likely to die before the lag-time would not benefit the treatment and identifying such patients would be very important. We propose the semiparametric piecewise accelerated failure time model and its inference procedure based on the semiparametric maximum likelihood method. It provides not only an overall treatment summary, but also a framework to identify patients who have less benefit from the immune-therapy in a unified way. Numerical experiments confirm that each parameter can be estimated with minimal bias. Through a real data analysis, we illustrate the evaluation of the effect of immune-oncology therapy and the characterization of covariates in which patients are unlikely to receive the benefit of treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17658v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hisato Sunami, Satoshi Hattori</dc:creator>
    </item>
    <item>
      <title>Comparison of global sensitivity analysis methods for a fire spread model with a segmented characteristic</title>
      <link>https://arxiv.org/abs/2407.17718</link>
      <description>arXiv:2407.17718v1 Announce Type: new 
Abstract: Global sensitivity analysis (GSA) can provide rich information for controlling output uncertainty. In practical applications, segmented models are commonly used to describe an abrupt model change. For segmented models, the complicated uncertainty propagation during the transition region may lead to different importance rankings of different GSA methods. If an unsuitable GSA method is applied, misleading results will be obtained, resulting in suboptimal or even wrong decisions. In this paper, four GSA indices, i.e., Sobol index, mutual information, delta index and PAWN index, are applied for a segmented fire spread model (Dry Eucalypt). The results show that four GSA indices give different importance rankings during the transition region since segmented characteristics affect different GSA indices in different ways. We suggest that analysts should rely on the results of different GSA indices according to their practical purpose, especially when making decisions for segmented models during the transition region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17718v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shi-Shun Chen, Xiao-Yang Li</dc:creator>
    </item>
    <item>
      <title>A new moment-independent uncertainty importance measure based on cumulative residual entropy for developing uncertainty reduction strategies</title>
      <link>https://arxiv.org/abs/2407.17719</link>
      <description>arXiv:2407.17719v1 Announce Type: new 
Abstract: Uncertainty reduction is vital for improving system reliability and reducing risks. To identify the best target for uncertainty reduction, uncertainty importance measure is commonly used to prioritize the significance of input variable uncertainties. Then, designers will take steps to reduce the uncertainties of variables with high importance. However, for variables with minimal uncertainty, the cost of controlling their uncertainties can be unacceptable. Therefore, uncertainty magnitude should also be considered in developing uncertainty reduction strategies. Although variance-based methods have been developed for this purpose, they are dependent on statistical moments and have limitations when dealing with highly-skewed distributions that are commonly encountered in practical applications. Motivated by this problem, we propose a new uncertainty importance measure based on cumulative residual entropy. The proposed measure is moment-independent based on the cumulative distribution function, which can handle the highly-skewed distributions properly. Numerical implementations for estimating the proposed measure are devised and verified. A real-world engineering case considering highly-skewed distributions is introduced to show the procedure of developing uncertainty reduction strategies considering uncertainty magnitude and corresponding cost. The results demonstrate that the proposed measure can present a different uncertainty reduction recommendation compared to the variance-based approach because of its moment-independent characteristic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17719v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shi-Shun Chen, Xiao-Yang Li</dc:creator>
    </item>
    <item>
      <title>Regularized Adjusted Plus-Minus Models for Evaluating and Scouting Football (Soccer) Players using Possession Sequences</title>
      <link>https://arxiv.org/abs/2407.17832</link>
      <description>arXiv:2407.17832v1 Announce Type: new 
Abstract: This paper presents a novel framework for evaluating players in association football (soccer). Our method uses possession sequences, i.e. sequences of consecutive on-ball actions, for deriving estimates for player strengths. On the surface, the methodology is similar to classical adjusted plus-minus rating models using mainly regularized regression techniques. However, by analyzing possessions, our framework is able to distinguish on-ball and off-ball contributions of players to the game. From a methodological viewpoint, the framework explores four different penalization schemes, which exploit football-specific structures such as the grouping of players into position groups as well as into common strength groups. These four models lead to four ways to rate players by considering the respective estimate of each model corresponding to the player. The ratings are used to analyze the 2017/18 season of the Spanish La Liga. We compare similarities as well as particular use cases of each of the penalized models and provide guidance for practitioners when using the individual model specifications. Finally, we conclude our analysis by providing a domain-specific statistical evaluation framework, which highlights the potential of the penalized regression approaches for evaluating players.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17832v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Bajons, Kurt Hornik</dc:creator>
    </item>
    <item>
      <title>Driving pattern interpretation based on action phases clustering</title>
      <link>https://arxiv.org/abs/2407.17518</link>
      <description>arXiv:2407.17518v1 Announce Type: cross 
Abstract: Current approaches to identifying driving heterogeneity face challenges in comprehending fundamental patterns from the perspective of underlying driving behavior mechanisms. The concept of Action phases was proposed in our previous work, capturing the diversity of driving characteristics with physical meanings. This study presents a novel framework to further interpret driving patterns by classifying Action phases in an unsupervised manner. In this framework, a Resampling and Downsampling Method (RDM) is first applied to standardize the length of Action phases. Then the clustering calibration procedure including ''Feature Selection'', ''Clustering Analysis'', ''Difference/Similarity Evaluation'', and ''Action phases Re-extraction'' is iteratively applied until all differences among clusters and similarities within clusters reach the pre-determined criteria. Application of the framework using real-world datasets revealed six driving patterns in the I80 dataset, labeled as ''Catch up'', ''Keep away'', and ''Maintain distance'', with both ''Stable'' and ''Unstable'' states. Notably, Unstable patterns are more numerous than Stable ones. ''Maintain distance'' is the most common among Stable patterns. These observations align with the dynamic nature of driving. Two patterns ''Stable keep away'' and ''Unstable catch up'' are missing in the US101 dataset, which is in line with our expectations as this dataset was previously shown to have less heterogeneity. This demonstrates the potential of driving patterns in describing driving heterogeneity. The proposed framework promises advantages in addressing label scarcity in supervised learning and enhancing tasks such as driving behavior modeling and driving trajectory prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17518v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xue Yao, Simeon C. Calvert, Serge P. Hoogendoorn</dc:creator>
    </item>
    <item>
      <title>Mapping the Technological Future: A Topic, Sentiment, and Emotion Analysis in Social Media Discourse</title>
      <link>https://arxiv.org/abs/2407.17522</link>
      <description>arXiv:2407.17522v1 Announce Type: cross 
Abstract: People worldwide are currently confronted with a number of technological challenges, which act as a potent source of uncertainty. The uncertainty arising from the volatility and unpredictability of technology (such as AI) and its potential consequences is widely discussed on social media. This study uses BERTopic modelling along with sentiment and emotion analysis on 1.5 million tweets from 2021 to 2023 to identify anticipated tech-driven futures and capture the emotions communicated by 400 key opinion leaders (KOLs). Findings indicate positive sentiment significantly outweighs negative, with a prevailing dominance of positive anticipatory emotions. Specifically, the 'Hope' score is approximately 10.33\% higher than the median 'Anxiety' score. KOLs emphasize 'Optimism' and benefits over 'Pessimism' and challenges. The study emphasizes the important role KOLs play in shaping future visions through anticipatory discourse and emotional tone during times of technological uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17522v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alina Landowska, Maciej Skorski, Krzysztof Rajda</dc:creator>
    </item>
    <item>
      <title>Integrating Ensemble Kalman Filter with AI-based Weather Prediction Model ClimaX</title>
      <link>https://arxiv.org/abs/2407.17781</link>
      <description>arXiv:2407.17781v1 Announce Type: cross 
Abstract: Artificial intelligence (AI)-based weather prediction research is growing rapidly and has shown to be competitive with the advanced dynamic numerical weather prediction models. However, research combining AI-based weather prediction models with data assimilation remains limited partially because long-term sequential data assimilation cycles are required to evaluate data assimilation systems. This study explores integrating the local ensemble transform Kalman filter (LETKF) with an AI-based weather prediction model ClimaX. Our experiments demonstrated that the ensemble data assimilation cycled stably for the AI-based weather prediction model using covariance inflation and localization techniques inside the LETKF. While ClimaX showed some limitations in capturing flow-dependent error covariance compared to dynamical models, the AI-based ensemble forecasts provided reasonable and beneficial error covariance in sparsely observed regions. These findings highlight the potential of AI models in weather forecasting and the importance of physical consistency and accurate error growth representation in improving ensemble data assimilation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17781v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shunji Kotsuki, Kenta Shiraishi, Atsushi Okazaki</dc:creator>
    </item>
    <item>
      <title>Predicting COVID-19 hospitalisation using a mixture of Bayesian predictive syntheses</title>
      <link>https://arxiv.org/abs/2308.06134</link>
      <description>arXiv:2308.06134v3 Announce Type: replace 
Abstract: This paper proposes a novel methodology called the mixture of Bayesian predictive syntheses (MBPS) for multiple time series count data for the challenging task of predicting the numbers of COVID-19 inpatients and isolated cases in Japan and Korea at the subnational-level. MBPS combines a set of predictive models and partitions the multiple time series into clusters based on their contribution to predicting the outcome. In this way, MBPS leverages the shared information within each cluster and is suitable for predicting COVID-19 inpatients since the data exhibit similar dynamics over multiple areas. Also, MBPS avoids using a multivariate count model, which is generally cumbersome to develop and implement. Our Japanese and Korean data analyses demonstrate that the proposed MBPS methodology has improved predictive accuracy and uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.06134v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Genya Kobayashi, Shonosuke Sugasawa, Yuki Kawakubo, Dongu Han, Taeryon Choi</dc:creator>
    </item>
    <item>
      <title>Reliability modeling and statistical inference of accelerated degradation data with memory effects and unit-to-unit variability</title>
      <link>https://arxiv.org/abs/2310.18567</link>
      <description>arXiv:2310.18567v2 Announce Type: replace 
Abstract: Accelerated degradation testing (ADT) is an effective way to evaluate the lifetime and reliability of highly reliable products. Markovian stochastic processes are usually applied to describe the degradation process. However, the degradation processes of some products are non-Markovian due to the interaction with environments. Besides, owing to the differences in materials and manufacturing processes, products from the same population exhibit diverse degradation paths. Motivated by this issue, an ADT model with memory effects and unit-to-unit variability (UtUV) is proposed in this article. The memory effect in the degradation process is captured by the fractional Brownian motion (FBM) and the UtUV is considered in the acceleration model. Then, the lifetime and reliability under the normal operating condition are presented. To give an accurate estimation of the memory effect, a statistical inference method is devised based on the expectation maximization (EM) algorithm. The effectiveness of the proposed method is verified by a simulation case and a microwave case. It is shown that the estimation of the memory effect obtained by the EM algorithm is much more accurate than the traditional method. Moreover, without considering UtUV in the ADT model, the estimation of the memory effect can be highly biased. The proposed ADT model is superior in both deterministic degradation trend predictions and degradation boundary quantification compared to existing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18567v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shi-Shun Chen, Xiao-Yang Li, Wenrui Xie</dc:creator>
    </item>
    <item>
      <title>One-shot Generative Distribution Matching for Augmented RF-based UAV Identification</title>
      <link>https://arxiv.org/abs/2301.08403</link>
      <description>arXiv:2301.08403v4 Announce Type: replace-cross 
Abstract: This work addresses the challenge of identifying Unmanned Aerial Vehicles (UAV) using radiofrequency (RF) fingerprinting in limited RF environments. The complexity and variability of RF signals, influenced by environmental interference and hardware imperfections, often render traditional RF-based identification methods ineffective. To address these complications, the study introduces the rigorous use of one-shot generative methods for augmenting transformed RF signals, offering a significant improvement in UAV identification. This approach shows promise in low-data regimes, outperforming deep generative methods like conditional generative adversarial networks (GANs) and variational auto-encoders (VAEs). The paper provides a theoretical guarantee for the effectiveness of one-shot generative models in augmenting limited data, setting a precedent for their application in limited RF environments. This research contributes to learning techniques in low-data regime scenarios, which may include atypical complex sequences beyond images and videos. The code and links to datasets used in this study are available at https://github.com/amir-kazemi/uav-rf-id.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08403v4</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amir Kazemi, Salar Basiri, Volodymyr Kindratenko, Srinivasa Salapaka</dc:creator>
    </item>
    <item>
      <title>Regularization and Model Selection for Ordinal-on-Ordinal Regression with Applications to Food Products' Testing and Survey Data</title>
      <link>https://arxiv.org/abs/2309.16373</link>
      <description>arXiv:2309.16373v2 Announce Type: replace-cross 
Abstract: Ordinal data are quite common in applied statistics. Although some model selection and regularization techniques for categorical predictors and ordinal response models have been developed over the past few years, less work has been done concerning ordinal-on-ordinal regression. Motivated by a consumer test and a survey on the willingness to pay for luxury food products consisting of Likert-type items, we propose a strategy for smoothing and selecting ordinally scaled predictors in the cumulative logit model. First, the group lasso is modified by the use of difference penalties on neighboring dummy coefficients, thus taking into account the predictors' ordinal structure. Second, a fused lasso-type penalty is presented for the fusion of predictor categories and factor selection. The performance of both approaches is evaluated in simulation studies and on real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16373v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aisouda Hoshiyar, Laura H. Gertheiss, Jan Gertheiss</dc:creator>
    </item>
    <item>
      <title>Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK package in R and Python</title>
      <link>https://arxiv.org/abs/2402.02290</link>
      <description>arXiv:2402.02290v2 Announce Type: replace-cross 
Abstract: We introduce the QuadratiK package that incorporates innovative data analysis methodologies. The presented software, implemented in both R and Python, offers a comprehensive set of goodness-of-fit tests and clustering techniques using kernel-based quadratic distances, thereby bridging the gap between the statistical and machine learning literatures. Our software implements one, two and k-sample tests for goodness of fit, providing an efficient and mathematically sound way to assess the fit of probability distributions. Expanded capabilities of our software include supporting tests for uniformity on the d-dimensional Sphere based on Poisson kernel densities. Particularly noteworthy is the incorporation of a unique clustering algorithm specifically tailored for spherical data that leverages a mixture of Poisson kernel-based densities on the sphere. Alongside this, our software includes additional graphical functions, aiding the users in validating, as well as visualizing and representing clustering results. This enhances interpretability and usability of the analysis. In summary, our R and Python packages serve as a powerful suite of tools, offering researchers and practitioners the means to delve deeper into their data, draw robust inference, and conduct potentially impactful analyses and inference across a wide array of disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02290v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>cs.MS</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Giovanni Saraceno, Marianthi Markatou, Raktim Mukhopadhyay, Mojgan Golzy</dc:creator>
    </item>
    <item>
      <title>Improving probabilistic forecasts of extreme wind speeds by training statistical post-processing models with weighted scoring rules</title>
      <link>https://arxiv.org/abs/2407.15900</link>
      <description>arXiv:2407.15900v2 Announce Type: replace-cross 
Abstract: Accurate forecasts of extreme wind speeds are of high importance for many applications. Such forecasts are usually generated by ensembles of numerical weather prediction (NWP) models, which however can be biased and have errors in dispersion, thus necessitating the application of statistical post-processing techniques. In this work we aim to improve statistical post-processing models for probabilistic predictions of extreme wind speeds. We do this by adjusting the training procedure used to fit ensemble model output statistics (EMOS) models - a commonly applied post-processing technique - and propose estimating parameters using the so-called threshold-weighted continuous ranked probability score (twCRPS), a proper scoring rule that places special emphasis on predictions over a threshold. We show that training using the twCRPS leads to improved extreme event performance of post-processing models for a variety of thresholds. We find a distribution body-tail trade-off where improved performance for probabilistic predictions of extreme events comes with worse performance for predictions of the distribution body. However, we introduce strategies to mitigate this trade-off based on weighted training and linear pooling. Finally, we consider some synthetic experiments to explain the training impact of the twCRPS and derive closed-form expressions of the twCRPS for a number of distributions, giving the first such collection in the literature. The results will enable researchers and practitioners alike to improve the performance of probabilistic forecasting models for extremes and other events of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15900v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jakob Benjamin Wessel, Christopher A. T. Ferro, Gavin R. Evans, Frank Kwasniok</dc:creator>
    </item>
  </channel>
</rss>
