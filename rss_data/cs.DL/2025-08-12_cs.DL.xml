<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Aug 2025 01:22:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Can Smaller Large Language Models Evaluate Research Quality?</title>
      <link>https://arxiv.org/abs/2508.07196</link>
      <description>arXiv:2508.07196v1 Announce Type: new 
Abstract: Although both Google Gemini (1.5 Flash) and ChatGPT (4o and 4o-mini) give research quality evaluation scores that correlate positively with expert scores in nearly all fields, and more strongly that citations in most, it is not known whether this is true for smaller Large Language Models (LLMs). In response, this article assesses Google's Gemma-3-27b-it, a downloadable LLM (60Gb). The results for 104,187 articles show that Gemma-3-27b-it scores correlate positively with an expert research quality score proxy for all 34 Units of Assessment (broad fields) from the UK Research Excellence Framework 2021. The Gemma-3-27b-it correlations have 83.8% of the strength of ChatGPT 4o and 94.7% of the strength of ChatGPT 4o-mini correlations. Differently from the two larger LLMs, the Gemma-3-27b-it correlations do not increase substantially when the scores are averaged across five repetitions, its scores tend to be lower, and its reports are relatively uniform in style. Overall, the results show that research quality score estimation can be conducted by offline LLMs, so this capability is not an emergent property of the largest LLMs. Moreover, score improvement through repetition is not a universal feature of LLMs. In conclusion, although the largest LLMs still have the highest research evaluation score estimation capability, smaller ones can also be used for this task, and this can be helpful for cost saving or when secure offline processing is needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07196v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike Thelwall</dc:creator>
    </item>
    <item>
      <title>Citation Issues in Wave Mechanics Theory of Microwave Absorption</title>
      <link>https://arxiv.org/abs/2508.06522</link>
      <description>arXiv:2508.06522v1 Announce Type: cross 
Abstract: The wave mechanics theory of microwave absorption challenges the long-standing impedance-matching and quarter-wavelength paradigms by demonstrating that conventional models mistakenly conflate bulk material parameters with thin-film phenomena. Drawing on a corpus of 35 peer-reviewed papers and preprints, the study performs a citation-pattern analysis and a logical audit of established theory. Results reveal a striking asymmetry in scholarly engagement, only a handful of supportive or neutral citations appear amid widespread silence, alongside critical logical flaws in impedance matching, notably its inconsistent treatment of penetration, reflection, and absorption from film. By re-framing absorption as a wave-mechanics process governed by interference at parallel interfaces, the wave mechanics framework restores energy-conservation consistency and provides experimentally verified design rules for film thickness, phase response, and broadband performance. The paper further situates the citation neglect within broader issues of peer-review bias and paradigm inertia, illustrating how cargo-cult scientific practices can impede theoretical progress. Recommendations are offered for researchers, editors, and institutions to foster open discourse, rigorously test competing models, and update curricula and design tools accordingly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06522v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.DL</category>
      <category>physics.optics</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yue Liu, Ying Liu, Michael G. B. Drew</dc:creator>
    </item>
    <item>
      <title>How is science discussed on Bluesky?</title>
      <link>https://arxiv.org/abs/2507.18840</link>
      <description>arXiv:2507.18840v2 Announce Type: replace 
Abstract: Amid the migration of academics from X, the social media platform Bluesky has emerged as a potential alternative. To assess its viability and relevance for science communication, this study presents the first large-scale analysis of scholarly article dissemination on Bluesky, exploring its potential as a new source of social media metrics. We collected and analysed over 2.6 million Bluesky posts referencing 532,302 scholarly articles from January 2023 to July 2025, integrating metadata from the OpenAlex database. Temporal trends, disciplinary coverage, language use, textual characteristics, and user engagement were examined. A sharp increase in scholarly activity on Bluesky was observed from November 2024 to January 2025, coinciding with broader academic shifts away from X. As on X, Bluesky posts primarily concern the health, social, and environmental sciences and are predominantly written in English. Nevertheless, Bluesky posts demonstrate substantially higher levels of interaction (likes, reposts, replies, and quotes) and greater textual originality than previously reported for X, suggesting both stronger interactive and more interpretive engagement. These findings highlight Bluesky's emerging role as a credible platform for science communication and a promising source for altmetrics. The platform may facilitate not only early visibility of research outputs but also more meaningful scholarly dialogue in the evolving social media landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18840v2</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Er-Te Zheng, Xiaorui Jiang, Zhichao Fang, Mike Thelwall</dc:creator>
    </item>
  </channel>
</rss>
