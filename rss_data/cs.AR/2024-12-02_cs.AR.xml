<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Dec 2024 04:26:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Stoch-IMC: A Bit-Parallel Stochastic In-Memory Computing Architecture Based on STT-MRAM</title>
      <link>https://arxiv.org/abs/2411.19344</link>
      <description>arXiv:2411.19344v1 Announce Type: new 
Abstract: In-memory computing (IMC) offloads parts of the computations to memory to fulfill the performance and energy demands of applications such as neuromorphic computing, machine learning, and image processing. Fortunately, the main features that stochastic computing (SC) and IMC share, which are low computation complexity and high bit-parallel computation capability, promise great potential for integrating SC and IMC. In this paper, we exploit this potential by using stochastic computation as an approximation method to present effective in-memory computations with a good trade-off among design parameters. To this end, first, commonly used stochastic arithmetic operations of applications are effectively implemented using the primitive logic gates of the IMC method. Next, the in-memory scheduling and mapping of applications are obtained efficiently by a proposed algorithm. This algorithm reduces the computation latency by enabling intra-subarray parallelism while considering the IMC method constraints. Subsequently, a bit-parallel stochastic IMC architecture, Stoch-IMC, is presented that enables bit parallelization of stochastic computations over memory subarrays/banks. To evaluate Stoch-IMC's effectiveness, various analyses were conducted. Results show average performance improvements of 135.7X and 124.2X across applications compared to binary IMC and related in-memory SC methods, respectively. The results also demonstrate an average energy reduction of 1.5X compared to binary IMC, with limited energy overhead relative to the in-memory SC method. Furthermore, the results reveal average lifetime improvements of 4.9X and 216.3X over binary IMC and in-memory SC methods, respectively, along with high bitflip tolerance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19344v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.aeue.2024.155614</arxiv:DOI>
      <arxiv:journal_reference>AEU - International Journal of Electronics and Communications, 2024</arxiv:journal_reference>
      <dc:creator>Amir M. Hajisadeghi, Hamid R. Zarandi, Mahmoud Momtazpour</dc:creator>
    </item>
    <item>
      <title>Core Placement Optimization of Many-core Brain-Inspired Near-Storage Systems for Spiking Neural Network Training</title>
      <link>https://arxiv.org/abs/2411.19430</link>
      <description>arXiv:2411.19430v1 Announce Type: new 
Abstract: With the increasing application scope of spiking neural networks (SNN), the complexity of SNN models has surged, leading to an exponential growth in demand for AI computility. As the new generation computing architecture of the neural networks, the efficiency and power consumption of distributed storage and parallel computing in the many-core near-memory computing system have attracted much attention. Among them, the mapping problem from logical cores to physical cores is one of the research hotspots. In order to improve the computing parallelism and system throughput of the many-core near-memory computing system, and to reduce power consumption, we propose a SNN training many-core deployment optimization method based on Off-policy Deterministic Actor-Critic. We utilize deep reinforcement learning as a nonlinear optimizer, treating the many-core topology as network graph features and using graph convolution to input the many-core structure into the policy network. We update the parameters of the policy network through near-end policy optimization to achieve deployment optimization of SNN models in the many-core near-memory computing architecture to reduce chip power consumption. To handle large-dimensional action spaces, we use continuous values matching the number of cores as the output of the policy network and then discretize them again to obtain new deployment schemes. Furthermore, to further balance inter-core computation latency and improve system throughput, we propose a model partitioning method with a balanced storage and computation strategy. Our method overcomes the problems such as uneven computation and storage loads between cores, and the formation of local communication hotspots, significantly reducing model training time, communication costs, and average flow load between cores in the many-core near-memory computing architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19430v1</guid>
      <category>cs.AR</category>
      <category>cs.NE</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xueke Zhu (Pengcheng Laboratory), Wenjie Lin (Pengcheng Laboratory), Yanyu Lin (Pengcheng Laboratory), Wenxiang Cheng (Pengcheng Laboratory), Zhengyu Ma (Pengcheng Laboratory), Yonghong Tian (Pengcheng Laboratory, Peking University), Huihui Zhou (Pengcheng Laboratory)</dc:creator>
    </item>
    <item>
      <title>PREBA: A Hardware/Software Co-Design for Multi-Instance GPU based AI Inference Servers</title>
      <link>https://arxiv.org/abs/2411.19114</link>
      <description>arXiv:2411.19114v1 Announce Type: cross 
Abstract: NVIDIA's Multi-Instance GPU (MIG) is a feature that enables system designers to reconfigure one large GPU into multiple smaller GPU slices. This work characterizes this emerging GPU and evaluates its effectiveness in designing high-performance AI inference servers. Our study reveals that the data preprocessing stage of AI inference causes significant performance bottlenecks to MIG. To this end, we present PREBA, which is a hardware/software co-design targeting MIG inference servers. Our first proposition is an FPGA-based data preprocessing accelerator that unlocks the full potential of MIG with domain-specific acceleration of data preprocessing. The MIG inference server unleashed from preprocessing overheads is then augmented with our dynamic batching system that enables high-performance inference. PREBA is implemented end-to-end in real systems, providing a 3.7x improvement in throughput, 3.4x reduction in tail latency, 3.5x improvement in energy-efficiency, and 3.0x improvement in cost-efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19114v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gwangoo Yeo, Jiin Kim, Yujeong Choi, Minsoo Rhu</dc:creator>
    </item>
  </channel>
</rss>
