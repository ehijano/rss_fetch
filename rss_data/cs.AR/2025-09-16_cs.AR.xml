<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Sep 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>ReCross: Efficient Embedding Reduction Scheme for In-Memory Computing using ReRAM-Based Crossbar</title>
      <link>https://arxiv.org/abs/2509.10627</link>
      <description>arXiv:2509.10627v1 Announce Type: new 
Abstract: Deep learning-based recommendation models (DLRMs) are widely deployed in commercial applications to enhance user experience. However, the large and sparse embedding layers in these models impose substantial memory bandwidth bottlenecks due to high memory access costs and irregular access patterns, leading to increased inference time and energy consumption. While resistive random access memory (ReRAM) based crossbars offer a fast and energy-efficient solution through in-memory embedding reduction operations, naively mapping embeddings onto crossbar arrays leads to poor crossbar utilization and thus degrades performance. We present ReCross, an efficient ReRAM-based in-memory computing (IMC) scheme designed to minimize execution time and enhance energy efficiency in DLRM embedding reduction. ReCross co-optimizes embedding access patterns and ReRAM crossbar characteristics by intelligently grouping and mapping co-occurring embeddings, replicating frequently accessed embeddings across crossbars, and dynamically selecting in-memory processing operations using a newly designed dynamic switch ADC circuit that considers runtime energy trade-offs. Experimental results demonstrate that ReCross achieves a 3.97x reduction in execution time and a 6.1x improvement in energy efficiency compared to state-of-the-art IMC approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10627v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu-Hong Lai, Chieh-Lin Tsai, Wen Sheng Lim, Han-Wen Hu, Tei-Wei Kuo, Yuan-Hao Chang</dc:creator>
    </item>
    <item>
      <title>DOSA: Differentiable Model-Based One-Loop Search for DNN Accelerators</title>
      <link>https://arxiv.org/abs/2509.10702</link>
      <description>arXiv:2509.10702v1 Announce Type: new 
Abstract: In the hardware design space exploration process, it is critical to optimize both hardware parameters and algorithm-to-hardware mappings. Previous work has largely approached this simultaneous optimization problem by separately exploring the hardware design space and the mapspace - both individually large and highly nonconvex spaces - independently. The resulting combinatorial explosion has created significant difficulties for optimizers.
  In this paper, we introduce DOSA, which consists of differentiable performance models and a gradient descent-based optimization technique to simultaneously explore both spaces and identify high-performing design points. Experimental results demonstrate that DOSA outperforms random search and Bayesian optimization by 2.80x and 12.59x, respectively, in improving DNN model energy-delay product, given a similar number of samples. We also demonstrate the modularity and flexibility of DOSA by augmenting our analytical model with a learned model, allowing us to optimize buffer sizes and mappings of a real DNN accelerator and attain a 1.82x improvement in energy-delay product.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10702v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Hong, Qijing Huang, Grace Dinh, Mahesh Subedar, Yakun Sophia Shao</dc:creator>
    </item>
    <item>
      <title>Design and Analysis of Approximate Hardware Accelerators for VVC Intra Angular Prediction</title>
      <link>https://arxiv.org/abs/2509.10751</link>
      <description>arXiv:2509.10751v1 Announce Type: new 
Abstract: The Versatile Video Coding (VVC) standard significantly improves compression efficiency over its predecessor, HEVC, but at the cost of substantially higher computational complexity, particularly in intra-frame prediction. This stage employs various directional modes, each requiring multiple multiplications between reference samples and constant coefficients. To optimize these operations at hardware accelerators, multiplierless constant multiplication (MCM) blocks offer a promising solution. However, VVC's interpolation filters have more than fifty distinct coefficients, making MCM implementations resource-intensive. This work proposes an approximation method to reduce the number of interpolation coefficients by averaging fixed subsets of them, therefore decreasing MCM block size and potentially lowering circuit area and power consumption. Six different MCM block architectures for angular intra prediction are introduced, in which five use the approximation method introduced in this work, and evaluate the trade-off between coefficient reduction and coding efficiency compared with a conventional multiplier architecture. Experimental results in ten videos demonstrate that only two MCM implementations exceed a 4% BD-Rate increase and 2.6% on average in the worst case, while two of the MCM implementations have circuit area reduction of 20% and 44%. For three of the architectures, parallel sample prediction modules were synthesized, showing a reduction of 30% gate area compared to single sample processing units, and a reduction in energy consumption for two of the implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10751v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas M. Leipnitz de Fraga, Cl\'audio Machado Diniz</dc:creator>
    </item>
    <item>
      <title>always_comm: An FPGA-based Hardware Accelerator for Audio/Video Compression and Transmission</title>
      <link>https://arxiv.org/abs/2509.11503</link>
      <description>arXiv:2509.11503v1 Announce Type: new 
Abstract: We present a design for an extensible video conferencing stack implemented entirely in hardware on a Nexys4 DDR FPGA, which uses the M-JPEG codec to compress video and a UDP networking stack to communicate between the FPGA and the receiving computer. This networking stack accepts real-time updates from both the video codec and the audio controller, which means that video will be able to be streamed at 30 FPS from the FPGA to a computer. On the computer side, a Python script reads the Ethernet packets and decodes the packets into the video and the audio for real time playback. We evaluate this architecture using both functional, simulation-driven verification in Cocotb and by synthesizing SystemVerilog RTL code using Vivado for deployment on our Nexys4 DDR FPGA, where we evaluate both end-to-end latency and throughput of video transmission.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11503v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishab Parthasarathy, Akshay Attaluri, Gilford Ting</dc:creator>
    </item>
    <item>
      <title>SuperUROP: An FPGA-Based Spatial Accelerator for Sparse Matrix Operations</title>
      <link>https://arxiv.org/abs/2509.11529</link>
      <description>arXiv:2509.11529v1 Announce Type: new 
Abstract: Solving sparse systems of linear equations is a fundamental problem in the field of numerical methods, with applications spanning from circuit design to urban planning. These problems can have millions of constraints, such as when laying out transistors on a circuit, or trying to optimize traffic light timings, making fast sparse solvers extremely important. However, existing state-of-the-art software-level solutions for solving sparse linear systems, termed iterative solvers, are extremely inefficient on current hardware. This inefficiency can be attributed to two key reasons: (1) poor short-term data reuse, which causes frequent, irregular memory accesses, and (2) complex data dependencies, which limit parallelism. Hence, in this paper, we present an FPGA implementation of the existing Azul accelerator, an SRAM-only hardware accelerator that achieves both high memory bandwidth utilization and arithmetic intensity. Azul features a grid of tiles, each of which is composed of a processing element (PE) and a small independent SRAM memory, which are all connected over a network on chip (NoC). We implement Azul on FPGA using simple RISC-V CPU cores connected to a memory hierarchy of different FPGA memory modules. We utilize custom RISC-V ISA augmentations to implement a task-based programming model for the various PEs, allowing communication over the NoC. Finally, we design simple distributed test cases so that we can functionally verify the FPGA implementation, verifying equivalent performance to an architectural simulation of the Azul framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11529v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishab Parthasarathy</dc:creator>
    </item>
    <item>
      <title>LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications</title>
      <link>https://arxiv.org/abs/2509.12053</link>
      <description>arXiv:2509.12053v1 Announce Type: new 
Abstract: Modern tensor applications, especially foundation models and generative AI applications require multiple input modalities (both vision and language), which increases the demand for flexible accelerator architecture. Existing frameworks suffer from the trade-off between design flexibility and productivity of RTL generation: either limited to very few hand-written templates or cannot automatically generate the RTL. To address this challenge, we propose the LEGO framework, which targets tensor applications and automatically generates spatial architecture design and outputs synthesizable RTL code without handwritten RTL design templates. Leveraging the affine-transformation-based architecture representation, LEGO front end finds interconnections between function units, synthesizes the memory system, and fuses different spatial dataflow designs based on data reuse analysis. LEGO back end then translates the hardware in a primitive-level graph to perform lower-level optimizations, and applies a set of linear-programming algorithms to optimally insert pipeline registers and reduce the overhead of unused logic when switching spatial dataflows. Our evaluation demonstrates that LEGO can achieve 3.2x speedup and 2.4x energy efficiency compared to previous work Gemmini, and can generate one architecture for diverse modern foundation models in generative AI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12053v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/HPCA61900.2025.00101</arxiv:DOI>
      <dc:creator>Yujun Lin, Zhekai Zhang, Song Han</dc:creator>
    </item>
    <item>
      <title>Side-channel Inference of User Activities in AR/VR Using GPU Profiling</title>
      <link>https://arxiv.org/abs/2509.10703</link>
      <description>arXiv:2509.10703v1 Announce Type: cross 
Abstract: Over the past decade, AR/VR devices have drastically changed how we interact with the digital world. Users often share sensitive information, such as their location, browsing history, and even financial data, within third-party apps installed on these devices, assuming a secure environment protected from malicious actors. Recent research has revealed that malicious apps can exploit such capabilities and monitor benign apps to track user activities, leveraging fine-grained profiling tools, such as performance counter APIs. However, app-to-app monitoring is not feasible on all AR/VR devices (e.g., Meta Quest), as a concurrent standalone app execution is disabled. In this paper, we present OVRWatcher, a novel side-channel primitive for AR/VR devices that infers user activities by monitoring low-resolution (1Hz) GPU usage via a background script, unlike prior work that relies on high-resolution profiling. OVRWatcher captures correlations between GPU metrics and 3D object interactions under varying speeds, distances, and rendering scenarios, without requiring concurrent app execution, access to application data, or additional SDK installations. We demonstrate the efficacy of OVRWatcher in fingerprinting both standalone AR/VR and WebXR applications. OVRWatcher also distinguishes virtual objects, such as products in immersive shopping apps selected by real users and the number of participants in virtual meetings, thereby revealing users' product preferences and potentially exposing confidential information from those meetings. OVRWatcher achieves over 99% accuracy in app fingerprinting and over 98% accuracy in object-level inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10703v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.14722/ndss.2026.231302</arxiv:DOI>
      <dc:creator>Seonghun Son, Chandrika Mukherjee, Reham Mohamed Aburas, Berk Gulmezoglu, Z. Berkay Celik</dc:creator>
    </item>
    <item>
      <title>Coordinated Reinforcement Learning Prefetching Architecture for Multicore Systems</title>
      <link>https://arxiv.org/abs/2509.10719</link>
      <description>arXiv:2509.10719v1 Announce Type: cross 
Abstract: Hardware prefetching is critical to fill the performance gap between CPU speeds and slower memory accesses. With multicore architectures becoming commonplace, traditional prefetchers are severely challenged. Independent core operation creates significant redundancy (up to 20% of prefetch requests are duplicates), causing unnecessary memory bus traffic and wasted bandwidth. Furthermore, cutting-edge prefetchers such as Pythia suffer from about a 10% performance loss when scaling from a single-core to a four-core system. To solve these problems, we propose CRL-Pythia, a coordinated reinforcement learning based prefetcher specifically designed for multicore systems. In this work, CRL-Pythia addresses these issues by enabling cross-core sharing of information and cooperative prefetching decisions, which greatly reduces redundant prefetch requests and improves learning convergence across cores. Our experiments demonstrate that CRL-Pythia outperforms single Pythia configurations in all cases, with approximately 12% IPC (instructions per cycle) improvement for bandwidth-constrained workloads, while imposing moderate hardware overhead. Our sensitivity analyses also verify its robustness and scalability, thereby making CRL-Pythia a practical and efficient solution to contemporary multicore systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10719v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Humaid Siddiqui, Fernando Guzman, Yufei Wu, Ruishu Ann</dc:creator>
    </item>
    <item>
      <title>Design and accuracy trade-offs in Computational Statistics</title>
      <link>https://arxiv.org/abs/2509.10934</link>
      <description>arXiv:2509.10934v1 Announce Type: cross 
Abstract: Statistical computations are becoming increasingly important. These computations often need to be performed in log-space because probabilities become extremely small due to repeated multiplications. While using logarithms effectively prevents numerical underflow, this paper shows that its cost is high in performance, resource utilization, and, notably, numerical accuracy. This paper then argues that using posit, a recently proposed floating-point format, is a better strategy for statistical computations operating on extremely small numbers because of its unique encoding mechanism. To that end, this paper performs a comprehensive analysis comparing posit, binary64, and logarithm representations, examining both individual arithmetic operations, statistical bioinformatics applications, and their accelerators. FPGA implementation results highlight that posit-based accelerators can achieve up to two orders of magnitude higher accuracy, up to 60\% lower resource utilization, and up to $1.3\times$ speedup, compared to log-space accelerators. Such improvement translates to $2\times$ performance per unit resource on the FPGA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10934v1</guid>
      <category>math.NA</category>
      <category>cs.AR</category>
      <category>cs.NA</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tiancheng Xu, Alan L. Cox, Scott Rixner</dc:creator>
    </item>
    <item>
      <title>Vital Signs Monitoring with mmWave OFDM JCAS System</title>
      <link>https://arxiv.org/abs/2509.11767</link>
      <description>arXiv:2509.11767v1 Announce Type: cross 
Abstract: Wireless techniques for monitoring human vital signs, such as heart and breathing rates, offer a promising solution in the context of joint communication and sensing (JCAS) with applications in medicine, sports, safety, security, and even the military. This paper reports experimental results obtained at the Fraunhofer Institute for Integrated Circuits in Ilmenau, demonstrating the effectiveness of an indoor orthogonal frequency-division multiplexing (OFDM) JCAS system for detecting human heart and breathing rates. The system operated in a bistatic configuration at an FR2 frequency of 26.5 GHz with a variable bandwidth of up to 1 GHz. Measurements were taken under various scenarios, including a subject lying down, sitting, or walking, in both line-of-sight and non-line-of-sight conditions, and with one or two subjects present simultaneously. The results indicate that while vital sign detection is generally feasible, its effectiveness is influenced by several factors, such as the subjects clothing, activity, as well as the distance and angle relative to the sensing system. In addition, no significant influence of bandwidth was detected since the vital signs information is encoded in the phase of the signal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11767v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakub Dobosz, Maximilian Engelhardt, Diego Dupleich, Maciej Stapor, Pawel Kulakowski</dc:creator>
    </item>
    <item>
      <title>Fast and low energy approximate full adder based on FELIX logic</title>
      <link>https://arxiv.org/abs/2505.06888</link>
      <description>arXiv:2505.06888v2 Announce Type: replace 
Abstract: In the "Big Data" era, a lot of data must be processed and moved between processing and memory units. New technologies and architectures have emerged to improve system performance and overcome the memory bottleneck. The memristor is a technology with both computing and memory capabilities. In-Memory Computing (IMC) can be performed by applying memristors to stateful design methods. The Fast and Energy-Efficient Logic in Memory (FELIX) logic is one of the stateful implementation logics compatible with memristive crossbar arrays. The way computations are performed can be changed to improve performance. Approximate design methods can be applied in error-resilient applications. In error-resilient applications, an acceptable amount of precision is lost while features such as hardware complexity, latency, and energy are improved. In this paper, using these two concepts, an approximate full adder circuit with exact Cout and approximate Sum outputs has been proposed using the FELIX design method for IMC in two different implementation approaches. The applied memristor count in the proposed FELIX-based Approximate Full Adder (FAFA) in the two proposed implementation approaches (FAFA1 and FAFA2) is improved by 14.28% and 28.57%, energy consumption is improved by 73.735% and 81.754%, respectively. The number of computational steps in both approaches is improved by 66.66% compared to the exact FELIX-based full adder. In this paper, two different scenarios are considered for evaluating the FAFA. In the 1st and 2nd scenarios, respectively, for the three and four Most Significant Bits (MSBs), the exact full adder is used, and for the five and four Least Significant Bits (LSBs), the FAFA is used. The results of error analysis and evaluations of the FAFA in three different image processing applications confirmed that FAFA has high accuracy and acceptable performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06888v2</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seyed Erfan Fatemieh, Samane Asgari, Mohammad Reza Reshadinezhad</dc:creator>
    </item>
    <item>
      <title>Accelerating LLM Inference via Dynamic KV Cache Placement in Heterogeneous Memory System</title>
      <link>https://arxiv.org/abs/2508.13231</link>
      <description>arXiv:2508.13231v2 Announce Type: replace 
Abstract: Large Language Model (LLM) inference is increasingly constrained by memory bandwidth, with frequent access to the key-value (KV) cache dominating data movement. While attention sparsity reduces some memory traffic, the relevance of past tokens varies over time, requiring the full KV cache to remain accessible and sustaining pressure on both bandwidth and capacity. With advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making heterogeneous memory systems a practical solution. This work investigates dynamic KV cache placement across such systems to maximize aggregated bandwidth utilization under capacity constraints. Rather than proposing a specific scheduling policy, we formulate the placement problem mathematically and derive a theoretical upper bound, revealing substantial headroom for runtime optimization. To our knowledge, this is the first formal treatment of dynamic KV cache scheduling in heterogeneous memory systems for LLM inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13231v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhua Fang, Rui Xie, Asad Ul Haq, Linsen Ma, Kaoutar El Maghraoui, Naigang Wang, Meng Wang, Liu Liu, Tong Zhang</dc:creator>
    </item>
    <item>
      <title>Gotta Hash 'Em All! Speeding Up Hash Functions for Zero-Knowledge Proof Applications</title>
      <link>https://arxiv.org/abs/2501.18780</link>
      <description>arXiv:2501.18780v2 Announce Type: replace-cross 
Abstract: Collision-resistant cryptographic hash functions (CRHs) are crucial for security, particularly for message authentication in Zero-knowledge Proof (ZKP) applications. However, traditional CRHs like SHA-2 or SHA-3, while optimized for CPUs, generate large circuits, rendering them inefficient in the ZK domain. Conversely, ZK-friendly hashes are designed for circuit efficiency but struggle on conventional hardware, often orders of magnitude slower than standard hashes due to their reliance on expensive finite field arithmetic. To bridge this performance gap, we present HashEmAll, a novel collection of FPGA-based realizations for three prominent ZK-friendly hashes: Griffin, Rescue-Prime, and Reinforced Concrete. Each offers distinct optimization profiles, with both area-optimized and latency-optimized variants available, allowing users to tailor hardware selection to specific application constraints regarding resource utilization and performance.
  Our extensive evaluation shows that latency-optimized HashEmAll designs outperform CPU implementations by at least $10 \times$, with the leading design achieving a $23 \times$ speedup. These gains are coupled with lower power consumption and compatibility with accessible FPGAs. Importantly, the highly parallel and pipelined architecture of HashEmAll enables significantly better practical scaling than CPU-based approaches towards building real-world ZKP applications, such as data commitments with Merkle Trees, by mitigating the hashing bottleneck for large trees. This highlights the suitability of HashEmAll for real-world ZKP applications involving large-scale data authentication. We also highlight the ability to translate the HashEmAll methodology to various ZK-friendly hash functions and different field sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18780v2</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nojan Sheybani, Tengkai Gong, Anees Ahmed, Nges Brian Njungle, Michel Kinsy, Farinaz Koushanfar</dc:creator>
    </item>
    <item>
      <title>Quantized Neural Networks for Microcontrollers: A Comprehensive Review of Methods, Platforms, and Applications</title>
      <link>https://arxiv.org/abs/2508.15008</link>
      <description>arXiv:2508.15008v3 Announce Type: replace-cross 
Abstract: The deployment of Quantized Neural Networks (QNNs) on resource-constrained devices, such as microcontrollers, has introduced significant challenges in balancing model performance, computational complexity, and memory constraints. Tiny Machine Learning (TinyML) addresses these issues by integrating advancements across machine learning algorithms, hardware acceleration, and software optimization to efficiently run deep neural networks on embedded systems. This survey presents a hardware-centric introduction to quantization, systematically reviewing essential quantization techniques employed to accelerate deep learning models for embedded applications. In particular, further emphasis is placed on the critical trade-offs between model performance and hardware capabilities. The survey further evaluates existing software frameworks and hardware platforms designed specifically for supporting QNN execution on microcontrollers. Moreover, we provide an analysis of the current challenges and an outline of promising future directions in the rapidly evolving domain of QNN deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15008v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hamza A. Abushahla, Dara Varam, Ariel J. N. Panopio, Mohamed I. AlHajri</dc:creator>
    </item>
  </channel>
</rss>
