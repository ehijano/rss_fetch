<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 May 2025 04:01:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Nonparametric Estimation of Conditional Survival Function with Time-Varying Covariates Using DeepONet</title>
      <link>https://arxiv.org/abs/2505.22748</link>
      <description>arXiv:2505.22748v1 Announce Type: new 
Abstract: Traditional survival models often rely on restrictive assumptions such as proportional hazards or instantaneous effects of time-varying covariates on the hazard function, which limit their applicability in real-world settings. We consider the nonparametric estimation of the conditional survival function, which leverages the flexibility of neural networks to capture the complex, potentially long-term non-instantaneous effects of time-varying covariates. In this work, we use Deep Operator Networks (DeepONet), a deep learning architecture designed for operator learning, to model the arbitrary effects of both time-varying and time-invariant covariates. Specifically, our method relaxes commonly used assumptions in hazard regressions by modeling the conditional hazard function as an unknown nonlinear operator of entire histories of time-varying covariates. The estimation is based on a loss function constructed from the nonparametric full likelihood for censored survival data. Simulation studies demonstrate that our method performs well, whereas the Cox model yields biased results when the assumption of instantaneous time-varying covariate effects is violated. We further illustrate its utility with the ADNI data, for which it yields a lower integrated Brier score than the Cox model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22748v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingqing Hu, Bin Nan</dc:creator>
    </item>
    <item>
      <title>A Network-Guided Penalized Regression with Application to Proteomics Data</title>
      <link>https://arxiv.org/abs/2505.22986</link>
      <description>arXiv:2505.22986v1 Announce Type: new 
Abstract: Network theory has proven invaluable in unraveling complex protein interactions. Previous studies have employed statistical methods rooted in network theory, including the Gaussian graphical model, to infer networks among proteins, identifying hub proteins based on key structural properties of networks such as degree centrality. However, there has been limited research examining a prognostic role of hub proteins on outcomes, while adjusting for clinical covariates in the context of high-dimensional data. To address this gap, we propose a network-guided penalized regression method. First, we construct a network using the Gaussian graphical model to identify hub proteins. Next, we preserve these identified hub proteins along with clinically relevant factors, while applying adaptive Lasso to non-hub proteins for variable selection. Our network-guided estimators are shown to have variable selection consistency and asymptotic normality. Simulation results suggest that our method produces better results compared to existing methods and demonstrates promise for advancing biomarker identification in proteomics research. Lastly, we apply our method to the Clinical Proteomic Tumor Analysis Consortium (CPTAC) data and identified hub proteins that may serve as prognostic biomarkers for various diseases, including rare genetic disorders and immune checkpoint for cancer immunotherapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22986v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seungjun Ahn, Eun Jeong Oh</dc:creator>
    </item>
    <item>
      <title>Revisit CP Tensor Decomposition: Statistical Optimality and Fast Convergence</title>
      <link>https://arxiv.org/abs/2505.23046</link>
      <description>arXiv:2505.23046v1 Announce Type: new 
Abstract: Canonical Polyadic (CP) tensor decomposition is a fundamental technique for analyzing high-dimensional tensor data. While the Alternating Least Squares (ALS) algorithm is widely used for computing CP decomposition due to its simplicity and empirical success, its theoretical foundation, particularly regarding statistical optimality and convergence behavior, remain underdeveloped, especially in noisy, non-orthogonal, and higher-rank settings.
  In this work, we revisit CP tensor decomposition from a statistical perspective and provide a comprehensive theoretical analysis of ALS under a signal-plus-noise model. We establish non-asymptotic, minimax-optimal error bounds for tensors of general order, dimensions, and rank, assuming suitable initialization. To enable such initialization, we propose Tucker-based Approximation with Simultaneous Diagonalization (TASD), a robust method that improves stability and accuracy in noisy regimes. Combined with ALS, TASD yields a statistically consistent estimator. We further analyze the convergence dynamics of ALS, identifying a two-phase pattern-initial quadratic convergence followed by linear refinement. We further show that in the rank-one setting, ALS with an appropriately chosen initialization attains optimal error within just one or two iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23046v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Runshi Tang, Julien Chhor, Olga Klopp, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Non-Gaussian Simultaneous Autoregressive Models with Missing Data</title>
      <link>https://arxiv.org/abs/2505.23070</link>
      <description>arXiv:2505.23070v1 Announce Type: new 
Abstract: Standard simultaneous autoregressive (SAR) models are usually assumed to have normally distributed errors, an assumption that is often violated in real-world datasets, which are frequently found to exhibit non-normal, skewed, and heavy-tailed characteristics. New SAR models are proposed to capture these non-Gaussian features. In this project, the spatial error model (SEM), a widely used SAR-type model, is considered. Three novel SEMs are introduced that extend the standard Gaussian SEM by incorporating Student's $t$-distributed errors after a one-to-one transformation is applied to the response variable. Variational Bayes (VB) estimation methods are developed for these models, and the framework is further extended to handle missing response data. Standard variational Bayes (VB) methods perform well with complete datasets; however, handling missing data requires a Hybrid VB (HVB) approach, which integrates a Markov chain Monte Carlo (MCMC) sampler to generate missing values. The proposed VB methods are evaluated using both simulated and real-world datasets, demonstrating their robustness and effectiveness in dealing with non-normal data and missing data in spatial models. Although the method is demonstrated using SAR models, the proposed model specifications and estimation approaches are widely applicable to various types of models for handling non-Gaussian data with missing values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23070v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjana Wijayawardhana, David Gunawan, Thomas Suesse</dc:creator>
    </item>
    <item>
      <title>Valid F-screening in linear regression</title>
      <link>https://arxiv.org/abs/2505.23113</link>
      <description>arXiv:2505.23113v1 Announce Type: new 
Abstract: Suppose that a data analyst wishes to report the results of a least squares linear regression only if the overall null hypothesis, $H_0^{1:p}: \beta_1= \beta_2 = \ldots = \beta_p=0$, is rejected. This practice, which we refer to as F-screening (since the overall null hypothesis is typically tested using an $F$-statistic), is in fact common practice across a number of applied fields. Unfortunately, it poses a problem: standard guarantees for the inferential outputs of linear regression, such as Type 1 error control of hypothesis tests and nominal coverage of confidence intervals, hold unconditionally, but fail to hold conditional on rejection of the overall null hypothesis. In this paper, we develop an inferential toolbox for the coefficients in a least squares model that are valid conditional on rejection of the overall null hypothesis. We develop selective p-values that lead to tests that control the selective Type 1 error, i.e., the Type 1 error conditional on having rejected the overall null hypothesis. Furthermore, they can be computed without access to the raw data, i.e., using only the standard outputs of a least squares linear regression, and therefore are suitable for use in a retrospective analysis of a published study. We also develop confidence intervals that attain nominal selective coverage, and point estimates that account for having rejected the overall null hypothesis. We show empirically that our selective procedure is preferable to an alternative approach that relies on sample splitting, and we demonstrate its performance via re-analysis of two datasets from the biomedical literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23113v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olivia McGough, Daniela Witten, Daniel Kessler</dc:creator>
    </item>
    <item>
      <title>Ultrahigh-dimensional Quadratic Discriminant Analysis Using Random Projections</title>
      <link>https://arxiv.org/abs/2505.23324</link>
      <description>arXiv:2505.23324v1 Announce Type: new 
Abstract: This paper investigates the effectiveness of implementing the Random Projection Ensemble (RPE) approach in Quadratic Discriminant Analysis (QDA) for ultrahigh-dimensional classification problems. Classical methods such as Linear Discriminant Analysis (LDA) and QDA are widely used, but face significant challenges when the dimension of the samples (say, $p$) exceeds the sample size (say, $n$). In particular, both LDA (using the Moore-Penrose inverse for covariance matrices) and QDA (even with known covariance matrices) may perform as poorly as random guessing when $p/n \to \infty$. The RPE method, known for addressing the curse of dimensionality, offers a fast and effective solution. This paper demonstrates the practical advantages of employing RPE on QDA in terms of classification performance as well as computational efficiency. We establish results for limiting perfect classification in both the population and sample versions of the proposed RPE-QDA classifier, under fairly general assumptions that allow for sub-exponential growth of $p$ relative to $n$. Several simulated and gene expression datasets are used to evaluate the performance of the proposed classifier in ultrahigh-dimensional scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23324v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annesha Deb, Minerva Mukhopadhyay, Subhajit Dutta</dc:creator>
    </item>
    <item>
      <title>Calibrated Bayesian inference for random fields on large irregular domains using the debiased spatial Whittle likelihood</title>
      <link>https://arxiv.org/abs/2505.23330</link>
      <description>arXiv:2505.23330v1 Announce Type: new 
Abstract: Bayesian inference for stationary random fields is computationally demanding. Whittle-type likelihoods in the frequency domain based on the fast Fourier Transform (FFT) have several appealing features: i) low computational complexity of only $\mathcal{O}(n \log n)$, where $n$ is the number of spatial locations, ii) robustness to assumptions of the data-generating process, iii) ability to handle missing data and irregularly spaced domains, and iv) flexibility in modelling the covariance function via the spectral density directly in the spectral domain. It is well known, however, that the Whittle likelihood suffers from bias and low efficiency for spatial data. The debiased Whittle likelihood is a recently proposed alternative with better frequentist properties. We propose a methodology for Bayesian inference for stationary random fields using the debiased spatial Whittle likelihood, with an adjustment from the composite likelihood literature. The adjustment is shown to give a well-calibrated Bayesian posterior as measured by coverage properties of credible sets, without sacrificing the quasi-linear computation time. We apply the method to simulated data and two real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23330v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Goodwin, Arthur Guillaumin, Matias Quiroz, Mattias Villani, Robert Kohn</dc:creator>
    </item>
    <item>
      <title>Ordinal regression for meta-analysis of test accuracy: a flexible approach for utilising all threshold data</title>
      <link>https://arxiv.org/abs/2505.23393</link>
      <description>arXiv:2505.23393v1 Announce Type: new 
Abstract: Standard methods for meta-analysis and network-meta-analysis of test accuracy do not fully utilise available evidence, as they analyse thresholds separately, resulting in a loss of data unless every study reports all thresholds - which rarely occurs. Furthermore, previously proposed "multiple threshold" models introduce different problems: making overly restrictive assumptions, or failing to provide summary sensitivity and specificity estimates across thresholds.
  To address this, we proposed a series of ordinal regression-based models, representing a natural extension of established frameworks. Our approach offers notable advantages: (i) complete data utilisation: rather than discarding information like standard methods, we incorporate all threshold data; (ii) threshold-specific inference: by providing summary accuracy estimates across thresholds, our models deliver critical information for clinical decision-making; (iii) enhanced flexibility: unlike previous "multiple thresholds" approaches, our methodology imposes fewer assumptions, leading to better accuracy estimates; (iv) our models use an induced-Dirichlet framework, allowing for either fixed-effects or random-effects cutpoint parameters, whilst also allowing for intuitive cutpoint priors.
  Our (ongoing) simulation study - based on real-world anxiety and depression screening data - demonstrates notably better accuracy estimates than previous approaches, even when the number of categories is high.
  Furthermore, we implemented these models in a user-friendly R package - MetaOrdDTA (https://github.com/CerulloE1996/MetaOrdDTA). The package uses Stan and produces MCMC summaries, sROC plots with credible/prediction regions, and meta-regression.
  Overall, our approach establishes a more comprehensive framework for synthesising test accuracy data, better serving systematic reviewers and clinical decision-makers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23393v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enzo Cerullo, Haley E. Jones, Tim Lucas, Nicola J. Cooper, Alex J. Sutton</dc:creator>
    </item>
    <item>
      <title>Robust Estimation of Double Autoregressive Models via Normal Mixture QMLE</title>
      <link>https://arxiv.org/abs/2505.23535</link>
      <description>arXiv:2505.23535v1 Announce Type: new 
Abstract: This paper investigates the estimation of the double autoregressive (DAR) model in the presence of skewed and heavy-tailed innovations. We propose a novel Normal Mixture Quasi-Maximum Likelihood Estimation (NM-QMLE) method to address the limitations of conventional quasi-maximum likelihood estimation (QMLE) under non-Gaussian conditions. By incorporating a normal mixture distribution into the quasi-likelihood framework, NM-QMLE effectively captures both heavy-tailed behavior and skewness. A critical contribution of this paper is addressing the often-overlooked challenge of selecting the appropriate number of mixture components, $K$, a key parameter that significantly impacts model performance. We systematically evaluate the effectiveness of different model selection criteria. Under regularity conditions, we establish the consistency and asymptotic normality of the NM-QMLE estimator for DAR($p$) models. Numerical simulations demonstrate that NM-QMLE outperforms commonly adopted QMLE methods in terms of estimation accuracy, particularly when the innovation distribution deviates from normality. Our results also show that while criteria like BIC and ICL improve parameter estimation of $K$, fixing a small order of components provides comparable accuracy. To further validate its practical applicability, we apply NM-QMLE to empirical data from the S\&amp;P 500 index and assess its performance through Value at Risk (VaR) estimation. The empirical findings highlight the effectiveness of NM-QMLE in modeling real-world financial data and improving risk assessment. By providing a robust and flexible estimation approach, NM-QMLE enhances the analysis of time series models with complex innovation structures, making it a valuable tool in econometrics and financial modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23535v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhao Chen, Chen Shi, Christina Dan Wang</dc:creator>
    </item>
    <item>
      <title>A Bayesian survival model induced by hurdle zero-modified power series discrete frailty with dispersion: an application in lung cancer</title>
      <link>https://arxiv.org/abs/2505.23568</link>
      <description>arXiv:2505.23568v1 Announce Type: new 
Abstract: Frailty survival models are widely used to capture unobserved heterogeneity among individuals in clinical and epidemiological research. This paper introduces a Bayesian survival model that features discrete frailty induced by the hurdle zero-modified power series (HZMPS) distribution. A key characteristic of HZMPS is the inclusion of a dispersion parameter, enhancing flexibility in capturing diverse heterogeneity patterns. Furthermore, this frailty specification allows the model to distinguish individuals with higher susceptibility to the event of interest from those potentially cured or no longer at risk. We employ a Bayesian framework for parameter estimation, enabling the incorporation of prior information and robust inference, even with limited data. A simulation study is performed to explore the limits of the model. Our proposal is also applied to a lung cancer study, in which patient variability plays a crucial role in disease progression and treatment response. The findings of this study highlight the importance of more flexible frailty models in survival data analysis and emphasize the potential of the Bayesian approach to modeling heterogeneity in biomedical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23568v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katy C. Molina, Joaqu\'in Mart\'inez-Minaya, Danilo Alvares, Vera D. Tomazella</dc:creator>
    </item>
    <item>
      <title>Bayesian kernel machine regression for heteroscedastic health outcome data</title>
      <link>https://arxiv.org/abs/2505.23644</link>
      <description>arXiv:2505.23644v1 Announce Type: new 
Abstract: The field of environmental epidemiology has placed an increasing emphasis on understanding the health effects of mixtures of metals, chemicals, and pollutants in recent years. Bayesian Kernel Machine Regression (BKMR) is a statistical method that has gained significant traction in environmental mixture studies due to its ability to account for complex non-linear relationships between the exposures and health outcome and its ability to identify interaction effects between the exposures. However, BKMR makes the crucial assumption that the error terms have a constant variance, and this assumption is not typically checked in practice. In this paper, we create a diagnostic function for checking this constant variance assumption in practice and develop Heteroscedastic BKMR (HBKMR) for environmental mixture analyses where this assumption is not met. By specifying a Bayesian hierarchical variance model for the error term variance parameters, HBKMR produces updated estimates of the environmental mixture's health effects and their corresponding 95% credible intervals. We apply HBKMR in two real-world case studies that motivated this work: 1) Examining the effects of prenatal metal exposures on behavioral problems in toddlers living in Suriname and 2) Assessing the impacts of metal exposures on simple reaction time in children living near coal-fired power plants in Kentucky. In both case studies, HBKMR provides a substantial improvement in model fit compared to BKMR, with differences in some of the mixture effect estimates and typically narrower 95% credible intervals after accounting for the heteroscedasticity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23644v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Melissa J. Smith, Ihsan E. Buker, Kristina M. Zierold, Lonnie Sears, Cassandra Newsom, Wilco Zijlmans, Maureen Lichtveld, Jeffrey K. Wickliffe</dc:creator>
    </item>
    <item>
      <title>Hub Detection in Gaussian Graphical Models</title>
      <link>https://arxiv.org/abs/2505.23707</link>
      <description>arXiv:2505.23707v1 Announce Type: new 
Abstract: Graphical models are popular tools for exploring relationships among a set of variables. The Gaussian graphical model (GGM) is an important class of graphical models, where the conditional dependence among variables is represented by nodes and edges in a graph. In many real applications, we are interested in detecting hubs in graphical models, which refer to nodes with a significant higher degree of connectivity compared to non-hub nodes. A typical strategy for hub detection consists of estimating the graphical model, and then using the estimated graph to identify hubs. Despite its simplicity, the success of this strategy relies on the accuracy of the estimated graph. In this paper, we directly target on the estimation of hubs, without the need of estimating the graph. We establish a novel connection between the presence of hubs in a graphical model, and the spectral decomposition of the underlying covariance matrix. Based on this connection, we propose the method of inverse principal components for hub detection (IPC-HD). Both consistency and convergence rates are established for IPC-HD. Our simulation study demonstrates the superior performance and fast computation of the proposed method compared to existing methods in the literature in terms of hub detection. Our application to a prostate cancer gene expression dataset detects several hub genes with close connections to tumor development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23707v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2025.2453250</arxiv:DOI>
      <dc:creator>Jos\'e \'A. S\'anchez G\'omez, Weibin Mo, Junlong Zhao, Yufeng Liu</dc:creator>
    </item>
    <item>
      <title>Approximate hierarchical Bayes small area estimation using NEF-QVF and poststratification</title>
      <link>https://arxiv.org/abs/2210.04980</link>
      <description>arXiv:2210.04980v3 Announce Type: replace 
Abstract: We propose an approximate hierarchical Bayes approach that uses the Natural Exponential Family with Quadratic Variance Function in combining information from multiple sources to improve traditional survey estimates of finite population means for small areas. Unlike other Bayesian approaches in finite population sampling, we do not assume a model for all units of the finite population and do not require linking sampled units to the finite population frame. We assume a model only for the finite population units in which the outcome variable is observed; because, for these units, the assumed model can be checked using existing statistical tools. We do not posit an elaborate model on the true means for unobserved units. Instead, we assume that population means of cells with the same combination of factor levels are identical across small areas, and that the population mean for a cell is identical to the mean of the observed units in that cell. We apply our proposed methodology to a real-life survey, linking information from multiple disparate data sources. We also provide practical ways of model selection that can be applied to a wider class of models under similar setting but for a diverse range of scientific problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.04980v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soumojit Das, Partha Lahiri</dc:creator>
    </item>
    <item>
      <title>D-optimal Subsampling Design for Massive Data Linear Regression</title>
      <link>https://arxiv.org/abs/2307.02236</link>
      <description>arXiv:2307.02236v4 Announce Type: replace 
Abstract: Data reduction is a fundamental challenge of modern technology, where classical statistical methods are not applicable because of computational limitations. We consider multiple linear regression for an extraordinarily large number of observations, but only a few covariates. Subsampling aims at the selection of a given proportion of the existing original data. Under distributional assumptions on the covariates, we derive D-optimal subsampling designs and study their theoretical properties. We make use of fundamental concepts of optimal design theory and an equivalence theorem from constrained convex optimization. The thus obtained subsampling designs provide simple rules for whether to accept or reject a data point, allowing for an easy algorithmic implementation. In addition, we propose a simplified subsampling method with lower computational complexity that deviates from the D-optimal design. We present a simulation study, comparing both subsampling schemes with the IBOSS method in the case of a fixed size of the subsample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02236v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Torsten Glemser, Rainer Schwabe</dc:creator>
    </item>
    <item>
      <title>Reconciling Overt Bias and Hidden Bias in Sensitivity Analysis for Matched Observational Studies</title>
      <link>https://arxiv.org/abs/2311.11216</link>
      <description>arXiv:2311.11216v4 Announce Type: replace 
Abstract: Matching is one of the most widely used causal inference designs in observational studies, but post-matching confounding bias remains a critical concern. This bias includes overt bias from inexact matching on measured confounders and hidden bias from the existence of unmeasured confounders. Researchers commonly apply the Rosenbaum-type sensitivity analysis framework after matching to assess the impact of these biases on causal conclusions. In this work, we show that this approach is often conservative and may overstate sensitivity to confounding bias because the classical solution to the Rosenbaum sensitivity model may allocate hypothetical hidden bias in ways that contradict the overt bias observed in the matched dataset. To address this problem, we propose an approach to enhance Rosenbaum-type sensitivity analysis by ensuring compatibility between hidden and overt biases. The main idea is to use post-matching overt bias information as a valid negative control to restrict the feasible set of hidden bias in sensitivity analysis. Our approach does not need to add any additional assumptions (beyond some mild regularity conditions) to Rosenbaum-type sensitivity analysis, and can produce uniformly more informative sensitivity analysis results than the conventional approach to Rosenbaum-type sensitivity analysis. Computationally, we show that our approach can be solved efficiently via iterative convex programming. Extensive simulations and a real data application demonstrate substantial gains in statistical power of sensitivity analysis. Importantly, the idea of our approach can also be directly applied to many other sensitivity analysis frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11216v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Heng, Yanxin Shen, Pengyun Wang</dc:creator>
    </item>
    <item>
      <title>Multicalibration for Modeling Censored Survival Data with Universal Adaptability</title>
      <link>https://arxiv.org/abs/2405.15948</link>
      <description>arXiv:2405.15948v3 Announce Type: replace 
Abstract: Traditional statistical and machine learning methods typically assume that the training and test data follow the same distribution. However, this assumption is frequently violated in real-world applications, where the training data in the source domain may under-represent specific subpopulations in the test data of the target domain. This paper addresses target-independent learning under covariate shift, focusing on multicalibration for survival probability and restricted mean survival time. A black-box post-processing boosting algorithm specifically designed for censored survival data is introduced. By leveraging pseudo-observations, our method produces a multicalibrated predictor that is competitive with inverse propensity score weighting in predicting the survival outcome in an unlabeled target domain, ensuring not only overall accuracy but also fairness across diverse subpopulations. Our theoretical analysis of pseudo-observations builds upon the functional delta method and the $p$-variational norm. The algorithm's sample complexity, convergence properties, and multicalibration guarantees for post-processed predictors are provided. Our results establish a fundamental connection between multicalibration and universal adaptability, demonstrating that our calibrated function is comparable to, or outperforms, the inverse propensity score weighting estimator. Extensive numerical simulations and a real-world case study on cardiovascular disease risk prediction using two large prospective cohort studies validate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15948v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanxuan Ye, Hongzhe Li</dc:creator>
    </item>
    <item>
      <title>Integer Programming for Generalized Causal Bootstrap Designs</title>
      <link>https://arxiv.org/abs/2410.21464</link>
      <description>arXiv:2410.21464v3 Announce Type: replace 
Abstract: In experimental causal inference, we distinguish between two sources of uncertainty: design uncertainty, due to the treatment assignment mechanism, and sampling uncertainty, when the sample is drawn from a super-population. This distinction matters in settings with small fixed samples and heterogeneous treatment effects, as in geographical experiments. The standard bootstrap procedure most often used by practitioners primarily estimates sampling uncertainty, and the causal bootstrap procedure, which accounts for design uncertainty, was developed for the completely randomized design and the difference-in-means estimator, whereas non-standard designs and estimators are often used in these low-power regimes. We address this gap by proposing an integer program which computes numerically the worst-case copula used as an input to the causal bootstrap method, in a wide range of settings. Specifically, we prove the asymptotic validity of our approach for unconfounded, conditionally unconfounded, and and individualistic with bounded confoundedness assignments, as well as generalizing to any linear-in-treatment and quadratic-in-treatment estimators. We demonstrate the refined confidence intervals achieved through simulations of small geographical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21464v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jennifer Brennan, S\'ebastien Lahaie, Adel Javanmard, Nick Doudchenko, Jean Pouget-Abadie</dc:creator>
    </item>
    <item>
      <title>Improve the Precision of Area Under the Curve Estimation for Recurrent Events Through Covariate Adjustment</title>
      <link>https://arxiv.org/abs/2410.24163</link>
      <description>arXiv:2410.24163v3 Announce Type: replace 
Abstract: The area under the curve (AUC) of the mean cumulative function (MCF) has recently been introduced as a novel estimand for evaluating treatment effects in recurrent event settings, offering an alternative to the commonly used Lin-Wei-Yang-Ying (LWYY) model. The AUC of the MCF provides a clinically interpretable summary measure that captures the overall burden of disease progression, regardless of whether the proportionality assumption holds. To improve the precision of the AUC estimation while preserving its unconditional interpretability, we propose a nonparametric covariate adjustment approach. This approach guarantees efficiency gain compared to unadjusted analysis, as demonstrated by theoretical asymptotic distributions, and is universally applicable to various randomization schemes, including both simple and covariate-adaptive designs. Extensive simulations across different scenarios further support its advantage in increasing statistical power. Our findings highlight the importance of covariate adjustment for the analysis of AUC in recurrent event settings, offering practical guidance for its application in randomized clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24163v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiren Sun, Tuo Wang, Yanyao Yi, Ting Ye, Jun Shao, Yu Du</dc:creator>
    </item>
    <item>
      <title>Estimation of time-varying treatment effects using marginal structural models dependent on partial treatment history</title>
      <link>https://arxiv.org/abs/2412.08042</link>
      <description>arXiv:2412.08042v2 Announce Type: replace 
Abstract: Inverse probability (IP) weighting of marginal structural models (MSMs) can provide consistent estimators of time-varying treatment effects under correct model specifications and identifiability assumptions, even in the presence of time-varying confounding. However, this method has two problems: (i) inefficiency due to IP-weights cumulating all time points and (ii) bias and inefficiency due to the MSM misspecification. To address these problems, we propose (i) new IP-weights for estimating parameters of the MSM that depends on partial treatment history and (ii) closed testing procedures for selecting partial treatment history (how far back in time the MSM depends on past treatments). All theoretical results are provided under known IP-weights. In simulation studies, our proposed methods outperformed existing methods both in terms of performance in estimating time-varying treatment effects and in selecting partial treatment history. Our proposed methods have also been applied to real data of hemodialysis patients with reasonable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08042v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nodoka Seya, Masataka Taguri, Takeo Ishii</dc:creator>
    </item>
    <item>
      <title>Matern and Generalized Wendland correlation models that parameterize hole effect, smoothness, and support</title>
      <link>https://arxiv.org/abs/2501.00558</link>
      <description>arXiv:2501.00558v2 Announce Type: replace 
Abstract: A huge literature in statistics and machine learning is devoted to parametric families of correlation functions, where the correlation parameters are used to understand the properties of an associated spatial random process in terms of smoothness and global or compact support. However, most of current parametric correlation functions attain only non-negative values. This work provides two new families that parameterize negative dependencies (aka hole effects), along with smoothness, and global or compact support. They generalize the celebrated Mat\'ern and Generalized Wendland models, respectively, which are attained as special cases. A link between the two new families is also established, showing that a specific reparameterization of the latter includes the former as a special limit case. Their performance in terms of estimation accuracy and goodness of best linear unbiased prediction is illustrated through synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00558v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xavier Emery, Moreno Bevilacqua, Emilio Porcu</dc:creator>
    </item>
    <item>
      <title>Network Weighted Functional Regression: a method for modeling dependencies between functional data in a network</title>
      <link>https://arxiv.org/abs/2501.18221</link>
      <description>arXiv:2501.18221v2 Announce Type: replace 
Abstract: This paper focuses on predicting continuous signals in a sensor lab network, particularly studying microclimate changes. We propose two novel concepts: Network Functional Data (NFD), which represents time series signals as functions on network nodes, and the Network Weighted Functional Regression (NWFR) model, which analyzes relationships between functional responses and predictors in a weighted network. Additionally, we introduce a functional conformal method to provide prediction bands with guaranteed coverage probabilities, independent of data distribution.
  Our statistical analysis on simulated and real-world data demonstrates that incorporating network structure enhances regression accuracy and improves the reliability of conformal prediction regions. These findings advance the analysis of complex network-structured data, offering a more precise and efficient approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18221v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Diana, Elvira Romano, Antonio Irpino</dc:creator>
    </item>
    <item>
      <title>Revisiting Optimal Allocations for Binary Responses: Insights from Considering Type-I Error Rate Control</title>
      <link>https://arxiv.org/abs/2502.06381</link>
      <description>arXiv:2502.06381v3 Announce Type: replace 
Abstract: This work revisits optimal response-adaptive designs from a type-I error rate perspective, highlighting when and how much these allocations exacerbate type-I error rate inflation - an issue previously undocumented. We explore a range of approaches from the literature that can be applied to reduce type-I error rate inflation. However, we found that all of these approaches fail to give a robust solution to the problem. To address this, we derive two optimal allocation proportions, incorporating the more robust score test (instead of the Wald test) with finite sample estimators (instead of the unknown true values) in the formulation of the optimization problem. One proportion optimizes statistical power and the other minimizes the total number failures in a trial while maintaining a fixed variance level. Through simulations based on an early-phase and a confirmatory trial we provide crucial practical insight into how these new optimal proportion designs can offer substantial patient outcomes advantages while controlling type-I error rate. While we focused on binary outcomes, the framework offers valuable insights that naturally extend to other outcome types, multi-armed trials and alternative measures of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06381v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Sof\'ia S. Villar, William F. Rosenberger</dc:creator>
    </item>
    <item>
      <title>Evaluating Decision Rules Across Many Weak Experiments</title>
      <link>https://arxiv.org/abs/2502.08763</link>
      <description>arXiv:2502.08763v2 Announce Type: replace 
Abstract: Technology firms conduct randomized controlled experiments ("A/B tests") to learn which actions to take to improve business outcomes. In firms with mature experimentation platforms, experimentation programs can consist of many thousands of tests. To effectively scale experimentation, firms rely on decision rules: standard operating procedures for mapping the results of an experiment to a choice of treatment arm to launch to the general user population. Despite the critical role of decision rules in translating experimentation into business decisions, rigorous guidance on how to evaluate and choose decision rules is scarce. This paper proposes to evaluate decision rules based on their cumulative returns to business north star metrics. Although intuitive and easy to explain to decision-makers, this quantity can be difficult to estimate, especially when experiments have weak signal-to-noise ratios. We develop a cross-validation estimator that is much less biased than the naive plug-in estimator under conditions realistic to digital experimentation. We demonstrate the efficacy of our approach via a case study of 123 historical A/B tests at Netflix, where we used it to show that a new decision rule would have increased cumulative returns to the north star metric by an estimated $33\%$, directly leading to the adoption of the new rule.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08763v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3711896.3737217</arxiv:DOI>
      <dc:creator>Winston Chou, Colin Gray, Nathan Kallus, Aur\'elien Bibaut, Simon Ejdemyr</dc:creator>
    </item>
    <item>
      <title>Unifying Perspectives: Plausible Counterfactual Explanations on Global, Group-wise, and Local Levels</title>
      <link>https://arxiv.org/abs/2405.17642</link>
      <description>arXiv:2405.17642v2 Announce Type: replace-cross 
Abstract: The growing complexity of AI systems has intensified the need for transparency through Explainable AI (XAI). Counterfactual explanations (CFs) offer actionable "what-if" scenarios on three levels: Local CFs providing instance-specific insights, Global CFs addressing broader trends, and Group-wise CFs (GWCFs) striking a balance and revealing patterns within cohesive groups. Despite the availability of methods for each granularity level, the field lacks a unified method that integrates these complementary approaches. We address this limitation by proposing a gradient-based optimization method for differentiable models that generates Local, Global, and Group-wise Counterfactual Explanations in a unified manner. We especially enhance GWCF generation by combining instance grouping and counterfactual generation into a single efficient process, replacing traditional two-step methods. Moreover, to ensure trustworthiness, we innovatively introduce the integration of plausibility criteria into the GWCF domain, making explanations both valid and realistic. Our results demonstrate the method's effectiveness in balancing validity, proximity, and plausibility while optimizing group granularity, with practical utility validated through practical use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17642v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oleksii Furman, Patryk Wielopolski, {\L}ukasz Lenkiewicz, Jerzy Stefanowski, Maciej Zi\k{e}ba</dc:creator>
    </item>
    <item>
      <title>JANET: Joint Adaptive predictioN-region Estimation for Time-series</title>
      <link>https://arxiv.org/abs/2407.06390</link>
      <description>arXiv:2407.06390v2 Announce Type: replace-cross 
Abstract: Conformal prediction provides machine learning models with prediction sets that offer theoretical guarantees, but the underlying assumption of exchangeability limits its applicability to time series data. Furthermore, existing approaches struggle to handle multi-step ahead prediction tasks, where uncertainty estimates across multiple future time points are crucial. We propose JANET (Joint Adaptive predictioN-region Estimation for Time-series), a novel framework for constructing conformal prediction regions that are valid for both univariate and multivariate time series. JANET generalises the inductive conformal framework and efficiently produces joint prediction regions with controlled K-familywise error rates, enabling flexible adaptation to specific application needs. Our empirical evaluation demonstrates JANET's superior performance in multi-step prediction tasks across diverse time series datasets, highlighting its potential for reliable and interpretable uncertainty quantification in sequential data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06390v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eshant English, Eliot Wong-Toi, Matteo Fontana, Stephan Mandt, Padhraic Smyth, Christoph Lippert</dc:creator>
    </item>
    <item>
      <title>The Empirical Mean is Minimax Optimal for Local Glivenko-Cantelli</title>
      <link>https://arxiv.org/abs/2410.02835</link>
      <description>arXiv:2410.02835v2 Announce Type: replace-cross 
Abstract: We revisit the recently introduced Local Glivenko-Cantelli setting, which studies distribution-dependent uniform convergence rates of the Empirical Mean Estimator (EME). In this work, we investigate generalizations of this setting where arbitrary estimators are allowed rather than just the EME. Can a strictly larger class of measures be learned? Can better risk decay rates be obtained? We provide exhaustive answers to these questions, which are both negative, provided the learner is barred from exploiting some infinite-dimensional pathologies. On the other hand, allowing such exploits does lead to a strictly larger class of learnable measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02835v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Doron Cohen, Aryeh Kontorovich, Roi Weiss</dc:creator>
    </item>
    <item>
      <title>A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data</title>
      <link>https://arxiv.org/abs/2505.20688</link>
      <description>arXiv:2505.20688v2 Announce Type: replace-cross 
Abstract: False discovery rate (FDR) control methods are essential for voxel-wise multiple testing in neuroimaging data analysis, where hundreds of thousands or even millions of tests are conducted to detect brain regions associated with disease-related changes. Classical FDR control methods (e.g., BH, q-value, and LocalFDR) assume independence among tests and often lead to high false non-discovery rates (FNR). Although various spatial FDR control methods have been developed to improve power, they still fall short of jointly addressing three major challenges in neuroimaging applications: capturing complex spatial dependencies, maintaining low variability in both false discovery proportion (FDP) and false non-discovery proportion (FNP) across replications, and achieving computational scalability for high-resolution data. To address these challenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR control method for voxel-wise multiple testing. It integrates the local index of significance (LIS)-based testing procedure with a novel fully connected hidden Markov random field (fcHMRF) designed to model complex spatial structures using a parsimonious parameterization. We develop an efficient expectation-maximization algorithm incorporating mean-field approximation, the Conditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and permutohedral lattice filtering, reducing the time complexity from quadratic to linear in the number of tests. Extensive simulations demonstrate that fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability in FDP and FNP, and a higher number of true positives compared to existing methods. Applied to an FDG-PET dataset from the Alzheimer's Disease Neuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain regions and offers notable advantages in computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20688v2</guid>
      <category>stat.ML</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 30 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taehyo Kim, Qiran Jia, Mony J. de Leon, Hai Shu</dc:creator>
    </item>
  </channel>
</rss>
