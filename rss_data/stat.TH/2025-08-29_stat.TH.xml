<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Aug 2025 04:04:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Palm distributions of superposed point processes for statistical inference</title>
      <link>https://arxiv.org/abs/2508.20924</link>
      <description>arXiv:2508.20924v1 Announce Type: new 
Abstract: Palm distributions play a central role in the study of point processes and their associated summary statistics. In this paper, we characterize the Palm distributions of the superposition of two independent point processes, establishing a simple mixture representation depending on the point processes' Palm distributions and moment measures. We explore two statistical applications enabled by our main result. First, we consider minimum contrast estimation for superposed point processes based on Ripley's $K$ function. Second, we focus on the class of shot-noise Cox processes and obtain a tractable expression for the Janossy density which leads to maximum likelihood estimation via a novel expectation-maximization algorithm. Both approaches are validated through numerical simulations. Extensions to the superposition of multiple point processes, and higher-order Palm distributions, are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20924v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Federico Camerlenghi, Lorenzo Ghilotti</dc:creator>
    </item>
    <item>
      <title>Nonparametric Inference for Noise Covariance Kernels in Parabolic SPDEs using Space-Time Infill-Asymptotics</title>
      <link>https://arxiv.org/abs/2508.20947</link>
      <description>arXiv:2508.20947v1 Announce Type: new 
Abstract: We develop an asymptotic limit theory for nonparametric estimation of the noise covariance kernel in linear parabolic stochastic partial differential equations (SPDEs) with additive colored noise, using space-time infill asymptotics. The method employs discretized infinite-dimensional realized covariations and requires only mild regularity assumptions on the kernel to ensure consistent estimation and asymptotic normality of the estimator. On this basis, we construct omnibus goodness-of-fit tests for the noise covariance that are independent of the SPDE's differential operator. Our framework accommodates a variety of spatial sampling schemes and allows for reliable inference even when spatial resolution is coarser than temporal resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20947v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Petersson, Dennis Schroers</dc:creator>
    </item>
    <item>
      <title>Pivotal inference for linear predictions in stationary processes</title>
      <link>https://arxiv.org/abs/2508.21025</link>
      <description>arXiv:2508.21025v1 Announce Type: new 
Abstract: In this paper we develop pivotal inference for the final (FPE) and relative final prediction error (RFPE) of linear forecasts in stationary processes. Our approach is based on a novel self-normalizing technique and avoids the estimation of the asymptotic variances of the empirical autocovariances. We provide pivotal confidence intervals for the (R)FPE, develop estimates for the minimal order of a linear prediction that is required to obtain a prespecified forecasting accuracy and also propose (pivotal) statistical tests for the hypotheses that the (R)FPE exceeds a given threshold. Additionally, we provide new (pivotal) inference tools for the partial autocorrelation, which do not require the assumption of an autoregressive process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21025v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Holger Dette, Sebastian K\"uhnert</dc:creator>
    </item>
    <item>
      <title>A Heterogeneous Spatiotemporal GARCH Model: A Predictive Framework for Volatility in Financial Networks</title>
      <link>https://arxiv.org/abs/2508.20101</link>
      <description>arXiv:2508.20101v1 Announce Type: cross 
Abstract: We introduce a heterogeneous spatiotemporal GARCH model for geostatistical data or processes on networks, e.g., for modelling and predicting financial return volatility across firms in a latent spatial framework. The model combines classical GARCH(p, q) dynamics with spatially correlated innovations and spatially varying parameters, estimated using local likelihood methods. Spatial dependence is introduced through a geostatistical covariance structure on the innovation process, capturing contemporaneous cross-sectional correlation. This dependence propagates into the volatility dynamics via the recursive GARCH structure, allowing the model to reflect spatial spillovers and contagion effects in a parsimonious and interpretable way. In addition, this modelling framework allows for spatial volatility predictions at unobserved locations. In an empirical application, we demonstrate how the model can be applied to financial stock networks. Unlike other spatial GARCH models, our framework does not rely on a fixed adjacency matrix; instead, spatial proximity is defined in a proxy space constructed from balance sheet characteristics. Using daily log returns of 50 publicly listed firms over a one-year period, we evaluate the model's predictive performance in a cross-validation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20101v1</guid>
      <category>q-fin.ST</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atika Aouri, Philipp Otto</dc:creator>
    </item>
    <item>
      <title>Operator learning meets inverse problems: A probabilistic perspective</title>
      <link>https://arxiv.org/abs/2508.20207</link>
      <description>arXiv:2508.20207v1 Announce Type: cross 
Abstract: Operator learning offers a robust framework for approximating mappings between infinite-dimensional function spaces. It has also become a powerful tool for solving inverse problems in the computational sciences. This chapter surveys methodological and theoretical developments at the intersection of operator learning and inverse problems. It begins by summarizing the probabilistic and deterministic approaches to inverse problems, and pays special attention to emerging measure-centric formulations that treat observed data or unknown parameters as probability distributions. The discussion then turns to operator learning by covering essential components such as data generation, loss functions, and widely used architectures for representing function-to-function maps. The core of the chapter centers on the end-to-end inverse operator learning paradigm, which aims to directly map observed data to the solution of the inverse problem without requiring explicit knowledge of the forward map. It highlights the unique challenge that noise plays in this data-driven inversion setting, presents structure-aware architectures for both point predictions and posterior estimates, and surveys relevant theory for linear and nonlinear inverse problems. The chapter also discusses the estimation of priors and regularizers, where operator learning is used more selectively within classical inversion algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20207v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas H. Nelsen, Yunan Yang</dc:creator>
    </item>
    <item>
      <title>Transfer Learning for Classification under Decision Rule Drift with Application to Optimal Individualized Treatment Rule Estimation</title>
      <link>https://arxiv.org/abs/2508.20942</link>
      <description>arXiv:2508.20942v1 Announce Type: cross 
Abstract: In this paper, we extend the transfer learning classification framework from regression function-based methods to decision rules. We propose a novel methodology for modeling posterior drift through Bayes decision rules. By exploiting the geometric transformation of the Bayes decision boundary, our method reformulates the problem as a low-dimensional empirical risk minimization problem. Under mild regularity conditions, we establish the consistency of our estimators and derive the risk bounds. Moreover, we illustrate the broad applicability of our method by adapting it to the estimation of optimal individualized treatment rules. Extensive simulation studies and analyses of real-world data further demonstrate both superior performance and robustness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20942v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaohan Wang, Yang Ning</dc:creator>
    </item>
    <item>
      <title>Bounds in Wasserstein Distance for Locally Stationary Processes</title>
      <link>https://arxiv.org/abs/2412.03414</link>
      <description>arXiv:2412.03414v2 Announce Type: replace 
Abstract: Locally stationary (LSPs) constitute an essential modeling paradigm for capturing the nuanced dynamics inherent in time series data whose statistical characteristics, including mean and variance, evolve smoothly across time. In this paper, we introduce a novel conditional probability distribution estimator specifically tailored for LSPs, employing the Nadaraya-Watson (NW) kernel smoothing methodology. The NW estimator, a prominent local averaging technique, leverages kernel smoothing to approximate the conditional distribution of a response variable given its covariates. We rigorously establish convergence rates for the NW-based conditional probability estimator in the univariate setting under the Wasserstein metric, providing explicit bounds and conditions that guarantee optimal performance. Extending this theoretical framework, we subsequently generalize our analysis to the multivariate scenario using the sliced Wasserstein distance, an approach particularly advantageous in circumventing the computational and analytical challenges typically associated with high-dimensional settings. To corroborate our theoretical contributions, we conduct extensive numerical simulations on synthetic datasets and provide empirical validations using real-world data, highlighting the estimator's practical relevance and effectiveness in capturing intricate temporal dependencies and underscoring its relevance for analyzing complex nonstationary phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03414v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Nino G. Tinio, Mokhtar Z. Alaya, Salim Bouzebda</dc:creator>
    </item>
    <item>
      <title>Large sample properties of GMM estimators under second-order identification</title>
      <link>https://arxiv.org/abs/2307.13475</link>
      <description>arXiv:2307.13475v2 Announce Type: replace-cross 
Abstract: Dovonon and Hall (Journal of Econometrics, 2018) proposed a limiting distribution theory for GMM estimators for a p - dimensional globally identified parameter vector {\phi} when local identification conditions fail at first-order but hold at second-order. They assumed that the first-order underidentification is due to the expected Jacobian having rank p-1 at the true value {\phi}_{0}, i.e., having a rank deficiency of one. After reparametrizing the model such that the last column of the Jacobian vanishes, they showed that the GMM estimator of the first p-1 parameters converges at rate T^{-1/2} and the GMM estimator of the remaining parameter, {\phi}_{p}, converges at rate T^{-1/4}. They also provided a limiting distribution of T^{1/4}({\phi}_{p}-{\phi}_{0,p}) subject to a (non-transparent) condition which they claimed to be not restrictive in general. However, as we show in this paper, their condition is in fact only satisfied when {\phi} is overidentified and the limiting distribution of T^{1/4}({\phi}_{p}-{\phi}_{0,p}), which is non-standard, depends on whether {\phi} is exactly identified or overidentified. In particular, the limiting distributions of the sign of T^{1/4}({\phi}_{p}-{\phi}_{0,p}) for the cases of exact and overidentification, respectively, are different and are obtained by using expansions of the GMM objective function of different orders. Unsurprisingly, we find that the limiting distribution theories of Dovonon and Hall (2018) for Indirect Inference (II) estimation under two different scenarios with second-order identification where the target function is a GMM estimator of the auxiliary parameter vector, are incomplete for similar reasons. We discuss how our results for GMM estimation can be used to complete both theories and how they can be used to obtain the limiting distributions of the II estimators in the case of exact identification under either scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.13475v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Kruiniger</dc:creator>
    </item>
    <item>
      <title>Transformers Meet In-Context Learning: A Universal Approximation Theory</title>
      <link>https://arxiv.org/abs/2506.05200</link>
      <description>arXiv:2506.05200v2 Announce Type: replace-cross 
Abstract: Large language models are capable of in-context learning, the ability to perform new tasks at test time using a handful of input-output examples, without parameter updates. We develop a universal approximation theory to elucidate how transformers enable in-context learning. For a general class of functions (each representing a distinct task), we demonstrate how to construct a transformer that, without any further weight updates, can predict based on a few noisy in-context examples with vanishingly small risk. Unlike prior work that frames transformers as approximators of optimization algorithms (e.g., gradient descent) for statistical learning tasks, we integrate Barron's universal function approximation theory with the algorithm approximator viewpoint. Our approach yields approximation guarantees that are not constrained by the effectiveness of the optimization algorithms being mimicked, extending far beyond convex problems like linear regression. The key is to show that (i) any target function can be nearly linearly represented, with small $\ell_1$-norm, over a set of universal features, and (ii) a transformer can be constructed to find the linear representation -- akin to solving Lasso -- at test time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05200v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gen Li, Yuchen Jiao, Yu Huang, Yuting Wei, Yuxin Chen</dc:creator>
    </item>
    <item>
      <title>Estimating non-linear functionals of trawl processes</title>
      <link>https://arxiv.org/abs/2508.19949</link>
      <description>arXiv:2508.19949v2 Announce Type: replace-cross 
Abstract: Trawl processes is a family of continuous-time, infinitely divisible, stationary processes whose correlation structure is entirely characterized by its so-called trawl function. This paper investigates the problem of estimating non-linear functionals of a trawl function under an in-fill and a long-span sampling scheme. Specifically, building on the work of \cite{SauriVeraart23}, we introduce non-parametric estimators for functionals of the type $\Psi_{t}(g)=\int_{0}^{t}g(a(s))\mathrm{d}s$ and $ \Lambda_t(g)=\int_{t}^{\infty}g(a(s))\mathrm{d}s$ where $a$ represents the trawl function of interest and $g$ a non-linear test function. We show that our estimator for $\Psi_{t}(g)$ is consistent and asymptotically Gaussian regardless of the memory of the process. We further demonstrates that the same phenomenon occurs for the estimation of $\Lambda_t(g)$ as long as $g(x)= \mathrm{O} (\lvert x\rvert^p)$, as $x\to0$, for some $p&gt;3$. Additionally, we illustrate how our results can be used to test the presence of $T$-dependent data that is robust to persistence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19949v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 29 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Orimar Sauri</dc:creator>
    </item>
  </channel>
</rss>
