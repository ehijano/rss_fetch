<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jan 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Transfer Learning for Individualized Treatment Rules: Application to Sepsis Patients Data from eICU-CRD and MIMIC-III Databases</title>
      <link>https://arxiv.org/abs/2501.02128</link>
      <description>arXiv:2501.02128v1 Announce Type: new 
Abstract: Modern precision medicine aims to utilize real-world data to provide the best treatment for an individual patient. An individualized treatment rule (ITR) maps each patient's characteristics to a recommended treatment scheme that maximizes the expected outcome of the patient. A challenge precision medicine faces is population heterogeneity, as studies on treatment effects are often conducted on source populations that differ from the populations of interest in terms of the distribution of patient characteristics. Our research goal is to explore a transfer learning algorithm that aims to address the population heterogeneity problem and obtain targeted, optimal, and interpretable ITRs. The algorithm incorporates a calibrated augmented inverse probability weighting (CAIPW) estimator for the average treatment effect (ATE) and employs value function maximization for the target population using Genetic Algorithm (GA) to produce our desired ITR. To demonstrate its practical utility, we apply this transfer learning algorithm to two large medical databases, Electronic Intensive Care Unit Collaborative Research Database (eICU-CRD) and Medical Information Mart for Intensive Care III (MIMIC-III). We first identify the important covariates, treatment options, and outcomes of interest based on the two databases, and then estimate the optimal linear ITRs for patients with sepsis. Our research introduces and applies new techniques for data fusion to obtain data-driven ITRs that cater to patients' individual medical needs in a population of interest. By emphasizing generalizability and personalized decision-making, this methodology extends its potential application beyond medicine to fields such as marketing, technology, social sciences, and education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02128v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andong Wang (North Carolina State University), Kelly Wentzlof (Indiana University), Johnny Rajala (University of Maryland), Miontranese Green (California State University, Long Beach), Yunshu Zhang (North Carolina State University), Shu Yang (North Carolina State University)</dc:creator>
    </item>
    <item>
      <title>Evaluation of the HeartSteps Online Sampling Algorithm</title>
      <link>https://arxiv.org/abs/2501.02137</link>
      <description>arXiv:2501.02137v1 Announce Type: new 
Abstract: Micro-randomized trials (MRTs), which sequentially randomize participants at multiple decision times, have gained prominence in digital intervention development. These sequential randomizations are often subject to certain constraints. In the MRT called HeartSteps V2V3, where an intervention is designed to interrupt sedentary behavior, two core design constraints need to be managed: an average of 1.5 interventions across days and the uniform delivery of interventions across decision times. Meeting both constraints, especially when the times allowed for randomization are not determined beforehand, is challenging. An online algorithm was implemented to meet these constraints in the HeartSteps V2V3 MRT. We present a case study using data from the HeartSteps V2V3 MRT, where we select appropriate metrics, discuss issues in making an accurate evaluation, and assess the algorithm's performance. Our evaluation shows that the algorithm performed well in meeting the two constraints. Furthermore, we identify areas for improvement and provide recommendations for designers of MRTs that need to satisfy these core design constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02137v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiang Meng, Walter Dempsey, Peng Liao, Nick Reid, Pedja Klasnja, Susan Murphy</dc:creator>
    </item>
    <item>
      <title>Second order asymptotics for discounted aggregate claims of continuous-time renewal risk models with constant interest force</title>
      <link>https://arxiv.org/abs/2501.02545</link>
      <description>arXiv:2501.02545v1 Announce Type: new 
Abstract: This paper investigates the second order asymptotic expansion for tail probabilities of discounted aggregate claims in continuous-time renewal risk models with constant interest force. Concretely, two types of continuous-time renewal risk models without and with by-claims are separately discussed. By constructing the asymptotic theory and weighted Kesten-type inequality of randomly weighted sums for second order subexponential random variables, second order asymptotic formulae for these two risk models are firstly built. In comparison of the first order asymptotic formulae, our results are more superior and precise, which are demonstrated by some simple numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02545v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingzhen Genga, Shijie Wanga, Yang Yang</dc:creator>
    </item>
    <item>
      <title>A data-driven merit order: Learning a fundamental electricity price model</title>
      <link>https://arxiv.org/abs/2501.02963</link>
      <description>arXiv:2501.02963v1 Announce Type: new 
Abstract: Power prices can be forecasted using data-driven models or fundamental models. Data-driven models learn from historical patterns, while fundamental models simulate electricity markets. Traditionally, fundamental models have been too computationally demanding to allow for intrinsic parameter estimation or frequent updates, which are essential for short-term forecasting. In this paper, we propose a novel data-driven fundamental model that combines the strengths of both approaches. We estimate the parameters of a fully fundamental merit order model using historical data, similar to how data-driven models work. This removes the need for fixed technical parameters or expert assumptions, allowing most parameters to be calibrated directly to observations. The model is efficient enough for quick parameter estimation and forecast generation. We apply it to forecast German day-ahead electricity prices and demonstrate that it outperforms both classical fundamental and purely data-driven models. The hybrid model effectively captures price volatility and sequential price clusters, which are becoming increasingly important with the expansion of renewable energy sources. It also provides valuable insights, such as fuel switches, marginal power plant contributions, estimated parameters, dispatched plants, and power generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02963v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Ghelasi, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>Equipoise calibration of clinical trial design</title>
      <link>https://arxiv.org/abs/2501.03009</link>
      <description>arXiv:2501.03009v1 Announce Type: new 
Abstract: Statistical methods for clinical trial design are currently unable to rely on a sufficiently precise and general definition of what is an adequately powered study. Operationally, this definition is needed to ensure an alignment by design between statistical significance and clinical interpretability. To address this gap, this paper shows how to calibrate randomised trial designs to establishing strong clinical equipoise imbalance. Among several equipoise models, the least informed population distribution of the pre-trial odds of the design hypotheses is recommended here as the most practical calibrator. Under this model, primary analysis outcomes of common phase 3 superiority designs are shown to provide at least 90% evidence of equipoise imbalance. Designs carrying 95% power at 5% false positive rate are shown to demonstrate even stronger equipoise imbalance, providing an operational definition of a robustly powered study. Equipoise calibration is then applied to design of clinical development plans comprising randomised phase 2 and phase 3 studies. Development plans using oncology clinical endpoints are shown to provide strong equipoise imbalance when positive outcomes are observed in phase 2 and in phase 3. Establishing equipoise imbalance on a statistical basis when a positive phase 2 is not confirmed in phase 3 is shown to require large sample sizes unlikely to be associated with clinically meaningful effect sizes. Equipoise calibration is proposed as an effective clinical trial methodology ensuring that the statistical properties of clinical trial outcomes are clinically interpretable. Strong equipoise imbalance is achieved for designs carrying 95% power at 5% false positive rate, regardless of whether the primary outcome is positive or negative. Sponsors should consider raising power of their designs beyond current practice to achieve more conclusive results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03009v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fabio Rigat</dc:creator>
    </item>
    <item>
      <title>User experience with educational technology in African slums</title>
      <link>https://arxiv.org/abs/2501.03167</link>
      <description>arXiv:2501.03167v1 Announce Type: new 
Abstract: This paper describes a project developed in co-operation with two dozen community libraries and schools in various slums and low-income regions in Kenya. The project was started in response to COVID-19, to allow students to solve computerised math drills while schools were closed. The number of students involved reached two thousand during the first 24 months of operation. The program uses a study environment, tutor-web, and access to this is provided by donating tablet computers to participating community libraries. Students are rewarded using tokens, SmileyCoins or SMLY, as they progress through the system and the libraries are free to sell for SMLY small food items, sanitary pads and even the tablets themselves. The rewards are designed to put an emphasis on secondary school mathematics, so as to prepare the students for applications into STEM subjects at university. Completion of the corresponding collection of drills gives SmileyCoin awards sufficient to purchase a tablet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03167v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.21125/inted.2023.0898</arxiv:DOI>
      <dc:creator>Gunnar Stefansson, Anna Helga Jonsdottir</dc:creator>
    </item>
    <item>
      <title>Selecting ChIP-Seq Normalization Methods from the Perspective of their Technical Conditions</title>
      <link>https://arxiv.org/abs/2501.02028</link>
      <description>arXiv:2501.02028v1 Announce Type: cross 
Abstract: Chromatin immunoprecipitation, followed by high throughput sequencing provides vital insights into locations on the genome with differential DNA occupancy between experimental states. However, since ChIP-Seq data is collected experimentally, it must be normalized between samples in order to properly assess which genomic regions have differential DNA occupancy via differential binding analysis. While between-sample normalization is a crucial for downstream differential binding analysis, the technical conditions underlying between-sample ChIP-Seq normalization methods have yet to be specifically examined. We identify three important technical conditions underlying ChIP-Seq between-sample normalization methods: symmetric differential DNA occupancy, equal total DNA occupancy, and equal background binding across experimental states. We categorize popular ChIP-Seq normalization methods based on their technical conditions and simulate ChIP-Seq read count data to exemplify the importance of satisfying a normalization method's technical conditions to downstream differential binding analysis. We assess the similarity between normalization methods in experimental CUT&amp;RUN data to externally verify our simulation findings. Our simulation and experimental results underscore that satisfying the technical conditions underlying the selected between-sample normalization methods is crucial to conducting biologically meaningful downstream differential binding analysis. We suggest that researchers use their understanding of the ChIP-Seq experiment at hand to guide their choice of between-sample normalization method when possible. Researchers could use the intersection of the differentially bound peaksets derived from different normalization methods to determine which regions have differential DNA occupancy between experimental states when there is uncertainty about which technical conditions are met.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02028v1</guid>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sara Colando, Danae Schulz, Johanna Hardin</dc:creator>
    </item>
    <item>
      <title>From Images to Detection: Machine Learning for Blood Pattern Classification</title>
      <link>https://arxiv.org/abs/2501.02151</link>
      <description>arXiv:2501.02151v1 Announce Type: cross 
Abstract: Bloodstain Pattern Analysis (BPA) helps us understand how bloodstains form, with a focus on their size, shape, and distribution. This aids in crime scene reconstruction and provides insight into victim positions and crime investigation. One challenge in BPA is distinguishing between different types of bloodstains, such as those from firearms, impacts, or other mechanisms. Our study focuses on differentiating impact spatter bloodstain patterns from gunshot bloodstain patterns. We distinguish patterns by extracting well-designed individual stain features, applying effective data consolidation methods, and selecting boosting classifiers. As a result, we have developed a model that excels in both accuracy and efficiency. In addition, we use outside data sources from previous studies to discuss the challenges and future directions for BPA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02151v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yilin Li, Weining Shen</dc:creator>
    </item>
    <item>
      <title>Randomization Tests for Monotone Spillover Effects</title>
      <link>https://arxiv.org/abs/2501.02454</link>
      <description>arXiv:2501.02454v1 Announce Type: cross 
Abstract: Randomization tests have gained popularity for causal inference under network interference because they are finite-sample valid with minimal assumptions. However, existing procedures are limited as they primarily focus on the existence of spillovers through sharp null hypotheses on potential outcomes. In this paper, we expand the scope of randomization procedures in network settings by developing new tests for the monotonicity of spillover effects. These tests offer insights into whether spillover effects increase, decrease, or exhibit ``diminishing returns" along certain network dimensions of interest. Our approach partitions the network into multiple (possibly overlapping) parts and testing a monotone contrast hypothesis in each sub-network. The test decisions can then be aggregated in various ways depending on how each test is constructed. We demonstrate our method through a re-analysis of a large-scale policing experiment in Colombia, which reveals evidence of monotonicity related to the ``crime displacement hypothesis". In particular, our analysis suggests that crime spillovers on a control street are increasing in the number of nearby streets treated with more intense policing, but the effect is diminishing at higher levels of exposure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02454v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shunzhuang Huang, Xinran Li, Panos Toulis</dc:creator>
    </item>
    <item>
      <title>Knowledge Distillation with Adapted Weight</title>
      <link>https://arxiv.org/abs/2501.02705</link>
      <description>arXiv:2501.02705v1 Announce Type: cross 
Abstract: Although large models have shown a strong capacity to solve large-scale problems in many areas including natural language and computer vision, their voluminous parameters are hard to deploy in a real-time system due to computational and energy constraints. Addressing this, knowledge distillation through Teacher-Student architecture offers a sustainable pathway to compress the knowledge of large models into more manageable sizes without significantly compromising performance. To enhance the robustness and interpretability of this framework, it is critical to understand how individual training data impact model performance, which is an area that remains underexplored. We propose the \textbf{Knowledge Distillation with Adaptive Influence Weight (KD-AIF)} framework which leverages influence functions from robust statistics to assign weights to training data, grounded in the four key SAFE principles: Sustainability, Accuracy, Fairness, and Explainability. This novel approach not only optimizes distillation but also increases transparency by revealing the significance of different data. The exploration of various update mechanisms within the KD-AIF framework further elucidates its potential to significantly improve learning efficiency and generalization in student models, marking a step toward more explainable and deployable Large Models. KD-AIF is effective in knowledge distillation while also showing exceptional performance in semi-supervised learning with outperforms existing baselines and methods in multiple benchmarks (CIFAR-100, CIFAR-10-4k, SVHN-1k, and GLUE).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02705v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sirong Wu, Xi Luo, Junjie Liu, Yuhui Deng</dc:creator>
    </item>
    <item>
      <title>Coarsened confounding for causal effects: a large-sample framework</title>
      <link>https://arxiv.org/abs/2501.03129</link>
      <description>arXiv:2501.03129v1 Announce Type: cross 
Abstract: There has been widespread use of causal inference methods for the rigorous analysis of observational studies and to identify policy evaluations. In this article, we consider coarsened exact matching, developed in Iacus et al. (2011). While they developed some statistical properties, in this article, we study the approach using asymptotics based on a superpopulation inferential framework. This methodology is generalized to what we termed as coarsened confounding, for which we propose two new algorithms. We develop asymptotic results for the average causal effect estimator as well as providing conditions for consistency. In addition, we provide an asymptotic justification for the variance formulae in Iacus et al. (2011). A bias correction technique is proposed, and we apply the proposed methodology to data from two well-known observational studi</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03129v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Debashis Ghosh, Lei Wang</dc:creator>
    </item>
    <item>
      <title>High-frequency lead-lag relationships in the Chinese stock index futures market: tick-by-tick dynamics of calendar spreads</title>
      <link>https://arxiv.org/abs/2501.03171</link>
      <description>arXiv:2501.03171v1 Announce Type: cross 
Abstract: Lead-lag relationships, integral to market dynamics, offer valuable insights into the trading behavior of high-frequency traders (HFTs) and the flow of information at a granular level. This paper investigates the lead-lag relationships between stock index futures contracts of different maturities in the Chinese financial futures market (CFFEX). Using high-frequency (tick-by-tick) data, we analyze how price movements in near-month futures contracts influence those in longer-dated contracts, such as next-month, quarterly, and semi-annual contracts. Our findings reveal a consistent pattern of price discovery, with the near-month contract leading the others by one tick, driven primarily by liquidity. Additionally, we identify a negative feedback effect of the "lead-lag spread" on the leading asset, which can predict returns of leading asset. Backtesting results demonstrate the profitability of trading based on the lead-lag spread signal, even after accounting for transaction costs. Altogether, our analysis offers valuable insights to understand and capitalize on the evolving dynamics of futures markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03171v1</guid>
      <category>q-fin.CP</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanlin Li, Xiyan Chen, Yingzheng Liu</dc:creator>
    </item>
    <item>
      <title>Shots and variance on noisy quantum circuits</title>
      <link>https://arxiv.org/abs/2501.03194</link>
      <description>arXiv:2501.03194v1 Announce Type: cross 
Abstract: We present a method for estimating the number of shots required for some desired variance in the results of a quantum circuit. First, we establish a baseline for a single qubit characterization of individual noise sources separately. We then extend the method to multi-qubit problems and test our method on two case studies. We will proceed to estimate the number of shots required for a desired variance in the result or, equivalently estimate the variance at a known number of shots. We will show we're able to estimate variance accurately to within a factor of 2. Following these, we also provide a closed-form expression for variance at a given number of shots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03194v1</guid>
      <category>quant-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manav Seksaria, Anil Prabhakar</dc:creator>
    </item>
    <item>
      <title>An Efficient Image Denoising Method Integrating Multi-resolution Local Clustering and Adaptive Smoothing</title>
      <link>https://arxiv.org/abs/2407.20210</link>
      <description>arXiv:2407.20210v2 Announce Type: replace 
Abstract: The importance of developing efficient image denoising methods is immense especially for modern applications such as image comparisons, image monitoring, medical image diagnostics, and so forth. Available methods in the vast literature on image denoising can address certain issues in image denoising, but no one single method can solve all such issues. For example, jump regression based methods can preserve linear edges well, but cannot preserve many other fine details of an image. On the other hand, local clustering based methods can preserve fine edge structures, but cannot perform well in presence of heavy noise. The proposed method uses various shapes and sizes of local neighborhood based on local information, and integrates this adaptive approach with the local clustering based smoothing. Theoretical justifications and numerical studies show that the proposed method indeed performs better than these two individual methods and outperforms many other state-of-the-art techniques as well. Such performance demonstrates vast potential of the applicability of the proposed method in many modern-day applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20210v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Subhasish Basak, Partha Sarathi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Statistical Emulations of Human Operational Motions in Industrial Environments</title>
      <link>https://arxiv.org/abs/2411.16929</link>
      <description>arXiv:2411.16929v2 Announce Type: replace 
Abstract: This paper addresses the critical and challenging task of developing emulators for simulating human operational motions in industrial workplaces. We conceptualize human motion as a sequence of human body shapes and develop statistical generative models for sequences of (body) shapes of human workers. We model these sequences as a continuous-time stochastic process on a Riemannian shape manifold. This modeling is challenging due to the nonlinearity of the shape manifold, variability in execution rates across observations, infinite dimensionality of stochastic processes, and population variability within and across action classes. This paper proposes multiple solutions to these challenges, incorporating time warping for temporal alignment, Riemannian geometry for tackling nonlinearity, and Shape- and Functional-PCA for dimension reduction. It imposes a Gaussian model on the resulting Euclidean spaces, uses them to emulate random sequences in an industrial setting and evaluates them comprehensively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16929v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanliang Chen, Chiwoo Park, Anuj Srivastava</dc:creator>
    </item>
    <item>
      <title>Predicting Customer Lifetime Value Using Recurrent Neural Net</title>
      <link>https://arxiv.org/abs/2412.20295</link>
      <description>arXiv:2412.20295v2 Announce Type: replace 
Abstract: This paper introduces a recurrent neural network approach for predicting user lifetime value in Software as a Service (SaaS) applications. The approach accounts for three connected time dimensions. These dimensions are the user cohort (the date the user joined), user age-in-system (the time since the user joined the service) and the calendar date the user is an age-in-system (i.e., contemporaneous information).The recurrent neural networks use a multi-cell architecture, where each cell resembles a long short-term memory neural network. The approach is applied to predicting both acquisition (new users) and rolling (existing user) lifetime values for a variety of time horizons. It is found to significantly improve median absolute percent error versus light gradient boost models and Buy Until You Die models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20295v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huigang Chen, Edwin Ng, Slawek Smyl, Gavin Steininger</dc:creator>
    </item>
    <item>
      <title>Grid-level impacts of renewable energy on thermal generation: efficiency, emissions and flexibility</title>
      <link>https://arxiv.org/abs/2501.01954</link>
      <description>arXiv:2501.01954v2 Announce Type: replace 
Abstract: Wind and solar generation constitute an increasing share of electricity supply globally. We find that this leads to shifts in the operational dynamics of thermal power plants. Using fixed effects panel regression across seven major U.S. balancing authorities, we analyze the impact of renewable generation on coal, natural gas combined cycle plants, and natural gas combustion turbines. Wind generation consistently displaces thermal output, while effects from solar vary significantly by region, achieving substantial displacement in areas with high solar penetration such as the California Independent System Operator but limited impacts in coal reliant grids such as the Midcontinent Independent System Operator. Renewable energy sources effectively reduce carbon dioxide emissions in regions with flexible thermal plants, achieving displacement effectiveness as high as one hundred and two percent in the California Independent System Operator and the Electric Reliability Council of Texas. However, in coal heavy areas such as the Midcontinent Independent System Operator and the Pennsylvania New Jersey Maryland Interconnection, inefficiencies from ramping and cycling reduce carbon dioxide displacement to as low as seventeen percent and often lead to elevated nitrogen oxides and sulfur dioxide emissions. These findings underscore the critical role of grid design, fuel mix, and operational flexibility in shaping the emissions benefits of renewables. Targeted interventions, including retrofitting high emitting plants and deploying energy storage, are essential to maximize emissions reductions and support the decarbonization of electricity systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01954v2</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhruv Suri, Jacques de Chalendar, Ines Azevedo</dc:creator>
    </item>
    <item>
      <title>Estimating the optimal time to perform a PET-PSMA exam in prostatectomized patients based on data from clinical practice</title>
      <link>https://arxiv.org/abs/2302.10861</link>
      <description>arXiv:2302.10861v2 Announce Type: replace-cross 
Abstract: Prostatectomized patients are at risk of resurgence, and for this reason, during a follow-up period, they are monitored for Prostate Specific Antigen (PSA) growth, an indicator of tumor progression. The presence of tumors can be evaluated with an expensive exam, called Positron Emission Tomography with Prostate-Specific Membrane Antigen (PET-PSMA). To justify the high cost of the PET-PSMA and, at the same time, to contain the risk for the patient, this exam should be recommended only when the evidence of tumor progression is strong. With the aim of estimating the optimal time to recommend the exam based on the patient's history and collected data, we build a hierarchical Bayesian model that describes, jointly, the PSA growth curve and the probability of a positive PET-PSMA. With our proposal we process all past and present information about the patients PSA measurement and PET-PSMA results, in order to give an informed estimate of the optimal time, improving current practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10861v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martina Amongero, Gianluca Mastrantonio, Stefano De Luca, Mauro Gasparini</dc:creator>
    </item>
    <item>
      <title>Robust inference for linear regression models with possibly skewed error distribution</title>
      <link>https://arxiv.org/abs/2404.03404</link>
      <description>arXiv:2404.03404v2 Announce Type: replace-cross 
Abstract: Traditional methods for linear regression generally assume that the underlying error distribution, equivalently the distribution of the responses, is normal. Yet, sometimes real life response data may exhibit a skewed pattern, and assuming normality would not give reliable results in such cases. This is often observed in cases of some biomedical, behavioral, socio-economic and other variables. In this paper, we propose to use the class of skew normal (SN) distributions, which also includes the ordinary normal distribution as its special case, as the model for the errors in a linear regression setup and perform subsequent statistical inference using the popular and robust minimum density power divergence approach to get stable insights in the presence of possible data contamination (e.g., outliers). We provide the asymptotic distribution of the proposed estimator of the regression parameters and also propose robust Wald-type tests of significance for these parameters. We provide an influence function analysis of these estimators and test statistics, and also provide level and power influence functions. Numerical verification including simulation studies and real data analysis is provided to substantiate the theory developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03404v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amarnath Nandy, Ayanendranath Basu, Abhik Ghosh</dc:creator>
    </item>
    <item>
      <title>Combinations of distributional regression algorithms with application in uncertainty estimation of corrected satellite precipitation products</title>
      <link>https://arxiv.org/abs/2407.01623</link>
      <description>arXiv:2407.01623v2 Announce Type: replace-cross 
Abstract: To facilitate effective decision-making, precipitation datasets should include uncertainty estimates. Quantile regression with machine learning has been proposed for issuing such estimates. Distributional regression offers distinct advantages over quantile regression, including the ability to model intermittency as well as a stronger ability to extrapolate beyond the training data, which is critical for predicting extreme precipitation. Therefore, here, we introduce the concept of distributional regression in precipitation dataset creation, specifically for the spatial prediction task of correcting satellite precipitation products. Building upon this concept, we formulated new ensemble learning methods that can be valuable not only for spatial prediction but also for other prediction problems. These methods exploit conditional zero-adjusted probability distributions estimated with generalized additive models for location, scale and shape (GAMLSS), spline-based GAMLSS and distributional regression forests as well as their ensembles (stacking based on quantile regression and equal-weight averaging). To identify the most effective methods for our specific problem, we compared them to benchmarks using a large, multi-source precipitation dataset. Stacking was shown to be superior to individual methods at most quantile levels when evaluated with the quantile loss function. Moreover, while the relative ranking of the methods varied across different quantile levels, stacking methods, and to a lesser extent mean combiners, exhibited lower variance in their performance across different quantiles compared to individual methods that occasionally ranked extremely low. Overall, a task-specific combination of multiple distributional regression algorithms could yield significant benefits in terms of stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01623v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.mlwa.2024.100615</arxiv:DOI>
      <arxiv:journal_reference>Machine Learning with Applications 19 (2025) 100615</arxiv:journal_reference>
      <dc:creator>Georgia Papacharalampous, Hristos Tyralis, Nikolaos Doulamis, Anastasios Doulamis</dc:creator>
    </item>
    <item>
      <title>Estimation of bid-ask spreads in the presence of serial dependence</title>
      <link>https://arxiv.org/abs/2407.17401</link>
      <description>arXiv:2407.17401v3 Announce Type: replace-cross 
Abstract: Starting from a basic model in which the dynamic of the transaction prices is a geometric Brownian motion disrupted by a microstructure white noise, corresponding to the random alternation of bids and asks, we propose moment-based estimators along with their statistical properties. We then make the model more realistic by considering serial dependence: we assume a geometric fractional Brownian motion for the price, then an Ornstein-Uhlenbeck process for the microstructure noise. In these two cases of serial dependence, we propose again consistent and asymptotically normal estimators. All our estimators are compared on simulated data with existing approaches, such as Roll, Corwin-Schultz, Abdi-Ranaldo, or Ardia-Guidotti-Kroencke estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17401v3</guid>
      <category>q-fin.ST</category>
      <category>q-fin.MF</category>
      <category>q-fin.TR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xavier Brouty, Matthieu Garcin, Hugo Roccaro</dc:creator>
    </item>
    <item>
      <title>Trimmed Mean for Partially Observed Functional Data</title>
      <link>https://arxiv.org/abs/2408.13062</link>
      <description>arXiv:2408.13062v2 Announce Type: replace-cross 
Abstract: In recent years, partially observable functional data has gained significant attention in practical applications and has become the focus of increasing interest in the literature. In this thesis, we build upon the concept of data integration depth for partially observable functions, as proposed by Elias et al. (2023), and the trimmed-mean estimator method along with its consistency proof introduced by Fraiman and Muniz (2001) for completely observable functions. We introduce the concept of trimmed mean specifically for partially observable functional data. Additionally, we address several theoretical and practical issues, including a proof of the strong consistency of the proposed trimmed mean, and we provide a simulation study. The results demonstrate that our estimator outperforms the ordinary mean in terms of accuracy and robustness when applied to partially observable functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13062v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiao Wang</dc:creator>
    </item>
    <item>
      <title>Denoising Variational Autoencoder as a Feature Reduction Pipeline for the Diagnosis of Autism based on Resting-state fMRI</title>
      <link>https://arxiv.org/abs/2410.00068</link>
      <description>arXiv:2410.00068v2 Announce Type: replace-cross 
Abstract: Autism spectrum disorders (ASDs) are developmental conditions characterized by restricted interests and difficulties in communication. The complexity of ASD has resulted in a deficiency of objective diagnostic biomarkers. Deep learning methods have gained recognition for addressing these challenges in neuroimaging analysis, but finding and interpreting such diagnostic biomarkers are still challenging computationally. Here, we propose a feature reduction pipeline using resting-state fMRI data. We used Craddock atlas and Power atlas to extract functional connectivity data from rs-fMRI, resulting in over 30 thousand features. By using a denoising variational autoencoder, our proposed pipeline further compresses the connectivity features into 5 latent Gaussian distributions, providing is a low-dimensional representation of the data to promote computational efficiency and interpretability. To test the method, we employed the extracted latent representations to classify ASD using traditional classifiers such as SVM on a large multi-site dataset. The 95% confidence interval for the prediction accuracy of SVM is [0.63, 0.76] after site harmonization using the extracted latent distributions. Without using DVAE for dimensionality reduction, the prediction accuracy is 0.70, which falls within the interval. The DVAE successfully encoded the diagnostic information from rs-fMRI data without sacrificing prediction performance. The runtime for training the DVAE and obtaining classification results from its extracted latent features was 7 times shorter compared to training classifiers directly on the raw data. Our findings suggest that the Power atlas provides more effective brain connectivity insights for diagnosing ASD than Craddock atlas. Additionally, we visualized the latent representations to gain insights into the brain networks contributing to the differences between ASD and neurotypical brains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00068v2</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyuan Zheng, Orren Ravid, Robert A. J. Barry, Yoojean Kim, Qian Wang, Young-geun Kim, Xi Zhu, Xiaofu He</dc:creator>
    </item>
    <item>
      <title>The ultimate issue error in scientific inference: mistaking parameters for hypotheses</title>
      <link>https://arxiv.org/abs/2411.15398</link>
      <description>arXiv:2411.15398v2 Announce Type: replace-cross 
Abstract: Statistical inference often conflates the probability of a parameter with the probability of a hypothesis, a critical misunderstanding termed the ultimate issue error. This error is pervasive across the social, biological, and medical sciences, where null hypothesis significance testing (NHST) is mistakenly understood to be testing hypotheses rather than evaluating parameter estimates. Here, we advocate for using the Weight of Evidence (WoE) approach, which integrates quantitative data with qualitative background information for more accurate and transparent inference. Through a detailed example involving the relationship between vitamin D (25-hydroxy vitamin D) levels and COVID-19 risk, we demonstrate how WoE quantifies support for hypotheses while accounting for study design biases, power, and confounding factors. These findings emphasise the necessity of combining statistical metrics with contextual evaluation. This offers a structured framework to enhance reproducibility, reduce false interpretations, and foster robust scientific conclusions across disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15398v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanley E. Lazic</dc:creator>
    </item>
  </channel>
</rss>
