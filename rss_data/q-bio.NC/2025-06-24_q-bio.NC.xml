<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Sex-Dependent Effects of Psychedelics on Myelination in APOE4 Mice</title>
      <link>https://arxiv.org/abs/2506.17293</link>
      <description>arXiv:2506.17293v1 Announce Type: new 
Abstract: Several studies have linked myelin abnormalities with neuropsychiatric disorders; others have implicated psychedelics as a potential therapeutic for such conditions. One risk factor for these demyelinating disorders is a mutation in the Apolipoprotein E gene known as APOE4. This variant impedes the cholesterol regulation of oligodendrocytes responsible for the myelination, or insulation, of neurons when compared to the wild-type phenotype. In this work, I advance knowledge of cellular pathways involved in the progression of APOE4-related diseases and elucidate the effects of psychedelics on the brain. Myelin sheaths are vital for maintaining neural pathways, and healthy oligodendrocytes serve as a prerequisite for axonal integrity. Further, the Kaufer Lab has observed significant behavioral differences between male and female APOE4 mice following psychedelic treatment with 2,5-Dimethoxy-4-iodoamphetamine, or DOI, a serotonin receptor ligand. The sex-dependent mechanisms influencing symptom differences and treatment outcomes in AD are unclear, and could be key to developing successful therapeutics for myelin-related issues. I hypothesize that administration of DOI will increase the myelination activity of oligodendrocytes in female APOE4 mice compared with their male counterparts or controls. Preliminary results show a significant increase in MBP in the CA1, or short-term, and CA2, or long term, areas in only female APOE4 mice post-introduction of DOI to the system. This aligns with behavioral data indicating fewer anxiety-related behaviors in female APOE4 mice after DOI administration. These findings reveal distinct biological mechanisms in male and female brain degeneration and suggest potential for sex-specific therapeutics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17293v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.TO</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Sanjana Shankar</dc:creator>
    </item>
    <item>
      <title>PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding</title>
      <link>https://arxiv.org/abs/2506.17310</link>
      <description>arXiv:2506.17310v1 Announce Type: new 
Abstract: While Large Language Models (LLMs) demonstrate strong performance across domains, their long-context capabilities are limited by transient neural activations causing information decay and unstructured feed-forward network (FFN) weights leading to semantic fragmentation. Inspired by the brain's working memory and cortical modularity, we propose PaceLLM, featuring two innovations: (1) a Persistent Activity (PA) Mechanism that mimics prefrontal cortex (PFC) neurons' persistent firing by introducing an activation-level memory bank to dynamically retrieve, reuse, and update critical FFN states, addressing contextual decay; and (2) Cortical Expert (CE) Clustering that emulates task-adaptive neural specialization to reorganize FFN weights into semantic modules, establishing cross-token dependencies and mitigating fragmentation. Extensive evaluations show that PaceLLM achieves 6% improvement on LongBench's Multi-document QA and 12.5-17.5% performance gains on Infinite-Bench tasks, while extending measurable context length to 200K tokens in Needle-In-A-Haystack (NIAH) tests. This work pioneers brain-inspired LLM optimization and is complementary to other works. Besides, it can be generalized to any model and enhance their long-context performance and interpretability without structural overhauls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17310v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CL</category>
      <category>cs.NE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kangcong Li, Peng Ye, Chongjun Tu, Lin Zhang, Chunfeng Song, Jiamin Wu, Tao Yang, Qihao Zheng, Tao Chen</dc:creator>
    </item>
    <item>
      <title>Challenges in Grounding Language in the Real World</title>
      <link>https://arxiv.org/abs/2506.17375</link>
      <description>arXiv:2506.17375v1 Announce Type: new 
Abstract: A long-term goal of Artificial Intelligence is to build a language understanding system that allows a human to collaborate with a physical robot using language that is natural to the human. In this paper we highlight some of the challenges in doing this, and propose a solution that integrates the abilities of a cognitive agent capable of interactive task learning in a physical robot with the linguistic abilities of a large language model. We also point the way to an initial implementation of this approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17375v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Lindes, Kaoutar Skiker</dc:creator>
    </item>
    <item>
      <title>Sequence-to-Sequence Models with Attention Mechanistically Map to the Architecture of Human Memory Search</title>
      <link>https://arxiv.org/abs/2506.17424</link>
      <description>arXiv:2506.17424v1 Announce Type: new 
Abstract: Past work has long recognized the important role of context in guiding how humans search their memory. While context-based memory models can explain many memory phenomena, it remains unclear why humans develop such architectures over possible alternatives in the first place. In this work, we demonstrate that foundational architectures in neural machine translation -- specifically, recurrent neural network (RNN)-based sequence-to-sequence models with attention -- exhibit mechanisms that directly correspond to those specified in the Context Maintenance and Retrieval (CMR) model of human memory. Since neural machine translation models have evolved to optimize task performance, their convergence with human memory models provides a deeper understanding of the functional role of context in human memory, as well as presenting new ways to model human memory. Leveraging this convergence, we implement a neural machine translation model as a cognitive model of human memory search that is both interpretable and capable of capturing complex dynamics of learning. We show that our model accounts for both averaged and optimal human behavioral patterns as effectively as context-based memory models. Further, we demonstrate additional strengths of the proposed model by evaluating how memory search performance emerges from the interaction of different model components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17424v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikolaus Salvatore, Qiong Zhang</dc:creator>
    </item>
    <item>
      <title>The Relationship between Cognition and Computation: "Global-first" Cognition versus Local-first Computation</title>
      <link>https://arxiv.org/abs/2506.17970</link>
      <description>arXiv:2506.17970v1 Announce Type: new 
Abstract: What fundamental research questions are essential for advancing toward brain-like AI or AGI (Artificial General Intelligence) capable of performing any intellectual task a human can? Should it be something like the Turing machine (1936), which answers the question "What is computation?" and lays the foundation for the entire field of computer science? Or should it be something like Shannon's mathematical theory of communication (1948), which answers the question "What is information?" and forms the basis for modern communication technology? We believe the key question today is the relationship between cognition and computation (RCC). For example, the widely discussed question "Will artificial intelligence replace the human mind?" is, in essence and in scientific terms, an issue concerning RCC. We have chosen to classify RCC into four categories: 1. The relationship between the primitives of cognition and the primitives of computation. 2. The relationship between the anatomical structure of neural representation of cognition and the computational architecture of artificial intelligence. 3. The relationship between emergents in cognition and emergents in computation. 4. The relationship between the mathematical foundations of cognition and computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17970v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Chen</dc:creator>
    </item>
    <item>
      <title>Perceptual multistability: a window for a multi-facet understanding of psychiatric disorders</title>
      <link>https://arxiv.org/abs/2506.18176</link>
      <description>arXiv:2506.18176v1 Announce Type: new 
Abstract: Perceptual multistability, observed across species and sensory modalities, offers valuable insights into numerous cognitive functions and dysfunctions. For instance, differences in temporal dynamics and information integration during percept formation often distinguish clinical from non-clinical populations. Computational psychiatry can elucidate these variations, through two primary approaches: (i) Bayesian modeling, which treats perception as an unconscious inference, and (ii) an active, information-seeking perspective (e.g., reinforcement learning) framing perceptual switches as internal actions. Our synthesis aims to leverage multistability to bridge these computational psychiatry subfields, linking human and animal studies as well as connecting behavior to underlying neural mechanisms. Perceptual multistability emerges as a promising non-invasive tool for clinical applications, facilitating translational research and enhancing our mechanistic understanding of cognitive processes and their impairments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18176v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shervin Safavi, Dana\'e Rolland, Philipp Sterzer, Renaud Jardri, Pantelis Leptourgos</dc:creator>
    </item>
    <item>
      <title>Projected Normal Distribution: Moment Approximations and Generalizations</title>
      <link>https://arxiv.org/abs/2506.17461</link>
      <description>arXiv:2506.17461v1 Announce Type: cross 
Abstract: The projected normal distribution, also known as the angular Gaussian distribution, is obtained by dividing a multivariate normal random variable $\mathbf{x}$ by its norm $\sqrt{\mathbf{x}^T \mathbf{x}}$. The resulting random variable follows a distribution on the unit sphere. No closed-form formulas for the moments of the projected normal distribution are known, which can limit its use in some applications. In this work, we derive analytic approximations to the first and second moments of the projected normal distribution using Taylor expansions and using results from the theory of quadratic forms of Gaussian random variables. Then, motivated by applications in systems neuroscience, we present generalizations of the projected normal distribution that divide the variable $\mathbf{x}$ by a denominator of the form $\sqrt{\mathbf{x}^T \mathbf{B} \mathbf{x} + c}$, where $\mathbf{B}$ is a symmetric positive definite matrix and $c$ is a non-negative number. We derive moment approximations as well as the density function for these other projected distributions. We show that the moments approximations are accurate for a wide range of dimensionalities and distribution parameters. Furthermore, we show that the moments approximations can be used to fit these distributions to data through moment matching. These moment matching methods should be useful for analyzing data across a range of applications where the projected normal distribution is used, and for applying the projected normal distribution and its generalizations to model data in neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17461v1</guid>
      <category>stat.ME</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Herrera-Esposito, Johannes Burge</dc:creator>
    </item>
    <item>
      <title>BrainSymphony: A Transformer-Driven Fusion of fMRI Time Series and Structural Connectivity</title>
      <link>https://arxiv.org/abs/2506.18314</link>
      <description>arXiv:2506.18314v1 Announce Type: cross 
Abstract: Existing foundation models for neuroimaging are often prohibitively large and data-intensive. We introduce BrainSymphony, a lightweight, parameter-efficient foundation model that achieves state-of-the-art performance while being pre-trained on significantly smaller public datasets. BrainSymphony's strong multimodal architecture processes functional MRI data through parallel spatial and temporal transformer streams, which are then efficiently distilled into a unified representation by a Perceiver module. Concurrently, it models structural connectivity from diffusion MRI using a novel signed graph transformer to encode the brain's anatomical structure. These powerful, modality-specific representations are then integrated via an adaptive fusion gate. Despite its compact design, our model consistently outperforms larger models on a diverse range of downstream benchmarks, including classification, prediction, and unsupervised network identification tasks. Furthermore, our model revealed novel insights into brain dynamics using attention maps on a unique external psilocybin neuroimaging dataset (pre- and post-administration). BrainSymphony establishes that architecturally-aware, multimodal models can surpass their larger counterparts, paving the way for more accessible and powerful research in computational neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18314v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moein Khajehnejad, Forough Habibollahi, Adeel Razi</dc:creator>
    </item>
    <item>
      <title>Bayesian Theory of Consciousness as Exchangeable Emotion-Cognition Inference</title>
      <link>https://arxiv.org/abs/2407.09488</link>
      <description>arXiv:2407.09488v2 Announce Type: replace 
Abstract: This paper proposes a unified framework in which consciousness emerges as a cycle-consistent, affectively anchored inference process, recursively structured by the interaction of emotion and cognition. Drawing from information theory, optimal transport, and the Bayesian brain hypothesis, we formalize emotion as a low-dimensional structural prior and cognition as a specificity-instantiating update. This emotion-cognition cycle minimizes joint uncertainty by aligning emotionally weighted priors with context-sensitive cognitive appraisals. Subjective experience thus arises as the informational footprint of temporally extended, affect-modulated simulation. We introduce the Exchangeable Integration Theory of Consciousness (EITC), modeling conscious episodes as conditionally exchangeable samples drawn from a latent affective self-model. This latent variable supports integration, via a unified cause-effect structure with nonzero irreducibility, and differentiation, by preserving contextual specificity across episodes. We connect this architecture to the Bayesian theory of consciousness through Rao-Blackwellized inference, which stabilizes inference by marginalizing latent self-structure while enabling adaptive updates. This mechanism ensures coherence, prevents inference collapse, and supports goal-directed simulation. The formal framework builds on De Finetti's exchangeability theorem, integrated information theory, and KL-regularized optimal transport. Overall, consciousness is reframed as a recursive inference process, shaped by emotion, refined by cognition, stabilized through exchangeability, and unified through a latent self-model that integrates experience across time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09488v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Xin Li</dc:creator>
    </item>
    <item>
      <title>Universal scale-free representations in human visual cortex</title>
      <link>https://arxiv.org/abs/2409.06843</link>
      <description>arXiv:2409.06843v2 Announce Type: replace 
Abstract: How does the human brain encode complex visual information? While previous research has characterized individual dimensions of visual representation in cortex, we still lack a comprehensive understanding of how visual information is organized across the full range of neural population activity. Here, analyzing fMRI responses to natural scenes across multiple individuals, we discover that neural representations in human visual cortex follow a remarkably consistent scale-free organization -- their variance systematically decays as a power law, detected across four orders of magnitude of latent dimensions. This scale-free structure appears consistently across multiple visual regions and across individuals, suggesting it reflects a fundamental organizing principle of visual processing. Critically, when we align neural responses across individuals using hyperalignment, we find that these representational dimensions are largely shared between people, revealing a universal high-dimensional spectrum of visual information that emerges despite individual differences in brain anatomy and visual experience. Traditional analysis approaches in cognitive neuroscience have focused primarily on a small number of high-variance dimensions, potentially missing crucial aspects of visual representation. Our results demonstrate that visual information is distributed across the full dimensionality of cortical activity in a systematic way, suggesting we need to move beyond low-dimensional characterizations to fully understand how the brain represents the visual world. This work reveals a new fundamental principle of neural coding in human visual cortex and highlights the importance of examining neural representations across their full dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06843v2</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Raj Magesh Gauthaman, Brice M\'enard, Michael F. Bonner</dc:creator>
    </item>
    <item>
      <title>A generalized neural tangent kernel for surrogate gradient learning</title>
      <link>https://arxiv.org/abs/2405.15539</link>
      <description>arXiv:2405.15539v2 Announce Type: replace-cross 
Abstract: State-of-the-art neural network training methods depend on the gradient of the network function. Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks. To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL). This method works well in practice but lacks theoretical foundation. The neural tangent kernel (NTK) has proven successful in the analysis of gradient descent. Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL. First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit. To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL. We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor. Further, we illustrate our findings with numerical experiments. Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15539v2</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Adv. Neural Inf. Process. Syst. 37 (2024) 9026-9085</arxiv:journal_reference>
      <dc:creator>Luke Eilers, Raoul-Martin Memmesheimer, Sven Goedeke</dc:creator>
    </item>
    <item>
      <title>Memorization to Generalization: Emergence of Diffusion Models from Associative Memory</title>
      <link>https://arxiv.org/abs/2505.21777</link>
      <description>arXiv:2505.21777v2 Announce Type: replace-cross 
Abstract: Hopfield networks are associative memory (AM) systems, designed for storing and retrieving patterns as local minima of an energy landscape. In the classical Hopfield model, an interesting phenomenon occurs when the amount of training data reaches its critical memory load $- spurious\,\,states$, or unintended stable points, emerge at the end of the retrieval dynamics, leading to incorrect recall. In this work, we examine diffusion models, commonly used in generative modeling, from the perspective of AMs. The training phase of diffusion model is conceptualized as memory encoding (training data is stored in the memory). The generation phase is viewed as an attempt of memory retrieval. In the small data regime the diffusion model exhibits a strong memorization phase, where the network creates distinct basins of attraction around each sample in the training set, akin to the Hopfield model below the critical memory load. In the large data regime, a different phase appears where an increase in the size of the training set fosters the creation of new attractor states that correspond to manifolds of the generated samples. Spurious states appear at the boundary of this transition and correspond to emergent attractor states, which are absent in the training set, but, at the same time, have distinct basins of attraction around them. Our findings provide: a novel perspective on the memorization-generalization phenomenon in diffusion models via the lens of AMs, theoretical prediction of existence of spurious states, empirical validation of this prediction in commonly-used diffusion models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21777v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bao Pham, Gabriel Raya, Matteo Negri, Mohammed J. Zaki, Luca Ambrogioni, Dmitry Krotov</dc:creator>
    </item>
  </channel>
</rss>
