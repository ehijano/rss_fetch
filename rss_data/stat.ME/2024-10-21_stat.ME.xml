<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Oct 2024 03:30:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Modeling Zero-Inflated Correlated Dental Data through Gaussian Copulas and Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2410.13949</link>
      <description>arXiv:2410.13949v1 Announce Type: new 
Abstract: We develop a new longitudinal count data regression model that accounts for zero-inflation and spatio-temporal correlation across responses. This project is motivated by an analysis of Iowa Fluoride Study (IFS) data, a longitudinal cohort study with data on caries (cavity) experience scores measured for each tooth across five time points. To that end, we use a hurdle model for zero-inflation with two parts: the presence model indicating whether a count is non-zero through logistic regression and the severity model that considers the non-zero counts through a shifted Negative Binomial distribution allowing overdispersion. To incorporate dependence across measurement occasion and teeth, these marginal models are embedded within a Gaussian copula that introduces spatio-temporal correlations. A distinct advantage of this formulation is that it allows us to determine covariate effects with population-level (marginal) interpretations in contrast to mixed model choices. Standard Bayesian sampling from such a model is infeasible, so we use approximate Bayesian computing for inference. This approach is applied to the IFS data to gain insight into the risk factors for dental caries and the correlation structure across teeth and time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13949v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anish Mukherjee, Jeremy T. Gaskins, Shoumi Sarkar, Steven Levy, Somnath Datta</dc:creator>
    </item>
    <item>
      <title>A note on Bayesian R-squared for generalized additive mixed models</title>
      <link>https://arxiv.org/abs/2410.14002</link>
      <description>arXiv:2410.14002v1 Announce Type: new 
Abstract: We present a novel Bayesian framework to decompose the posterior predictive variance in a fitted Generalized Additive Mixed Model (GAMM) into explained and unexplained components. This decomposition enables a rigorous definition of Bayesian $R^{2}$. We show that the new definition aligns with the intuitive Bayesian $R^{2}$ proposed by Gelman, Goodrich, Gabry, and Vehtari (2019) [\emph{The American Statistician}, \textbf{73}(3), 307-309], but extends its applicability to a broader class of models. Furthermore, we introduce a partial Bayesian $R^{2}$ to quantify the contribution of individual model terms to the explained variation in the posterior predictions</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14002v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdollah Jalilian, Aki Vehtari, Luigi Sedda</dc:creator>
    </item>
    <item>
      <title>Confidence interval for the sensitive fraction in Item Count Technique model</title>
      <link>https://arxiv.org/abs/2410.14301</link>
      <description>arXiv:2410.14301v1 Announce Type: new 
Abstract: The problem is in the estimation of the fraction of population with a sensitive characteristic. We consider the Item Count Technique an indirect method of questioning designed to protect respondents' privacy. The exact confidence interval for the sensitive fraction is constructed. The length of the proposed CI depends on both the given parameter of the model and the sample size. For these CI the model's parameter is established in relation to the provided level of the privacy protection of the interviewee. The optimal sample size for obtaining a CI of a given length is discussed in the context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14301v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stanislaw Jaworski, Wojciech Zielinski</dc:creator>
    </item>
    <item>
      <title>Adaptive L-statistics for high dimensional test problem</title>
      <link>https://arxiv.org/abs/2410.14308</link>
      <description>arXiv:2410.14308v1 Announce Type: new 
Abstract: In this study, we focus on applying L-statistics to the high-dimensional one-sample location test problem. Intuitively, an L-statistic with $k$ parameters tends to perform optimally when the sparsity level of the alternative hypothesis matches $k$. We begin by deriving the limiting distributions for both L-statistics with fixed parameters and those with diverging parameters. To ensure robustness across varying sparsity levels of alternative hypotheses, we first establish the asymptotic independence between L-statistics with fixed and diverging parameters. Building on this, we propose a Cauchy combination test that integrates L-statistics with different parameters. Both simulation results and real-data applications highlight the advantages of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14308v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huifang Ma, Long Feng, Zhaojun Wang</dc:creator>
    </item>
    <item>
      <title>To Vary or Not To Vary: A Simple Empirical Bayes Factor for Testing Variance Components</title>
      <link>https://arxiv.org/abs/2410.14459</link>
      <description>arXiv:2410.14459v1 Announce Type: new 
Abstract: Random effects are a flexible addition to statistical models to capture structural heterogeneity in the data, such as spatial dependencies, individual differences, temporal dependencies, or non-linear effects. Testing for the presence (or absence) of random effects is an important but challenging endeavor however, as testing a variance component, which must be non-negative, is a boundary problem. Various methods exist which have potential shortcomings or limitations. As a flexible alternative, we propose a flexible empirical Bayes factor (EBF) for testing for the presence of random effects. Rather than testing whether a variance component equals zero or not, the proposed EBF tests the equivalent assumption of whether all random effects are zero. The Bayes factor is `empirical' because the distribution of the random effects on the lower level, which serves as a prior, is estimated from the data as it is part of the model. Empirical Bayes factors can be computed using the output from classical (MLE) or Bayesian (MCMC) approaches. Analyses on synthetic data were carried out to assess the general behavior of the criterion. To illustrate the methodology, the EBF is used for testing random effects under various models including logistic crossed mixed effects models, spatial random effects models, dynamic structural equation models, random intercept cross-lagged panel models, and nonlinear regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14459v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabio Vieira, Hongwei Zhao, Joris Mulder</dc:creator>
    </item>
    <item>
      <title>Bin-Conditional Conformal Prediction of Fatalities from Armed Conflict</title>
      <link>https://arxiv.org/abs/2410.14507</link>
      <description>arXiv:2410.14507v1 Announce Type: new 
Abstract: Forecasting of armed conflicts is an important area of research that has the potential to save lives and prevent suffering. However, most existing forecasting models provide only point predictions without any individual-level uncertainty estimates. In this paper, we introduce a novel extension to conformal prediction algorithm which we call bin-conditional conformal prediction. This method allows users to obtain individual-level prediction intervals for any arbitrary prediction model while maintaining a specific level of coverage across user-defined ranges of values. We apply the bin-conditional conformal prediction algorithm to forecast fatalities from armed conflict. Our results demonstrate that the method provides well-calibrated uncertainty estimates for the predicted number of fatalities. Compared to standard conformal prediction, the bin-conditional method outperforms offers improved calibration of coverage rates across different values of the outcome, but at the cost of wider prediction intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14507v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Randahl, Jonathan P. Williams, H{\aa}vard Hegre</dc:creator>
    </item>
    <item>
      <title>The Traveling Bandit: A Framework for Bayesian Optimization with Movement Costs</title>
      <link>https://arxiv.org/abs/2410.14533</link>
      <description>arXiv:2410.14533v1 Announce Type: new 
Abstract: This paper introduces a framework for Bayesian Optimization (BO) with metric movement costs, addressing a critical challenge in practical applications where input alterations incur varying costs. Our approach is a convenient plug-in that seamlessly integrates with the existing literature on batched algorithms, where designs within batches are observed following the solution of a Traveling Salesman Problem. The proposed method provides a theoretical guarantee of convergence in terms of movement costs for BO. Empirically, our method effectively reduces average movement costs over time while maintaining comparable regret performance to conventional BO methods. This framework also shows promise for broader applications in various bandit settings with movement costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14533v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyuan Chen, Raed Al Kontar</dc:creator>
    </item>
    <item>
      <title>Tensor Decomposition with Unaligned Observations</title>
      <link>https://arxiv.org/abs/2410.14046</link>
      <description>arXiv:2410.14046v1 Announce Type: cross 
Abstract: This paper presents a canonical polyadic (CP) tensor decomposition that addresses unaligned observations. The mode with unaligned observations is represented using functions in a reproducing kernel Hilbert space (RKHS). We introduce a versatile loss function that effectively accounts for various types of data, including binary, integer-valued, and positive-valued types. Additionally, we propose an optimization algorithm for computing tensor decompositions with unaligned observations, along with a stochastic gradient method to enhance computational efficiency. A sketching algorithm is also introduced to further improve efficiency when using the $\ell_2$ loss function. To demonstrate the efficacy of our methods, we provide illustrative examples using both synthetic data and an early childhood human microbiome dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14046v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Runshi Tang, Tamara Kolda, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Predicting the trajectory of intracranial pressure in patients with traumatic brain injury: evaluation of a foundation model for time series</title>
      <link>https://arxiv.org/abs/2410.14333</link>
      <description>arXiv:2410.14333v1 Announce Type: cross 
Abstract: Patients with traumatic brain injury (TBI) often experience pathological increases in intracranial pressure (ICP), leading to intracranial hypertension (tIH), a common and serious complication. Early warning of an impending rise in ICP could potentially improve patient outcomes by enabling preemptive clinical intervention. However, the limited availability of patient data poses a challenge in developing reliable prediction models. In this study, we aim to determine whether foundation models, which leverage transfer learning, may offer a promising solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14333v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Florian D. van Leeuwen, Shubhayu Bhattacharyay, Alex Carriero, Ethan Jacob Moyer, Richard Moberg</dc:creator>
    </item>
    <item>
      <title>Spectral Representations for Accurate Causal Uncertainty Quantification with Gaussian Processes</title>
      <link>https://arxiv.org/abs/2410.14483</link>
      <description>arXiv:2410.14483v1 Announce Type: cross 
Abstract: Accurate uncertainty quantification for causal effects is essential for robust decision making in complex systems, but remains challenging in non-parametric settings. One promising framework represents conditional distributions in a reproducing kernel Hilbert space and places Gaussian process priors on them to infer posteriors on causal effects, but requires restrictive nuclear dominant kernels and approximations that lead to unreliable uncertainty estimates. In this work, we introduce a method, IMPspec, that addresses these limitations via a spectral representation of the Hilbert space. We show that posteriors in this model can be obtained explicitly, by extending a result in Hilbert space regression theory. We also learn the spectral representation to optimise posterior calibration. Our method achieves state-of-the-art performance in uncertainty quantification and causal Bayesian optimisation across simulations and a healthcare application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14483v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugh Dance, Peter Orbanz, Arthur Gretton</dc:creator>
    </item>
    <item>
      <title>Matrix normal distribution and elliptic distribution</title>
      <link>https://arxiv.org/abs/2410.14490</link>
      <description>arXiv:2410.14490v1 Announce Type: cross 
Abstract: In this paper, we introduce the matrix normal distribution according to the tensor decomposition of its covariance. Based on the canonical diagonal form, the moment generating function of sample covariance matrix and the distribution of latent roots are explicitly calculated. We also discuss the connections between matrix normal distributions, elliptic distributions, and their relevance to multivariate analysis and matrix variate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14490v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Haoming Wang</dc:creator>
    </item>
    <item>
      <title>Identifiability of Sparse Causal Effects using Instrumental Variables</title>
      <link>https://arxiv.org/abs/2203.09380</link>
      <description>arXiv:2203.09380v4 Announce Type: replace 
Abstract: Exogenous heterogeneity, for example, in the form of instrumental variables can help us learn a system's underlying causal structure and predict the outcome of unseen intervention experiments. In this paper, we consider linear models in which the causal effect from covariates $X$ on a response $Y$ is sparse. We provide conditions under which the causal coefficient becomes identifiable from the observed distribution. These conditions can be satisfied even if the number of instruments is as small as the number of causal parents. We also develop graphical criteria under which identifiability holds with probability one if the edge coefficients are sampled randomly from a distribution that is absolutely continuous with respect to Lebesgue measure and $Y$ is childless. As an estimator, we propose spaceIV and prove that it consistently estimates the causal effect if the model is identifiable and evaluate its performance on simulated data. If identifiability does not hold, we show that it may still be possible to recover a subset of the causal parents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.09380v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niklas Pfister, Jonas Peters</dc:creator>
    </item>
    <item>
      <title>Typical Algorithms for Estimating Hurst Exponent of Time Sequence: A Data Analyst's Perspective</title>
      <link>https://arxiv.org/abs/2310.19051</link>
      <description>arXiv:2310.19051v3 Announce Type: replace 
Abstract: The Hurst exponent is a significant indicator for characterizing the self-similarity and long-term memory properties of time sequences. It has wide applications in physics, technologies, engineering, mathematics, statistics, economics, psychology and so on. Currently, available methods for estimating the Hurst exponent of time sequences can be divided into different categories: time-domain methods and spectrum-domain methods based on the representation of time sequence, linear regression methods and Bayesian methods based on parameter estimation methods. Although various methods are discussed in literature, there are still some deficiencies: the descriptions of the estimation algorithms are just mathematics-oriented and the pseudo-codes are missing; the effectiveness and accuracy of the estimation algorithms are not clear; the classification of estimation methods is not considered and there is a lack of guidance for selecting the estimation methods. In this work, the emphasis is put on thirteen dominant methods for estimating the Hurst exponent. For the purpose of decreasing the difficulty of implementing the estimation methods with computer programs, the mathematical principles are discussed briefly and the pseudo-codes of algorithms are presented with necessary details. It is expected that the survey could help the researchers to select, implement and apply the estimation algorithms of interest in practical situations in an easy way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19051v3</guid>
      <category>stat.ME</category>
      <category>cs.MS</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hong-Yan Zhang, Zhi-Qiang Feng, Si-Yu Feng, Yu Zhou</dc:creator>
    </item>
    <item>
      <title>FedECA: A Federated External Control Arm Method for Causal Inference with Time-To-Event Data in Distributed Settings</title>
      <link>https://arxiv.org/abs/2311.16984</link>
      <description>arXiv:2311.16984v4 Announce Type: replace 
Abstract: External control arms (ECA) can inform the early clinical development of experimental drugs and provide efficacy evidence for regulatory approval. However, the main challenge in implementing ECA lies in accessing real-world or historical clinical trials data. Indeed, regulations protecting patients' rights by strictly controlling data processing make pooling data from multiple sources in a central server often difficult. To address these limitations, we develop a new method, 'FedECA' that leverages federated learning (FL) to enable inverse probability of treatment weighting (IPTW) for time-to-event outcomes on separate cohorts without needing to pool data. To showcase the potential of FedECA, we apply it in different settings of increasing complexity culminating with a real-world use-case in which FedECA provides evidence for a differential effect between two drugs that would have otherwise gone unnoticed. By sharing our code, we hope FedECA will foster the creation of federated research networks and thus accelerate drug development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16984v4</guid>
      <category>stat.ME</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean Ogier du Terrail, Quentin Klopfenstein, Honghao Li, Imke Mayer, Nicolas Loiseau, Mohammad Hallal, Michael Debouver, Thibault Camalon, Thibault Fouqueray, Jorge Arellano Castro, Zahia Yanes, Laetitia Dahan, Julien Ta\"ieb, Pierre Laurent-Puig, Jean-Baptiste Bachet, Shulin Zhao, Remy Nicolle, J\'erome Cros, Daniel Gonzalez, Robert Carreras-Torres, Adelaida Garcia Velasco, Kawther Abdilleh, Sudheer Doss, F\'elix Balazard, Mathieu Andreux</dc:creator>
    </item>
    <item>
      <title>Individualized Multi-Treatment Response Curves Estimation using RBF-net with Shared Neurons</title>
      <link>https://arxiv.org/abs/2401.16571</link>
      <description>arXiv:2401.16571v5 Announce Type: replace 
Abstract: Heterogeneous treatment effect estimation is an important problem in precision medicine. Specific interests lie in identifying the differential effect of different treatments based on some external covariates. We propose a novel non-parametric treatment effect estimation method in a multi-treatment setting. Our non-parametric modeling of the response curves relies on radial basis function (RBF)-nets with shared hidden neurons. Our model thus facilitates modeling commonality among the treatment outcomes. The estimation and inference schemes are developed under a Bayesian framework using thresholded best linear projections and implemented via an efficient Markov chain Monte Carlo algorithm, appropriately accommodating uncertainty in all aspects of the analysis. The numerical performance of the method is demonstrated through simulation experiments. Applying our proposed method to MIMIC data, we obtain several interesting findings related to the impact of different treatment strategies on the length of ICU stay and 12-hour SOFA score for sepsis patients who are home-discharged.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16571v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Chang, Arkaprava Roy</dc:creator>
    </item>
    <item>
      <title>A More Credible Approach to Multivariable Mendelian Randomization</title>
      <link>https://arxiv.org/abs/2402.00307</link>
      <description>arXiv:2402.00307v3 Announce Type: replace 
Abstract: Multivariable Mendelian randomization (MVMR) uses genetic variants as instrumental variables to infer the direct effects of multiple exposures on an outcome. However, unlike univariable MR, MVMR often faces greater challenges with many weak instruments, which can lead to bias not necessarily toward zero and inflation of type I errors. In this work, we introduce a new asymptotic regime that allows exposures to have different degrees of instrument strength, providing a more credible theoretical framework for studying MVMR estimators. Our analysis of the widely used multivariable inverse-variance weighted method shows that it is often biased and tends to produce misleadingly narrow confidence intervals in the presence of many weak instruments. To address this, we propose a spectral regularized estimator and show that the estimator is consistent and asymptotically normal under many weak instruments. We demonstrate through simulations and real applications that our proposed estimator would bring more credibility to MVMR analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00307v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinxiang Wu, Hyunseung Kang, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Recoverability of Causal Effects under Presence of Missing Data: a Longitudinal Case Study</title>
      <link>https://arxiv.org/abs/2402.14562</link>
      <description>arXiv:2402.14562v3 Announce Type: replace 
Abstract: Missing data in multiple variables is a common issue. We investigate the applicability of the framework of graphical models for handling missing data to a complex longitudinal pharmacological study of children with HIV treated with an efavirenz-based regimen as part of the CHAPAS-3 trial. Specifically, we examine whether the causal effects of interest, defined through static interventions on multiple continuous variables, can be recovered (estimated consistently) from the available data only. So far, no general algorithms are available to decide on recoverability, and decisions have to be made on a case-by-case basis. We emphasize sensitivity of recoverability to even the smallest changes in the graph structure, and present recoverability results for three plausible missingness directed acyclic graphs (m-DAGs) in the CHAPAS-3 study, informed by clinical knowledge. Furthermore, we propose the concept of ``closed missingness mechanisms'' and show that under these mechanisms an available case analysis is admissible for consistent estimation for any type of statistical and causal query, even if the underlying missingness mechanism is of missing not at random (MNAR) type. Both simulations and theoretical considerations demonstrate how, in the assumed MNAR setting of our study, a complete or available case analysis can be superior to multiple imputation, and estimation results vary depending on the assumed missingness DAG. Our analyses demonstrate an innovative application of missingness DAGs to complex longitudinal real-world data, while highlighting the sensitivity of the results with respect to the assumed causal model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14562v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anastasiia Holovchak, Helen McIlleron, Paolo Denti, Michael Schomaker</dc:creator>
    </item>
    <item>
      <title>Differentially Private Boxplots</title>
      <link>https://arxiv.org/abs/2405.20415</link>
      <description>arXiv:2405.20415v2 Announce Type: replace 
Abstract: Despite the potential of differentially private data visualization to harmonize data analysis and privacy, research in this area remains relatively underdeveloped. Boxplots are a widely popular visualization used for summarizing a dataset and for comparison of multiple datasets. Consequentially, we introduce a differentially private boxplot. We evaluate its effectiveness for displaying location, scale, skewness and tails of a given empirical distribution. In our theoretical exposition, we show that the location and scale of the boxplot are estimated with optimal sample complexity, and the skewness and tails are estimated consistently. In simulations, we show that this boxplot performs similarly to a non-private boxplot, and it outperforms a boxplot naively constructed from existing differentially private quantile algorithms. Additionally, we conduct a real data analysis of Airbnb listings, which shows that comparable analysis can be achieved through differentially private boxplot visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20415v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelly Ramsay, Jairo Diaz-Rodriguez</dc:creator>
    </item>
    <item>
      <title>Experimenting on Markov Decision Processes with Local Treatments</title>
      <link>https://arxiv.org/abs/2407.19618</link>
      <description>arXiv:2407.19618v2 Announce Type: replace 
Abstract: Utilizing randomized experiments to evaluate the effect of short-term treatments on the short-term outcomes has been well understood and become the golden standard in industrial practice. However, as service systems become increasingly dynamical and personalized, much focus is shifting toward maximizing long-term cumulative outcomes, such as customer lifetime value, through lifetime exposure to interventions. To bridge this gap, we investigate the randomized experiments within dynamical systems modeled as Markov Decision Processes (MDPs). Our goal is to assess the impact of treatment and control policies on long-term cumulative rewards from relatively short-term observations. We first develop optimal inference techniques for assessing the effects of general treatment patterns. Furthermore, recognizing that many real-world treatments tend to be fine-grained and localized for practical efficiency and operational convenience, we then propose methods to harness this localized structure by sharing information on the non-targeted states. Our new estimator effectively overcomes the variance lower bound for general treatments while matching the more stringent lower bound incorporating the local treatment structure. Furthermore, our estimator can optimally achieve a linear reduction with the number of test arms for a major part of the variance. Finally, we explore scenarios with perfect knowledge of the control arm and design estimators that further improve inference efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19618v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuze Chen, David Simchi-Levi, Chonghuan Wang</dc:creator>
    </item>
    <item>
      <title>Identifying treatment response subgroups in observational time-to-event data</title>
      <link>https://arxiv.org/abs/2408.03463</link>
      <description>arXiv:2408.03463v3 Announce Type: replace 
Abstract: Identifying patient subgroups with different treatment responses is an important task to inform medical recommendations, guidelines, and the design of future clinical trials. Existing approaches for subgroup analysis primarily rely on Randomised Controlled Trials (RCTs), in which treatment assignment is randomised. RCTs' patient cohorts are often constrained by cost, rendering them not representative of the heterogeneity of patients likely to receive treatment in real-world clinical practice. When applied to observational studies, subgroup analysis approaches suffer from significant statistical biases particularly because of the non-randomisation of treatment. Our work introduces a novel, outcome-guided method for identifying treatment response subgroups in observational studies. Our approach assigns each patient to a subgroup associated with two time-to-event distributions: one under treatment and one under control regime. It hence positions itself in between individualised and average treatment effect estimation. The assumptions of our model result in a simple correction of the statistical bias from treatment non-randomisation through inverse propensity weighting. In experiments, our approach significantly outperforms the current state-of-the-art method for outcome-guided subgroup analysis in both randomised and observational treatment regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03463v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Jeanselme, Chang Ho Yoon, Fabian Falck, Brian Tom, Jessica Barrett</dc:creator>
    </item>
    <item>
      <title>Principal component analysis for max-stable distributions</title>
      <link>https://arxiv.org/abs/2408.10650</link>
      <description>arXiv:2408.10650v2 Announce Type: replace 
Abstract: Principal component analysis (PCA) is one of the most popular dimension reduction techniques in statistics and is especially powerful when a multivariate distribution is concentrated near a lower-dimensional subspace. Multivariate extreme value distributions have turned out to provide challenges for the application of PCA since their constraint support impedes the detection of lower-dimensional structures and heavy-tails can imply that second moments do not exist, thereby preventing the application of classical variance-based techniques for PCA. We adapt PCA to max-stable distributions using a regression setting and employ max-linear maps to project the random vector to a lower-dimensional space while preserving max-stability. We also provide a characterization of those distributions which allow for a perfect reconstruction from the lower-dimensional representation. Finally, we demonstrate how an optimal projection matrix can be consistently estimated and show viability in practice with a simulation study and application to a benchmark dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10650v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Reinbott, Anja Jan{\ss}en</dc:creator>
    </item>
    <item>
      <title>Incremental effects for continuous exposures</title>
      <link>https://arxiv.org/abs/2409.11967</link>
      <description>arXiv:2409.11967v2 Announce Type: replace 
Abstract: Causal inference problems often involve continuous treatments, such as dose, duration, or frequency. However, identifying and estimating standard dose-response estimands requires that everyone has some chance of receiving any level of the exposure (i.e., positivity). To avoid this assumption, we consider stochastic interventions based on exponentially tilting the treatment distribution by some parameter $\delta$ (i.e. an incremental effect); this increases or decreases the likelihood a unit receives a given treatment level. We derive the efficient influence function and semiparametric efficiency bound for these incremental effects under continuous exposures. We then show estimation depends on the size of the tilt, as measured by $\delta$. In particular, we derive new minimax lower bounds illustrating how the best possible root mean squared error scales with an effective sample size of $n / \delta$, instead of $n$. Further, we establish new convergence rates and bounds on the bias of double machine learning-style estimators. Our novel analysis gives a better dependence on $\delta$ compared to standard analyses by using mixed supremum and $L_2$ norms. Finally, we show that taking $\delta \to \infty$ gives a new estimator of the dose-response curve at the edge of the support, and give a detailed study of convergence rates in this regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11967v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Schindl, Shuying Shen, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>Priors for Reducing Asymptotic Bias of the Posterior Mean</title>
      <link>https://arxiv.org/abs/2409.19673</link>
      <description>arXiv:2409.19673v3 Announce Type: replace 
Abstract: It is shown that the first-order term of the asymptotic bias of the posterior mean is removed by a suitable choice of a prior density. In regular statistical models including exponential families, and linear and logistic regression models, such a prior is given by the squared Jeffreys prior. We also explain the relationship between the proposed prior distribution, the moment matching prior, and the prior distribution that reduces the bias term of the posterior mode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19673v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miyata Yoichi, Yanagimoto Takemi</dc:creator>
    </item>
    <item>
      <title>Models for spatiotemporal data with some missing locations and application to emergency calls models calibration</title>
      <link>https://arxiv.org/abs/2410.11103</link>
      <description>arXiv:2410.11103v2 Announce Type: replace 
Abstract: We consider two classes of models for spatiotemporal data: one without covariates and one with covariates. If $\mathcal{T}$ is a partition of time and $\mathcal{I}$ a partition of the studied area into zones and if $\mathcal{C}$ is the set of arrival types, we assume that the process of arrivals for time interval $t \in \mathcal{T}$, zone $i \in \mathcal{I}$, and arrival type $c \in \mathcal{C}$ is Poisson with some intensity $\lambda_{c,i,t}$. We discussed the calibration and implementation of such models in \cite{laspatedpaper, laspatedmanual} with corresponding software LASPATED (Library for the Analysis of SPAtioTEmporal Discrete data) available on GitHub at https://github.com/vguigues/LASPATED. In this paper, we discuss the extension of these models when some of the locations are missing in the historical data. We propose three models to deal with missing locations and implemented them both in Matlab and C++. The corresponding code is available on GitHub as an extension of LASPATED at https://github.com/vguigues/LASPATED/Missing_Data. We tested our implementation using the process of emergency calls to an Emergency Health Service where many calls come with missing locations and show the importance and benefit of using models that consider missing locations, rather than discarding the calls with missing locations for the calibration of statistical models for such calls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11103v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Guigues, Anton Kleywegt, Victor Hugo Nascimento, Lucas Lucas Rafael de Andrade</dc:creator>
    </item>
    <item>
      <title>Sparse Causal Effect Estimation using Two-Sample Summary Statistics in the Presence of Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2410.12300</link>
      <description>arXiv:2410.12300v2 Announce Type: replace 
Abstract: Observational genome-wide association studies are now widely used for causal inference in genetic epidemiology. To maintain privacy, such data is often only publicly available as summary statistics, and often studies for the endogenous covariates and the outcome are available separately. This has necessitated methods tailored to two-sample summary statistics. Current state-of-the-art methods modify linear instrumental variable (IV) regression -- with genetic variants as instruments -- to account for unmeasured confounding. However, since the endogenous covariates can be high dimensional, standard IV assumptions are generally insufficient to identify all causal effects simultaneously. We ensure identifiability by assuming the causal effects are sparse and propose a sparse causal effect two-sample IV estimator, spaceTSIV, adapting the spaceIV estimator by Pfister and Peters (2022) for two-sample summary statistics. We provide two methods, based on L0- and L1-penalization, respectively. We prove identifiability of the sparse causal effects in the two-sample setting and consistency of spaceTSIV. The performance of spaceTSIV is compared with existing two-sample IV methods in simulations. Finally, we showcase our methods using real proteomic and gene-expression data for drug-target discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12300v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shimeng Huang, Niklas Pfister, Jack Bowden</dc:creator>
    </item>
    <item>
      <title>Inference for Low-rank Models without Estimating the Rank</title>
      <link>https://arxiv.org/abs/2311.16440</link>
      <description>arXiv:2311.16440v2 Announce Type: replace-cross 
Abstract: This paper studies the inference about linear functionals of high-dimensional low-rank matrices. While most existing inference methods would require consistent estimation of the true rank, our procedure is robust to rank misspecification, making it a promising approach in applications where rank estimation can be unreliable. We estimate the low-rank spaces using pre-specified weighting matrices, known as diversified projections. A novel statistical insight is that, unlike the usual statistical wisdom that overfitting mainly introduces additional variances, the over-estimated low-rank space also gives rise to a non-negligible bias due to an implicit ridge-type regularization. We develop a new inference procedure and show that the central limit theorem holds as long as the pre-specified rank is no smaller than the true rank. In one of our applications, we study multiple testing with incomplete data in the presence of confounding factors and show that our method remains valid as long as the number of controlled confounding factors is at least as large as the true number, even when no confounding factors are present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16440v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jungjun Choi, Hyukjun Kwon, Yuan Liao</dc:creator>
    </item>
    <item>
      <title>Ensemble Kalman filter meets model predictive control in chaotic systems</title>
      <link>https://arxiv.org/abs/2403.06371</link>
      <description>arXiv:2403.06371v3 Announce Type: replace-cross 
Abstract: Although data assimilation originates from control theory, the relationship between modern data assimilation methods in geoscience and model predictive control has not been extensively explored. In the present paper, I discuss that the modern data assimilation methods in geoscience and model predictive control essentially minimize the similar quadratic cost functions. Inspired by this similarity, I propose a new ensemble Kalman filter (EnKF)-based method for controlling spatio-temporally chaotic systems, which can be applied to high-dimensional and nonlinear Earth systems. In this method, the reference vector, which serves as the control target, is assimilated into the state space as a pseudo-observation by ensemble Kalman smoother to obtain the appropriate perturbation to be added to a system. A proof-of-concept experiment using the Lorenz 63 model is presented. The system is constrained in one wing of the butterfly attractor without tipping to the other side by reasonably small control perturbations which are comparable with previous works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06371v3</guid>
      <category>physics.geo-ph</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yohei Sawada</dc:creator>
    </item>
    <item>
      <title>Contextual Linear Optimization with Bandit Feedback</title>
      <link>https://arxiv.org/abs/2405.16564</link>
      <description>arXiv:2405.16564v2 Announce Type: replace-cross 
Abstract: Contextual linear optimization (CLO) uses predictive contextual features to reduce uncertainty in random cost coefficients and thereby improve average-cost performance. An example is the stochastic shortest path problem with random edge costs (e.g., traffic) and contextual features (e.g., lagged traffic, weather). Existing work on CLO assumes the data has fully observed cost coefficient vectors, but in many applications, we can only see the realized cost of a historical decision, that is, just one projection of the random cost coefficient vector, to which we refer as bandit feedback. We study a class of offline learning algorithms for CLO with bandit feedback, which we term induced empirical risk minimization (IERM), where we fit a predictive model to directly optimize the downstream performance of the policy it induces. We show a fast-rate regret bound for IERM that allows for misspecified model classes and flexible choices of the optimization estimate, and we develop computationally tractable surrogate losses. A byproduct of our theory of independent interest is fast-rate regret bound for IERM with full feedback and misspecified policy class. We compare the performance of different modeling choices numerically using a stochastic shortest path example and provide practical insights from the empirical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16564v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichun Hu, Nathan Kallus, Xiaojie Mao, Yanchen Wu</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximation and Output Analysis for High-Dimensional MCMC</title>
      <link>https://arxiv.org/abs/2407.05492</link>
      <description>arXiv:2407.05492v2 Announce Type: replace-cross 
Abstract: The widespread use of Markov Chain Monte Carlo (MCMC) methods for high-dimensional applications has motivated research into the scalability of these algorithms with respect to the dimension of the problem. Despite this, numerous problems concerning output analysis in high-dimensional settings have remained unaddressed. We present novel quantitative Gaussian approximation results for a broad range of MCMC algorithms. Notably, we analyse the dependency of the obtained approximation errors on the dimension of both the target distribution and the feature space. We demonstrate how these Gaussian approximations can be applied in output analysis. This includes determining the simulation effort required to guarantee Markov chain central limit theorems and consistent estimation of the variance and effective sample size in high-dimensional settings. We give quantitative convergence bounds for termination criteria and show that the termination time of a wide class of MCMC algorithms scales polynomially in dimension while ensuring a desired level of precision. Our results offer guidance to practitioners for obtaining appropriate standard errors and deciding the minimum simulation effort of MCMC algorithms in both multivariate and high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05492v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ardjen Pengel, Jun Yang, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Granger Causality in Extremes</title>
      <link>https://arxiv.org/abs/2407.09632</link>
      <description>arXiv:2407.09632v2 Announce Type: replace-cross 
Abstract: We introduce a rigorous mathematical framework for Granger causality in extremes, designed to identify causal links from extreme events in time series. Granger causality plays a pivotal role in uncovering directional relationships among time-varying variables. While this notion gains heightened importance during extreme and highly volatile periods, state-of-the-art methods primarily focus on causality within the body of the distribution, often overlooking causal mechanisms that manifest only during extreme events. Our framework is designed to infer causality mainly from extreme events by leveraging the causal tail coefficient. We establish equivalences between causality in extremes and other causal concepts, including (classical) Granger causality, Sims causality, and structural causality. We prove other key properties of Granger causality in extremes and show that the framework is especially helpful under the presence of hidden confounders. We also propose a novel inference method for detecting the presence of Granger causality in extremes from data. Our method is model-free, can handle non-linear and high-dimensional time series, outperforms current state-of-the-art methods in all considered setups, both in performance and speed, and was found to uncover coherent effects when applied to financial and extreme weather observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09632v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juraj Bodik, Olivier C. Pasche</dc:creator>
    </item>
    <item>
      <title>Multi-marginal Schr\"odinger Bridges with Iterative Reference Refinement</title>
      <link>https://arxiv.org/abs/2408.06277</link>
      <description>arXiv:2408.06277v3 Announce Type: replace-cross 
Abstract: Practitioners often aim to infer an unobserved population trajectory using sample snapshots at multiple time points. E.g., given single-cell sequencing data, scientists would like to learn how gene expression changes over a cell's life cycle. But sequencing any cell destroys that cell. So we can access data for any particular cell only at a single time point, but we have data across many cells. The deep learning community has recently explored using Schr\"odinger bridges (SBs) and their extensions in similar settings. However, existing methods either (1) interpolate between just two time points or (2) require a single fixed reference dynamic (often set to Brownian motion within SBs). But learning piecewise from adjacent time points can fail to capture long-term dependencies. And practitioners are typically able to specify a model family for the reference dynamic but not the exact values of the parameters within it. So we propose a new method that (1) learns the unobserved trajectories from sample snapshots across multiple time points and (2) requires specification only of a family of reference dynamics, not a single fixed one. We demonstrate the advantages of our method on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06277v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunyi Shen, Renato Berlinghieri, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Testing Causal Explanations: A Case Study for Understanding the Effect of Interventions on Chronic Kidney Disease</title>
      <link>https://arxiv.org/abs/2410.12047</link>
      <description>arXiv:2410.12047v2 Announce Type: replace-cross 
Abstract: Randomized controlled trials (RCTs) are the standard for evaluating the effectiveness of clinical interventions. To address the limitations of RCTs on real-world populations, we developed a methodology that uses a large observational electronic health record (EHR) dataset. Principles of regression discontinuity (rd) were used to derive randomized data subsets to test expert-driven interventions using dynamic Bayesian Networks (DBNs) do-operations. This combined method was applied to a chronic kidney disease (CKD) cohort of more than two million individuals and used to understand the associational and causal relationships of CKD variables with respect to a surrogate outcome of &gt;=40% decline in estimated glomerular filtration rate (eGFR). The associational and causal analyses depicted similar findings across DBNs from two independent healthcare systems. The associational analysis showed that the most influential variables were eGFR, urine albumin-to-creatinine ratio, and pulse pressure, whereas the causal analysis showed eGFR as the most influential variable, followed by modifiable factors such as medications that may impact kidney function over time. This methodology demonstrates how real-world EHR data can be used to provide population-level insights to inform improved healthcare delivery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12047v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panayiotis Petousis (on behalf of CURE-CKD), David Gordon (on behalf of CURE-CKD), Susanne B. Nicholas (on behalf of CURE-CKD), Alex A. T. Bui (on behalf of CURE-CKD)</dc:creator>
    </item>
  </channel>
</rss>
