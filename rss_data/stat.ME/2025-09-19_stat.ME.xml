<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Sep 2025 04:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Environmental Risk Assessment via Nonhomogeneous Hidden Semi-Markov Models with Penalized Vector Auto-Regression</title>
      <link>https://arxiv.org/abs/2509.14387</link>
      <description>arXiv:2509.14387v1 Announce Type: new 
Abstract: Motivated by the study of pollution trends in the city of Bergen, we introduce a flexible statistical framework for modeling multivariate air pollution data via a nonhomogeneous Hidden Semi-Markov Vector Auto-Regression. The hidden process captures unobserved environmental conditions, while the vector autoregressive structure accounts for temporal autocorrelation and cross-pollutant dependencies. The model further allows time-varying environmental conditions to influence both the average levels of pollutant concentrations and the duration of different transient states. Parameters are estimated via maximum likelihood using a tailored Expectation-Maximization (EM) algorithm, integrated with state-specific $\ell_1$ regularization to control overfitting and automatically select relevant temporal lags. The proposal is tested on simulated data under different scenarios and then applied to daily concentrations of nitrogens and particulate matter recorded in a urban area. Environmental risk is assessed by a Shapley value-based decomposition that attribute marginal risk contributions. This approach offers a comprehensive framework for multivariate environmental risk modeling, enabling better identification of high-pollution episodes and informing policy interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14387v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Mingione, Pierfrancesco Alaimo Di Loro, Francesco Lagona, Antonello Maruotti</dc:creator>
    </item>
    <item>
      <title>Rate doubly robust estimation for weighted average treatment effects</title>
      <link>https://arxiv.org/abs/2509.14502</link>
      <description>arXiv:2509.14502v1 Announce Type: new 
Abstract: The weighted average treatment effect (WATE) defines a versatile class of causal estimands for populations characterized by propensity score weights, including the average treatment effect (ATE), treatment effect on the treated (ATT), on controls (ATC), and for the overlap population (ATO). WATE has broad applicability in social and medical research, as many datasets from these fields align with its framework. However, the literature lacks a systematic investigation into the robustness and efficiency conditions for WATE estimation. Although doubly robust (DR) estimators are well-studied for ATE, their applicability to other WATEs remains uncertain. This paper investigates whether widely used WATEs admit DR or rate doubly robust (RDR) estimators and assesses the role of nuisance function accuracy, particularly with machine learning. Using semiparametric efficient influence function (EIF) theory and double/debiased machine learning (DML), we propose three RDR estimators under specific rate and regularity conditions and evaluate their performance via Monte Carlo simulations. Applications to NHANES data on smoking and blood lead levels, and SIPP data on 401(k) eligibility, demonstrate the methods' practical relevance in medical and social sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14502v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Wang, Yi Liu, Shu Yang</dc:creator>
    </item>
    <item>
      <title>A Review of Statistical Methods for Handling Nonignorable Missing Data using Instrument Approach</title>
      <link>https://arxiv.org/abs/2509.14520</link>
      <description>arXiv:2509.14520v1 Announce Type: new 
Abstract: Nonignorable missing data, where the probability of missingness depends on unobserved values, presents a significant challenge in statistical analysis. Traditional methods often rely on strong parametric assumptions that are difficult to verify and may lead to biased estimates if misspecified. Recent advances have introduced the concept of a nonresponse instrument or shadow variable as a powerful tool to enhance model identifiability without requiring full parametric specification. This paper provides a comprehensive review of statistical methods that leverage instrumental variables to address nonignorable missingness, focusing on two predominant semiparametric frameworks: one with a parametric data model and a nonparametric propensity model, and the other with a parametric propensity model and a nonparametric data model. We discuss key developments, methodological insights, and remaining challenges in this rapidly evolving field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14520v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujie Zhao</dc:creator>
    </item>
    <item>
      <title>Semiparametric Learning from Open-Set Label Shift Data</title>
      <link>https://arxiv.org/abs/2509.14522</link>
      <description>arXiv:2509.14522v1 Announce Type: new 
Abstract: We study the open-set label shift problem, where the test data may include a novel class absent from training. This setting is challenging because both the class proportions and the distribution of the novel class are not identifiable without extra assumptions. Existing approaches often rely on restrictive separability conditions, prior knowledge, or computationally infeasible procedures, and some may lack theoretical guarantees. We propose a semiparametric density ratio model framework that ensures identifiability while allowing overlap between novel and known classes. Within this framework, we develop maximum empirical likelihood estimators and confidence intervals for class proportions, establish their asymptotic validity, and design a stable Expectation-Maximization algorithm for computation. We further construct an approximately optimal classifier based on posterior probabilities with theoretical guarantees. Simulations and a real data application confirm that our methods improve both estimation accuracy and classification performance compared with existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14522v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyan Liu, Yukun Liu, Qinglong Tian, Pengfei Li, Jing Qin</dc:creator>
    </item>
    <item>
      <title>Randomization inference for stepped-wedge designs with noncompliance with application to a palliative care pragmatic trial</title>
      <link>https://arxiv.org/abs/2509.14598</link>
      <description>arXiv:2509.14598v1 Announce Type: new 
Abstract: While palliative care is increasingly commonly delivered to hospitalized patients with serious illnesses, few studies have estimated its causal effects. Courtright et al. (2016) adopted a cluster-randomized stepped-wedge design to assess the effect of palliative care on a patient-centered outcome. The randomized intervention was a nudge to administer palliative care but did not guarantee receipt of palliative care, resulting in noncompliance (compliance rate ~30%). A subsequent analysis using methods suited for standard trial designs produced statistically anomalous results, as an intention-to-treat analysis found no effect while an instrumental variable analysis did (Courtright et al., 2024). This highlights the need for a more principled approach to address noncompliance in stepped-wedge designs. We provide a formal causal inference framework for the stepped-wedge design with noncompliance by introducing a relevant causal estimand and corresponding estimators and inferential procedures. Through simulation, we compare an array of estimators across a range of stepped-wedge designs and provide practical guidance in choosing an analysis method. Finally, we apply our recommended methods to reanalyze the trial of Courtright et al. (2016), producing point estimates suggesting a larger effect than the original analysis of (Courtright et al., 2024), but intervals that did not reach statistical significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14598v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Zhang, Zhe Chen, Katherine R. Courtright, Scott D. Halpern, Michael O. Harhay, Dylan S. Small, Fan Li</dc:creator>
    </item>
    <item>
      <title>Alternative Likelihood Approximations for High-Dimensional Intervals for Lasso</title>
      <link>https://arxiv.org/abs/2509.14971</link>
      <description>arXiv:2509.14971v1 Announce Type: new 
Abstract: Classical frequentist approaches to inference for the lasso emphasize exact coverage for each feature, which requires debiasing and severs the connection between confidence intervals and the original lasso estimates. To address this, in earlier work we introduced the idea of average coverage, allowing for biased intervals that align with the lasso point estimates, and proposed the Relaxed Lasso Posterior (RL-P) intervals, which leverage the Bayesian interpretation of the lasso penalty as a Laplace prior together with a Normal likelihood conditional on the selected features. While RL-P achieves approximate average coverage, its intervals need not contain the lasso estimates. In this work, we propose alternative constructions based on different likelihood approximations to the full high-dimensional likelihood, yielding intervals that remain centered on the lasso estimates while still achieving average coverage. Our results continue to demonstrate that intentionally biased intervals provide a principled and practically useful framework for inference in high-dimensional regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14971v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Logan Harris, Patrick Breheny</dc:creator>
    </item>
    <item>
      <title>Modelling peaks over thresholds in panel data: a two-level grouped panel generalized Pareto regression</title>
      <link>https://arxiv.org/abs/2509.15023</link>
      <description>arXiv:2509.15023v1 Announce Type: new 
Abstract: Panel data arise in a wide range of application areas, and developing modelling methods for extreme values under such a setup is essential for reliable risk assessment and management. When choosing to model the marginal distributions of univariate extremes, one may wish to balance the flexibility in capturing the heterogeneity among margins and the efficiency of estimation. This can be achieved through a combination of regression techniques and assuming a latent group structure based on parameter values, which needs to be estimated from data. Building on an existing method, we propose a two-level grouped panel generalized Pareto regression framework, which models peaks over high thresholds in panel data. While retaining the wide applicability of the original modelling strategy, which is largely domain-knowledge-free, our new methodology uses the information of extreme events more exhaustively and allows the exploration of a broader model space, where parsimony and good model fit can be achieved simultaneously. We also address several estimation challenges associated with high-dimensional optimization and group structure identification. The finite-sample performance of our methodology is carefully evaluated through simulation studies. With an application to the summer river flow data from 31 stations in the upper Danube basin, we show that our methodology can effectively improve estimation efficiency while discovering patterns in the tail behavior that can be omitted by domain-knowledge-based regionalization and the existing method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15023v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zefan Liu, Natalia Nolde</dc:creator>
    </item>
    <item>
      <title>Sequential sample size calculations and learning curves safeguard the robust development of a clinical prediction model for individuals</title>
      <link>https://arxiv.org/abs/2509.15134</link>
      <description>arXiv:2509.15134v1 Announce Type: new 
Abstract: When prospectively developing a new clinical prediction model (CPM), fixed sample size calculations are typically conducted before data collection based on sensible assumptions. But if the assumptions are inaccurate the actual sample size required to develop a reliable model may be very different. To safeguard against this, adaptive sample size approaches have been proposed, based on sequential evaluation of a models predictive performance. Aim: illustrate and extend sequential sample size calculations for CPM development by (i) proposing stopping rules based on minimising uncertainty (instability) and misclassification of individual-level predictions, and (ii) showcasing how it safeguards against inaccurate fixed sample size calculations. Using the sequential approach repeats the pre-defined model development strategy every time a chosen number (e.g., 100) of participants are recruited and adequately followed up. At each stage, CPM performance is evaluated using bootstrapping, leading to prediction and classification stability statistics and plots, alongside optimism-adjusted measures of calibration and discrimination. Our approach is illustrated for development of acute kidney injury using logistic regression CPMs. The fixed sample size calculation, based on perceived sensible assumptions suggests recruiting 342 patients to minimise overfitting; however, the sequential approach reveals that a much larger sample size of 1100 is required to minimise overfitting (targeting population-level stability). If the stopping rule criteria also target small uncertainty and misclassification probability of individual predictions, the sequential approach suggests an even larger sample size (n=1800). Our sequential sample size approach allows users to dynamically monitor individual-level prediction and classification instability and safeguard against using inaccurate assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15134v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amardeep Legha, Joie Ensor, Rebecca Whittle, Lucinda Archer, Ben Van Calster, Evangelia Christodoulou, Kym I. E. Snell, Mohsen Sadatsafavi, Gary S. Collins, Richard D. Riley</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for spatio-temporal hidden Markov models using the exchange algorithm</title>
      <link>https://arxiv.org/abs/2509.15164</link>
      <description>arXiv:2509.15164v1 Announce Type: new 
Abstract: Spatio-temporal hidden Markov models are extremely difficult to estimate because their latent joint distributions are available only in trivial cases. In the estimation phase, these latent distributions are usually substituted with pseudo-distributions, which could affect the estimation results, in particular in the presence of strong dependence between the latent variables. In this work, we propose a spatio-temporal hidden Markov model where the latent process is an extension of the autologistic model. We show how inference can be carried out in a Bayesian framework using an approximate exchange algorithm, which circumvents the impractical calculations of the normalizing constants that arise in the model. Our proposed method leads to a Markov chain Monte Carlo sampler that targets the correct posterior distribution of the model and not a pseudo-posterior. In addition, we develop a new initialization approach for the approximate exchange method, reducing the computational time of the algorithm. An extensive simulation study shows that the approximate exchange algorithm generally outperforms the pseudo-distribution approach, yielding more accurate parameter estimates. Finally, the proposed methodology is applied to a real-world case study analyzing rainfall levels across Italian regions over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15164v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Tancini, Riccardo Rastelli, Francesco Bartolucci</dc:creator>
    </item>
    <item>
      <title>Consistent causal discovery with equal error variances: a least-squares perspective</title>
      <link>https://arxiv.org/abs/2509.15197</link>
      <description>arXiv:2509.15197v1 Announce Type: cross 
Abstract: We consider the problem of recovering the true causal structure among a set of variables, generated by a linear acyclic structural equation model (SEM) with the error terms being independent and having equal variances. It is well-known that the true underlying directed acyclic graph (DAG) encoding the causal structure is uniquely identifiable under this assumption. In this work, we establish that the sum of minimum expected squared errors for every variable, while predicted by the best linear combination of its parent variables, is minimised if and only if the causal structure is represented by any supergraph of the true DAG. This property is further utilised to design a Bayesian DAG selection method that recovers the true graph consistently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15197v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anamitra Chaudhuri, Yang Ni, Anirban Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Post-selection inference for causal effects after causal discovery</title>
      <link>https://arxiv.org/abs/2405.06763</link>
      <description>arXiv:2405.06763v4 Announce Type: replace 
Abstract: Algorithms for constraint-based causal discovery select graphical causal models among a space of possible candidates (e.g., all directed acyclic graphs) by executing a sequence of conditional independence tests. These may be used to inform the estimation of causal effects (e.g., average treatment effects) when there is uncertainty about which covariates ought to be adjusted for, or which variables act as confounders versus mediators. However, naively using the data twice, for model selection and estimation, would lead to invalid confidence intervals. Moreover, if the selected graph is incorrect, the inferential claims may apply to a selected functional that is distinct from the actual causal effect. We propose an approach to post-selection inference that is based on a resampling and screening procedure, which essentially performs causal discovery multiple times with randomly varying intermediate test statistics. Then, an estimate of the target causal effect and corresponding confidence sets are constructed from a union of individual graph-based estimates and intervals. We show that this construction has asymptotically correct coverage for the true causal effect parameter. Importantly, the guarantee holds for a fixed population-level effect, not a data-dependent or selection-dependent quantity. Most of our exposition focuses on the PC-algorithm for learning directed acyclic graphs and the multivariate Gaussian case for simplicity, but the approach is general and modular, so it may be used with other conditional independence based discovery algorithms and distributional families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06763v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting-Hsuan Chang, Zijian Guo, Daniel Malinsky</dc:creator>
    </item>
    <item>
      <title>A novel framework for quantifying nominal outlyingness</title>
      <link>https://arxiv.org/abs/2408.07463</link>
      <description>arXiv:2408.07463v3 Announce Type: replace 
Abstract: Outlier detection is an important data mining tool that becomes particularly challenging when dealing with nominal data. First and foremost, flagging observations as outlying requires a well-defined notion of nominal outlyingness. This paper presents a definition of nominal outlyingness and introduces a general framework for quantifying outlyingness of nominal data. The proposed framework makes use of ideas from the association rule mining literature and can be used for calculating scores that indicate how outlying a nominal observation is. Methods for determining the involved hyperparameter values are presented and the concepts of variable contributions and outlyingness depth are introduced, in an attempt to enhance interpretability of the results. The proposed framework is evaluated on both synthetic and publicly available data sets, demonstrating comparable performance to state-of-the-art frequent pattern mining algorithms and even outperforming them in certain cases. The ideas presented can serve as a tool for assessing the degree to which an observation differs from the rest of the data, under the assumption of sequences of nominal levels having been generated from a Multinomial distribution with varying event probabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07463v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Efthymios Costa, Ioanna Papatsouma</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Sensitivity Analysis of Multiple Test Procedures Under Dependence</title>
      <link>https://arxiv.org/abs/2410.08080</link>
      <description>arXiv:2410.08080v5 Announce Type: replace 
Abstract: This article introduces a sensitivity analysis method for Multiple Testing Procedures (MTPs) using marginal $p$-values. The method is based on the Dirichlet process (DP) prior distribution, specified to support the entire space of MTPs, where each MTP controls either the family-wise error rate (FWER) or the false discovery rate (FDR) under arbitrary dependence between $p$-values. This DP MTP sensitivity analysis method provides uncertainty quantification for MTPs, by accounting for uncertainty in the selection of such MTPs and their respective threshold decisions regarding which number of smallest $p$-values are significant discoveries, from a given set of null hypothesis tested, while measuring each $p$-value's probability of significance over the DP prior predictive distribution of this space of all MTPs, and reducing the possible conservativeness of using only one such MTP for multiple testing. The DP MTP sensitivity analysis method is illustrated through the analysis of over twenty-eight thousand $p$-values arising from hypothesis tests performed on a 2022 dataset of a representative sample of three million U.S. high school students observed on 239 variables. They include tests which, respectively, relate variables about the disruption caused by school closures during the COVID-19 pandemic, with various mathematical cognition, academic achievement, and student background variables. R software code for the DP MTP sensitivity analysis method is provided in the Code and Data Supplement of this article (available upon request).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08080v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Karabatsos</dc:creator>
    </item>
    <item>
      <title>High-dimensional Statistical Inference and Variable Selection Using Sufficient Dimension Association</title>
      <link>https://arxiv.org/abs/2410.19031</link>
      <description>arXiv:2410.19031v2 Announce Type: replace 
Abstract: Simultaneous variable selection and statistical inference is challenging in high-dimensional data analysis. Most existing post-selection inference methods require explicitly specified regression models, which are often linear, as well as sparsity in the regression model. The performance of such procedures can be poor under either misspecified nonlinear models or a violation of the sparsity assumption. In this paper, we propose a sufficient dimension association (SDA) technique that measures the association between each predictor and the response variable conditioning on other predictors in the high-dimensional setting. Our proposed SDA method requires neither a specific form of regression model nor sparsity in the regression. Alternatively, our method assumes normalized or Gaussian-distributed predictors with a Markov blanket property. We propose an estimator for the SDA and prove asymptotic properties for the estimator. We construct three types of test statistics for the SDA and propose a multiple testing procedure to control the false discovery rate. Extensive simulation studies have been conducted to show the validity and superiority of our SDA method. Gene expression data from the Alzheimer Disease Neuroimaging Initiative are used to demonstrate a real application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19031v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shangyuan Ye, Shauna Rakshe, Ye Liang</dc:creator>
    </item>
    <item>
      <title>The Principle of Redundant Reflection</title>
      <link>https://arxiv.org/abs/2503.21719</link>
      <description>arXiv:2503.21719v3 Announce Type: replace 
Abstract: The fact that redundant information does not update a rational belief implies that rational beliefs are updated using Bayes rule. In the framework of Hild (1998a), this is true under mild conditions for discrete, continuous, and arbitrary measure spaces. We prove this result and illustrate it with two examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21719v3</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Metodiev, Maarten Marsman, Lourens Waldorp, Quentin F. Gronau, Eric-Jan Wagenmakers</dc:creator>
    </item>
    <item>
      <title>A new look at fiducial inference</title>
      <link>https://arxiv.org/abs/2504.19172</link>
      <description>arXiv:2504.19172v2 Announce Type: replace 
Abstract: Since the idea of fiducial inference was put forward by Fisher, researchers have been attempting to place it within a rigorous and well motivated framework. It is fair to say that a general definition has remained elusive. In this paper we start with a representation of Bayesian posterior distributions provided by Doob that relies on martingales. This is explicit in defining how a true parameter value should depend on a random sample and hence an approach to "inverse probability" as originally conceived by Fisher. Taking this as our cue, we introduce a definition of fiducial inference that can be regarded as general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19172v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pier Giovanni Bissiri, Chris Holmes, Stephen Walker</dc:creator>
    </item>
    <item>
      <title>Identification and estimation of vaccine effectiveness in the test-negative design under equi-confounding</title>
      <link>https://arxiv.org/abs/2504.20360</link>
      <description>arXiv:2504.20360v5 Announce Type: replace 
Abstract: The test-negative design (TND) is widely used to evaluate vaccine effectiveness in real-world settings. In a TND study, individuals with similar symptoms who seek care are tested, and effectiveness is estimated by comparing vaccination histories of test-positive cases and test-negative controls. The TND is often justified on the grounds that it reduces confounding due to unmeasured health-seeking behavior, although this has not been formally described using potential outcomes. At the same time, concerns persist that conditioning on test receipt can introduce selection bias. We provide a formal justification of the TND under an assumption of odds ratio equi-confounding, where unmeasured confounders affect test-positive and test-negative individuals equivalently on the odds ratio scale. Health-seeking behavior is one plausible example. We also show that these results hold under the outcome-dependent sampling used in TNDs. We discuss the design implications of the equi-confounding assumption and provide alternative estimators for the marginal risk ratio among the vaccinated under equi-confounding, including outcome modeling and inverse probability weighting estimators as well as a semiparametric estimator that is doubly robust. When equi-confounding does not hold, we suggest a straightforward sensitivity analysis that parameterizes the magnitude of the deviation on the odds ratio scale. A simulation study evaluates the empirical performance of our proposed estimators under a wide range of scenarios. Finally, we discuss broader uses of test-negative outcomes to de-bias cohort studies in which testing is triggered by symptoms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20360v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Christopher B. Boyer, Kendrick Qijun Li, Xu Shi, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Model-robust standardization in cluster-randomized trials</title>
      <link>https://arxiv.org/abs/2505.19336</link>
      <description>arXiv:2505.19336v2 Announce Type: replace 
Abstract: In cluster-randomized trials, generalized linear mixed models and generalized estimating equations have conventionally been the default analytic methods for estimating the average treatment effect as routine practice. However, recent studies have demonstrated that their treatment effect coefficient estimators may correspond to ambiguous estimands when the models are misspecified or when there exists informative cluster sizes. In this article, we present a unified approach that standardizes output from a given regression model to ensure estimand-aligned inference for the treatment effect parameters in cluster-randomized trials. We introduce estimators for both the cluster-average and the individual-average treatment effects (marginal estimands) that are always consistent regardless of whether the specified working regression models align with the unknown data generating process. We further explore the use of a deletion-based jackknife variance estimator for inference. The development of our approach also motivates a natural test for informative cluster size. Extensive simulation experiments are designed to demonstrate the advantage of the proposed estimators under a variety of scenarios. The proposed model-robust standardization methods are implemented in the MRStdCRT R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19336v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fan Li, Jiaqi Tong, Xi Fang, Chao Cheng, Brennan C. Kahan, Bingkai Wang</dc:creator>
    </item>
    <item>
      <title>The Why and How of Convex Clustering</title>
      <link>https://arxiv.org/abs/2507.09077</link>
      <description>arXiv:2507.09077v2 Announce Type: replace 
Abstract: This survey reviews a clustering method based on solving a convex optimization problem. Despite the plethora of existing clustering methods, convex clustering has several uncommon features that distinguish it from prior art. The optimization problem is free of spurious local minima, and its unique global minimizer is stable with respect to all its inputs, including the data, a tuning parameter, and weight hyperparameters. Its single tuning parameter controls the number of clusters and can be chosen using standard techniques from penalized regression. We give intuition into the behavior and theory for convex clustering as well as practical guidance. We highlight important algorithms and discuss how their computational costs scale with the problem size. Finally, we highlight the breadth of its uses and flexibility to be combined and integrated with other inferential methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09077v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Eric C. Chi, Aaron J. Molstad, Zheming Gao, Jocelyn T. Chi</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Multi-order Tensor Factor Models</title>
      <link>https://arxiv.org/abs/2508.13418</link>
      <description>arXiv:2508.13418v2 Announce Type: replace 
Abstract: We propose a novel framework in high-dimensional factor models to simultaneously analyse multiple tensor time series, each with potentially different tensor orders and dimensionality. The connection between different tensor time series is through their global factors that are correlated to each other. A salient feature of our model is that when all tensor time series have the same order, it can be regarded as an extension of multilevel factor models from vectors to general tensors. Under very mild conditions, we separate the global and local components in the proposed model. Parameter estimation is thoroughly discussed, including a consistent factor number estimator. With strong correlation between global factors and noise allowed, we derive the rates of convergence of our estimators, which can be more superior than those of existing methods for multilevel factor models. We also develop estimators that are more computationally efficient, with rates of convergence spelt out. Extensive experiments are performed under various settings, corroborating with the pronounced theoretical results. As a real application example, we analyse a set of taxi data to study the traffic flow between Times Squares and its neighbouring areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13418v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen</dc:creator>
    </item>
    <item>
      <title>Generalized Correlation Regression for Disentangling Dependence in Clustered Data</title>
      <link>https://arxiv.org/abs/2509.01774</link>
      <description>arXiv:2509.01774v2 Announce Type: replace 
Abstract: Clustered and longitudinal data are pervasive in scientific studies, from prenatal health programs to clinical trials and public health surveillance. Such data often involve non-Gaussian responses--including binary, categorical, and count outcomes--that exhibit complex correlation structures driven by multilevel clustering, covariates, over-dispersion, or zero inflation. Conventional approaches such as mixed-effects models and generalized estimating equations (GEEs) can capture some of these dependencies, but they are often too rigid or impose restrictive assumptions that limit interpretability and predictive performance.
  We investigate \emph{generalized correlation regression} (GCR), a unified framework that models correlations directly as functions of interpretable covariates while simultaneously estimating marginal means. By applying a generalized $z$-transformation, GCR guarantees valid correlation matrices, accommodates unbalanced cluster sizes, and flexibly incorporates covariates such as time, space, or group membership into the dependence structure. Through applications to modern prenatal care, a longitudinal toenail infection trial, and clustered health count data, we show that GCR not only achieves superior predictive performance over standard methods, but also reveals family-, community-, and individual-level drivers of dependence that are obscured under conventional modeling. These results demonstrate the broad applied value of GCR for analyzing binary, count, and categorical data in clustered and longitudinal settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01774v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yibo Wang, Chenlei Leng, Cheng Yong Tang</dc:creator>
    </item>
    <item>
      <title>System Reliability Estimation via Shrinkage</title>
      <link>https://arxiv.org/abs/2509.12420</link>
      <description>arXiv:2509.12420v2 Announce Type: replace 
Abstract: In a coherent reliability system composed of multiple components configured according to a specific structure function, the distribution of system time to failure, or system lifetime, is often of primary interest. Accurate estimation of system reliability is critical in a wide range of engineering and industrial applications, forming decisions in system design, maintenance planning, and risk assessment. The system lifetime distribution can be estimated directly using the observed system failure times. However, when component-level lifetime data is available, it can yield improved estimates of system reliability. In this work, we demonstrate that under nonparametric assumptions about the component time-to-failure distributions, traditional estimators such as the Product-Limit Estimator (PLE) can be further improved under specific loss functions. We propose a novel methodology that enhances the nonparametric system reliability estimation through a shrinkage transformation applied to component-level estimators. This shrinkage approach leads to improved efficiency in estimating system reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12420v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Beidi Qiang, Edsel Pena</dc:creator>
    </item>
    <item>
      <title>Nonlinear Causality in Time Series Networks: With Application to Motor Imagery vs Execution</title>
      <link>https://arxiv.org/abs/2409.10374</link>
      <description>arXiv:2409.10374v3 Announce Type: replace-cross 
Abstract: Causal interactions in time series networks can be dynamic and nonlinear, making it difficult to identify them using conventional linear causality estimations. We propose a novel approach, called Threshold Autoregressive Modeling for Causality (TAR4C), a causality detection approach built on threshold autoregressive (TAR) models, where a potential driver (cause variable) acts both as a predictor and as a trigger (switching threshold) that governs which autoregressive process the target (effect variable) follows. Threshold nonlinearity is conceptualized here to determine causality. The flow of the target is forced to transition between regimes with distinct dynamics when the driver exceeds a data-driven threshold in the past. We propose a two-stage inference procedure: Stage 1 tests for threshold connectivity (TC); Stage 2, conditional on a detected threshold effect, estimates threshold Granger causality (TGC). TAR4C is applied to a multichannel EEG dataset collected from a motor imagery and execution experiment. Delay-dependent directional interactions are observed among channels across different sites of the EEG map. The real-world application demonstrates the usefulness of the proposed approach for determining nonlinear causal connectivity in complex time-series networks, such as brain circuitry. The proposed model-based methodology extends to other complex networks of time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10374v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Compactly-supported nonstationary kernels for computing exact Gaussian processes on big data</title>
      <link>https://arxiv.org/abs/2411.05869</link>
      <description>arXiv:2411.05869v3 Announce Type: replace-cross 
Abstract: The Gaussian process (GP) is a widely used probabilistic machine learning method with implicit uncertainty characterization for stochastic function approximation, stochastic modeling, and analyzing real-world measurements of nonlinear processes. Traditional implementations of GPs involve stationary kernels (also termed covariance functions) that limit their flexibility, and exact methods for inference that prevent application to data sets with more than about ten thousand points. Modern approaches to address stationarity assumptions generally fail to accommodate large data sets, while all attempts to address scalability focus on approximating the Gaussian likelihood, which can involve subjectivity and lead to inaccuracies. In this work, we explicitly derive an alternative kernel that can discover and encode both sparsity and nonstationarity. We embed the kernel within a fully Bayesian GP model and leverage high-performance computing resources to enable the analysis of massive data sets. We demonstrate the favorable performance of our novel kernel relative to existing exact and approximate GP methods across a variety of synthetic data examples. Furthermore, we conduct space-time prediction based on more than one million measurements of daily maximum temperature and verify that our results outperform state-of-the-art methods in the Earth sciences. More broadly, having access to exact GPs that use ultra-scalable, sparsity-discovering, nonstationary kernels allows GP methods to truly compete with a wide variety of machine learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05869v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark D. Risser, Marcus M. Noack, Hengrui Luo, Ronald Pandolfi</dc:creator>
    </item>
    <item>
      <title>Statistical Methods in Generative AI</title>
      <link>https://arxiv.org/abs/2509.07054</link>
      <description>arXiv:2509.07054v2 Announce Type: replace-cross 
Abstract: Generative Artificial Intelligence is emerging as an important technology, promising to be transformative in many areas. At the same time, generative AI techniques are based on sampling from probabilistic models, and by default, they come with no guarantees about correctness, safety, fairness, or other properties. Statistical methods offer a promising potential approach to improve the reliability of generative AI techniques. In addition, statistical methods are also promising for improving the quality and efficiency of AI evaluation, as well as for designing interventions and experiments in AI. In this paper, we review some of the existing work on these topics, explaining both the general statistical techniques used, as well as their applications to generative AI. We also discuss limitations and potential future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07054v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>ALIGNS: Unlocking nomological networks in psychological measurement through a large language model</title>
      <link>https://arxiv.org/abs/2509.09723</link>
      <description>arXiv:2509.09723v2 Announce Type: replace-cross 
Abstract: Psychological measurement is critical to many disciplines. Despite advances in measurement, building nomological networks, theoretical maps of how concepts and measures relate to establish validity, remains a challenge 70 years after Cronbach and Meehl proposed them as fundamental to validation. This limitation has practical consequences: clinical trials may fail to detect treatment effects, and public policy may target the wrong outcomes. We introduce Analysis of Latent Indicators to Generate Nomological Structures (ALIGNS), a large language model-based system trained with validated questionnaire measures. ALIGNS provides three comprehensive nomological networks containing over 550,000 indicators across psychology, medicine, social policy, and other fields. This represents the first application of large language models to solve a foundational problem in measurement validation. We report classification accuracy tests used to develop the model, as well as three evaluations. In the first evaluation, the widely used NIH PROMIS anxiety and depression instruments are shown to converge into a single dimension of emotional distress. The second evaluation examines child temperament measures and identifies four potential dimensions not captured by current frameworks, and questions one existing dimension. The third evaluation, an applicability check, engages expert psychometricians who assess the system's importance, accessibility, and suitability. ALIGNS is freely available at nomologicalnetwork.org, complementing traditional validation methods with large-scale nomological analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09723v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai R. Larsen, Sen Yan, Roland M. Mueller, Lan Sang, Mikko R\"onkk\"o, Ravi Starzl, Donald Edmondson</dc:creator>
    </item>
  </channel>
</rss>
