<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CG updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CG</link>
    <description>cs.CG updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CG" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Apr 2024 04:00:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the orthogonal Gr\"unbaum partition problem in dimension three</title>
      <link>https://arxiv.org/abs/2404.01504</link>
      <description>arXiv:2404.01504v1 Announce Type: cross 
Abstract: Gr\"unbaum's equipartition problem asked if for any measure on $\mathbb{R}^d$ there are always $d$ hyperplanes which divide $\mathbb{R}^d$ into $2^d$ $\mu$-equal parts. This problem is known to have a positive answer for $d\le 3$ and a negative one for $d\ge 5$. A variant of this question is to require the hyperplanes to be mutually orthogonal. This variant is known to have a positive answer for $d\le 2$ and there is reason to expect it to have a negative answer for $d\ge 3$. In this note we exhibit measures that prove this. Additionally, we describe an algorithm that checks if a set of $8n$ in $\mathbb{R}^3$ can be split evenly by $3$ mutually orthogonal planes. To our surprise, it seems the probability that a random set of $8$ points chosen uniformly and independently in the unit cube does not admit such a partition is less than $0.001$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01504v1</guid>
      <category>math.CO</category>
      <category>cs.CG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gerardo L. Maldonado, Edgardo Rold\'an-Pensado</dc:creator>
    </item>
    <item>
      <title>A Linear Time and Space Local Point Cloud Geometry Encoder via Vectorized Kernel Mixture (VecKM)</title>
      <link>https://arxiv.org/abs/2404.01568</link>
      <description>arXiv:2404.01568v1 Announce Type: cross 
Abstract: We propose VecKM, a novel local point cloud geometry encoder that is descriptive, efficient and robust to noise. VecKM leverages a unique approach by vectorizing a kernel mixture to represent the local point clouds. Such representation is descriptive and robust to noise, which is supported by two theorems that confirm its ability to reconstruct and preserve the similarity of the local shape. Moreover, VecKM is the first successful attempt to reduce the computation and memory costs from $O(n^2+nKd)$ to $O(nd)$ by sacrificing a marginal constant factor, where $n$ is the size of the point cloud and $K$ is neighborhood size. The efficiency is primarily due to VecKM's unique factorizable property that eliminates the need of explicitly grouping points into neighborhoods. In the normal estimation task, VecKM demonstrates not only 100x faster inference speed but also strongest descriptiveness and robustness compared with existing popular encoders. In classification and segmentation tasks, integrating VecKM as a preprocessing module achieves consistently better performance than the PointNet, PointNet++, and point transformer baselines, and runs consistently faster by up to 10x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01568v1</guid>
      <category>cs.CV</category>
      <category>cs.CG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dehao Yuan, Cornelia Ferm\"uller, Tahseen Rabbani, Furong Huang, Yiannis Aloimonos</dc:creator>
    </item>
    <item>
      <title>Optimizing Visibility-based Search in Polygonal Domains</title>
      <link>https://arxiv.org/abs/2402.05420</link>
      <description>arXiv:2402.05420v2 Announce Type: replace 
Abstract: Given a geometric domain $P$, visibility-based search problems seek routes for one or more mobile agents (``watchmen'') to move within $P$ in order to be able to see a portion (or all) of $P$, while optimizing objectives, such as the length(s) of the route(s), the size (e.g., area or volume) of the portion seen, the probability of detecting a target distributed within $P$ according to a prior distribution, etc. The classic watchman route problem seeks a shortest route for an observer, with omnidirectional vision, to see all of $P$. In this paper we study bicriteria optimization problems for a single mobile agent within a polygonal domain $P$ in the plane, with the criteria of route length and area seen. Specifically, we address the problem of computing a minimum length route that sees at least a specified area of $P$ (minimum length, for a given area quota). We also study the problem of computing a length-constrained route that sees as much area as possible. We provide hardness results and approximation algorithms. In particular, for a simple polygon $P$ we provide the first fully polynomial-time approximation scheme for the problem of computing a shortest route seeing an area quota, as well as a (slightly more efficient) polynomial dual approximation. We also consider polygonal domains $P$ (with holes) and the special case of a planar domain consisting of a union of lines. Our results yield the first approximation algorithms for computing a time-optimal search route in $P$ to guarantee some specified probability of detection of a static target within $P$, randomly distributed in $P$ according to a given prior distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05420v2</guid>
      <category>cs.CG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>In 19th Scandinavian Symposium on Algorithm Theory, SWAT 2024</arxiv:journal_reference>
      <dc:creator>Kien C. Huynh, Joseph S. B. Mitchell, Linh Nguyen, Valentin Polishchuk</dc:creator>
    </item>
    <item>
      <title>Sweeping Arrangements of Non-Piercing Curves in Plane</title>
      <link>https://arxiv.org/abs/2403.16474</link>
      <description>arXiv:2403.16474v2 Announce Type: replace 
Abstract: Let $\Gamma$ be a finite set of Jordan curves in the plane. For any curve $\gamma \in \Gamma$, we denote the bounded region enclosed by $\gamma$ as $\tilde{\gamma}$. We say that $\Gamma$ is a non-piercing family if for any two curves $\alpha , \beta \in \Gamma$, $\tilde{\alpha} \setminus \tilde{\beta}$ is a connected region. A non-piercing family of curves generalizes a family of $2$-intersecting curves in which each pair of curves intersect in at most two points. Snoeyink and Hershberger (``Sweeping Arrangements of Curves'', SoCG '89) proved that if we are given a family $\mathcal{C}$ of $2$-intersecting curves and a fixed curve $C\in\mathcal{C}$, then the arrangement can be \emph{swept} by $C$, i.e., $C$ can be continuously shrunk to any point $p \in \tilde{C}$ in such a way that the we have a family of $2$-intersecting curves throughout the process. In this paper, we generalize the result of Snoeyink and Hershberger to the setting of non-piercing curves. We show that given an arrangement of non-piercing curves $\Gamma$, and a fixed curve $\gamma\in \Gamma$, the arrangement can be swept by $\gamma$ so that the arrangement remains non-piercing throughout the process. We also give a shorter and simpler proof of the result of Snoeyink and Hershberger and cite applications of their result, where our result leads to a generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16474v2</guid>
      <category>cs.CG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suryendu Dalal, Rahul Gangopadhyay, Rajiv Raman, Saurabh Ray</dc:creator>
    </item>
    <item>
      <title>Local and global topological complexity measures OF ReLU neural network functions</title>
      <link>https://arxiv.org/abs/2204.06062</link>
      <description>arXiv:2204.06062v2 Announce Type: replace-cross 
Abstract: We apply a generalized piecewise-linear (PL) version of Morse theory due to Grunert-Kuhnel-Rote to define and study new local and global notions of topological complexity for fully-connected feedforward ReLU neural network functions, F: R^n -&gt; R. Along the way, we show how to construct, for each such F, a canonical polytopal complex K(F) and a deformation retract of the domain onto K(F), yielding a convenient compact model for performing calculations. We also give a construction showing that local complexity can be arbitrarily high.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.06062v2</guid>
      <category>math.AT</category>
      <category>cs.CG</category>
      <category>cs.LG</category>
      <category>math.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Elisenda Grigsby, Kathryn Lindsey, Marissa Masden</dc:creator>
    </item>
    <item>
      <title>Few-shot point cloud reconstruction and denoising via learned Guassian splats renderings and fine-tuned diffusion features</title>
      <link>https://arxiv.org/abs/2404.01112</link>
      <description>arXiv:2404.01112v2 Announce Type: replace-cross 
Abstract: Existing deep learning methods for the reconstruction and denoising of point clouds rely on small datasets of 3D shapes. We circumvent the problem by leveraging deep learning methods trained on billions of images. We propose a method to reconstruct point clouds from few images and to denoise point clouds from their rendering by exploiting prior knowledge distilled from image-based deep learning models. To improve reconstruction in constraint settings, we regularize the training of a differentiable renderer with hybrid surface and appearance by introducing semantic consistency supervision. In addition, we propose a pipeline to finetune Stable Diffusion to denoise renderings of noisy point clouds and we demonstrate how these learned filters can be used to remove point cloud noise coming without 3D supervision. We compare our method with DSS and PointRadiance and achieved higher quality 3D reconstruction on the Sketchfab Testset and SCUT Dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01112v2</guid>
      <category>cs.CV</category>
      <category>cs.CG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pietro Bonazzi</dc:creator>
    </item>
  </channel>
</rss>
