<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Sep 2025 01:35:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Impact of Structural Changes on Learning Capacity in the Fly Olfactory Neural Circuit</title>
      <link>https://arxiv.org/abs/2509.19351</link>
      <description>arXiv:2509.19351v1 Announce Type: new 
Abstract: The Drosophila mushroom body (MB) is known to be involved in olfactory learning and memory; the synaptic plasticity of the Kenyon cell (KC) to mushroom body output neuron (MBON) synapses plays a key role in the learning process. Previous research has focused on projection neuron (PN) to Kenyon cell (KC) connectivity within the MB; we examine how perturbations to the mushroom body circuit structure and changes in connectivity, specifically within the KC to mushroom body output neuron (MBON) neural circuit, affect the MBONs' ability to distinguish between odor classes. We constructed a neural network that incorporates the connectivity between PNs, KCs, and MBONs. To train our model, we generated ten artificial input classes, which represent the projection neuron activity in response to different odors. We collected data on the number of KC-to-MBON connections, MBON error rates, and KC-to-MBON synaptic weights, among other metrics. We observed that MBONs with very few presynaptic KCs consistently performed worse than others in the odor classification task. The developmental types of KCs also played a significant role in each MBON's output. We performed random and targeted KC ablation and observed that ablating developmentally mature KCs had a greater negative impact on MBONs' learning capacity than ablating immature KCs. Random and targeted pruning of KC-MBON synaptic connections yielded results largely consistent with the ablation experiments. To further explore the various types of KCs, we also performed rewiring experiments in the PN to KC circuit. Our study furthers our understanding of olfactory neuroplasticity and provides important clues to understanding learning and memory in general. Understanding how the olfactory circuits process and learn can also have potential applications in artificial intelligence and treatments for neurodegenerative diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19351v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Katherine Xie, Gabriel Koch Ocker</dc:creator>
    </item>
    <item>
      <title>Ising dynamics on multilayer networks with heterogeneous layers</title>
      <link>https://arxiv.org/abs/2509.20216</link>
      <description>arXiv:2509.20216v1 Announce Type: cross 
Abstract: Multilayer networks provide a framework to study complex systems with multiple types of interactions, multiple dynamical processes, and/or multiple subsystems. When studying a dynamical process on a multilayer network, it is important to consider how both layer structure and heterogeneity across layers impacts the overall dynamics. As a concrete example, we study Ising dynamics on multilayer networks and investigate how network structure affects its qualitative features. We focus primarily on multiplex networks, which are multilayer networks in which interlayer edges occur only between manifestations of the same entity on different layers, although we also consider one empirical example with a more general multilayer structure. We use numerical simulations and a mean-field approximation to examine the steady-state behavior of the Ising dynamics as a function of temperature (which is a key model parameter) for a variety of two-layer multilayer networks from both models and empirical data. We examine both the steady-state behavior and a metastable state in which the two layers are anti-aligned, and we explore the effects of interlayer coupling strength and structural heterogeneity. In synthetic multilayer networks with core--periphery structure, we show that interlayer edges that involve peripheral nodes can exert more influence than interlayer edges that involve only core nodes. Finally, we consider empirical multilayer networks from biological and social systems. Our work illustrates how heterogeneity across the layers of a multilayer network influences dynamics on the whole network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20216v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suman S. Kulkarni, Christopher W. Lynn, Mason A. Porter, Dani S. Bassett</dc:creator>
    </item>
    <item>
      <title>Deciphering Functions of Neurons in Vision-Language Models</title>
      <link>https://arxiv.org/abs/2502.18485</link>
      <description>arXiv:2502.18485v4 Announce Type: replace 
Abstract: The burgeoning growth of open-sourced vision-language models (VLMs) has catalyzed a plethora of applications across diverse domains. Ensuring the transparency and interpretability of these models is critical for fostering trustworthy and responsible AI systems. In this study, our objective is to delve into the internals of VLMs to interpret the functions of individual neurons. We observe the activations of neurons with respects to the input visual tokens and text tokens, and reveal some interesting findings. Particularly, we found that there are neurons responsible for only visual or text information, or both, respectively, which we refer to them as visual neurons, text neurons, and multi-modal neurons, respectively. We build a framework that automates the explanation of neurons with the assistant of GPT-4o. Meanwhile, for visual neurons, we propose an activation simulator to assess the reliability of the explanations for visual neurons. System statistical analyses on top of one representative VLM of LLaVA, uncover the behaviors/characteristics of different categories of neurons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18485v4</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaqi Xu, Cuiling Lan, Yan Lu</dc:creator>
    </item>
    <item>
      <title>Geometric Hyperscanning of Affect under Active Inference</title>
      <link>https://arxiv.org/abs/2506.08599</link>
      <description>arXiv:2506.08599v4 Announce Type: replace 
Abstract: Second-person neuroscience holds social cognition as embodied meaning co-regulation through reciprocal interaction, modeled here as coupled active inference with affect emerging as inference over identity-relevant surprise. Each agent maintains a self-model that tracks violations in its predictive coherence while recursively modeling the other. Valence is computed from self-model prediction error, weighted by self-relevance, and modulated by prior affective states and by what we term temporal aiming, which captures affective appraisal over time. This accommodates shifts in the self-other boundary, allowing affect to emerge at individual and dyadic levels. We propose a novel method termed geometric hyperscanning, based on the Forman-Ricci curvature, to empirically operationalize these processes: it tracks topological reconfigurations in inter-brain networks, with its entro-py serving as a proxy for affective phase transitions such as rupture, co-regulation, and re-attunement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08599v4</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Hinrichs, Mahault Albarracin, Dimitris Bolis, Yuyue Jiang, Leonardo Christov-Moore, Leonhard Schilbach</dc:creator>
    </item>
    <item>
      <title>When Purple Perceived Only at Fixation: A Fixation and Distance-Dependent Color Illusion</title>
      <link>https://arxiv.org/abs/2509.11582</link>
      <description>arXiv:2509.11582v3 Announce Type: replace-cross 
Abstract: In this paper a novel optical illusion is described in which purple structures are perceived as purple at the point of fixation, while the surrounding structures of the same purple color are perceived toward a blue hue. As the viewing distance increases, a greater number of purple structures revert to a purple appearance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11582v3</guid>
      <category>physics.optics</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hinnerk Schulz-Hildebrandt</dc:creator>
    </item>
  </channel>
</rss>
