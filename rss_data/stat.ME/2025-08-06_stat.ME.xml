<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 Aug 2025 01:36:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A multi-stage Bayesian approach to fit spatial point process models</title>
      <link>https://arxiv.org/abs/2508.02922</link>
      <description>arXiv:2508.02922v1 Announce Type: new 
Abstract: Spatial point process (SPP) models are commonly used to analyze point pattern data, including presence-only data in ecology. Current methods for fitting these models are computationally expensive because they require numerical quadrature and algorithm supervision (i.e., tuning) in the Bayesian setting. We propose a flexible and efficient multi-stage recursive Bayesian approach to fitting SPP models that leverages parallel computing resources to estimate point process model coefficients and derived quantities. We show how this method can be extended to study designs with compact observation windows and allows for posterior prediction of total abundance and points in unobserved areas, which can be used for downstream analyses. We demonstrate this approach using a simulation study and analyze data from aerial imagery surveys to improve our understanding of spatially explicit abundance of harbor seals (Phoca vitulina) in Johns Hopkins Inlet, a protected tidewater glacial fjord in Glacier Bay National Park, Alaska.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02922v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rachael Ren, Mevin B. Hooten, Toryn L. J. Schafer, Nicholas M. Calzada, Benjamin Hoose, Jamie N. Womble, Scott Gende</dc:creator>
    </item>
    <item>
      <title>Sensitivity of weighted least squares estimators to omitted variables</title>
      <link>https://arxiv.org/abs/2508.02954</link>
      <description>arXiv:2508.02954v1 Announce Type: new 
Abstract: This paper introduces tools for assessing the sensitivity, to unobserved confounding, of a common estimator of the causal effect of a treatment on an outcome that employs weights: the weighted linear regression of the outcome on the treatment and observed covariates. We demonstrate through the omitted variable bias framework that the bias of this estimator is a function of two intuitive sensitivity parameters: (i) the proportion of weighted variance in the treatment that unobserved confounding explains given the covariates and (ii) the proportion of weighted variance in the outcome that unobserved confounding explains given the covariates and the treatment, i.e., two weighted partial $R^2$ values. Following previous work, we define sensitivity statistics that lend themselves well to routine reporting, and derive formal bounds on the strength of the unobserved confounding with (a multiple of) the strength of select dimensions of the covariates, which help the user determine if unobserved confounding that would alter one's conclusions is plausible. We also propose tools for adjusted inference. A key choice we make is to examine only how the (weighted) outcome model is influenced by unobserved confounding, rather than examining how the weights have been biased by omitted confounding. One benefit of this choice is that the resulting tool applies with any weights (e.g., inverse-propensity score, matching, or covariate balancing weights). Another benefit is that we can rely on simple omitted variable bias approaches that, for example, impose no distributional assumptions on the data or unobserved confounding, and can address bias from misspecification in the observed data. We make these tools available in the weightsense package for the R computing language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02954v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonard Wainstein, Chad Hazlett</dc:creator>
    </item>
    <item>
      <title>Novel measures and estimators of income inequality</title>
      <link>https://arxiv.org/abs/2508.02965</link>
      <description>arXiv:2508.02965v1 Announce Type: new 
Abstract: In this paper, we propose new income inequality measures that approximate the Gini coefficient and analyze the asymptotic properties of their estimators, including strong consistency and limiting distribution. Generalizations to the measures and estimators are developed. Simulation studies assess finite-sample performance, and an empirical example demonstrates practical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02965v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>Bayesian Sensitivity Analyses for Policy Evaluation with Difference-in-Differences under Violations of Parallel Trends</title>
      <link>https://arxiv.org/abs/2508.02970</link>
      <description>arXiv:2508.02970v1 Announce Type: new 
Abstract: Violations of the parallel trends assumption pose significant challenges for causal inference in difference-in-differences (DiD) studies, especially in policy evaluations where pre-treatment dynamics and external shocks may bias estimates. In this work, we propose a Bayesian DiD framework to allow us to estimate the effect of policies when parallel trends is violated. To address potential deviations from the parallel trends assumption, we introduce a formal sensitivity parameter representing the extent of the violation, specify an autoregressive AR(1) prior on this term to robustly model temporal correlation, and explore a range of prior specifications - including fixed, fully Bayesian, and empirical Bayes (EB) approaches calibrated from pre-treatment data. By systematically comparing posterior treatment effect estimates across prior configurations when evaluating Philadelphia's sweetened beverage tax using Baltimore as a control, we show how Bayesian sensitivity analyses support robust and interpretable policy conclusions under violations of parallel trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02970v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seong Woo Han, Nandita Mitra, Gary Hettinger, Arman Oganisian</dc:creator>
    </item>
    <item>
      <title>Two-sample comparison through additive tree models for density ratios</title>
      <link>https://arxiv.org/abs/2508.03059</link>
      <description>arXiv:2508.03059v1 Announce Type: new 
Abstract: The ratio of two densities characterizes their differences. We consider learning the density ratio given i.i.d. observations from each of the two distributions. We propose additive tree models for the density ratio along with efficient algorithms for training these models using a new loss function called the balancing loss. With this loss, additive tree models for the density ratio can be trained using algorithms original designed for supervised learning. Specifically, they can be trained from both an optimization perspective that parallels tree boosting and from a (generalized) Bayesian perspective that parallels Bayesian additive regression trees (BART). For the former, we present two boosting algorithms -- one based on forward-stagewise fitting and the other based on gradient boosting, both of which produce a point estimate for the density ratio function. For the latter, we show that due to the loss function's resemblance to an exponential family kernel, the new loss can serve as a pseudo-likelihood for which conjugate priors exist, thereby enabling effective generalized Bayesian inference on the density ratio using backfitting samplers designed for BART. The resulting uncertainty quantification on the inferred density ratio is critical for applications involving high-dimensional and complex distributions in which uncertainty given limited data can often be substantial. We provide insights on the balancing loss through its close connection to the exponential loss in binary classification and to the variational form of f-divergence, in particular that of the squared Hellinger distance. Our numerical experiments demonstrate the accuracy of the proposed approach while providing unique capabilities in uncertainty quantification. We demonstrate the application of our method in a case study involving assessing the quality of generative models for microbiome compositional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03059v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naoki Awaya, Yuliang Xu, Li Ma</dc:creator>
    </item>
    <item>
      <title>Poisson Inventory Models with Many Items: An Empirical Bayes Approach</title>
      <link>https://arxiv.org/abs/2508.03074</link>
      <description>arXiv:2508.03074v1 Announce Type: new 
Abstract: We consider inventory decisions with many items, each of which has Poisson demand. The rate of demand for individual items is estimated on the basis of observations of past demand. The problem is to determine the items to hold in stock and the amount of each one. Our setting provides a natural framework for the application of the empirical Bayes methodology. We show how to do this in practice and demonstrate the importance of making posterior estimates of different demand levels, rather than just estimating the Poisson rate. We also address the question of when it is beneficial to separately analyse a group of items which are distinguished in some way. An example occurs when looking at inventory for a book retailer, who may find it advantageous to look separately at certain types of book (e.g. biographies). The empirical Bayes methodology is valuable when dealing with items having Poisson demand, and can be effective even with relatively small numbers of distinct items (e.g. 100). We discuss the best way to apply an empirical Bayes methodology in this context, and also show that doing this in the wrong way will reduce or eliminate the potential benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03074v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Anderson, Nam Ho-Nguyen, Peter Radchenko</dc:creator>
    </item>
    <item>
      <title>Adaptive Data-Borrowing for Improving Treatment Effect Estimation using External Controls</title>
      <link>https://arxiv.org/abs/2508.03282</link>
      <description>arXiv:2508.03282v1 Announce Type: new 
Abstract: Randomized controlled trials (RCTs) often exhibit limited inferential efficiency in estimating treatment effects due to small sample sizes. In recent years, the combination of external controls has gained increasing attention as a means of improving the efficiency of RCTs. However, external controls are not always comparable to RCTs, and direct borrowing without careful evaluation can introduce substantial bias and reduce the efficiency of treatment effect estimation. In this paper, we propose a novel influence-based adaptive sample borrowing approach that effectively quantifies the "comparability'' of each sample in the external controls using influence function theory. Given a selected set of borrowed external controls, we further derive a semiparametric efficient estimator under an exchangeability assumption. Recognizing that the exchangeability assumption may not hold for all possible borrowing sets, we conduct a detailed analysis of the asymptotic bias and variance of the proposed estimator under violations of exchangeability. Building on this bias-variance trade-off, we further develop a data-driven approach to select the optimal subset of external controls for borrowing. Extensive simulations and real-world applications demonstrate that the proposed approach significantly enhances treatment effect estimation efficiency in RCTs, outperforming existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03282v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinwei Yang, Jingyi Li, Peng Wu</dc:creator>
    </item>
    <item>
      <title>Robust fuzzy clustering with cellwise outliers</title>
      <link>https://arxiv.org/abs/2508.03310</link>
      <description>arXiv:2508.03310v1 Announce Type: new 
Abstract: Fuzzy clustering is a technique for identifying subgroups in heterogeneous populations by quantifying unit membership degrees. The magnitude of the latter depends on the desired level of fuzzification, based on the purpose of the analysis. We combine the advantages of fuzzy clustering with a robust approach able to detecting cellwise outliers, i.e., anomalous cells in a data matrix. The proposed methodology is formulated within a probabilistic framework and estimated via an Expectation-Maximization algorithm for missing data. It includes an additional step for flagging contaminated cells, which are then treated as missing information. The strengths of the model are illustrated through two real-world applications: the first one identifies individuals at potential risk of obesity based on their physiological measurements, while the second one analyzes well-being across regions of the OECD countries. We also explore the effects of the model's tuning parameters and provide guidance for users on how to set them suitably.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03310v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgia Zaccaria, Lorenzo Benzakour, Luis A. Garc\'ia-Escudero, Francesca Greselin, Agust\'in Mayo-\'Iscar</dc:creator>
    </item>
    <item>
      <title>A Note on The Rationale Behind Using Parental Longevity as a Proxy in Mendelian Randomization Studies</title>
      <link>https://arxiv.org/abs/2508.03431</link>
      <description>arXiv:2508.03431v1 Announce Type: new 
Abstract: In many cohorts (such as the UK Biobank) on which Mendelian Randomization studies are routinely performed, data on participants' longevity is inadequate as the majority of participants are still living. To nevertheless estimate effects on longevity, it is increasingly common for researchers to substitute participants' `parental attained age', i.e. parental lifespan or current age (which is routinely collected in UK Biobank), as a proxy outcome. The common approach to performing this clever trick appears to be based on a solid understanding of its underlying assumptions. However, we have not seen these assumptions (or the causal effects whose identification they enable) clearly stated anywhere in the literature. In this note, we fill that gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03431v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Shahn, Rehana Rasul, C. Mary Schooling</dc:creator>
    </item>
    <item>
      <title>A New Perspective on High Dimensional Confidence Intervals</title>
      <link>https://arxiv.org/abs/2508.03504</link>
      <description>arXiv:2508.03504v1 Announce Type: new 
Abstract: Classically, confidence intervals are required to have consistent coverage across all values of the parameter. However, this will inevitably break down if the underlying estimation procedure is biased. For this reason, many efforts have focused on debiased versions of the lasso for interval construction. In the process of debiasing, however, the connection to the original estimates are often obscured. In this work, we offer a different perspective focused on average coverage in contrast to individual coverage. This perspective results in confidence intervals that better reflect the original assumptions, as opposed to debiased intervals, which often do not even contain the original lasso estimates. To this end we propose a method based on the Relaxed Lasso that gives approximately correct average coverage and compare this to debiased methods which attempt to produce correct individual coverage. With this new definition of coverage we also briefly revisit the bootstrap, which Chatterjee and Lahiri (2010) showed was inconsistent for lasso, but find that it fails even under this alternative coverage definition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03504v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Logan Harris, Patrick Breheny</dc:creator>
    </item>
    <item>
      <title>UnMuted: Defining SARS-CoV-2 Lineages According to Temporally Consistent Mutation Clusters in Wastewater Samples</title>
      <link>https://arxiv.org/abs/2508.03508</link>
      <description>arXiv:2508.03508v1 Announce Type: new 
Abstract: SARS-CoV-2 lineages are defined according to placement in a phylogenetic tree, but approximated by a list of mutations based on sequences collected from clinical sampling. Wastewater lineage abundance is generally found under the assumption that the mutation frequency is approximately equal to the sum of the abundances of the lineages to which it belongs. By leveraging numerous samples collected over time, I am able to estimate the temporal trends of the abundance of lineages as well as the definitions of those lineages. This is accomplished by assuming that collections of mutations that appear together over time constitute lineages, then estimating the proportions as before. Three main models are considered: Two that incorporate an explicit temporal trend with different constraints on the abundances, and one that does not estimate a temporal component. It is found that estimated lineage definitions correspond to known lineage definitions with matching temporal trends for the lineage abundances, despite having no information from clinical samples. I refer to this set of methods as "UnMuted" since the mutations are allowed to speak for themselves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03508v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Devan Becker</dc:creator>
    </item>
    <item>
      <title>A New Approach to Partial Conjunction Analysis in Neuroimaging</title>
      <link>https://arxiv.org/abs/2508.03675</link>
      <description>arXiv:2508.03675v1 Announce Type: new 
Abstract: The problem of identifying the brain regions activated through a particular cognitive task is pivotal in neuroimaging. This problem becomes even more complex if we have several cognitive tasks or several subjects. In this paper, we view this problem as a partial conjunction (PC) hypotheses testing problem, i.e., we are testing whether a specific brain region is activated in at least $\gamma$ (for some pre-fixed $\gamma$) subjects. We propose the application of a recent advance in the simultaneous statistical inference literature to activation localization in neuroimaging. We apply the recently proposed CoFilter method to neuroimaging data to discover brain regions activated in at least $\gamma$ subjects. Our proposal has two distinct advantages. First, it alleviates the conservativeness displayed by the traditional multiple testing procedures in testing PC hypotheses by eliminating many of the conservative PC $p$-values. Second, it is especially suitable for several high-dimensional studies, each of which examines a large number of null hypotheses. We also compare the performance of our proposal with existing methods for testing PC hypotheses through extensive simulation studies on neuroimaging data and a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03675v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monitirtha Dey, Anna Vesely, Thorsten Dickhaus</dc:creator>
    </item>
    <item>
      <title>funOCLUST: Clustering Functional Data with Outliers</title>
      <link>https://arxiv.org/abs/2508.00110</link>
      <description>arXiv:2508.00110v1 Announce Type: cross 
Abstract: Functional data present unique challenges for clustering due to their infinite-dimensional nature and potential sensitivity to outliers. An extension of the OCLUST algorithm to the functional setting is proposed to address these issues. The approach leverages the OCLUST framework, creating a robust method to cluster curves and trim outliers. The methodology is evaluated on both simulated and real-world functional datasets, demonstrating strong performance in clustering and outlier identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00110v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katharine M. Clark, Paul D. McNicholas</dc:creator>
    </item>
    <item>
      <title>Likelihood Matching for Diffusion Models</title>
      <link>https://arxiv.org/abs/2508.03636</link>
      <description>arXiv:2508.03636v1 Announce Type: cross 
Abstract: We propose a Likelihood Matching approach for training diffusion models by first establishing an equivalence between the likelihood of the target data distribution and a likelihood along the sample path of the reverse diffusion. To efficiently compute the reverse sample likelihood, a quasi-likelihood is considered to approximate each reverse transition density by a Gaussian distribution with matched conditional mean and covariance, respectively. The score and Hessian functions for the diffusion generation are estimated by maximizing the quasi-likelihood, ensuring a consistent matching of both the first two transitional moments between every two time points. A stochastic sampler is introduced to facilitate computation that leverages on both the estimated score and Hessian information. We establish consistency of the quasi-maximum likelihood estimation, and provide non-asymptotic convergence guarantees for the proposed sampler, quantifying the rates of the approximation errors due to the score and Hessian estimation, dimensionality, and the number of diffusion steps. Empirical and simulation evaluations demonstrate the effectiveness of the proposed Likelihood Matching and validate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03636v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei Qian, Wu Su, Yanqi Huang, Song Xi Chen</dc:creator>
    </item>
    <item>
      <title>Optimized imaging prefiltering for enhanced image segmentation</title>
      <link>https://arxiv.org/abs/2508.03653</link>
      <description>arXiv:2508.03653v1 Announce Type: cross 
Abstract: The Box-Cox transformation, introduced in 1964, is a widely used statistical tool for stabilizing variance and improving normality in data analysis. Its application in image processing, particularly for image enhancement, has gained increasing attention in recent years. This paper investigates the use of the Box-Cox transformation as a preprocessing step for image segmentation, with a focus on the estimation of the transformation parameter. We evaluate the effectiveness of the transformation by comparing various segmentation methods, highlighting its advantages for traditional machine learning techniques-especially in situations where no training data is available. The results demonstrate that the transformation enhances feature separability and computational efficiency, making it particularly beneficial for models like discriminant analysis. In contrast, deep learning models did not show consistent improvements, underscoring the differing impacts of the transformation across model types and image characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03653v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronny Vallejos, Felipe Osorio, Sebastian Vidal, Grisel Britos</dc:creator>
    </item>
    <item>
      <title>Nonparametric data segmentation in multivariate time series via joint characteristic functions</title>
      <link>https://arxiv.org/abs/2305.07581</link>
      <description>arXiv:2305.07581v5 Announce Type: replace 
Abstract: Modern time series data often exhibit complex dependence and structural changes which are not easily characterised by shifts in the mean or model parameters. We propose a nonparametric data segmentation methodology for multivariate time series termed NP-MOJO. By considering joint characteristic functions between the time series and its lagged values, NP-MOJO is able to detect change points in the marginal distribution, but also those in possibly non-linear serial dependence, all without the need to pre-specify the type of changes. We show the theoretical consistency of NP-MOJO in estimating the total number and the locations of the change points, and demonstrate the good performance of NP-MOJO against a variety of change point scenarios. We further demonstrate its usefulness in applications to seismology and economic time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.07581v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/biomet/asaf024</arxiv:DOI>
      <arxiv:journal_reference>Biometrika 112 (2) (2025): asaf024</arxiv:journal_reference>
      <dc:creator>Euan T. McGonigle, Haeran Cho</dc:creator>
    </item>
    <item>
      <title>Bounds on causal effects in $2^{K}$ factorial experiments with non-compliance</title>
      <link>https://arxiv.org/abs/2407.12114</link>
      <description>arXiv:2407.12114v4 Announce Type: replace 
Abstract: Factorial experiments are ubiquitous in the social and biomedical sciences, but when units fail to comply with each assigned factors, identification and estimation of the average treatment effects become impossible without strong assumptions. Leveraging an instrumental variables approach, previous studies have shown how to identify and estimate the causal effect of treatment uptake among respondents who comply with treatment. A major caveat is that these identification results rely on strong assumptions on the effect of randomization on treatment uptake. This paper shows how to bound these complier average treatment effects for bounded outcomes under more mild assumptions on non-compliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12114v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Matthew Blackwell, Nicole E. Pashley</dc:creator>
    </item>
    <item>
      <title>Identification and multiply robust estimation of causal effects via instrumental variables from an auxiliary population</title>
      <link>https://arxiv.org/abs/2407.18166</link>
      <description>arXiv:2407.18166v2 Announce Type: replace 
Abstract: Estimating causal effects in a target population with unmeasured confounders is challenging, especially when instrumental variables (IVs) are unavailable. However, IVs from auxiliary populations with similar problems can help infer causal effects in the target population. While the homogeneous conditional average treatment effect assumption has been widely used for effect transportability, it has not been explored in IV-based data fusion. We include it as a basic approach, though it may be biased when treatment effect heterogeneity exists. As an alternative approach, we introduce the equi-confounding assumption that the unmeasured confounding bias remains the same after adjusting for observed covariates, while allowing conditional average treatment effects to differ across populations. This allows us to identify the confounding bias in the auxiliary population and remove it from the treatment-outcome association in the target population to recover the causal effect. We develop multiply robust estimators under both approaches and demonstrate them through simulation studies and a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18166v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Jiapeng Liu, Peng Ding, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>Stratification in Randomised Clinical Trials and Analysis of Covariance: Some Simple Theory and Recommendations</title>
      <link>https://arxiv.org/abs/2408.06760</link>
      <description>arXiv:2408.06760v3 Announce Type: replace 
Abstract: A simple device for balancing for a continuous covariate in clinical trials is to stratify by whether the covariate is above or below some target value, typically the predicted median. This raises an issue as to which model should be used for modelling the effect of treatment on the outcome variable, $Y$. Should one fit, the stratum indicator, $S$, the continuous covariate, $X$, both or neither?
  This question has sometimes been investigated using simulations targeting the overall effect on inferences about treatment, in terms, for example, of power for a given alternative hypothesis. However, when a covariate is added to a linear model there are three consequences for inference: 1) the mean square error effect, 2) the variance inflation factor and 3) second order precision. We consider that it is valuable to consider these three factors separately, even if, ultimately, it is their joint effect that matters. We present some simple theory, concentrating in particular on the variance inflation factor, that may be used to guide trialists in their choice of model. We also consider the case where the precise form of the relationship between the outcome and the covariate is not known. We conclude by recommending that the continuous covariate should always be in the model but that, depending on circumstances, there may be some justification in fitting the stratum indicator also.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06760v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stephen Senn, Franz K\"onig, Martin Posch</dc:creator>
    </item>
    <item>
      <title>Anytime-valid FDR control with the stopped e-BH procedure</title>
      <link>https://arxiv.org/abs/2502.08539</link>
      <description>arXiv:2502.08539v3 Announce Type: replace 
Abstract: The recent e-Benjamini-Hochberg (e-BH) procedure for multiple hypothesis testing is known to control the false discovery rate (FDR) under arbitrary dependence between the input e-values. This paper points out an important subtlety when applying the e-BH procedure with e-processes, which are sequential generalizations of e-values (where the data are observed sequentially). Since adaptively stopped e-processes are e-values, the e-BH procedure can be repeatedly applied at every time step, and one can continuously monitor the e-processes and the rejection sets obtained. One would hope that the "stopped e-BH procedure" (se-BH) has an FDR guarantee for the rejection set obtained at any stopping time. However, while this is true if the data in different streams are independent, it is not true in full generality, because each stopped e-process is an e-value only for stopping times in its own local filtration, but the se-BH procedure employs a stopping time with respect to a global filtration. This can cause information to leak across time, allowing one stream to know its future by knowing past data of another stream. This paper formulates a simple causal condition under which local e-processes are also global e-processes and thus the se-BH procedure does indeed control the FDR. The condition excludes unobserved confounding from the past and is met under most reasonable scenarios including genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08539v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Can a calibration metric be both testable and actionable?</title>
      <link>https://arxiv.org/abs/2502.19851</link>
      <description>arXiv:2502.19851v2 Announce Type: replace 
Abstract: Forecast probabilities often serve as critical inputs for binary decision making. In such settings, calibration$\unicode{x2014}$ensuring forecasted probabilities match empirical frequencies$\unicode{x2014}$is essential. Although the common notion of Expected Calibration Error (ECE) provides actionable insights for decision making, it is not testable: it cannot be empirically estimated in many practical cases. Conversely, the recently proposed Distance from Calibration (dCE) is testable, but it is not actionable since it lacks decision-theoretic guarantees needed for high-stakes applications. To resolve this question, we consider Cutoff Calibration Error, a calibration measure that bridges this gap by assessing calibration over intervals of forecasted probabilities. We show that Cutoff Calibration Error is both testable and actionable, and we examine its implications for popular post-hoc calibration methods, such as isotonic regression and Platt scaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19851v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raphael Rossellini, Jake A. Soloff, Rina Foygel Barber, Zhimei Ren, Rebecca Willett</dc:creator>
    </item>
    <item>
      <title>A Hybrid Mixture of $t$-Factor Analyzers for Clustering High-dimensional Data</title>
      <link>https://arxiv.org/abs/2504.21120</link>
      <description>arXiv:2504.21120v2 Announce Type: replace 
Abstract: This paper develops a novel hybrid approach for estimating the mixture model of $t$-factor analyzers (MtFA) that employs multivariate $t$-distribution and factor model to cluster and characterize grouped data. The traditional estimation method for MtFA faces computational challenges, particularly in high-dimensional settings, where the eigendecomposition of large covariance matrices and the iterative nature of Expectation-Maximization (EM) algorithms lead to scalability issues. We propose a computational scheme that integrates a profile likelihood method into the EM framework to efficiently obtain the model parameter estimates. The effectiveness of our approach is demonstrated through simulations showcasing its superior computational efficiency compared to the existing method, while preserving clustering accuracy and resilience against outliers. Our method is applied to cluster the Gamma-ray bursts, reinforcing several claims in the literature that Gamma-ray bursts have heterogeneous subpopulations and providing characterizations of the estimated groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21120v2</guid>
      <category>stat.ME</category>
      <category>astro-ph.HE</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazeem Kareem, Fan Dai</dc:creator>
    </item>
    <item>
      <title>Design of Experiments for Emulations: A Selective Review from a Modeling Perspective</title>
      <link>https://arxiv.org/abs/2505.09596</link>
      <description>arXiv:2505.09596v2 Announce Type: replace 
Abstract: Space-filling designs are crucial for efficient computer experiments, enabling accurate surrogate modeling and uncertainty quantification in many scientific and engineering applications, such as digital twin systems and cyber-physical systems. In this work, we will provide a comprehensive review on key design methodologies, including Maximin/miniMax designs, Latin hypercubes, and projection-based designs. Moreover, we will connect the space-filling design criteria like the fill distance to Gaussian process performance. Numerical studies are conducted to investigate the practical trade-offs among various design types, with the discussion on emerging challenges in high-dimensional and constrained settings. The paper concludes with future directions in adaptive sampling and machine learning integration, providing guidance for improving computational experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09596v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinwei Deng, Lulu Kang, C. Devon Lin</dc:creator>
    </item>
    <item>
      <title>Adaptive Inference through Bayesian and Inverse Bayesian Inference with Symmetry-Bias in Nonstationary Environments</title>
      <link>https://arxiv.org/abs/2505.12796</link>
      <description>arXiv:2505.12796v4 Announce Type: replace 
Abstract: This study proposes a novel inference framework known as Bayesian and inverse Bayesian (BIB) inference, which incorporates symmetry bias into the Bayesian updating process to perform both conventional and inverse Bayesian updates concurrently. The model was evaluated in a sequential estimation task involving observations drawn from a Gaussian distribution with a stochastically time-varying mean. Conventional Bayesian inference is constrained by a fundamental trade-off between adaptability to abrupt environmental changes and accuracy during stable periods.The BIB framework addresses this limitation by dynamically modulating the learning rate via inverse Bayesian updates, thereby enhancing adaptive flexibility. Notably, the BIB model exhibited spontaneous bursts in the learning rate during environmental transitions, transiently entering high-sensitivity states that facilitated rapid adaptation. This burst-relaxation dynamic serves as a mechanism for balancing adaptability and accuracy. Furthermore, avalanche analysis and detrended fluctuation analysis revealed that the BIB system likely operates near a critical state-a property not observed in standard Bayesian inference. This suggests that the BIB model uniquely achieves a coexistence of computational efficiency and critical dynamics, resolving the adaptability-accuracy trade-off while maintaining a scale-free behavior. These findings offer a new computational perspective on scale-free dynamics in natural systems and provide valuable insights for the design of adaptive inference systems in nonstationary environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12796v4</guid>
      <category>stat.ME</category>
      <category>cs.MA</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuji Shinohara, Daiki Morita, Hayato Hirai, Ryosuke Kuribayashi, Nobuhito Manome, Toru Moriyama, Yoshihiro Nakajima, Yukio-Pegio Gunji, Ung-il Chung</dc:creator>
    </item>
    <item>
      <title>Filtrated Kinematic Connectivity Analysis for Lower-limb Joint Effective Age Evaluation</title>
      <link>https://arxiv.org/abs/2506.11369</link>
      <description>arXiv:2506.11369v3 Announce Type: replace 
Abstract: To understand and communicate the risk of chronic lower-limb joint diseases associated with aging, it is crucial to investigate the relationship between age and gait dynamics, particularly through angular kinematics. One key challenge is that angular kinematic trajectories are highly interconnected, and the structures of the interconnections vary across different components. Neglecting the interconnections and the variability in the connectivity structures impairs the understanding of age-associated gait coordination. To this end, we develop a novel kinematic connectivity analysis framework, grounded in multiple functional regression, to evaluate lower-limb joint effective age and uncover age-related kinematic features. The proposed approach is built upon the concept of filtration, a widely used tool in network analysis and topological data analysis for multi-resolution exploration. Specifically, we develop a forest-structured covariate grouping framework in which different kinematic trajectories are aggregated hierarchically to capture both (partially) shared and idiosyncratic motion signatures which are strongly associated with aging. We also develop a novel filtrated functional partial least squares approach for model estimation and feature extraction. Compared to existing approaches, our proposed approach demonstrates superior predictive power while providing novel insights into the coordinated evolution of angular kinematics during aging. In addition, the proposed framework is broadly applicable and can be readily extended in other scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11369v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhao Jiao, Hernando Ombao, Ian W. McKeague</dc:creator>
    </item>
    <item>
      <title>In silico clinical trials in drug development: a systematic review</title>
      <link>https://arxiv.org/abs/2503.08746</link>
      <description>arXiv:2503.08746v3 Announce Type: replace-cross 
Abstract: In the context of clinical research, computational models have received increasing attention over the past decades. In this systematic review, we aimed to provide an overview of the role of so-called in silico clinical trials (ISCTs) in medical applications. Exemplary for the broad field of clinical medicine, we focused on in silico (IS) methods applied in drug development, sometimes also referred to as model informed drug development (MIDD). We searched PubMed and ClinicalTrials.gov for published articles and registered clinical trials related to ISCTs. We identified 202 articles and 48 trials, and of these, 76 articles and 19 trials were directly linked to drug development. We extracted information from all 202 articles and 48 clinical trials and conducted a more detailed review of the methods used in the 76 articles that are connected to drug development. Regarding application, most articles and trials focused on cancer and imaging-related research while rare and pediatric diseases were only addressed in 14 articles and 5 trials, respectively. While some models were informed combining mechanistic knowledge with clinical or preclinical (in-vivo or in-vitro) data, the majority of models were fully data-driven, illustrating that clinical data is a crucial part in the process of generating synthetic data in ISCTs. Regarding reproducibility, a more detailed analysis revealed that only 24% (18 out of 76) of the articles provided an open-source implementation of the applied models, and in only 20% of the articles the generated synthetic data were publicly available. Despite the widely raised interest, we also found that it is still uncommon for ISCTs to be part of a registered clinical trial and their application is restricted to specific diseases leaving potential benefits of ISCTs not fully exploited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08746v3</guid>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bohua Chen, Lucia Chantal Schneider, Christian R\"over, Emmanuelle Comets, Markus Christian Elze, Andrew Hooker, Joanna IntHout, Anne-Sophie Jannot, Daria Julkowska, Yanis Mimouni, Marina Savelieva, Nigel Stallard, Moreno Ursino, Marc Vandemeulebroecke, Sebastian Weber, Martin Posch, Sarah Zohar, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Stereographic Multi-Try Metropolis Algorithms for Heavy-tailed Sampling</title>
      <link>https://arxiv.org/abs/2505.12487</link>
      <description>arXiv:2505.12487v2 Announce Type: replace-cross 
Abstract: Markov chain Monte Carlo (MCMC) methods for sampling from heavy-tailed distributions present unique challenges, particularly in high dimensions. Multi-proposal MCMC algorithms have recently gained attention for their potential to improve performance, especially through parallel implementation on modern hardware. This paper introduces a novel family of gradient-free MCMC algorithms that combine the multi-try Metropolis (MTM) with stereographic MCMC framework, specifically designed for efficient sampling from heavy-tailed targets. The proposed stereographic multi-try Metropolis (SMTM) algorithm not only outperforms traditional Euclidean MTM and existing stereographic random-walk Metropolis methods, but also avoids the pathological convergence behavior often observed in MTM and demonstrates strong robustness to tuning. These properties are supported by scaling analysis and extensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12487v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 06 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhihao Wang, Jun Yang</dc:creator>
    </item>
  </channel>
</rss>
