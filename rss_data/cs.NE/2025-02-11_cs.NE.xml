<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Feb 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Hybrid Tabu Scatter Search Algorithm for Simulation-Based Optimization of Multi-Objective Runway Operations Scheduling</title>
      <link>https://arxiv.org/abs/2502.05594</link>
      <description>arXiv:2502.05594v1 Announce Type: new 
Abstract: This dissertation addresses the growing challenge of air traffic flow management by proposing a simulation-based optimization (SbO) approach for multi-objective runway operations scheduling. The goal is to optimize airport capacity utilization while minimizing delays, fuel consumption, and environmental impacts. Given the NP-Hard complexity of the problem, traditional analytical methods often rely on oversimplifications and fail to account for real-world uncertainties, limiting their practical applicability. The proposed SbO framework integrates a discrete-event simulation model to handle stochastic conditions and a hybrid Tabu-Scatter Search algorithm to identify Pareto-optimal solutions, explicitly incorporating uncertainty and fairness among aircraft as key objectives. Computational experiments using real-world data from a major U.S. airport demonstrate the approach's effectiveness and tractability, outperforming traditional methods such as First-Come-First-Served (FCFS) and deterministic approaches while maintaining schedule fairness. The algorithm's ability to generate trade-off solutions between competing objectives makes it a promising decision support tool for air traffic controllers managing complex runway operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05594v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.25777/pv7d-2e13</arxiv:DOI>
      <dc:creator>Bulent Soykan</dc:creator>
    </item>
    <item>
      <title>Aerial Reliable Collaborative Communications for Terrestrial Mobile Users via Evolutionary Multi-Objective Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2502.05824</link>
      <description>arXiv:2502.05824v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs) have emerged as the potential aerial base stations (BSs) to improve terrestrial communications. However, the limited onboard energy and antenna power of a UAV restrict its communication range and transmission capability. To address these limitations, this work employs collaborative beamforming through a UAV-enabled virtual antenna array to improve transmission performance from the UAV to terrestrial mobile users, under interference from non-associated BSs and dynamic channel conditions. Specifically, we introduce a memory-based random walk model to more accurately depict the mobility patterns of terrestrial mobile users. Following this, we formulate a multi-objective optimization problem (MOP) focused on maximizing the transmission rate while minimizing the flight energy consumption of the UAV swarm. Given the NP-hard nature of the formulated MOP and the highly dynamic environment, we transform this problem into a multi-objective Markov decision process and propose an improved evolutionary multi-objective reinforcement learning algorithm. Specifically, this algorithm introduces an evolutionary learning approach to obtain the approximate Pareto set for the formulated MOP. Moreover, the algorithm incorporates a long short-term memory network and hyper-sphere-based task selection method to discern the movement patterns of terrestrial mobile users and improve the diversity of the obtained Pareto set. Simulation results demonstrate that the proposed method effectively generates a diverse range of non-dominated policies and outperforms existing methods. Additional simulations demonstrate the scalability and robustness of the proposed CB-based method under different system parameters and various unexpected circumstances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05824v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Geng Sun, Jian Xiao, Jiahui Li, Jiacheng Wang, Jiawen Kang, Dusit Niyato, Shiwen Mao</dc:creator>
    </item>
    <item>
      <title>Extract-QD Framework: A Generic Approach for Quality-Diversity in Noisy, Stochastic or Uncertain Domains</title>
      <link>https://arxiv.org/abs/2502.06585</link>
      <description>arXiv:2502.06585v1 Announce Type: new 
Abstract: Quality-Diversity (QD) has demonstrated potential in discovering collections of diverse solutions to optimisation problems. Originally designed for deterministic environments, QD has been extended to noisy, stochastic, or uncertain domains through various Uncertain-QD (UQD) methods. However, the large number of UQD methods, each with unique constraints, makes selecting the most suitable one challenging. To remedy this situation, we present two contributions: first, the Extract-QD Framework (EQD Framework), and second, Extract-ME (EME), a new method derived from it. The EQD Framework unifies existing approaches within a modular view, and facilitates developing novel methods by interchanging modules. We use it to derive EME, a novel method that consistently outperforms or matches the best existing methods on standard benchmarks, while previous methods show varying performance. In a second experiment, we show how our EQD Framework can be used to augment existing QD algorithms and in particular the well-established Policy-Gradient-Assisted-MAP-Elites method, and demonstrate improved performance in uncertain domains at no additional evaluation cost. For any new uncertain task, our contributions now provide EME as a reliable "first guess" method, and the EQD Framework as a tool for developing task-specific approaches. Together, these contributions aim to lower the cost of adopting UQD insights in QD applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06585v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manon Flageat, Johann Huber, Fran\c{c}ois Helenon, Stephane Doncieux, Antoine Cully</dc:creator>
    </item>
    <item>
      <title>Do Spikes Protect Privacy? Investigating Black-Box Model Inversion Attacks in Spiking Neural Networks</title>
      <link>https://arxiv.org/abs/2502.05509</link>
      <description>arXiv:2502.05509v1 Announce Type: cross 
Abstract: As machine learning models become integral to security-sensitive applications, concerns over data leakage from adversarial attacks continue to rise. Model Inversion (MI) attacks pose a significant privacy threat by enabling adversaries to reconstruct training data from model outputs. While MI attacks on Artificial Neural Networks (ANNs) have been widely studied, Spiking Neural Networks (SNNs) remain largely unexplored in this context. Due to their event-driven and discrete computations, SNNs introduce fundamental differences in information processing that may offer inherent resistance to such attacks. A critical yet underexplored aspect of this threat lies in black-box settings, where attackers operate through queries without direct access to model parameters or gradients-representing a more realistic adversarial scenario in deployed systems. This work presents the first study of black-box MI attacks on SNNs. We adapt a generative adversarial MI framework to the spiking domain by incorporating rate-based encoding for input transformation and decoding mechanisms for output interpretation. Our results show that SNNs exhibit significantly greater resistance to MI attacks than ANNs, as demonstrated by degraded reconstructions, increased instability in attack convergence, and overall reduced attack effectiveness across multiple evaluation metrics. Further analysis suggests that the discrete and temporally distributed nature of SNN decision boundaries disrupts surrogate modeling, limiting the attacker's ability to approximate the target model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05509v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.NE</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hamed Poursiami, Ayana Moshruba, Maryam Parsa</dc:creator>
    </item>
    <item>
      <title>The late-stage training dynamics of (stochastic) subgradient descent on homogeneous neural networks</title>
      <link>https://arxiv.org/abs/2502.05668</link>
      <description>arXiv:2502.05668v1 Announce Type: cross 
Abstract: We analyze the implicit bias of constant step stochastic subgradient descent (SGD). We consider the setting of binary classification with homogeneous neural networks - a large class of deep neural networks with ReLU-type activation functions such as MLPs and CNNs without biases. We interpret the dynamics of normalized SGD iterates as an Euler-like discretization of a conservative field flow that is naturally associated to the normalized classification margin. Owing to this interpretation, we show that normalized SGD iterates converge to the set of critical points of the normalized margin at late-stage training (i.e., assuming that the data is correctly classified with positive normalized margin). Up to our knowledge, this is the first extension of the analysis of Lyu and Li (2020) on the discrete dynamics of gradient descent to the nonsmooth and stochastic setting. Our main result applies to binary classification with exponential or logistic losses. We additionally discuss extensions to more general settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05668v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sholom Schechtman, Nicolas Schreuder</dc:creator>
    </item>
    <item>
      <title>On the Convergence and Stability of Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning, and Online Decision Transformers</title>
      <link>https://arxiv.org/abs/2502.05672</link>
      <description>arXiv:2502.05672v1 Announce Type: cross 
Abstract: This article provides a rigorous analysis of convergence and stability of Episodic Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning and Online Decision Transformers. These algorithms performed competitively across various benchmarks, from games to robotic tasks, but their theoretical understanding is limited to specific environmental conditions. This work initiates a theoretical foundation for algorithms that build on the broad paradigm of approaching reinforcement learning through supervised learning or sequence modeling. At the core of this investigation lies the analysis of conditions on the underlying environment, under which the algorithms can identify optimal solutions. We also assess whether emerging solutions remain stable in situations where the environment is subject to tiny levels of noise. Specifically, we study the continuity and asymptotic convergence of command-conditioned policies, values and the goal-reaching objective depending on the transition kernel of the underlying Markov Decision Process. We demonstrate that near-optimal behavior is achieved if the transition kernel is located in a sufficiently small neighborhood of a deterministic kernel. The mentioned quantities are continuous (with respect to a specific topology) at deterministic kernels, both asymptotically and after a finite number of learning cycles. The developed methods allow us to present the first explicit estimates on the convergence and stability of policies and values in terms of the underlying transition kernels. On the theoretical side we introduce a number of new concepts to reinforcement learning, like working in segment spaces, studying continuity in quotient topologies and the application of the fixed-point theory of dynamical systems. The theoretical study is accompanied by a detailed investigation of example environments and numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05672v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miroslav \v{S}trupl, Oleg Szehr, Francesco Faccio, Dylan R. Ashley, Rupesh Kumar Srivastava, J\"urgen Schmidhuber</dc:creator>
    </item>
    <item>
      <title>Universal point spread function engineering for 3D optical information processing</title>
      <link>https://arxiv.org/abs/2502.06025</link>
      <description>arXiv:2502.06025v1 Announce Type: cross 
Abstract: Point spread function (PSF) engineering has been pivotal in the remarkable progress made in high-resolution imaging in the last decades. However, the diversity in PSF structures attainable through existing engineering methods is limited. Here, we report universal PSF engineering, demonstrating a method to synthesize an arbitrary set of spatially varying 3D PSFs between the input and output volumes of a spatially incoherent diffractive processor composed of cascaded transmissive surfaces. We rigorously analyze the PSF engineering capabilities of such diffractive processors within the diffraction limit of light and provide numerical demonstrations of unique imaging capabilities, such as snapshot 3D multispectral imaging without involving any spectral filters, axial scanning or digital reconstruction steps, which is enabled by the spatial and spectral engineering of 3D PSFs. Our framework and analysis would be important for future advancements in computational imaging, sensing and diffractive processing of 3D optical information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06025v1</guid>
      <category>physics.optics</category>
      <category>cs.NE</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Sadman Sakib Rahman, Aydogan Ozcan</dc:creator>
    </item>
    <item>
      <title>Physics-Guided Foundation Model for Scientific Discovery: An Application to Aquatic Science</title>
      <link>https://arxiv.org/abs/2502.06084</link>
      <description>arXiv:2502.06084v1 Announce Type: cross 
Abstract: Physics-guided machine learning (PGML) has become a prevalent approach in studying scientific systems due to its ability to integrate scientific theories for enhancing machine learning (ML) models. However, most PGML approaches are tailored to isolated and relatively simple tasks, which limits their applicability to complex systems involving multiple interacting processes and numerous influencing features. In this paper, we propose a \textit{\textbf{P}hysics-\textbf{G}uided \textbf{F}oundation \textbf{M}odel (\textbf{PGFM})} that combines pre-trained ML models and physics-based models and leverages their complementary strengths to improve the modeling of multiple coupled processes. To effectively conduct pre-training, we construct a simulated environmental system that encompasses a wide range of influencing features and various simulated variables generated by physics-based models. The model is pre-trained in this system to adaptively select important feature interactions guided by multi-task objectives. We then fine-tune the model for each specific task using true observations, while maintaining consistency with established physical theories, such as the principles of mass and energy conservation. We demonstrate the effectiveness of this methodology in modeling water temperature and dissolved oxygen dynamics in real-world lakes. The proposed PGFM is also broadly applicable to a range of scientific fields where physics-based models are being used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06084v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Runlong Yu, Chonghao Qiu, Robert Ladwig, Paul Hanson, Yiqun Xie, Xiaowei Jia</dc:creator>
    </item>
    <item>
      <title>Utilizing Novelty-based Evolution Strategies to Train Transformers in Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2502.06301</link>
      <description>arXiv:2502.06301v1 Announce Type: cross 
Abstract: In this paper, we experiment with novelty-based variants of OpenAI-ES, the NS-ES and NSR-ES algorithms, and evaluate their effectiveness in training complex, transformer-based architectures designed for the problem of reinforcement learning such as Decision Transformers. We also test if we can accelerate the novelty-based training of these larger models by seeding the training by a pretrained models. By this, we build on our previous work, where we tested the ability of evolution strategies - specifically the aforementioned OpenAI-ES - to train the Decision Transformer architecture. The results were mixed. NS-ES showed progress, but it would clearly need many more iterations for it to yield interesting results. NSR-ES, on the other hand, proved quite capable of being straightforwardly used on larger models, since its performance appears as similar between the feed-forward model and Decision Transformer, as it was for the OpenAI-ES in our previous work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06301v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maty\'a\v{s} Lorenc</dc:creator>
    </item>
    <item>
      <title>Unsupervised Particle Tracking with Neuromorphic Computing</title>
      <link>https://arxiv.org/abs/2502.06771</link>
      <description>arXiv:2502.06771v1 Announce Type: cross 
Abstract: We study the application of a neural network architecture for identifying charged particle trajectories via unsupervised learning of delays and synaptic weights using a spike-time-dependent plasticity rule. In the considered model, the neurons receive time-encoded information on the position of particle hits in a tracking detector for a particle collider, modeled according to the geometry of the Compact Muon Solenoid Phase II detector. We show how a spiking neural network is capable of successfully identifying in a completely unsupervised way the signal left by charged particles in the presence of conspicuous noise from accidental or combinatorial hits. These results open the way to applications of neuromorphic computing to particle tracking, motivating further studies into its potential for real-time, low-power particle tracking in future high-energy physics experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06771v1</guid>
      <category>hep-ex</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuele Coradin (INFN, Sezione di Padova, Italy), Fabio Cufino (Universit\`a di Bologna, Dipartimento di Fisica, Italy), Muhammad Awais (INFN, Sezione di Padova, Italy), Tommaso Dorigo (INFN, Sezione di Padova, Italy), Enrico Lupi (INFN, Sezione di Padova, Italy), Eleonora Porcu (Universit\`a di Bologna, Dipartimento di Fisica, Italy), Jinu Raj (Central University of Tamil Nadu, India), Fredrik Sandin (Lule{\aa} University of Technology, Sweden), Mia Tosi (INFN, Sezione di Padova, Italy)</dc:creator>
    </item>
    <item>
      <title>LLMs as Meta-Reviewers' Assistants: A Case Study</title>
      <link>https://arxiv.org/abs/2402.15589</link>
      <description>arXiv:2402.15589v2 Announce Type: replace-cross 
Abstract: One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves assimilating diverse opinions from multiple expert peers, formulating one's self-judgment as a senior expert, and then summarizing all these perspectives into a concise holistic overview to make an overall recommendation. This process is time-consuming and can be compromised by human factors like fatigue, inconsistency, missing tiny details, etc. Given the latest major developments in Large Language Models (LLMs), it is very compelling to rigorously study whether LLMs can help metareviewers perform this important task better. In this paper, we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to assist meta-reviewers in better comprehending multiple experts perspectives by generating a controlled multi-perspective summary (MPS) of their opinions. To achieve this, we prompt three LLMs with different types/levels of prompts based on the recently proposed TELeR taxonomy. Finally, we perform a detailed qualitative study of the MPSs generated by the LLMs and report our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15589v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eftekhar Hossain, Sanjeev Kumar Sinha, Naman Bansal, Alex Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Guttikonda, Mousumi Akter, Md. Mahadi Hassan, Matthew Freestone, Matthew C. Williams Jr., Dongji Feng, Santu Karmaker</dc:creator>
    </item>
    <item>
      <title>SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization</title>
      <link>https://arxiv.org/abs/2411.10958</link>
      <description>arXiv:2411.10958v4 Announce Type: replace-cross 
Abstract: Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. To further enhance the efficiency of attention computation compared to SageAttention while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrices $(Q, K)$ to INT4 in a hardware-friendly thread-level granularity and quantize matrices $(\widetilde P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the accuracy of INT4 $QK^\top$. Third, we propose a two-level accumulation strategy for $\widetilde PV$ to enhance the accuracy of FP8 $\widetilde PV$. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 4.5x on RTX4090, respectively. Moreover, SageAttention2 matches the speed of FlashAttention3(fp8) on the Hopper GPUs, while delivering much higher accuracy. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for language, image, and video generation. The code is available at https://github.com/thu-ml/SageAttention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10958v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <category>cs.PF</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen</dc:creator>
    </item>
    <item>
      <title>Simplified derivations for high-dimensional convex learning problems</title>
      <link>https://arxiv.org/abs/2412.01110</link>
      <description>arXiv:2412.01110v4 Announce Type: replace-cross 
Abstract: Statistical-physics calculations in machine learning and theoretical neuroscience often involve lengthy derivations that obscure physical interpretation. We present concise, non-replica derivations of key results and highlight their underlying similarities. Using a cavity approach, we analyze high-dimensional learning problems: perceptron classification of points and manifolds, and kernel ridge regression. These problems share a common structure--a bipartite system of interacting feature and datum variables--enabling a unified analysis. For perceptron-capacity problems, we identify a symmetry that allows derivation of correct capacities through a na\"ive method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01110v4</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David G. Clark, Haim Sompolinsky</dc:creator>
    </item>
    <item>
      <title>A Survey on Large Language Models with some Insights on their Capabilities and Limitations</title>
      <link>https://arxiv.org/abs/2501.04040</link>
      <description>arXiv:2501.04040v2 Announce Type: replace-cross 
Abstract: The rapid advancement of artificial intelligence, particularly with the development of Large Language Models (LLMs) built on the transformer architecture, has redefined the capabilities of natural language processing. These models now exhibit remarkable performance across various language-related tasks, such as text generation, question answering, translation, and summarization, often rivaling human-like comprehension. More intriguingly, LLMs have demonstrated emergent abilities extending beyond their core functions, showing proficiency in tasks like commonsense reasoning, code generation, and arithmetic. This survey paper explores the foundational components, scaling mechanisms, and architectural strategies that drive these capabilities. Emphasizing models like GPT and LLaMA, we analyze the impact of exponential data and computational growth on LLM performance, while also addressing the trade-offs associated with scaling. We also examine LLM applications across sectors, such as healthcare, finance, education, and law, highlighting their adaptability and potential to solve domain-specific challenges. Central to this work are the questions of how LLMs generalize across diverse tasks, exhibit planning, and reasoning abilities, and whether these emergent abilities can be systematically elicited or enhanced. In particular, we provide some insights into the CoT (Chain of Thought) and PoT (Plan of Thought) abilities within LLMs, focusing on how pre-training data influences their emergence. Additionally, we investigate LLM-modulo frameworks that integrate external systems, allowing LLMs to handle complex, dynamic tasks. By analyzing these factors, this paper aims to foster the ongoing discussion on the capabilities and limits of LLMs, promoting their responsible development and application in novel and increasingly complex environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04040v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Matarazzo, Riccardo Torlone</dc:creator>
    </item>
    <item>
      <title>Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models</title>
      <link>https://arxiv.org/abs/2501.18280</link>
      <description>arXiv:2501.18280v2 Announce Type: replace-cross 
Abstract: The security issue of large language models (LLMs) has gained significant attention recently, with various defense mechanisms developed to prevent harmful outputs, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the distribution of text embedding model outputs is significantly biased with a large mean. Inspired by this observation, we propose novel efficient methods to search for universal magic words that can attack text embedding models. The universal magic words as suffixes can move the embedding of any text towards the bias direction, therefore manipulate the similarity of any text pair and mislead safeguards. By appending magic words to user prompts and requiring LLMs to end answers with magic words, attackers can jailbreak the safeguard. To eradicate this security risk, we also propose defense mechanisms against such attacks, which can correct the biased distribution of text embeddings in a train-free manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18280v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 11 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang</dc:creator>
    </item>
  </channel>
</rss>
