<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Nov 2024 05:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Generative AI for Data Augmentation in Wireless Networks: Analysis, Applications, and Case Study</title>
      <link>https://arxiv.org/abs/2411.08341</link>
      <description>arXiv:2411.08341v1 Announce Type: new 
Abstract: Data augmentation is a powerful technique to mitigate data scarcity. However, owing to fundamental differences in wireless data structures, traditional data augmentation techniques may not be suitable for wireless data. Fortunately, Generative Artificial Intelligence (GenAI) can be an effective alternative to wireless data augmentation due to its excellent data generation capability. This article systemically explores the potential and effectiveness of GenAI-driven data augmentation in wireless networks. We first briefly review data augmentation techniques, discuss their limitations in wireless networks, and introduce generative data augmentation, including reviewing GenAI models and their applications in data augmentation. We then explore the application prospects of GenAI-driven data augmentation in wireless networks from the physical, network, and application layers, which provides a GenAI-driven data augmentation architecture for each application. Subsequently, we propose a general generative diffusion model-based data augmentation framework for Wi-Fi gesture recognition, which uses transformer-based diffusion models to generate high-quality channel state information data. Furthermore, we develop residual neural network models for Wi-Fi gesture recognition to evaluate the role of augmented data and conduct a case study based on a real dataset. Simulation results demonstrate the effectiveness of the proposed framework. Finally, we discuss research directions for generative data augmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08341v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbo Wen, Jiawen Kang, Dusit Niyato, Yang Zhang, Jiacheng Wang, Biplab Sikdar, Ping Zhang</dc:creator>
    </item>
    <item>
      <title>Peak Age of Incorrect Information of Reactive ALOHA Reporting Under Imperfect Feedback</title>
      <link>https://arxiv.org/abs/2411.08487</link>
      <description>arXiv:2411.08487v1 Announce Type: new 
Abstract: Age of Incorrect Information (AoII) is particularly relevant in systems where real time responses to anomalies are required, such as natural disaster alerts, cybersecurity warnings, or medical emergency notifications. Keeping system control with wrong information for too long can lead to inappropriate responses. In this paper, we study the Peak AoII (PAoII) for multi-source status reporting by independent devices over a collision channel, following a zero-threshold ALOHA access where nodes observing an anomaly immediately start transmitting about it. If a collision occurs, nodes reduce the transmission probability to allow for a resolution. Finally, wrong or lost feedback messages may lead a node that successfully updated the destination to believe a collision happened. The PAoII for this scenario is computed in closed-form. We are eventually able to derive interesting results concerning the minimization of PAoII, which can be traded against the overall goodput and energy efficiency, but may push the system to the edge of congestion collapse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08487v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Chiariotti, Andrea Munari, Leonardo Badia, Petar Popovski</dc:creator>
    </item>
    <item>
      <title>Joint Model Caching and Resource Allocation in Generative AI-Enabled Wireless Edge Networks</title>
      <link>https://arxiv.org/abs/2411.08672</link>
      <description>arXiv:2411.08672v1 Announce Type: new 
Abstract: With the rapid advancement of artificial intelligence (AI), generative AI (GenAI) has emerged as a transformative tool, enabling customized and personalized AI-generated content (AIGC) services. However, GenAI models with billions of parameters require substantial memory capacity and computational power for deployment and execution, presenting significant challenges to resource-limited edge networks. In this paper, we address the joint model caching and resource allocation problem in GenAI-enabled wireless edge networks. Our objective is to balance the trade-off between delivering high-quality AIGC and minimizing the delay in AIGC service provisioning. To tackle this problem, we employ a deep deterministic policy gradient (DDPG)-based reinforcement learning approach, capable of efficiently determining optimal model caching and resource allocation decisions for AIGC services in response to user mobility and time-varying channel conditions. Numerical results demonstrate that DDPG achieves a higher model hit ratio and provides superior-quality, lower-latency AIGC services compared to other benchmark solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08672v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhang Liu, Hongyang Du, Lianfen Huang, Zhibin Gao, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>SANDWICH: Towards an Offline, Differentiable, Fully-Trainable Wireless Neural Ray-Tracing Surrogate</title>
      <link>https://arxiv.org/abs/2411.08767</link>
      <description>arXiv:2411.08767v1 Announce Type: new 
Abstract: Wireless ray-tracing (RT) is emerging as a key tool for three-dimensional (3D) wireless channel modeling, driven by advances in graphical rendering. Current approaches struggle to accurately model beyond 5G (B5G) network signaling, which often operates at higher frequencies and is more susceptible to environmental conditions and changes. Existing online learning solutions require real-time environmental supervision during training, which is both costly and incompatible with GPU-based processing. In response, we propose a novel approach that redefines ray trajectory generation as a sequential decision-making problem, leveraging generative models to jointly learn the optical, physical, and signal properties within each designated environment. Our work introduces the Scene-Aware Neural Decision Wireless Channel Raytracing Hierarchy (SANDWICH), an innovative offline, fully differentiable approach that can be trained entirely on GPUs. SANDWICH offers superior performance compared to existing online learning methods, outperforms the baseline by 4e^-2 radian in RT accuracy, and only fades 0.5 dB away from toplined channel gain estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08767v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yifei Jin, Ali Maatouk, Sarunas Girdzijauskas, Shugong Xu, Leandros Tassiulas, Rex Ying</dc:creator>
    </item>
    <item>
      <title>EDM: An Ultra-Low Latency Ethernet Fabric for Memory Disaggregation</title>
      <link>https://arxiv.org/abs/2411.08300</link>
      <description>arXiv:2411.08300v1 Announce Type: cross 
Abstract: Achieving low remote memory access latency remains the biggest challenge in realizing memory disaggregation over Ethernet inside the datacenter. We present EDM that tries to overcome this challenge using two key ideas. First, while the existing network protocols for remote memory access over Ethernet, such as TCP/IP and RDMA, are implemented on top of Ethernet's MAC layer, EDM takes a rather radical approach of implementing the entire network protocol stack for remote memory access within the Physical layer (PHY) of the Ethernet. This overcomes fundamental latency and bandwidth overheads imposed by the MAC layer, especially for small memory messages. Second, EDM implements a centralized, fast, in-network traffic scheduler for memory traffic within the PHY of the Ethernet switch. Inspired by the classic Parallel Iterative Matching (PIM) algorithm, the scheduler dynamically reserves bandwidth between compute and memory nodes by creating virtual circuits in the switch's PHY, thus eliminating the queuing delay and layer 2 packet processing delay at the switch for memory traffic, with high bandwidth utilization. Our FPGA testbed shows that EDM's network fabric incurs a latency of only $\sim$300 ns for remote memory access in an unloaded network, which is an order of magnitude lower than state-of-the-art Ethernet-based solutions such as RoCEv2 and comparable to the emerging PCIe-based solutions such as CXL. Larger-scale network simulations show that even at high network loads, EDM's latency is within 1.3$\times$ its unloaded latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08300v1</guid>
      <category>cs.OS</category>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Weigao Su, Vishal Shrivastav</dc:creator>
    </item>
    <item>
      <title>Towards Smart Microfarming in an Urban Computing Continuum</title>
      <link>https://arxiv.org/abs/2408.02992</link>
      <description>arXiv:2408.02992v2 Announce Type: replace 
Abstract: Microfarming and urban computing have evolved as two distinct sustainability pillars of urban living today. In this paper, we combine these two concepts, while majorly extending them jointly towards novel concepts of smart microfarming and urban computing continuum. Smart microfarming is proposed with applications of artificial intelligence (AI) in microfarming, while an urban computing continuum is proposed as a major extension of the concept towards an efficient Internet of Things (IoT) -edge-cloud continuum. We propose and build a system architecture for a plant recommendation system that uses machine learning (ML) at the edge to find, from a pool of given plants, the most suitable ones for a given microfarm using monitored soil values obtained from IoT sensor devices. Moreover, we propose to integrate long-distance LongRange (LoRa) communication solution for sending the data from IoT to the edge system, due to its unlicensed nature and potential for open source implementations. Finally, we propose to integrate open source and less constrained application protocol solutions, such as Advanced Message Queuing Protocol (AMQP) and Hypertext Transport Protocol (HTTP) protocols, for storing the data in the cloud. An experimental setup is used to evaluate and analyze the performance and reliability of the data collection procedure and the quality of the recommendation solution. Furthermore, collaborative filtering is used for the completion of an incomplete information about soils and plants. Finally, various ML algorithms are applied to identify and recommend the optimal plan for a specific microfarm in an urban area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02992v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marla Grunewald, Mounir Bensalem, Jasenka Dizdarevi\'c, Admela Jukan</dc:creator>
    </item>
    <item>
      <title>Effective ML Model Versioning in Edge Networks</title>
      <link>https://arxiv.org/abs/2411.01078</link>
      <description>arXiv:2411.01078v3 Announce Type: replace 
Abstract: Machine learning (ML) models, data and software need to be regularly updated whenever essential version updates are released and feasible for integration. This is a basic but most challenging requirement to satisfy in the edge, due to the various system constraints and the major impact that an update can have on robustness and stability. In this paper, we formulate for the first time the ML model versioning optimization problem, and propose effective solutions, including the update automation with reinforcement learning (RL) based algorithm. We study the edge network environment due to the known constraints in performance, response time, security, and reliability, which make updates especially challenging. The performance study shows that model version updates can be fully and effectively automated with reinforcement learning method. We show that for every range of server load values, the proper versioning can be found that improves security, reliability and/or ML model accuracy, while assuring a comparably lower response time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01078v3</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fin Gentzen, Mounir Bensalem, Admela Jukan</dc:creator>
    </item>
    <item>
      <title>OFDM Reference Signal Pattern Design Criteria for Integrated Communication and Sensing</title>
      <link>https://arxiv.org/abs/2401.09643</link>
      <description>arXiv:2401.09643v3 Announce Type: replace-cross 
Abstract: Extended ambiguity performance (EAP), which includes all grating lobes and side peaks, indicates the maximum detectable region without undesired peaks for target parameter estimation and is critical to radar sensor design. Driven by EAP requirements of bi-static sensing, we propose design criteria for orthogonal frequency division multiplexing (OFDM) reference signal (RS) patterns. The design not only improves EAP in both time delay and Doppler shift domains under different types of sensing algorithms, but also reduces resource overhead for integrated communication and sensing. With minimal modifications of post-FFT processing for current RS patterns, guard interval is extended beyond conventional cyclic prefix (CP), while maintaining inter-symbol-interference-(ISI)-free delay estimation. For standard-resolution sensing algorithms, a staggering offset of a linear slope that is relatively prime to the RS comb size is suggested. As for super-resolution sensing algorithms, necessary and sufficient conditions of comb RS staggering offsets, plus new patterns synthesized therefrom, are derived for the corresponding achievable EAP. Furthermore, we generalize the RS pattern design criterion for super-resolution sensing algorithms to irregular forms, which minimizes number of resource elements (REs) for associated algorithms to eliminate all side peaks. Starting from staggered comb pattern in current positioning RS, our generalized design eventually removes any regular form for ultimate flexibility. Overall, the proposed techniques are promising to extend the ISI- and ambiguity-free range of distance and speed estimates for radar sensing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09643v3</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JIOT.2024.3495562</arxiv:DOI>
      <dc:creator>Rui Zhang, Shawn Tsai, Tzu-Han Chou, Jiaying Ren, Wenze Qu, Oliver Sun</dc:creator>
    </item>
  </channel>
</rss>
