<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.GN updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.GN</link>
    <description>econ.GN updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.GN" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Jan 2026 02:35:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>AI-exposed jobs deteriorated before ChatGPT</title>
      <link>https://arxiv.org/abs/2601.02554</link>
      <description>arXiv:2601.02554v1 Announce Type: new 
Abstract: Public debate links worsening job prospects for AI-exposed occupations to the release of ChatGPT in late 2022. Using monthly U.S. unemployment insurance records, we measure occupation- and location-specific unemployment risk and find that risk rose in AI-exposed occupations beginning in early 2022, months before ChatGPT. Analyzing millions of LinkedIn profiles, we show that graduate cohorts from 2021 onward entered AI-exposed jobs at lower rates than earlier cohorts, with gaps opening before late 2022. Finally, from millions of university syllabi, we find that graduates taking more AI-exposed curricula had higher first-job pay and shorter job searches after ChatGPT. Together, these results point to forces pre-dating generative AI and to the ongoing value of LLM-relevant education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02554v1</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Morgan R. Frank, Alireza Javadian Sabet, Lisa Simon, Sarah H. Bana, Renzhe Yu</dc:creator>
    </item>
    <item>
      <title>Revealed Decision Rules in Choices Under Risk</title>
      <link>https://arxiv.org/abs/2601.02964</link>
      <description>arXiv:2601.02964v1 Announce Type: new 
Abstract: We study decision making under risk when perception may be menu-dependent. Behavior is modeled as the outcome of a small library of psychologically interpretable, menu-specific rules that transform each objective menu into a perceived one. At each menu, the applied rule must make the realized choice a strict improvement under a dominance benchmark on perceived lotteries. We introduce the Maximum Rule Concentration Index (MRCI), the maximal Herfindahl-Hirschman concentration of rule shares over all locally admissible assignments, and diagnostics that distinguish rules that unify behavior across many menus from rules that mainly act as substitutes. We provide a MIQP formulation, a scalable heuristic, and a finite-sample permutation test of excess concentration relative to a menu-independent random-choice benchmark. Applied to the CPC18 dataset (N=686 subjects, each making 500-700 repeated binary lottery choices), the mean MRCI is 0.545, and 64.1% of subjects reject random choice at the 1% level. Concentration gains are primarily driven by modal-payoff focusing, salience-thinking, and regret-based comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02964v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avner Seror</dc:creator>
    </item>
    <item>
      <title>Two-Step Regularized HARX to Measure Volatility Spillovers in Multi-Dimensional Systems</title>
      <link>https://arxiv.org/abs/2601.03146</link>
      <description>arXiv:2601.03146v2 Announce Type: new 
Abstract: We identify volatility spillovers across commodities, equities, and treasuries using a hybrid HAR-ElasticNet framework on daily realized volatility for six futures markets over 2002--2025. Our two step procedure estimates own-volatility dynamics via OLS to preserve persistence (roughly 0.99), then applies ElasticNet regularization to cross-market spillovers. The sparse network structure that emerges shows equity markets (ES, NQ) act as the primary volatility transmitters, while crude oil (CL) ends up being the largest receiver of cross-market shocks. Agricultural commodities stay isolated from the larger network. A simple univariate HAR model achieves equally performing point forecasts as our model, but our approach reveals network structure that univariate models cannot. Joint Impulse Response Functions trace how shocks propagate through the network. Our contribution is to demonstrate that hybrid estimation methods can identify meaningful spillover pathways while preserving forecast performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03146v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mindy L. Mallory</dc:creator>
    </item>
    <item>
      <title>Fair Distribution of Digital Payments: Balancing Transaction Flows for Regulatory Compliance</title>
      <link>https://arxiv.org/abs/2601.02369</link>
      <description>arXiv:2601.02369v1 Announce Type: cross 
Abstract: The concentration of digital payment transactions in just two UPI apps like PhonePe and Google Pay has raised concerns of duopoly in India s digital financial ecosystem. To address this, the National Payments Corporation of India (NPCI) has mandated that no single UPI app should exceed 30 percent of total transaction volume. Enforcing this cap, however, poses a significant computational challenge: how to redistribute user transactions across apps without causing widespread user inconvenience while maintaining capacity limits? In this paper, we formalize this problem as the Minimum Edge Activation Flow (MEAF) problem on a bipartite network of users and apps, where activating an edge corresponds to a new app installation. The objective is to ensure a feasible flow respecting app capacities while minimizing additional activations. We further prove that Minimum Edge Activation Flow is NP-Complete. To address the computational challenge, we propose scalable heuristics, named Decoupled Two-Stage Allocation Strategy (DTAS), that exploit flow structure and capacity reuse. Experiments on large semi-synthetic transaction network data show that DTAS finds solutions close to the optimal ILP within seconds, offering a fast and practical way to enforce transaction caps fairly and efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02369v1</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashlesha Hota, Shashwat Kumar, Daman Deep Singh, Abolfazl Asudeh, Palash Dey, Abhijnan Chakraborty</dc:creator>
    </item>
    <item>
      <title>Detecting and Mitigating Treatment Leakage in Text-Based Causal Inference: Distillation and Sensitivity Analysis</title>
      <link>https://arxiv.org/abs/2601.02400</link>
      <description>arXiv:2601.02400v1 Announce Type: cross 
Abstract: Text-based causal inference increasingly employs textual data as proxies for unobserved confounders, yet this approach introduces a previously undertheorized source of bias: treatment leakage. Treatment leakage occurs when text intended to capture confounding information also contains signals predictive of treatment status, thereby inducing post-treatment bias in causal estimates. Critically, this problem can arise even when documents precede treatment assignment, as authors may employ future-referencing language that anticipates subsequent interventions. Despite growing recognition of this issue, no systematic methods exist for identifying and mitigating treatment leakage in text-as-confounder applications. This paper addresses this gap through three contributions. First, we provide formal statistical and set-theoretic definitions of treatment leakage that clarify when and why bias occurs. Second, we propose four text distillation methods -- similarity-based passage removal, distant supervision classification, salient feature removal, and iterative nullspace projection -- designed to eliminate treatment-predictive content while preserving confounder information. Third, we validate these methods through simulations using synthetic text and an empirical application examining International Monetary Fund structural adjustment programs and child mortality. Our findings indicate that moderate distillation optimally balances bias reduction against confounder retention, whereas overly stringent approaches degrade estimate precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02400v1</guid>
      <category>econ.EM</category>
      <category>cs.CL</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adel Daoud, Richard Johansson, Connor T. Jerzak</dc:creator>
    </item>
    <item>
      <title>Modeling Policy and Resource Dynamics in the Construction Sector of Developing Countries: A System Dynamics Approach Using Sudan as a Case Study</title>
      <link>https://arxiv.org/abs/2601.02405</link>
      <description>arXiv:2601.02405v1 Announce Type: cross 
Abstract: Construction industries in developing countries face systemic challenges such as chronic project delays, cost overruns, and regulatory inefficiencies. This paper presents a system dynamics (SD) modeling framework for analyzing policy and resource dynamics within the construction sector in Sudan, with broader applicability to Least Developed Countries (LDCs). The model incorporates key variables related to workforce, material supply, financing, and policy delays, and is calibrated using genetic algorithms (GAs) based on sectoral data and expert input. Simulation results across four policy scenarios indicate that regulatory reform and workforce training are the most effective levers for improving project performance. Specifically, implementing streamlined regulatory procedures reduced project delays by up to 32%, while investment in human capital decreased cost overruns by 28% over a 10-year simulation horizon. In contrast, scenarios focusing solely on material supply or financial inputs produced limited gains without corresponding policy or labor improvements. Sensitivity analysis further revealed that the system is highly responsive to macroeconomic stability and public investment flows. The study demonstrates that a hybrid SD-GA modeling approach offers a valuable decision-support tool for policymakers seeking to improve infrastructure delivery under uncertainty. Recommendations include phased regulatory reforms, targeted capacity building, and integrating modeling tools into strategic infrastructure planning in LDCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02405v1</guid>
      <category>physics.soc-ph</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malik Dongla, Mohamed Khalafalla</dc:creator>
    </item>
    <item>
      <title>Cognitive biases shape the evolution of zero-sum norms</title>
      <link>https://arxiv.org/abs/2511.16453</link>
      <description>arXiv:2511.16453v2 Announce Type: replace 
Abstract: Why do maladaptive perceptions and norms, such as zero-sum interpretations of interaction, persist even when they undermine cooperation and investment? We develop a framework where bounded rationality and heterogeneous cognitive biases shape the evolutionary dynamics of norm coordination. Extending evolutionary game theory with quantal response equilibria and prospect-theoretic utility, we show that subjective evaluation of payoffs systematically alters population-level equilibrium selection, generating stable but inefficient attractors. Counterintuitively, our analysis demonstrates that the benefit of rationality and the cost of risk aversion on welfare behave in nonmonotone ways: intermediate precision enhances coordination, while excessive precision or strong loss aversion leads to persistent lock-in at low-payoff and zero-sum equilibria. These dynamics produce an endogenous equity-efficiency trade-off: parameter configurations that raise aggregate welfare also increase inequality, while more equal distributions are associated with lower efficiency. The results highlight how distorted payoff perceptions can anchor societies in divergent institutional trajectories, offering a behavioral-evolutionary explanation for persistent zero-sum norms and inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16453v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaak Mengesha, Meiqi Sun, Debraj Roy</dc:creator>
    </item>
    <item>
      <title>When Indemnity Insurance Fails: Parametric Coverage under Binding Budget and Risk Constraints</title>
      <link>https://arxiv.org/abs/2512.21973</link>
      <description>arXiv:2512.21973v2 Announce Type: replace 
Abstract: In high-risk environments, traditional indemnity insurance is often unaffordable or ineffective, despite its well-known optimality under expected utility. We compare excess-of-loss indemnity insurance with parametric insurance within a common mean-variance framework, allowing for fixed costs, heterogeneous premium loadings, and binding budget constraints. We show that, once these realistic frictions are introduced, parametric insurance can yield higher welfare for risk-averse individuals, even under the same utility objective. The welfare advantage arises precisely when indemnity insurance becomes impractical, and disappears once both contracts are unconstrained. Our results help reconcile classical insurance theory with the growing use of parametric risk transfer in high-risk settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21973v2</guid>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Avanzi, Debbie Kusch Falden, Mogens Steffensen</dc:creator>
    </item>
  </channel>
</rss>
