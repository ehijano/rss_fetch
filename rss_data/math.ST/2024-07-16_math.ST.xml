<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Jul 2024 01:52:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Projective Wishart Distributions</title>
      <link>https://arxiv.org/abs/2407.09582</link>
      <description>arXiv:2407.09582v1 Announce Type: new 
Abstract: We are interested in the distribution of Wishart samples after forgetting their scaling factors. We call such a distribution a projective Wishart distribution. We show that projective Wishart distributions have strong links with the affine-invariant geometry of symmetric positive definite matrices in the real case or Hermitian positive definite matrices in the complex case. First, the Fr{\'e}chet mean of a projective Wishart distribution is the covariance parameter, up to a scaling factor, of the corresponding Wishart distribution. Second, in the case of 2 by 2 matrices, the densities have simple expressions in term of the affine-invariant distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09582v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-38271-0_44</arxiv:DOI>
      <arxiv:journal_reference>Geometric Science of Information. GSI 2023, Nielsen, F., Barbaresco, F., Aug 2023, Saint-Malo (FR), France. pp.444-451</arxiv:journal_reference>
      <dc:creator>Emmanuel Chevallier (AMU, FRESNEL)</dc:creator>
    </item>
    <item>
      <title>Bayesian inference of covariate-parameter relationships for population modelling</title>
      <link>https://arxiv.org/abs/2407.09640</link>
      <description>arXiv:2407.09640v1 Announce Type: new 
Abstract: We consider population modelling using parametrised ordinary differential equation initial value problems (ODE-IVPs), in the setting where the parameter vector for each individual is not known, but a vector of covariates is available for each individual and follows an unknown distribution. For each individual, only one component of the solution to the ODE-IVP can be observed at only finitely many observation times. An important problem is to identify a covariate-parameter relationship that maps covariate vectors to parameter vectors. Such settings and problems arise in pharmacology, where the observations are blood drug concentrations, the covariates are clinically observable quantities, and the covariate-parameter relationship is used for personalised drug dosing. We use the framework presented in (Nickl, 2023) to analyse a nonparametric, nonlinear Bayesian inverse problem for the unknown covariate-parameter relationship, for a fixed finite time design, random covariate design, and a family of rescaled Gaussian priors. We provide sufficient conditions for posterior contraction, local asymptotic normality, and a Bernstein-von Mises theorem for linear functionals, and show theoretically that these conditions are met on a two-compartment model from the pharmacokinetics literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09640v1</guid>
      <category>math.ST</category>
      <category>math.CA</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Cheng Lie</dc:creator>
    </item>
    <item>
      <title>An Introduction to Permutation Processes (version 0.5)</title>
      <link>https://arxiv.org/abs/2407.09664</link>
      <description>arXiv:2407.09664v1 Announce Type: new 
Abstract: These lecture notes were prepared for a special topics course in the Department of Statistics at the University of Washington, Seattle. They comprise the first eight chapters of a book currently in progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09664v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fang Han</dc:creator>
    </item>
    <item>
      <title>Q statistics in data depth: fundamental theory revisited and variants</title>
      <link>https://arxiv.org/abs/2407.09678</link>
      <description>arXiv:2407.09678v1 Announce Type: new 
Abstract: Recently, data depth has been widely used to rank multivariate data. The study of the depth-based $Q$ statistic, originally proposed by Liu and Singh (1993), has become increasingly popular when it can be used as a quality index to differentiate between two samples. Based on the existing theoretical foundations, more and more variants have been developed for increasing power in the two sample test. However, the asymptotic expansion of the $Q$ statistic in the important foundation work of Zuo and He (2006) currently has an optimal rate $m^{-3/4}$ slower than the target $m^{-1}$, leading to limitations in higher-order expansions for developing more powerful tests.
  We revisit the existing assumptions and add two new plausible assumptions to obtain the target rate by applying a new proof method based on the Hoeffding decomposition and the Cox-Reid expansion.
  The aim of this paper is to rekindle interest in asymptotic data depth theory, to place Q-statistical inference on a firmer theoretical basis, to show its variants in current research, to open the door to the development of new theories for further variants requiring higher-order expansions, and to explore more of its potential applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09678v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Min Gao, Yiting Chen, Xiaoping Shi, Wenzhi Yang</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for the Rough Homogenization Limit of Multiscale Fractional Ornstein-Uhlenbeck Processes</title>
      <link>https://arxiv.org/abs/2407.09703</link>
      <description>arXiv:2407.09703v1 Announce Type: new 
Abstract: Most real-world systems exhibit a multiscale behaviour that needs to be taken into consideration when fitting the effective dynamics to data sampled at a given scale. In the case of stochastic multiscale systems driven by Brownian motion, it has been shown that in order for the Maximum Likelihood Estimators of the parameters of the limiting dynamics to be consistent, data needs to be subsampled at an appropriate rate. Recent advances in extracting effective dynamics for fractional multiscale systems make the same question relevant in the fractional diffusion setting. We study the problem of parameter estimation of the diffusion coefficient in this context. In particular, we consider the multiscale fractional Ornstein-Uhlenbeck system (fractional kinetic Brownian motion) and we provide convergence results for the Maximum Likelihood Estimator of the diffusion coefficient of the limiting dynamics, using multiscale data. To do so, we derive asymptotic bounds for the spectral norm of the inverse covariance matrix of fractional Gaussian noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09703v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Ramses Alonso-Martin, Horatio Boedihardjo, Anastasia Papavasiliou</dc:creator>
    </item>
    <item>
      <title>TrIM: Transformed Iterative Mondrian Forests for Gradient-based Dimension Reduction and High-Dimensional Regression</title>
      <link>https://arxiv.org/abs/2407.09964</link>
      <description>arXiv:2407.09964v1 Announce Type: new 
Abstract: We propose a computationally efficient algorithm for gradient-based linear dimension reduction and high-dimensional regression. The algorithm initially computes a Mondrian forest and uses this estimator to identify a relevant feature subspace of the inputs from an estimate of the expected gradient outer product (EGOP) of the regression function. In addition, we introduce an iterative approach known as Transformed Iterative Mondrian (TrIM) forest to improve the Mondrian forest estimator by using the EGOP estimate to update the set of features and weights used by the Mondrian partitioning mechanism. We obtain consistency guarantees and convergence rates for the estimation of the EGOP matrix and the random forest estimator obtained from one iteration of the TrIM algorithm. Lastly, we demonstrate the effectiveness of our proposed algorithm for learning the relevant feature subspace across a variety of settings with both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09964v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo Baptista, Eliza O'Reilly, Yangxinyu Xie</dc:creator>
    </item>
    <item>
      <title>Maximum mean discrepancies of Farey sequences</title>
      <link>https://arxiv.org/abs/2407.10214</link>
      <description>arXiv:2407.10214v1 Announce Type: new 
Abstract: We identify a large class of positive-semidefinite kernels for which a certain polynomial rate of convergence of maximum mean discrepancies of Farey sequences is equivalent to the Riemann hypothesis. This class includes all Mat\'ern kernels of order at least one-half.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10214v1</guid>
      <category>math.ST</category>
      <category>math.NT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toni Karvonen, Anatoly Zhigljavsky</dc:creator>
    </item>
    <item>
      <title>Theory and inference for multivariate autoregressive binary models with an application to absence-presence data in ecology</title>
      <link>https://arxiv.org/abs/2407.10260</link>
      <description>arXiv:2407.10260v1 Announce Type: new 
Abstract: We introduce a general class of autoregressive models for studying the dynamic of multivariate binary time series with stationary exogenous covariates. Using a high-level set of assumptions, we show that existence of a stationary path for such models is almost automatic and does not require parameter restrictions when the noise term is not compactly supported. We then study in details statistical inference in a dynamic version of a multivariate probit type model, as a particular case of our general construction. To avoid a complex likelihood optimization, we combine pseudo-likelihood and pairwise likelihood methods for which asymptotic results are obtained for a single path analysis and also for panel data, using ergodic theorems for multi-indexed partial sums. The latter scenario is particularly important for analyzing absence-presence of species in Ecology, a field where data are often collected from surveys at various locations. Our results also give a theoretical background for such models which are often used by the practitioners but without a probabilistic framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10260v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Franchi, Lionel Truquet</dc:creator>
    </item>
    <item>
      <title>A nonparametric test for rough volatility</title>
      <link>https://arxiv.org/abs/2407.10659</link>
      <description>arXiv:2407.10659v1 Announce Type: new 
Abstract: We develop a nonparametric test for deciding whether volatility of an asset follows a standard semimartingale process, with paths of finite quadratic variation, or a rough process with paths of infinite quadratic variation. The test utilizes the fact that volatility is rough if and only if volatility increments are negatively autocorrelated at high frequencies. It is based on the sample autocovariance of increments of spot volatility estimates computed from high-frequency asset return data. By showing a feasible CLT for this statistic under the null hypothesis of semimartingale volatility paths, we construct a test with fixed asymptotic size and an asymptotic power equal to one. The test is derived under very general conditions for the data-generating process. In particular, it is robust to jumps with arbitrary activity and to the presence of market microstructure noise. In an application of the test to SPY high-frequency data, we find evidence for rough volatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10659v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>q-fin.MF</category>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carsten H. Chong, Viktor Todorov</dc:creator>
    </item>
    <item>
      <title>Adaptation to inhomogeneous smoothness for densities with irregularities</title>
      <link>https://arxiv.org/abs/2407.10673</link>
      <description>arXiv:2407.10673v1 Announce Type: new 
Abstract: We estimate on a compact interval densities with isolated irregularities, such as discontinuities or discontinuities in some derivatives. From independent and identically distributed observations we construct a kernel estimator with non-constant bandwidth, in particular in the vicinity of irregularities. It attains faster rates, for the risk $L_{p}, p\geq 1$, than usual estimators with a fixed global bandwidth. Optimality of the rate found is established by a lower bound result. We then propose an adaptive method inspired by Lepski's method for automatically selecting the variable bandwidth, without any knowledge of the regularity of the density nor of the points where the regularity breaks down. The procedure is illustrated numerically on examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10673v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C\'eline Duval, \'Emeline Schmisser</dc:creator>
    </item>
    <item>
      <title>Sampling from the Random Linear Model via Stochastic Localization Up to the AMP Threshold</title>
      <link>https://arxiv.org/abs/2407.10763</link>
      <description>arXiv:2407.10763v1 Announce Type: new 
Abstract: The Approximate Message Passing (AMP) algorithm has garnered significant attention in recent years for solving linear inverse problems, particularly in the field of Bayesian inference for high-dimensional models. In this paper, we consider sampling from the posterior in the linear inverse problem, with an i.i.d. random design matrix. We develop a sampling algorithm by integrating the AMP algorithm and stochastic localization. We give a proof for the convergence in smoothed KL divergence between the distribution of the samples generated by our algorithm and the target distribution, whenever the noise variance $\Delta$ is below $\Delta_{\rm AMP}$, which is the computation threshold for mean estimation introduced in (Barbier et al., 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10763v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Cui, Zhiyuan Yu, Jingbo Liu</dc:creator>
    </item>
    <item>
      <title>Exploring the Statistical Properties of Outputs from a Process Inspired by Geometrical Interpretation of Newton's Method</title>
      <link>https://arxiv.org/abs/2407.09583</link>
      <description>arXiv:2407.09583v1 Announce Type: cross 
Abstract: In this paper, the statistical properties of Newton s method algorithm output in a specific case have been studied. The relative frequency density of this sample converges to a well-defined function, prompting us to explore its distribution. Through rigorous mathematical proof, we demonstrate that the probability density function follows a Cauchy distribution. Additionally, a new method to generate a uniform distribution is proposed. To further confirm our findings, we employed statistical tests, including the Kolmogorov-Smirnov test and Anderson-Darling test, which showed high p-values. Furthermore, we show that the distribution of the distance between two successive outputs can be obtained through a transformation method applied to the Cauchy distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09583v1</guid>
      <category>physics.data-an</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taki Kirouani</dc:creator>
    </item>
    <item>
      <title>Granger Causality in Extremes</title>
      <link>https://arxiv.org/abs/2407.09632</link>
      <description>arXiv:2407.09632v1 Announce Type: cross 
Abstract: We introduce a rigorous mathematical framework for Granger causality in extremes, designed to identify causal links from extreme events in time series. Granger causality plays a pivotal role in uncovering directional relationships among time-varying variables. While this notion gains heightened importance during extreme and highly volatile periods, state-of-the-art methods primarily focus on causality within the body of the distribution, often overlooking causal mechanisms that manifest only during extreme events. Our framework is designed to infer causality mainly from extreme events by leveraging the causal tail coefficient. We establish equivalences between causality in extremes and other causal concepts, including (classical) Granger causality, Sims causality, and structural causality. We prove other key properties of Granger causality in extremes and show that the framework is especially helpful under the presence of hidden confounders. We also propose a novel inference method for detecting the presence of Granger causality in extremes from data. Our method is model-free, can handle non-linear and high-dimensional time series, outperforms current state-of-the-art methods in all considered setups, both in performance and speed, and was found to uncover coherent effects when applied to financial and extreme weather observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09632v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juraj Bodik, Olivier Pasche</dc:creator>
    </item>
    <item>
      <title>Estimation of Integrated Volatility Functionals with Kernel Spot Volatility Estimators</title>
      <link>https://arxiv.org/abs/2407.09759</link>
      <description>arXiv:2407.09759v1 Announce Type: cross 
Abstract: For a multidimensional It\^o semimartingale, we consider the problem of estimating integrated volatility functionals. Jacod and Rosenbaum (2013) studied a plug-in type of estimator based on a Riemann sum approximation of the integrated functional and a spot volatility estimator with a forward uniform kernel. Motivated by recent results that show that spot volatility estimators with general two-side kernels of unbounded support are more accurate, in this paper, an estimator using a general kernel spot volatility estimator as the plug-in is considered. A biased central limit theorem for estimating the integrated functional is established with an optimal convergence rate. Unbiased central limit theorems for estimators with proper de-biasing terms are also obtained both at the optimal convergence regime for the bandwidth and when applying undersmoothing. Our results show that one can significantly reduce the estimator's bias by adopting a general kernel instead of the standard uniform kernel. Our proposed bias-corrected estimators are found to maintain remarkable robustness against bandwidth selection in a variety of sampling frequencies and functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09759v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e E. Figueroa-L\'opez, Jincheng Pang, Bei Wu</dc:creator>
    </item>
    <item>
      <title>Robust estimation of parameters in logistic regression via solving the Cramer-von Mises type L2 optimization problem</title>
      <link>https://arxiv.org/abs/1703.07044</link>
      <description>arXiv:1703.07044v2 Announce Type: replace 
Abstract: This paper proposes a novel method to estimate parameters in a logistic regression model. After obtaining the estimators, their asymptotic properties are rigorously investigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:1703.07044v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiwoong Kim</dc:creator>
    </item>
    <item>
      <title>A hypothesis test for the domain of attraction of a random variable</title>
      <link>https://arxiv.org/abs/2210.07885</link>
      <description>arXiv:2210.07885v3 Announce Type: replace 
Abstract: In this work we address the problem of detecting whether a sampled probability distribution of a random variable $V$ has infinite first moment. This issue is notably important when the sample results from complex numerical simulation methods. For example, such a situation occurs when one simulates stochastic particle systems with complex and singular McKean-Vlasov interaction kernels. As stated, the detection problem is ill-posed. We thus propose and analyze an asymptotic hypothesis test for independent copies of a given random variable which is supposed to belong to an unknown domain of attraction of a stable law. The null hypothesis $\mathbf{H_0}$ is: `$X=\sqrt{V}$ is in the domain of attraction of the Normal law' and the alternative hypothesis is $\mathbf{H_1}$: `$X$ is in the domain of attraction of a stable law with index smaller than 2'. Our key observation is that~$X$ cannot have a finite second moment when $\mathbf{H_0}$ is rejected (and therefore $\mathbf{H_1}$ is accepted).
  Surprisingly, we find it useful to derive our test from the statistics of random processes. More precisely, our hypothesis test is based on a statistic which is inspired by methodologies to determine whether a semimartingale has jumps from the observation of one single path at discrete times.
  We justify our test by proving asymptotic properties of discrete time functionals of Brownian bridges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.07885v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1051/ps/2024010</arxiv:DOI>
      <dc:creator>H\'ector Olivero, Denis Talay</dc:creator>
    </item>
    <item>
      <title>An asymptotic expansion of the empirical angular measure for bivariate extremal dependence</title>
      <link>https://arxiv.org/abs/2305.16733</link>
      <description>arXiv:2305.16733v4 Announce Type: replace 
Abstract: The angular measure on the unit sphere characterizes the first-order dependence structure of the components of a random vector in extreme regions and is defined in terms of standardized margins. Its statistical recovery is an important step in learning problems involving observations far away from the center. In the common situation that the components of the vector have different distributions, the rank transformation offers a convenient and robust way of standardizing data in order to build an empirical version of the angular measure based on the most extreme observations. We provide a functional asymptotic expansion for the empirical angular measure in the bivariate case based on the theory of weak convergence in the space of bounded functions. From the expansion, not only can the known asymptotic distribution of the empirical angular measure be recovered, it also enables to find expansions and weak limits for other statistics based on the associated empirical process or its quantile version.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.16733v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>St\'ephane Lhaut, Johan Segers</dc:creator>
    </item>
    <item>
      <title>Stein's Method of Moments</title>
      <link>https://arxiv.org/abs/2305.19031</link>
      <description>arXiv:2305.19031v4 Announce Type: replace 
Abstract: Stein operators allow to characterise probability distributions via differential operators. Based on these characterisations, we develop a new method of point estimation for marginal parameters of strictly stationary and ergodic processes, which we call Stein's Method of Moments (SMOM). These SMOM estimators satisfy the desirable classical properties such as consistency and asymptotic normality. As a consequence of the usually simple form of the operator, we obtain explicit estimators in cases where standard methods such as (pseudo-) maximum likelihood estimation require a numerical procedure to calculate the estimate. In addition, with our approach, one can choose from a large class of test functions which allows to improve significantly on the moment estimator. Moreover, for i.i.d. observations, we retrieve data-dependent functions that result in asymptotically efficient estimators and give a sequence of explicit SMOM estimators that converge to the maximum likelihood estimator. Our simulation study demonstrates that for a number of important univariate continuous probability distributions our SMOM estimators possess excellent small sample behaviour, often outperforming the maximum likelihood estimator and other widely-used methods in terms of lower bias and mean squared error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.19031v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bruno Ebner, Adrian Fischer, Robert E. Gaunt, Babette Picker, Yvik Swan</dc:creator>
    </item>
    <item>
      <title>A minimum Wasserstein distance approach to Fisher's combination of independent discrete p-values</title>
      <link>https://arxiv.org/abs/2309.07692</link>
      <description>arXiv:2309.07692v2 Announce Type: replace 
Abstract: This paper introduces a comprehensive framework to adjust a discrete test statistic for improving its hypothesis testing procedure. The adjustment minimizes the Wasserstein distance to a null-approximating continuous distribution, tackling some fundamental challenges inherent in combining statistical significances derived from discrete distributions. The related theory justifies Lancaster's mid-p and mean-value chi-squared statistics for Fisher's combination as special cases. However, in order to counter the conservative nature of Lancaster's testing procedures, we propose an updated null-approximating distribution. It is achieved by further minimizing the Wasserstein distance to the adjusted statistics within a proper distribution family. Specifically, in the context of Fisher's combination, we propose an optimal gamma distribution as a substitute for the traditionally used chi-squared distribution. This new approach yields an asymptotically consistent test that significantly improves type I error control and enhances statistical power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07692v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Gonzalo Contador, Zheyang Wu</dc:creator>
    </item>
    <item>
      <title>Even naive trees are consistent</title>
      <link>https://arxiv.org/abs/2404.06850</link>
      <description>arXiv:2404.06850v4 Announce Type: replace 
Abstract: The last decade has shed some light on theoretical properties such as their consistency for regression tasks. In the current paper, we propose a new class of very simple learners based on so-called naive trees. These naive trees partition the feature space completely at random and independent of the data. Although counter-intuitive, we prove these naive trees and ensembles are consistent under fairly general assumptions. However, naive trees appear to be too simple for actual application. We therefore analyze their finite sample properties in a simulation and small benchmark study. We find a slow convergence speed and a rather poor predictive performance. Based on these results, we finally discuss to what extent consistency proofs help to justify the application of complex learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06850v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nico F\"oge, Markus Pauly, Lena Schmid, Marc Ditzhaus</dc:creator>
    </item>
    <item>
      <title>The High-Dimensional Asymptotics of Principal Component Regression</title>
      <link>https://arxiv.org/abs/2405.11676</link>
      <description>arXiv:2405.11676v2 Announce Type: replace 
Abstract: We study principal components regression (PCR) in an asymptotic high-dimensional regression setting, where the number of data points is proportional to the dimension. We derive exact limiting formulas for the estimation and prediction risks, which depend in a complicated manner on the eigenvalues of the population covariance, the alignment between the population PCs and the true signal, and the number of selected PCs. A key challenge in the high-dimensional setting stems from the fact that the sample covariance is an inconsistent estimate of its population counterpart, so that sample PCs may fail to fully capture potential latent low-dimensional structure in the data. We demonstrate this point through several case studies, including that of a spiked covariance model.
  To calculate the asymptotic prediction risk, we leverage tools from random matrix theory which to our knowledge have not seen much use to date in the statistics literature: multi-resolvent traces and their associated eigenvector overlap measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11676v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alden Green, Elad Romanov</dc:creator>
    </item>
    <item>
      <title>Sample Average Approximation for Stochastic Programming with Equality Constraints</title>
      <link>https://arxiv.org/abs/2206.09963</link>
      <description>arXiv:2206.09963v3 Announce Type: replace-cross 
Abstract: We revisit the sample average approximation (SAA) approach for non-convex stochastic programming. We show that applying the SAA approach to problems with expected value equality constraints does not necessarily result in asymptotic optimality guarantees as the sample size increases. To address this issue, we relax the equality constraints. Then, we prove the asymptotic optimality of the modified SAA approach under mild smoothness and boundedness conditions on the equality constraint functions. Our analysis uses random set theory and concentration inequalities to characterize the approximation error from the sampling procedure. We apply our approach and analysis to the problem of stochastic optimal control for nonlinear dynamical systems under external disturbances modeled by a Wiener process. Numerical results on relevant stochastic programs show the reliability of the proposed approach. Results on a rocket-powered descent problem show that our computed solutions allow for significant uncertainty reduction compared to a deterministic baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.09963v3</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Lew, Riccardo Bonalli, Marco Pavone</dc:creator>
    </item>
    <item>
      <title>UTOPIA: Universally Trainable Optimal Prediction Intervals Aggregation</title>
      <link>https://arxiv.org/abs/2306.16549</link>
      <description>arXiv:2306.16549v2 Announce Type: replace-cross 
Abstract: Uncertainty quantification in prediction presents a compelling challenge with vast applications across various domains, including biomedical science, economics, and weather forecasting. There exists a wide array of methods for constructing prediction intervals, such as quantile regression and conformal prediction. However, practitioners often face the challenge of selecting the most suitable method for a specific real-world data problem. In response to this dilemma, we introduce a novel and universally applicable strategy called Universally Trainable Optimal Predictive Intervals Aggregation (UTOPIA). This technique excels in efficiently aggregating multiple prediction intervals while maintaining a small average width of the prediction band and ensuring coverage. UTOPIA is grounded in linear or convex programming, making it straightforward to train and implement. In the specific case where the prediction methods are elementary basis functions, as in kernel and spline bases, our method becomes the construction of a prediction band. Our proposed methodologies are supported by theoretical guarantees on the coverage probability and the average width of the aggregated prediction interval, which are detailed in this paper. The practicality and effectiveness of UTOPIA are further validated through its application to synthetic data and two real-world datasets in finance and macroeconomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16549v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianqing Fan, Jiawei Ge, Debarghya Mukherjee</dc:creator>
    </item>
    <item>
      <title>Adaptive Matrix Change Point Detection: Leveraging Structured Mean Shifts</title>
      <link>https://arxiv.org/abs/2401.17473</link>
      <description>arXiv:2401.17473v2 Announce Type: replace-cross 
Abstract: In high-dimensional time series, the component processes are often assembled into a matrix to display their interrelationship. We focus on detecting mean shifts with unknown change point locations in these matrix time series. Series that are activated by a change may cluster along certain rows (columns), which forms mode-specific change point alignment. Leveraging mode-specific change point alignments may substantially enhance the power for change point detection. Yet, there may be no mode-specific alignments in the change point structure. We propose a powerful test to detect mode-specific change points, yet robust to non-mode-specific changes. We show the validity of using the multiplier bootstrap to compute the p-value of the proposed methods, and derive non-asymptotic bounds on the size and power of the tests. We also propose a parallel bootstrap, a computationally efficient approach for computing the p-value of the proposed adaptive test. In particular, we show the consistency of the proposed test, under mild regularity conditions. To obtain the theoretical results, we derive new, sharp bounds on Gaussian approximation and multiplier bootstrap approximation, which are of independent interest for high dimensional problems with diverging sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17473v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Zhang, Kung-Sik Chan</dc:creator>
    </item>
    <item>
      <title>GIST: Gibbs self-tuning for locally adaptive Hamiltonian Monte Carlo</title>
      <link>https://arxiv.org/abs/2404.15253</link>
      <description>arXiv:2404.15253v2 Announce Type: replace-cross 
Abstract: We present a novel and flexible framework for localized tuning of Hamiltonian Monte Carlo (HMC) samplers by Gibbs sampling the algorithm's tuning parameters conditionally based on the position and momentum at each step. For adaptively sampling path lengths, the framework encompasses randomized HMC, multinomial HMC, the No-U-Turn Sampler (NUTS), and the Apogee-to-Apogee Path Sampler as special cases. The Gibbs self-tuning (GIST) framework is illustrated with an alternative to NUTS for locally adapting path lengths, evaluated with an exact Hamiltonian for an ill-conditioned normal and with the leapfrog algorithm for a test suite of diverse models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15253v2</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nawaf Bou-Rabee, Bob Carpenter, Milo Marsden</dc:creator>
    </item>
  </channel>
</rss>
