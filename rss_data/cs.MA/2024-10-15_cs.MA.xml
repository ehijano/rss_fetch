<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MA</link>
    <description>cs.MA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Oct 2024 02:04:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Crowd IQ -- Aggregating Opinions to Boost Performance</title>
      <link>https://arxiv.org/abs/2410.10004</link>
      <description>arXiv:2410.10004v1 Announce Type: new 
Abstract: We show how the quality of decisions based on the aggregated opinions of the crowd can be conveniently studied using a sample of individual responses to a standard IQ questionnaire. We aggregated the responses to the IQ questionnaire using simple majority voting and a machine learning approach based on a probabilistic graphical model. The score for the aggregated questionnaire, Crowd IQ, serves as a quality measure of decisions based on aggregating opinions, which also allows quantifying individual and crowd performance on the same scale. We show that Crowd IQ grows quickly with the size of the crowd but saturates, and that for small homogeneous crowds the Crowd IQ significantly exceeds the IQ of even their most intelligent member. We investigate alternative ways of aggregating the responses and the impact of the aggregation method on the resulting Crowd IQ. We also discuss Contextual IQ, a method of quantifying the individual participant's contribution to the Crowd IQ based on the Shapley value from cooperative game theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10004v1</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5555/2343576.2343653</arxiv:DOI>
      <dc:creator>Michal Kosinski, Yoram Bachrach, Thore Graepel, Giergji Kasneci, Jurgen Van Gael</dc:creator>
    </item>
    <item>
      <title>A Multi-LLM Orchestration Engine for Personalized, Context-Rich Assistance</title>
      <link>https://arxiv.org/abs/2410.10039</link>
      <description>arXiv:2410.10039v1 Announce Type: new 
Abstract: In recent years, large language models have demonstrated remarkable capabilities in natural language understanding and generation. However, these models often struggle with hallucinations and maintaining long term contextual relevance, particularly when dealing with private or local data. This paper presents a novel architecture that addresses these challenges by integrating an orchestration engine that utilizes multiple LLMs in conjunction with a temporal graph database and a vector database. The proposed system captures user interactions, builds a graph representation of conversations, and stores nodes and edges that map associations between key concepts, entities, and behaviors over time. This graph based structure allows the system to develop an evolving understanding of the user preferences, providing personalized and contextually relevant answers. In addition to this, a vector database encodes private data to supply detailed information when needed, allowing the LLM to access and synthesize complex responses. To further enhance reliability, the orchestration engine coordinates multiple LLMs to generate comprehensive answers and iteratively reflect on their accuracy. The result is an adaptive, privacy centric AI assistant capable of offering deeper, more relevant interactions while minimizing the risk of hallucinations. This paper outlines the architecture, methodology, and potential applications of this system, contributing a new direction in personalized, context aware AI assistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10039v1</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sumedh Rasal</dc:creator>
    </item>
    <item>
      <title>Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent Graph Attention Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2410.10071</link>
      <description>arXiv:2410.10071v1 Announce Type: new 
Abstract: In order to avoid repeated task offloading and realize the reuse of popular task computing results, we construct a novel content caching-assisted vehicular edge computing (VEC) framework. In the face of irregular network topology and unknown environmental dynamics, we further propose a multi-agent graph attention reinforcement learning (MGARL) based edge caching scheme, which utilizes the graph attention convolution kernel to integrate the neighboring nodes' features of each agent and further enhance the cooperation among agents. Our simulation results show that our proposed scheme is capable of improving the utilization of caching resources while reducing the long-term task computing latency compared to the baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10071v1</guid>
      <category>cs.MA</category>
      <category>cs.ET</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinjin Shen, Yan Lin, Yijin Zhang, Weibin Zhang, Feng Shu, Jun Li</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Actor-Critics in Autonomous Cyber Defense</title>
      <link>https://arxiv.org/abs/2410.09134</link>
      <description>arXiv:2410.09134v1 Announce Type: cross 
Abstract: The need for autonomous and adaptive defense mechanisms has become paramount in the rapidly evolving landscape of cyber threats. Multi-Agent Deep Reinforcement Learning (MADRL) presents a promising approach to enhancing the efficacy and resilience of autonomous cyber operations. This paper explores the application of Multi-Agent Actor-Critic algorithms which provides a general form in Multi-Agent learning to cyber defense, leveraging the collaborative interactions among multiple agents to detect, mitigate, and respond to cyber threats. We demonstrate each agent is able to learn quickly and counter act on the threats autonomously using MADRL in simulated cyber-attack scenarios. The results indicate that MADRL can significantly enhance the capability of autonomous cyber defense systems, paving the way for more intelligent cybersecurity strategies. This study contributes to the growing body of knowledge on leveraging artificial intelligence for cybersecurity and sheds light for future research and development in autonomous cyber operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09134v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingjun Wang, Remington Dechene</dc:creator>
    </item>
    <item>
      <title>The Condorcet Dimension of Metric Spaces</title>
      <link>https://arxiv.org/abs/2410.09201</link>
      <description>arXiv:2410.09201v1 Announce Type: cross 
Abstract: A Condorcet winning set is a set of candidates such that no other candidate is preferred by at least half the voters over all members of the set. The Condorcet dimension, which is the minimum cardinality of a Condorcet winning set, is known to be at most logarithmic in the number of candidates. We study the case of elections where voters and candidates are located in a $2$-dimensional space with preferences based upon proximity voting. Our main result is that the Condorcet dimension is at most $3$, under both the Manhattan norm and the infinity norm, natural measures in electoral systems. We also prove that any set of voter preferences can be embedded into a metric space of sufficiently high dimension for any $p$-norm, including the Manhattan and infinity norms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09201v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandra Lassota, Adrian Vetta, Bernhard von Stengel</dc:creator>
    </item>
    <item>
      <title>Two-person positive shortest path games have Nash equlibria in pure stationary strategies</title>
      <link>https://arxiv.org/abs/2410.09257</link>
      <description>arXiv:2410.09257v1 Announce Type: cross 
Abstract: We prove that every finite two-person positive shortest path game has a Nash equilibrium (NE) in pure stationary strategies, which can be computed in polynomial time. The existence result holds also for graphs with finite out-degrees. Moreover, we prove that a terminal NE exists provided at least one of two players can guarantee reaching a terminal. If no one can do it, in other words, if each of two players can cut all terminals from the initial position $s$, then, obviously, a cyclic NE exists, although its cost is infinite for both players, since we restrict ourselves to positive games. We conjecture that a terminal NE exists too, provided there exists a directed path from $s$ to a terminal. However, this is open.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09257v1</guid>
      <category>cs.DM</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Endre Boros, Khaled Elbassioni, Vladimir Gurvich, Mikhail Vyalyi</dc:creator>
    </item>
    <item>
      <title>Transformers as Game Players: Provable In-context Game-playing Capabilities of Pre-trained Models</title>
      <link>https://arxiv.org/abs/2410.09701</link>
      <description>arXiv:2410.09701v1 Announce Type: cross 
Abstract: The in-context learning (ICL) capability of pre-trained models based on the transformer architecture has received growing interest in recent years. While theoretical understanding has been obtained for ICL in reinforcement learning (RL), the previous results are largely confined to the single-agent setting. This work proposes to further explore the in-context learning capabilities of pre-trained transformer models in competitive multi-agent games, i.e., in-context game-playing (ICGP). Focusing on the classical two-player zero-sum games, theoretical guarantees are provided to demonstrate that pre-trained transformers can provably learn to approximate Nash equilibrium in an in-context manner for both decentralized and centralized learning settings. As a key part of the proof, constructional results are established to demonstrate that the transformer architecture is sufficiently rich to realize celebrated multi-agent game-playing algorithms, in particular, decentralized V-learning and centralized VI-ULCB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09701v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>math.IT</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengshuai Shi, Kun Yang, Jing Yang, Cong Shen</dc:creator>
    </item>
    <item>
      <title>STACKFEED: Structured Textual Actor-Critic Knowledge Base Editing with FeedBack</title>
      <link>https://arxiv.org/abs/2410.10584</link>
      <description>arXiv:2410.10584v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) often generate incorrect or outdated information, especially in low-resource settings or when dealing with private data. To address this, Retrieval-Augmented Generation (RAG) uses external knowledge bases (KBs), but these can also suffer from inaccuracies. We introduce STACKFEED, a novel Structured Textual Actor-Critic Knowledge base editing with FEEDback approach that iteratively refines the KB based on expert feedback using a multi-actor, centralized critic reinforcement learning framework. Each document is assigned to an actor, modeled as a ReACT agent, which performs structured edits based on document-specific targeted instructions from a centralized critic. Experimental results show that STACKFEED significantly improves KB quality and RAG system performance, enhancing accuracy by up to 8% over baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10584v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naman Gupta, Shashank Kirtania, Priyanshu Gupta, Krishna Kariya, Sumit Gulwani, Arun Iyer, Suresh Parthasarathy, Arjun Radhakrishna, Sriram K. Rajamani, Gustavo Soares</dc:creator>
    </item>
    <item>
      <title>Deep Calibration of Multi-Agent Model for Simulating Real-World Stock Trading</title>
      <link>https://arxiv.org/abs/2307.12987</link>
      <description>arXiv:2307.12987v2 Announce Type: replace 
Abstract: Multi-agent market model is a stock trading simulation system, which generates order flow given the agent variable of the model. We study calibrating the agent variable to simulate the order flow of any given historical trading day. In contrast to the traditional calibration that relies on the inefficient iterative search, we propose DeepCal, the first search-free approach that uses deep learning to calibrate multi-agent market model. DeepCal learns from a novel surrogate-trading loss function to address the non-differentiable issue induced by the multi-agent model and introduces a condition-aware variable estimator, adapting the trading simulation to different market conditions to enhance explainability. Through extensive experiments on real order-book data over a whole year, DeepCal has demonstrated comparable simulation accuracy (&lt;0.36 in Kolmogorov-Smirnov statistic) to traditional search-based approaches without the need for variable search, and can effectively capture the correlation between agent variable and multiple market-condition indexes~(PPI, PMI, CPI, market trend and market noise).</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12987v2</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianlang He, Keyan Lu, Xianfeng Jiao, Tianfan Xu, Chang Xu, Yang Liu, Weiqing Liu, S. -H. Gary Chan, Jiang Bian</dc:creator>
    </item>
    <item>
      <title>Communication-Efficient Soft Actor-Critic Policy Collaboration via Regulated Segment Mixture</title>
      <link>https://arxiv.org/abs/2312.10123</link>
      <description>arXiv:2312.10123v3 Announce Type: replace 
Abstract: Multi-Agent Reinforcement Learning (MARL) has emerged as a foundational approach for addressing diverse, intelligent control tasks in various scenarios like the Internet of Vehicles, Internet of Things, and Unmanned Aerial Vehicles. However, the widely assumed existence of a central node for centralized, federated learning-assisted MARL might be impractical in highly dynamic environments. This can lead to excessive communication overhead, potentially overwhelming the system. To address these challenges, we design a novel communication-efficient, fully distributed algorithm for collaborative MARL under the frameworks of Soft Actor-Critic (SAC) and Decentralized Federated Learning (DFL), named RSM-MASAC. In particular, RSM-MASAC enhances multi-agent collaboration and prioritizes higher communication efficiency in dynamic systems by incorporating the concept of segmented aggregation in DFL and augmenting multiple model replicas from received neighboring policy segments, which are subsequently employed as reconstructed referential policies for mixing. Distinctively diverging from traditional RL approaches, RSM-MASAC introduces new bounds under the framework of Maximum Entropy Reinforcement Learning (MERL). Correspondingly, it adopts a theory-guided mixture metric to regulate the selection of contributive referential policies, thus guaranteeing soft policy improvement during the communication-assisted mixing phase. Finally, the extensive simulations in mixed-autonomy traffic control scenarios verify the effectiveness and superiority of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10123v3</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoxue Yu, Rongpeng Li, Chengchao Liang, Zhifeng Zhao</dc:creator>
    </item>
    <item>
      <title>Norm Violation Detection in Multi-Agent Systems using Large Language Models: A Pilot Study</title>
      <link>https://arxiv.org/abs/2403.16517</link>
      <description>arXiv:2403.16517v2 Announce Type: replace 
Abstract: Norms are an important component of the social fabric of society by prescribing expected behaviour. In Multi-Agent Systems (MAS), agents interacting within a society are equipped to possess social capabilities such as reasoning about norms and trust. Norms have long been of interest within the Normative Multi-Agent Systems community with researchers studying topics such as norm emergence, norm violation detection and sanctioning. However, these studies have some limitations: they are often limited to simple domains, norms have been represented using a variety of representations with no standard approach emerging, and the symbolic reasoning mechanisms generally used may suffer from a lack of extensibility and robustness. In contrast, Large Language Models (LLMs) offer opportunities to discover and reason about norms across a large range of social situations. This paper evaluates the capability of LLMs to detecting norm violations. Based on simulated data from 80 stories in a household context, with varying complexities, we investigated whether 10 norms are violated. For our evaluations we first obtained the ground truth from three human evaluators for each story. Then, the majority result was compared against the results from three well-known LLM models (Llama 2 7B, Mixtral 7B and ChatGPT-4). Our results show the promise of ChatGPT-4 for detecting norm violations, with Mixtral some distance behind. Also, we identify areas where these models perform poorly and discuss implications for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16517v2</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shawn He, Surangika Ranathunga, Stephen Cranefield, Bastin Tony Roy Savarimuthu</dc:creator>
    </item>
    <item>
      <title>Tax Credits and Household Behavior: The Roles of Myopic Decision-Making and Liquidity in a Simulated Economy</title>
      <link>https://arxiv.org/abs/2408.10391</link>
      <description>arXiv:2408.10391v2 Announce Type: replace 
Abstract: There has been a growing interest in multi-agent simulators in the domain of economic modeling. However, contemporary research often involves developing reinforcement learning (RL) based models that focus solely on a single type of agents, such as households, firms, or the government. Such an approach overlooks the adaptation of interacting agents thereby failing to capture the complexity of real-world economic systems. In this work, we consider a multi-agent simulator comprised of RL agents of numerous types, including heterogeneous households, firm, central bank and government. In particular, we focus on the crucial role of the government in distributing tax credits to households. We conduct two broad categories of comprehensive experiments dealing with the impact of tax credits on 1) households with varied degrees of myopia (short-sightedness in spending and saving decisions), and 2) households with diverse liquidity profiles. The first category of experiments examines the impact of the frequency of tax credits (e.g. annual vs quarterly) on consumption patterns of myopic households. The second category of experiments focuses on the impact of varying tax credit distribution strategies on households with differing liquidities. We validate our simulation model by reproducing trends observed in real households upon receipt of unforeseen, uniform tax credits, as documented in a JPMorgan Chase report. Based on the results of the latter, we propose an innovative tax credit distribution strategy for the government to reduce inequality among households. We demonstrate the efficacy of this strategy in improving social welfare in our simulation results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10391v2</guid>
      <category>cs.MA</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kshama Dwarakanath, Jialin Dong, Svitlana Vyetrenko</dc:creator>
    </item>
    <item>
      <title>The Patterns of Life Human Mobility Simulation</title>
      <link>https://arxiv.org/abs/2410.00185</link>
      <description>arXiv:2410.00185v2 Announce Type: replace 
Abstract: We demonstrate the Patterns of Life Simulation to create realistic simulations of human mobility in a city. This simulation has recently been used to generate massive amounts of trajectory and check-in data. Our demonstration focuses on using the simulation twofold: (1) using the graphical user interface (GUI), and (2) running the simulation headless by disabling the GUI for faster data generation. We further demonstrate how the Patterns of Life simulation can be used to simulate any region on Earth by using publicly available data from OpenStreetMap. Finally, we also demonstrate recent improvements to the scalability of the simulation allows simulating up to 100,000 individual agents for years of simulation time. During our demonstration, as well as offline using our guides on GitHub, participants will learn: (1) The theories of human behavior driving the Patters of Life simulation, (2) how to simulate to generate massive amounts of synthetic yet realistic trajectory data, (3) running the simulation for a region of interest chosen by participants using OSM data, (4) learn the scalability of the simulation and understand the properties of generated data, and (5) manage thousands of parallel simulation instances running concurrently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00185v2</guid>
      <category>cs.MA</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hossein Amiri, Will Kohn, Shiyang Ruan, Joon-Seok Kim, Hamdi Kavak, Andrew Crooks, Dieter Pfoser, Carola Wenk, Andreas Zufle</dc:creator>
    </item>
    <item>
      <title>Distributed Optimization Methods for Multi-Robot Systems: Part I -- A Tutorial</title>
      <link>https://arxiv.org/abs/2301.11313</link>
      <description>arXiv:2301.11313v2 Announce Type: replace-cross 
Abstract: Distributed optimization provides a framework for deriving distributed algorithms for a variety of multi-robot problems. This tutorial constitutes the first part of a two-part series on distributed optimization applied to multi-robot problems, which seeks to advance the application of distributed optimization in robotics. In this tutorial, we demonstrate that many canonical multi-robot problems can be cast within the distributed optimization framework, such as multi-robot simultaneous localization and planning (SLAM), multi-robot target tracking, and multi-robot task assignment problems. We identify three broad categories of distributed optimization algorithms: distributed first-order methods, distributed sequential convex programming, and the alternating direction method of multipliers (ADMM). We describe the basic structure of each category and provide representative algorithms within each category. We then work through a simulation case study of multiple drones collaboratively tracking a ground vehicle. We compare solutions to this problem using a number of different distributed optimization algorithms. In addition, we implement a distributed optimization algorithm in hardware on a network of Rasberry Pis communicating with XBee modules to illustrate robustness to the challenges of real-world communication networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.11313v2</guid>
      <category>cs.RO</category>
      <category>cs.MA</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ola Shorinwa, Trevor Halsted, Javier Yu, Mac Schwager</dc:creator>
    </item>
    <item>
      <title>Distributed Optimization Methods for Multi-Robot Systems: Part II -- A Survey</title>
      <link>https://arxiv.org/abs/2301.11361</link>
      <description>arXiv:2301.11361v2 Announce Type: replace-cross 
Abstract: Although the field of distributed optimization is well-developed, relevant literature focused on the application of distributed optimization to multi-robot problems is limited. This survey constitutes the second part of a two-part series on distributed optimization applied to multi-robot problems. In this paper, we survey three main classes of distributed optimization algorithms -- distributed first-order methods, distributed sequential convex programming methods, and alternating direction method of multipliers (ADMM) methods -- focusing on fully-distributed methods that do not require coordination or computation by a central computer. We describe the fundamental structure of each category and note important variations around this structure, designed to address its associated drawbacks. Further, we provide practical implications of noteworthy assumptions made by distributed optimization algorithms, noting the classes of robotics problems suitable for these algorithms. Moreover, we identify important open research challenges in distributed optimization, specifically for robotics problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.11361v2</guid>
      <category>cs.RO</category>
      <category>cs.MA</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ola Shorinwa, Trevor Halsted, Javier Yu, Mac Schwager</dc:creator>
    </item>
    <item>
      <title>A Scalable and Parallelizable Digital Twin Framework for Sustainable Sim2Real Transition of Multi-Agent Reinforcement Learning Systems</title>
      <link>https://arxiv.org/abs/2403.10996</link>
      <description>arXiv:2403.10996v4 Announce Type: replace-cross 
Abstract: Multi-agent reinforcement learning (MARL) systems usually require significantly long training times due to their inherent complexity. Furthermore, deploying them in the real world demands a feature-rich environment along with multiple embodied agents, which may not be feasible due to budget or space limitations, not to mention energy consumption and safety issues. This work tries to address these pain points by presenting a sustainable digital twin framework capable of accelerating MARL training by selectively scaling parallelized workloads on-demand, and transferring the trained policies from simulation to reality using minimal hardware resources. The applicability of the proposed digital twin framework is highlighted through two representative use cases, which cover cooperative as well as competitive classes of MARL problems. We study the effect of agent and environment parallelization on training time and that of systematic domain randomization on zero-shot sim2real transfer across both the case studies. Results indicate up to 76.3% reduction in training time with the proposed parallelization scheme and as low as 2.9% sim2real gap using the suggested deployment method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10996v4</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chinmay Vilas Samak, Tanmay Vilas Samak, Venkat Krovi</dc:creator>
    </item>
    <item>
      <title>No-Regret Learning for Stackelberg Equilibrium Computation in Newsvendor Pricing Games</title>
      <link>https://arxiv.org/abs/2404.00203</link>
      <description>arXiv:2404.00203v3 Announce Type: replace-cross 
Abstract: We introduce the application of online learning in a Stackelberg game pertaining to a system with two learning agents in a dyadic exchange network, consisting of a supplier and retailer, specifically where the parameters of the demand function are unknown. In this game, the supplier is the first-moving leader, and must determine the optimal wholesale price of the product. Subsequently, the retailer who is the follower, must determine both the optimal procurement amount and selling price of the product. In the perfect information setting, this is known as the classical price-setting Newsvendor problem, and we prove the existence of a unique Stackelberg equilibrium when extending this to a two-player pricing game. In the framework of online learning, the parameters of the reward function for both the follower and leader must be learned, under the assumption that the follower will best respond with optimism under uncertainty. A novel algorithm based on contextual linear bandits with a measurable uncertainty set is used to provide a confidence bound on the parameters of the stochastic demand. Consequently, optimal finite time regret bounds on the Stackelberg regret, along with convergence guarantees to an approximate Stackelberg equilibrium, are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00203v3</guid>
      <category>cs.CE</category>
      <category>cs.MA</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Larkin Liu, Yuming Rong</dc:creator>
    </item>
  </channel>
</rss>
