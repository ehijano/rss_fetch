<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Feb 2025 02:49:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>LIBRA: Measuring Bias of Large Language Model from a Local Context</title>
      <link>https://arxiv.org/abs/2502.01679</link>
      <description>arXiv:2502.01679v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have significantly advanced natural language processing applications, yet their widespread use raises concerns regarding inherent biases that may reduce utility or harm for particular social groups. Despite the advancement in addressing LLM bias, existing research has two major limitations. First, existing LLM bias evaluation focuses on the U.S. cultural context, making it challenging to reveal stereotypical biases of LLMs toward other cultures, leading to unfair development and use of LLMs. Second, current bias evaluation often assumes models are familiar with the target social groups. When LLMs encounter words beyond their knowledge boundaries that are unfamiliar in their training data, they produce irrelevant results in the local context due to hallucinations and overconfidence, which are not necessarily indicative of inherent bias. This research addresses these limitations with a Local Integrated Bias Recognition and Assessment Framework (LIBRA) for measuring bias using datasets sourced from local corpora without crowdsourcing. Implementing this framework, we develop a dataset comprising over 360,000 test cases in the New Zealand context. Furthermore, we propose the Enhanced Idealized CAT Score (EiCAT), integrating the iCAT score with a beyond knowledge boundary score (bbs) and a distribution divergence-based bias measurement to tackle the challenge of LLMs encountering words beyond knowledge boundaries. Our results show that the BERT family, GPT-2, and Llama-3 models seldom understand local words in different contexts. While Llama-3 exhibits larger bias, it responds better to different cultural contexts. The code and dataset are available at: https://github.com/ipangbo/LIBRA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01679v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Pang, Tingrui Qiao, Caroline Walker, Chris Cunningham, Yun Sing Koh</dc:creator>
    </item>
    <item>
      <title>Auditing a Dutch Public Sector Risk Profiling Algorithm Using an Unsupervised Bias Detection Tool</title>
      <link>https://arxiv.org/abs/2502.01713</link>
      <description>arXiv:2502.01713v1 Announce Type: new 
Abstract: Algorithms are increasingly used to automate or aid human decisions, yet recent research shows that these algorithms may exhibit bias across legally protected demographic groups. However, data on these groups may be unavailable to organizations or external auditors due to privacy legislation. This paper studies bias detection using an unsupervised clustering tool when data on demographic groups are unavailable. We collaborate with the Dutch Executive Agency for Education to audit an algorithm that was used to assign risk scores to college students at the national level in the Netherlands between 2012-2023. Our audit covers more than 250,000 students from the whole country. The unsupervised clustering tool highlights known disparities between students with a non-European migration background and Dutch origin. Our contributions are three-fold: (1) we assess bias in a real-world, large-scale and high-stakes decision-making process by a governmental organization; (2) we use simulation studies to highlight potential pitfalls of using the unsupervised clustering tool to detect true bias when demographic group data are unavailable and provide recommendations for valid inferences; (3) we provide the unsupervised clustering tool in an open-source library. Our work serves as a starting point for a deliberative assessment by human experts to evaluate potential discrimination in algorithmic-supported decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01713v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Floris Holstege, Mackenzie Jorgensen, Kirtan Padh, Jurriaan Parie, Joel Persson, Krsto Prorokovic, Lukas Snoek</dc:creator>
    </item>
    <item>
      <title>The Effects of Enterprise Social Media on Communication Networks</title>
      <link>https://arxiv.org/abs/2502.01787</link>
      <description>arXiv:2502.01787v1 Announce Type: new 
Abstract: Enterprise social media platforms (ESMPs) are web-based platforms with standard social media functionality, e.g., communicating with others, posting links and files, liking content, etc., yet all users are part of the same company. The first contribution of this work is the use of a difference-in-differences analysis of $99$ companies to measure the causal impact of ESMPs on companies' communication networks across the full spectrum of communication technologies used within companies: email, instant messaging, and ESMPs. Adoption caused companies' communication networks to grow denser and more well-connected by adding new, novel ties that often, but not exclusively, involve communication from one to many employees. Importantly, some new ties also bridge otherwise separate parts of the corporate communication network. The second contribution of this work, utilizing data on Microsoft's own communication network, is understanding how these communication technologies connect people across the corporate hierarchy. Compared to email and instant messaging, ESMPs excel at connecting nodes distant in the corporate hierarchy both vertically (between leaders and employees) and horizontally (between employees in similar roles but different sectors). Also, influence in ESMPs is more `democratic' than elsewhere, with high-influence nodes well-distributed across the corporate hierarchy. Overall, our results suggest that ESMPs boost information flow within companies and increase employees' attention to what is happening outside their immediate working group, above and beyond email and instant messaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01787v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manoel Horta Ribeiro, Teny Shapiro, Siddharth Suri</dc:creator>
    </item>
    <item>
      <title>Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs</title>
      <link>https://arxiv.org/abs/2502.01926</link>
      <description>arXiv:2502.01926v1 Announce Type: new 
Abstract: Algorithmic fairness has conventionally adopted a perspective of racial color-blindness (i.e., difference unaware treatment). We contend that in a range of important settings, group difference awareness matters. For example, differentiating between groups may be necessary in legal contexts (e.g., the U.S. compulsory draft applies to men but not women) and harm assessments (e.g., calling a girl a terrorist may be less harmful than calling a Muslim person one). In our work we first introduce an important distinction between descriptive (fact-based), normative (value-based), and correlation (association-based) benchmarks. This distinction is significant because each category requires distinct interpretation and mitigation tailored to its specific characteristics. Then, we present a benchmark suite composed of eight different scenarios for a total of 16k questions that enables us to assess difference awareness. Finally, we show results across ten models that demonstrate difference awareness is a distinct dimension of fairness where existing bias mitigation strategies may backfire.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01926v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelina Wang, Michelle Phan, Daniel E. Ho, Sanmi Koyejo</dc:creator>
    </item>
    <item>
      <title>Licensing Open Government Data</title>
      <link>https://arxiv.org/abs/2502.01996</link>
      <description>arXiv:2502.01996v1 Announce Type: new 
Abstract: This article focuses on the legal issues associated with open government data licenses. This study compares current open data licenses and argues that licensing terms reflect policy considerations, which are quite different from those contemplated in business transactions or shared in typical commons communities. This article investigates the ambiguous legal status of data together with the new wave of open government data, which concerns some fundamental intellectual property (IP) questions not covered by, or analyzed in depth in, the current literature. Moreover, this study suggests that government should choose or adapt open data licenses according to their own IP regimes. In the end, this article argues that the design or choice of open government data license forms an important element of information policy; government, therefore, should make this decision in accordance with their policy goals and in compliance with their own jurisdictions' IP laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01996v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Hastings Business Law Journal, Vol. 14., No. 2 (2017)</arxiv:journal_reference>
      <dc:creator>Jyh-An Lee</dc:creator>
    </item>
    <item>
      <title>Large language models in climate and sustainability policy: limits and opportunities</title>
      <link>https://arxiv.org/abs/2502.02191</link>
      <description>arXiv:2502.02191v1 Announce Type: new 
Abstract: As multiple crises threaten the sustainability of our societies and pose at risk the planetary boundaries, complex challenges require timely, updated, and usable information. Natural-language processing (NLP) tools enhance and expand data collection and processing and knowledge utilization capabilities to support the definition of an inclusive, sustainable future. In this work, we apply different NLP techniques, tools and approaches to climate and sustainability documents to derive policy-relevant and actionable measures. We focus on general and domain-specific large language models (LLMs) using a combination of static and prompt-based methods. We find that the use of LLMs is successful at processing, classifying and summarizing heterogeneous text-based data. However, we also encounter challenges related to human intervention across different workflow stages and knowledge utilization for policy processes. Our work presents a critical but empirically grounded application of LLMs to complex policy problems and suggests avenues to further expand Artificial Intelligence-powered computational social sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02191v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca Larosa, Sergio Hoyas, H. Alberto Conejero, Javier Garcia-Martinez, Francesco Fuso Nerini, Ricardo Vinuesa</dc:creator>
    </item>
    <item>
      <title>Imperfect Knowledge Management -- A Case Study in a Chilean Manufacturing Company</title>
      <link>https://arxiv.org/abs/2502.01656</link>
      <description>arXiv:2502.01656v1 Announce Type: cross 
Abstract: To conceptualize living systems based on the processes that create them, rather than their interactions with the environment, as in systems theory. Maturana and Varela (1969) at the University of Chile introduced the term autopoiesis (from Greek self and production). This concept emphasizes autonomy as the defining feature of living systems. It describes them as self-sustaining entities that preserve their identity through continuous self-renewal to preserve their unity. Furthermore, these systems can only be understood in reference to themselves, as all internal activities are inherently self-determined by self-production and self-referentiality. This thesis introduces the Fuzzy Autopoietic Knowledge Management (FAKM) model, which integrates the system theory of living systems, the cybernetic theory of viable systems, and the autopoiesis theory of autopoietic systems. The goal is to move beyond traditional knowledge management models that rely on Cartesian dualism (cognition/action) where knowledge is treated as symbolic information processing. Instead, the FAKM model adopts a dualism of organization/structure to define an autopoietic system within a sociotechnical approach. The model is experimentally applied to a manufacturing company in the Maule Region, south of Santiago, Chile.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01656v1</guid>
      <category>cs.DB</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>HAL tel-04583537 , version 1 (22-05-2024)</arxiv:journal_reference>
      <dc:creator>Leoncio Jimenez</dc:creator>
    </item>
    <item>
      <title>Large Language Models' Accuracy in Emulating Human Experts' Evaluation of Public Sentiments about Heated Tobacco Products on Social Media</title>
      <link>https://arxiv.org/abs/2502.01658</link>
      <description>arXiv:2502.01658v1 Announce Type: cross 
Abstract: Sentiment analysis of alternative tobacco products on social media is important for tobacco control research. Large Language Models (LLMs) can help streamline the labor-intensive human sentiment analysis process. This study examined the accuracy of LLMs in replicating human sentiment evaluation of social media messages about heated tobacco products (HTPs).
  The research used GPT-3.5 and GPT-4 Turbo to classify 500 Facebook and 500 Twitter messages, including anti-HTPs, pro-HTPs, and neutral messages. The models evaluated each message up to 20 times, and their majority label was compared to human evaluators.
  Results showed that GPT-3.5 accurately replicated human sentiment 61.2% of the time for Facebook messages and 57.0% for Twitter messages. GPT-4 Turbo performed better, with 81.7% accuracy for Facebook and 77.0% for Twitter. Using three response instances, GPT-4 Turbo achieved 99% of the accuracy of twenty instances. GPT-4 Turbo also had higher accuracy for anti- and pro-HTPs messages compared to neutral ones. Misclassifications by GPT-3.5 often involved anti- or pro-HTPs messages being labeled as neutral or irrelevant, while GPT-4 Turbo showed improvements across all categories.
  In conclusion, LLMs can be used for sentiment analysis of HTP-related social media messages, with GPT-4 Turbo reaching around 80% accuracy compared to human experts. However, there's a risk of misrepresenting overall sentiment due to differences in accuracy across sentiment categories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01658v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwanho Kim, Soojong Kim</dc:creator>
    </item>
    <item>
      <title>Benchmark on Peer Review Toxic Detection: A Challenging Task with a New Dataset</title>
      <link>https://arxiv.org/abs/2502.01676</link>
      <description>arXiv:2502.01676v1 Announce Type: cross 
Abstract: Peer review is crucial for advancing and improving science through constructive criticism. However, toxic feedback can discourage authors and hinder scientific progress. This work explores an important but underexplored area: detecting toxicity in peer reviews. We first define toxicity in peer reviews across four distinct categories and curate a dataset of peer reviews from the OpenReview platform, annotated by human experts according to these definitions. Leveraging this dataset, we benchmark a variety of models, including a dedicated toxicity detection model, a sentiment analysis model, several open-source large language models (LLMs), and two closed-source LLMs. Our experiments explore the impact of different prompt granularities, from coarse to fine-grained instructions, on model performance. Notably, state-of-the-art LLMs like GPT-4 exhibit low alignment with human judgments under simple prompts but achieve improved alignment with detailed instructions. Moreover, the model's confidence score is a good indicator of better alignment with human judgments. For example, GPT-4 achieves a Cohen's Kappa score of 0.56 with human judgments, which increases to 0.63 when using only predictions with a confidence score higher than 95%. Overall, our dataset and benchmarks underscore the need for continued research to enhance toxicity detection capabilities of LLMs. By addressing this issue, our work aims to contribute to a healthy and responsible environment for constructive academic discourse and scientific collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01676v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Man Luo, Bradley Peterson, Rafael Gan, Hari Ramalingame, Navya Gangrade, Ariadne Dimarogona, Imon Banerjee, Phillip Howard</dc:creator>
    </item>
    <item>
      <title>Firewalls to Secure Dynamic LLM Agentic Networks</title>
      <link>https://arxiv.org/abs/2502.01822</link>
      <description>arXiv:2502.01822v1 Announce Type: cross 
Abstract: Future LLM agents are likely to communicate on behalf of users with other entity-representing agents on tasks that entail long-horizon plans with interdependent goals. Current work does not focus on such agentic networks, nor does it address their challenges. Thus, we first identify the required properties of agents' communication, which should be proactive and adaptable. It needs to satisfy 1) privacy: agents should not share more than what is needed for the task, and 2) security: the communication must preserve integrity and maintain utility against selfish entities. We design a use case (travel planning) as a testbed that exemplifies these requirements, and we show examples of how this can go wrong. Next, we propose a practical design, inspired by established network security principles, for constrained LLM agentic networks that balance adaptability, security, and privacy. Our framework automatically constructs and updates task-specific rules from prior simulations to build firewalls. We offer layers of defense to 1) convert free-form input to a task-specific protocol, 2) dynamically abstract users' data to a task-specific degree of permissiveness, and 3) self-correct the agents' trajectory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01822v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sahar Abdelnabi, Amr Gomaa, Eugene Bagdasarian, Per Ola Kristensson, Reza Shokri</dc:creator>
    </item>
    <item>
      <title>Can LLMs Assist Annotators in Identifying Morality Frames? -- Case Study on Vaccination Debate on Social Media</title>
      <link>https://arxiv.org/abs/2502.01991</link>
      <description>arXiv:2502.01991v2 Announce Type: cross 
Abstract: Nowadays, social media is pivotal in shaping public discourse, especially on polarizing issues like vaccination, where diverse moral perspectives influence individual opinions. In NLP, data scarcity and complexity of psycholinguistic tasks, such as identifying morality frames, make relying solely on human annotators costly, time-consuming, and prone to inconsistency due to cognitive load. To address these issues, we leverage large language models (LLMs), which are adept at adapting new tasks through few-shot learning, utilizing a handful of in-context examples coupled with explanations that connect examples to task principles. Our research explores LLMs' potential to assist human annotators in identifying morality frames within vaccination debates on social media. We employ a two-step process: generating concepts and explanations with LLMs, followed by human evaluation using a "think-aloud" tool. Our study shows that integrating LLMs into the annotation process enhances accuracy, reduces task difficulty, lowers cognitive load, suggesting a promising avenue for human-AI collaboration in complex psycholinguistic tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01991v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tunazzina Islam, Dan Goldwasser</dc:creator>
    </item>
    <item>
      <title>ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping</title>
      <link>https://arxiv.org/abs/2502.02072</link>
      <description>arXiv:2502.02072v1 Announce Type: cross 
Abstract: The rapid evolution of Large Language Models (LLMs) has transformed natural language processing but raises critical concerns about biases inherent in their deployment and use across diverse linguistic and sociocultural contexts. This paper presents a framework named ASCenD BDS (Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping). The framework presents approach to detecting bias, discrimination, stereotyping across various categories such as gender, caste, age, disability, socioeconomic status, linguistic variations, etc., using an approach which is Adaptive, Stochastic and Context-Aware. The existing frameworks rely heavily on usage of datasets to generate scenarios for detection of Bias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ. However, such an approach provides point solutions. As a result, these datasets provide a finite number of scenarios for assessment. The current framework overcomes this limitation by having features which enable Adaptability, Stochasticity, Context Awareness. Context awareness can be customized for any nation or culture or sub-culture (for example an organization's unique culture). In this paper, context awareness in the Indian context has been established. Content has been leveraged from Indian Census 2011 to have a commonality of categorization. A framework has been developed using Category, Sub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability, Stochasticity and Context awareness. The framework has been described in detail in Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories were developed by a team of consultants at Saint Fox Consultancy Private Ltd. The concept has been tested out in SFCLabs as part of product development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02072v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajiv Bahl, Venkatesan N, Parimal Aglawe, Aastha Sarasapalli, Bhavya Kancharla, Chaitanya kolukuluri, Harish Mohite, Japneet Hora, Kiran Kakollu, Rahul Diman, Shubham Kapale, Sri Bhagya Kathula, Vamsikrishna Motru, Yogeshwar Reddy</dc:creator>
    </item>
    <item>
      <title>Privacy by Design for Self-Sovereign Identity Systems: An in-depth Component Analysis completed by a Design Assistance Dashboard</title>
      <link>https://arxiv.org/abs/2502.02520</link>
      <description>arXiv:2502.02520v1 Announce Type: cross 
Abstract: The use of Self-Sovereign Identity (SSI) systems for digital identity management is gaining traction and interest. Countries such as Bhutan have already implemented an SSI infrastructure to manage the identity of their citizens. The EU, thanks to the revised eIDAS regulation, is opening the door for SSI vendors to develop SSI systems for the planned EU digital identity wallet. These developments, which fall within the sovereign domain, raise questions about individual privacy.
  The purpose of this article is to help SSI solution designers make informed choices to ensure that the designed solution is privacy-friendly. The observation is that the range of possible solutions is very broad, from DID and DID resolution methods to verifiable credential types, publicly available information (e.g. in a blockchain), type of infrastructure, etc. As a result, the article proposes (1) to group the elementary building blocks of a SSI system into 5 structuring layers, (2) to analyze for each layer the privacy implications of using the chosen building block, and (3) to provide a design assistance dashboard that gives the complete picture of the SSI, and shows the interdependencies between architectural choices and technical building blocks, allowing designers to make informed choices and graphically achieve a SSI solution that meets their need for privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02520v1</guid>
      <category>cs.ET</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Montassar Naghmouchi, Maryline Laurent</dc:creator>
    </item>
    <item>
      <title>Innovating the software engineering class through multi-team development</title>
      <link>https://arxiv.org/abs/2502.02578</link>
      <description>arXiv:2502.02578v1 Announce Type: cross 
Abstract: Often software engineering classes have the student concentrate on designing and planning the project but stop short of actual student team development of code. This leads to criticism by employers of new graduates that they are missing skills in working in teams and coordinating multiple overlapping changes to a code base. Additionally, students that are not actively experiencing team development are unprepared to understand and modify existing legacy-code bases written by others. This paper presents a new approach to teaching undergraduate software engineering that emphasizes not only software engineering methodology but also experiencing development as a member of a team and modifying a legacy code base. Our innovative software engineering course begins with learning the fundamentals of software engineering, followed by examining an existing framework of a social media application. The students are then grouped into multiple software teams, each focusing on a different aspect of the app. The separate teams must define requirements, design, and provide documentation on the services. Using an Agile development approach, the teams incrementally add to the code base and demonstrate features as the application evolves. Subsequent iterations of the class pick up the prior students code base, providing experience working with a legacy code base. Preliminary results of using this approach at the university are presented in this paper including quantitative analysis. Analysis of student software submissions to the cloud-based code repository shows student engagement and contributions over the span of the course. Positive student evaluations show the effectiveness of applying the principles of software engineering to the development of a complex solution in a team environment. Keywords: Software engineering, teaching, college computer science, innovative methods, agile.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02578v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.21125/inted.2023.0503</arxiv:DOI>
      <arxiv:journal_reference>INTED2023 Proceedings. 2023; 1776-1781</arxiv:journal_reference>
      <dc:creator>Allan Brockenbrough</dc:creator>
    </item>
    <item>
      <title>Sustainable Data Management: Indefinite Static Data at Rest with Machine-Readable Printed Optical Data Sheets (MRPODS)</title>
      <link>https://arxiv.org/abs/2312.10275</link>
      <description>arXiv:2312.10275v2 Announce Type: replace 
Abstract: In an era where both commercial and private sectors place a premium on the longevity of digital data storage, the imperative to bolster resilience of digital information while simultaneously curbing costs and reducing failure rates becomes paramount. This study delves into the unique attributes of optical encoding methodologies, which are poised to offer enduring stability for digital data. Despite their promising potential, there remains a notable dearth of comprehensive analyses comparing various optical encoding techniques in terms of their durability. This research is thus dedicated to exploring the financial and environmental implications of employing technology to transcribe digital data into a machine-readable optical format, assessing both the advantages and limitations inherent in this approach. Our empirical findings reveal a marked increase in the efficiency of machine-readable optical encoding over conventional digital storage methods, particularly as the volume of data diminishes and the expected lifespan of storage extends indefinitely. This paper aims to illuminate key aspects of long-term digital data storage within business contexts, focusing on aspects such as cost, dependability, legibility, and confidentiality of optically encoded digital information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10275v2</guid>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ray Doll</dc:creator>
    </item>
    <item>
      <title>BadRobot: Jailbreaking Embodied LLMs in the Physical World</title>
      <link>https://arxiv.org/abs/2407.20242</link>
      <description>arXiv:2407.20242v4 Announce Type: replace 
Abstract: Embodied AI represents systems where AI is integrated into physical entities. Large Language Model (LLM), which exhibits powerful language understanding abilities, has been extensively employed in embodied AI by facilitating sophisticated task planning. However, a critical safety issue remains overlooked: could these embodied LLMs perpetrate harmful behaviors? In response, we introduce BadRobot, a novel attack paradigm aiming to make embodied LLMs violate safety and ethical constraints through typical voice-based user-system interactions. Specifically, three vulnerabilities are exploited to achieve this type of attack: (i) manipulation of LLMs within robotic systems, (ii) misalignment between linguistic outputs and physical actions, and (iii) unintentional hazardous behaviors caused by world knowledge's flaws. Furthermore, we construct a benchmark of various malicious physical action queries to evaluate BadRobot's attack performance. Based on this benchmark, extensive experiments against existing prominent embodied LLM frameworks (e.g., Voxposer, Code as Policies, and ProgPrompt) demonstrate the effectiveness of our BadRobot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20242v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>International Conference on Learning Representations (ICLR) 2025</arxiv:journal_reference>
      <dc:creator>Hangtao Zhang, Chenyu Zhu, Xianlong Wang, Ziqi Zhou, Changgan Yin, Minghui Li, Lulu Xue, Yichen Wang, Shengshan Hu, Aishan Liu, Peijin Guo, Leo Yu Zhang</dc:creator>
    </item>
    <item>
      <title>Mental-Perceiver: Audio-Textual Multi-Modal Learning for Estimating Mental Disorders</title>
      <link>https://arxiv.org/abs/2408.12088</link>
      <description>arXiv:2408.12088v2 Announce Type: replace 
Abstract: Mental disorders, such as anxiety and depression, have become a global concern that affects people of all ages. Early detection and treatment are crucial to mitigate the negative effects these disorders can have on daily life. Although AI-based detection methods show promise, progress is hindered by the lack of publicly available large-scale datasets. To address this, we introduce the Multi-Modal Psychological assessment corpus (MMPsy), a large-scale dataset containing audio recordings and transcripts from Mandarin-speaking adolescents undergoing automated anxiety/depression assessment interviews. MMPsy also includes self-reported anxiety/depression evaluations using standardized psychological questionnaires. Leveraging this dataset, we propose Mental-Perceiver, a deep learning model for estimating mental disorders from audio and textual data. Extensive experiments on MMPsy and the DAIC-WOZ dataset demonstrate the effectiveness of Mental-Perceiver in anxiety and depression detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12088v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinghui Qin, Changsong Liu, Tianchi Tang, Dahuang Liu, Minghao Wang, Qianying Huang, Rumin Zhang</dc:creator>
    </item>
    <item>
      <title>Impact of Stricter Content Moderation on Parler's Users' Discourse</title>
      <link>https://arxiv.org/abs/2310.08844</link>
      <description>arXiv:2310.08844v2 Announce Type: replace-cross 
Abstract: Social media platforms employ various content moderation techniques to remove harmful, offensive, and hate speech content. The moderation level varies across platforms; even over time, it can evolve in a platform. For example, Parler, a fringe social media platform popular among conservative users, was known to have the least restrictive moderation policies, claiming to have open discussion spaces for their users. However, after linking the 2021 US Capitol Riots and the activity of some groups on Parler, such as QAnon and Proud Boys, on January 12, 2021, Parler was removed from the Apple and Google App Store and suspended from Amazon Cloud hosting service. Parler would have to modify their moderation policies to return to these online stores. After a month of downtime, Parler was back online with a new set of user guidelines, which reflected stricter content moderation, especially regarding the \emph{hate speech} policy. In this paper, we studied the moderation changes performed by Parler and their effect on the toxicity of its content. We collected a large longitudinal Parler dataset with 17M parleys from 432K active users from February 2021 to January 2022, after its return to the Internet and App Store. To the best of our knowledge, this is the first study investigating the effectiveness of content moderation techniques using data-driven approaches and also the first Parler dataset after its brief hiatus. Our quasi-experimental time series analysis indicates that after the change in Parler's moderation, the severe forms of toxicity (above a threshold of 0.5) immediately decreased and sustained. In contrast, the trend did not change for less severe threats and insults (a threshold between 0.5 - 0.7). Finally, we found an increase in the factuality of the news sites being shared, as well as a decrease in the number of conspiracy or pseudoscience sources being shared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08844v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nihal Kumarswamy, Mohit Singhal, Shirin Nilizadeh</dc:creator>
    </item>
    <item>
      <title>Auction-Based Regulation for Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2410.01871</link>
      <description>arXiv:2410.01871v2 Announce Type: replace-cross 
Abstract: In an era of "moving fast and breaking things", regulators have moved slowly to pick up the safety, bias, and legal debris left in the wake of broken Artificial Intelligence (AI) deployment. While there is much-warranted discussion about how to address the safety, bias, and legal woes of state-of-the-art AI models, rigorous and realistic mathematical frameworks to regulate AI are lacking. Our paper addresses this challenge, proposing an auction-based regulatory mechanism that provably incentivizes devices (i) to deploy compliant models and (ii) to participate in the regulation process. We formulate AI regulation as an all-pay auction where enterprises submit models for approval. The regulator enforces compliance thresholds and further rewards models exhibiting higher compliance than their peers. We derive Nash Equilibria demonstrating that rational agents will submit models exceeding the prescribed compliance threshold. Empirical results show that our regulatory auction boosts compliance rates by 20% and participation rates by 15% compared to baseline regulatory mechanisms, outperforming simpler frameworks that merely impose minimum compliance standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01871v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Bornstein, Zora Che, Suhas Julapalli, Abdirisak Mohamed, Amrit Singh Bedi, Furong Huang</dc:creator>
    </item>
    <item>
      <title>What constitutes a Deep Fake? The blurry line between legitimate processing and manipulation under the EU AI Act</title>
      <link>https://arxiv.org/abs/2412.09961</link>
      <description>arXiv:2412.09961v2 Announce Type: replace-cross 
Abstract: When does a digital image resemble reality? The relevance of this question increases as the generation of synthetic images -- so called deep fakes -- becomes increasingly popular. Deep fakes have gained much attention for a number of reasons -- among others, due to their potential to disrupt the political climate. In order to mitigate these threats, the EU AI Act implements specific transparency regulations for generating synthetic content or manipulating existing content. However, the distinction between real and synthetic images is -- even from a computer vision perspective -- far from trivial. We argue that the current definition of deep fakes in the AI act and the corresponding obligations are not sufficiently specified to tackle the challenges posed by deep fakes. By analyzing the life cycle of a digital photo from the camera sensor to the digital editing features, we find that: (1.) Deep fakes are ill-defined in the EU AI Act. The definition leaves too much scope for what a deep fake is. (2.) It is unclear how editing functions like Google's ``best take'' feature can be considered as an exception to transparency obligations. (3.) The exception for substantially edited images raises questions about what constitutes substantial editing of content and whether or not this editing must be perceptible by a natural person. Our results demonstrate that complying with the current AI Act transparency obligations is difficult for providers and deployers. As a consequence of the unclear provisions, there is a risk that exceptions may be either too broad or too limited. We intend our analysis to foster the discussion on what constitutes a deep fake and to raise awareness about the pitfalls in the current AI Act transparency obligations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09961v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kristof Meding, Christoph Sorge</dc:creator>
    </item>
    <item>
      <title>Gender Bias and Property Taxes</title>
      <link>https://arxiv.org/abs/2412.12610</link>
      <description>arXiv:2412.12610v2 Announce Type: replace-cross 
Abstract: Gender bias distorts the economic behavior and outcomes of women and households. We investigate gender biases in property taxes. We analyze records of more than 100,000 property tax appeal hearings and more than 2.7 years of associated audio recordings, considering how panelist and appellant genders associate with hearing outcomes. We first observe that female appellants fare systematically worse than male appellants in their hearings. Second, we show that, whereas male appellants' hearing outcomes do not vary meaningfully with the gender composition of the panel they face, those of female appellants' do, such that female appellants obtain systematically lesser (greater) reductions to their home values when facing female (male) panelists. Employing a multi-modal large language model (M-LLM), we next construct measures of participant behavior and tone from hearing audio recordings. We observe markedly different behaviors between male and female appellants and, in the case of male appellants, we find that these differences also depend on the gender of the panelists they face (e.g., male appellants appear to behave systematically more aggressively towards female panelists). In contrast, the behavior of female appellants remains relatively constant, regardless of their panel's gender. Finally, we show that female appellants continue to fare worse in front of female panels, even when we condition upon the appelant's in-hearing behavior and tone. Our results are thus consistent with the idea that gender biases are driven, at least in part, by unvoiced beliefs and perceptions on the part of ARB panelists. Our study documents the presence of gender biases in property appraisal appeal hearings and highlights the potential value of generative AI for analyzing large-scale, unstructured administrative data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12610v2</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gordon Burtch, Alejandro Zentner</dc:creator>
    </item>
    <item>
      <title>Embracing Dialectic Intersubjectivity: Coordination of Different Perspectives in Content Analysis with LLM Persona Simulation</title>
      <link>https://arxiv.org/abs/2502.00903</link>
      <description>arXiv:2502.00903v2 Announce Type: replace-cross 
Abstract: This study attempts to advancing content analysis methodology from consensus-oriented to coordination-oriented practices, thereby embracing diverse coding outputs and exploring the dynamics among differential perspectives. As an exploratory investigation of this approach, we evaluate six GPT-4o configurations to analyze sentiment in Fox News and MSNBC transcripts on Biden and Trump during the 2020 U.S. presidential campaign, examining patterns across these models. By assessing each model's alignment with ideological perspectives, we explore how partisan selective processing could be identified in LLM-Assisted Content Analysis (LACA). Findings reveal that partisan persona LLMs exhibit stronger ideological biases when processing politically congruent content. Additionally, intercoder reliability is higher among same-partisan personas compared to cross-partisan pairs. This approach enhances the nuanced understanding of LLM outputs and advances the integrity of AI-driven social science research, enabling simulations of real-world implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00903v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 05 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Taewoo Kang, Kjerstin Thorson, Tai-Quan Peng, Dan Hiaeshutter-Rice, Sanguk Lee, Stuart Soroka</dc:creator>
    </item>
  </channel>
</rss>
