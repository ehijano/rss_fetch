<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Jun 2025 01:53:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the Impact of Monte Carlo Statistical Uncertainty on Surrogate-based Design Optimization</title>
      <link>https://arxiv.org/abs/2506.00018</link>
      <description>arXiv:2506.00018v1 Announce Type: new 
Abstract: In multi-objective design tasks, the computational cost increases rapidly when high-fidelity simulations are used to evaluate objective functions. Surrogate models help mitigate this cost by approximating the simulation output, simplifying the design process. However, under high uncertainty, surrogate models trained on noisy data can produce inaccurate predictions, as their performance depends heavily on the quality of training data. This study investigates the impact of data uncertainty on two multi-objective design problems modelled using Monte Carlo transport simulations: a neutron moderator and an ion-to-neutron converter. For each, a grid search was performed using five different tally uncertainty levels to generate training data for neural network surrogate models. These models were then optimized using NSGA-III. The recovered Pareto-fronts were analyzed across uncertainty levels, and the impact of training data quality on optimization outcomes was quantified. Average simulation times were also compared to evaluate the trade-off between accuracy and computational cost. Results show that the influence of simulation uncertainty is strongly problem-dependent. In the neutron moderator case, higher uncertainties led to exaggerated objective sensitivities and distorted Pareto-fronts, reducing normalized hypervolume. In contrast, the ion-to-neutron converter task was less affected--low-fidelity simulations produced results similar to those from high-fidelity data. These findings suggest that a fixed-fidelity approach is not optimal. Surrogate models can still recover the Pareto-front under noisy conditions, and multi-fidelity studies can help identify the appropriate uncertainty level for each problem, enabling better trade-offs between computational efficiency and optimization accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00018v1</guid>
      <category>stat.AP</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Omer F. Erdem, David P. Broughton, Josef Svoboda, Chengkun Huang, Majdi I. Radaideh</dc:creator>
    </item>
    <item>
      <title>Learning Spatio-Temporal Vessel Behavior using AIS Trajectory Data and Markovian Models in the Gulf of St. Lawrence</title>
      <link>https://arxiv.org/abs/2506.00025</link>
      <description>arXiv:2506.00025v1 Announce Type: new 
Abstract: Maritime Mobility is at the center of the global economy, and analyzing and understanding such data at scale is critical for ocean conservation and governance. Accordingly, this work introduces a spatio-temporal analytical framework based on discrete-time Markov chains to analyze vessel movement patterns in the Gulf of St. Lawrence, emphasizing changes induced during the COVID-19 pandemic. We discretize the ocean space into hexagonal cells and construct mobility signatures for individual vessel types using the frequency of cell transitions and the dwell time within each cell. These features are used to build origin-destination matrices and spatial transition probability models that characterize vessel dynamics at different temporal resolutions. Under multiple vessel types, we contribute with a temporal evolution analysis of mobility patterns during pandemic times, highlighting significant but transient changes to recurring transportation behaviors. Our findings indicate vessel-specific mobility signatures consistent across spatially disjoint regions, suggesting that those are latent behavioral invariants. Besides, we observe significant temporal deviations among passenger and fishing vessels during the pandemic, indicating a strong influence of social isolation policies and operational limitations imposed on non-essential maritime activity in this region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00025v1</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>math.PR</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Spadon, Ruixin Song, Vaishnav Vaidheeswaran, Md Mahbub Alam, Floris Goerlandt, Ronald Pelot</dc:creator>
    </item>
    <item>
      <title>Deriving Production Functions in Economics Through Data-Driven Dynamical Systems</title>
      <link>https://arxiv.org/abs/2506.00032</link>
      <description>arXiv:2506.00032v1 Announce Type: new 
Abstract: In their seminal 1928 work, Charles Cobb and Paul Douglas empirically validated the Cobb-Douglas production function through statistical analysis of U.S. economic data from 1899 to 1923. While this established the function's theoretical foundation for growth models like Solow-Swan and its extensions, it simultaneously revealed a fundamental limitation: their methodology could not determine whether alternative production functions might equally explain the observed data.
  This paper presents a novel dynamical systems approach to production function estimation. By modeling economic growth trajectories as dynamical systems, we derive production functions as time-independent invariants -- a method that systematically generates all possible functional forms compatible with observed data.
  Applying this framework to Cobb and Douglas's original dataset yields two key results: First, we demonstrate that the Cobb-Douglas form emerges naturally from exponential growth dynamics in labor, capital, and production. Second, we show how combining fundamental invariants of this exponential system generates the CES production function as a special case. Our methodology bridges statistical analysis with mathematical systems theory, providing both a verification mechanism for classical results and a tool for discovering new functional relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00032v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roman G. Smirnov</dc:creator>
    </item>
    <item>
      <title>Probabilistic Spatial Interpolation of Sparse Data using Diffusion Models</title>
      <link>https://arxiv.org/abs/2506.00033</link>
      <description>arXiv:2506.00033v1 Announce Type: new 
Abstract: The large underlying assumption of climate models today relies on the basis of a "confident" initial condition, a reasonably plausible snapshot of the Earth for which all future predictions depend on. However, given the inherently chaotic nature of our system, this assumption is complicated by sensitive dependence, where small uncertainties in initial conditions can lead to exponentially diverging outcomes over time. This challenge is particularly salient at global spatial scales and over centennial timescales, where data gaps are not just common but expected. The source of uncertainty is two-fold: (1) sparse, noisy observations from satellites and ground stations, and (2) internal variability stemming from the simplifying approximations within the models themselves.
  In practice, data assimilation methods are used to reconcile this missing information by conditioning model states on partial observations. Our work builds on this idea but operates at the extreme end of sparsity. We propose a conditional data imputation framework that reconstructs full temperature fields from as little as 1% observational coverage. The method leverages a diffusion model guided by a prekriged mask, effectively inferring the full-state fields from minimal data points. We validate our framework over the Southern Great Plains, focusing on afternoon (12:00-6:00 PM) temperature fields during the summer months of 2018-2020. Across varying observational densities--from swath data to isolated in-situ sensors--our model achieves strong reconstruction accuracy, highlighting its potential to fill in critical data gaps in both historical reanalysis and real-time forecasting pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00033v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valerie Tsao, Nathaniel W. Chaney, Manolis Veveakis</dc:creator>
    </item>
    <item>
      <title>Probabilistic intraday electricity price forecasting using generative machine learning</title>
      <link>https://arxiv.org/abs/2506.00044</link>
      <description>arXiv:2506.00044v1 Announce Type: new 
Abstract: The growing importance of intraday electricity trading in Europe calls for improved price forecasting and tailored decision-support tools. In this paper, we propose a novel generative neural network model to generate probabilistic path forecasts for intraday electricity prices and use them to construct effective trading strategies for Germany's continuous-time intraday market. Our method demonstrates competitive performance in terms of statistical evaluation metrics compared to two state-of-the-art statistical benchmark approaches. To further assess its economic value, we consider a realistic fixed-volume trading scenario and propose various strategies for placing market sell orders based on the path forecasts. Among the different trading strategies, the price paths generated by our generative model lead to higher profit gains than the benchmark methods. Our findings highlight the potential of generative machine learning tools in electricity price forecasting and underscore the importance of economic evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00044v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jieyu Chen, Sebastian Lerch, Melanie Schienle, Tomasz Serafin, Rafa{\l} Weron</dc:creator>
    </item>
    <item>
      <title>Assessing Climate-Driven Mortality Risk: A Stochastic Approach with Distributed Lag Non-Linear Models</title>
      <link>https://arxiv.org/abs/2506.00561</link>
      <description>arXiv:2506.00561v1 Announce Type: new 
Abstract: Assessing climate-driven mortality risk has become an emerging area of research in recent decades. In this paper, we propose a novel approach to explicitly incorporate climate-driven effects into both single- and multi-population stochastic mortality models. The new model consists of two components: a stochastic mortality model, and a distributed lag non-linear model (DLNM). The first component captures the non-climate long-term trend and volatility in mortality rates. The second component captures non-linear and lagged effects of climate variables on mortality, as well as the impact of heat waves and cold waves across different age groups. For model calibration, we propose a backfitting algorithm that allows us to disentangle the climate-driven mortality risk from the non-climate-driven stochastic mortality risk. We illustrate the effectiveness and superior performance of our model using data from three European regions: Athens, Lisbon, and Rome. Furthermore, we utilize future UTCI data generated from climate models to provide mortality projections into 2045 across these regions under two Representative Concentration Pathway (RCP) scenarios. The projections show a noticeable decrease in winter mortality alongside a rise in summer mortality, driven by a general increase in UTCI over time. Although we expect slightly lower overall mortality in the short term under RCP8.5 compared to RCP2.6, a long-term increase in total mortality is anticipated under the RCP8.5 scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00561v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiacheng Min, Han Li, Thomas Nagler, Shuanming Li</dc:creator>
    </item>
    <item>
      <title>Assessing Honey Bee Colony Health Using Temperature Time Series</title>
      <link>https://arxiv.org/abs/2506.00602</link>
      <description>arXiv:2506.00602v1 Announce Type: new 
Abstract: Honey bees face an increasing number of stressors that disrupt the natural behaviour of colonies and, in extreme cases, can lead to their collapse. Quantifying the status and resilience of colonies is essential to measure the impact of stressors and to identify colonies at risk. In this manuscript, we present and apply new methodologies to efficiently diagnose the status of a honey bee colony from widely available time series of hive and environmental temperature. Healthy hives have a remarkable ability to control temperature near the brood area. Our method exploits this fact and quantifies the status of a hive by measuring how resilient they are to extreme environmental temperatures, which act as natural stressors. Analysing 22 hives during different times of the year, including 3 hives that collapsed, we find the statistical signatures of stress that reveal whether honeybees are doing well or are at risk of failure. Based on these analyses, we propose a simple scale of hive status (stable, warning, and collapse) that can be determined based on a few temperature measurements. Our approach offers a lower-cost and practical bee-monitoring solution, providing a non-invasive way to track hive conditions and trigger interventions to save the hives from collapse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00602v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karina Arias-Calluari, Theotime Colin, Tanya Latty, Mary Myerscough, Eduardo G. Altmann</dc:creator>
    </item>
    <item>
      <title>Understanding the European energy crisis through structural causal models</title>
      <link>https://arxiv.org/abs/2506.00680</link>
      <description>arXiv:2506.00680v1 Announce Type: new 
Abstract: Natural gas supplies in Europe were disrupted and energy prices soared in the context of Russia's invasion of Ukraine. Electricity prices in France experienced the largest relative increase among European countries, even though natural gas plays a negligible role in the French electricity system. In this article, we demonstrate the importance of causal statistical methods and propose causal graphs to investigate the French electricity market and pinpoint key influencing factors on electricity prices and net exports. We demonstrate that a causal approach resolves paradoxical results of simple correlation studies and enables a quantitative analysis of indirect causal effects. We introduce a linear structural causal model as well as non-linear tree-based machine learning combined with Shapley flows. The models elucidate the interplay of gas prices and the unavailability of nuclear power plants during the energy crisis: The high unavailability made France dependent on imports and linked prices to neighbouring countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00680v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Tausendfreund, Sarah Schreyer, Florian Immig, Ulrich Oberhofer, Julius Trebbien, Aaron Praktiknjo, Benjamin Sch\"afer, Dirk Witthaut</dc:creator>
    </item>
    <item>
      <title>Pluri-Gaussian rapid updating of geological domains</title>
      <link>https://arxiv.org/abs/2506.01575</link>
      <description>arXiv:2506.01575v1 Announce Type: new 
Abstract: Over the past decade, the rapid updating of resource knowledge and the integration of real-time sensor information have garnered attention in both industry and academia. However, most studies on rapid resource model updating have focused on continuous variables, such as grade variables and coal quality parameters. Geological domain modelling is an essential component of resource estimation, which is why it is crucial to extend data assimilation techniques to enable the rapid updating of categorical variables. In this paper, a methodology inspired by pluri-Gaussian simulation is proposed for near-real-time updating of geological domains, followed by the updating of grade variables within these domain boundaries. The proposed algorithm consists of a Gibbs sampler for converting geological domains into Gaussian random fields, an ensemble Kalman filter with multiple data assimilations (EnKF-MDA) for rapid updating, and rotation based iterative Gaussianisation (RBIG) for multi-Gaussian transformation. We demonstrate the algorithm using a synthetic case study with observations sampled from the ground truth, as well as a real case study that uses production drilling samples for joint updating of geological domains and grade variables. Both case studies are based on real data from an iron oxide-copper-gold deposit in South Australia. This approach enhances resource knowledge by incorporating both categorical and continuous variables, leading to improved reproduction of domain geometries, closer matches between predictions and observations, and more geologically realistic resource models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01575v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sultan Abulkhair, Peter Dowd, Chaoshui Xu</dc:creator>
    </item>
    <item>
      <title>Spillovers and Effect Attenuation in Firearm Policy Research in the United States</title>
      <link>https://arxiv.org/abs/2506.01695</link>
      <description>arXiv:2506.01695v1 Announce Type: new 
Abstract: In the United States, firearm-related deaths and injuries are a major public health issue. Because of limited federal action, state policies are particularly important, and their evaluation informs the actions of other policymakers. The movement of firearms across state and local borders, however, can undermine the effectiveness of these policies and have statistical consequences for their empirical evaluation. This movement causes spillover and bypass effects of policies, wherein interventions affect nearby control states and the lack of intervention in nearby states reduces the effectiveness in the intervention states. While some causal inference methods exist to account for spillover effects and reduce bias, these do not necessarily align well with the data available for firearm research or with the most policy-relevant estimands. Integrated data infrastructure and new methods are necessary for a better understanding of the effects these policies would have if widely adopted. In the meantime, appropriately understanding and interpreting effect estimates from quasi-experimental analyses is crucial for ensuring that effective policies are not dismissed due to these statistical challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01695v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lee Kennedy-Shaffer, Alan Hamilton Kennedy</dc:creator>
    </item>
    <item>
      <title>Modelling benthic animals in space and time using Bayesian Point Process with cross validation: the case of Holoturians</title>
      <link>https://arxiv.org/abs/2506.01763</link>
      <description>arXiv:2506.01763v2 Announce Type: new 
Abstract: Understanding the spatial distribution of Holothurians is an essential task for ecosystem monitoring and sustainable management, particularly in the Mediterranean habitats. However, species distribution modeling is often complicated by the presence-only nature of the data and heterogeneous sampling designs. This study develops a spatio-temporal framework based on Log-Gaussian Cox Processes to analyze Holothurians' positions collected across nine survey campaigns conducted from 2022 to 2024 near Giglio Island, Italy. The surveys combined high-resolution photogrammetry with diver-based visual censuses, leading to varying detection probabilities across habitats, especially within Posidonia oceanica meadows. We adopt a model with a shared spatial Gaussian process component to accommodate this complexity, accounting for habitat structure, environmental covariates, and temporal variability. Model estimation is performed using Integrated Nested Laplace Approximation. We evaluate the predictive performances of alternative model specifications through a novel k-fold cross-validation strategy for point processes, using the Continuous Ranked Probability Score. Our approach provides a flexible and computationally efficient framework for integrating heterogeneous presence-only data in marine ecology and comparing the predictive ability of alternative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01763v2</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Poggio, Gian Mario Sangiovanni, Gianluca Mastrantonio, Giovanna Jona Lasinio, Edoardo Casoli, Stefano Moro, Daniele Ventura</dc:creator>
    </item>
    <item>
      <title>Improving statistical learning methods via features selection without replacement sampling and random projection</title>
      <link>https://arxiv.org/abs/2506.00053</link>
      <description>arXiv:2506.00053v1 Announce Type: cross 
Abstract: Cancer is fundamentally a genetic disease characterized by genetic and epigenetic alterations that disrupt normal gene expression, leading to uncontrolled cell growth and metastasis. High-dimensional microarray datasets pose challenges for classification models due to the "small n, large p" problem, resulting in overfitting. This study makes three different key contributions: 1) we propose a machine learning-based approach integrating the Feature Selection Without Re-placement (FSWOR) technique and a projection method to improve classification accuracy. 2) We apply the Kendall statistical test to identify the most significant genes from the brain cancer mi-croarray dataset (GSE50161), reducing the feature space from 54,675 to 20,890 genes.3) we apply machine learning models using k-fold cross validation techniques in which our model incorpo-rates ensemble classifiers with LDA projection and Na\"ive Bayes, achieving a test score of 96%, outperforming existing methods by 9.09%. The results demonstrate the effectiveness of our ap-proach in high-dimensional gene expression analysis, improving classification accuracy while mitigating overfitting. This study contributes to cancer biomarker discovery, offering a robust computational method for analyzing microarray data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00053v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sulaiman khan, Muhammad Ahmad, Fida Ullah, Carlos Aguilar Iba\~nez, Jos\'e Eduardo Valdez Rodriguez</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian Knowledge Tracing in Undergraduate Engineering Education</title>
      <link>https://arxiv.org/abs/2506.00057</link>
      <description>arXiv:2506.00057v1 Announce Type: cross 
Abstract: Educators teaching entry-level university engineering modules face the challenge of identifying which topics students find most difficult and how to support diverse student needs effectively. This study demonstrates a rigorous yet interpretable statistical approach -- hierarchical Bayesian modeling -- that leverages detailed student response data to quantify both skill difficulty and individual student abilities. Using a large-scale dataset from an undergraduate Statics course, we identified clear patterns of skill mastery and uncovered distinct student subgroups based on their learning trajectories. Our analysis reveals that certain concepts consistently present challenges, requiring targeted instructional support, while others are readily mastered and may benefit from enrichment activities. Importantly, the hierarchical Bayesian method provides educators with intuitive, reliable metrics without sacrificing predictive accuracy. This approach allows for data-informed decisions, enabling personalized teaching strategies to improve student engagement and success. By combining robust statistical methods with clear interpretability, this study equips educators with actionable insights to better support diverse learner populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00057v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiwei Sun</dc:creator>
    </item>
    <item>
      <title>Constrained Bayesian Optimization under Bivariate Gaussian Process with Application to Cure Process Optimization</title>
      <link>https://arxiv.org/abs/2506.00174</link>
      <description>arXiv:2506.00174v1 Announce Type: cross 
Abstract: Bayesian Optimization, leveraging Gaussian process models, has proven to be a powerful tool for minimizing expensive-to-evaluate objective functions by efficiently exploring the search space. Extensions such as constrained Bayesian Optimization have further enhanced Bayesian Optimization's utility in practical scenarios by focusing the search within feasible regions defined by a black-box constraint function. However, constrained Bayesian Optimization in is developed based on the independence Gaussian processes assumption between objective and constraint functions, which may not hold in real-world applications. To address this issue, we use the bivariate Gaussian process model to characterize the dependence between the objective and constraint functions and developed the constrained expected improvement acquisition function under this model assumption. We show case the performance of the proposed approach with an application to cure process optimization in Manufacturing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00174v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yezhuo Li, Qiong Zhang, Madhura Limaye, Gang Li</dc:creator>
    </item>
    <item>
      <title>Improved Risk Ratio Approximation by Complementary Log-Log Models: A Comparison with Logistic Models</title>
      <link>https://arxiv.org/abs/2506.00889</link>
      <description>arXiv:2506.00889v1 Announce Type: cross 
Abstract: Odds ratios obtained from logistic models fail to approximate risk ratios with common outcomes, leading to potential misinterpretations about exposure effects by practitioners. This article investigates the complementary log-log models as a practical alternative to produce risk ratio approximation. We demonstrate that the corresponding effect measure of complementary log-log models, called the complementary log ratio in this article, consistently provides a closer approximation to risk ratios than odds ratios. To compare the approximation accuracy, we adopt the one-parameter Aranda-Ordaz family of link functions, which includes both the logit and complementary log-log link functions as special cases. Within this unified framework, we implement a theoretical comparison of approximation accuracy between the complementary log ratio and the odds ratio, showing that the former always produces smaller approximation bias. Simulation studies further reinforce our theoretical findings. Given that the complementary log-log model is easily implemented in standard statistical software such as R and SAS, we encourage more frequent use of this model as a simple and effective alternative to logistic models when the goal is to approximate risk ratios more accurately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00889v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuji Tsubota, Kenji Beppu</dc:creator>
    </item>
    <item>
      <title>Modeling temporal hypergraphs</title>
      <link>https://arxiv.org/abs/2506.01408</link>
      <description>arXiv:2506.01408v1 Announce Type: cross 
Abstract: Networks representing social, biological, technological or other systems are often characterized by higher-order interaction involving any number of nodes. Temporal hypergraphs are given by ordered sequences of hyperedges representing sets of nodes interacting at given points in time. In this paper we discuss how a recently proposed model family for time-stamped hyperedges - relational hyperevent models (RHEM) - can be employed to define tailored null distributions for temporal hypergraphs. RHEM can be specified with a given vector of temporal hyperedge statistics - functions that quantify the structural position of hyperedges in the history of previous hyperedges - and equate expected values of these statistics with their empirically observed values. This allows, for instance, to analyze the overrepresentation or underrepresentation of temporal hyperedge configurations in a model that reproduces the observed distributions of possibly complex sub-configurations, including but going beyond node degrees. Concrete examples include, but are not limited to, preferential attachment, repetition of subsets of any given size, triadic closure, homophily, and degree assortativity for subsets of any order.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01408v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\"urgen Lerner, Marian-Gabriel H\^ancean, Matjaz Perc</dc:creator>
    </item>
    <item>
      <title>Characterization based Goodness-of-Fit for Generalized Pareto Distribution: A Blend of Stein's Identity and Dynamic Survival Extropy</title>
      <link>https://arxiv.org/abs/2506.01473</link>
      <description>arXiv:2506.01473v1 Announce Type: cross 
Abstract: This paper proposes a goodness of fit test for the generalized Pareto distribution (GPD). Firstly, we provide two characterizations of GPD based on Stein's identity and dynamic survival extropy. These characterizations are used to test GPD separately for the positive and negative shape parameter cases. A Monte Carlo simulation is conducted to provide the critical values and power of the proposed test against a good number of alternatives. Our test is simple to use and it has asymptotic normality and relatively high power, which strengthened the purpose of proposing it. Considering the case of right censored data, we provide the procedure to handle censored case too. A few real-life applications are also included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01473v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaurav Kandpal, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>A nonparametric statistical method for deconvolving densities in the analysis of proteomic data</title>
      <link>https://arxiv.org/abs/2506.01540</link>
      <description>arXiv:2506.01540v1 Announce Type: cross 
Abstract: In medical research, often, genomic or proteomic data are collected, with measurements frequently subject to uncertainties or errors, making it crucial to accurately separate the signals of the genes or proteins, respectively, from the noise. Such a signal separation is also of interest in skin aging research in which intrinsic aging driven by genetic factors and extrinsic, i.e.\ environmentally induced, aging are investigated by considering, e.g., the proteome of skin fibroblasts. Since extrinsic influences on skin aging can only be measured alongside intrinsic ones, it is essential to isolate the pure extrinsic signal from the combined intrinisic and extrinsic signal. In such situations, deconvolution methods can be employed to estimate the signal's density function from the data. However, existing nonparametric deconvolution approaches often fail when the variance of the mixed distribution is substantially greater than the variance of the target distribution, which is a common issue in genomic and proteomic data.
  We, therefore, propose a new nonparametric deconvolution method called N-Power Fourier Deconvolution (NPFD) that addresses this issue by employing the $N$-th power of the Fourier transform of transformed densities. This procedure utilizes the Fourier transform inversion theorem and exploits properties of Fourier transforms of density functions to mitigate numerical inaccuracies through exponentiation, leading to accurate and smooth density estimation. An extensive simulation study demonstrates that NPFD effectively handles the variance issues and performs comparably or better than existing deconvolution methods in most scenarios. Moreover, applications to real medical data, particularly to proteomic data from fibroblasts affected by intrinsic and extrinsic aging, show how NPFD can be employed to estimate the pure extrinsic density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01540v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akin Anarat, Jean Krutmann, Holger Schwender</dc:creator>
    </item>
    <item>
      <title>Stock Market Telepathy: Graph Neural Networks Predicting the Secret Conversations between MINT and G7 Countries</title>
      <link>https://arxiv.org/abs/2506.01945</link>
      <description>arXiv:2506.01945v1 Announce Type: cross 
Abstract: Emerging economies, particularly the MINT countries (Mexico, Indonesia, Nigeria, and T\"urkiye), are gaining influence in global stock markets, although they remain susceptible to the economic conditions of developed countries like the G7 (Canada, France, Germany, Italy, Japan, the United Kingdom, and the United States). This interconnectedness and sensitivity of financial markets make understanding these relationships crucial for investors and policymakers to predict stock price movements accurately. To this end, we examined the main stock market indices of G7 and MINT countries from 2012 to 2024, using a recent graph neural network (GNN) algorithm called multivariate time series forecasting with graph neural network (MTGNN). This method allows for considering complex spatio-temporal connections in multivariate time series. In the implementations, MTGNN revealed that the US and Canada are the most influential G7 countries regarding stock indices in the forecasting process, and Indonesia and T\"urkiye are the most influential MINT countries. Additionally, our results showed that MTGNN outperformed traditional methods in forecasting the prices of stock market indices for MINT and G7 countries. Consequently, the study offers valuable insights into economic blocks' markets and presents a compelling empirical approach to analyzing global stock market dynamics using MTGNN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01945v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nurbanu Bursa</dc:creator>
    </item>
    <item>
      <title>A Bayesian Inference Approach for Reducing Inter-Investigator Variability in Sampling-Based Land Cover Classification</title>
      <link>https://arxiv.org/abs/2403.15720</link>
      <description>arXiv:2403.15720v3 Announce Type: replace 
Abstract: Land cover classification faces persistent challenges with inter-investigator variability and salt-and-pepper noise. Although cloud platforms such as Google Earth Engine have made land cover classification more accessible, these issues persist, particularly when multiple investigators contribute to the process. This study developed a robust classification approach that integrates unsupervised clustering of investigator maps with a Bayesian inference framework using Dirichlet distributions. In this study, 44 investigators collected stratified reference samples across four land cover classes using point-based visual interpretation in Saitama City, Japan. We trained three different classifiers, Random Forests (RF), Support Vector Machines (SVM), and Single hidden layer Feed-forward Neural Networks (SFNN), and enhanced the system by implementing unsupervised clustering (k-Means or k-Medoids) to group reliable maps based on entropy characteristics. The Bayesian framework, employing Dirichlet distributions for both likelihood and prior distributions, enables sequential probability updates while preserving probabilistic class assignments. The Bayesian inference from the SVM classification maps achieved the highest mean overall accuracy of 0.857 for Monte Carlo sampling from the referenced JAXA land use land cover map, improving upon the non-Bayesian SVM map (0.855, p &lt; 0.001). Analysis revealed a strong correlation (r=0.710) between investigators' labeling quality and classification accuracy, suggesting that selecting high-quality investigator maps improves the robustness of fusion. The Interspersion and Juxtaposition Index (IJI) showed that fused maps from SVM-based maps selected by k-Means reduced salt-and-pepper noise (IJI: 56.652) compared to baseline maps (IJI: 69.867). Our approach demonstrates an effective approach for combining multiple land cover classifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15720v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01431161.2025.2496529</arxiv:DOI>
      <dc:creator>Narumasa Tsutsumida, Akira Kato</dc:creator>
    </item>
    <item>
      <title>Handling multivariable missing data in causal mediation analysis estimating interventional effects</title>
      <link>https://arxiv.org/abs/2403.17396</link>
      <description>arXiv:2403.17396v3 Announce Type: replace 
Abstract: The interventional effects approach to causal mediation analysis is increasingly common in epidemiologic research, given its potential to address policy-relevant questions about hypothetical mediator interventions. Multiple imputation (MI) is widely used for handling missing data in epidemiologic studies. However, guidance is lacking on best practices for using MI when estimating interventional mediation effects, specifically regarding the role of the missingness mechanism in the method's performance, how to appropriately specify the MI model when g-computation is used for effect estimation, and suitable approaches to variance estimation. To address this gap, we conducted simulations based on the Victorian Adolescent Health Cohort Study. We considered seven missingness mechanisms involving varying assumptions about the influence of an intermediate confounder, a mediator, and/or the outcome on missingness in key variables. We compared the performance of complete-case analysis, six MI approaches using fully conditional specification (differing in how the imputation model was tailored), and a "substantive model compatible" multiple imputation-fully conditional specification approach. We evaluated MIBoot (MI, then bootstrap) and BootMI (bootstrap, then MI) approaches for variance estimation. All MI approaches, apart from those clearly diverging from best practice, yielded approximately unbiased estimates when none of the intermediate confounder, mediator, and outcome variables influenced missingness in any of these variables, and showed non-negligible bias otherwise. We observed the largest bias for interventional effects when each of the intermediate confounders, mediators, and outcomes influenced their own missingness. BootMI returned variance estimates with smaller bias than MIBoot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17396v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1097/EDE.00000000000018662025.10.1097/EDE.0000000000001866</arxiv:DOI>
      <arxiv:journal_reference>Epidemiology 36(4):p 487-499, July 2025</arxiv:journal_reference>
      <dc:creator>S. Ghazaleh Dashti, Katherine J. Lee, Julie A. Simpson, John B. Carlin, Margarita Moreno-Betancur</dc:creator>
    </item>
    <item>
      <title>Active Learning For Repairable Hardware Systems With Partial Coverage</title>
      <link>https://arxiv.org/abs/2503.16315</link>
      <description>arXiv:2503.16315v2 Announce Type: replace 
Abstract: Identifying the optimal diagnostic test and hardware system instance to infer reliability characteristics using field data is challenging, especially when constrained by fixed budgets and minimal maintenance cycles. Active Learning (AL) has shown promise for parameter inference with limited data and budget constraints in machine learning/deep learning tasks. However, AL for reliability model parameter inference remains underexplored for repairable hardware systems. It requires specialized AL Acquisition Functions (AFs) that consider hardware aging and the fact that a hardware system consists of multiple sub-systems, which may undergo only partial testing during a given diagnostic test. To address these challenges, we propose a relaxed Mixed Integer Semidefinite Program (MISDP) AL AF that incorporates Diagnostic Coverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing budgets. Furthermore, we design empirical-based simulation experiments focusing on two diagnostic testing scenarios: (1) partial tests of a hardware system with overlapping subsystem coverage, and (2) partial tests where one diagnostic test fully subsumes the subsystem coverage of another. We evaluate our proposed approach against the most widely used AL AF in the literature (entropy), as well as several intuitive AL AFs tailored for reliability model parameter inference. Our proposed AF ranked best on average among the alternative AFs across 6,000 experimental configurations, with respect to Area Under the Curve (AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error (MSE) curves, with statistical significance calculated at a 0.05 alpha level using a Friedman hypothesis test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16315v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Potter, Beyza Kalkanl{\i}, Deniz Erdo\u{g}mu\c{s}, Michael Everett</dc:creator>
    </item>
    <item>
      <title>Open High-Resolution Satellite Imagery: The WorldStrat Dataset -- With Application to Super-Resolution</title>
      <link>https://arxiv.org/abs/2207.06418</link>
      <description>arXiv:2207.06418v2 Announce Type: replace-cross 
Abstract: Analyzing the planet at scale with satellite imagery and machine learning is a dream that has been constantly hindered by the cost of difficult-to-access highly-representative high-resolution imagery. To remediate this, we introduce here the WorldStrat dataset. The largest and most varied such publicly available dataset, at Airbus SPOT 6/7 satellites' high resolution of up to 1.5 m/pixel, empowered by European Space Agency's Phi-Lab as part of the ESA-funded QueryPlanet project, we curate nearly 10,000 sqkm of unique locations to ensure stratified representation of all types of land-use across the world: from agriculture to ice caps, from forests to multiple urbanization densities. We also enrich those with locations typically under-represented in ML datasets: sites of humanitarian interest, illegal mining sites, and settlements of persons at risk. We temporally-match each high-resolution image with multiple low-resolution images from the freely accessible lower-resolution Sentinel-2 satellites at 10 m/pixel. We accompany this dataset with an open-source Python package to: rebuild or extend the WorldStrat dataset, train and infer baseline algorithms, and learn with abundant tutorials, all compatible with the popular EO-learn toolbox. We hereby hope to foster broad-spectrum applications of ML to satellite imagery, and possibly develop from free public low-resolution Sentinel2 imagery the same power of analysis allowed by costly private high-resolution imagery. We illustrate this specific point by training and releasing several highly compute-efficient baselines on the task of Multi-Frame Super-Resolution. High-resolution Airbus imagery is CC BY-NC, while the labels and Sentinel2 imagery are CC BY, and the source code and pre-trained models under BSD. The dataset is available at https://zenodo.org/record/6810791 and the software package at https://github.com/worldstrat/worldstrat .</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.06418v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Cornebise, Ivan Or\v{s}oli\'c, Freddie Kalaitzis</dc:creator>
    </item>
    <item>
      <title>Examining marginal properness in the external validation of survival models with squared and logarithmic losses</title>
      <link>https://arxiv.org/abs/2212.05260</link>
      <description>arXiv:2212.05260v3 Announce Type: replace-cross 
Abstract: Scoring rules promote rational and honest decision-making, which is important for model evaluation and becoming increasingly important for automated procedures such as `AutoML'. In this paper we survey common squared and logarithmic scoring rules for survival analysis, with a focus on their theoretical and empirical properness. We introduce a marginal definition of properness and show that both the Integrated Survival Brier Score (ISBS) and the Right-Censored Log-Likelihood (RCLL) are theoretically improper under this definition. We also investigate a new class of losses that may inform future survival scoring rules. Simulation experiments reveal that both the ISBS and RCLL behave as proper scoring rules in practice. The RCLL showed no violations across all settings, while ISBS exhibited only minor, negligible violations at extremely small sample sizes, suggesting one can trust results from historical experiments. As such we advocate for both the RCLL and ISBS in external validation of models, including in automated procedures. However, we note practical challenges in estimating these losses including estimation of censoring distributions and densities; as such further research is required to advance development of robust and honest evaluation in survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05260v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Sonabend, John Zobolas, Riccardo Be Bin, Philipp Kopper, Lukas Burk, Andreas Bender</dc:creator>
    </item>
    <item>
      <title>Prior selection for the precision parameter of Dirichlet Process Mixtures</title>
      <link>https://arxiv.org/abs/2502.00864</link>
      <description>arXiv:2502.00864v2 Announce Type: replace-cross 
Abstract: Consider a Dirichlet process mixture model (DPM) with random precision parameter $\alpha$, inducing $K_n$ clusters over $n$ observations through its latent random partition. Our goal is to specify the prior distribution $p\left(\alpha\mid\boldsymbol{\eta}\right)$, including its fixed parameter vector $\boldsymbol{\eta}$, in a way that is meaningful.
  Existing approaches can be broadly categorised into three groups. Those in the first group depend on the sample size $n$, and often rely on the linkage between $p\left(\alpha\mid\boldsymbol{\eta}\right)$ and $p\left(K_n\right)$ to draw conclusions on how to best choose $\boldsymbol{\eta}$ to reflect one's prior knowledge of $K_{n}$; we call them sample-size-dependent. Those in the second and third group consist instead of using quasi-degenerate or improper priors, respectively.
  In this article, we show how all three methods have limitations, especially for large $n$. Then we propose an alternative methodology which does not depend on $K_n$ or on the size of the available sample, but rather on the relationship between the largest stick lengths in the stick-breaking construction of the DPM; and which reflects those prior beliefs in $p\left(\alpha\mid\boldsymbol{\eta}\right)$. We conclude with an example where existing sample-size-dependent approaches fail, while our sample-size-independent approach continues to be feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00864v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlo Vicentini, Ian Hyla Jermyn</dc:creator>
    </item>
    <item>
      <title>Revisiting Optimal Allocations for Binary Responses: Insights from Considering Type-I Error Rate Control</title>
      <link>https://arxiv.org/abs/2502.06381</link>
      <description>arXiv:2502.06381v4 Announce Type: replace-cross 
Abstract: This work revisits optimal response-adaptive designs from a type-I error rate perspective, highlighting when and how much these allocations exacerbate type-I error rate inflation - an issue previously undocumented. We explore a range of approaches from the literature that can be applied to reduce type-I error rate inflation. However, we found that all of these approaches fail to give a robust solution to the problem. To address this, we derive two optimal allocation proportions, incorporating the more robust score test (instead of the Wald test) with finite sample estimators (instead of the unknown true values) in the formulation of the optimization problem. One proportion optimizes statistical power and the other minimizes the total number failures in a trial while maintaining a fixed variance level. Through simulations based on an early-phase and a confirmatory trial we provide crucial practical insight into how these new optimal proportion designs can offer substantial patient outcomes advantages while controlling type-I error rate. While we focused on binary outcomes, the framework offers valuable insights that naturally extend to other outcome types, multi-armed trials and alternative measures of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06381v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pin, Sof\'ia S. Villar, William F. Rosenberger</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Experiments with Latent Outcomes: Key Results and Their Implications for Design and Analysis</title>
      <link>https://arxiv.org/abs/2505.21909</link>
      <description>arXiv:2505.21909v2 Announce Type: replace-cross 
Abstract: How should researchers analyze randomized experiments in which the main outcome is measured in multiple ways but each measure contains some degree of error? We describe modeling approaches that enable researchers to identify causal parameters of interest, suggest ways that experimental designs can be augmented so as to make linear latent variable models more credible, and discuss empirical tests of key modeling assumptions. We show that when experimental researchers invest appropriately in multiple outcome measures, an optimally weighted index of the outcome measures enables researchers to obtain efficient and interpretable estimates of causal parameters by applying standard regression methods, and that weights may be obtained using instrumental variables regression. Maximum likelihood and generalized method of moments estimators can be used to obtain estimates and standard errors in a single step. An empirical application illustrates the gains in precision and robustness that multiple outcome measures can provide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21909v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu, Donald P. Green</dc:creator>
    </item>
  </channel>
</rss>
