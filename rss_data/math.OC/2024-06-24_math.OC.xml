<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Jun 2024 02:34:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Decentralized Concurrent Learning with Coordinated Momentum and Restart</title>
      <link>https://arxiv.org/abs/2406.14802</link>
      <description>arXiv:2406.14802v1 Announce Type: new 
Abstract: This paper studies the stability and convergence properties of a class of multi-agent concurrent learning (CL) algorithms with momentum and restart. Such algorithms can be integrated as part of the estimation pipelines of data-enabled multi-agent control systems to enhance transient performance while maintaining stability guarantees. However, characterizing restarting policies that yield stable behaviors in decentralized CL systems, especially when the network topology of the communication graph is directed, has remained an open problem. In this paper, we provide an answer to this problem by synergistically leveraging tools from graph theory and hybrid dynamical systems theory. Specifically, we show that under a cooperative richness condition on the overall multi-agent system's data, and by employing coordinated periodic restart with a frequency that is tempered by the level of asymmetry of the communication graph, the resulting decentralized dynamics exhibit robust asymptotic stability properties, characterized in terms of input-to-state stability bounds, and also achieve a desirable transient performance. To demonstrate the practical implications of the theoretical findings, three applications are also presented: cooperative parameter estimation over networks with private data sets, cooperative model-reference adaptive control, and cooperative data-enabled feedback optimization of nonlinear plants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14802v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel E. Ochoa, Muhammad U. Javed, Xudong Chen, Jorge I. Poveda</dc:creator>
    </item>
    <item>
      <title>Converse Theorems for Certificates of Safety and Stability</title>
      <link>https://arxiv.org/abs/2406.14823</link>
      <description>arXiv:2406.14823v1 Announce Type: new 
Abstract: Motivated by the key role of control barrier functions (CBFs) in assessing safety and enabling the synthesis of safe controllers in nonlinear control systems, this paper presents a suite of converse results on CBFs. Given any safe set, we first identify a set of general sufficient conditions which guarantee the existence of a CBF. Our technical analysis also enables us to define an extended notion of CBF which is always guaranteed to exist if the set is safe. We next turn our attention to the problem of joint safety and stability, and give conditions under which the notions of control Lyapunov-barrier function (CLBF) and compatible control Lyapunov function (CLF) and CBF pair are guaranteed to exist. Finally, we identify conditions under which a CLBF and a compatible CLF-CBF pair can be constructed from a non-compatible CLF-CBF pair. Throughout the paper, we intersperse different examples and counterexamples to motivate our results and position them within the state of the art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14823v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pol Mestres, Jorge Cort\'es</dc:creator>
    </item>
    <item>
      <title>An effective subgradient algorithm via Mifflin's line search for nonsmooth nonconvex multiobjective optimization</title>
      <link>https://arxiv.org/abs/2406.14905</link>
      <description>arXiv:2406.14905v1 Announce Type: new 
Abstract: We propose a descent subgradient algorithm for unconstrained nonsmooth nonconvex multiobjective optimization problems. To find a descent direction, we present an iterative process that efficiently approximates the Goldstein subdifferential of each objective function. To this end, we develop a new variant of Mifflin's line search in which the subgradients are arbitrary and its finite convergence is proved under a semismooth assumption. To reduce the number of subgradient evaluations, we employ a backtracking line search that identifies the objectives requiring an improvement in the current approximation of the Goldstein subdifferential. Meanwhile, for the remaining objectives, new subgradients are not computed. Unlike bundle-type methods, the proposed approach can handle nonconvexity without the need for algorithmic adjustments. Moreover, the quadratic subproblems have a simple structure, and hence the method is easy to implement. We analyze the global convergence of the proposed method and prove that any accumulation point of the generated sequence satisfies a necessary Pareto optimality condition. Furthermore, our convergence analysis addresses a theoretical challenge in a recently developed subgradient method. Through numerical experiments, we observe the practical capability of the proposed method and evaluate its efficiency when applied to a diverse range of nonsmooth test problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14905v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Morteza Maleknia, Majid Soleimani-damaneh</dc:creator>
    </item>
    <item>
      <title>Passivity theorems for input-to-state stability of forced Lur'e inclusions and equations, and consequent entrainment-type properties</title>
      <link>https://arxiv.org/abs/2406.15099</link>
      <description>arXiv:2406.15099v1 Announce Type: new 
Abstract: A suite of input-to-state stability results are presented for a class of forced differential inclusions, so-called Lur'e inclusions. As a consequence, semi-global incremental input-to-state stability results for systems of forced Lur'e differential equations are derived. The results are in the spirit of the passivity theorem from control theory as both the linear and nonlinear components of the Lur'e inclusion (or equation) are assumed to satisfy passivity-type conditions. These results provide a basis for the analysis of forced Lur'e differential equations subject to (almost) periodic forcing terms and, roughly speaking, ensure the existence and attractivity of (almost) periodic state- and output-responses, comprising another focus of the present work. One ultimate aim of the study is to provide a robust and rigorous theoretical foundation for a well-defined and tractable ``frequency response'' of forced Lur'e systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15099v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.CA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Guiver</dc:creator>
    </item>
    <item>
      <title>Approximate Controllability of Linear Fractional Impulsive Evolution Equations in Hilbert Spaces</title>
      <link>https://arxiv.org/abs/2406.15114</link>
      <description>arXiv:2406.15114v1 Announce Type: new 
Abstract: In this paper, we investigate the approximate controllability of linear fractional impulsive evolution equations in Hilbert spaces. We provide a representation of solutions utilizing impulsive operators. Necessary and sufficient conditions for the approximate controllability of linear fractional impulsive evolution equations are established in terms of the impulsive resolvent operator. An example is presented to demonstrate the application of the obtained theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15114v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javad A. Asadzade, Nazim I. Mahmudov</dc:creator>
    </item>
    <item>
      <title>Dynamic Modelling of a Controlled Orthotropic Plate: Analytic and Data-Driven Approaches in the Frequency Domain</title>
      <link>https://arxiv.org/abs/2406.15116</link>
      <description>arXiv:2406.15116v1 Announce Type: new 
Abstract: This paper is devoted to the mathematical modelling of a vibrating orthotropic plate equipped with a laminated piezosensor, under the influence of a lumped force actuation. We employ the Kirchhoff plate theory to derive the corresponding partial differential equation, assuming free boundary conditions. Analytical solutions for this boundary value problem are explored in the form of series expansions, using products of Krylov functions. Utilizing Galerkin's method, this mathematical model is transformed into an infinite-dimensional control system characterized by modal coordinates. The transfer function of such a system is explicitly evaluated in the single-input single-output case. The computation of coefficients for finite-dimensional approximate systems is formalized in an algorithm with an arbitrary number of degrees of freedom. Our numerical study confirms that the modeled input-output behavior shows acceptable agreement over the given frequency range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15116v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Zuyev, Francesco Pellicano, Antonio Zippo, Giovanni Iarriccio</dc:creator>
    </item>
    <item>
      <title>Approximation of shape optimization problems with non-smooth PDE constraints</title>
      <link>https://arxiv.org/abs/2406.15146</link>
      <description>arXiv:2406.15146v1 Announce Type: new 
Abstract: This paper is concerned with a shape optimization problem governed by a non-smooth PDE, i.e., the nonlinearity in the state equation is not necessarily differentiable. We follow the functional variational approach of [33] where the set of admissible shapes is parametrized by a large class of continuous mappings. This methodology allows for both boundary and topological variations. It has the advantage that one can rewrite the shape optimization problem as a control problem in a function space. To overcome the lack of convexity of the set of admissible controls, we provide an essential density property. This permits us to show that each parametrization associated to the optimal shape is the limit of global optima of non-smooth distributed optimal control problems. The admissible set of the approximating minimization problems is a convex subset of a Hilbert space of functions. Moreover, its structure is such that one can derive strong stationary optimality conditions [5]. This opens the door to future research concerning sharp first-order necessary optimality conditions in form of a qualified optimality system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15146v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Livia Betz</dc:creator>
    </item>
    <item>
      <title>Uniform Convergence of Adversarially Robust Classifiers</title>
      <link>https://arxiv.org/abs/2406.14682</link>
      <description>arXiv:2406.14682v1 Announce Type: cross 
Abstract: In recent years there has been significant interest in the effect of different types of adversarial perturbations in data classification problems. Many of these models incorporate the adversarial power, which is an important parameter with an associated trade-off between accuracy and robustness. This work considers a general framework for adversarially-perturbed classification problems, in a large data or population-level limit. In such a regime, we demonstrate that as adversarial strength goes to zero that optimal classifiers converge to the Bayes classifier in the Hausdorff distance. This significantly strengthens previous results, which generally focus on $L^1$-type convergence. The main argument relies upon direct geometric comparisons and is inspired by techniques from geometric measure theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14682v1</guid>
      <category>math.AP</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rachel Morris, Ryan Murray</dc:creator>
    </item>
    <item>
      <title>Preferential Multi-Objective Bayesian Optimization</title>
      <link>https://arxiv.org/abs/2406.14699</link>
      <description>arXiv:2406.14699v1 Announce Type: cross 
Abstract: Preferential Bayesian optimization (PBO) is a framework for optimizing a decision-maker's latent preferences over available design choices. While preferences often involve multiple conflicting objectives, existing work in PBO assumes that preferences can be encoded by a single objective function. For example, in robotic assistive devices, technicians often attempt to maximize user comfort while simultaneously minimizing mechanical energy consumption for longer battery life. Similarly, in autonomous driving policy design, decision-makers wish to understand the trade-offs between multiple safety and performance attributes before committing to a policy. To address this gap, we propose the first framework for PBO with multiple objectives. Within this framework, we present dueling scalarized Thompson sampling (DSTS), a multi-objective generalization of the popular dueling Thompson algorithm, which may be of interest beyond the PBO setting. We evaluate DSTS across four synthetic test functions and two simulated exoskeleton personalization and driving policy design tasks, showing that it outperforms several benchmarks. Finally, we prove that DSTS is asymptotically consistent. As a direct consequence, this result provides, to our knowledge, the first convergence guarantee for dueling Thompson sampling in the PBO setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14699v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raul Astudillo, Kejun Li, Maegan Tucker, Chu Xin Cheng, Aaron D. Ames, Yisong Yue</dc:creator>
    </item>
    <item>
      <title>Learning to Cover: Online Learning and Optimization with Irreversible Decisions</title>
      <link>https://arxiv.org/abs/2406.14777</link>
      <description>arXiv:2406.14777v1 Announce Type: cross 
Abstract: We define an online learning and optimization problem with irreversible decisions contributing toward a coverage target. At each period, a decision-maker selects facilities to open, receives information on the success of each one, and updates a machine learning model to guide future decisions. The goal is to minimize costs across a finite horizon under a chance constraint reflecting the coverage target. We derive an optimal algorithm and a tight lower bound in an asymptotic regime characterized by a large target number of facilities $m\to\infty$ but a finite horizon $T\in\mathbb{Z}_+$. We find that the regret grows sub-linearly at a rate $\Theta\left(m^{\frac{1}{2}\cdot\frac{1}{1-2^{-T}}}\right)$, thus converging exponentially fast to $\Theta(\sqrt{m})$. We establish the robustness of this result to the learning environment; we also extend it to a more complicated facility location setting in a bipartite facility-customer graph with a target on customer coverage. Throughout, constructive proofs identify a policy featuring limited exploration initially for learning purposes, and fast exploitation later on for optimization purposes once uncertainty gets mitigated. These findings underscore the benefits of limited online learning and optimization, in that even a few rounds can provide significant benefits as compared to a no-learning baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14777v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Jacquillat, Michael Lingzhi Li</dc:creator>
    </item>
    <item>
      <title>Towards Dynamic Resource Allocation and Client Scheduling in Hierarchical Federated Learning: A Two-Phase Deep Reinforcement Learning Approach</title>
      <link>https://arxiv.org/abs/2406.14910</link>
      <description>arXiv:2406.14910v1 Announce Type: cross 
Abstract: Federated learning (FL) is a viable technique to train a shared machine learning model without sharing data. Hierarchical FL (HFL) system has yet to be studied regrading its multiple levels of energy, computation, communication, and client scheduling, especially when it comes to clients relying on energy harvesting to power their operations. This paper presents a new two-phase deep deterministic policy gradient (DDPG) framework, referred to as ``TP-DDPG'', to balance online the learning delay and model accuracy of an FL process in an energy harvesting-powered HFL system. The key idea is that we divide optimization decisions into two groups, and employ DDPG to learn one group in the first phase, while interpreting the other group as part of the environment to provide rewards for training the DDPG in the second phase. Specifically, the DDPG learns the selection of participating clients, and their CPU configurations and the transmission powers. A new straggler-aware client association and bandwidth allocation (SCABA) algorithm efficiently optimizes the other decisions and evaluates the reward for the DDPG. Experiments demonstrate that with substantially reduced number of learnable parameters, the TP-DDPG can quickly converge to effective polices that can shorten the training time of HFL by 39.4% compared to its benchmarks, when the required test accuracy of HFL is 0.9.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14910v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojing Chen, Zhenyuan Li, Wei Ni, Xin Wang, Shunqing Zhang, Yanzan Sun, Shugong Xu, Qingqi Pei</dc:creator>
    </item>
    <item>
      <title>Stochastic Optimisation Framework using the Core Imaging Library and Synergistic Image Reconstruction Framework for PET Reconstruction</title>
      <link>https://arxiv.org/abs/2406.15159</link>
      <description>arXiv:2406.15159v1 Announce Type: cross 
Abstract: We introduce a stochastic framework into the open--source Core Imaging Library (CIL) which enables easy development of stochastic algorithms. Five such algorithms from the literature are developed, Stochastic Gradient Descent, Stochastic Average Gradient (-Am\'elior\'e), (Loopless) Stochastic Variance Reduced Gradient. We showcase the functionality of the framework with a comparative study against a deterministic algorithm on a simulated 2D PET dataset, with the use of the open-source Synergistic Image Reconstruction Framework. We observe that stochastic optimisation methods can converge in fewer passes of the data than a standard deterministic algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15159v1</guid>
      <category>math.NA</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>eess.IV</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evangelos Papoutsellis, Casper da Costa-Luis, Daniel Deidda, Claire Delplancke, Margaret Duff, Gemma Fardell, Ashley Gillman, Jakob S. J{\o}rgensen, Zeljko Kereta, Evgueni Ovtchinnikov, Edoardo Pasca, Georg Schramm, Kris Thielemans</dc:creator>
    </item>
    <item>
      <title>Large Batch Analysis for Adagrad Under Anisotropic Smoothness</title>
      <link>https://arxiv.org/abs/2406.15244</link>
      <description>arXiv:2406.15244v1 Announce Type: cross 
Abstract: Adaptive gradient algorithms have been widely adopted in training large-scale deep neural networks, especially large foundation models. Despite their huge success in practice, their theoretical advantages over stochastic gradient descent (SGD) have not been fully understood, especially in the large batch-size setting commonly used in practice. This is because the only theoretical result that can demonstrate the benefit of Adagrad over SGD was obtained in the original paper of Adagrad for nonsmooth objective functions. However, for nonsmooth objective functions, there can be a linear slowdown of convergence when batch size increases, and thus a convergence analysis based on nonsmooth assumption cannot be used for large batch algorithms. In this work, we resolve this gap between theory and practice by providing a new analysis of Adagrad on both convex and nonconvex smooth objectives suitable for the large batch setting. It is shown that under the anisotropic smoothness and noise conditions, increased batch size does not slow down convergence for Adagrad, and thus it can still achieve a faster convergence guarantee over SGD even in the large batch setting. We present detailed comparisons between SGD and Adagrad to provide a better understanding of the benefits of adaptive gradient methods. Experiments in logistic regression and instruction following fine-tuning tasks provide strong evidence to support our theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15244v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxing Liu, Rui Pan, Tong Zhang</dc:creator>
    </item>
    <item>
      <title>Network-Based Optimal Control of Pollution Growth</title>
      <link>https://arxiv.org/abs/2406.15338</link>
      <description>arXiv:2406.15338v1 Announce Type: cross 
Abstract: This paper studies a model for the optimal control (by a centralized economic agent which we call the planner) of pollution diffusion over time and space. The controls are the investments in production and depollution and the goal is to maximize an intertemporal utility function. The main novelty is the fact that the spatial component has a network structure. Moreover, in such a time-space setting we also analyze the trade-off between the use of green or non-green technologies: this also seems to be a novelty in such a setting. Extending methods of previous papers, we can solve explicitly the problem in the case of linear costs of pollution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15338v1</guid>
      <category>econ.TH</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fausto Gozzi, Marta Leocata, Giulia Pucci</dc:creator>
    </item>
    <item>
      <title>Algorithms for optimal control of hybrid systems with sliding motion</title>
      <link>https://arxiv.org/abs/2101.04754</link>
      <description>arXiv:2101.04754v3 Announce Type: replace 
Abstract: This paper concerns two algorithms for solving optimal control problems with hybrid systems. The first algorithm aims at hybrid systems exhibiting sliding modes. The first algorithm has several features which distinguishes it from the other algorithms for problems described by hybrid systems. First of all, it can cope with hybrid systems which exhibit sliding modes. Secondly, the systems motion on the switching surface is described by index 2 differential--algebraic equations and that guarantees accurate tracking of the sliding motion surface. Thirdly, the gradients of the problems functionals are evaluated with the help of adjoint equations. The adjoint equations presented in the paper take into account sliding motion and exhibit jump conditions at transition times. We state optimality conditions in the form of the weak maximum principle for optimal control problems with hybrid systems exhibiting sliding modes and with piecewise differentiable controls. The second algorithm is for optimal control problems with hybrid systems which do not exhibit sliding motion. In the case of this algorithm we assume that control functions are measurable functions. For each algorithm, we show that every accumulation point of the sequence generated by the algorithm satisfies the weak maximum principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.04754v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radoslaw Pytlak, Damian Suski</dc:creator>
    </item>
    <item>
      <title>Identifiability, the KL property in metric spaces, and subgradient curves</title>
      <link>https://arxiv.org/abs/2205.02868</link>
      <description>arXiv:2205.02868v2 Announce Type: replace 
Abstract: Identifiability, and the closely related idea of partial smoothness, unify classical active set methods and more general notions of solution structure. Diverse optimization algorithms generate iterates in discrete time that are eventually confined to identifiable sets. We present two fresh perspectives on identifiability. The first distills the notion to a simple metric property, applicable not just in Euclidean settings but to optimization over manifolds and beyond; the second reveals analogous continuous-time behavior for subgradient descent curves. The Kurdya-Lojasiewicz property typically governs convergence in both discrete and continuous time: we explore its interplay with identifiability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.02868v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10208-024-09652-z</arxiv:DOI>
      <arxiv:journal_reference>Foundations of Computational Mathematics, 2024</arxiv:journal_reference>
      <dc:creator>Adrian Lewis, Tonghua Tian</dc:creator>
    </item>
    <item>
      <title>On the Correlation Gap of Matroids</title>
      <link>https://arxiv.org/abs/2209.09896</link>
      <description>arXiv:2209.09896v3 Announce Type: replace 
Abstract: A set function can be extended to the unit cube in various ways; the correlation gap measures the ratio between two natural extensions. This quantity has been identified as the performance guarantee in a range of approximation algorithms and mechanism design settings. It is known that the correlation gap of a monotone submodular function is at least $1-1/e$, and this is tight for simple matroid rank functions.
  We initiate a fine-grained study of the correlation gap of matroid rank functions. In particular, we present an improved lower bound on the correlation gap as parametrized by the rank and girth of the matroid. We also show that for any matroid, the correlation gap of its weighted matroid rank function is minimized under uniform weights. Such improved lower bounds have direct applications for submodular maximization under matroid constraints, mechanism design, and contention resolution schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.09896v3</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edin Husi\'c, Zhuan Khye Koh, Georg Loho, L\'aszl\'o A. V\'egh</dc:creator>
    </item>
    <item>
      <title>Forward-Backward algorithms for weakly convex problems</title>
      <link>https://arxiv.org/abs/2303.14021</link>
      <description>arXiv:2303.14021v3 Announce Type: replace 
Abstract: We investigate the convergence properties of exact and inexact forward-backward algorithms to minimise the sum of two weakly convex functions defined on a Hilbert space, where one has a Lipschitz-continuous gradient. We show that the exact forward-backward algorithm converges strongly to a global solution, provided that the objective function satisfies a sharpness condition. For the inexact forward-backward algorithm, the same condition ensures that the distance from the iterates to the solution set approaches a positive threshold depending on the accuracy level of the proximal computations. As an application of the considered setting, we provide numerical experiments related to discrete tomography.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.14021v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ewa Bednarczuk, Giovanni Bruccola, Gabriele Scrivanti, The Hung Tran</dc:creator>
    </item>
    <item>
      <title>Variational Poisson Denoising via Augmented Lagrangian Methods</title>
      <link>https://arxiv.org/abs/2304.06434</link>
      <description>arXiv:2304.06434v2 Announce Type: replace 
Abstract: In this paper, we denoise a given noisy image by minimizing a smoothness promoting function over a set of local similarity measures which compare the mean of the given image and some candidate image on a large collection of subboxes. The associated convex optimization problem possesses a huge number of constraints which are induced by extended real-valued functions stemming from the Kullback--Leibler divergence. Alternatively, these nonlinear constraints can be reformulated as affine ones, which makes the model seemingly more tractable. For the numerical treatment of both formulations of the model (i.e., the original one as well as the one with affine constraints), we propose a rather general augmented Lagrangian method which is capable of handling the huge amount of constraints. A self-contained, derivative-free, global convergence theory is provided, allowing an extension to other problem classes. For the solution of the resulting subproblems in the setting of our suggested image denoising models, we make use of a suitable stochastic gradient method. Results of several numerical experiments are presented in order to compare both formulations and the associated augmented Lagrangian methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.06434v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Christian Kanzow, Fabius Kr\"amer, Patrick Mehlitz, Gerd Wachsmuth, Frank Werner</dc:creator>
    </item>
    <item>
      <title>Projecting onto a Capped Rotated Second-Order Cone</title>
      <link>https://arxiv.org/abs/2307.07290</link>
      <description>arXiv:2307.07290v2 Announce Type: replace 
Abstract: This paper establishes a closed-form expression for projecting onto a capped rotated second-order cone. This convex set arises in the perspective relaxation of mixed-integer nonlinear programs (MINLP) with binary indicator variables. The rapid computation of the projection onto this set is expected to enable the development of effective methods for solving the continuous relaxation of MINLPs whose feasible region may involve a Cartesian product of a large number of such sets. The closed-form established herein consists of seven cases, one of which is a solution of a cubic equation and another is a solution of a quartic equation. Although quartic equations possess closed-form solutions, numerical solutions are typically used in practice. Based on bounds that we prove using additional case analysis, we develop a specialized bisection-based method to solve the resulting quartic equation. In experiments we first demonstrate that the projection problem is solved faster and more accurately with our closed-form, together with a standard polynomial equation solver, compared with a general state-of-the-art interior-point solver and compared with a state-of-the art conic first-order method solver. We also demonstrate the efficacy of our bisection-based specialized numerical method for solving the quartic equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07290v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noam Goldberg, Ishy Zagdoun</dc:creator>
    </item>
    <item>
      <title>Extragradient method with feasible inexact projection to variational inequality problem</title>
      <link>https://arxiv.org/abs/2309.00648</link>
      <description>arXiv:2309.00648v2 Announce Type: replace 
Abstract: The variational inequality problem in finite-dimensional Euclidean space is addressed in this paper, and two inexact variants of the extragradient method are proposed to solve it. Instead of computing exact projections on the constraint set, as in previous versions extragradient method, the proposed methods compute feasible inexact projections on the constraint set using a relative error criterion. The first version of the proposed method provided is a counterpart to the classic form of the extragradient method with constant steps. In order to establish its convergence we need to assume that the operator is pseudo-monotone and Lipschitz continuous, as in the standard approach. For the second version, instead of a fixed step size, the method presented finds a suitable step size in each iteration by performing a line search. Like the classical extragradient method, the proposed method does just two projections into the feasible set in each iteration. A full convergence analysis is provided, with no Lipschitz continuity assumption of the operator defining the variational inequality problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.00648v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>R. D\'iaz Mill\'an, O. P. Ferreira, J. Ugon</dc:creator>
    </item>
    <item>
      <title>Total variation regularization for recovering the spatial source term in a time-fractional diffusion equation</title>
      <link>https://arxiv.org/abs/2310.12029</link>
      <description>arXiv:2310.12029v2 Announce Type: replace 
Abstract: In this paper, we consider an inverse space-dependent source problem for a time-fractional diffusion equation. To deal with the ill-posedness of the problem, we transform the problem into an optimal control problem with total variational (TV) regularization. In contrast to the classical Tikhonov model incorporating $L^2$ penalty terms, the inclusion of a TV term proves advantageous in reconstructing solutions that exhibit discontinuities or piecewise constancy. The control problem is approximated by a fully discrete scheme, and convergence results are provided within this framework. Furthermore, a lineraed primal-dual iterative algorithm is proposed to solve the discrete control model based on an equivalent saddle-point reformulation, and several numerical experiments are presented to demonstrate the efficiency of the algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12029v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bin Fan</dc:creator>
    </item>
    <item>
      <title>A clustering approach for pairwise comparison matrices</title>
      <link>https://arxiv.org/abs/2402.06061</link>
      <description>arXiv:2402.06061v3 Announce Type: replace 
Abstract: We consider clustering in group decision making where the opinions are given by pairwise comparison matrices. In particular, the k-medoids model is suggested to classify the matrices since it has a linear programming problem formulation that may contain any condition on the properties of the cluster centres. Its objective function depends on the measure of dissimilarity between the matrices but not on the weights derived from them. Our methodology provides a convenient tool for decision support, for instance, it can be used to quantify the reliability of the aggregation. The proposed theoretical framework is applied to a large-scale experimental dataset, on which it is able to automatically detect some mistakes made by the decision-makers, as well as to identify a common source of inconsistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06061v3</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kolos Csaba \'Agoston, S\'andor Boz\'oki, L\'aszl\'o Csat\'o</dc:creator>
    </item>
    <item>
      <title>Hilbert Space-Valued LQ Mean Field Games: An Infinite-Dimensional Analysis</title>
      <link>https://arxiv.org/abs/2403.01012</link>
      <description>arXiv:2403.01012v2 Announce Type: replace 
Abstract: This paper presents a comprehensive study of Hilbert space-valued linear-quadratic (LQ) mean field games (MFGs), generalizing the classic LQ mean field game theory to scenarios involving $N$ agent whose dynamics are governed by infinite-dimensional stochastic equations. In this framework, both the state and control processes of each agent take values in separable Hilbert spaces. Moreoever, all agents are coupled through the average state of the population which appears in their linear dynamics and quadratic cost functional. Specifically, the dynamics of each agent incorporates an infinite-dimensional noise, namely a $Q$-Wiener process, and an unbounded operator. The diffusion coefficient of each agent also involves the state, control, and the average state processes. We first study the well-posedness of a system of $N$ general coupled infinite-dimensional stochastic evolution equations, which forms the foundation of MFGs in Hilbert spaces. Subsequently, we address the limiting Hilbert space-valued MFG as the number of agents approaches infinity and develop an infinite-dimensional variant of the Nash Certainty Equivalence principle. We characterize a unique Nash equilibrium for the limiting model and demonstrate that the associated best-response strategies constitute an $\epsilon$-Nash equilibrium for the original $N$-player game in Hilbert spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01012v2</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <category>q-fin.RM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanchao Liu, Dena Firoozi</dc:creator>
    </item>
    <item>
      <title>Strong asymptotic convergence of a slowly damped inertial primal-dual dynamical system controlled by a Tikhonov regularization term</title>
      <link>https://arxiv.org/abs/2406.08836</link>
      <description>arXiv:2406.08836v3 Announce Type: replace 
Abstract: We propose a slowly damped inertial primal-dual dynamical system controlled by a Tikhonov regularization term, where the inertial term is introduced only for the primal variable, for the linearly constrained convex optimization problem in a Hilbert space. Under mild conditions on the underlying parameters, by a Lyapunov analysis approach, we prove the strong asymptotic convergence of the trajectory of the proposed dynamic to the minimal norm element of the primal-dual solution set of the problem, along with convergence rate results for the primal-dual gap, the objective residual and the feasibility violation. We perform some numerical experiments to illustrate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08836v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting-Ting Zhu, Rong Hu, Ya-Ping Fang</dc:creator>
    </item>
    <item>
      <title>Straight-Through meets Sparse Recovery: the Support Exploration Algorithm</title>
      <link>https://arxiv.org/abs/2301.13584</link>
      <description>arXiv:2301.13584v2 Announce Type: replace-cross 
Abstract: The {\it straight-through estimator} (STE) is commonly used to optimize quantized neural networks, yet its contexts of effective performance are still unclear despite empirical successes.To make a step forward in this comprehension, we apply STE to a well-understood problem: {\it sparse support recovery}. We introduce the {\it Support Exploration Algorithm} (SEA), a novel algorithm promoting sparsity, and we analyze its performance in support recovery (a.k.a. model selection) problems. SEA explores more supports than the state-of-the-art, leading to superior performance in experiments, especially when the columns of $A$ are strongly coherent.The theoretical analysis considers recovery guarantees when the linear measurements matrix $A$ satisfies the {\it Restricted Isometry Property} (RIP).The sufficient conditions of recovery are comparable but more stringent than those of the state-of-the-art in sparse support recovery. Their significance lies mainly in their applicability to an instance of the STE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.13584v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ICML 2024, the 41st International Conference on Machine Learning, Jul 2024, Vienna, Austria</arxiv:journal_reference>
      <dc:creator>Mimoun Mohamed (QARMA, I2M), Fran\c{c}ois Malgouyres (IMT), Valentin Emiya (QARMA), Caroline Chaux (IPAL)</dc:creator>
    </item>
    <item>
      <title>Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks</title>
      <link>https://arxiv.org/abs/2402.09226</link>
      <description>arXiv:2402.09226v2 Announce Type: replace-cross 
Abstract: This paper examines gradient flow dynamics of two-homogeneous neural networks for small initializations, where all weights are initialized near the origin. For both square and logistic losses, it is shown that for sufficiently small initializations, the gradient flow dynamics spend sufficient time in the neighborhood of the origin to allow the weights of the neural network to approximately converge in direction to the Karush-Kuhn-Tucker (KKT) points of a neural correlation function that quantifies the correlation between the output of the neural network and corresponding labels in the training data set. For square loss, it has been observed that neural networks undergo saddle-to-saddle dynamics when initialized close to the origin. Motivated by this, this paper also shows a similar directional convergence among weights of small magnitude in the neighborhood of certain saddle points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09226v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshay Kumar, Jarvis Haupt</dc:creator>
    </item>
    <item>
      <title>Random Pareto front surfaces</title>
      <link>https://arxiv.org/abs/2405.01404</link>
      <description>arXiv:2405.01404v2 Announce Type: replace-cross 
Abstract: The goal of multi-objective optimisation is to identify the Pareto front surface which is the set obtained by connecting the best trade-off points. Typically this surface is computed by evaluating the objectives at different points and then interpolating between the subset of the best evaluated trade-off points. In this work, we propose to parameterise the Pareto front surface using polar coordinates. More precisely, we show that any Pareto front surface can be equivalently represented using a scalar-valued length function which returns the projected length along any positive radial direction. We then use this representation in order to rigorously develop the theory and applications of stochastic Pareto front surfaces. In particular, we derive many Pareto front surface statistics of interest such as the expectation, covariance and quantiles. We then discuss how these can be used in practice within a design of experiments setting, where the goal is to both infer and use the Pareto front surface distribution in order to make effective decisions. Our framework allows for clear uncertainty quantification and we also develop advanced visualisation techniques for this purpose. Finally we discuss the applicability of our ideas within multivariate extreme value theory and illustrate our methodology in a variety of numerical examples, including a case study with a real-world air pollution data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01404v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ben Tu, Nikolas Kantas, Robert M. Lee, Behrang Shafei</dc:creator>
    </item>
    <item>
      <title>Prescribed exponential stabilization of a one-layer neural network with delayed feedback: Insights in seizure prevention and neural control</title>
      <link>https://arxiv.org/abs/2406.13730</link>
      <description>arXiv:2406.13730v2 Announce Type: replace-cross 
Abstract: This paper provides control-oriented delay-based modelling of a one-layer neural network of Hopfield-type subject to an external input designed as delayed feedback. The specificity of such a model is that it makes the considered neuron less susceptible to seizure caused by its inherent dynamic instability. This modelling exploits a recently set partial pole placement for linear functional differential equations, which relies on the coexistence of real spectral values, allowing the explicit prescription of the closed-loop solution's exponential decay. The proposed framework improves some pioneering and scarce results from the literature on the characterization of the exact solution's exponential decay when a simple real spectral value exists. Indeed, it improves neural stability when the inherent dynamic is stable and provides insights into the design of a one-layer neural network that can be stabilized exponentially with delayed feedback and with a prescribed decay rate regardless of whether the inherent neuron dynamic is stable or unstable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13730v2</guid>
      <category>math.SP</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyprien Tamekue, Islam Boussaada, Karim Trabelsi</dc:creator>
    </item>
  </channel>
</rss>
