<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.ST</link>
    <description>q-fin.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Mar 2025 03:28:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Behavioral Machine Learning? Computer Predictions of Corporate Earnings also Overreact</title>
      <link>https://arxiv.org/abs/2303.16158</link>
      <description>arXiv:2303.16158v2 Announce Type: replace 
Abstract: Machine learning algorithms are known to outperform human analysts in predicting corporate earnings, leading to their rapid adoption. However, we show that leading methods (XGBoost, neural nets, ChatGPT) systematically overreact to news. The overreaction is primarily due to biases in the training data and we show that it cannot be eliminated without compromising accuracy. Analysts with machine learning training overreact much less than do traditional analysts. We provide a model showing that there is a tradeoff between predictive power and rational behavior. Our findings suggest that AI tools reduce but do not eliminate behavioral biases in financial markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.16158v2</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.GN</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Murray Z. Frank, Jing Gao, Keer Yang</dc:creator>
    </item>
    <item>
      <title>Generalized Factor Neural Network Model for High-dimensional Regression</title>
      <link>https://arxiv.org/abs/2502.11310</link>
      <description>arXiv:2502.11310v2 Announce Type: replace-cross 
Abstract: We tackle the challenges of modeling high-dimensional data sets, particularly those with latent low-dimensional structures hidden within complex, non-linear, and noisy relationships. Our approach enables a seamless integration of concepts from non-parametric regression, factor models, and neural networks for high-dimensional regression. Our approach introduces PCA and Soft PCA layers, which can be embedded at any stage of a neural network architecture, allowing the model to alternate between factor modeling and non-linear transformations. This flexibility makes our method especially effective for processing hierarchical compositional data. We explore ours and other techniques for imposing low-rank structures on neural networks and examine how architectural design impacts model performance. The effectiveness of our method is demonstrated through simulation studies, as well as applications to forecasting future price movements of equity ETF indices and nowcasting with macroeconomic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11310v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zichuan Guo, Mihai Cucuringu, Alexander Y. Shestopaloff</dc:creator>
    </item>
  </channel>
</rss>
