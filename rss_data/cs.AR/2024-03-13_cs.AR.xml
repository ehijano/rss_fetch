<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Mar 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 13 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>From English to ASIC: Hardware Implementation with Large Language Model</title>
      <link>https://arxiv.org/abs/2403.07039</link>
      <description>arXiv:2403.07039v1 Announce Type: new 
Abstract: In the realm of ASIC engineering, the landscape has been significantly reshaped by the rapid development of LLM, paralleled by an increase in the complexity of modern digital circuits. This complexity has escalated the requirements for HDL coding, necessitating a higher degree of precision and sophistication. However, challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities in accurately interpreting and implementing hardware specifications. To address these challenges, a strategy focusing on the fine-tuning of the leading-edge nature language model and the reshuffling of the HDL code dataset has been developed. The fine-tuning aims to enhance models' proficiency in generating precise and efficient ASIC design, while the dataset reshuffling is intended to broaden the scope and improve the quality of training material. The model demonstrated significant improvements compared to the base model, with approximately 10% to 20% increase in accuracy across a wide range of temperature for the pass@1 metric. This approach is expected to facilitate a simplified and more efficient LLM-assisted framework for complex circuit design, leveraging their capabilities to meet the sophisticated demands of HDL coding and thus streamlining the ASIC development process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07039v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emil Goh, Maoyang Xiang, I-Chyn Wey, T. Hui Teo</dc:creator>
    </item>
    <item>
      <title>The Dawn of AI-Native EDA: Promises and Challenges of Large Circuit Models</title>
      <link>https://arxiv.org/abs/2403.07257</link>
      <description>arXiv:2403.07257v1 Announce Type: new 
Abstract: Within the Electronic Design Automation (EDA) domain, AI-driven solutions have emerged as formidable tools, yet they typically augment rather than redefine existing methodologies. These solutions often repurpose deep learning models from other domains, such as vision, text, and graph analytics, applying them to circuit design without tailoring to the unique complexities of electronic circuits. Such an AI4EDA approach falls short of achieving a holistic design synthesis and understanding, overlooking the intricate interplay of electrical, logical, and physical facets of circuit data. This perspective paper argues for a paradigm shift from AI4EDA towards AI-native EDA, integrating AI at the core of the design process. Pivotal to this vision is the development of a multimodal circuit representation learning technique, poised to provide a comprehensive understanding by harmonizing and extracting insights from varied data sources, such as functional specifications, RTL designs, circuit netlists, and physical layouts.
  We champion the creation of large circuit models (LCMs) that are inherently multimodal, crafted to decode and express the rich semantics and structures of circuit data, thus fostering more resilient, efficient, and inventive design methodologies. Embracing this AI-native philosophy, we foresee a trajectory that transcends the current innovation plateau in EDA, igniting a profound shift-left in electronic design methodology. The envisioned advancements herald not just an evolution of existing EDA tools but a revolution, giving rise to novel instruments of design-tools that promise to radically enhance design productivity and inaugurate a new epoch where the optimization of circuit performance, power, and area (PPA) is achieved not incrementally, but through leaps that redefine the benchmarks of electronic systems' capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07257v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei Chen (Huawei Noah's Ark Lab), Yiqi Chen (Peking University), Zhufei Chu (Ningbo University), Wenji Fang (Hong Kong University of Science and Technology), Tsung-Yi Ho (The Chinese University of Hong Kong), Yu Huang (Huawei HiSilicon), Sadaf Khan (The Chinese University of Hong Kong), Min Li (Huawei Noah's Ark Lab), Xingquan Li (Peng Cheng Laboratory), Yun Liang (Peking University), Yibo Lin (Peking University), Jinwei Liu (The Chinese University of Hong Kong), Yi Liu (The Chinese University of Hong Kong), Guojie Luo (Peking University), Zhengyuan Shi (The Chinese University of Hong Kong), Guangyu Sun (Peking University), Dimitrios Tsaras (Huawei Noah's Ark Lab), Runsheng Wang (Peking University), Ziyi Wang (The Chinese University of Hong Kong), Xinming Wei (Peking University), Zhiyao Xie (Hong Kong University of Science and Technology), Qiang Xu (The Chinese University of Hong Kong), Chenhao Xue (Peking University), Evangeline F. Y. Young (The Chinese University of Hong Kong), Bei Yu (The Chinese University of Hong Kong), Mingxuan Yuan (Huawei Noah's Ark Lab), Haoyi Zhang (Peking University), Zuodong Zhang (Peking University), Yuxiang Zhao (Peking University), Hui-Ling Zhen (Huawei Noah's Ark Lab), Ziyang Zheng (The Chinese University of Hong Kong), Binwu Zhu (The Chinese University of Hong Kong), Keren Zhu (The Chinese University of Hong Kong), Sunan Zou (Peking University)</dc:creator>
    </item>
    <item>
      <title>Performance Analysis of Matrix Multiplication for Deep Learning on the Edge</title>
      <link>https://arxiv.org/abs/2403.07731</link>
      <description>arXiv:2403.07731v1 Announce Type: new 
Abstract: The devices designed for the Internet-of-Things encompass a large variety of distinct processor architectures, forming a highly heterogeneous zoo. In order to tackle this, we employ a simulator to estimate the performance of the matrix-matrix multiplication (GEMM) kernel on processors designed to operate at the edge. Our simulator adheres to the modern implementations of GEMM, advocated by GotoBLAS2, BLIS, OpenBLAS, etc., to carefully account for the amount of data transfers across the memory hierarchy of different algorithmic variants of the kernel. %Armed with this tool, A small collection of experiments provide the necessary data to calibrate the simulator and deliver highly accurate estimations of the execution time for a given processor architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07731v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-23220-6_5</arxiv:DOI>
      <arxiv:journal_reference>High Performance Computing. ISC High Performance 2022 International Workshops. ISC High Performance 2022. Lecture Notes in Computer Science, vol 13387. Springer, Cham</arxiv:journal_reference>
      <dc:creator>Cristian Ram\'irez, Adri\'an Castell\'o, H\'ector Mart\'inez, Enrique S. Quintana-Ort\'i</dc:creator>
    </item>
    <item>
      <title>Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs</title>
      <link>https://arxiv.org/abs/2311.04417</link>
      <description>arXiv:2311.04417v2 Announce Type: replace 
Abstract: The relentless advancement of artificial intelligence (AI) and machine learning (ML) applications necessitates the development of specialized hardware accelerators capable of handling the increasing complexity and computational demands. Traditional computing architectures, based on the von Neumann model, are being outstripped by the requirements of contemporary AI/ML algorithms, leading to a surge in the creation of accelerators like the Graphcore Intelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit (RDU), and enhanced GPU platforms. These hardware accelerators are characterized by their innovative data-flow architectures and other design optimizations that promise to deliver superior performance and energy efficiency for AI/ML tasks.
  This research provides a preliminary evaluation and comparison of these commercial AI/ML accelerators, delving into their hardware and software design features to discern their strengths and unique capabilities. By conducting a series of benchmark evaluations on common DNN operators and other AI/ML workloads, we aim to illuminate the advantages of data-flow architectures over conventional processor designs and offer insights into the performance trade-offs of each platform. The findings from our study will serve as a valuable reference for the design and performance expectations of research prototypes, thereby facilitating the development of next-generation hardware accelerators tailored for the ever-evolving landscape of AI/ML applications. Through this analysis, we aspire to contribute to the broader understanding of current accelerator technologies and to provide guidance for future innovations in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04417v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongwu Peng, Caiwen Ding, Tong Geng, Sutanay Choudhury, Kevin Barker, Ang Li</dc:creator>
    </item>
    <item>
      <title>Quantum Control Machine: The Limits of Control Flow in Quantum Programming</title>
      <link>https://arxiv.org/abs/2304.15000</link>
      <description>arXiv:2304.15000v4 Announce Type: replace-cross 
Abstract: Quantum algorithms for tasks such as factorization, search, and simulation rely on control flow such as branching and iteration that depends on the value of data in superposition. High-level programming abstractions for control flow, such as switches, loops, and higher-order functions, are ubiquitous in classical languages. By contrast, many quantum languages do not provide high-level abstractions for control flow in superposition, and instead require the use of hardware-level logic gates to implement such control flow.
  The reason for this gap is that whereas a classical computer supports control flow using a program counter that can depend on data, the typical architecture of a quantum computer does not provide a program counter that can depend on data in superposition. As a result, the complete set of control flow abstractions that can be correctly realized on a quantum computer has not yet been established.
  In this work, we provide a complete characterization of the properties of control flow abstractions that are correctly realizable on a quantum computer. First, we prove that even on a quantum computer whose program counter exists in superposition, one cannot correctly realize control flow in quantum algorithms by lifting the classical conditional jump instruction to work in superposition. This theorem denies the ability to directly lift general abstractions for control flow such as the $\lambda$-calculus from classical to quantum programming.
  In response, we present the necessary and sufficient conditions for control flow to be correctly realizable on a quantum computer. We introduce the quantum control machine, an instruction set architecture featuring a conditional jump that is restricted to satisfy these conditions. We show how this design enables a developer to correctly express control flow in quantum algorithms using a program counter in place of logic gates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.15000v4</guid>
      <category>cs.PL</category>
      <category>cs.AR</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3649811</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Program. Lang., Vol. 8, No. OOPSLA1, Article 94. Publication date: April 2024</arxiv:journal_reference>
      <dc:creator>Charles Yuan, Agnes Villanyi, Michael Carbin</dc:creator>
    </item>
    <item>
      <title>PreRoutGNN for Timing Prediction with Order Preserving Partition: Global Circuit Pre-training, Local Delay Learning and Attentional Cell Modeling</title>
      <link>https://arxiv.org/abs/2403.00012</link>
      <description>arXiv:2403.00012v2 Announce Type: replace-cross 
Abstract: Pre-routing timing prediction has been recently studied for evaluating the quality of a candidate cell placement in chip design. It involves directly estimating the timing metrics for both pin-level (slack, slew) and edge-level (net delay, cell delay), without time-consuming routing. However, it often suffers from signal decay and error accumulation due to the long timing paths in large-scale industrial circuits. To address these challenges, we propose a two-stage approach. First, we propose global circuit training to pre-train a graph auto-encoder that learns the global graph embedding from circuit netlist. Second, we use a novel node updating scheme for message passing on GCN, following the topological sorting sequence of the learned graph embedding and circuit graph. This scheme residually models the local time delay between two adjacent pins in the updating sequence, and extracts the lookup table information inside each cell via a new attention mechanism. To handle large-scale circuits efficiently, we introduce an order preserving partition scheme that reduces memory consumption while maintaining the topological dependencies. Experiments on 21 real world circuits achieve a new SOTA R2 of 0.93 for slack prediction, which is significantly surpasses 0.59 by previous SOTA method. Code will be available at: https://github.com/Thinklab-SJTU/EDA-AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00012v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruizhe Zhong, Junjie Ye, Zhentao Tang, Shixiong Kai, Mingxuan Yuan, Jianye Hao, Junchi Yan</dc:creator>
    </item>
  </channel>
</rss>
