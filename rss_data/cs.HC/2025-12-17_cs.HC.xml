<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Dec 2025 02:35:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>From Framework to Practice: Designing a Real-World Telehealth Application for Palliative Care</title>
      <link>https://arxiv.org/abs/2512.13693</link>
      <description>arXiv:2512.13693v1 Announce Type: new 
Abstract: As digital health solutions continue to reshape healthcare delivery, telehealth software applications have become vital for improving accessibility, continuity of care, and patient outcomes. This paper presents an analysis of designing a software application focused on Enhanced Telehealth Capabilities (ETHC) for palliative care, integrating across three socio-technical dimensions: quality, human values, and real-world. Designing for quality attributes -- such as performance, maintainability, safety, and security -- ensured that the system is technically robust and compliant with clinical standards. Designing for human values -- empathy, inclusivity, accessibility, and transparency -- helped enhance patient experience, trust, and ethical alignment. Designing for real-world -- through a multidisciplinary, experience-based co-design approach involving clinicians, patients, and carers that guided iterative cycles of prototyping, usability testing, and real-world evaluation -- ensured continuous refinement of features and alignment with clinical practice. The resulting telehealth software solution demonstrated that our socio-technical design framework was successful in producing a secure, equitable, and resilient digital health application. Our design approach can assist others designing software in health and other domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13693v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Zhou, Rashina Hoda, Andy Li, Chris Bain, Laura Bird, Emmy Trinh, Peter Poon, Teresa O Brien, Mahima Kalla, Olivia Metcalf, Wendy Chapman, Joycelyn Ling, Sam Georgy, David Bevan</dc:creator>
    </item>
    <item>
      <title>Learning to Car-Follow Using an Inertia-Oriented Driving Technique: A Before-and-After Study on a Closed Circuit</title>
      <link>https://arxiv.org/abs/2512.13694</link>
      <description>arXiv:2512.13694v1 Announce Type: new 
Abstract: For decades, car following and traffic flow models have assumed that drivers default driving strategy is to maintain a safe distance. Several previous studies have questioned whether the Driving to Keep Distance is a traffic invariant. Therefore, the acceleration deceleration torque asymmetry of drivers must necessarily determine the observed patterns of traffic oscillations. Those studies indicate that drivers can adopt alternative CF strategies, such as Driving to Keep Inertia, by following basic instructions. The present work extends the evidence from previous research by showing the effectiveness of a DI course that immediately translates into practice on a closed circuit. Twelve drivers were invited to follow a lead car that varied its speed on a real circuit. Then, the driver took a DI course and returned to the same real car following scenario. Drivers generally adopted DD as the default CF mode in the pretest, both in field and simulated PC conditions, yielding very similar results. After taking the full DI course, drivers showed significantly less acceleration, deceleration, and speed variability than did the pretest, both in the field and in the simulated conditions, which indicates that drivers adopted the DI strategy. This study is the first to show the potential of adopting a DI strategy in a real circuit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13694v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.RO</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kostantinos Mattas, Antonio Lucas-Alba, Tomer Toledo, Oscar M. Melchor, Shlomo Bekhor, Biagio Ciuffo</dc:creator>
    </item>
    <item>
      <title>Juicy Text: Onomatopoeia and Semantic Text Effects for Juicy Player Experiences</title>
      <link>https://arxiv.org/abs/2512.13695</link>
      <description>arXiv:2512.13695v1 Announce Type: new 
Abstract: Juiciness is visual pizzazz used to improve player experience and engagement in games. Most research has focused on juicy particle effects. However, text effects are also commonly used in games, albeit not always juiced up. One type is onomatopoeia, a well-defined element of human language that has been translated to visual media, such as comic books and games. Another is semantic text, often used to provide performance feedback in games. In this work, we explored the relationship between juiciness and text effects, aiming to replicate juicy user experiences with text-based juice and combining particle and text juice. We show in a multi-phase within-subjects experiment that users rate juicy text effects similarly to particles effects, with comparable performance, and more reliable feedback. We also hint at potential improvement in user experience when both are combined, and how text stimuli may be perceived differently than other visual ones. We contribute empirical findings on the juicy-text connection in the context of visual effects for interactive media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13695v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3678957.3685755</arxiv:DOI>
      <arxiv:journal_reference>ICMI 2024</arxiv:journal_reference>
      <dc:creator>\'Emilie Fabre, Katie Seaborn, Adrien Alexandre Verhulst, Yuta Itoh, Jun Rekimoto</dc:creator>
    </item>
    <item>
      <title>LAPPI: Interactive Optimization with LLM-Assisted Preference-Based Problem Instantiation</title>
      <link>https://arxiv.org/abs/2512.14138</link>
      <description>arXiv:2512.14138v1 Announce Type: new 
Abstract: Many real-world tasks, such as trip planning or meal planning, can be formulated as combinatorial optimization problems. However, using optimization solvers is difficult for end users because it requires problem instantiation: defining candidate items, assigning preference scores, and specifying constraints. We introduce LAPPI (LLM-Assisted Preference-based Problem Instantiation), an interactive approach that uses large language models (LLMs) to support users in this instantiation process. Through natural language conversations, the system helps users transform vague preferences into well-defined optimization problems. These instantiated problems are then passed to existing optimization solvers to generate solutions. In a user study on trip planning, our method successfully captured user preferences and generated feasible plans that outperformed both conventional and prompt-engineering approaches. We further demonstrate LAPPI's versatility by adapting it to an additional use case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14138v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>So Kuroki, Manami Nakagawa, Shigeo Yoshida, Yuki Koyama, Kozuno Tadashi</dc:creator>
    </item>
    <item>
      <title>The Trust in AI-Generated Health Advice (TAIGHA) Scale and Short Version (TAIGHA-S): Development and Validation Study</title>
      <link>https://arxiv.org/abs/2512.14278</link>
      <description>arXiv:2512.14278v1 Announce Type: new 
Abstract: Artificial Intelligence tools such as large language models are increasingly used by the public to obtain health information and guidance. In health-related contexts, following or rejecting AI-generated advice can have direct clinical implications. Existing instruments like the Trust in Automated Systems Survey assess trustworthiness of generic technology, and no validated instrument measures users' trust in AI-generated health advice specifically. This study developed and validated the Trust in AI-Generated Health Advice (TAIGHA) scale and its four-item short form (TAIGHA-S) as theory-based instruments measuring trust and distrust, each with cognitive and affective components. The items were developed using a generative AI approach, followed by content validation with 10 domain experts, face validation with 30 lay participants, and psychometric validation with 385 UK participants who received AI-generated advice in a symptom-assessment scenario. After automated item reduction, 28 items were retained and reduced to 10 based on expert ratings. TAIGHA showed excellent content validity (S-CVI/Ave=0.99) and CFA confirmed a two-factor model with excellent fit (CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03). Internal consistency was high ({\alpha}=0.95). Convergent validity was supported by correlations with the Trust in Automated Systems Survey (r=0.67/-0.66) and users' reliance on the AI's advice (r=0.37 for trust), while divergent validity was supported by low correlations with reading flow and mental load (all |r|&lt;0.25). TAIGHA-S correlated highly with the full scale (r=0.96) and showed good reliability ({\alpha}=0.88). TAIGHA and TAIGHA-S are validated instruments for assessing user trust and distrust in AI-generated health advice. Reporting trust and distrust separately permits a more complete evaluation of AI interventions, and the short scale is well-suited for time-constrained settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14278v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marvin Kopka, Azeem Majeed, Gabriella Spinelli, Austen El-Osta, Markus Feufel</dc:creator>
    </item>
    <item>
      <title>Creating Opportunities: Co-designing an mHealth App with Older Adults</title>
      <link>https://arxiv.org/abs/2512.14641</link>
      <description>arXiv:2512.14641v1 Announce Type: new 
Abstract: We conducted a qualitative co-design study with four adults aged 60+ to gather design insights on a Figma prototype and a generative AI (GenAI) chatbot for an app aimed at providing an AI coach to support older adults' physical activity. The initial design for both incorporates several novel aspects: a curated health knowledge base, personalised responses based on goals and health history, privacy considerations, integration with wearables for physical activity context, as well as dynamic context injection. The study yielded feedback on improving both the proposed user experience in the app and the conversation flow with the chatbot, and it will aid future work aimed at implementing a GenAI-powered health coach for older adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14641v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinav Choudhry, Bashab Mazumder, Lauren Alyssa Marks, Roqaya Elmenshawy, Devorah Kletenik, Sean Mullen, Rachel F. Adler</dc:creator>
    </item>
    <item>
      <title>State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models</title>
      <link>https://arxiv.org/abs/2512.13762</link>
      <description>arXiv:2512.13762v1 Announce Type: cross 
Abstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13762v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>TK Lee</dc:creator>
    </item>
    <item>
      <title>Beyond Procedural Compliance: Human Oversight as a Dimension of Well-being Efficacy in AI Governance</title>
      <link>https://arxiv.org/abs/2512.13768</link>
      <description>arXiv:2512.13768v1 Announce Type: cross 
Abstract: Major AI ethics guidelines and laws, including the EU AI Act, call for effective human oversight, but do not define it as a distinct and developable capacity. This paper introduces human oversight as a well-being capacity, situated within the emerging Well-being Efficacy framework. The concept integrates AI literacy, ethical discernment, and awareness of human needs, acknowledging that some needs may be conflicting or harmful. Because people inevitably project desires, fears, and interests into AI systems, oversight requires the competence to examine and, when necessary, restrain problematic demands.
  The authors argue that the sustainable and cost-effective development of this capacity depends on its integration into education at every level, from professional training to lifelong learning. The frame of human oversight as a well-being capacity provides a practical path from high-level regulatory goals to the continuous cultivation of human agency and responsibility essential for safe and ethical AI. The paper establishes a theoretical foundation for future research on the pedagogical implementation and empirical validation of well-being effectiveness in multiple contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13768v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yao Xie, Walter Cullen</dc:creator>
    </item>
    <item>
      <title>EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models</title>
      <link>https://arxiv.org/abs/2512.13806</link>
      <description>arXiv:2512.13806v1 Announce Type: cross 
Abstract: Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13806v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Siegfried Ludwig, Stylianos Bakas, Konstantinos Barmpas, Georgios Zoumpourlis, Dimitrios A. Adamos, Nikolaos Laskaris, Yannis Panagakis, Stefanos Zafeiriou</dc:creator>
    </item>
    <item>
      <title>A Fair, Flexible, Zero-Waste Digital Electricity Market: A First-Principles Approach Combining Automatic Market Making, Holarchic Architectures and Shapley Theory</title>
      <link>https://arxiv.org/abs/2512.13871</link>
      <description>arXiv:2512.13871v2 Announce Type: cross 
Abstract: This thesis presents a fundamental rethink of electricity market design at the wholesale and balancing layers. Rather than treating markets as static spot clearing mechanisms, it reframes them as a continuously online, event driven dynamical control system: a two sided marketplace operating directly on grid physics.
  Existing energy only, capacity augmented, and zonal market designs are shown to admit no shock robust Nash equilibrium under realistic uncertainty, instead relying on price caps, uplift, and regulatory intervention to preserve solvency and security. In response, the thesis develops a holarchic Automatic Market Maker (AMM) in which prices are bounded, exogenous control signals derived from physical tightness rather than emergent equilibrium outcomes.
  The AMM generalises nodal and zonal pricing through nested scarcity layers, from node to cluster to zone to region to system, such that participant facing prices inherit from the tightest binding constraint. Nodal and zonal pricing therefore emerge as special cases of a unified scarcity propagation rule.
  Beyond pricing, the AMM functions as a scarcity aware control system and a digitally enforceable rulebook for fair access and proportional allocation under shortage. Fuel costs are recovered through pay as bid energy dispatch consistent with merit order, while non fuel operating and capital costs are allocated according to adequacy, flexibility, and locational contribution.
  Large scale simulations demonstrate bounded input bounded output stability, controllable procurement costs, zero structural waste, and improved distributional outcomes. The architecture is climate aligned and policy configurable, but requires a managed transition and new operational tools for system operators and market participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13871v2</guid>
      <category>eess.SY</category>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaun Sweeney, Robert Shorten, Mark O'Malley</dc:creator>
    </item>
    <item>
      <title>Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming</title>
      <link>https://arxiv.org/abs/2512.13914</link>
      <description>arXiv:2512.13914v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.
  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13914v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bhargav Chickmagalur Nanjundappa, Spandan Maaheshwari</dc:creator>
    </item>
    <item>
      <title>Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025</title>
      <link>https://arxiv.org/abs/2512.14012</link>
      <description>arXiv:2512.14012v1 Announce Type: cross 
Abstract: The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14012v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruanqianqian Huang, Avery Reyna, Sorin Lerner, Haijun Xia, Brian Hempel</dc:creator>
    </item>
    <item>
      <title>Multilingual and Continuous Backchannel Prediction: A Cross-lingual Study</title>
      <link>https://arxiv.org/abs/2512.14085</link>
      <description>arXiv:2512.14085v1 Announce Type: cross 
Abstract: We present a multilingual, continuous backchannel prediction model for Japanese, English, and Chinese, and use it to investigate cross-linguistic timing behavior. The model is Transformer-based and operates at the frame level, jointly trained with auxiliary tasks on approximately 300 hours of dyadic conversations. Across all three languages, the multilingual model matches or surpasses monolingual baselines, indicating that it learns both language-universal cues and language-specific timing patterns. Zero-shot transfer with two-language training remains limited, underscoring substantive cross-lingual differences. Perturbation analyses reveal distinct cue usage: Japanese relies more on short-term linguistic information, whereas English and Chinese are more sensitive to silence duration and prosodic variation; multilingual training encourages shared yet adaptable representations and reduces overreliance on pitch in Chinese. A context-length study further shows that Japanese is relatively robust to shorter contexts, while Chinese benefits markedly from longer contexts. Finally, we integrate the trained model into a real-time processing software, demonstrating CPU-only inference. Together, these findings provide a unified model and empirical evidence for how backchannel timing differs across languages, informing the design of more natural, culturally-aware spoken dialogue systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14085v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Koji Inoue, Mikey Elmers, Yahui Fu, Zi Haur Pang, Taiga Mori, Divesh Lala, Keiko Ochi, Tatsuya Kawahara</dc:creator>
    </item>
    <item>
      <title>Towards Explainable Quantum AI: Informing the Encoder Selection of Quantum Neural Networks via Visualization</title>
      <link>https://arxiv.org/abs/2512.14181</link>
      <description>arXiv:2512.14181v1 Announce Type: cross 
Abstract: Quantum Neural Networks (QNNs) represent a promising fusion of quantum computing and neural network architectures, offering speed-ups and efficient processing of high-dimensional, entangled data. A crucial component of QNNs is the encoder, which maps classical input data into quantum states. However, choosing suitable encoders remains a significant challenge, largely due to the lack of systematic guidance and the trial-and-error nature of current approaches. This process is further impeded by two key challenges: (1) the difficulty in evaluating encoded quantum states prior to training, and (2) the lack of intuitive methods for analyzing an encoder's ability to effectively distinguish data features. To address these issues, we introduce a novel visualization tool, XQAI-Eyes, which enables QNN developers to compare classical data features with their corresponding encoded quantum states and to examine the mixed quantum states across different classes. By bridging classical and quantum perspectives, XQAI-Eyes facilitates a deeper understanding of how encoders influence QNN performance. Evaluations across diverse datasets and encoder designs demonstrate XQAI-Eyes's potential to support the exploration of the relationship between encoder design and QNN effectiveness, offering a holistic and transparent approach to optimizing quantum encoders. Moreover, domain experts used XQAI-Eyes to derive two key practices for quantum encoder selection, grounded in the principles of pattern preservation and feature mapping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14181v1</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaolun Ruan, Feng Liang, Rohan Ramakrishna, Chao Ren, Rudai Yan, Qiang Guan, Jiannan Li, Yong Wang</dc:creator>
    </item>
    <item>
      <title>Not all Chess960 positions are equally complex</title>
      <link>https://arxiv.org/abs/2512.14319</link>
      <description>arXiv:2512.14319v1 Announce Type: cross 
Abstract: We analyze strategic complexity across all 960 Chess960 (Fischer Random Chess) starting positions. Stockfish evaluations show a near-universal first-move advantage for White ($\langle E \rangle = +0.30 \pm 0.14$ pawns), indicating that the advantage conferred by moving first is a robust structural feature of the game. To quantify decision difficulty, we introduce an information-based measure $S(n)$ describing the cumulative information required to identify optimal moves over the first $n$ plies. This measure decomposes into contributions from White and Black, $S_W$ and $S_B$, yielding a total opening complexity $S_{\mathrm{tot}} = S_W + S_B$ and a decision asymmetry $A=S_B-S_W$. Across the ensemble, $S_{\mathrm{tot}}$ varies by a factor of three, while $A$ spans from $-2.5$ to $+1.8$ bits, showing that some openings burden White and others Black. The mean $\langle A \rangle = -0.25$ bits indicates a slight tendency for White to face harder opening decisions. Standard chess (position \#518, \texttt{RNBQKBNR}) exhibits above-average asymmetry (91st percentile) but typical overall complexity (47th percentile). The most complex opening is \#226 (\texttt{BNRQKBNR}), whereas \#198 (\texttt{QNBRKBNR})is the most balanced, with both evaluation and asymmetry near zero. These results reveal a highly heterogeneous Chess960 landscape in which small rearrangements of the back-rank pieces can significantly alter strategic depth and competitive fairness. Remarkably, the classical starting position-despite centuries of cultural selection-lies far from the most balanced configuration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14319v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.HC</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Barthelemy</dc:creator>
    </item>
    <item>
      <title>MoT: A Model-Driven Low-Code Approach for Simplifying Cloud-of-Things Application Development</title>
      <link>https://arxiv.org/abs/2512.14613</link>
      <description>arXiv:2512.14613v1 Announce Type: cross 
Abstract: The integration of cloud computing and the Internet of Things (IoT) is essential for scalable, intelligent systems. However, developing cloud-of-things (CoT) applications remains challenging. It requires significant technical expertise and lacks standardized, model-driven methodologies. Current approaches fail to ensure interoperability, automation, and efficiency. This study introduces the Model of Things (MoT), a model-based approach that incorporates low-code principles to simplify CoT development. MoT reduces technical barriers by providing a custom UML profile designed for IoT and cloud services. To evaluate MoT, we conducted a case study and a Technology Acceptance Model (TAM) questionnaire. The results confirmed MoT's feasibility, demonstrating that it streamlines CoT application development and deployment. Users found MoT accessible, even with limited IoT experience, and reported high perceived ease of use and usefulness. Qualitative feedback highlighted MoT's ability to reduce complexity and speed up development. MoT offers a promising, model-driven solution for CoT application development. By lowering entry barriers and promoting automation, it enhances both efficiency and flexibility. This study represents a step toward a more user-friendly framework, enabling broader adoption of CoT technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14613v1</guid>
      <category>cs.SE</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristiano Welter, Kleinner Farias</dc:creator>
    </item>
    <item>
      <title>How K-12 Educators Use AI: LLM-Assisted Qualitative Analysis at Scale</title>
      <link>https://arxiv.org/abs/2507.17985</link>
      <description>arXiv:2507.17985v3 Announce Type: replace 
Abstract: This study investigates how K-12 educators use generative AI tools in real-world instructional contexts and how large language models (LLMs) can support scalable qualitative analysis of these interactions. Drawing on over 13,000 unscripted educator-AI conversations from an open-access platform, we examine educators' use of AI for lesson planning, differentiation, assessment, and pedagogical reflection. Methodologically, we introduce a replicable, LLM-assisted qualitative analysis pipeline that supports inductive theme discovery, codebook development, and large-scale annotation while preserving researcher control over conceptual synthesis. Empirically, the findings surface concrete patterns in how educators prompt, adapt, and evaluate AI-generated suggestions as part of their instructional reasoning. This work demonstrates the feasibility of combining LLM support with qualitative rigor to analyze complex educator behaviors at scale and inform the design of AI-powered educational tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17985v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Liu, Lief Esbenshade, Shawon Sarkar, Victor Tian, Zachary Zhang, Kevin He, Min Sun</dc:creator>
    </item>
    <item>
      <title>One Size Fits None: A Personalized Framework for Urban Accessibility Using Exponential Decay</title>
      <link>https://arxiv.org/abs/2512.08941</link>
      <description>arXiv:2512.08941v2 Announce Type: replace 
Abstract: This study develops a personalized accessibility framework that integrates exponential decay functions with user-customizable weighting systems. The framework enables real-time, personalized urban evaluation based on individual priorities and lifestyle requirements. The methodology employs grid-based discretization and a two-stage computational architecture that separates intensive preprocessing from lightweight real-time calculations. The computational architecture demonstrates that accessibility modelling can be made accessible to non-technical users through interactive interfaces, enabling fine-grained spatial analysis and identification of accessibility variations within neighbourhoods. The research contributes to Sustainable Development Goal 11's vision of inclusive, sustainable cities by providing tools for understanding how different populations experience identical urban spaces, supporting evidence-based policy development that addresses accessibility gaps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08941v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Prabhanjana Ghuriki, S. Chanti</dc:creator>
    </item>
    <item>
      <title>Immutable Explainability: Fuzzy Logic and Blockchain for Verifiable Affective AI</title>
      <link>https://arxiv.org/abs/2512.11065</link>
      <description>arXiv:2512.11065v2 Announce Type: replace 
Abstract: Affective artificial intelligence has made substantial advances in recent years; yet two critical issues persist, particularly in sensitive applications. First, these systems frequently operate as 'black boxes', leaving their decision-making processes opaque. Second, audit logs often lack reliability, as the entity operating the system may alter them. In this work, we introduce the concept of Immutable Explainability, an architecture designed to address both challenges simultaneously. Our approach combines an interpretable inference engine - implemented through fuzzy logic to produce a transparent trace of each decision - with a cryptographic anchoring mechanism that records this trace on a blockchain, ensuring that it is tamper-evident and independently verifiable. To validate the approach, we implemented a heuristic pipeline integrating lexical and prosodic analysis within an explicit Mamdani-type multimodal fusion engine. Each inference generates an auditable record that is subsequently anchored on a public blockchain (Sepolia Testnet). We evaluated the system using the Spanish MEACorpus 2023, employing both the original corpus transcriptions and those generated by Whisper. The results show that our fuzzy-fusion approach outperforms baseline methods (linear and unimodal fusion). Beyond these quantitative outcomes, our primary objective is to establish a foundation for affective AI systems that offer transparent explanations, trustworthy audit trails, and greater user control over personal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11065v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcelo Fransoy, Alejandro Hossian, Hern\'an Merlino</dc:creator>
    </item>
    <item>
      <title>Mirror Skin: In Situ Visualization of Robot Touch Intent on Robotic Skin</title>
      <link>https://arxiv.org/abs/2512.11472</link>
      <description>arXiv:2512.11472v2 Announce Type: replace 
Abstract: Effective communication of robotic touch intent is a key factor in promoting safe and predictable physical human-robot interaction (pHRI). While intent communication has been widely studied, existing approaches lack the spatial specificity and semantic depth necessary to convey robot touch actions. We present Mirror Skin, a cephalopod-inspired concept that utilizes high-resolution, mirror-like visual feedback on robotic skin. By mapping in-situ visual representations of a human's body parts onto the corresponding robot's touch region, Mirror Skin communicates who shall initiate touch, where it will occur, and when it is imminent. To inform the design of Mirror Skin, we conducted a structured design exploration with experts in virtual reality (VR), iteratively refining six key dimensions. A subsequent controlled user study demonstrated that Mirror Skin significantly enhances accuracy and reduces response times for interpreting touch intent. These findings highlight the potential of visual feedback on robotic skin to communicate human-robot touch interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11472v2</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Wagmann, Matti Kr\"uger, Chao Wang, J\"urgen Steimle</dc:creator>
    </item>
    <item>
      <title>Love First, Know Later: Persona-Based Romantic Compatibility Through LLM Text World Engines</title>
      <link>https://arxiv.org/abs/2512.11844</link>
      <description>arXiv:2512.11844v2 Announce Type: replace 
Abstract: We propose Love First, Know Later: a paradigm shift in computational matching that simulates interactions first, then assesses compatibility. Instead of comparing static profiles, our framework leverages LLMs as text world engines that operate in dual capacity-as persona-driven agents following behavioral policies and as the environment modeling interaction dynamics. We formalize compatibility assessment as a reward-modeling problem: given observed matching outcomes, we learn to extract signals from simulations that predict human preferences. Our key insight is that relationships hinge on responses to critical moments-we translate this observation from relationship psychology into mathematical hypotheses, enabling effective simulation. Theoretically, we prove that as LLM policies better approximate human behavior, the induced matching converges to optimal stable matching. Empirically, we validate on speed dating data for initial chemistry and divorce prediction for long-term stability. This paradigm enables interactive, personalized matching systems where users iteratively refine their agents, unlocking future possibilities for transparent and interactive compatibility assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11844v2</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyang Shang, Zhengyang Yan, Xuan Liu</dc:creator>
    </item>
    <item>
      <title>Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels</title>
      <link>https://arxiv.org/abs/2512.13142</link>
      <description>arXiv:2512.13142v2 Announce Type: replace-cross 
Abstract: As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13142v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anika Sharma, Malavika Mampally, Chidaksh Ravuru, Kandyce Brennan, Neil Gaikwad</dc:creator>
    </item>
  </channel>
</rss>
