<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Jun 2024 04:00:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>QuickCurve: revisiting slightly non-planar 3D printing</title>
      <link>https://arxiv.org/abs/2406.03966</link>
      <description>arXiv:2406.03966v1 Announce Type: new 
Abstract: Additive manufacturing builds physical objects by accumulating layers upon layers of solidified material. This process is typically done with horizontal planar layers. However, fused filament printers have the capability to extrude material along 3D curves. The idea of depositing out-of-plane, also known as non-planar printing, has spawned a trend of research towards algorithms that could generate non-planar deposition paths automatically from a 3D object. In this paper we introduce a novel algorithm for this purpose. Our method optimizes for a curved slicing surface. This surface is intersected with the input model to extract non-planar layers, with the objective of accurately reproducing the model top surfaces while avoiding collisions. Our formulation leads to a simple and efficient approach that only requires solving for a single least-square problem. Notably, it does not require a tetrahedralization of the input or iterative solver passes, while being more general than simpler approaches. We further explore how to orient the paths to follow the principal curvatures of the surfaces, how to filter spurious tiny features damaging the results, and how to achieve a good compromise of mixing planar and non-planar strategies within the same part. We present a complete formulation and its implementation, and demonstrate our method on a variety of 3D printed models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03966v1</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Emilio Ottonello, Pierre-Alexandre Hugron, Alberto Parmiggiani, Sylvain Lefebvre</dc:creator>
    </item>
    <item>
      <title>A Versatile Collage Visualization Technique</title>
      <link>https://arxiv.org/abs/2406.04008</link>
      <description>arXiv:2406.04008v1 Announce Type: new 
Abstract: Collage techniques are commonly used in visualization to organize a collection of geometric shapes, facilitating the representation of visual features holistically, as seen in word clouds or circular packing diagrams. Typically, packing methods rely on object-space optimization techniques, which often necessitate customizing the optimization process to suit the complexity of geometric primitives and the specific application requirements. In this paper, we introduce a versatile image-space collage technique designed to pack geometric elements into a given shape. Leveraging a differential renderer and image-space losses, our optimization process is highly efficient and can easily accommodate various loss functions. We demonstrate the diverse visual expressiveness of our approach across various visualization applications. The evaluation confirmed the benefits of our method in terms of both visual quality and time performance. The project page is https://szuviz.github.io/pixel-space-collage-technique/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04008v1</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenyu Wang, Daniel Cohen-Or, Min Lu</dc:creator>
    </item>
    <item>
      <title>Interactive zoom display in smartphone-based digital holographic microscope for 3D imaging</title>
      <link>https://arxiv.org/abs/2406.04014</link>
      <description>arXiv:2406.04014v1 Announce Type: new 
Abstract: Digital holography has applications in bio-imaging because it can simultaneously obtain the amplitude and phase information of a microscopic sample in a single shot, thus facilitating non-contact, noninvasive observation of the 3D shape of transparent objects (phase objects, which can be mapped with the phase information,) and moving objects. The combination of digital holography and microscopy is called digital holographic microscopy (DHM). In this study, we propose a compact and inexpensive smartphone-based DHM system for 3D imaging; this system includes an optical system comprising a 3D printer using commercially available image sensors and semiconductor lasers; further, an Android-based application is used to reconstruct the holograms acquired by this optical system, thus outlining the amplitude and phase information of the observed object. Also, by utilizing scalable diffraction calculation methods and touchscreen interaction, we implemented zoom functionality through pinch-in gestures. The study results showed that the DHM system successfully obtained the amplitude and phase information of the observed object via the acquired holograms in an almost real time manner. Thus, this study showed that it is possible to construct a low cost and compact DHM system that includes a 3D printer to construct the optical system and a smartphone application to reconstruct the holograms. This system is also expected to contribute to biology fieldwork and pathological diagnosis in remote areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04014v1</guid>
      <category>cs.GR</category>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Nagahama</dc:creator>
    </item>
    <item>
      <title>Hi5: 2D Hand Pose Estimation with Zero Human Annotation</title>
      <link>https://arxiv.org/abs/2406.03599</link>
      <description>arXiv:2406.03599v1 Announce Type: cross 
Abstract: We propose a new large synthetic hand pose estimation dataset, Hi5, and a novel inexpensive method for collecting high-quality synthetic data that requires no human annotation or validation. Leveraging recent advancements in computer graphics, high-fidelity 3D hand models with diverse genders and skin colors, and dynamic environments and camera movements, our data synthesis pipeline allows precise control over data diversity and representation, ensuring robust and fair model training. We generate a dataset with 583,000 images with accurate pose annotation using a single consumer PC that closely represents real-world variability. Pose estimation models trained with Hi5 perform competitively on real-hand benchmarks while surpassing models trained with real data when tested on occlusions and perturbations. Our experiments show promising results for synthetic data as a viable solution for data representation problems in real datasets. Overall, this paper provides a promising new approach to synthetic data creation and annotation that can reduce costs and increase the diversity and quality of data for hand pose estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03599v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masum Hasan, Cengiz Ozel, Nina Long, Alexander Martin, Samuel Potter, Tariq Adnan, Sangwu Lee, Amir Zadeh, Ehsan Hoque</dc:creator>
    </item>
    <item>
      <title>Gear-NeRF: Free-Viewpoint Rendering and Tracking with Motion-aware Spatio-Temporal Sampling</title>
      <link>https://arxiv.org/abs/2406.03723</link>
      <description>arXiv:2406.03723v1 Announce Type: cross 
Abstract: Extensions of Neural Radiance Fields (NeRFs) to model dynamic scenes have enabled their near photo-realistic, free-viewpoint rendering. Although these methods have shown some potential in creating immersive experiences, two drawbacks limit their ubiquity: (i) a significant reduction in reconstruction quality when the computing budget is limited, and (ii) a lack of semantic understanding of the underlying scenes. To address these issues, we introduce Gear-NeRF, which leverages semantic information from powerful image segmentation models. Our approach presents a principled way for learning a spatio-temporal (4D) semantic embedding, based on which we introduce the concept of gears to allow for stratified modeling of dynamic regions of the scene based on the extent of their motion. Such differentiation allows us to adjust the spatio-temporal sampling resolution for each region in proportion to its motion scale, achieving more photo-realistic dynamic novel view synthesis. At the same time, almost for free, our approach enables free-viewpoint tracking of objects of interest - a functionality not yet achieved by existing NeRF-based methods. Empirical studies validate the effectiveness of our method, where we achieve state-of-the-art rendering and tracking performance on multiple challenging datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03723v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.MM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinhang Liu, Yu-Wing Tai, Chi-Keung Tang, Pedro Miraldo, Suhas Lohit, Moitreya Chatterjee</dc:creator>
    </item>
    <item>
      <title>High-Order Continuous Geometrical Validity</title>
      <link>https://arxiv.org/abs/2406.03756</link>
      <description>arXiv:2406.03756v1 Announce Type: cross 
Abstract: We propose a conservative algorithm to test the geometrical validity of simplicial (triangles, tetrahedra), tensor product (quadrilaterals, hexahedra), and mixed (prisms) elements of arbitrary polynomial order as they deform over a piecewise-linear trajectory.
  Our algorithm uses a combination of adaptive B\'ezier refinement and bisection search to determine if, when, and where the Jacobian determinant of an element's polynomial geometric map becomes negative in the transition from one configuration to another.
  Unlike previous approaches, our method preserves its properties also when implemented using floating point arithmetic: This feature comes at a small additional runtime cost compared to existing inexact methods, making it a drop-in replacement for current validity tests, while providing superior robustness and generality.
  To prove the practical effectiveness of our algorithm, we demonstrate its use in a high-order Incremental Potential Contact (IPC) elastodynamic simulator, and we experimentally show that it prevents invalid, simulation-breaking configurations that would otherwise occur using inexact methods, without the need for manual parameter tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03756v1</guid>
      <category>cs.CG</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federico Sichetti, Zizhou Huang, Marco Attene, Denis Zorin, Enrico Puppo, Daniele Panozzo</dc:creator>
    </item>
    <item>
      <title>More Bang For Your Buck(et): Fast and Space-efficient Hardware-accelerated Coarse-granular Indexing on GPUs</title>
      <link>https://arxiv.org/abs/2406.03965</link>
      <description>arXiv:2406.03965v1 Announce Type: cross 
Abstract: In recent work, we have shown that NVIDIA's raytracing cores on RTX video cards can be exploited to realize hardware-accelerated lookups for GPU-resident database indexes. On a high level, the concept materializes all keys as triangles in a 3D scene and indexes them. Lookups are performed by firing rays into the scene and utilizing the index structure to detect hits in a hardware-accelerated fashion. While this approach called RTIndeX (or short RX) is indeed promising, it currently suffers from three limitations: (1) significant memory overhead per key, (2) slow range-lookups, and (3) poor updateability. In this work, we show that all three problems can be tackled by a single design change: Generalizing RX to become a coarse-granular index cgRX. Instead of indexing individual keys, cgRX indexes buckets of keys which are post-filtered after retrieval. This drastically reduces the memory overhead, leads to the generation of a smaller and more efficient index structure, and enables fast range-lookups as well as updates. We will see that representing the buckets in the 3D space such that the lookup of a key is performed both correctly and efficiently requires the careful orchestration of firing rays in a specific sequence. Our experimental evaluation shows that cgRX offers the most bang for the buck(et) by providing a throughput in relation to the memory footprint that is 1.5-3x higher than for the comparable range-lookup supporting baselines. At the same time, cgRX improves the range-lookup performance over RX by up to 2x and offers practical updateability that is up to 5.5x faster than rebuilding from scratch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03965v1</guid>
      <category>cs.DB</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Justus Henneberg, Felix Schuhknecht, Rosina Kharal, Trevor Brown</dc:creator>
    </item>
    <item>
      <title>Global Parameterization-based Texture Space Optimization</title>
      <link>https://arxiv.org/abs/2406.04115</link>
      <description>arXiv:2406.04115v1 Announce Type: cross 
Abstract: Texture mapping is a common technology in the area of computer graphics, it maps the 3D surface space onto the 2D texture space. However, the loose texture space will reduce the efficiency of data storage and GPU memory addressing in the rendering process. Many of the existing methods focus on repacking given textures, but they still suffer from high computational cost and hardly produce a wholly tight texture space. In this paper, we propose a method to optimize the texture space and produce a new texture mapping which is compact based on global parameterization. The proposed method is computationally robust and efficient. Experiments show the effectiveness of the proposed method and the potency in improving the storage and rendering efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04115v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Chen, Yuxue Ren, Na Lei, Zhongxuan Luo, Xianfeng Gu</dc:creator>
    </item>
    <item>
      <title>Improving Physics-Augmented Continuum Neural Radiance Field-Based Geometry-Agnostic System Identification with Lagrangian Particle Optimization</title>
      <link>https://arxiv.org/abs/2406.04155</link>
      <description>arXiv:2406.04155v1 Announce Type: cross 
Abstract: Geometry-agnostic system identification is a technique for identifying the geometry and physical properties of an object from video sequences without any geometric assumptions. Recently, physics-augmented continuum neural radiance fields (PAC-NeRF) has demonstrated promising results for this technique by utilizing a hybrid Eulerian-Lagrangian representation, in which the geometry is represented by the Eulerian grid representations of NeRF, the physics is described by a material point method (MPM), and they are connected via Lagrangian particles. However, a notable limitation of PAC-NeRF is that its performance is sensitive to the learning of the geometry from the first frames owing to its two-step optimization. First, the grid representations are optimized with the first frames of video sequences, and then the physical properties are optimized through video sequences utilizing the fixed first-frame grid representations. This limitation can be critical when learning of the geometric structure is difficult, for example, in a few-shot (sparse view) setting. To overcome this limitation, we propose Lagrangian particle optimization (LPO), in which the positions and features of particles are optimized through video sequences in Lagrangian space. This method allows for the optimization of the geometric structure across the entire video sequence within the physical constraints imposed by the MPM. The experimental results demonstrate that the LPO is useful for geometric correction and physical identification in sparse-view settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04155v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuhiro Kaneko</dc:creator>
    </item>
    <item>
      <title>ReFiNe: Recursive Field Networks for Cross-modal Multi-scene Representation</title>
      <link>https://arxiv.org/abs/2406.04309</link>
      <description>arXiv:2406.04309v1 Announce Type: cross 
Abstract: The common trade-offs of state-of-the-art methods for multi-shape representation (a single model "packing" multiple objects) involve trading modeling accuracy against memory and storage. We show how to encode multiple shapes represented as continuous neural fields with a higher degree of precision than previously possible and with low memory usage. Key to our approach is a recursive hierarchical formulation that exploits object self-similarity, leading to a highly compressed and efficient shape latent space. Thanks to the recursive formulation, our method supports spatial and global-to-local latent feature fusion without needing to initialize and maintain auxiliary data structures, while still allowing for continuous field queries to enable applications such as raytracing. In experiments on a set of diverse datasets, we provide compelling qualitative results and demonstrate state-of-the-art multi-scene reconstruction and compression results with a single network per dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04309v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergey Zakharov, Katherine Liu, Adrien Gaidon, Rares Ambrus</dc:creator>
    </item>
    <item>
      <title>Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion</title>
      <link>https://arxiv.org/abs/2406.04338</link>
      <description>arXiv:2406.04338v1 Announce Type: cross 
Abstract: In recent years, there has been rapid development in 3D generation models, opening up new possibilities for applications such as simulating the dynamic movements of 3D objects and customizing their behaviors. However, current 3D generative models tend to focus only on surface features such as color and shape, neglecting the inherent physical properties that govern the behavior of objects in the real world. To accurately simulate physics-aligned dynamics, it is essential to predict the physical properties of materials and incorporate them into the behavior prediction process. Nonetheless, predicting the diverse materials of real-world objects is still challenging due to the complex nature of their physical attributes. In this paper, we propose \textbf{Physics3D}, a novel method for learning various physical properties of 3D objects through a video diffusion model. Our approach involves designing a highly generalizable physical simulation system based on a viscoelastic material model, which enables us to simulate a wide range of materials with high-fidelity capabilities. Moreover, we distill the physical priors from a video diffusion model that contains more understanding of realistic object materials. Extensive experiments demonstrate the effectiveness of our method with both elastic and plastic materials. Physics3D shows great potential for bridging the gap between the physical world and virtual neural space, providing a better integration and application of realistic physical principles in virtual environments. Project page: https://liuff19.github.io/Physics3D.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04338v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangfu Liu, Hanyang Wang, Shunyu Yao, Shengjun Zhang, Jie Zhou, Yueqi Duan</dc:creator>
    </item>
    <item>
      <title>MVTN: Learning Multi-View Transformations for 3D Understanding</title>
      <link>https://arxiv.org/abs/2212.13462</link>
      <description>arXiv:2212.13462v2 Announce Type: replace-cross 
Abstract: Multi-view projection techniques have shown themselves to be highly effective in achieving top-performing results in the recognition of 3D shapes. These methods involve learning how to combine information from multiple view-points. However, the camera view-points from which these views are obtained are often fixed for all shapes. To overcome the static nature of current multi-view techniques, we propose learning these view-points. Specifically, we introduce the Multi-View Transformation Network (MVTN), which uses differentiable rendering to determine optimal view-points for 3D shape recognition. As a result, MVTN can be trained end-to-end with any multi-view network for 3D shape classification. We integrate MVTN into a novel adaptive multi-view pipeline that is capable of rendering both 3D meshes and point clouds. Our approach demonstrates state-of-the-art performance in 3D classification and shape retrieval on several benchmarks (ModelNet40, ScanObjectNN, ShapeNet Core55). Further analysis indicates that our approach exhibits improved robustness to occlusion compared to other methods. We also investigate additional aspects of MVTN, such as 2D pretraining and its use for segmentation. To support further research in this area, we have released MVTorch, a PyTorch library for 3D understanding and generation using multi-view projections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.13462v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdullah Hamdi, Faisal AlZahrani, Silvio Giancola, Bernard Ghanem</dc:creator>
    </item>
    <item>
      <title>DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling</title>
      <link>https://arxiv.org/abs/2402.08876</link>
      <description>arXiv:2402.08876v2 Announce Type: replace-cross 
Abstract: In recent years, there has been a growing interest in training Neural Networks to approximate Unsigned Distance Fields (UDFs) for representing open surfaces in the context of 3D reconstruction. However, UDFs are non-differentiable at the zero level set which leads to significant errors in distances and gradients, generally resulting in fragmented and discontinuous surfaces. In this paper, we propose to learn a hyperbolic scaling of the unsigned distance field, which defines a new Eikonal problem with distinct boundary conditions. This allows our formulation to integrate seamlessly with state-of-the-art continuously differentiable implicit neural representation networks, largely applied in the literature to represent signed distance fields. Our approach not only addresses the challenge of open surface representation but also demonstrates significant improvement in reconstruction quality and training performance. Moreover, the unlocked field's differentiability allows the accurate computation of essential topological properties such as normal directions and curvatures, pervasive in downstream tasks such as rendering. Through extensive experiments, we validate our approach across various data sets and against competitive baselines. The results demonstrate enhanced accuracy and up to an order of magnitude increase in speed compared to previous methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08876v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miguel Fainstein, Viviana Siless, Emmanuel Iarussi</dc:creator>
    </item>
  </channel>
</rss>
