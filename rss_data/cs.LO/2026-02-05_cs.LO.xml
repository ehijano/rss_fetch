<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Feb 2026 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian Networks and Proof-Nets: the proof-theory of Bayesian Inference</title>
      <link>https://arxiv.org/abs/2602.04045</link>
      <description>arXiv:2602.04045v1 Announce Type: new 
Abstract: We study the correspondence between Bayesian Networks and graphical representation of proofs in linear logic. The goal of this paper is threefold: to develop a proof-theoretical account of Bayesian inference (in the spirit of the Curry-Howard correspondence between proofs and programs), to provide compositional graphical methods, and to take into account computational efficiency.
  We exploit the fact that the decomposition of a graph is more flexible than that of a proof-tree, or of a type-derivation, even if
  compositionality becomes more challenging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04045v1</guid>
      <category>cs.LO</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R\'emi Di Guardia, Thomas Ehrhard, J\'er\^ome Evrard, Claudia Faggian</dc:creator>
    </item>
    <item>
      <title>Intentic Semantics for Potentialist Truthmaking</title>
      <link>https://arxiv.org/abs/2602.04488</link>
      <description>arXiv:2602.04488v1 Announce Type: new 
Abstract: This draft introduces the technical machinery of a semantic framework for potentialist truthmaking based on our innovation of intentic states, which are structured partial models accounting for our distinction between non-hypothetical and hypothetical reasoning. The framework is developed for first-order logic in a purely relational language and is compatible with both classical and intuitionistic settings. Truthmaking is defined via a recursive construction over intentic states, yielding a semantic consequence relation that is shown to be sound and complete with respect to standard natural deduction. The resulting structure supports two natural extension relations, corresponding to truthmaking growth and hypothetical refinement, which are shown to satisfy the axioms governing Linnebo's bi-modal potentialist semantics.
  Moreover, we investigate the computational properties of the non-hypothetical fragment of natural deduction. Motivated by proof-theoretic and semantic considerations, we formulate a conjecture that non-hypothetical logic is decidable over Peano Arithmetic in a purely relational axiomatization, and more ambitiously over any fixed Peano Arithmetic theorem taken as an additional axiom. A schematic proof-search procedure is drafted to support this conjecture, identifying structural sources of finiteness. While preliminary, this analysis suggests a strong subformula discipline for normal non-hypothetical proofs and provides a proof-theoretic foundation for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04488v1</guid>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Gorbow</dc:creator>
    </item>
    <item>
      <title>Abstract Framework for All-Path Reachability Analysis</title>
      <link>https://arxiv.org/abs/2602.04641</link>
      <description>arXiv:2602.04641v1 Announce Type: new 
Abstract: An all-path reachability (APR, for short) predicate is a pair of a source set and a target set, which are subsets of an object set. APR predicates have been defined for abstract reduction systems (ARSs, for short) and then extended to logically constrained term rewrite systems (LCTRSs, for short) as pairs of constrained terms that represent sets of terms modeling configurations, states, etc. An APR predicate is said to be partially (or demonically) valid w.r.t. a rewrite system if every finite maximal reduction sequence of the system starting from any element in the source set includes an element in the target set. Partial validity of APR predicates w.r.t. ARSs is defined by means of two inference rules, which can be considered a proof system to construct (possibly infinite) derivation trees for partial validity. On the other hand, a proof system for LCTRSs consists of four inference rules, and thus there is a gap between the inference rules for partial validity w.r.t. ARSs and LCTRSs. In this paper, we revisit the framework for APR analysis and adapt it to verification of not only safety but also liveness properties. To this end, we first reformulate an abstract framework for partial validity w.r.t. ARSs so that there is a one-to-one correspondence between the inference rules for ARSs and LCTRSs. Secondly, we show how to apply APR analysis to safety verification. Thirdly, to apply APR analysis to liveness verification, we introduce a novel stronger validity of APR predicates, called total validity, which requires not only finite but also infinite execution paths to reach target sets. Finally, for a partially valid APR predicate with a cyclic-proof tree, we show a necessary and sufficient condition for the tree to ensure total validity, showing how to apply APR analysis to liveness verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04641v1</guid>
      <category>cs.LO</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Misaki Kojima, Naoki Nishida</dc:creator>
    </item>
    <item>
      <title>CSLib: The Lean Computer Science Library</title>
      <link>https://arxiv.org/abs/2602.04846</link>
      <description>arXiv:2602.04846v1 Announce Type: new 
Abstract: We introduce CSLib, an open-source framework for proving computer-science-related theorems and writing formally verified code in the Lean proof assistant. CSLib aims to be for computer science what Lean's Mathlib is for mathematics. Mathlib has been tremendously impactful: it is a key reason for Lean's popularity within the mathematics research community, and it has also played a critical role in the training of AI systems for mathematical reasoning. However, the base of computer science knowledge in Lean is currently quite limited. CSLib will vastly enhance this knowledge base and provide infrastructure for using this knowledge in real-world verification projects. By doing so, CSLib will (1) enable the broad use of Lean in computer science education and research, and (2) facilitate the manual and AI-aided engineering of large-scale formally verified systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04846v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clark Barrett, Swarat Chaudhuri, Fabrizio Montesi, Jim Grundy, Pushmeet Kohli, Leonardo de Moura, Alexandre Rademaker, Sorrachai Yingchareonthawornchai</dc:creator>
    </item>
    <item>
      <title>PICID: Proof-Driven Clause Learning in Neural Network Verification</title>
      <link>https://arxiv.org/abs/2503.12083</link>
      <description>arXiv:2503.12083v2 Announce Type: replace 
Abstract: Current Deep Neural Network (DNN) verifiers are typically designed to prioritize scalability over reliability. Reliability can be reinforced through the generation of proofs that are checkable by trusted, external proof checkers. To date, only a handful of verifiers support proof production; and these rely on verifier-specific formats, and balance between scalability, proof detail, and the trustworthiness of their proof checker. In this tool paper, we introduce PICID, a DNN verifier that produces proofs in the standard Alethe format for SMT solving, checkable by multiple existing checkers. PICID implements a parallel CDCL(T) architecture that integrates a state-of-the-art, proof-producing SAT solver with the Marabou DNN verifier. Furthermore, PICID leverages UNSAT proofs to derive conflict clauses. Our evaluation shows that PICID generates valid proofs in the vast majority of cases and significantly outperforms existing tools that produce comparable proofs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12083v2</guid>
      <category>cs.LO</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omri Isac, Idan Refaeli, Haoze Wu, Clark Barrett, Guy Katz</dc:creator>
    </item>
    <item>
      <title>Strong Normalization for the Safe Fragment of a Minimal Rewrite System: A Triple-Lexicographic Proof and a Conjecture on the Unprovability of Full Termination for Any Relational Operator-Only TRS</title>
      <link>https://arxiv.org/abs/2512.00081</link>
      <description>arXiv:2512.00081v3 Announce Type: replace 
Abstract: This paper presents KO7, a minimal operator-only term rewriting system with seven constructors and eight reduction rules, designed to investigate self-verifying computation. No external axioms, imported logic, or pre-existing arithmetic are used.
  Standard termination methods fail on this system. We document ten proof strategies that collapse at the same barrier: duplication of the step argument in the recursor. The rec-succ rule redistributes its argument into both the function application and the recursive call, defeating every additive measure. This failure is not a defect but a necessary feature of any system capable of sequential computation.
  To resolve this, we define SafeStep, a guarded fragment restricted by a delta-phase bit. For this fragment, a novel triple-lexicographic measure (combining phase bits, Dershowitz-Manna multiset ordering, and ordinal ranking) succeeds, supporting a mechanically verified normalizer with proven totality and soundness.
  We state a fundamental conjecture: No relational operator-only TRS can have its full-system termination proved by internally definable methods. A relational TRS is one capable of internalized sequential computation, a property shared by Turing machines, lambda calculus, and recursive functions. All such systems require a recursor that inherently creates the duplication barrier.
  This reveals Pre-Arithmetic Incompleteness. Unlike Goedel's theorems, which require arithmetic sufficient for encoding, this barrier appears at a more primitive layer: in operator systems possessing only order and recursion, before arithmetic emerges.
  The complete Lean 4 formalization (around 7,000 LOC) is available at https://github.com/MosesRahnama/OperatorKO7.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00081v3</guid>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Moses Rahnama</dc:creator>
    </item>
    <item>
      <title>Color Refinement for Relational Structures</title>
      <link>https://arxiv.org/abs/2407.16022</link>
      <description>arXiv:2407.16022v3 Announce Type: replace-cross 
Abstract: Color Refinement, also known as Naive Vertex Classification, is a classical method to distinguish graphs by iteratively computing a coloring of their vertices. While it is mainly used as an imperfect way to test for isomorphism, the algorithm permeated many other, seemingly unrelated, areas of computer science. The method is algorithmically simple, and it has a well-understood distinguishing power: It is logically characterized by Cai, F\"urer and Immerman (1992), who showed that it distinguishes precisely those graphs that can be distinguished by a sentence of first-order logic with counting quantifiers and only two variables. A combinatorial characterization is given by Dvo\v{r}\'ak (2010), who shows that it distinguishes precisely those graphs that can be distinguished by the number of homomorphisms from some tree.
  In this paper, we introduce Relational Color Refinement (RCR, for short), a generalization of the Color Refinement method from graphs to arbitrary relational structures, whose distinguishing power admits the equivalent combinatorial and logical characterizations as Color Refinement has on graphs: We show that RCR distinguishes precisely those structures that can be distinguished by the number of homomorphisms from an acyclic relational structure. Further, we show that RCR distinguishes precisely those structures that can be distinguished by a sentence of the guarded fragment of first-order logic with counting quantifiers.
  Additionally, we show that for every fixed finite relational signature, RCR can be implemented to run on structures of that signature in time $O(N\cdot \log N)$, where $N$ denotes the number of tuples present in the structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16022v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.LO</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Scheidt, Nicole Schweikardt</dc:creator>
    </item>
    <item>
      <title>Fine-tuned LLM-based Code Migration Framework</title>
      <link>https://arxiv.org/abs/2512.13515</link>
      <description>arXiv:2512.13515v2 Announce Type: replace-cross 
Abstract: The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13515v2</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <category>cs.LO</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oleg Grynets, Vasyl Lyashkevych, Dmytro Baran, Maksym Orliansky, Taras Zelenyy, Markiian Leshchyshyn</dc:creator>
    </item>
  </channel>
</rss>
