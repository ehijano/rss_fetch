<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Sep 2024 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Next-generation Probabilistic Computing Hardware with 3D MOSAICs, Illusion Scale-up, and Co-design</title>
      <link>https://arxiv.org/abs/2409.11422</link>
      <description>arXiv:2409.11422v1 Announce Type: new 
Abstract: The vast majority of 21st century AI workloads are based on gradient-based deterministic algorithms such as backpropagation. One of the key reasons for the dominance of deterministic ML algorithms is the emergence of powerful hardware accelerators (GPU and TPU) that have enabled the wide-scale adoption and implementation of these algorithms. Meanwhile, discrete and probabilistic Monte Carlo algorithms have long been recognized as one of the most successful algorithms in all of computing with a wide range of applications. Specifically, Markov Chain Monte Carlo (MCMC) algorithm families have emerged as the most widely used and effective method for discrete combinatorial optimization and probabilistic sampling problems. We adopt a hardware-centric perspective on probabilistic computing, outlining the challenges and potential future directions to advance this field. We identify two critical research areas: 3D integration using MOSAICs (Monolithic/Stacked/Assembled ICs) and the concept of Illusion, a hardware-agnostic distributed computing framework designed to scale probabilistic accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11422v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tathagata Srimani, Robert Radway, Masoud Mohseni, Kerem \c{C}amsar{\i}, Subhasish Mitra</dc:creator>
    </item>
    <item>
      <title>Massively parallel CMA-ES with increasing population</title>
      <link>https://arxiv.org/abs/2409.11765</link>
      <description>arXiv:2409.11765v1 Announce Type: new 
Abstract: The Increasing Population Covariance Matrix Adaptation Evolution Strategy (IPOP-CMA-ES) algorithm is a reference stochastic optimizer dedicated to blackbox optimization, where no prior knowledge about the underlying problem structure is available. This paper aims at accelerating IPOP-CMA-ES thanks to high performance computing and parallelism when solving large optimization problems. We first show how BLAS and LAPACK routines can be introduced in linear algebra operations, and we then propose two strategies for deploying IPOP-CMA-ES efficiently on large-scale parallel architectures with thousands of CPU cores. The first parallel strategy processes the multiple searches in the same ordering as the sequential IPOP-CMA-ES, while the second one processes concurrently these multiple searches. These strategies are implemented in MPI+OpenMP and compared on 6144 cores of the supercomputer Fugaku. We manage to obtain substantial speedups (up to several thousand) and even super-linear ones, and we provide an in-depth analysis of our results to understand precisely the superior performance of our second strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11765v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Redon (CRIStAL, BONUS), Pierre Fortin (CRIStAL, BONUS), Bilel Derbel (CRIStAL, BONUS), Miwako Tsuji (RIKEN CCS), Mitsuhisa Sato (RIKEN CCS)</dc:creator>
    </item>
    <item>
      <title>A Green Multi-Attribute Client Selection for Over-The-Air Federated Learning: A Grey-Wolf-Optimizer Approach</title>
      <link>https://arxiv.org/abs/2409.11442</link>
      <description>arXiv:2409.11442v1 Announce Type: cross 
Abstract: Federated Learning (FL) has gained attention across various industries for its capability to train machine learning models without centralizing sensitive data. While this approach offers significant benefits such as privacy preservation and decreased communication overhead, it presents several challenges, including deployment complexity and interoperability issues, particularly in heterogeneous scenarios or resource-constrained environments. Over-the-air (OTA) FL was introduced to tackle these challenges by disseminating model updates without necessitating direct device-to-device connections or centralized servers. However, OTA-FL brought forth limitations associated with heightened energy consumption and network latency. In this paper, we propose a multi-attribute client selection framework employing the grey wolf optimizer (GWO) to strategically control the number of participants in each round and optimize the OTA-FL process while considering accuracy, energy, delay, reliability, and fairness constraints of participating devices. We evaluate the performance of our multi-attribute client selection approach in terms of model loss minimization, convergence time reduction, and energy efficiency. In our experimental evaluation, we assessed and compared the performance of our approach against the existing state-of-the-art methods. Our results demonstrate that the proposed GWO-based client selection outperforms these baselines across various metrics. Specifically, our approach achieves a notable reduction in model loss, accelerates convergence time, and enhances energy efficiency while maintaining high fairness and reliability indicators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11442v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maryam Ben Driss, Essaid Sabir, Halima Elbiaze, Abdoulaye Banir\'e Diallo, Mohamed Sadik</dc:creator>
    </item>
    <item>
      <title>Advances in APPFL: A Comprehensive and Extensible Federated Learning Framework</title>
      <link>https://arxiv.org/abs/2409.11585</link>
      <description>arXiv:2409.11585v1 Announce Type: cross 
Abstract: Federated learning (FL) is a distributed machine learning paradigm enabling collaborative model training while preserving data privacy. In today's landscape, where most data is proprietary, confidential, and distributed, FL has become a promising approach to leverage such data effectively, particularly in sensitive domains such as medicine and the electric grid. Heterogeneity and security are the key challenges in FL, however; most existing FL frameworks either fail to address these challenges adequately or lack the flexibility to incorporate new solutions. To this end, we present the recent advances in developing APPFL, an extensible framework and benchmarking suite for federated learning, which offers comprehensive solutions for heterogeneity and security concerns, as well as user-friendly interfaces for integrating new algorithms or adapting to new applications. We demonstrate the capabilities of APPFL through extensive experiments evaluating various aspects of FL, including communication efficiency, privacy preservation, computational performance, and resource utilization. We further highlight the extensibility of APPFL through case studies in vertical, hierarchical, and decentralized FL. APPFL is open-sourced at https://github.com/APPFL/APPFL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11585v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zilinghan Li, Shilan He, Ze Yang, Minseok Ryu, Kibaek Kim, Ravi Madduri</dc:creator>
    </item>
    <item>
      <title>CountChain: A Decentralized Oracle Network for Counting Systems</title>
      <link>https://arxiv.org/abs/2409.11592</link>
      <description>arXiv:2409.11592v1 Announce Type: cross 
Abstract: Blockchain integration in industries like online advertising is hindered by its connectivity limitations to off-chain data. These industries heavily rely on precise counting systems for collecting and analyzing off-chain data. This requires mechanisms, often called oracles, to feed off-chain data into smart contracts. However, current oracle solutions are ill-suited for counting systems since the oracles do not know when to expect the data, posing a significant challenge.
  To address this, we present CountChain, a decentralized oracle network for counting systems. In CountChain, data is received by all oracle nodes, and any node can submit a proposition request. Each proposition contains enough data to evaluate the occurrence of an event. Only randomly selected nodes participate in a game to evaluate the truthfulness of each proposition by providing proof and some stake. Finally, the propositions with the outcome of True increment the counter in a smart contract. Thus, instead of a contract calling oracles for data, in CountChain, the oracles call a smart contract when the data is available. Furthermore, we present a formal analysis and experimental evaluation of the system's parameters on over half a million data points to obtain optimal system parameters. In such conditions, our game-theoretical analysis demonstrates that a Nash equilibrium exists wherein all rational parties participate with honesty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11592v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Behkish Nassirzadeh, Stefanos Leonardos, Albert Heinle, Anwar Hasan, Vijay Ganesh</dc:creator>
    </item>
    <item>
      <title>Blockchain-Enabled IoV: Secure Communication and Trustworthy Decision-Making</title>
      <link>https://arxiv.org/abs/2409.11621</link>
      <description>arXiv:2409.11621v1 Announce Type: cross 
Abstract: The Internet of Vehicles (IoV), which enables interactions between vehicles, infrastructure, and the environment, faces challenges in maintaining communication security and reliable automated decisions. This paper introduces a decentralized framework comprising a primary layer for managing inter-vehicle communication and a sub-layer for securing intra-vehicle interactions. By implementing blockchain-based protocols like Blockchain-integrated Secure Authentication (BiSA) and Decentralized Blockchain Name Resolution (DBNR), the framework ensures secure, decentralized identity management and reliable data exchanges, thereby supporting safe and efficient autonomous vehicle operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11621v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyi Sun, Qi Shi, Guodong Jin, Hao Xu, Erwu Liu</dc:creator>
    </item>
    <item>
      <title>Navigating High-Degree Heterogeneity: Federated Learning in Aerial and Space Networks</title>
      <link>https://arxiv.org/abs/2406.17951</link>
      <description>arXiv:2406.17951v2 Announce Type: replace-cross 
Abstract: Federated learning offers a compelling solution to the challenges of networking and data privacy within aerial and space networks by utilizing vast private edge data and computing capabilities accessible through drones, balloons, and satellites. While current research has focused on optimizing the learning process, computing efficiency, and minimizing communication overhead, the heterogeneity issue and class imbalance remain a significant barrier to rapid model convergence. In this paper, we explore the influence of heterogeneity on class imbalance, which diminishes performance in Aerial and Space Networks (ASNs)-based federated learning. We illustrate the correlation between heterogeneity and class imbalance within grouped data and show how constraints such as battery life exacerbate the class imbalance challenge. Our findings indicate that ASNs-based FL faces heightened class imbalance issues even with similar levels of heterogeneity compared to other scenarios. Finally, we analyze the impact of varying degrees of heterogeneity on FL training and evaluate the efficacy of current state-of-the-art algorithms under these conditions. Our results reveal that the heterogeneity challenge is more pronounced in ASNs-based federated learning and that prevailing algorithms often fail to effectively address high levels of heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17951v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fan Dong, Henry Leung, Steve Drew</dc:creator>
    </item>
    <item>
      <title>Mask-Encoded Sparsification: Mitigating Biased Gradients in Communication-Efficient Split Learning</title>
      <link>https://arxiv.org/abs/2408.13787</link>
      <description>arXiv:2408.13787v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel framework designed to achieve a high compression ratio in Split Learning (SL) scenarios where resource-constrained devices are involved in large-scale model training. Our investigations demonstrate that compressing feature maps within SL leads to biased gradients that can negatively impact the convergence rates and diminish the generalization capabilities of the resulting models. Our theoretical analysis provides insights into how compression errors critically hinder SL performance, which previous methodologies underestimate. To address these challenges, we employ a narrow bit-width encoded mask to compensate for the sparsification error without increasing the order of time complexity. Supported by rigorous theoretical analysis, our framework significantly reduces compression errors and accelerates the convergence. Extensive experiments also verify that our method outperforms existing solutions regarding training efficiency and communication complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13787v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 27th European Conference on Artificial Intelligence, 2024</arxiv:journal_reference>
      <dc:creator>Wenxuan Zhou, Zhihao Qu, Shen-Huan Lyu, Miao Cai, Baoliu Ye</dc:creator>
    </item>
  </channel>
</rss>
