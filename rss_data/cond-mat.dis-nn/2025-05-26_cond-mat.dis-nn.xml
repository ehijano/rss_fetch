<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 May 2025 03:13:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Relationship of structural disorder and stability of supercooled liquid state with glass-forming ability of metallic glasses</title>
      <link>https://arxiv.org/abs/2505.17874</link>
      <description>arXiv:2505.17874v1 Announce Type: new 
Abstract: We performed calorimetric studies of 26 metallic glasses and calculated the excess entropy and excess enthalpy with respect to their counterpart crystals. On this basis, we introduced a dimensionless entropy-based parameter {\sigma}scl, which characterizes structural disordering and stability of the supercooled liquid state upon heating. A very good correlation of {\sigma}scl with literature data on the critical cooling rate Rc and critical diameter Dmax of metallic glasses is shown. We also introduced another dimensionless parameter {\eta}scl based on the excess enthalpy of glass and showed that {\eta}scl provides equally good correlation with Rc and Dmax. Possible relationship of structural disordering and glass-forming ability in the supercooled liquid range with the defect structure of glass is discussed. The obtained results provide a new window for the understandingof the glass-forming ability of metallic glasses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17874v1</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. B. Cui, R. A. Konchakov, G. V. Afonin, A. S. Makarov, G. J. Lyu, J. C. Qiao, N. P. Kobelev, V. A. Khonik</dc:creator>
    </item>
    <item>
      <title>Non-isothermal stress relaxation in conventional and high-entropy metallic glasses and its relationship to themixing and excess entropy</title>
      <link>https://arxiv.org/abs/2505.17889</link>
      <description>arXiv:2505.17889v1 Announce Type: new 
Abstract: We performed calorimetric and torsion stress relaxation measurements upon linear heating of six conventional and high-entropy metallic glasses with the mixing entropy {\Delta}Smix ranging from 0.86R to 1.79R (R is the universal gas constant). It is shown that high-entropy metallic glasses ({\Delta}Smix &gt; 1.5 R) exhibit significantly greater resistance to stress relaxation. Based on calorimetric data, we calculated the excess entropy of glass relative to the counterpart crystalline state and introduced an entropy-based dimensionless parameter {\Delta}S, which characterizes the rise of the entropy and structural disordering of glass in the supercooled liquid region. It is shown that the depth of stress relaxation at a given temperature decreases with {\Delta}Smix but increases with {\Delta}S. Possible reasons for this relationship are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17889v1</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G. V. Afonin, S. L. Scherbakov, R. A. Konchakov, N. P. Kobelev, J. B. Cui, J. C. Qiao, V. A. Khonik</dc:creator>
    </item>
    <item>
      <title>Preferential attachment and power-law degree distributions in heterogeneous multilayer hypergraphs</title>
      <link>https://arxiv.org/abs/2505.18068</link>
      <description>arXiv:2505.18068v1 Announce Type: new 
Abstract: We include complex connectivity structures and heterogeneity in models of multilayer networks or multilayer hypergraphs growing by preferential attachment. We consider the most generic connectivity structure, where the probability of acquiring a new hyperlink depends linearly on the vector of hyperdegrees of the node across all layers, as well as on the layer of the new hyperlink and the features of both linked nodes. We derive the consistency conditions that imply a power-law hyperdegree distribution for each class of nodes within each layer and of any order. For generic connectivity structures, we predict that the exponent of the power-law distribution is universal for all layers and all orders of hyperlinks, and it depends exclusively on the type of node.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18068v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Francesco Di Lauro, Luca Ferretti</dc:creator>
    </item>
    <item>
      <title>Global Optimization Through Heterogeneous Oscillator Ising Machines</title>
      <link>https://arxiv.org/abs/2505.17027</link>
      <description>arXiv:2505.17027v1 Announce Type: cross 
Abstract: Oscillator Ising machines (OIMs) are networks of coupled oscillators that seek the minimum energy state of an Ising model. Since many NP-hard problems are equivalent to the minimization of an Ising Hamiltonian, OIMs have emerged as a promising computing paradigm for solving complex optimization problems that are intractable on existing digital computers. However, their performance is sensitive to the choice of tunable parameters, and convergence guarantees are mostly lacking. Here, we show that lower energy states are more likely to be stable, and that convergence to the global minimizer is often improved by introducing random heterogeneities in the regularization parameters. Our analysis relates the stability properties of Ising configurations to the spectral properties of a signed graph Laplacian. By examining the spectra of random ensembles of these graphs, we show that the probability of an equilibrium being asymptotically stable depends inversely on the value of the Ising Hamiltonian, biasing the system toward low-energy states. Our numerical results confirm our findings and demonstrate that heterogeneously designed OIMs efficiently converge to globally optimal solutions with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17027v1</guid>
      <category>math.OC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed Allibhoy, Arthur N. Montanari, Fabio Pasqualetti, Adilson E. Motter</dc:creator>
    </item>
    <item>
      <title>Role of Nonstabilizerness in Quantum Optimization</title>
      <link>https://arxiv.org/abs/2505.17185</link>
      <description>arXiv:2505.17185v1 Announce Type: cross 
Abstract: Quantum optimization has emerged as a promising approach for tackling complicated classical optimization problems using quantum devices. However, the extent to which such algorithms harness genuine quantum resources and the role of these resources in their success remain open questions. In this work, we investigate the resource requirements of the Quantum Approximate Optimization Algorithm (QAOA) through the lens of the resource theory of nonstabilizerness. We demonstrate that the nonstabilizerness in QAOA increases with circuit depth before it reaches a maximum, to fall again during the approach to the final solution state -- creating a barrier that limits the algorithm's capability for shallow circuits. We find curves corresponding to different depths to collapse under a simple rescaling, and we reveal a nontrivial relationship between the final nonstabilizerness and the success probability. Finally, we identify a similar nonstabilizerness barrier also in adiabatic quantum annealing. Our results provide deeper insights into how quantum resources influence quantum optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17185v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.other</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiara Capecci, Gopal Chandra Santra, Alberto Bottarelli, Emanuele Tirrito, Philipp Hauke</dc:creator>
    </item>
    <item>
      <title>Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training</title>
      <link>https://arxiv.org/abs/2505.17638</link>
      <description>arXiv:2505.17638v1 Announce Type: cross 
Abstract: Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time $\tau_\mathrm{gen}$ at which models begin to generate high-quality samples, and a later time $\tau_\mathrm{mem}$ beyond which memorization emerges. Crucially, we find that $\tau_\mathrm{mem}$ increases linearly with the training set size $n$, while $\tau_\mathrm{gen}$ remains constant. This creates a growing window of training times with $n$ where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when $n$ becomes larger than a model-dependent threshold that overfitting disappears at infinite training times. These findings reveal a form of implicit dynamical regularization in the training dynamics, which allow to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17638v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tony Bonnaire, Rapha\"el Urfin, Giulio Biroli, Marc M\'ezard</dc:creator>
    </item>
    <item>
      <title>Multiplexity amplifies geometry in networks</title>
      <link>https://arxiv.org/abs/2505.17688</link>
      <description>arXiv:2505.17688v1 Announce Type: cross 
Abstract: Many real-world network are multilayer, with nontrivial correlations across layers. Here we show that these correlations amplify geometry in networks. We focus on mutual clustering--a measure of the amount of triangles that are present in all layers among the same triplets of nodes--and find that this clustering is abnormally high in many real-world networks, even when clustering in each individual layer is weak. We explain this unexpected phenomenon using a simple multiplex network model with latent geometry: links that are most congruent with this geometry are the ones that persist across layers, amplifying the cross-layer triangle overlap. This result reveals a different dimension in which multilayer networks are radically distinct from their constituent layers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17688v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jasper van der Kolk, Dmitri Krioukov, Mari\'an Bogu\~n\'a, M. \'Angeles Serrano</dc:creator>
    </item>
    <item>
      <title>The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks</title>
      <link>https://arxiv.org/abs/2505.17958</link>
      <description>arXiv:2505.17958v1 Announce Type: cross 
Abstract: We study the high-dimensional asymptotics of empirical risk minimization (ERM) in over-parametrized two-layer neural networks with quadratic activations trained on synthetic data. We derive sharp asymptotics for both training and test errors by mapping the $\ell_2$-regularized learning problem to a convex matrix sensing task with nuclear norm penalization. This reveals that capacity control in such networks emerges from a low-rank structure in the learned feature maps. Our results characterize the global minima of the loss and yield precise generalization thresholds, showing how the width of the target function governs learnability. This analysis bridges and extends ideas from spin-glass methods, matrix factorization, and convex optimization and emphasizes the deep link between low-rank matrix sensing and learning in quadratic neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17958v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vittorio Erba, Emanuele Troiani, Lenka Zdeborov\'a, Florent Krzakala</dc:creator>
    </item>
    <item>
      <title>Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions</title>
      <link>https://arxiv.org/abs/2505.18046</link>
      <description>arXiv:2505.18046v1 Announce Type: cross 
Abstract: The Restricted Boltzmann Machine (RBM) is one of the simplest generative neural networks capable of learning input distributions. Despite its simplicity, the analysis of its performance in learning from the training data is only well understood in cases that essentially reduce to singular value decomposition of the data. Here, we consider the limit of a large dimension of the input space and a constant number of hidden units. In this limit, we simplify the standard RBM training objective into a form that is equivalent to the multi-index model with non-separable regularization. This opens a path to analyze training of the RBM using methods that are established for multi-index models, such as Approximate Message Passing (AMP) and its state evolution, and the analysis of Gradient Descent (GD) via the dynamical mean-field theory. We then give rigorous asymptotics of the training dynamics of RBM on data generated by the spiked covariance model as a prototype of a structure suitable for unsupervised learning. We show in particular that RBM reaches the optimal computational weak recovery threshold, aligning with the BBP transition, in the spiked covariance model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18046v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>stat.ML</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizhou Xu, Florent Krzakala, Lenka Zdeborov\'a</dc:creator>
    </item>
    <item>
      <title>Power-law banded random matrix ensemble as a model for quantum many-body Hamiltonians</title>
      <link>https://arxiv.org/abs/2503.08825</link>
      <description>arXiv:2503.08825v2 Announce Type: replace 
Abstract: We explore interpretations of the power-law banded random matrix (PLBRM) ensemble as Hamiltonians of one-dimensional quantum many-body systems. We introduce and compare a number of labeling schemes for assigning random matrix basis indices to many-body basis vectors. We compare the physical properties of the resulting Hamiltonians, focusing on the half-system eigenstate bipartite entanglement entropy. We show and quantify how the different PLBRM phases (ergodic, weakly ergodic, localized), known from the single-particle interpretation, can be interpreted as entanglement transitions in the quantum many-body interpretation. For the weakly ergodic phase, where spectral edge and bulk eigenstates show distinct behavior, we perform a detailed scaling analysis to provide a quantitative picture of the boundaries between different types of entanglement scaling behaviors. In particular, we identify and characterize an intermediate set of eigenstates whose entanglement entropy have volume law scaling but non-vanishing deviation from the Page value expected for maximally ergodic states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08825v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wouter Buijsman, Masudul Haque, Ivan M. Khaymovich</dc:creator>
    </item>
    <item>
      <title>Cat states carrying long-range correlations in the many-body localized phase</title>
      <link>https://arxiv.org/abs/2504.10566</link>
      <description>arXiv:2504.10566v2 Announce Type: replace 
Abstract: Despite considerable efforts over the last decade, the high-energy phase diagram of the random-field Heisenberg chain still eludes our understanding, in particular the nature of the non-ergodic many-body localized (MBL) regime expected at strong disorder. In this work, we revisit this paradigmatic model by studying the statistics of rare atypical events of strongly correlated spin pairs traversing the entire system. They occur for unexpectedly strong disorder, i.e., in a regime where standard estimates fail to detect any instability. We then identify these very peculiar high-energy eigenstates, which exhibit system-wide ${\cal{O}}(1)$ correlations, as nearly degenerate pairs of resonant cat states of the form $|{\Phi}_{\pm}\rangle\sim {|{\alpha_1}\rangle}\pm {|{\alpha_2}\rangle}$, where ${|{\alpha_1}\rangle}$ and ${|{\alpha_2}\rangle}$ are spin basis states. We propose a simple and generic analytical description of this new class of eigenstates that exhibit system-spanning entanglement. This analytical ansatz guides us in our search for rare hidden cat states in exponentially large many-body spectra. This also enables a systematic numerical inspection of the microscopic anatomy of these unconventional pairs, which appear in a wide range of disorder strengths. In the light of recent studies and ongoing debates on the MBL problem, our results offer new perspectives and stimulating challenges to this very active field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10566v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.str-el</category>
      <category>quant-ph</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Laflorencie, Jeanne Colbois, Fabien Alet</dc:creator>
    </item>
    <item>
      <title>Very persistent random walkers reveal transitions in landscape topology</title>
      <link>https://arxiv.org/abs/2505.16653</link>
      <description>arXiv:2505.16653v2 Announce Type: replace 
Abstract: We study the typical behavior of random walkers on the microcanonical configuration space of mean-field disordered systems. Passive walks have an ergodicity-breaking transition at precisely the energy density associated with the dynamical glass transition, but persistent walks remain ergodic at lower energies. In models where the energy landscape is thoroughly understood, we show that, in the limit of infinite persistence time, the ergodicity-breaking transition coincides with a transition in the topology of microcanonical configuration space. We conjecture that this correspondence generalizes to other models, and use it to determine the topological transition energy in situations where the landscape properties are ambiguous.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16653v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaron Kent-Dobias</dc:creator>
    </item>
    <item>
      <title>Parameter Symmetry Potentially Unifies Deep Learning Theory</title>
      <link>https://arxiv.org/abs/2502.05300</link>
      <description>arXiv:2502.05300v2 Announce Type: replace-cross 
Abstract: The dynamics of learning in modern large AI systems is hierarchical, often characterized by abrupt, qualitative shifts akin to phase transitions observed in physical systems. While these phenomena hold promise for uncovering the mechanisms behind neural networks and language models, existing theories remain fragmented, addressing specific cases. In this position paper, we advocate for the crucial role of the research direction of parameter symmetries in unifying these fragmented theories. This position is founded on a centralizing hypothesis for this direction: parameter symmetry breaking and restoration are the unifying mechanisms underlying the hierarchical learning behavior of AI models. We synthesize prior observations and theories to argue that this direction of research could lead to a unified understanding of three distinct hierarchies in neural networks: learning dynamics, model complexity, and representation formation. By connecting these hierarchies, our position paper elevates symmetry -- a cornerstone of theoretical physics -- to become a potential fundamental principle in modern AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05300v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liu Ziyin, Yizhou Xu, Tomaso Poggio, Isaac Chuang</dc:creator>
    </item>
    <item>
      <title>Where You Place the Norm Matters: From Prejudiced to Neutral Initializations</title>
      <link>https://arxiv.org/abs/2505.11312</link>
      <description>arXiv:2505.11312v2 Announce Type: replace-cross 
Abstract: Normalization layers, such as Batch Normalization and Layer Normalization, are central components in modern neural networks, widely adopted to improve training stability and generalization. While their practical effectiveness is well documented, a detailed theoretical understanding of how normalization affects model behavior, starting from initialization, remains an important open question. In this work, we investigate how both the presence and placement of normalization within hidden layers influence the statistical properties of network predictions before training begins. In particular, we study how these choices shape the distribution of class predictions at initialization, which can range from unbiased (Neutral) to highly concentrated (Prejudiced) toward a subset of classes. Our analysis shows that normalization placement induces systematic differences in the initial prediction behavior of neural networks, which in turn shape the dynamics of learning. By linking architectural choices to prediction statistics at initialization, our work provides a principled understanding of how normalization can influence early training behavior and offers guidance for more controlled and interpretable network design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11312v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Francazi, Francesco Pinto, Aurelien Lucchi, Marco Baity-Jesi</dc:creator>
    </item>
  </channel>
</rss>
