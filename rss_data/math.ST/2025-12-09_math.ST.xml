<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Dec 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>From Tail Universality to Bernstein-von Mises: A Unified Statistical Theory of Semi-Implicit Variational Inference</title>
      <link>https://arxiv.org/abs/2512.06107</link>
      <description>arXiv:2512.06107v1 Announce Type: new 
Abstract: Semi-implicit variational inference (SIVI) constructs approximate posteriors of the form $q(\theta) = \int k(\theta | z) r(dz)$, where the conditional kernel is parameterized and the mixing base is fixed and tractable. This paper develops a unified "approximation-optimization-statistics'' theory for such families.
  On the approximation side, we show that under compact L1-universality and a mild tail-dominance condition, semi-implicit families are dense in L1 and can achieve arbitrarily small forward Kullback-Leibler (KL) error. We also identify two sharp obstructions to global approximation: (i) an Orlicz tail-mismatch condition that induces a strictly positive forward-KL gap, and (ii) structural restrictions, such as non-autoregressive Gaussian kernels, that force "branch collapse'' in conditional distributions. For each obstruction we give a minimal structural modification that restores approximability.
  On the optimization side, we establish finite-sample oracle inequalities and prove that the empirical SIVI objectives L(K,n) $\Gamma$-converge to their population limit as n and K tend to infinity. These results give consistency of empirical maximizers, quantitative control of finite-K surrogate bias, and stability of the resulting variational posteriors.
  Combining the approximation and optimization analyses yields the first general end-to-end statistical theory for SIVI: we characterize precisely when SIVI can recover the target distribution, when it cannot, and how architectural and algorithmic choices govern the attainable asymptotic behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06107v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean Plummer</dc:creator>
    </item>
    <item>
      <title>Subsampling Confidence Bound for Persistent Diagram via Time-delay Embedding</title>
      <link>https://arxiv.org/abs/2512.06324</link>
      <description>arXiv:2512.06324v1 Announce Type: new 
Abstract: Time-delay embedding is a fundamental technique in Topological Data Analysis (TDA) for reconstructing the phase space dynamics of time-series data. While persistent homology effectively identifies topological features, such as cycles associated with periodicity, a rigorous statistical framework for quantifying the uncertainty of these features has been lacking in this context. In this paper, we propose a subsampling based method to construct confidence sets for persistence diagrams derived from time-delay embeddings. We establish finite sample guarantees for the validity of these confidence bounds under regularity conditions specifically for $C^{1,1}$ functions with positive reach and prove their asymptotic convergence as the embedding dimension tends to infinity. This framework provides a principled statistical test for periodicity, enabling the distinction between true periodic signals and non-periodic approximations. Simulation studies demonstrate that our method achieves detection performance comparable to the Generalized Lomb-Scargle periodogram on periodic data while exhibiting superior robustness in distinguishing non-periodic signals with time-varying frequencies, such as chirp signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06324v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Donghyun Park, Junhyun An, Taehyoung Kim, Jisu Kim</dc:creator>
    </item>
    <item>
      <title>Double Local-to-Unity: Estimation under Nearly Nonstationary Volatility</title>
      <link>https://arxiv.org/abs/2512.06823</link>
      <description>arXiv:2512.06823v1 Announce Type: new 
Abstract: This article develops a moderate-deviation limit theory for autoregressive models with jointly persistent mean and volatility dynamics. The autoregressive coefficient is allowed to drift toward unity slower than the classical 1/n rate, while the volatility persistence parameter also converges to one at an even slower, logarithmic order, so that the conditional variance process is itself nearly nonstationary and its unconditional moments may diverge. This double localization allows the variance process to be nearly nonstationary and to evolve slowly, as observed in financial data and during asset price bubble episodes. Under standard regularity conditions, we establish consistency and distributional limits for the OLS estimator of the autoregressive coefficient that remains valid in the presence of highly persistent stochastic volatility. We show that the effective normalization for least squares inference is governed by an average volatility scale, and we derive martingale limit theorems for the OLS estimator under joint drift and volatility dynamics. In a mildly stationary regime (where the autoregressive root approaches one from below), the OLS estimator is asymptotically normal. In a mildly explosive regime (where the root approaches one from above), an OLS based self normalized statistic converges to a Cauchy limit. Strikingly, in both regimes, the limiting laws of our statistics are invariant to the detailed specification of the volatility process, even though the conditional variance is itself nearly nonstationary. Overall, the results extend moderate-deviation asymptotics to settings with drifting volatility persistence, unify local to unity inference with nearly nonstationary stochastic volatility, and deliver practically usable volatility robust statistics for empirical work in settings approaching instability and exhibiting bubbles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06823v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abir Sarkar, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>On-line Pick-Freeze Mirror algorithm for Sensitity Analysis</title>
      <link>https://arxiv.org/abs/2512.06974</link>
      <description>arXiv:2512.06974v1 Announce Type: new 
Abstract: The main objective of this paper is to propose a new approach for estimating the entire collection of Sobol' indices simultaneously.
  Our approach exploits the fact that Sobol' indices can be rewritten as solutions to an optimization problem over the simplex of $\R^d$, to construct an online sequence of estimators using a stochastic mirror descent algorithm. We prove that our estimation procedure is consistent and provide a non-asymptotic upper bound for its rate of convergence. Furthermore, we demonstrate the numerical accuracy of our method and compare it with other classical estimation procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06974v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manon Costa, S\'ebastien Gadat, Xavier Gendre, Thierry Klein</dc:creator>
    </item>
    <item>
      <title>Estimation of the elasticity for CKLS model from high-frequency observations</title>
      <link>https://arxiv.org/abs/2512.07301</link>
      <description>arXiv:2512.07301v1 Announce Type: new 
Abstract: We investigate parametric estimation of the elasticity parameter in the CKLS diffusion based on high-frequency data. First, we transform the CKLS diffusion to a CIR-type one via a smooth state-space mapping and the general Girsanov change of measure. This transformation enables the applications of existing inference tools for CIR processes while ensuring possibilities of transferring the resulting limit theorems back to the original probability space. However, because Feller's condition fails, many existing high-frequency likelihood-based procedures cannot be applied directly, since their discretization schemes approximate likelihood terms involving the reciprocal of the process by Riemann sums that are no longer well-defined once the paths are allowed to hit zero. Instead, we estimate the drift coefficient of the transformed CIR-type model via a procedure based on its positive Harris recurrence, which is valid in the high-frequency regime. Exploiting the drift-elasticity relationship implied by the CKLS--CIR transformation, with the help of an initial estimation, we obtain an estimator of the CKLS elasticity from the CIR drift estimator in the transformed model. This yields a closed-form estimator of the elasticity parameter with an explicit asymptotic variance. We establish its $p$-consistency, stable convergence in law, and asymptotic normality. Finally, we show that stable convergence in law is invariant under equivalent changes of measure, thereby guaranteeing that the Gaussian limit remains invariant under the original measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07301v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boyuan Ning, Yasutaka Shimizu</dc:creator>
    </item>
    <item>
      <title>Nonparametric optimal density estimation for censored circular data</title>
      <link>https://arxiv.org/abs/2512.07380</link>
      <description>arXiv:2512.07380v1 Announce Type: new 
Abstract: We consider the problem of estimating the probability density function of a circular random variable observed under censoring. To this end, we introduce a projection estimator constructed via a regression approach on linear sieves. We first establish a lower bound for the mean integrated squared error in the case of Sobolev densities, thereby identifying the minimax rate of convergence for this estimation problem. We then derive a matching upper bound for the same risk, showing that the proposed estimator attains the minimax rate when the underlying density belongs to a Sobolev class. Finally, we develop a data-driven version of the procedure that preserves this optimal rate, thus yielding an adaptive estimator. The practical performance of the method is demonstrated through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07380v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Conanec (LAGA), Claire Lacour (LAMA), Thanh Mai Pham Ngoc (LAGA)</dc:creator>
    </item>
    <item>
      <title>A model-free Screening procedure</title>
      <link>https://arxiv.org/abs/2512.07423</link>
      <description>arXiv:2512.07423v1 Announce Type: new 
Abstract: In this article, we propose a generic screening method for selecting explanatory variables correlated with the response variable Y . We make no assumptions about the existence of a model that could link Y with a subset of explanatory variables, nor about the distribution of the variables. Our procedure can therefore be described as ''model-free'' and can be applied in a wide range of situations. In order to obtain precise theoretical guarantees (Sure Screening Property and control of the False Positive Rate), we establish a Berry-Esseen type inequality for the studentized statistic of the slope estimator. We illustrate our selection procedure using two simulated examples and a real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07423v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J Dedecker (MAP5 - UMR 8145), M L Taupin (LaMME), A S Tocquet (LaMME)</dc:creator>
    </item>
    <item>
      <title>A multivariate extension of Azadkia-Chatterjee's rank coefficient</title>
      <link>https://arxiv.org/abs/2512.07443</link>
      <description>arXiv:2512.07443v1 Announce Type: new 
Abstract: The Azadkia-Chatterjee coefficient is a rank-based measure of dependence between a random variable $Y \in \mathbb{R}$ and a random vector ${\boldsymbol Z} \in \mathbb{R}^{d_Z}$. This paper proposes a multivariate extension that measures dependence between random vectors ${\boldsymbol Y} \in \mathbb{R}^{d_Y}$ and ${\boldsymbol Z} \in \mathbb{R}^{d_Z}$, based on $n$ i.i.d. samples. The proposed coefficient converges almost surely to a limit with the following properties: i) it lies in $[0, 1]$; ii) it equals zero if and only if ${\boldsymbol Y}$ and ${\boldsymbol Z}$ are independent; and iii) it equals one if and only if ${\boldsymbol Y}$ is almost surely a function of ${\boldsymbol Z}$. Remarkably, the only assumption required by this convergence is that ${\boldsymbol Y}$ is not almost surely a constant. We further prove that under the same mild condition, the coefficient is asymptotically normal when ${\boldsymbol Y}$ and ${\boldsymbol Z}$ are independent and propose a merge sort based algorithm to calculate this coefficient in time complexity $O(n (\log n)^{d_Y})$. Finally, we show that it can be used to measure conditional dependence between ${\boldsymbol Y}$ and ${\boldsymbol Z}$ conditional on a third random vector ${\boldsymbol X}$, and prove that the measure is monotonic with respect to the deviation from an independence distribution under certain model restrictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07443v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenjie Huang, Zonghan Li, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>Location and scatter halfspace median under {\alpha}-symmetric distributions</title>
      <link>https://arxiv.org/abs/2512.07634</link>
      <description>arXiv:2512.07634v1 Announce Type: new 
Abstract: In a landmark result, Chen et al. (2018) showed that multivariate medians induced by halfspace depth attain the minimax optimal convergence rate under Huber contamination and elliptical symmetry, for both location and scatter estimation. We extend some of these findings to the broader family of {\alpha}-symmetric distributions, which includes both elliptically symmetric and multivariate heavy-tailed distributions. For location estimation, we establish an upper bound on the estimation error of the location halfspace median under the Huber contamination model. An analogous result for the standard scatter halfspace median matrix is feasible only under the assumption of elliptical symmetry, as ellipticity is deeply embedded in the definition of scatter halfspace depth. To address this limitation, we propose a modified scatter halfspace depth that better accommodates {\alpha}-symmetric distributions, and derive an upper bound for the corresponding {\alpha}-scatter median matrix. Additionally, we identify several key properties of scatter halfspace depth for {\alpha}-symmetric distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07634v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10485252.2025.2600417</arxiv:DOI>
      <dc:creator>Filip Bo\v{c}inec, Stanislav Nagy</dc:creator>
    </item>
    <item>
      <title>Non-Asymptotic Error Bounds for Causally Conditioned Directed Information Rates of Gaussian Sequences</title>
      <link>https://arxiv.org/abs/2512.06238</link>
      <description>arXiv:2512.06238v1 Announce Type: cross 
Abstract: Directed information and its causally conditioned variations are often used to measure causal influences between random processes. In practice, these quantities must be measured from data. Non-asymptotic error bounds for these estimates are known for sequences over finite alphabets, but less is known for real-valued data. This paper examines the case in which the data are sequences of Gaussian vectors. We provide an explicit formula for causally conditioned directed information rate based on optimal prediction and define an estimator based on this formula. We show that our estimator gives an error of order $O\left(N^{-1/2}\log(N)\right)$ with high probability, where $N$ is the total sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06238v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuping Zheng, Andrew Lamperski</dc:creator>
    </item>
    <item>
      <title>Theoretical Compression Bounds for Wide Multilayer Perceptrons</title>
      <link>https://arxiv.org/abs/2512.06288</link>
      <description>arXiv:2512.06288v1 Announce Type: cross 
Abstract: Pruning and quantization techniques have been broadly successful in reducing the number of parameters needed for large neural networks, yet theoretical justification for their empirical success falls short. We consider a randomized greedy compression algorithm for pruning and quantization post-training and use it to rigorously show the existence of pruned/quantized subnetworks of multilayer perceptrons (MLPs) with competitive performance. We further extend our results to structured pruning of MLPs and convolutional neural networks (CNNs), thus providing a unified analysis of pruning in wide networks. Our results are free of data assumptions, and showcase a tradeoff between compressibility and network width. The algorithm we consider bears some similarities with Optimal Brain Damage (OBD) and can be viewed as a post-training randomized version of it. The theoretical results we derive bridge the gap between theory and application for pruning/quantization, and provide a justification for the empirical success of compression in wide multilayer perceptrons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06288v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Houssam El Cheairi, David Gamarnik, Rahul Mazumder</dc:creator>
    </item>
    <item>
      <title>Hierarchical Clustering With Confidence</title>
      <link>https://arxiv.org/abs/2512.06522</link>
      <description>arXiv:2512.06522v1 Announce Type: cross 
Abstract: Agglomerative hierarchical clustering is one of the most widely used approaches for exploring how observations in a dataset relate to each other. However, its greedy nature makes it highly sensitive to small perturbations in the data, often producing different clustering results and making it difficult to separate genuine structure from spurious patterns. In this paper, we show how randomizing hierarchical clustering can be useful not just for measuring stability but also for designing valid hypothesis testing procedures based on the clustering results.
  We propose a simple randomization scheme together with a method for constructing a valid p-value at each node of the hierarchical clustering dendrogram that quantifies evidence against performing the greedy merge. Our test controls the Type I error rate, works with any hierarchical linkage without case-specific derivations, and simulations show it is substantially more powerful than existing selective inference approaches. To demonstrate the practical utility of our p-values, we develop an adaptive $\alpha$-spending procedure that estimates the number of clusters, with a probabilistic guarantee on overestimation. Experiments on simulated and real data show that this estimate yields powerful clustering and can be used, for example, to assess clustering stability across multiple runs of the randomized algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06522v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Wu, Jacob Bien, Snigdha Panigrahi</dc:creator>
    </item>
    <item>
      <title>Geometrical representation and dependence structure of three-dimensional Bernoulli distributions</title>
      <link>https://arxiv.org/abs/2512.06786</link>
      <description>arXiv:2512.06786v1 Announce Type: cross 
Abstract: This paper fully characterizes the geometrical structure of the class of distributions of three-dimensional Bernoulli random variables with equal means, $p$. We find all the geometrical generators in closed form as functions of $p$. This result stems from an algebraic representation of the class that encodes the statistical properties of Bernoulli distributions. We study extremal negative dependence within the class and provide an application example by finding the impact of negative dependence to minimal aggregate risk. The application relies on a game theory approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06786v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Fontana, Patrizia Semeraro</dc:creator>
    </item>
    <item>
      <title>Statistical analysis of Inverse Entropy-regularized Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2512.06956</link>
      <description>arXiv:2512.06956v1 Announce Type: cross 
Abstract: Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $\pi^\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06956v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denis Belomestny, Alexey Naumov, Sergey Samsonov</dc:creator>
    </item>
    <item>
      <title>Learning Paths to Multi-Sector Equilibrium: Belief Dynamics Under Uncertain Returns to Scale</title>
      <link>https://arxiv.org/abs/2512.07013</link>
      <description>arXiv:2512.07013v1 Announce Type: cross 
Abstract: This paper explores the dynamics of learning in a multi-sector general equilibrium model where firms operate under incomplete information about their production returns to scale. Firms iteratively update their beliefs using maximum a-posteriori estimation, derived from observed production outcomes, to refine their knowledge of their returns to scale. The implications of these learning dynamics for market equilibrium and the conditions under which firms can effectively learn their true returns to scale are the key objects of this study. Our results shed light on how idiosyncratic shocks influence the learning process and demonstrate that input decisions encode all pertinent information for belief updates. Additionally, we show that a long-memory (path-dependent) learning which keeps track of all past estimations ends up having a worse performance than a short-memory (path-independent) approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07013v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Nasini, Rabia Nessah, Bertrand Wigniolle</dc:creator>
    </item>
    <item>
      <title>Generalized Fr\'{e}chet means with random minimizing domains and its strong consistency</title>
      <link>https://arxiv.org/abs/2311.10958</link>
      <description>arXiv:2311.10958v2 Announce Type: replace 
Abstract: This paper introduces a novel extension of Fr\'{e}chet means, called \textit{generalized Fr\'{e}chet means} as a comprehensive framework for characterizing features in probability distributions in general topological spaces. The generalized Fr\'{e}chet means are defined as minimizers of a suitably defined cost function. The framework encompasses various extensions of Fr\'{e}chet means in the literature. The most distinctive difference of the new framework from the previous works is that we allow the domain of minimization of the empirical means be random and different from that of the population means. This expands the applicability of the Fr\'{e}chet mean framework to diverse statistical scenarios, including dimension reduction for manifold-valued data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10958v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaesung Park, Sungkyu Jung</dc:creator>
    </item>
    <item>
      <title>Theoretical Guarantees for the Subspace-Constrained Tyler's Estimator</title>
      <link>https://arxiv.org/abs/2403.18658</link>
      <description>arXiv:2403.18658v3 Announce Type: replace 
Abstract: This work analyzes the subspace-constrained Tyler's estimator (STE), a method designed to recover a low-dimensional subspace from a dataset that may be heavily corrupted by outliers. The STE has previously been shown to be competitive for fundamental computer vision problems. We assume a weak inlier-outlier model and allow the inlier fraction to fall below the threshold at which robust subspace recovery becomes computationally hard. We show that, in this setting, if the initialization of STE satisfies a certain condition, then STE-which is computationally efficient-can effectively recover the underlying subspace. Furthermore, we establish approximate recovery guarantees for STE in the presence of noisy inliers. Finally, under the asymptotic generalized haystack model, we demonstrate that STE initialized with Tyler's M-estimator (TME) recovers the subspace even when the inlier fraction is too small for TME to succeed on its own.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18658v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gilad Lerman, Teng Zhang</dc:creator>
    </item>
    <item>
      <title>A Particle Algorithm for Mean-Field Variational Inference</title>
      <link>https://arxiv.org/abs/2412.20385</link>
      <description>arXiv:2412.20385v3 Announce Type: replace 
Abstract: Variational inference is a fast and scalable alternative to Markov chain Monte Carlo and has been widely applied to posterior inference tasks in statistics and machine learning. A traditional approach for implementing mean-field variational inference (MFVI) is coordinate ascent variational inference (CAVI), which relies crucially on parametric assumptions on complete conditionals. We introduce a novel particle-based algorithm for MFVI, named PArticle VI (PAVI), for nonparametric mean-field approximation. We obtain non-asymptotic error bounds for our algorithm. To our knowledge, this is the first end-to-end guarantee for particle-based MFVI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20385v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiang Du, Kaizheng Wang, Edith Zhang, Chenyang Zhong</dc:creator>
    </item>
    <item>
      <title>Detecting non-uniform patterns on high-dimensional hyperspheres</title>
      <link>https://arxiv.org/abs/2506.00444</link>
      <description>arXiv:2506.00444v2 Announce Type: replace 
Abstract: We propose a new probabilistic characterization of the uniform distribution on the hypersphere in terms of the distribution of inner products, extending the ideas of \citep{cuesta2009projection,cuesta2007sharp} in a data-driven manner. Using this characterization, we define a new distance that quantifies the deviation of an arbitrary distribution from uniformity.
  As an application, we construct a novel nonparametric test for the problem of testing uniformity, namely the task of determining whether a set of \(n\) i.i.d.\ random points on the \(p\)-dimensional hypersphere is approximately uniformly distributed. The proposed test is asymptotically a Brownian bridge and it can detect any alternative lying outside a ball of radius \(1/n\) with respect to the proposed distance, in both high and low-dimensional settings.
  We then prove a matching lower bound with respect to this distance and study its behavior when restricted to parametric models. In particular, we show that the minimax detection thresholds with respect to this distance coincide with the usual minimax thresholds in two important families: (i) the class of Fisher--von Mises--Langevin (FvML) alternatives, and (ii) a class of low-rank uniform distributions. Thus, the proposed test is optimal in these models. We also derive the limiting distributions of the test under the corresponding local alternatives.
  As a byproduct of our analysis, we determine the detection threshold in the high-dimensional regime for testing the intrinsic dimension of the uniform distribution on $\mathbb{S}^{p-1}$; that is, for testing whether the distribution is uniformly supported on $\mathbb{S}^{p-1}$ against the alternative that it is uniformly distributed on \[ \mathbb{S}^{p-1} \cap H, \] for some $k$-dimensional linear subspace $H \subset \mathbb{R}^p$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00444v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tiefeng Jiang, Tuan Pham</dc:creator>
    </item>
    <item>
      <title>AR-sieve Bootstrap for High-dimensional Time Series</title>
      <link>https://arxiv.org/abs/2112.00414</link>
      <description>arXiv:2112.00414v2 Announce Type: replace-cross 
Abstract: This paper proposes a new AR-sieve bootstrap approach to high-dimensional time series. The major challenge of classical bootstrap methods on high-dimensional time series is two-fold: curse of dimensionality and temporal dependence. To address such a difficulty, we utilize factor modeling to reduce dimension and capture temporal dependence simultaneously. A factor-based bootstrap procedure is constructed, which performs an AR-sieve bootstrap on the extracted low-dimensional common factor time series and then recovers the bootstrap samples for the original data from the factor model. Asymptotic properties for bootstrap mean statistics and extreme eigenvalues are established. Various simulation studies further demonstrate the advantages of the new AR-sieve bootstrap in high-dimensional scenarios. An empirical application on particulate matter (PM) concentration data is studied, where bootstrap confidence intervals for mean vectors and autocovariance matrices are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.00414v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daning Bi, Han Lin Shang, Yanrong Yang, Huanjun Zhu</dc:creator>
    </item>
    <item>
      <title>Static and Dynamic BART for Rank-Order Data</title>
      <link>https://arxiv.org/abs/2308.10231</link>
      <description>arXiv:2308.10231v5 Announce Type: replace-cross 
Abstract: Ranking lists are often provided at regular time intervals in a range of applications, including economics, sports, marketing, and politics. Most popular methods for rank-order data postulate a linear specification for the latent scores, which determine the observed ranks, and ignore the temporal dependence of the ranking lists. To address these issues, novel nonparametric static (ROBART) and autoregressive (ARROBART) models are developed, with latent scores defined as nonlinear Bayesian additive regression tree functions of covariates. To make inferences in the dynamic ARROBART model, closed-form filtering, predictive, and smoothing distributions for the latent time-varying scores are derived. These results are applied in a Gibbs sampler with data augmentation for posterior inference. The proposed methods are shown to outperform existing competitors in simulation studies, static data applications to electoral data, stated preferences for sushi and movies, and dynamic data applications to economic complexity rankings of countries and weekly pollster rankings of NCAA football teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10231v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Iacopini, Eoghan O'Neill, Luca Rossini</dc:creator>
    </item>
    <item>
      <title>Closed-form $\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\ell_p$ bias</title>
      <link>https://arxiv.org/abs/2509.21181</link>
      <description>arXiv:2509.21181v3 Announce Type: replace-cross 
Abstract: For overparameterized linear regression with isotropic Gaussian design and minimum-$\ell_p$ interpolator $p\in(1,2]$, we give a unified, high-probability characterization for the scaling of the family of parameter norms $ \\{ \lVert \widehat{w_p} \rVert_r \\}_{r \in [1,p]} $ with sample size.
  We solve this basic, but unresolved question through a simple dual-ray analysis, which reveals a competition between a signal *spike* and a *bulk* of null coordinates in $X^\top Y$, yielding closed-form predictions for (i) a data-dependent transition $n_\star$ (the "elbow"), and (ii) a universal threshold $r_\star=2(p-1)$ that separates $\lVert \widehat{w_p} \rVert_r$'s which plateau from those that continue to grow with an explicit exponent.
  This unified solution resolves the scaling of *all* $\ell_r$ norms within the family $r\in [1,p]$ under $\ell_p$-biased interpolation, and explains in one picture which norms saturate and which increase as $n$ grows.
  We then study diagonal linear networks (DLNs) trained by gradient descent. By calibrating the initialization scale $\alpha$ to an effective $p_{\mathrm{eff}}(\alpha)$ via the DLN separable potential, we show empirically that DLNs inherit the same elbow/threshold laws, providing a predictive bridge between explicit and implicit bias.
  Given that many generalization proxies depend on $\lVert \widehat {w_p} \rVert_r$, our results suggest that their predictive power will depend sensitively on which $l_r$ norm is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21181v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuofeng Zhang, Ard Louis</dc:creator>
    </item>
    <item>
      <title>Model Monitoring: A General Framework with an Application to Non-life Insurance Pricing</title>
      <link>https://arxiv.org/abs/2510.04556</link>
      <description>arXiv:2510.04556v2 Announce Type: replace-cross 
Abstract: Maintaining the predictive performance of pricing models is challenging when insurance portfolios and data-generating mechanisms evolve over time. Focusing on non-life insurance, we adopt the concept-drift terminology from machine learning and distinguish virtual drift from real concept drift in an actuarial setting. Methodologically, we (i) formalize deviance loss and Murphy's score decomposition to assess global and local auto-calibration; (ii) study the Gini score as a rank-based performance measure, derive its asymptotic distribution, and develop a consistent bootstrap estimator of its asymptotic variance; and (iii) combine these results into a statistically grounded, model-agnostic monitoring framework that integrates a Gini-based ranking drift test with global and local auto-calibration tests. An application to a modified motor insurance portfolio with controlled concept-drift scenarios illustrates how the framework guides decisions on refitting or recalibrating pricing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04556v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexej Brauer, Paul Menzel, Mario V. W\"uthrich</dc:creator>
    </item>
  </channel>
</rss>
