<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Apr 2024 04:00:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bayesian Networks for Variational System Identification</title>
      <link>https://arxiv.org/abs/2404.10137</link>
      <description>arXiv:2404.10137v1 Announce Type: new 
Abstract: This paper details how the Bayesian-network structure of the posterior distribution of state-space models can be exploited to build improved parameterizations for system identification using variational inference. Three different parameterizations of the assumed state-path posterior distribution are proposed based on this representation: time-varying, steady-state, and convolution-smoother; each resulting in a different parameter estimation method. In contrast to existing methods for variational system identification, the proposed estimators can be implemented with unconstrained optimization methods. Furthermore, when applied to mini-batches in conjunction with stochastic optimization methods, the convolution-smoother formulation enables identification of large linear and nonlinear state-space systems from very large datasets. For linear systems, the method achieves the same performance as the inherently sequential prediction-error methods using and embarrassingly parallel algorithm that benefits from large speedups when computed in modern graphical processing units (GPUs). The ability of the proposed estimators to identify large models, work with large datasets split into mini-batches, and be work in parallel on GPUs make them well-suited for identifying deep models for applications in systems and control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10137v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Dimas Abreu Archanjo Dutra</dc:creator>
    </item>
    <item>
      <title>Using Multi-Source Data to Identify High-Emitting Heavy-Duty Diesel Vehicles</title>
      <link>https://arxiv.org/abs/2404.10243</link>
      <description>arXiv:2404.10243v1 Announce Type: new 
Abstract: Identifying and managing high-emitters among heavy-duty diesel vehicles is a key to mitigating urban air pollution, as a small number of such vehicles could contribute a significant amount of total transport emissions. On-board monitoring (OBM) systems can directly monitor the real-time emission performance of heavy-duty vehicles on road and have become part of the future emissions compliance framework. The challenge, however, lies in the frequent unavailability of OBM data, affecting the effective screening of high-emitting vehicles. This work proposes to bridge the gap by integrating OBM data with remote sensing data to create a comprehensive monitoring system. OBM data is used to characterize the detailed real-world NOx emission performance of both normally-behaving vehicles and high-emitters at various vehicle operating conditions. Remote sensing data is employed to screen out candidate high-emitting vehicles based on thresholds determined by OBM data. Finally, the dynamic NOx emission reduction potential across all roads is mapped by combining the trajectory data for each vehicle with the emission data. A case study in Chengdu, China, utilizing emission and traffic data from heavy-duty vehicles for transporting construction waste (a.k.a. slag trucks), reveals the national threshold for identifying high-emitters via remote sensing might be too lenient, particularly in the medium speed range. An emission reduction of 18.8% in the China V slag truck fleet could be achieved by implementing this novel method in practice in Chengdu. This approach establishes a reliable and ongoing scheme for pinpointing high-emitters through multi-source data, which allows local authorities to develop more robust and targeted strategies to mitigate urban air pollution from heavy-duty diesel vehicles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10243v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoqian Yang, Ke Han, Linwei Liao, Jiaxin Wu</dc:creator>
    </item>
    <item>
      <title>JCGM 101-compliant uncertainty evaluation using virtual experiments</title>
      <link>https://arxiv.org/abs/2404.10530</link>
      <description>arXiv:2404.10530v1 Announce Type: new 
Abstract: Virtual experiments (VEs), a modern tool in metrology, can be used to help perform an uncertainty evaluation for the measurand. Current guidelines in metrology do not cover the many possibilities to incorporate VEs into an uncertainty evaluation, and it is often difficult to assess if the intended use of a VE complies with said guidelines. In recent work, it was shown that a VE can be used in conjunction with real measurement data and a Monte Carlo procedure to produce equal results to a supplement of the Guide to the Expression of Uncertainty in Measurement. However, this was shown only for linear measurement models. In this work, we extend this Monte Carlo approach to a common class of non-linear measurement models and more complex VEs, providing a reference approach for suitable uncertainty evaluations involving VEs. Numerical examples are given to show that the theoretical derivations hold in a practical scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10530v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Finn Hughes, Manuel Marschall, Gerd W\"ubbeler, Gertjan Kok, Marcel van Dijk, Clemens Elster</dc:creator>
    </item>
    <item>
      <title>Data-driven subgrouping of patient trajectories with chronic diseases: Evidence from low back pain</title>
      <link>https://arxiv.org/abs/2404.10580</link>
      <description>arXiv:2404.10580v1 Announce Type: new 
Abstract: Clinical data informs the personalization of health care with a potential for more effective disease management. In practice, this is achieved by subgrouping, whereby clusters with similar patient characteristics are identified and then receive customized treatment plans with the goal of targeting subgroup-specific disease dynamics. In this paper, we propose a novel mixture hidden Markov model for subgrouping patient trajectories from chronic diseases. Our model is probabilistic and carefully designed to capture different trajectory phases of chronic diseases (i.e., "severe", "moderate", and "mild") through tailored latent states. We demonstrate our subgrouping framework based on a longitudinal study across 847 patients with non-specific low back pain. Here, our subgrouping framework identifies 8 subgroups. Further, we show that our subgrouping framework outperforms common baselines in terms of cluster validity indices. Finally, we discuss the applicability of the model to other chronic and long-lasting diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10580v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christof Naumzik, Alice Kongsted, Werner Vach, Stefan Feuerriegel</dc:creator>
    </item>
    <item>
      <title>A Strategy Transfer and Decision Support Approach for Epidemic Control in Experience Shortage Scenarios</title>
      <link>https://arxiv.org/abs/2404.10004</link>
      <description>arXiv:2404.10004v1 Announce Type: cross 
Abstract: Epidemic outbreaks can cause critical health concerns and severe global economic crises. For countries or regions with new infectious disease outbreaks, it is essential to generate preventive strategies by learning lessons from others with similar risk profiles. A Strategy Transfer and Decision Support Approach (STDSA) is proposed based on the profile similarity evaluation. There are four steps in this method: (1) The similarity evaluation indicators are determined from three dimensions, i.e., the Basis of National Epidemic Prevention &amp; Control, Social Resilience, and Infection Situation. (2) The data related to the indicators are collected and preprocessed. (3) The first round of screening on the preprocessed dataset is conducted through an improved collaborative filtering algorithm to calculate the preliminary similarity result from the perspective of the infection situation. (4) Finally, the K-Means model is used for the second round of screening to obtain the final similarity values. The approach will be applied to decision-making support in the context of COVID-19. Our results demonstrate that the recommendations generated by the STDSA model are more accurate and aligned better with the actual situation than those produced by pure K-means models. This study will provide new insights into preventing and controlling epidemics in regions that lack experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10004v1</guid>
      <category>cs.LG</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>X. Xiao, P. Chen, X. Cao, K. Liu, L. Deng, D. Zhao, Z. Chen, Q. Deng, F. Yu, H. Zhang</dc:creator>
    </item>
    <item>
      <title>OptiGrad: A Fair and more Efficient Price Elasticity Optimization via a Gradient Based Learning</title>
      <link>https://arxiv.org/abs/2404.10275</link>
      <description>arXiv:2404.10275v1 Announce Type: cross 
Abstract: This paper presents a novel approach to optimizing profit margins in non-life insurance markets through a gradient descent-based method, targeting three key objectives: 1) maximizing profit margins, 2) ensuring conversion rates, and 3) enforcing fairness criteria such as demographic parity (DP). Traditional pricing optimization, which heavily lean on linear and semi definite programming, encounter challenges in balancing profitability and fairness. These challenges become especially pronounced in situations that necessitate continuous rate adjustments and the incorporation of fairness criteria. Specifically, indirect Ratebook optimization, a widely-used method for new business price setting, relies on predictor models such as XGBoost or GLMs/GAMs to estimate on downstream individually optimized prices. However, this strategy is prone to sequential errors and struggles to effectively manage optimizations for continuous rate scenarios. In practice, to save time actuaries frequently opt for optimization within discrete intervals (e.g., range of [-20\%, +20\%] with fix increments) leading to approximate estimations. Moreover, to circumvent infeasible solutions they often use relaxed constraints leading to suboptimal pricing strategies. The reverse-engineered nature of traditional models complicates the enforcement of fairness and can lead to biased outcomes. Our method addresses these challenges by employing a direct optimization strategy in the continuous space of rates and by embedding fairness through an adversarial predictor model. This innovation not only reduces sequential errors and simplifies the complexities found in traditional models but also directly integrates fairness measures into the commercial premium calculation. We demonstrate improved margin performance and stronger enforcement of fairness highlighting the critical need to evolve existing pricing strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10275v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Grari, Marcin Detyniecki</dc:creator>
    </item>
    <item>
      <title>Covariate Ordered Systematic Sampling as an Improvement to Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2404.10381</link>
      <description>arXiv:2404.10381v1 Announce Type: cross 
Abstract: The Randomized Controlled Trial (RCT) or A/B testing is considered the gold standard method for estimating causal effects. Fisher famously advocated randomly allocating experiment units into treatment and control groups to preclude systematic biases. We propose a variant of systematic sampling called Covariate Ordered Systematic Sampling (COSS). In COSS, we order experimental units using a pre-experiment covariate and allocate them alternately into treatment and control groups. Using theoretical proofs, experiments on simulated data, and hundreds of A/B tests conducted within 3 real-world marketing campaigns, we show how our method achieves better sensitivity gains than commonly used variance reduction techniques like CUPED while retaining the simplicity of RCTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10381v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deddy Jobson, Li Yilin, Naoki Nishimura, Yang Jie, Koya Ohashi, Takeshi Matsumoto</dc:creator>
    </item>
    <item>
      <title>Exploring selective image matching methods for zero-shot and few-sample unsupervised domain adaptation of urban canopy prediction</title>
      <link>https://arxiv.org/abs/2404.10626</link>
      <description>arXiv:2404.10626v1 Announce Type: cross 
Abstract: We explore simple methods for adapting a trained multi-task UNet which predicts canopy cover and height to a new geographic setting using remotely sensed data without the need of training a domain-adaptive classifier and extensive fine-tuning. Extending previous research, we followed a selective alignment process to identify similar images in the two geographical domains and then tested an array of data-based unsupervised domain adaptation approaches in a zero-shot setting as well as with a small amount of fine-tuning. We find that the selective aligned data-based image matching methods produce promising results in a zero-shot setting, and even more so with a small amount of fine-tuning. These methods outperform both an untransformed baseline and a popular data-based image-to-image translation model. The best performing methods were pixel distribution adaptation and fourier domain adaptation on the canopy cover and height tasks respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10626v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Francis, Stephen Law</dc:creator>
    </item>
    <item>
      <title>Weighting methods for truncation by death in cluster-randomized trials</title>
      <link>https://arxiv.org/abs/2404.10629</link>
      <description>arXiv:2404.10629v1 Announce Type: cross 
Abstract: Patient-centered outcomes, such as quality of life and length of hospital stay, are the focus in a wide array of clinical studies. However, participants in randomized trials for elderly or critically and severely ill patient populations may have truncated or undefined non-mortality outcomes if they do not survive through the measurement time point. To address truncation by death, the survivor average causal effect (SACE) has been proposed as a causally interpretable subgroup treatment effect defined under the principal stratification framework. However, the majority of methods for estimating SACE have been developed in the context of individually-randomized trials. Only limited discussions have been centered around cluster-randomized trials (CRTs), where methods typically involve strong distributional assumptions for outcome modeling. In this paper, we propose two weighting methods to estimate SACE in CRTs that obviate the need for potentially complicated outcome distribution modeling. We establish the requisite assumptions that address latent clustering effects to enable point identification of SACE, and we provide computationally-efficient asymptotic variance estimators for each weighting estimator. In simulations, we evaluate our weighting estimators, demonstrating their finite-sample operating characteristics and robustness to certain departures from the identification assumptions. We illustrate our methods using data from a CRT to assess the impact of a sedation protocol on mechanical ventilation among children with acute respiratory failure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10629v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dane Isenberg, Michael Harhay, Nandita Mitra, Fan Li</dc:creator>
    </item>
    <item>
      <title>Robust Multivariate Functional Control Chart</title>
      <link>https://arxiv.org/abs/2207.07978</link>
      <description>arXiv:2207.07978v3 Announce Type: replace 
Abstract: In modern Industry 4.0 applications, a huge amount of data is acquired during manufacturing processes that are often contaminated with anomalous observations in the form of both casewise and cellwise outliers. These can seriously reduce the performance of control charting procedures, especially in complex and high-dimensional settings. To mitigate this issue in the context of profile monitoring, we propose a new framework, referred to as robust multivariate functional control chart (RoMFCC), that is able to monitor multivariate functional data while being robust to both functional casewise and cellwise outliers. The RoMFCC relies on four main elements: (I) a functional univariate filter to identify functional cellwise outliers to be replaced by missing components; (II) a robust multivariate functional data imputation method of missing values; (III) a casewise robust dimensionality reduction; (IV) a monitoring strategy for the multivariate functional quality characteristic. An extensive Monte Carlo simulation study is performed to compare the RoMFCC with competing monitoring schemes already appeared in the literature. Finally, a motivating real-case study is presented where the proposed framework is used to monitor a resistance spot welding process in the automotive industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.07978v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/00401706.2024.2327346</arxiv:DOI>
      <dc:creator>Christian Capezza, Fabio Centofanti, Antonio Lepore, Biagio Palumbo</dc:creator>
    </item>
    <item>
      <title>An Empirical Bayes Approach for Estimating Skill Models for Professional Darts Players</title>
      <link>https://arxiv.org/abs/2302.10750</link>
      <description>arXiv:2302.10750v2 Announce Type: replace 
Abstract: We perform an exploratory data analysis on a data-set for the top 16 professional darts players from the 2019 season. We use this data-set to fit player skill models which can then be used in dynamic zero-sum games (ZSGs) that model real-world matches between players. We propose an empirical Bayesian approach based on the Dirichlet-Multinomial (DM) model that overcomes limitations in the data. Specifically we introduce two DM-based skill models where the first model borrows strength from other darts players and the second model borrows strength from other regions of the dartboard. We find these DM-based models outperform simpler benchmark models with respect to Brier and Spherical scores, both of which are proper scoring rules. We also show in ZSGs settings that the difference between DM-based skill models and the simpler benchmark models is practically significant. Finally, we use our DM-model to analyze specific situations that arose in real-world darts matches during the 2019 season.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10750v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin B. Haugh, Chun Wang</dc:creator>
    </item>
  </channel>
</rss>
