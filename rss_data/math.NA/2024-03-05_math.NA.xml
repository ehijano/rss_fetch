<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Mar 2024 14:39:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Mar 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Extrapolated Plug-and-Play Three-Operator Splitting Methods for Nonconvex Optimization with Applications to Image Restoration</title>
      <link>https://arxiv.org/abs/2403.01144</link>
      <description>arXiv:2403.01144v1 Announce Type: new 
Abstract: This paper investigates the convergence properties and applications of the three-operator splitting method, also known as Davis-Yin splitting (DYS) method, integrated with extrapolation and Plug-and-Play (PnP) denoiser within a nonconvex framework. We first propose an extrapolated DYS method to effectively solve a class of structural nonconvex optimization problems that involve minimizing the sum of three possible nonconvex functions. Our approach provides an algorithmic framework that encompasses both extrapolated forward-backward splitting and extrapolated Douglas-Rachford splitting methods.To establish the convergence of the proposed method, we rigorously analyze its behavior based on the Kurdyka-{\L}ojasiewicz property, subject to some tight parameter conditions. Moreover, we introduce two extrapolated PnP-DYS methods with convergence guarantee, where the traditional regularization prior is replaced by a gradient step-based denoiser. This denoiser is designed using a differentiable neural network and can be reformulated as the proximal operator of a specific nonconvex functional. We conduct extensive experiments on image deblurring and image super-resolution problems, where our results showcase the advantage of the extrapolation strategy and the superior performance of the learning-based model that incorporates the PnP denoiser in terms of achieving high-quality recovery images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01144v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongming Wu, Chaoyan Huang, Tieyong Zeng</dc:creator>
    </item>
    <item>
      <title>Efficient Alternative Finite Difference WENO Schemes for Hyperbolic Conservation Laws</title>
      <link>https://arxiv.org/abs/2403.01264</link>
      <description>arXiv:2403.01264v1 Announce Type: new 
Abstract: Higher order finite difference Weighted Essentially Non-Oscillatory (WENO) schemes for conservation laws are extremely popular because, for multidimensional problems, they offer high order accuracy at a fraction of the cost of finite volume WENO or DG schemes. Such schemes come in two formulations. The very popular classical finite difference WENO (FD-WENO) method (Shu and Osher, J. Comput. Phys., 83 (1989) 32-78) relies two reconstruction steps applied to two split fluxes. However, the method cannot accommodate different types of Riemann solvers and cannot preserve free stream boundary conditions on curvilinear meshes. This limits its utility. The alternative finite difference WENO (AFD-WENO) method can overcome these deficiencies, however, much less work has been done on this method. The reasons are three-fold. First, it is difficult for the casual reader to understand the intricate logic that requires higher order derivatives of the fluxes to be evaluated at zone boundaries. The analytical methods for deriving the update equation for AFD-WENO schemes are somewhat recondite. To overcome that difficulty, we provide an easily accessible script that is based on a computer algebra system in Appendix A of this paper. Second, the method relies on interpolation rather than reconstruction, and WENO interpolation formulae have not been documented in the literature as thoroughly as WENO reconstruction formulae. In this paper, we explicitly provide all necessary WENO interpolation formulae that are needed for implementing AFD-WENO up to ninth order. The third reason is that AFD-WENO requires higher order derivatives of the fluxes to be available at zone boundaries. Since those derivatives are usually obtained by finite differencing the zone-centered fluxes, they become susceptible to a Gibbs phenomenon when the solution ...</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01264v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s42967-023-00360-z</arxiv:DOI>
      <dc:creator>Dinshaw S. Balsara, Deepak Bhoriya, Chi-Wang Shu, Harish Kumar</dc:creator>
    </item>
    <item>
      <title>Efficient Alternative Finite Difference WENO Schemes for Hyperbolic Systems with Non-Conservative Products</title>
      <link>https://arxiv.org/abs/2403.01266</link>
      <description>arXiv:2403.01266v1 Announce Type: new 
Abstract: Higher order finite difference Weighted Essentially Non-Oscillatory (WENO) schemes for conservation laws represent a technology that has been reasonably consolidated. They are extremely popular because, when applied to multidimensional problems, they offer high order accuracy at a fraction of the cost of finite volume WENO or DG schemes. They come in two flavors. There is the classical finite difference WENO (FD-WENO) method (Shu and Osher, J. Comput. Phys., 83 (1989) 32-78). However, in recent years there is also an alternative finite difference WENO (AFD-WENO) method which has recently been formalized into a very useful general-purpose algorithm for conservation laws (Balsara et al., Efficient Alternative Finite Difference WENO Schemes for Hyperbolic Conservation Laws, submitted to CAMC (2023)). However, the FD-WENO algorithm has only very recently been formulated for hyperbolic systems with non-conservative products (Balsara et al., Efficient Finite Difference WENO Scheme for Hyperbolic Systems with Non-Conservative Products, to appear CAMC (2023)). In this paper we show that there are substantial advantages in obtaining an AFD-WENO algorithm for hyperbolic systems with non-conservative products. Such an algorithm is documented in this paper. We present an AFD-WENO formulation in fluctuation form that is carefully engineered to retrieve the flux form when that is warranted and nevertheless extends to non-conservative products. The method is flexible because it allows any Riemann solver to be used. The formulation we arrive at is such that when non-conservative products are absent it reverts exactly to the formulation in the second citation above which is in exact flux conservation form. The ability to transition to a precise conservation form when non-conservative products are absent ensures, via the Lax-Wendroff theorem, that shock locations will be exactly ...</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01266v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s42967-024-00374-1</arxiv:DOI>
      <dc:creator>Dinshaw S. Balsara, Deepak Bhoriya, Chi-Wang Shu, Harish Kumar</dc:creator>
    </item>
    <item>
      <title>A fractional-order trace-dev-div inequality</title>
      <link>https://arxiv.org/abs/2403.01291</link>
      <description>arXiv:2403.01291v1 Announce Type: new 
Abstract: The trace-dev-div inequality in $H^s$ controls the trace in the norm of $H^s$ by that of the deviatoric part plus the $H^{s-1}$ norm of the divergence of a quadratic tensor field different from the constant unit matrix. This is well known for $s=0$ and established for orders $0\le s\le 1$ and arbitrary space dimension in this note. For mixed and least-squares finite element error analysis in linear elasticity, this inequality allows to establish robustness with respect to the Lam\'e parameter $\lambda$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01291v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carsten Carstensen, Norbert Heuer</dc:creator>
    </item>
    <item>
      <title>An RBF partition of unity method for geometry reconstruction and PDE solution in thin structures</title>
      <link>https://arxiv.org/abs/2403.01486</link>
      <description>arXiv:2403.01486v1 Announce Type: new 
Abstract: The main respiratory muscle, the diaphragm, is an example of a thin structure. We aim to perform detailed numerical simulations of the muscle mechanics based on individual patient data. This requires a representation of the diaphragm geometry extracted from medical image data. We design an adaptive reconstruction method based on a least-squares radial basis function partition of unity method. The method is adapted to thin structures by subdividing the structure rather than the surrounding space, and by introducing an anisotropic scaling of local subproblems. The resulting representation is an infinitely smooth level set function, which is stabilized such that there are no spurious zero level sets. We show reconstruction results for 2D cross sections of the diaphragm geometry as well as for the full 3D geometry. We also show solutions to basic PDE test problems in the reconstructed geometries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01486v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elisabeth Larsson, Pierre-Fr\'ed\'eric Villard, Igor Tominec, Ulrika Sundin, Andreas Michael, Nicola Cacciani</dc:creator>
    </item>
    <item>
      <title>Fast Algorithm for Quasi-2D Coulomb Systems</title>
      <link>https://arxiv.org/abs/2403.01521</link>
      <description>arXiv:2403.01521v1 Announce Type: new 
Abstract: Quasi-2D Coulomb systems are of fundamental importance and have attracted much attention in many areas nowadays. Their reduced symmetry gives rise to interesting collective behaviors, but also brings great challenges for particle-based simulations. Here, we propose a novel algorithm framework to address the $\mathcal O(N^2)$ simulation complexity associated with the long-range nature of Coulomb interactions. First, we introduce an efficient Sum-of-Exponentials (SOE) approximation for the long-range kernel associated with Ewald splitting, achieving uniform convergence in terms of inter-particle distance, which reduces the complexity to $\mathcal{O}(N^{7/5})$. We then introduce a random batch sampling method in the periodic dimensions, the stochastic approximation is proven to be both unbiased and with reduced variance via a tailored importance sampling strategy, further reducing the computational cost to $\mathcal{O}(N)$. The performance of our algorithm is demonstrated via varies numerical examples. Notably, it achieves a speedup of $2\sim 3$ orders of magnitude comparing with Ewald2D method, enabling molecular dynamics (MD) simulations with up to $10^6$ particles on a single core. The present approach is therefore well-suited for large-scale particle-based simulations of Coulomb systems under confinement, making it possible to investigate the role of Coulomb interaction in many practical situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01521v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zecheng Gan, Xuanzhao Gao, Jiuyang Liang, Zhenli Xu</dc:creator>
    </item>
    <item>
      <title>Adaptive multiplication of $\mathcal{H}^2$-matrices with block-relative error control</title>
      <link>https://arxiv.org/abs/2403.01566</link>
      <description>arXiv:2403.01566v1 Announce Type: new 
Abstract: The discretization of non-local operators, e.g., solution operators of partial differential equations or integral operators, leads to large densely populated matrices. $\mathcal{H}^2$-matrices take advantage of local low-rank structures in these matrices to provide an efficient data-sparse approximation that allows us to handle large matrices efficiently, e.g., to reduce the storage requirements to $\mathcal{O}(n k)$ for $n$-dimensional matrices with local rank $k$, and to reduce the complexity of the matrix-vector multiplication to $\mathcal{O}(n k)$ operations.
  In order to perform more advanced operations, e.g., to construct efficient preconditioners or evaluate matrix functions, we require algorithms that take $\mathcal{H}^2$-matrices as input and approximate the result again by $\mathcal{H}^2$-matrices, ideally with controllable accuracy. In this manuscript, we introduce an algorithm that approximates the product of two $\mathcal{H}^2$-matrices and guarantees block-relative error estimates for the submatrices of the result. It uses specialized tree structures to represent the exact product in an intermediate step, thereby allowing us to apply mathematically rigorous error control strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01566v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steffen B\"orm</dc:creator>
    </item>
    <item>
      <title>Bayesian inference via geometric optics approximation</title>
      <link>https://arxiv.org/abs/2403.01655</link>
      <description>arXiv:2403.01655v1 Announce Type: new 
Abstract: Markov chain Monte Carlo (MCMC) simulations have been widely used to generate samples from the complex posterior distribution in Bayesian inferences. However, these simulations often require multiple computations of the forward model in the likelihood function for each drawn sample. This computational burden renders MCMC sampling impractical when the forward model is computationally expensive, such as in the case of partial differential equation models. In this paper, we propose a novel sampling approach called the geometric optics approximation method (GOAM) for Bayesian inverse problems, which entirely circumvents the need for MCMC simulations. Our method is rooted in the problem of reflector shape design, which focuses on constructing a reflecting surface that redirects rays from a source, with a predetermined density, towards a target domain while achieving a desired density distribution. The key idea is to consider the unnormalized Bayesian posterior as the density on the target domain within the optical system and define a geometric optics approximation measure with respect to posterior by a reflecting surface. Consequently, once such a reflecting surface is obtained, we can utilize it to draw an arbitrary number of independent and uncorrelated samples from the posterior measure for Bayesian inverse problems. In theory, we have shown that the geometric optics approximation measure is well-posed. The efficiency and robustness of our proposed sampler, employing the geometric optics approximation method, are demonstrated through several numerical examples provided in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01655v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zejun Sun, Guang-Hui Zheng</dc:creator>
    </item>
    <item>
      <title>Singular value decompositions of third-order reduced biquaternion tensors</title>
      <link>https://arxiv.org/abs/2403.01690</link>
      <description>arXiv:2403.01690v1 Announce Type: new 
Abstract: In this paper, we introduce the applications of third-order reduced biquaternion tensors in color video processing. We first develop algorithms for computing the singular value decomposition (SVD) of a third-order reduced biquaternion tensor via a new Ht-product. As theoretical applications, we define the Moore-Penrose inverse of a third-order reduced biquaternion tensor and develop its characterizations. In addition, we discuss the general (or Hermitian) solutions to reduced biquaternion tensor equation $\mathcal{A}\ast_{Ht} \mathcal{X}=\mathcal{B}$ as well as its least-square solution. Finally, we compress the color video by this SVD, and the experimental data shows that our method is faster than the compared scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01690v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cui-E Yu, Xin Liu, Yang Zhang</dc:creator>
    </item>
    <item>
      <title>HOSCF: Efficient decoupling algorithms for finding the best rank-one approximation of higher-order tensors</title>
      <link>https://arxiv.org/abs/2403.01778</link>
      <description>arXiv:2403.01778v1 Announce Type: new 
Abstract: Best rank-one approximation is one of the most fundamental tasks in tensor computation. In order to fully exploit modern multi-core parallel computers, it is necessary to develop decoupling algorithms for computing the best rank-one approximation of higher-order tensors at large scales. In this paper, we first build a bridge between the rank-one approximation of tensors and the eigenvector-dependent nonlinear eigenvalue problem (NEPv), and then develop an efficient decoupling algorithm, namely the higher-order self-consistent field (HOSCF) algorithm, inspired by the famous self-consistent field (SCF) iteration frequently used in computational chemistry. The convergence theory of the HOSCF algorithm and an estimation of the convergence speed are further presented. In addition, we propose an improved HOSCF (iHOSCF) algorithm that incorporates the Rayleigh quotient iteration, which can significantly accelerate the convergence of HOSCF. Numerical experiments show that the proposed algorithms can efficiently converge to the best rank-one approximation of both synthetic and real-world tensors and can scale with high parallel scalability on a modern parallel computer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01778v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuanfu Xiao, Zeyu Li, Chao Yang</dc:creator>
    </item>
    <item>
      <title>Simulation-based High-Speed Elongational Rheometer for Carreau-type Materials</title>
      <link>https://arxiv.org/abs/2403.01812</link>
      <description>arXiv:2403.01812v1 Announce Type: new 
Abstract: For the simulation-based design of fiber melt spinning processes, the accurate modeling of the processed polymer with regard to its material behavior is crucial. In this work, we develop a high-speed elongational rheometer for Carreau-type materials, making use of process simulations and fiber diameter measurements. The procedure is based on a unified formulation of the fiber spinning model for all material types (Newtonian and non-Newtonian), whose material laws are strictly monotone in the strain rate. The parametrically described material law for the elongational viscosity implies a nonlinear optimization problem for the parameter identification, for which we propose an efficient, robust gradient-based method. The work can be understood as a proof of concept, a generalization to other, more complex materials is possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01812v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Kannengiesser, Walter Arne, Alexander Bier, Nicole Marheineke, Dirk W. Schubert, Raimund Wegener</dc:creator>
    </item>
    <item>
      <title>Macroscopic auxiliary asymptotic preserving neural networks for the linear radiative transfer equations</title>
      <link>https://arxiv.org/abs/2403.01820</link>
      <description>arXiv:2403.01820v1 Announce Type: new 
Abstract: We develop a Macroscopic Auxiliary Asymptotic-Preserving Neural Network (MA-APNN) method to solve the time-dependent linear radiative transfer equations (LRTEs), which have a multi-scale nature and high dimensionality. To achieve this, we utilize the Physics-Informed Neural Networks (PINNs) framework and design a new adaptive exponentially weighted Asymptotic-Preserving (AP) loss function, which incorporates the macroscopic auxiliary equation that is derived from the original transfer equation directly and explicitly contains the information of the diffusion limit equation. Thus, as the scale parameter tends to zero, the loss function gradually transitions from the transport state to the diffusion limit state. In addition, the initial data, boundary conditions, and conservation laws serve as the regularization terms for the loss. We present several numerical examples to demonstrate the effectiveness of MA-APNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01820v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyan Li, Song Jiang, Wenjun Sun, Liwei Xu, Guanyu Zhou</dc:creator>
    </item>
    <item>
      <title>Numerical Simulation of Phase Transition with the Hyperbolic Godunov-Peshkov-Romenski Model</title>
      <link>https://arxiv.org/abs/2403.01847</link>
      <description>arXiv:2403.01847v1 Announce Type: new 
Abstract: In this paper, a thermodynamically consistent solution of the interfacial Riemann problem for the first-order hyperbolic continuum model of Godunov, Peshkov and Romenski (GPR model) is presented. In the presence of phase transition, interfa- cial physics are governed by molecular interaction on a microscopic scale, beyond the scope of the macroscopic continuum model in the bulk phases. The developed two-phase Riemann solvers tackle this multi-scale problem, by incorporating a local thermodynamic model to predict the interfacial entropy production. Using phenomenological relations of non-equilibrium thermodynamics, interfacial mass and heat fluxes are derived from the entropy production and provide closure at the phase boundary. We employ the proposed Riemann solvers in an efficient sharp in- terface level-set Ghost-Fluid framework to provide coupling conditions at phase in- terfaces under phase transition. As a single-phase benchmark, a Rayleigh-B\'enard convection is studied to compare the hyperbolic thermal relaxation formulation of the GPR model against the hyperbolic-parabolic Euler-Fourier system. The novel interfacial Riemann solvers are validated against molecular dynamics simulations of evaporating shock tubes with the Lennard-Jones shifted and truncated potential. On a macroscopic scale, evaporating shock tubes are computed for the material n- Dodecane and compared against Euler-Fourier results. Finally, the efficiency and robustness of the scheme is demonstrated with shock-droplet interaction simula- tions that involve both phase transfer and surface tension, while featuring severe interface deformations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01847v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pascal Mossier, Steven J\"ons, Simone Chiocchetti, Andrea D. Beck, Claus-Dieter Munz</dc:creator>
    </item>
    <item>
      <title>High curvature means low-rank: On the sectional curvature of Grassmann and Stiefel manifolds and the underlying matrix trace inequalities</title>
      <link>https://arxiv.org/abs/2403.01879</link>
      <description>arXiv:2403.01879v1 Announce Type: new 
Abstract: Methods and algorithms that work with data on nonlinear manifolds are collectively summarised under the term `Riemannian computing'. In practice, curvature can be a key limiting factor for the performance of Riemannian computing methods. Yet, curvature can also be a powerful tool in the theoretical analysis of Riemannian algorithms. In this work, we investigate the sectional curvature of the Stiefel and Grassmann manifold from the quotient space view point. On the Grassmannian, tight curvature bounds are known since the late 1960ies. On the Stiefel manifold under the canonical metric, it was believed that the sectional curvature does not exceed 5/4. For both of these manifolds, the sectional curvature is given by the Frobenius norm of certain structured commutator brackets of skew-symmetric matrices. We provide refined inequalities for such terms and pay special attention to the maximizers of the curvature bounds. In this way, we prove that the global bound of 5/4 for Stiefel holds indeed. With this addition, a complete account of the curvature bounds in all admissible dimensions is obtained. We observe that `high curvature means low-rank', more precisely, for the Stiefel and Grassmann manifolds, the global curvature maximum is attained at tangent plane sections that are spanned by rank-two matrices. Numerical examples are included for illustration purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01879v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ralf Zimmermann, Jakob Stoye</dc:creator>
    </item>
    <item>
      <title>Projected Newton method for large-scale Bayesian linear inverse problems</title>
      <link>https://arxiv.org/abs/2403.01920</link>
      <description>arXiv:2403.01920v1 Announce Type: new 
Abstract: Computing the regularized solution of Bayesian linear inverse problems as well as the corresponding regularization parameter is highly desirable in many applications. This paper proposes a novel iterative method, termed the Projected Newton method (PNT), that can simultaneously update the regularization parameter and solution step by step without requiring any high-cost matrix inversions or decompositions. By reformulating the Tikhonov regularization as a constrained minimization problem and writing its Lagrangian function, a Newton-type method coupled with a Krylov subspace method, called the generalized Golub-Kahan bidiagonalization, is employed for the unconstrained Lagrangian function. The resulting PNT algorithm only needs solving a small-scale linear system to get a descent direction of a merit function at each iteration, thus significantly reducing computational overhead. Rigorous convergence results are proved, showing that PNT always converges to the unique regularized solution and the corresponding Lagrangian multiplier. Experimental results on both small and large-scale Bayesian inverse problems demonstrate its excellent convergence property, robustness and efficiency. Given that the most demanding computational tasks in PNT are primarily matrix-vector products, it is particularly well-suited for large-scale problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01920v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haibo Li</dc:creator>
    </item>
    <item>
      <title>Improving the accuracy of the Newmark method through backward error analysis</title>
      <link>https://arxiv.org/abs/2403.02029</link>
      <description>arXiv:2403.02029v1 Announce Type: new 
Abstract: We use backward error analysis for differential equations to obtain modified or distorted equations describing the behaviour of the Newmark scheme applied to the transient structural dynamics equation. Using these results, we show how to construct compensation terms from the original parameters of the system, which improve the performance of Newmark simulations without changing the time step or modifying the scheme itself. Two such compensations are given: one eliminates numerical damping, while the other achieves fourth-order accurate calculations using the traditionally second-order Newmark method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02029v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Don\'at M. Tak\'acs, Tam\'as F\"ul\"op</dc:creator>
    </item>
    <item>
      <title>Exponential Expressivity of ReLU$^k$ Neural Networks on Gevrey Classes with Point Singularities</title>
      <link>https://arxiv.org/abs/2403.02035</link>
      <description>arXiv:2403.02035v1 Announce Type: new 
Abstract: We analyze deep Neural Network emulation rates of smooth functions with point singularities in bounded, polytopal domains $\mathrm{D} \subset \mathbb{R}^d$, $d=2,3$. We prove exponential emulation rates in Sobolev spaces in terms of the number of neurons and in terms of the number of nonzero coefficients for Gevrey-regular solution classes defined in terms of weighted Sobolev scales in $\mathrm{D}$, comprising the countably-normed spaces of I.M. Babu\v{s}ka and B.Q. Guo.
  As intermediate result, we prove that continuous, piecewise polynomial high order (``$p$-version'') finite elements with elementwise polynomial degree $p\in\mathbb{N}$ on arbitrary, regular, simplicial partitions of polyhedral domains $\mathrm{D} \subset \mathbb{R}^d$, $d\geq 2$ can be exactly emulated by neural networks combining ReLU and ReLU$^2$ activations. On shape-regular, simplicial partitions of polytopal domains $\mathrm{D}$, both the number of neurons and the number of nonzero parameters are proportional to the number of degrees of freedom of the finite element space, in particular for the $hp$-Finite Element Method of I.M. Babu\v{s}ka and B.Q. Guo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02035v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joost A. A. Opschoor, Christoph Schwab</dc:creator>
    </item>
    <item>
      <title>Analysis on aggregation and block smoothers in multigrid methods for block Toeplitz linear systems</title>
      <link>https://arxiv.org/abs/2403.02139</link>
      <description>arXiv:2403.02139v1 Announce Type: new 
Abstract: We present novel improvements in the context of symbol-based multigrid procedures for solving large block structured linear systems. We study the application of an aggregation-based grid transfer operator that transforms the symbol of a block Toeplitz matrix from matrix-valued to scalar-valued at the coarser level. Our convergence analysis of the Two-Grid Method (TGM) reveals the connection between the features of the scalar-valued symbol at the coarser level and the properties of the original matrix-valued one. This allows us to prove the convergence of a V-cycle multigrid with standard grid transfer operators for scalar Toeplitz systems at the coarser levels. Consequently, we extend the class of suitable smoothers for block Toeplitz matrices, focusing on the efficiency of block strategies, particularly the relaxed block Jacobi method. General conditions on smoothing parameters are derived, with emphasis on practical applications where these parameters can be calculated with negligible computational cost. We test the proposed strategies on linear systems stemming from the discretization of differential problems with $\mathbb{Q}_{d} $ Lagrangian FEM or B-spline with non-maximal regularity. The numerical results show in both cases computational advantages compared to existing methods for block structured linear systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02139v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bolten, Marco Donatelli, Paola Ferrari, Isabella Furci</dc:creator>
    </item>
    <item>
      <title>Multi-Derivative Runge-Kutta Flux Reconstruction for hyperbolic conservation laws</title>
      <link>https://arxiv.org/abs/2403.02141</link>
      <description>arXiv:2403.02141v1 Announce Type: new 
Abstract: We extend the fourth order, two stage Multi-Derivative Runge Kutta (MDRK) scheme of Li and Du to the Flux Reconstruction (FR) framework by writing both of the stages in terms of a time averaged flux and then use the approximate Lax-Wendroff procedure. Numerical flux is computed in each stage using D2 dissipation and EA flux, enhancing Fourier CFL stability and accuracy respectively. A subcell based blending limiter is developed for the MDRK scheme, which ensures that the limited scheme is provably admissibility preserving. Along with being admissibility preserving, the blending scheme is constructed to minimize dissipation errors by using Gauss-Legendre solution points and performing MUSCL-Hancock reconstruction on subcells. The accuracy enhancement of the blending scheme is numerically verified on compressible Euler's equations, with test cases involving shocks and small-scale structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02141v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arpit Babbar, Praveen Chandrashekar</dc:creator>
    </item>
    <item>
      <title>Sharp-interface limits for brittle fracture via the inverse-deformation formulation</title>
      <link>https://arxiv.org/abs/2403.00838</link>
      <description>arXiv:2403.00838v1 Announce Type: cross 
Abstract: We derive sharp-interface models for one-dimensional brittle fracture via the inverse-deformation approach. Methods of Gamma-convergence are employed to obtain the singular limits of previously proposed models. The latter feature a local, non-convex stored energy of inverse strain, augmented by small interfacial energy, formulated in terms of the inverse-strain gradient. They predict spontaneous fracture with exact crack-opening discontinuities, without the use of damage (phase) fields or pre-existing cracks; crack faces are endowed with a thin layer of surface energy. The models obtained herewith inherit the same properties, except that surface energy is now concentrated at the crack faces. Accordingly, we construct energy-minimizing configurations. For a composite bar with a breakable layer, our results predict a pattern of equally spaced cracks whose number is given as an increasing function of applied load.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00838v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy J. Healey, Roberto Paroni, Phoebus Rosakis</dc:creator>
    </item>
    <item>
      <title>On complex algebraic singularities of some genuinely nonlinear PDEs</title>
      <link>https://arxiv.org/abs/2403.00874</link>
      <description>arXiv:2403.00874v1 Announce Type: cross 
Abstract: In this manuscript, we highlight a new phenomenon of complex algebraic singularity formation for solutions of a large class of genuinely nonlinear partial differential equations (PDEs). We start from a unique Cauchy datum, which is holomorphic ramified around the smooth locus and is sufficiently singular. Then, we expect the existence of a solution which should be holomorphic ramified around the singular locus S defined by the vanishing of the discriminant of an algebraic equation. Notice, moreover, that the monodromy of the Cauchy datum is Abelian, whereas one of the solutions is non-Abelian. Moreover, the singular locus S depends on the Cauchy datum in contrast to the Leray principle (stated for linear problems only). This phenomenon is due to the fact that the PDE is genuinely nonlinear and that the Cauchy datum is sufficiently singular. First, we investigate the case of the inviscid Burgers equation. Later, we state a general conjecture that describes the expected phenomenon. We view this Conjecture as a working programme allowing us to develop interesting new Mathematics. We also state another Conjecture 2, which is a particular case of the general Conjecture but keeps all the flavour and difficulty of the subject. Then, we propose a new algorithm with a map F such that a fixed point of F would give a solution to the problem associated with Conjecture 2. Then, we perform convincing, elaborate numerical tests that suggest that a Banach norm should exist for which the mapping F should be a contraction so that the solution (with the above specific algebraic structure) should be unique. This work is a continuation of Leichtnam (1993).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00874v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.CV</category>
      <category>math.NA</category>
      <category>nlin.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Denys Dutykh, \'Eric Leichtnam</dc:creator>
    </item>
    <item>
      <title>The sequence of pseudo-equilibria describes the long-time behaviour of the NNLIF model with large delay</title>
      <link>https://arxiv.org/abs/2403.00971</link>
      <description>arXiv:2403.00971v1 Announce Type: cross 
Abstract: There is a wide range of mathematical models that describe populations of large numbers of neurons. In this article, we focus on nonlinear noisy leaky integrate and fire (NNLIF) models that describe neuronal activity at the level of the membrane potential of neurons. We introduce a set of novel states, which we call "pseudo-equilibria", and give evidence of their defining role in the behaviour of the NNLIF system when a significant synaptic delay is considered. The advantage is that these states are determined solely by the system's parameters and are derived from a sequence of firing rates that result from solving a recurrence equation. We propose a new strategy to show convergence to an equilibrium for a weakly connected system with large transmission delay, based on following the sequence of pseudo-equilibria. Unlike with the direct entropy dissipation method, this technique allows us to see how a large delay favours convergence. We also present a detailed numerical study to support our results. This study explores the overall behaviour of the NNLIF system and helps us understand, among other phenomena, periodic solutions in strongly inhibitory networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00971v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mar\'ia J. C\'aceres, Jos\'e A. Ca\~nizo, Alejandro Ramos-Lora</dc:creator>
    </item>
    <item>
      <title>Advancing parabolic operators in thermodynamic MHD models II: Evaluating a Practical Time Step Limit for Unconditionally Stable Methods</title>
      <link>https://arxiv.org/abs/2403.01004</link>
      <description>arXiv:2403.01004v1 Announce Type: cross 
Abstract: Unconditionally stable time stepping schemes are useful and often practically necessary for advancing parabolic operators in multi-scale systems. However, serious accuracy problems may emerge when taking time steps that far exceed the explicit stability limits. In our previous work, we compared the accuracy and performance of advancing parabolic operators in a thermodynamic MHD model using an implicit method and an explicit super time-stepping (STS) method. We found that while the STS method outperformed the implicit one with overall good results, it was not able to damp oscillatory behavior in the solution efficiently, hindering its practical use. In this follow-up work, we evaluate an easy-to-implement method for selecting a practical time step limit (PTL) for unconditionally stable schemes. This time step is used to `cycle' the operator-split thermal conduction and viscosity parabolic operators. We test the new time step with both an implicit and STS scheme for accuracy, performance, and scaling. We find that, for our test cases here, the PTL dramatically improves the STS solution, matching or improving the solution of the original implicit scheme, while retaining most of its performance and scaling advantages. The PTL shows promise to allow more accurate use of unconditionally stable schemes for parabolic operators and reliable use of STS methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01004v1</guid>
      <category>cs.CE</category>
      <category>astro-ph.GA</category>
      <category>astro-ph.SR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronald M. Caplan, Craig D. Johnston, Lars K. S. Daldoff, Jon A. Linker</dc:creator>
    </item>
    <item>
      <title>Edge-guided Low-light Image Enhancement with Inertial Bregman Alternating Linearized Minimization</title>
      <link>https://arxiv.org/abs/2403.01142</link>
      <description>arXiv:2403.01142v1 Announce Type: cross 
Abstract: Prior-based methods for low-light image enhancement often face challenges in extracting available prior information from dim images. To overcome this limitation, we introduce a simple yet effective Retinex model with the proposed edge extraction prior. More specifically, we design an edge extraction network to capture the fine edge features from the low-light image directly. Building upon the Retinex theory, we decompose the low-light image into its illumination and reflectance components and introduce an edge-guided Retinex model for enhancing low-light images. To solve the proposed model, we propose a novel inertial Bregman alternating linearized minimization algorithm. This algorithm addresses the optimization problem associated with the edge-guided Retinex model, enabling effective enhancement of low-light images. Through rigorous theoretical analysis, we establish the convergence properties of the algorithm. Besides, we prove that the proposed algorithm converges to a stationary point of the problem through nonconvex optimization theory. Furthermore, extensive experiments are conducted on multiple real-world low-light image datasets to demonstrate the efficiency and superiority of the proposed scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01142v1</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoyan Huang, Zhongming Wu, Tieyong Zeng</dc:creator>
    </item>
    <item>
      <title>Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise</title>
      <link>https://arxiv.org/abs/2403.01204</link>
      <description>arXiv:2403.01204v1 Announce Type: cross 
Abstract: We propose SGD-exp, a stochastic gradient descent approach for linear and ReLU regressions under Massart noise (adversarial semi-random corruption model) for the fully streaming setting. We show novel nearly linear convergence guarantees of SGD-exp to the true parameter with up to $50\%$ Massart corruption rate, and with any corruption rate in the case of symmetric oblivious corruptions. This is the first convergence guarantee result for robust ReLU regression in the streaming setting, and it shows the improved convergence rate over previous robust methods for $L_1$ linear regression due to a choice of an exponentially decaying step size, known for its efficiency in practice. Our analysis is based on the drift analysis of a discrete stochastic process, which could also be interesting on its own.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01204v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Halyun Jeong, Deanna Needell, Elizaveta Rebrova</dc:creator>
    </item>
    <item>
      <title>Network analysis using Krylov subspace trajectories</title>
      <link>https://arxiv.org/abs/2403.01269</link>
      <description>arXiv:2403.01269v1 Announce Type: cross 
Abstract: We describe a set of network analysis methods based on the rows of the Krylov subspace matrix computed from a network adjacency matrix via power iteration using a non-random initial vector. We refer to these node-specific row vectors as Krylov subspace trajectories. While power iteration using a random initial starting vector is commonly applied to the network adjacency matrix to compute eigenvector centrality values, this application only uses the final vector generated after numerical convergence. Importantly, use of a random initial vector means that the intermediate results of power iteration are also random and lack a clear interpretation. To the best of our knowledge, use of intermediate power iteration results for network analysis has been limited to techniques that leverage just a single pre-convergence solution, e.g., Power Iteration Clustering. In this paper, we explore methods that apply power iteration with a non-random inital vector to the network adjacency matrix to generate Krylov subspace trajectories for each node. These non-random trajectories provide important information regarding network structure, node importance, and response to perturbations. We have created this short preprint in part to generate feedback from others in the network analysis community who might be aware of similar existing work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01269v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.NA</category>
      <category>cs.SI</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>H. Robert Frost</dc:creator>
    </item>
    <item>
      <title>A face-centred finite volume method for laminar and turbulent incompressible flows</title>
      <link>https://arxiv.org/abs/2403.01496</link>
      <description>arXiv:2403.01496v1 Announce Type: cross 
Abstract: This work develops, for the first time, a face-centred finite volume (FCFV) solver for the simulation of laminar and turbulent viscous incompressible flows. The formulation relies on the Reynolds-averaged Navier-Stokes (RANS) equations coupled with the negative Spalart-Allmaras (SA) model and three novel convective stabilisations, inspired by Riemann solvers, are derived and compared numerically. The resulting method achieves first-order convergence of the velocity, the velocity-gradient tensor and the pressure. FCFV accurately predicts engineering quantities of interest, such as drag and lift, on unstructured meshes and, by avoiding gradient reconstruction, the method is insensitive to mesh quality, even in the presence of highly distorted and stretched cells. A monolithic and a staggered solution strategies for the RANS-SA system are derived and compared numerically. Numerical benchmarks, involving laminar and turbulent, steady and transient cases are used to assess the performance, accuracy and robustness of the proposed FCFV method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01496v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luan M. Vieira, Matteo Giacomini, Ruben Sevilla, Antonio Huerta</dc:creator>
    </item>
    <item>
      <title>Generalized pair-wise logit dynamic and its connection to a mean field game: theoretical and computational investigations focusing on resource management</title>
      <link>https://arxiv.org/abs/2403.01657</link>
      <description>arXiv:2403.01657v1 Announce Type: cross 
Abstract: Logit dynamics are evolution equations that describe transitions to equilibria of actions among many players. We formulate a pair-wise logit dynamic in a continuous action space with a generalized exponential function, which we call a generalized pair-wise logit dynamic, depicted by a new evolution equation nonlocal in space. We prove the well-posedness and approximability of the generalized pair-wise logit dynamic to show that it is computationally implementable. We also show that this dynamic has an explicit connection to a mean field game of a controlled pure-jump process, with which the two different mathematical models can be understood in a unified way. Particularly, we show that the generalized pair-wise logit dynamic is derived as a myopic version of the corresponding mean field game, and that the conditions to guarantee the existence of unique solutions are different from each other. The key in this procedure is to find the objective function to be optimized in the mean field game based on the logit function. The monotonicity of the utility is unnecessary for the generalized pair-wise logit dynamic but crucial for the mean field game. Finally, we present applications of the two approaches to fisheries management problems with collected data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01657v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hidekazu Yoshioka, Motoh Tsujimura</dc:creator>
    </item>
    <item>
      <title>Homotopy Methods for Convex Optimization</title>
      <link>https://arxiv.org/abs/2403.02095</link>
      <description>arXiv:2403.02095v1 Announce Type: cross 
Abstract: Convex optimization encompasses a wide range of optimization problems, containing many efficiently solvable subclasses. Interior point methods are currently the state-of-the-art approach for solving such problems, particularly effective for classes like semidefinite programming, quadratic programming, and geometric programming. However, their success hinges on the construction of self-concordant barrier functions for the feasible sets. In this work, we introduce an alternative method for tackling convex optimization problems, employing a homotopy. With this technique, the feasible set of a trivial optimization problem is continuously transformed into the target one, while tracking the solutions. We conduct an analysis of this approach, focusing on its application to semidefinite programs, hyperbolic programs, and convex optimization problems with a single convexity constraint. Moreover, we demonstrate that our approach numerically outperforms state-of-the-art methods in several interesting cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02095v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.AG</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Klingler, Tim Netzer</dc:creator>
    </item>
    <item>
      <title>Exploring Well-Posedness and Asymptotic Behavior in an Advection-Diffusion-Reaction (ADR) Model</title>
      <link>https://arxiv.org/abs/2403.02339</link>
      <description>arXiv:2403.02339v1 Announce Type: cross 
Abstract: In this paper, the existence, uniqueness, and positivity of solutions, as well as the asymptotic behavior through a finite fractal dimensional global attractor for a general Advection-Diffusion-Reaction (ADR) equation, are investigated. Our findings are innovative, as we employ semigroups and global attractors theories to achieve these results. Also, an analytical solution of a two-dimensional Advection-Diffusion Equation is presented. And finally, two Explicit Finite Difference schemes are used to simulate solutions in the two- and three-dimensional cases. The numerical simulations are conducted with predefined initial and Dirichlet boundary conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02339v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Elghandouri, Khalil Ezzinbi, Lamiae Saidi</dc:creator>
    </item>
    <item>
      <title>Implicit high-order gas-kinetic schemes for compressible flows on three-dimensional unstructured meshes I: steady flows</title>
      <link>https://arxiv.org/abs/2304.09485</link>
      <description>arXiv:2304.09485v2 Announce Type: replace 
Abstract: In the previous studies, the high-order gas-kinetic schemes (HGKS) have achieved successes for unsteady flows on three-dimensional unstructured meshes. In this paper, to accelerate the rate of convergence for steady flows, the implicit non-compact and compact HGKSs are developed. For non-compact scheme, the simple weighted essentially non-oscillatory (WENO) reconstruction is used to achieve the spatial accuracy, where the stencils for reconstruction contain two levels of neighboring cells. Incorporate with the nonlinear generalized minimal residual (GMRES) method, the implicit non-compact HGKS is developed. In order to improve the resolution and parallelism of non-compact HGKS, the implicit compact HGKS is developed with Hermite WENO (HWENO) reconstruction, in which the reconstruction stencils only contain one level of neighboring cells. The cell averaged conservative variable is also updated with GMRES method. Simultaneously, a simple strategy is used to update the cell averaged gradient by the time evolution of spatial-temporal coupled gas distribution function. To accelerate the computation, the implicit non-compact and compact HGKSs are implemented with the graphics processing unit (GPU) using compute unified device architecture (CUDA). A variety of numerical examples, from the subsonic to supersonic flows, are presented to validate the accuracy, robustness and efficiency of both inviscid and viscous flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09485v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaqing Yang, Liang Pan, Kun Xu</dc:creator>
    </item>
    <item>
      <title>A multilevel Monte Carlo algorithm for SDEs driven by countably dimensional Wiener process and Poisson random measure</title>
      <link>https://arxiv.org/abs/2307.16640</link>
      <description>arXiv:2307.16640v2 Announce Type: replace 
Abstract: In this paper, we investigate the properties of standard and multilevel Monte Carlo methods for weak approximation of solutions of stochastic differential equations (SDEs) driven by the infinite-dimensional Wiener process and Poisson random measure with Lipschitz payoff function. The error of the truncated dimension randomized numerical scheme, which is determined by two parameters, i.e grid density $n \in \mathbb{N}_{+}$ and truncation dimension parameter $M \in \mathbb{N}_{+},$ is of the order $n^{-1/2}+\delta(M)$ such that $\delta(\cdot)$ is positive and decreasing to $0$. We derive complexity model and provide proof for the upper complexity bound of the multilevel Monte Carlo method which depends on two increasing sequences of parameters for both $n$ and $M.$ The complexity is measured in terms of upper bound for mean-squared error and compared with the complexity of the standard Monte Carlo algorithm. The results from numerical experiments as well as Python and CUDA C implementation are also reported.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16640v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micha{\l} Sobieraj</dc:creator>
    </item>
    <item>
      <title>Efficient least squares approximation and collocation methods using radial basis functions</title>
      <link>https://arxiv.org/abs/2308.14490</link>
      <description>arXiv:2308.14490v3 Announce Type: replace 
Abstract: We describe an efficient method for the approximation of functions using radial basis functions (RBFs), and extend this to a solver for boundary value problems on irregular domains. The method is based on RBFs with centers on a regular grid defined on a bounding box, with some of the centers outside the computational domain. The equation is discretized using collocation with oversampling, with collocation points inside the domain only, resulting in a rectangular linear system to be solved in a least squares sense. The goal of this paper is the efficient solution of that rectangular system. We show that the least squares problem splits into a regular part, which can be expedited with the FFT, and a low rank perturbation, which is treated separately with a direct solver. The rank of the perturbation is influenced by the irregular shape of the domain and by the weak enforcement of boundary conditions at points along the boundary. The solver extends the AZ algorithm which was previously proposed for function approximation involving frames and other overcomplete sets. The solver has near optimal log-linear complexity for univariate problems, and loses optimality for higher-dimensional problems but remains faster than a direct solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14490v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiqing Zhou, Daan Huybrechs</dc:creator>
    </item>
    <item>
      <title>Adaptive operator learning for infinite-dimensional Bayesian inverse problems</title>
      <link>https://arxiv.org/abs/2310.17844</link>
      <description>arXiv:2310.17844v2 Announce Type: replace 
Abstract: The fundamental computational issues in Bayesian inverse problems (BIP) governed by partial differential equations (PDEs) stem from the requirement of repeated forward model evaluations. A popular strategy to reduce such costs is to replace expensive model simulations with computationally efficient approximations using operator learning, motivated by recent progress in deep learning. However, using the approximated model directly may introduce a modeling error, exacerbating the already ill-posedness of inverse problems. Thus, balancing between accuracy and efficiency is essential for the effective implementation of such approaches. To this end, we develop an adaptive operator learning framework that can reduce modeling error gradually by forcing the surrogate to be accurate in local areas. This is accomplished by adaptively fine-tuning the pre-trained approximate model with train- ing points chosen by a greedy algorithm during the posterior computational process. To validate our approach, we use DeepOnet to construct the surrogate and unscented Kalman inversion (UKI) to approximate the BIP solution, respectively. Furthermore, we present a rigorous convergence guarantee in the linear case using the UKI framework. The approach is tested on a number of benchmarks, including the Darcy flow, the heat source inversion problem, and the reaction-diffusion problem. The numerical results show that our method can significantly reduce computational costs while maintaining inversion accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17844v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiwei Gao, Liang Yan, Tao Zhou</dc:creator>
    </item>
    <item>
      <title>Homogeneous algorithms and solvable problems on cones</title>
      <link>https://arxiv.org/abs/2311.15767</link>
      <description>arXiv:2311.15767v2 Announce Type: replace 
Abstract: We consider linear problems in the worst case setting. That is, given a linear operator and a pool of admissible linear measurements, we want to approximate the values of the operator uniformly on a convex and balanced set by means of algorithms that use at most $n$ such measurements. It is known that, in general, linear algorithms do not yield an optimal approximation. However, as we show in this paper, an optimal approximation can always be obtained with a homogeneous algorithm. This is of interest to us for two reasons. First, the homogeneity allows us to extend any error bound on the unit ball to the full input space. Second, homogeneous algorithms are better suited to tackle problems on cones, a scenario that is far less understood than the classical situation of balls. We use the optimality of homogeneous algorithms to prove solvability for a family of problems defined on cones. We illustrate our results by several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15767v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Krieg, Peter Kritzer</dc:creator>
    </item>
    <item>
      <title>Pseudo-Differential Neural Operator: Generalized Fourier Neural Operator for Learning Solution Operators of Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2201.11967</link>
      <description>arXiv:2201.11967v3 Announce Type: replace-cross 
Abstract: Learning the mapping between two function spaces has garnered considerable research attention. However, learning the solution operator of partial differential equations (PDEs) remains a challenge in scientific computing. Fourier neural operator (FNO) was recently proposed to learn solution operators, and it achieved an excellent performance. In this study, we propose a novel \textit{pseudo-differential integral operator} (PDIO) to analyze and generalize the Fourier integral operator in FNO. PDIO is inspired by a pseudo-differential operator, which is a generalized differential operator characterized by a certain symbol. We parameterize this symbol using a neural network and demonstrate that the neural network-based symbol is contained in a smooth symbol class. Subsequently, we verify that the PDIO is a bounded linear operator, and thus is continuous in the Sobolev space. We combine the PDIO with the neural operator to develop a \textit{pseudo-differential neural operator} (PDNO) and learn the nonlinear solution operator of PDEs. We experimentally validate the effectiveness of the proposed model by utilizing Darcy flow and the Navier-Stokes equation. The obtained results indicate that the proposed PDNO outperforms the existing neural operator approaches in most experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.11967v3</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jin Young Shin, Jae Yong Lee, Hyung Ju Hwang</dc:creator>
    </item>
    <item>
      <title>The effect of smooth parametrizations on nonconvex optimization landscapes</title>
      <link>https://arxiv.org/abs/2207.03512</link>
      <description>arXiv:2207.03512v5 Announce Type: replace-cross 
Abstract: We develop new tools to study landscapes in nonconvex optimization. Given one optimization problem, we pair it with another by smoothly parametrizing the domain. This is either for practical purposes (e.g., to use smooth optimization algorithms with good guarantees) or for theoretical purposes (e.g., to reveal that the landscape satisfies a strict saddle property). In both cases, the central question is: how do the landscapes of the two problems relate? More precisely: how do desirable points such as local minima and critical points in one problem relate to those in the other problem? A key finding in this paper is that these relations are often determined by the parametrization itself, and are almost entirely independent of the cost function. Accordingly, we introduce a general framework to study parametrizations by their effect on landscapes. The framework enables us to obtain new guarantees for an array of problems, some of which were previously treated on a case-by-case basis in the literature. Applications include: optimizing low-rank matrices and tensors through factorizations; solving semidefinite programs via the Burer-Monteiro approach; training neural networks by optimizing their weights and biases; and quotienting out symmetries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.03512v5</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10107-024-02058-3</arxiv:DOI>
      <arxiv:journal_reference>Math. Program. (2024)</arxiv:journal_reference>
      <dc:creator>Eitan Levin, Joe Kileel, Nicolas Boumal</dc:creator>
    </item>
    <item>
      <title>The Parametric Complexity of Operator Learning</title>
      <link>https://arxiv.org/abs/2306.15924</link>
      <description>arXiv:2306.15924v3 Announce Type: replace-cross 
Abstract: Neural operator architectures employ neural networks to approximate operators mapping between Banach spaces of functions; they may be used to accelerate model evaluations via emulation, or to discover models from data. Consequently, the methodology has received increasing attention over recent years, giving rise to the rapidly growing field of operator learning. The first contribution of this paper is to prove that for general classes of operators which are characterized only by their $C^r$- or Lipschitz-regularity, operator learning suffers from a ``curse of parametric complexity'', which is an infinite-dimensional analogue of the well-known curse of dimensionality encountered in high-dimensional approximation problems. The result is applicable to a wide variety of existing neural operators, including PCA-Net, DeepONet and the FNO. The second contribution of the paper is to prove that this general curse can be overcome for solution operators defined by the Hamilton-Jacobi equation; this is achieved by leveraging additional structure in the underlying solution operator, going beyond regularity. To this end, a novel neural operator architecture is introduced, termed HJ-Net, which explicitly takes into account characteristic information of the underlying Hamiltonian system. Error and complexity estimates are derived for HJ-Net which show that this architecture can provably beat the curse of parametric complexity related to the infinite-dimensional input and output function spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15924v3</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Lanthaler, Andrew M. Stuart</dc:creator>
    </item>
    <item>
      <title>Tackling the Curse of Dimensionality with Physics-Informed Neural Networks</title>
      <link>https://arxiv.org/abs/2307.12306</link>
      <description>arXiv:2307.12306v5 Announce Type: replace-cross 
Abstract: The curse-of-dimensionality taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs, as Richard E. Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. We develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and randomly samples a subset of these dimensional pieces in each iteration of training PINNs. We prove theoretically the convergence and other desired properties of the proposed method. We demonstrate in various diverse tests that the proposed method can solve many notoriously hard high-dimensional PDEs, including the Hamilton-Jacobi-Bellman (HJB) and the Schr\"{o}dinger equations in tens of thousands of dimensions very fast on a single GPU using the PINNs mesh-free approach. Notably, we solve nonlinear PDEs with nontrivial, anisotropic, and inseparable solutions in 100,000 effective dimensions in 12 hours on a single GPU using SDGD with PINNs. Since SDGD is a general training methodology of PINNs, it can be applied to any current and future variants of PINNs to scale them up for arbitrary high-dimensional PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12306v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheyuan Hu, Khemraj Shukla, George Em Karniadakis, Kenji Kawaguchi</dc:creator>
    </item>
    <item>
      <title>Hutchinson Trace Estimation for High-Dimensional and High-Order Physics-Informed Neural Networks</title>
      <link>https://arxiv.org/abs/2312.14499</link>
      <description>arXiv:2312.14499v2 Announce Type: replace-cross 
Abstract: Physics-Informed Neural Networks (PINNs) have proven effective in solving partial differential equations (PDEs), especially when some data are available by seamlessly blending data and physics. However, extending PINNs to high-dimensional and even high-order PDEs encounters significant challenges due to the computational cost associated with automatic differentiation in the residual loss. Herein, we address the limitations of PINNs in handling high-dimensional and high-order PDEs by introducing Hutchinson Trace Estimation (HTE). Starting with the second-order high-dimensional PDEs ubiquitous in scientific computing, HTE transforms the calculation of the entire Hessian matrix into a Hessian vector product (HVP). This approach alleviates the computational bottleneck via Taylor-mode automatic differentiation and significantly reduces memory consumption from the Hessian matrix to HVP. We further showcase HTE's convergence to the original PINN loss and its unbiased behavior under specific conditions. Comparisons with Stochastic Dimension Gradient Descent (SDGD) highlight the distinct advantages of HTE, particularly in scenarios with significant variance among dimensions. We further extend HTE to higher-order and higher-dimensional PDEs, specifically addressing the biharmonic equation. By employing tensor-vector products (TVP), HTE efficiently computes the colossal tensor associated with the fourth-order high-dimensional biharmonic equation, saving memory and enabling rapid computation. The effectiveness of HTE is illustrated through experimental setups, demonstrating comparable convergence rates with SDGD under memory and speed constraints. Additionally, HTE proves valuable in accelerating the Gradient-Enhanced PINN (gPINN) version as well as the Biharmonic equation. Overall, HTE opens up a new capability in scientific machine learning for tackling high-order and high-dimensional PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14499v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cma.2024.116883</arxiv:DOI>
      <arxiv:journal_reference>Computer Methods in Applied Mechanics and Engineering, Volume 424, 1 May 2024, 116883</arxiv:journal_reference>
      <dc:creator>Zheyuan Hu, Zekun Shi, George Em Karniadakis, Kenji Kawaguchi</dc:creator>
    </item>
  </channel>
</rss>
