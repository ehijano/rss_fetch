<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Mar 2025 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Role of Databases in GenAI Applications</title>
      <link>https://arxiv.org/abs/2503.04847</link>
      <description>arXiv:2503.04847v1 Announce Type: new 
Abstract: Generative AI (GenAI) is transforming industries by enabling intelligent content generation, automation, and decision-making. However, the effectiveness of GenAI applications depends significantly on efficient data storage, retrieval, and contextual augmentation. This paper explores the critical role of databases in GenAI workflows, emphasizing the importance of choosing the right database architecture to optimize performance, accuracy, and scalability. It categorizes database roles into conversational context (key-value/document databases), situational context (relational databases/data lakehouses), and semantic context (vector databases) each serving a distinct function in enriching AI-generated responses. Additionally, the paper highlights real-time query processing, vector search for semantic retrieval, and the impact of database selection on model efficiency and scalability. By leveraging a multi-database approach, GenAI applications can achieve more context-aware, personalized, and high-performing AI-driven solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04847v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santosh Bhupathi</dc:creator>
    </item>
    <item>
      <title>Leveraging Approximate Caching for Faster Retrieval-Augmented Generation</title>
      <link>https://arxiv.org/abs/2503.05530</link>
      <description>arXiv:2503.05530v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enhances the reliability of large language model (LLM) answers by integrating external knowledge. However, RAG increases the end-to-end inference time since looking for relevant documents from large vector databases is computationally expensive. To address this, we introduce Proximity, an approximate key-value cache that optimizes the RAG workflow by leveraging similarities in user queries. Instead of treating each query independently, Proximity reuses previously retrieved documents when similar queries appear, reducing reliance on expensive vector database lookups. We evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it significantly improves retrieval efficiency while maintaining response accuracy. Proximity reduces retrieval latency by up to 59% while maintaining accuracy and lowers the computational burden on the vector database. We also experiment with different similarity thresholds and quantify the trade-off between speed and recall. Our work shows that approximate caching is a viable and effective strategy for optimizing RAG-based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05530v1</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3721146.3721941</arxiv:DOI>
      <dc:creator>Shai Bergman, Zhang Ji, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos</dc:creator>
    </item>
    <item>
      <title>Vbox: Efficient Black-Box Serializability Verification</title>
      <link>https://arxiv.org/abs/2503.05163</link>
      <description>arXiv:2503.05163v1 Announce Type: cross 
Abstract: Verifying the serializability of transaction histories is essential for users to know if the DBMS ensures the claimed serializable isolation level without potential bugs. Black-box serializability verification is a promising approach. Existing verification methods often have one or more limitations such as incomplete detection of data anomalies, long verification time, high memory usage, or dependence on specific concurrency control protocols. In this paper, a new black-box serializability verification method called \textsf{Vbox} is proposed. \textsf{Vbox} is powered by a number of new techniques, including the support for predicate database operations, comprehensive applications of transactions' time information in the verification process, and a simplified satisfiability (SAT) problem formulation and its efficient solver. In this paper, \textsf{Vbox} is verified to be correct, efficient, and capable of detecting more data anomalies, while not relying on any specific concurrency control protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05163v1</guid>
      <category>cs.PL</category>
      <category>cs.DB</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weihua Sun, Zhaonian Zou</dc:creator>
    </item>
    <item>
      <title>Femur: A Flexible Framework for Fast and Secure Querying from Public Key-Value Store</title>
      <link>https://arxiv.org/abs/2503.05376</link>
      <description>arXiv:2503.05376v1 Announce Type: cross 
Abstract: With increasing demands for privacy, it becomes necessary to protect sensitive user query data when accessing public key-value databases. Existing Private Information Retrieval (PIR) schemes provide full security but suffer from poor scalability, limiting their applicability in large-scale deployment. We argue that in many real-world scenarios, a more practical solution should allow users to flexibly determine the privacy levels of their queries in a theoretically guided way, balancing security and performance based on specific needs. To formally provide provable guarantees, we introduce a novel concept of distance-based indistinguishability, which can facilitate users to comfortably relax their security requirements. We then design Femur, an efficient framework to securely query public key-value stores with flexible security and performance trade-offs. It uses a space-efficient learned index to convert query keys into storage locations, obfuscates these locations with extra noise provably derived by the distance-based indistinguishability theory, and sends the expanded range to the server. The server then adaptively utilizes the best scheme to retrieve data. We also propose a novel variable-range PIR scheme optimized for bandwidth-constrained environments. Experiments show that Femur outperforms the state-of-the-art designs even when ensuring the same full security level. When users are willing to relax their privacy requirements, Femur can further improve the performance gains to up to 163.9X, demonstrating an effective trade-off between security and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05376v1</guid>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaoyi Zhang, Liqiang Peng, Mo Sha, Weiran Liu, Xiang Li, Sheng Wang, Feifei Li, Mingyu Gao, Huanchen Zhang</dc:creator>
    </item>
    <item>
      <title>Are Your LLM-based Text-to-SQL Models Secure? Exploring SQL Injection via Backdoor Attacks</title>
      <link>https://arxiv.org/abs/2503.05445</link>
      <description>arXiv:2503.05445v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown state-of-the-art results in translating natural language questions into SQL queries (Text-to-SQL), a long-standing challenge within the database community. However, security concerns remain largely unexplored, particularly the threat of backdoor attacks, which can introduce malicious behaviors into models through fine-tuning with poisoned datasets. In this work, we systematically investigate the vulnerabilities of LLM-based Text-to-SQL models and present ToxicSQL, a novel backdoor attack framework. Our approach leverages stealthy {semantic and character-level triggers} to make backdoors difficult to detect and remove, ensuring that malicious behaviors remain covert while maintaining high model accuracy on benign inputs. Furthermore, we propose leveraging SQL injection payloads as backdoor targets, enabling the generation of malicious yet executable SQL queries, which pose severe security and privacy risks in language model-based SQL development. We demonstrate that injecting only 0.44% of poisoned data can result in an attack success rate of 79.41%, posing a significant risk to database security. Additionally, we propose detection and mitigation strategies to enhance model reliability. Our findings highlight the urgent need for security-aware Text-to-SQL development, emphasizing the importance of robust defenses against backdoor threats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05445v1</guid>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Meiyu Lin, Haichuan Zhang, Jiale Lao, Renyuan Li, Yuanchun Zhou, Carl Yang, Yang Cao, Mingjie Tang</dc:creator>
    </item>
    <item>
      <title>Algorithmic Data Minimization for Machine Learning over Internet-of-Things Data Streams</title>
      <link>https://arxiv.org/abs/2503.05675</link>
      <description>arXiv:2503.05675v1 Announce Type: cross 
Abstract: Machine learning can analyze vast amounts of data generated by IoT devices to identify patterns, make predictions, and enable real-time decision-making. By processing sensor data, machine learning models can optimize processes, improve efficiency, and enhance personalized user experiences in smart systems. However, IoT systems are often deployed in sensitive environments such as households and offices, where they may inadvertently expose identifiable information, including location, habits, and personal identifiers. This raises significant privacy concerns, necessitating the application of data minimization -- a foundational principle in emerging data regulations, which mandates that service providers only collect data that is directly relevant and necessary for a specified purpose. Despite its importance, data minimization lacks a precise technical definition in the context of sensor data, where collections of weak signals make it challenging to apply a binary "relevant and necessary" rule. This paper provides a technical interpretation of data minimization in the context of sensor streams, explores practical methods for implementation, and addresses the challenges involved. Through our approach, we demonstrate that our framework can reduce user identifiability by up to 16.7% while maintaining accuracy loss below 1%, offering a viable path toward privacy-preserving IoT data processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05675v1</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ted Shaowang, Shinan Liu, Jonatas Marques, Nick Feamster, Sanjay Krishnan</dc:creator>
    </item>
    <item>
      <title>ECCOS: Efficient Capability and Cost Coordinated Scheduling for Multi-LLM Serving</title>
      <link>https://arxiv.org/abs/2502.20576</link>
      <description>arXiv:2502.20576v2 Announce Type: replace 
Abstract: As large language models (LLMs) are increasingly deployed as service endpoints in systems, the surge in query volume creates significant scheduling challenges. Existing scheduling frameworks mainly target at latency optimization while neglecting the capability of LLMs to serve different level of queries, which could lead to computational resource waste. This paper addresses this challenge by proposing a capability-cost coordinated scheduling framework, ECCOS, for multi-LLM serving, which explicitly constrains response quality and workload to optimize LLM inference cost. Specifically, it introduces the two-stage scheduling by designing a multi-objective predictor and a constrained optimizer. The predictor estimates both model capabilities and computational costs through training-based and retrieval-based approaches, while the optimizer determines cost-optimal assignments under quality and workload constraints. It also introduces QAServe, a dataset collected for sample-wise response quality and costs by zero-shot prompting different LLMs on knowledge QA and mathematical reasoning. Extensive experiments demonstrate that ECCOS improves success rates by 6.30% while reducing costs by 10.15% compared to existing methods, consuming less than 0.5% of LLM response time. The code is available at: https://github.com/agiresearch/ECCOS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20576v2</guid>
      <category>cs.DB</category>
      <category>cs.CL</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Mei, Wujiang Xu, Shuhang Lin, Yongfeng Zhang</dc:creator>
    </item>
    <item>
      <title>Decidability of Querying First-Order Theories via Countermodels of Finite Width</title>
      <link>https://arxiv.org/abs/2304.06348</link>
      <description>arXiv:2304.06348v4 Announce Type: replace-cross 
Abstract: We propose a generic framework for establishing the decidability of a wide range of logical entailment problems (briefly called querying), based on the existence of countermodels that are structurally simple, gauged by certain types of width measures (with treewidth and cliquewidth as popular examples). As an important special case of our framework, we identify logics exhibiting width-finite finitely universal model sets, warranting decidable entailment for a wide range of homomorphism-closed queries, subsuming a diverse set of practically relevant query languages. As a particularly powerful width measure, we propose to employ Blumensath's partitionwidth, which subsumes various other commonly considered width measures and exhibits highly favorable computational and structural properties. Focusing on the formalism of existential rules as a popular showcase, we explain how finite partitionwidth sets of rules subsume other known abstract decidable classes but - leveraging existing notions of stratification - also cover a wide range of new rulesets. We expose natural limitations for fitting the class of finite unification sets into our picture and suggest several options for remedy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.06348v4</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.DM</category>
      <category>math.LO</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thomas Feller, Tim S. Lyon, Piotr Ostropolski-Nalewaja, Sebastian Rudolph</dc:creator>
    </item>
  </channel>
</rss>
