<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.OT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.OT</link>
    <description>stat.OT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.OT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Jul 2024 01:46:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Future of Data Science Education</title>
      <link>https://arxiv.org/abs/2407.11824</link>
      <description>arXiv:2407.11824v1 Announce Type: new 
Abstract: The definition of Data Science is a hotly debated topic. For many, the definition is a simple shortcut to Artificial Intelligence or Machine Learning. However, there is far more depth and nuance to the field of Data Science than a simple shortcut can provide. The School of Data Science at the University of Virginia has developed a novel model for the definition of Data Science. This model is based on identifying a unified understanding of the data work done across all areas of Data Science. It represents a generational leap forward in how we understand and teach Data Science. In this paper we will present the core features of the model and explain how it unifies various concepts going far beyond the analytics component of AI. From this foundation we will present our Undergraduate Major curriculum in Data Science and demonstrate how it prepares students to be well-rounded Data Science team members and leaders. The paper will conclude with an in-depth overview of the Foundations of Data Science course designed to introduce students to the field while also implementing proven STEM oriented pedagogical methods. These include, for example, specifications grading, active learning lectures, guest lectures from industry experts and weekly gamification labs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11824v1</guid>
      <category>stat.OT</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Wright, Peter Alonzi, Ali Riveria</dc:creator>
    </item>
    <item>
      <title>A concise proof of Benford's law</title>
      <link>https://arxiv.org/abs/2407.11076</link>
      <description>arXiv:2407.11076v1 Announce Type: cross 
Abstract: This article presents a concise proof of the famous Benford's law when the distribution has a Riemann integrable probability density function and provides a criterion to judge whether a distribution obeys the law. The proof is intuitive and elegant, accessible to anyone with basic knowledge of calculus, revealing that the law originates from the basic property of the human number system. The criterion can bring great convenience to the field of fraud detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11076v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.fmre.2023.01.002</arxiv:DOI>
      <arxiv:journal_reference>Fundamental Research 4 (2024) 842-845</arxiv:journal_reference>
      <dc:creator>Luohan Wang, Bo-Qiang Ma</dc:creator>
    </item>
    <item>
      <title>Ensemble Transport Filter via Optimized Maximum Mean Discrepancy</title>
      <link>https://arxiv.org/abs/2407.11518</link>
      <description>arXiv:2407.11518v1 Announce Type: cross 
Abstract: In this paper, we present a new ensemble-based filter method by reconstructing the analysis step of the particle filter through a transport map, which directly transports prior particles to posterior particles. The transport map is constructed through an optimization problem described by the Maximum Mean Discrepancy loss function, which matches the expectation information of the approximated posterior and reference posterior. The proposed method inherits the accurate estimation of the posterior distribution from particle filtering. To improve the robustness of Maximum Mean Discrepancy, a variance penalty term is used to guide the optimization. It prioritizes minimizing the discrepancy between the expectations of highly informative statistics for the approximated and reference posteriors. The penalty term significantly enhances the robustness of the proposed method and leads to a better approximation of the posterior. A few numerical examples are presented to illustrate the advantage of the proposed method over the ensemble Kalman filter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11518v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.OT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dengfei Zeng, Lijian Jiang</dc:creator>
    </item>
  </channel>
</rss>
