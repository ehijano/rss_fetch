<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Apr 2024 04:01:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Variance-informed Rounding Uncertainty Analysis for Floating-point Statistical Models</title>
      <link>https://arxiv.org/abs/2404.12556</link>
      <description>arXiv:2404.12556v1 Announce Type: new 
Abstract: Advancements in computer hardware have made it possible to utilize low- and mixed-precision arithmetic for enhanced computational efficiency. In practical predictive modeling, however, it is vital to quantify uncertainty due to rounding along other sources like measurement, sampling, and numerical discretization. Traditional deterministic rounding uncertainty analysis (DBEA) assumes that the rounding errors equal the unit roundoff $u$. However, despite providing strong guarantees, DBEA severely overestimates rounding uncertainty. This work presents a novel probabilistic rounding uncertainty analysis called VIBEA. By treating rounding errors as i.i.d. random variables and leveraging concentration inequalities, VIBEA provides high-confidence estimates for rounding uncertainty using higher-order rounding error statistics. The presented framework is valid for all problem sizes $n$, unlike DBEA, which necessitates $nu&lt;1$. Further, it can account for the potential cancellation of rounding errors, resulting in rounding uncertainty estimates that grow slowly with $n$. We show that for $n&gt;n_c(u)$, VIBEA produces tighter estimates for rounding uncertainty than DBEA. We also show that VIBEA improves existing probabilistic rounding uncertainty analysis techniques for $n\ge3$ by using higher-order rounding error statistics. We conduct numerical experiments on random vector dot products, a linear system solution, and a stochastic boundary value problem. We show that quantifying rounding uncertainty along with traditional sources (numerical discretization, sampling, parameters) enables a more efficient allocation of computational resources, thereby balancing computational efficiency with predictive accuracy. This study is a step towards a comprehensive mixed-precision approach that improves model reliability and enables budgeting of computational resources in predictive modeling and decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12556v1</guid>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahil Bhola, Karthik Duraisamy</dc:creator>
    </item>
    <item>
      <title>Bootstrap confidence intervals: A comparative simulation study</title>
      <link>https://arxiv.org/abs/2404.12967</link>
      <description>arXiv:2404.12967v1 Announce Type: new 
Abstract: Bootstrap is a widely used technique that allows estimating the properties of a given estimator, such as its bias and standard error. In this paper, we evaluate and compare five bootstrap-based methods for making confidence intervals: two of them (Normal and Studentized) based on the bootstrap estimate of the standard error; another two (Quantile and Better) based on the estimated distribution of the parameter estimator; and finally, considering an interval constructed based on Bayesian bootstrap, relying on the notion of credible interval. The methods are compared through Monte Carlo simulations in different scenarios, including samples with autocorrelation induced by a copula model. The results are also compared with respect to the coverage rate, the median interval length and a novel indicator, proposed in this paper, combining both of them. The results show that the Studentized method has the best coverage rate, although the smallest intervals are attained by the Bayesian method. In general, all methods are appropriate and demonstrated good performance even in the scenarios violating the independence assumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12967v1</guid>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vin\'icius Litvinoff Justus, Vitor Batista Rodrigues, Alex Rodrigo dos Santos Sousa</dc:creator>
    </item>
    <item>
      <title>Neural Methods for Amortised Parameter Inference</title>
      <link>https://arxiv.org/abs/2404.12484</link>
      <description>arXiv:2404.12484v1 Announce Type: cross 
Abstract: Simulation-based methods for making statistical inference have evolved dramatically over the past 50 years, keeping pace with technological advancements. The field is undergoing a new revolution as it embraces the representational capacity of neural networks, optimisation libraries, and graphics processing units for learning complex mappings between data and inferential targets. The resulting tools are amortised, in the sense that they allow inference to be made quickly through fast feedforward operations. In this article we review recent progress made in the context of point estimation, approximate Bayesian inference, the automatic construction of summary statistics, and likelihood approximation. The review also covers available software, and includes a simple illustration to showcase the wide array of tools available for amortised inference and the benefits they offer over state-of-the-art Markov chain Monte Carlo methods. The article concludes with an overview of relevant topics and an outlook on future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12484v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Zammit-Mangion, Matthew Sainsbury-Dale, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>A rate-distortion framework for MCMC algorithms: geometry and factorization of multivariate Markov chains</title>
      <link>https://arxiv.org/abs/2404.12589</link>
      <description>arXiv:2404.12589v1 Announce Type: cross 
Abstract: We introduce a framework rooted in a rate distortion problem for Markov chains, and show how a suite of commonly used Markov Chain Monte Carlo (MCMC) algorithms are specific instances within it, where the target stationary distribution is controlled by the distortion function. Our approach offers a unified variational view on the optimality of algorithms such as Metropolis-Hastings, Glauber dynamics, the swapping algorithm and Feynman-Kac path models. Along the way, we analyze factorizability and geometry of multivariate Markov chains. Specifically, we demonstrate that induced chains on factors of a product space can be regarded as information projections with respect to a particular divergence. This perspective yields Han--Shearer type inequalities for Markov chains as well as applications in the context of large deviations and mixing time comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12589v1</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael C. H. Choi, Youjia Wang, Geoffrey Wolfer</dc:creator>
    </item>
    <item>
      <title>Why not a thin plate spline for spatial models? A comparative study using Bayesian inference</title>
      <link>https://arxiv.org/abs/2404.12756</link>
      <description>arXiv:2404.12756v1 Announce Type: cross 
Abstract: Spatial modelling often uses Gaussian random fields to capture the stochastic nature of studied phenomena. However, this approach incurs significant computational burdens (O(n3)), primarily due to covariance matrix computations. In this study, we propose to use a low-rank approximation of a thin plate spline as a spatial random effect in Bayesian spatial models. We compare its statistical performance and computational efficiency with the approximated Gaussian random field (by the SPDE method). In this case, the dense matrix of the thin plate spline is approximated using a truncated spectral decomposition, resulting in computational complexity of O(kn2) operations, where k is the number of knots. Bayesian inference is conducted via the Hamiltonian Monte Carlo algorithm of the probabilistic software Stan, which allows us to evaluate performance and diagnostics for the proposed models. A simulation study reveals that both models accurately recover the parameters used to simulate data. However, models using a thin plate spline demonstrate superior execution time to achieve the convergence of chains compared to the models utilizing an approximated Gaussian random field. Furthermore, thin plate spline models exhibited better computational efficiency for simulated data coming from different spatial locations. In a real application, models using a thin plate spline as spatial random effect produced similar results in estimating a relative index of abundance for a benthic marine species when compared to models incorporating an approximated Gaussian random field. Although they were not the more computational efficient models, their simplicity in parametrization, execution time and predictive performance make them a valid alternative for spatial modelling under Bayesian inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12756v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joaquin Cavieres, Paula Moraga, Cole C. Monnahan</dc:creator>
    </item>
    <item>
      <title>Geodesic slice sampling on Riemannian manifolds</title>
      <link>https://arxiv.org/abs/2312.00417</link>
      <description>arXiv:2312.00417v2 Announce Type: replace 
Abstract: We propose a theoretically justified and practically applicable slice sampling based Markov chain Monte Carlo (MCMC) method for approximate sampling from probability measures on Riemannian manifolds. The latter naturally arise as posterior distributions in Bayesian inference of matrix-valued parameters, for example belonging to either the Stiefel or the Grassmann manifold. Our method, called geodesic slice sampling, is reversible with respect to the distribution of interest, and generalizes Hit-and-run slice sampling on $\mathbb{R}^{d}$ to Riemannian manifolds by using geodesics instead of straight lines. We demonstrate the robustness of our sampler's performance compared to other MCMC methods dealing with manifold valued distributions through extensive numerical experiments, on both synthetic and real data. In particular, we illustrate its remarkable ability to cope with anisotropic target densities, without using gradient information and preconditioning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00417v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain Durmus, Samuel Gruffaz, Mareike Hasenpflug, Daniel Rudolf</dc:creator>
    </item>
    <item>
      <title>A tutorial on spatiotemporal partially observed Markov process models via the R package spatPomp</title>
      <link>https://arxiv.org/abs/2101.01157</link>
      <description>arXiv:2101.01157v4 Announce Type: replace-cross 
Abstract: We describe a computational framework for modeling and statistical inference on high-dimensional stochastic dynamic systems. Our primary motivation is the investigation of metapopulation dynamics arising from a collection of spatially distributed, interacting biological populations. To make progress on this goal, we embed it in a more general problem: inference for a collection of interacting partially observed nonlinear non-Gaussian stochastic processes. Each process in the collection is called a unit; in the case of spatiotemporal models, the units correspond to distinct spatial locations. The dynamic state for each unit may be discrete or continuous, scalar or vector valued. In metapopulation applications, the state can represent a structured population or the abundances of a collection of species at a single location. We consider models where the collection of states has a Markov property. A sequence of noisy measurements is made on each unit, resulting in a collection of time series. A model of this form is called a spatiotemporal partially observed Markov process (SpatPOMP). The R package spatPomp provides an environment for implementing SpatPOMP models, analyzing data using existing methods, and developing new inference approaches. Our presentation of spatPomp reviews various methodologies in a unifying notational framework. We demonstrate the package on a simple Gaussian system and on a nontrivial epidemiological model for measles transmission within and between cities. We show how to construct user-specified SpatPOMP models within spatPomp.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.01157v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kidus Asfaw, Joonha Park, Aaron A. King, Edward L. Ionides</dc:creator>
    </item>
  </channel>
</rss>
