<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Feb 2025 02:48:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Trend estimation for time series with polynomial-tailed noise</title>
      <link>https://arxiv.org/abs/2502.08280</link>
      <description>arXiv:2502.08280v1 Announce Type: new 
Abstract: For time series data observed at non-random and possibly non-equidistant time points, we estimate the trend function nonparametrically. Under the assumption of a bounded total variation of the function and low-order moment conditions on the errors we propose a nonlinear wavelet estimator which uses a Haar-type basis adapted to a possibly non-dyadic sample size. An appropriate thresholding scheme for sparse signals with an additive polynomial-tailed noise is first derived in an abstract framework and then applied to the problem of trend estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08280v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael H. Neumann, Anne Leucht</dc:creator>
    </item>
    <item>
      <title>A comparison of Dirichlet kernel regression methods on the simplex</title>
      <link>https://arxiv.org/abs/2502.08461</link>
      <description>arXiv:2502.08461v1 Announce Type: new 
Abstract: An asymmetric Dirichlet kernel version of the Gasser-M\"uller estimator is introduced for regression surfaces on the simplex, extending the univariate analog proposed by Chen [Statist. Sinica, 10(1) (2000), pp. 73-91]. Its asymptotic properties are investigated under the condition that the design points are known and fixed, including an analysis of its mean integrated squared error (MISE) and its asymptotic normality. The estimator is also applicable in a random design setting. A simulation study compares its performance with two recently proposed alternatives: the Nadaraya--Watson estimator with Dirichlet kernel and the local linear smoother with Dirichlet kernel. The results show that the local linear smoother consistently outperforms the others. To illustrate its applicability, the local linear smoother is applied to the GEMAS dataset to analyze the relationship between soil composition and pH levels across various agricultural and grazing lands in Europe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08461v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanen Daayeb, Christian Genest, Salah Khardani, Nicolas Klutchnikoff, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Moment Estimator-Based Extreme Quantile Estimation with Erroneous Observations: Application to Elliptical Extreme Quantile Region Estimation</title>
      <link>https://arxiv.org/abs/2502.08510</link>
      <description>arXiv:2502.08510v1 Announce Type: new 
Abstract: In many application areas of extreme value theory, the variables of interest are not directly observable but instead contain errors. In this article, we quantify the effect of these errors in moment-based extreme value index estimation, and in corresponding extreme quantile estimation. We consider all, short-, light-, and heavy-tailed distributions. In particular, we derive conditions under which the error is asymptotically negligible. As an application, we consider affine equivariant extreme quantile region estimation under multivariate elliptical distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08510v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaakko Pere, Pauliina Ilmonen, Lauri Viitasaari</dc:creator>
    </item>
    <item>
      <title>Network Goodness-of-Fit for the block-model family</title>
      <link>https://arxiv.org/abs/2502.08609</link>
      <description>arXiv:2502.08609v1 Announce Type: new 
Abstract: The block-model family has four popular network models (SBM, DCBM, MMSBM, and DCMM). A fundamental problem is, how well each of these models fits with real networks. We propose GoF-MSCORE as a new Goodness-of-Fit (GoF) metric for DCMM (the broadest one among the four), with two main ideas. The first is to use cycle count statistics as a general recipe for GoF. The second is a novel network fitting scheme. GoF-MSCORE is a flexible GoF approach, and we further extend it to SBM, DCBM, and MMSBM. This gives rise to a series of GoF metrics covering each of the four models in the block-model family.
  We show that for each of the four models, if the assumed model is correct, then the corresponding GoF metric converges to $N(0, 1)$ as the network sizes diverge. We also analyze the powers and show that these metrics are optimal in many settings. In comparison, many other GoF ideas face challenges: they may lack a parameter-free limiting null, or are non-optimal in power, or face an analytical hurdle. Note that a parameter-free limiting null is especially desirable as many network models have a large number of unknown parameters. The limiting nulls of our GoF metrics are always $N(0,1)$, which are parameter-free as desired.
  For 12 frequently-used real networks, we use the proposed GoF metrics to show that DCMM fits well with almost all of them. We also show that SBM, DCBM, and MMSBM do not fit well with many of these networks, especially when the networks are relatively large. To complement with our study on GoF, we also show that the DCMM is nearly as broad as the rank-$K$ network model. Based on these results, we recommend the DCMM as a promising model for undirected networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08609v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiashun Jin, Zheng Tracy Ke, Jiajun Tang, Jingming Wang</dc:creator>
    </item>
    <item>
      <title>Model-free Methods for Event History Analysis and Efficient Adjustment (PhD Thesis)</title>
      <link>https://arxiv.org/abs/2502.07906</link>
      <description>arXiv:2502.07906v1 Announce Type: cross 
Abstract: This thesis contains a series of independent contributions to statistics, unified by a model-free perspective. The first chapter elaborates on how a model-free perspective can be used to formulate flexible methods that leverage prediction techniques from machine learning. Mathematical insights are obtained from concrete examples, and these insights are generalized to principles that permeate the rest of the thesis. The second chapter studies the concept of local independence, which describes whether the evolution of one stochastic process is directly influenced by another. To test local independence, we define a model-free parameter called the Local Covariance Measure (LCM). We formulate an estimator for the LCM, from which a test of local independence is proposed. We discuss how the size and power of the proposed test can be controlled uniformly and investigate the test in a simulation study. The third chapter focuses on covariate adjustment, a method used to estimate the effect of a treatment by accounting for observed confounding. We formulate a general framework that facilitates adjustment for any subset of covariate information. We identify the optimal covariate information for adjustment and, based on this, introduce the Debiased Outcome-adapted Propensity Estimator (DOPE) for efficient estimation of treatment effects. An instance of DOPE is implemented using neural networks, and we demonstrate its performance on simulated and real data. The fourth and final chapter introduces a model-free measure of the conditional association between an exposure and a time-to-event, which we call the Aalen Covariance Measure (ACM). We develop a model-free estimation method and show that it is doubly robust, ensuring $\sqrt{n}$-consistency provided that the nuisance functions can be estimated with modest rates. A simulation study demonstrates the use of our estimator in several settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07906v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Mangulad Christgau</dc:creator>
    </item>
    <item>
      <title>The nature of mathematical models</title>
      <link>https://arxiv.org/abs/2502.07948</link>
      <description>arXiv:2502.07948v1 Announce Type: cross 
Abstract: Modeling has become a widespread, useful tool in mathematics applied to diverse fields, from physics to economics to biomedicine. Practitioners of modeling may use algebraic or differential equations, to the elements of which they attribute an intuitive relationship with some relevant aspect of reality they wish to represent. More sophisticated expressions may include stochasticity, either as observation error or system noise. However, a clear, unambiguous mathematical definition of what a model is and of what is the relationship between the model and the real-life phenomena it purports to represent has so far not been formulated. The present work aims to fill this gap, motivating the definition of a mathematical model as an operator on a Hilbert space of random variables, identifying the experimental realization as the map between the theoretical space of model construction and the computational space of statistical model identification, and tracing the relationship of the geometry of the model manifold in the abstract setting with the corresponding geometry of the prediction surfaces in statistical estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07948v1</guid>
      <category>stat.OT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andrea De Gaetano</dc:creator>
    </item>
    <item>
      <title>Abstract questionnaires and FS-decision digraphs</title>
      <link>https://arxiv.org/abs/2502.08522</link>
      <description>arXiv:2502.08522v1 Announce Type: cross 
Abstract: A questionnaire is a sequence of multiple choice questions aiming to collect data on a population. We define an abstract questionnaire as an ordered pair $(N,{\cal M})$, where $N$ is a positive integer and ${\cal M}=(m_0,m_1,\ldots,m_{N-1})$ is an $N$-tuple of positive integers, with $m_i$, for $i \in \{0, 1, \ldots, N-1 \}$, as the number of possible answers to question $i$. An abstract questionnaire may be endowed with a skip-list (which tells us which questions to skip based on the sequence of answers to the earlier questions) and a flag-set (which tells us which sequences of answers are of special interest). An FS-decision tree is a decision tree of an abstract questionnaire that also incorporates the information contained in the skip-list and flag-set. The main objective of this paper is to represent the abstract questionnaire using a directed graph, which we call an FS-decision digraph, that contains the full information of an FS-decision tree, but is in general much more concise. We present an algorithm for constructing a fully reduced FS-decision digraph, and develop the theory that supports it. In addition, we show how to generate all possible orderings of the questions in an abstract questionnaire that respect a given precedence relation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08522v1</guid>
      <category>math.CO</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiaye Chen, Suzan Kadri, Mateja \v{S}ajna, Ioana \c{S}chiopu-Kratina</dc:creator>
    </item>
    <item>
      <title>Anytime-valid FDR control with the stopped e-BH procedure</title>
      <link>https://arxiv.org/abs/2502.08539</link>
      <description>arXiv:2502.08539v1 Announce Type: cross 
Abstract: The recent e-Benjamini-Hochberg (e-BH) procedure for multiple hypothesis testing is known to control the false discovery rate (FDR) under arbitrary dependence between the input e-values. This paper points out an important subtlety when applying the e-BH procedure with e-processes, which are sequential generalizations of e-values (where the data are observed sequentially). Since adaptively stopped e-processes are e-values, the e-BH procedure can be repeatedly applied at every time step, and one can continuously monitor the e-processes and the rejection sets obtained. One would hope that the "stopped e-BH procedure" (se-BH) has an FDR guarantee for the rejection set obtained at any stopping time. However, while this is true if the data in different streams are independent, it is not true in full generality, because each stopped e-process is an e-value only for stopping times in its own local filtration, but the se-BH procedure employs a stopping time with respect to a global filtration. This can cause information to leak across time, allowing one stream to know its future by knowing past data of another stream. This paper formulates a simple causal condition under which local e-processes are also global e-processes and thus the se-BH procedure does indeed control the FDR. The condition excludes unobserved confounding from the past and is met under most reasonable scenarios including genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08539v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Robustly Learning Monotone Generalized Linear Models via Data Augmentation</title>
      <link>https://arxiv.org/abs/2502.08611</link>
      <description>arXiv:2502.08611v1 Announce Type: cross 
Abstract: We study the task of learning Generalized Linear models (GLMs) in the agnostic model under the Gaussian distribution. We give the first polynomial-time algorithm that achieves a constant-factor approximation for \textit{any} monotone Lipschitz activation. Prior constant-factor GLM learners succeed for a substantially smaller class of activations. Our work resolves a well-known open problem, by developing a robust counterpart to the classical GLMtron algorithm (Kakade et al., 2011). Our robust learner applies more generally, encompassing all monotone activations with bounded $(2+\zeta)$-moments, for any fixed $\zeta&gt;0$ -- a condition that is essentially necessary. To obtain our results, we leverage a novel data augmentation technique with decreasing Gaussian noise injection and prove a number of structural results that may be useful in other settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08611v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikos Zarifis, Puqian Wang, Ilias Diakonikolas, Jelena Diakonikolas</dc:creator>
    </item>
    <item>
      <title>Fast Convergence of $\Phi$-Divergence Along the Unadjusted Langevin Algorithm and Proximal Sampler</title>
      <link>https://arxiv.org/abs/2410.10699</link>
      <description>arXiv:2410.10699v2 Announce Type: replace 
Abstract: We study the mixing time of two popular discrete-time Markov chains in continuous space, the Unadjusted Langevin Algorithm and the Proximal Sampler, which are discretizations of the Langevin dynamics. We extend mixing time analyses for these Markov chains to hold in $\Phi$-divergence. We show that any $\Phi$-divergence arising from a twice-differentiable strictly convex function $\Phi$ converges to $0$ exponentially fast along these Markov chains, under the assumption that their stationary distributions satisfy the corresponding $\Phi$-Sobolev inequality, which holds for example when the target distribution of the Langevin dynamics is strongly log-concave. Our setting includes as special cases popular mixing time regimes, namely the mixing in chi-squared divergence under a Poincar\'e inequality, and the mixing in relative entropy under a log-Sobolev inequality. Our results follow by viewing the sampling algorithms as noisy channels and bounding the contraction coefficients arising in the appropriate strong data processing inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10699v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddharth Mitra, Andre Wibisono</dc:creator>
    </item>
    <item>
      <title>Direct sampling from conditional distributions by sequential maximum likelihood estimations</title>
      <link>https://arxiv.org/abs/2502.00812</link>
      <description>arXiv:2502.00812v2 Announce Type: replace 
Abstract: We can directly sample from the conditional distribution of any log-affine model. The algorithm is a Markov chain on a bounded integer lattice, and its transition probability is the ratio of the UMVUE (uniformly minimum variance unbiased estimator) of the expected counts to the total number of counts. The computation of the UMVUE accounts for most of the computational cost, which makes the implementation challenging. Here, we investigated an approximate algorithm that replaces the UMVUE with the MLE (maximum likelihood estimator). Although it is generally not exact, it is efficient and easy to implement; no prior study is required, such as about the connection matrices of the holonomic ideal in the original algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00812v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuhei Mano</dc:creator>
    </item>
    <item>
      <title>An Extension of the Unified Skew-Normal Family of Distributions and Application to Bayesian Binary Regression</title>
      <link>https://arxiv.org/abs/2209.03474</link>
      <description>arXiv:2209.03474v4 Announce Type: replace-cross 
Abstract: We consider the Bayesian binary regression model and we introduce a new class of distributions, the Perturbed Unified Skew-Normal (pSUN, henceforth), which generalizes the Unified Skew-Normal (SUN) class. We show that the new class is conjugate to any binary regression model, provided that the link function may be expressed as a scale mixture of Gaussian CDFs. We discuss in detail the popular logit case, and we show that, when a logistic regression model is combined with a Gaussian prior, posterior summaries such as cumulants and normalizing constants can easily be obtained through the use of an importance sampling approach, opening the way to straightforward variable selection procedures. For more general prior distributions, the proposed methodology is based on a simple Gibbs sampler algorithm. We also claim that, in the p&gt;n case, our proposal presents better performances - both in terms of mixing and accuracy - compared to the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.03474v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paolo Onorati, Brunero Liseo</dc:creator>
    </item>
    <item>
      <title>Variational Causal Inference</title>
      <link>https://arxiv.org/abs/2209.05935</link>
      <description>arXiv:2209.05935v4 Announce Type: replace-cross 
Abstract: Estimating an individual's potential outcomes under counterfactual treatments is a challenging task for traditional causal inference and supervised learning approaches when the outcome is high-dimensional (e.g. gene expressions, impulse responses, human faces) and covariates are relatively limited. In this case, to construct one's outcome under a counterfactual treatment, it is crucial to leverage individual information contained in its observed factual outcome on top of the covariates. We propose a deep variational Bayesian framework that rigorously integrates two main sources of information for outcome construction under a counterfactual treatment: one source is the individual features embedded in the high-dimensional factual outcome; the other source is the response distribution of similar subjects (subjects with the same covariates) that factually received this treatment of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.05935v4</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>q-bio.GN</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulun Wu, Layne C. Price, Zichen Wang, Vassilis N. Ioannidis, Robert A. Barton, George Karypis</dc:creator>
    </item>
    <item>
      <title>Counterfactual Generative Modeling with Variational Causal Inference</title>
      <link>https://arxiv.org/abs/2410.12730</link>
      <description>arXiv:2410.12730v2 Announce Type: replace-cross 
Abstract: Estimating an individual's counterfactual outcomes under interventions is a challenging task for traditional causal inference and supervised learning approaches when the outcome is high-dimensional (e.g. gene expressions, facial images) and covariates are relatively limited. In this case, to predict one's outcomes under counterfactual treatments, it is crucial to leverage individual information contained in the observed outcome in addition to the covariates. Prior works using variational inference in counterfactual generative modeling have been focusing on neural adaptations and model variants within the conditional variational autoencoder formulation, which we argue is fundamentally ill-suited to the notion of counterfactual in causal inference. In this work, we present a novel variational Bayesian causal inference framework and its theoretical backings to properly handle counterfactual generative modeling tasks, through which we are able to conduct counterfactual supervision end-to-end during training without any counterfactual samples, and encourage disentangled exogenous noise abduction that aids the correct identification of causal effect in counterfactual generations. In experiments, we demonstrate the advantage of our framework compared to state-of-the-art models in counterfactual generative modeling on multiple benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12730v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulun Wu, Louie McConnell, Claudia Iriondo</dc:creator>
    </item>
    <item>
      <title>Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices</title>
      <link>https://arxiv.org/abs/2502.01512</link>
      <description>arXiv:2502.01512v2 Announce Type: replace-cross 
Abstract: Circular and non-flat data distributions are prevalent across diverse domains of data science, yet their specific geometric structures often remain underutilized in machine learning frameworks. A principled approach to accounting for the underlying geometry of such data is pivotal, particularly when extending statistical models, like the pervasive Gaussian distribution. In this work, we tackle those issue by focusing on the manifold of symmetric positive definite matrices, a key focus in information geometry. We introduced a non-isotropic wrapped Gaussian by leveraging the exponential map, we derive theoretical properties of this distribution and propose a maximum likelihood framework for parameter estimation. Furthermore, we reinterpret established classifiers on SPD through a probabilistic lens and introduce new classifiers based on the wrapped Gaussian model. Experiments on synthetic and real-world datasets demonstrate the robustness and flexibility of this geometry-aware distribution, underscoring its potential to advance manifold-based data analysis. This work lays the groundwork for extending classical machine learning and statistical methods to more complex and structured data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01512v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thibault de Surrel, Fabien Lotte, Sylvain Chevallier, Florian Yger</dc:creator>
    </item>
  </channel>
</rss>
