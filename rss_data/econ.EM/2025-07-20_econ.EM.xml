<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Jul 2025 04:03:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Combining stated and revealed preferences</title>
      <link>https://arxiv.org/abs/2507.13552</link>
      <description>arXiv:2507.13552v1 Announce Type: new 
Abstract: Can stated preferences inform counterfactual analyses of actual choice? This research proposes a novel approach to researchers who have access to both stated choices in hypothetical scenarios and actual choices, matched or unmatched. The key idea is to use stated choices to identify the distribution of individual unobserved heterogeneity. If this unobserved heterogeneity is the source of endogeneity, the researcher can correct for its influence in a demand function estimation using actual choices and recover causal effects. Bounds on causal effects are derived in the case, where stated choice and actual choices are observed in unmatched data sets. These data combination bounds are of independent interest. We derive a valid bootstrap inference for the bounds and show its good performance in a simulation experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13552v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romuald Meango, Marc Henry, Ismael Mourifie</dc:creator>
    </item>
    <item>
      <title>Who With Whom? Learning Optimal Matching Policies</title>
      <link>https://arxiv.org/abs/2507.13567</link>
      <description>arXiv:2507.13567v1 Announce Type: new 
Abstract: There are many economic contexts where the productivity and welfare performance of institutions and policies depend on who matches with whom. Examples include caseworkers and job seekers in job search assistance programs, medical doctors and patients, teachers and students, attorneys and defendants, and tax auditors and taxpayers, among others. Although reallocating individuals through a change in matching policy can be less costly than training personnel or introducing a new program, methods for learning optimal matching policies and their statistical performance are less studied than methods for other policy interventions. This paper develops a method to learn welfare optimal matching policies for two-sided matching problems in which a planner matches individuals based on the rich set of observable characteristics of the two sides. We formulate the learning problem as an empirical optimal transport problem with a match cost function estimated from training data, and propose estimating an optimal matching policy by maximizing the entropy regularized empirical welfare criterion. We derive a welfare regret bound for the estimated policy and characterize its convergence. We apply our proposal to the problem of matching caseworkers and job seekers in a job search assistance program, and assess its welfare performance in a simulation study calibrated with French administrative data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13567v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yagan Hazard, Toru Kitagawa</dc:creator>
    </item>
    <item>
      <title>Debiased Machine Learning for Unobserved Heterogeneity: High-Dimensional Panels and Measurement Error Models</title>
      <link>https://arxiv.org/abs/2507.13788</link>
      <description>arXiv:2507.13788v1 Announce Type: new 
Abstract: Developing robust inference for models with nonparametric Unobserved Heterogeneity (UH) is both important and challenging. We propose novel Debiased Machine Learning (DML) procedures for valid inference on functionals of UH, allowing for partial identification of multivariate target and high-dimensional nuisance parameters. Our main contribution is a full characterization of all relevant Neyman-orthogonal moments in models with nonparametric UH, where relevance means informativeness about the parameter of interest. Under additional support conditions, orthogonal moments are globally robust to the distribution of the UH. They may still involve other high-dimensional nuisance parameters, but their local robustness reduces regularization bias and enables valid DML inference. We apply these results to: (i) common parameters, average marginal effects, and variances of UH in panel data models with high-dimensional controls; (ii) moments of the common factor in the Kotlarski model with a factor loading; and (iii) smooth functionals of teacher value-added. Monte Carlo simulations show substantial efficiency gains from using efficient orthogonal moments relative to ad-hoc choices. We illustrate the practical value of our approach by showing that existing estimates of the average and variance effects of maternal smoking on child birth weight are robust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13788v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Facundo Arga\~naraz, Juan Carlos Escanciano</dc:creator>
    </item>
    <item>
      <title>Adapting to Misspecification</title>
      <link>https://arxiv.org/abs/2305.14265</link>
      <description>arXiv:2305.14265v5 Announce Type: replace 
Abstract: Empirical research typically involves a robustness-efficiency tradeoff. A researcher seeking to estimate a scalar parameter can invoke strong assumptions to motivate a restricted estimator that is precise but may be heavily biased, or they can relax some of these assumptions to motivate a more robust, but variable, unrestricted estimator. When a bound on the bias of the restricted estimator is available, it is optimal to shrink the unrestricted estimator towards the restricted estimator. For settings where a bound on the bias of the restricted estimator is unknown, we propose adaptive estimators that minimize the percentage increase in worst case risk relative to an oracle that knows the bound. We show that adaptive estimators solve a weighted convex minimax problem and provide lookup tables facilitating their rapid computation. Revisiting some well known empirical studies where questions of model specification arise, we examine the advantages of adapting to -- rather than testing for -- misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14265v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy B. Armstrong, Patrick Kline, Liyang Sun</dc:creator>
    </item>
    <item>
      <title>Revisiting Randomization with the Cube Method</title>
      <link>https://arxiv.org/abs/2407.13613</link>
      <description>arXiv:2407.13613v3 Announce Type: replace 
Abstract: We introduce a new randomization procedure for experiments based on the cube method, which achieves near-exact covariate balance. This ensures compliance with standard balance tests and allows for balancing on many covariates, enabling more precise estimation of treatment effects using pre-experimental information. We derive theoretical bounds on imbalance as functions of sample size and covariate dimension, and establish consistency and asymptotic normality of the resulting estimators. Simulations show substantial improvements in precision and covariate balance over existing methods, particularly when the number of covariates is large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13613v3</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laurent Davezies, Guillaume Hollard, Pedro Vergara Merino</dc:creator>
    </item>
    <item>
      <title>Fitting Dynamically Misspecified Models: An Optimal Transportation Approach</title>
      <link>https://arxiv.org/abs/2412.20204</link>
      <description>arXiv:2412.20204v2 Announce Type: replace 
Abstract: This paper considers filtering, parameter estimation, and testing for potentially dynamically misspecified state-space models. When dynamics are misspecified, filtered values of state variables often do not satisfy model restrictions, making them hard to interpret, and parameter estimates may fail to characterize the dynamics of filtered variables. To address this, a sequential optimal transportation approach is used to generate a model-consistent sample by mapping observations from a flexible reduced-form to the structural conditional distribution iteratively. Filtered series from the generated sample are model-consistent. Specializing to linear processes, a closed-form Optimal Transport Filtering algorithm is derived. Minimizing the discrepancy between generated and actual observations defines an Optimal Transport Estimator. Its large sample properties are derived. A specification test determines if the model can reproduce the sample path, or if the discrepancy is statistically significant. Empirical applications to trend-cycle decomposition, DSGE models, and affine term structure models illustrate the methodology and the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20204v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-Jacques Forneron, Zhongjun Qu</dc:creator>
    </item>
    <item>
      <title>Projection Inference for set-identified SVARs</title>
      <link>https://arxiv.org/abs/2504.14106</link>
      <description>arXiv:2504.14106v2 Announce Type: replace 
Abstract: We study the properties of projection inference for set-identified Structural Vector Autoregressions. A nominal $1-\alpha$ projection region collects the structural parameters that are compatible with a $1-\alpha$ Wald ellipsoid for the model's reduced-form parameters (autoregressive coefficients and the covariance matrix of residuals).
  We show that projection inference can be applied to a general class of stationary models, is computationally feasible, and -- as the sample size grows large -- it produces regions for the structural parameters and their identified set with both frequentist coverage and \emph{robust} Bayesian credibility of at least $1-\alpha$.
  A drawback of the projection approach is that both coverage and robust credibility may be strictly above their nominal level. Following the work of \cite{Kaido_Molinari_Stoye:2014}, we `calibrate' the radius of the Wald ellipsoid to guarantee that -- for a given posterior on the reduced-form parameters -- the robust Bayesian credibility of the projection method is exactly $1-\alpha$. If the bounds of the identified set are differentiable, our calibrated projection also covers the identified set with probability $1-\alpha$. %eliminating the excess of robust Bayesian credibility also eliminates excessive frequentist coverage.
  We illustrate the main results of the paper using the demand/supply-model for the U.S. labor market in Baumeister_Hamilton(2015)</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14106v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bulat Gafarov, Matthias Meier, Jos\'e Luis Montiel Olea</dc:creator>
    </item>
    <item>
      <title>Better Understanding Triple Differences Estimators</title>
      <link>https://arxiv.org/abs/2505.09942</link>
      <description>arXiv:2505.09942v3 Announce Type: replace 
Abstract: Triple Differences (DDD) designs are widely used in empirical work to relax parallel trends assumptions in Difference-in-Differences (DiD) settings. This paper highlights that common DDD implementations -- such as taking the difference between two DiDs or applying three-way fixed effects regressions -- are generally invalid when identification requires conditioning on covariates. In staggered adoption settings, the common DiD practice of pooling all not-yet-treated units as a comparison group can introduce additional bias, even when covariates are not required for identification. These insights challenge conventional empirical strategies and underscore the need for estimators tailored specifically to DDD structures. We develop regression adjustment, inverse probability weighting, and doubly robust estimators that remain valid under covariate-adjusted DDD parallel trends. For staggered designs, we demonstrate how to effectively utilize multiple comparison groups to obtain more informative inferences. Simulations and three empirical applications highlight bias reductions and precision gains relative to standard approaches. A companion R package is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09942v3</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcelo Ortiz-Villavicencio, Pedro H. C. Sant'Anna</dc:creator>
    </item>
    <item>
      <title>An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model</title>
      <link>https://arxiv.org/abs/2502.14131</link>
      <description>arXiv:2502.14131v4 Announce Type: replace-cross 
Abstract: We study the problem of estimating Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy-Regularized Inverse Reinforcement Learning (offline MaxEnt-IRL) in machine learning. The objective is to recover reward or $Q^*$ functions that govern agent behavior from offline behavior data. In this paper, we propose a globally convergent gradient-based method for solving these problems without the restrictive assumption of linearly parameterized rewards. The novelty of our approach lies in introducing the Empirical Risk Minimization (ERM) based IRL/DDC framework, which circumvents the need for explicit state transition probability estimation in the Bellman equation. Furthermore, our method is compatible with non-parametric estimation techniques such as neural networks. Therefore, the proposed method has the potential to be scaled to high-dimensional, infinite state spaces. A key theoretical insight underlying our approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL) condition -- a property that, while weaker than strong convexity, is sufficient to ensure fast global convergence guarantees. Through a series of synthetic experiments, we demonstrate that our approach consistently outperforms benchmark methods and state-of-the-art alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14131v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enoch H. Kang, Hema Yoganarasimhan, Lalit Jain</dc:creator>
    </item>
  </channel>
</rss>
