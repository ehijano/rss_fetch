<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 May 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Basilisk: A 34 mm2 End-to-End Open-Source 64-bit Linux-Capable RISC-V SoC in 130nm BiCMOS</title>
      <link>https://arxiv.org/abs/2505.10060</link>
      <description>arXiv:2505.10060v1 Announce Type: new 
Abstract: End-to-end open-source electronic design automation (OSEDA) enables a collaborative approach to chip design conducive to supply chain diversification and zero-trust step-by-step design verification. However, existing end-to-end OSEDA flows have mostly been demonstrated on small designs and have not yet enabled large, industry-grade chips such as Linux-capable systems-on-chip (SoCs). This work presents Basilisk, the largest end-to-end open-source SoC to date. Basilisk's 34 mm2, 2.7 MGE design features a 64-bit Linux-capable RISC-V core, a lightweight 124 MB/s DRAM controller, and extensive IO, including a USB 1.1 host, a video output, and a fully digital 62 Mb/s chip-to-chip (C2C) link. We implement Basilisk in IHP's open 130 nm BiCMOS technology, significantly improving on the state-of-the-art (SoA) OSEDA flow. Our enhancements of the Yosys-based synthesis flow improve design timing and area by 2.3x and 1.6x, respectively, while consuming significantly less system resources. By tuning OpenROAD place and route (P&amp;R) to our design and technology, we decrease the die size by 12%. The fabricated Basilisk chip reaches 62 MHz at its nominal 1.2 V core voltage and up to 102 MHz at 1.64 V. It achieves a peak energy efficiency of 18.9 DP MFLOP/s/W at 0.88 V.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10060v1</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe Sauter, Thomas Benz, Paul Scheffler, Martin Povi\v{s}er, Frank K. G\"urkaynak, Luca Benini</dc:creator>
    </item>
    <item>
      <title>An Integrated UVM-TLM Co-Simulation Framework for RISC-V Functional Verification and Performance Evaluation</title>
      <link>https://arxiv.org/abs/2505.10145</link>
      <description>arXiv:2505.10145v1 Announce Type: new 
Abstract: The burgeoning RISC-V ecosystem necessitates efficient verification methodologies for complex processors. Traditional approaches often struggle to concurrently evaluate functional correctness and performance, or balance simulation speed with modeling accuracy. This paper introduces an integrated co-simulation framework leveraging Universal Verification Methodology (UVM) and Transaction-Level Modeling (TLM) for RISC-V processor validation. We present a configurable UVM-TLM model (vmodel) of a superscalar, out-of-order RISC-V core, featuring key microarchitectural modeling techniques such as credit-based pipeline flow control. This environment facilitates unified functional verification via co-simulation against the Spike ISA simulator and enables early-stage performance assessment using benchmarks like CoreMark, orchestrated within UVM. The methodology prioritizes integration, simulation efficiency, and acceptable fidelity for architectural exploration over cycle-level precision. Experimental results validate functional correctness and significant simulation speedup over RTL approaches, accelerating design iterations and enhancing verification coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10145v1</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruizhi Qiu, Yang Liu</dc:creator>
    </item>
    <item>
      <title>Enabling Syscall Intercept for RISC-V</title>
      <link>https://arxiv.org/abs/2505.10217</link>
      <description>arXiv:2505.10217v1 Announce Type: cross 
Abstract: The European Union technological sovereignty strategy centers around the RISC-V Instruction Set Architecture, with the European Processor Initiative leading efforts to build production-ready processors. Focusing on realizing a functional RISC-V ecosystem, the BZL initiative (www.bzl.es) is making an effort to create a software stack along with the hardware. In this work, we detail the efforts made in porting a widely used syscall interception library, mainly used on AdHocFS (i.e., DAOS, GekkoFS), to RISC-V and how we overcame some of the limitations encountered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10217v1</guid>
      <category>cs.OS</category>
      <category>cs.AR</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Petar Andri\'c, Aaron Call, Ramon Nou</dc:creator>
    </item>
    <item>
      <title>Scalable 28nm IC implementation of coupled oscillator network featuring tunable topology and complexity</title>
      <link>https://arxiv.org/abs/2505.10248</link>
      <description>arXiv:2505.10248v1 Announce Type: cross 
Abstract: Integrated circuit implementations of coupled oscillator networks have recently gained increased attention. The focus is usually on using these networks for analogue computing, for example for solving computational optimization tasks. For use within analog computing, these networks are run close to critical dynamics. On the other hand, such networks are also used as an analogy of transport networks such as electrical power grids to answer the question of how exactly such critical dynamic states can be avoided. However, simulating large network of coupled oscillators is computationally intensive, with specifc regards to electronic ones. We have developed an integrated circuit using integrated Phase-Locked Loop (PLL) with modifications, that allows to flexibly vary the topology as well as a complexity parameter of the network during operation. The proposed architecture, inspired by the brain, employs a clustered architecture, with each cluster containing 7 PLLs featuring programmable coupling mechanisms. Additionally, the inclusion of a RISC-V processor enables future algorithmic implementations. Thus, we provide a practical alternative for large-scale network simulations both in the field of analog computing and transport network stability research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10248v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Y. Neyaz, A. Ashok, M. Schiek, C. Grewing, A. Zambanini, S. van Waasen</dc:creator>
    </item>
    <item>
      <title>Demystifying AI Platform Design for Distributed Inference of Next-Generation LLM models</title>
      <link>https://arxiv.org/abs/2406.01698</link>
      <description>arXiv:2406.01698v3 Announce Type: replace 
Abstract: Large language models (LLMs) have shown remarkable performance across a wide range of applications, often outperforming human experts. However, deploying these gigantic models efficiently for diverse inference use cases requires carefully designed hardware platforms with ample computing, memory, and network resources. With constant innovation in LLM serving optimizations and model architecture evolving at breakneck speed, the hardware requirements to meet Service Level Objectives (SLOs) remain an open research question.
  To answer the question, we present an analytical tool, GenZ, to efficiently navigate the relationship between diverse LLM model architectures(Dense, GQA, MoE, Mamba), LLM serving optimizations(Chunking, Speculative decoding, quanitization), and AI platform design parameters. Our tool estimates LLM inference performance metrics for the given scenario. We have validated against real hardware platforms running various different LLM models, achieving a max geomean error of 5.82.We use GenZ to identify compute, memory capacity, memory bandwidth, network latency, and network bandwidth requirements across diverse LLM inference use cases. We also study diverse architectural choices in use today (inspired by LLM serving platforms from several vendors) to help inform computer architects designing next-generation AI hardware accelerators and platforms. The trends and insights derived from GenZ can guide AI engineers deploying LLMs as well as computer architects designing next-generation hardware accelerators and platforms. Ultimately, this work sheds light on the platform design considerations for unlocking the full potential of large language models across a spectrum of applications. The source code is available at https://github.com/abhibambhaniya/GenZ-LLM-Analyzer . Users can also be tried it on at https://genz-llm-analyzer.streamlit.app/ without any setup on your web browser.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01698v3</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abhimanyu Bambhaniya, Ritik Raj, Geonhwa Jeong, Souvik Kundu, Sudarshan Srinivasan, Suvinay Subramanian, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna</dc:creator>
    </item>
    <item>
      <title>GPU Performance Portability needs Autotuning</title>
      <link>https://arxiv.org/abs/2505.03780</link>
      <description>arXiv:2505.03780v2 Announce Type: replace 
Abstract: As LLMs grow in complexity, achieving state-of-the-art performance requires tight co-design across algorithms, software, and hardware. Today's reliance on a single dominant platform limits portability, creates vendor lock-in, and raises barriers for new AI hardware. In this work, we make the case for combining just-in-time (JIT) compilation with kernel parameter autotuning to enable portable LLM inference with state-of-the-art performance without code changes. Focusing on flash attention -- a widespread performance critical LLM kernel -- we demonstrate that this approach explores up to 15x more kernel parameter configurations, produces significantly more diverse code across multiple dimensions, and even outperforms vendor-optimized implementations by up to 230%, all while reducing kernel code size by 70x and eliminating manual code optimizations. Our results highlight autotuning as a promising path to unlocking model portability across GPU vendors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03780v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Burkhard Ringlein, Thomas Parnell, Radu Stoica</dc:creator>
    </item>
    <item>
      <title>Spec2Assertion: Automatic Pre-RTL Assertion Generation using Large Language Models with Progressive Regularization</title>
      <link>https://arxiv.org/abs/2505.07995</link>
      <description>arXiv:2505.07995v2 Announce Type: replace 
Abstract: SystemVerilog Assertions (SVAs) play a critical role in detecting and debugging functional bugs in digital chip design. However, generating SVAs has traditionally been a manual, labor-intensive, and error-prone process. Recent advances in automatic assertion generation, particularly those using machine learning and large language models (LLMs), have shown promising potential, though most approaches remain in the early stages of development. In this work, we introduce Spec2Assertion, a new technique for automatically generating assertions from design specifications prior to RTL implementation. It leverages LLMs with progressive regularization and incorporates Chain-of-Thought (CoT) prompting to guide assertion synthesis. Additionally, we propose a new evaluation methodology that assesses assertion quality across a broad range of scenarios. Experiments on multiple benchmark designs show that Spec2Assertion generates 70% more syntax-correct assertions with 2X quality improvement on average compared to a recent state-of-the-art approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07995v2</guid>
      <category>cs.AR</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fenghua Wu, Evan Pan, Rahul Kande, Michael Quinn, Aakash Tyagi, David Kebo Houngninou, Jeyavijayan Rajendran, Jiang Hu</dc:creator>
    </item>
    <item>
      <title>Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities</title>
      <link>https://arxiv.org/abs/2505.06085</link>
      <description>arXiv:2505.06085v2 Announce Type: replace-cross 
Abstract: The increasing demand for generative AI as Large Language Models (LLMs) services has driven the need for specialized hardware architectures that optimize computational efficiency and energy consumption. This paper evaluates the performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic linear algebra kernels at reduced numerical precision, a fundamental operation in LLM computations. We present a detailed characterization of Grayskull's execution model, gridsize, matrix dimensions, data formats, and numerical precision impact computational efficiency. Furthermore, we compare Grayskull's performance against state-of-the-art architectures with tensor acceleration, including Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100). Whilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a competitive trade-off between power consumption and computational throughput, reaching a peak of 1.55 TFLOPs/Watt with BF16.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06085v2</guid>
      <category>cs.PF</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiari Pizzini Cavagna, Daniele Cesarini, Andrea Bartolini</dc:creator>
    </item>
    <item>
      <title>"vcd2df" -- Leveraging Data Science Insights for Hardware Security Research</title>
      <link>https://arxiv.org/abs/2505.06470</link>
      <description>arXiv:2505.06470v2 Announce Type: replace-cross 
Abstract: In this work, we hope to expand the universe of security practitioners of open-source hardware by creating a bridge from hardware design languages (HDLs) to data science languages like Python and R through libraries that convert VCD (value change dump) files into data frames, the expected input type of the modern data science tools. We show how insights can be derived in high-level languages from register transfer level (RTL) trace data. Additional, we show a promising future direction in hardware security research leveraging the parallelism of the Spark DataFrame.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06470v2</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Calvin Deutschbein, Jimmy Ostler, Hriday Raj</dc:creator>
    </item>
  </channel>
</rss>
