<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Jul 2024 01:51:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Spatio-temporal areal models to support small area estimation: An application to national-scale forest carbon monitoring</title>
      <link>https://arxiv.org/abs/2407.09909</link>
      <description>arXiv:2407.09909v1 Announce Type: new 
Abstract: National Forest Inventory (NFI) programs can provide vital information on the status, trend, and change in forest parameters. These programs are being increasingly asked to provide forest parameter estimates for spatial and temporal extents smaller than their current design and accompanying design-based methods can deliver with desired levels of uncertainty. Many NFI designs and estimation methods focus on status and are not well equipped to provide acceptable estimates for trend and change parameters, especially over small spatial domains and/or short time periods. Fine-scale space-time indexed estimates are critical to a variety of environmental, ecological, and economic monitoring efforts. Estimates for forest carbon status, trend, and change are of particular importance to international initiatives to track carbon dynamics. Model-based small area estimation (SAE) methods for NFI and similar ecological monitoring data typically pursue inference on status within small spatial domains, with few demonstrated methods that account for spatio-temporal dependence needed for trend and change estimation. We propose a spatio-temporal Bayesian model framework that delivers statistically valid estimates with full uncertainty quantification for status, trend, and change. The framework accommodates a variety of space and time dependency structures, and we detail model configurations for different settings. Through analysis of simulated datasets, we compare the relative performance of candidate models and a traditional direct estimator. We then apply candidate models to a large-scale NFI dataset to demonstrate the utility of the proposed framework for providing unique quantification of forest carbon dynamics in the contiguous United States. We also provide computationally efficient algorithms, software, and data to reproduce our results and for benchmarking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09909v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elliot S. Shannon, Andrew O. Finley, Paul B. May, Grant M. Domke, Hans-Erik Andersen, George C. Gaines, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>WeConvene: Learned Image Compression with Wavelet-Domain Convolution and Entropy Model</title>
      <link>https://arxiv.org/abs/2407.09983</link>
      <description>arXiv:2407.09983v1 Announce Type: new 
Abstract: Recently learned image compression (LIC) has achieved great progress and even outperformed the traditional approach using DCT or discrete wavelet transform (DWT). However, LIC mainly reduces spatial redundancy in the autoencoder networks and entropy coding, but has not fully removed the frequency-domain correlation explicitly as in DCT or DWT. To leverage the best of both worlds, we propose a surprisingly simple but efficient framework, which introduces the DWT to both the convolution layers and entropy coding of CNN-based LIC. First, in both the core and hyperprior autoencoder networks, we propose a Wavelet-domain Convolution (WeConv) module, which performs convolution after DWT, and then converts the data back to spatial domain via inverse DWT. This module is used at selected layers in a CNN network to reduce the frequency-domain correlation explicitly and make the signal sparser in DWT domain. We also propose a wavelet-domain Channel-wise Auto-Regressive entropy Model (WeChARM), where the output latent representations from the encoder network are first transformed by the DWT, before applying quantization and entropy coding, as in the traditional paradigm. Moreover, the entropy coding is split into two steps. We first code all low-frequency DWT coefficients, and then use them as prior to code high-frequency coefficients. The channel-wise entropy coding is further used in each step. By combining WeConv and WeChARM, the proposed WeConvene scheme achieves superior R-D performance compared to other state-of-the-art LIC methods as well as the latest H.266/VVC. For the Kodak dataset and the baseline network with -0.4% BD-Rate saving over H.266/VVC, introducing WeConv with the simplest Haar transform improves the saving to -4.7%. This is quite impressive given the simplicity of the Haar transform. Enabling Haar-based WeChARM entropy coding further boosts the saving to -8.2%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09983v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haisheng Fu, Jie Liang, Zhenman Fang, Jingning Han, Feng Liang, Guohe Zhang</dc:creator>
    </item>
    <item>
      <title>Low Volatility Stock Portfolio Through High Dimensional Bayesian Cointegration</title>
      <link>https://arxiv.org/abs/2407.10175</link>
      <description>arXiv:2407.10175v1 Announce Type: new 
Abstract: We employ a Bayesian modelling technique for high dimensional cointegration estimation to construct low volatility portfolios from a large number of stocks. The proposed Bayesian framework effectively identifies sparse and important cointegration relationships amongst large baskets of stocks across various asset spaces, resulting in portfolios with reduced volatility. Such cointegration relationships persist well over the out-of-sample testing time, providing practical benefits in portfolio construction and optimization. Further studies on drawdown and volatility minimization also highlight the benefits of including cointegrated portfolios as risk management instruments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10175v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>q-fin.PM</category>
      <category>q-fin.ST</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parley R Yang, Alexander Y Shestopaloff</dc:creator>
    </item>
    <item>
      <title>Hidden Markov models with an unknown number of states and a repulsive prior on the state parameters</title>
      <link>https://arxiv.org/abs/2407.10869</link>
      <description>arXiv:2407.10869v1 Announce Type: new 
Abstract: Hidden Markov models (HMMs) offer a robust and efficient framework for analyzing time series data, modelling both the underlying latent state progression over time and the observation process, conditional on the latent state. However, a critical challenge lies in determining the appropriate number of underlying states, often unknown in practice. In this paper, we employ a Bayesian framework, treating the number of states as a random variable and employing reversible jump Markov chain Monte Carlo to sample from the posterior distributions of all parameters, including the number of states. Additionally, we introduce repulsive priors for the state parameters in HMMs, and hence avoid overfitting issues and promote parsimonious models with dissimilar state components. We perform an extensive simulation study comparing performance of models with independent and repulsive prior distributions on the state parameters, and demonstrate our proposed framework on two ecological case studies: GPS tracking data on muskox in Antarctica and acoustic data on Cape gannets in South Africa. Our results highlight how our framework effectively explores the model space, defined by models with different latent state dimensions, while leading to latent states that are distinguished better and hence are more interpretable, enabling better understanding of complex dynamic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10869v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Rotous, Alex Diana, Alessio Farcomeni, Eleni Material, Andr\'ea Thiebault</dc:creator>
    </item>
    <item>
      <title>Valid standard errors for Bayesian quantile regression with clustered and independent data</title>
      <link>https://arxiv.org/abs/2407.09772</link>
      <description>arXiv:2407.09772v1 Announce Type: cross 
Abstract: In Bayesian quantile regression, the most commonly used likelihood is the asymmetric Laplace (AL) likelihood. The reason for this choice is not that it is a plausible data-generating model but that the corresponding maximum likelihood estimator is identical to the classical estimator by Koenker and Bassett (1978), and in that sense, the AL likelihood can be thought of as a working likelihood. AL-based quantile regression has been shown to produce good finite-sample Bayesian point estimates and to be consistent. However, if the AL distribution does not correspond to the data-generating distribution, credible intervals based on posterior standard deviations can have poor coverage. Yang, Wang, and He (2016) proposed an adjustment to the posterior covariance matrix that produces asymptotically valid intervals. However, we show that this adjustment is sensitive to the choice of scale parameter for the AL likelihood and can lead to poor coverage when the sample size is small to moderate. We therefore propose using Infinitesimal Jackknife (IJ) standard errors (Giordano &amp; Broderick, 2023). These standard errors do not require resampling but can be obtained from a single MCMC run. We also propose a version of IJ standard errors for clustered data. Simulations and applications to real data show that the IJ standard errors have good frequentist properties, both for independent and clustered data. We provide an R-package that computes IJ standard errors for clustered or independent data after estimation with the brms wrapper in R for Stan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09772v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ji, JoonHo Lee, Sophia Rabe-Hesketh</dc:creator>
    </item>
    <item>
      <title>The GPT Surprise: Offering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters Exam Performances</title>
      <link>https://arxiv.org/abs/2407.09975</link>
      <description>arXiv:2407.09975v1 Announce Type: cross 
Abstract: Large language models (LLMs) are quickly being adopted in a wide range of learning experiences, especially via ubiquitous and broadly accessible chat interfaces like ChatGPT and Copilot. This type of interface is readily available to students and teachers around the world, yet relatively little research has been done to assess the impact of such generic tools on student learning. Coding education is an interesting test case, both because LLMs have strong performance on coding tasks, and because LLM-powered support tools are rapidly becoming part of the workflow of professional software engineers. To help understand the impact of generic LLM use on coding education, we conducted a large-scale randomized control trial with 5,831 students from 146 countries in an online coding class in which we provided some students with access to a chat interface with GPT-4. We estimate positive benefits on exam performance for adopters, the students who used the tool, but over all students, the advertisement of GPT-4 led to a significant average decrease in exam participation. We observe similar decreases in other forms of course engagement. However, this decrease is modulated by the student's country of origin. Offering access to LLMs to students from low human development index countries increased their exam participation rate on average. Our results suggest there may be promising benefits to using LLMs in an introductory coding class, but also potential harms for engagement, which makes their longer term impact on student success unclear. Our work highlights the need for additional investigations to help understand the potential impact of future adoption and integration of LLMs into classrooms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09975v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Allen Nie, Yash Chandak, Miroslav Suzara, Malika Ali, Juliette Woodrow, Matt Peng, Mehran Sahami, Emma Brunskill, Chris Piech</dc:creator>
    </item>
    <item>
      <title>AB$\mathbb{C}$MB: Deep Delensing Assisted Likelihood-Free Inference from CMB Polarization Maps</title>
      <link>https://arxiv.org/abs/2407.10013</link>
      <description>arXiv:2407.10013v1 Announce Type: cross 
Abstract: The existence of a cosmic background of primordial gravitational waves (PGWB) is a robust prediction of inflationary cosmology, but it has so far evaded discovery. The most promising avenue of its detection is via measurements of Cosmic Microwave Background (CMB) $B$-polarization. However, this is not straightforward due to (a) the fact that CMB maps are distorted by gravitational lensing and (b) the high-dimensional nature of CMB data, which renders likelihood-based analysis methods computationally extremely expensive. In this paper, we introduce an efficient likelihood-free, end-to-end inference method to directly infer the posterior distribution of the tensor-to-scalar ratio $r$ from lensed maps of the Stokes $Q$ and $U$ polarization parameters. Our method employs a generative model to delense the maps and utilizes the Approximate Bayesian Computation (ABC) algorithm to sample $r$. We demonstrate that our method yields unbiased estimates of $r$ with well-calibrated uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10013v1</guid>
      <category>astro-ph.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Yi, Yanan Fan, Jan Hamann, Pietro Li\`o, Yuguang Wang</dc:creator>
    </item>
    <item>
      <title>The inverse Kalman filter</title>
      <link>https://arxiv.org/abs/2407.10089</link>
      <description>arXiv:2407.10089v1 Announce Type: cross 
Abstract: In this study, we introduce a new approach, the inverse Kalman filter (IKF), which enables accurate matrix-vector multiplication between a covariance matrix from a dynamic linear model and any real-valued vector with linear computational cost. We incorporate the IKF with the conjugate gradient algorithm, which substantially accelerates the computation of matrix inversion for a general form of covariance matrices, whereas other approximation approaches may not be directly applicable. We demonstrate the scalability and efficiency of the IKF approach through distinct applications, including nonparametric estimation of particle interaction functions and predicting incomplete lattices of correlated data, using both simulation and real-world observations, including cell trajectory and satellite radar interferogram.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10089v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Fang, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Improving On-Time Undergraduate Graduation Rate For Undergraduate Students Using Predictive Analytics</title>
      <link>https://arxiv.org/abs/2407.10253</link>
      <description>arXiv:2407.10253v1 Announce Type: cross 
Abstract: The on-time graduation rate among universities in Puerto Rico is significantly lower than in the mainland United States. This problem is noteworthy because it leads to substantial negative consequences for the student, both socially and economically, the educational institution and the local economy. This project aims to develop a predictive model that accurately detects students early in their academic pursuit at risk of not graduating on time. Various predictive models are developed to do this, and the best model, the one with the highest performance, is selected. Using a dataset containing information from 24432 undergraduate students at the University of Puerto Rico at Mayaguez, the predictive performance of the models is evaluated in two scenarios: Group I includes both the first year of college and pre-college factors, and Group II only considers pre-college factors. Overall, for both scenarios, the boosting model, trained on the oversampled dataset, is the most successful at predicting who will not graduate on time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10253v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramineh Lopez-Yazdani, Roberto Rivera</dc:creator>
    </item>
    <item>
      <title>Growth of Science: How long will the United States uphold its position?</title>
      <link>https://arxiv.org/abs/2407.10771</link>
      <description>arXiv:2407.10771v1 Announce Type: cross 
Abstract: Policymakers often assess the growth of science in a country and compare it with that of other countries to set future planning for scientific research focusing on the sustainable development and economic growth of the country. Here, we study the growth of science for the period of 1996-2020 corresponding to the top fifty countries with the highest publications in 2020. It is found that the annual growth rates of scientific and technical journal publications exhibit Taylor's power law behavior indicating the dependence of the variance on the mean growth rate and the distributions of annual growth rates follow skew-symmetric distributions. Furthermore, we have computed the entropy based on annual publication numbers among the countries to assess the spatial disparity in the system. The entropy is found to increase mostly linear with time reducing the disparity among the countries. By performing the linear regression analysis, we predict that around the year 2046, all the countries excluding China may equally contribute towards the growth of science. We have also assessed the stability of the current USA ranking by computing the entropy between the USA and other countries. Based on the regression analysis, it is estimated that three potential countries such as Indonesia, India, and Iran may take the ranks ahead of the USA around the years 2024, 2029, and 2041 respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10771v1</guid>
      <category>physics.soc-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dipak Patra</dc:creator>
    </item>
    <item>
      <title>Quantifying distribution system resilience from utility data: large event risk and benefits of investments</title>
      <link>https://arxiv.org/abs/2407.10773</link>
      <description>arXiv:2407.10773v1 Announce Type: cross 
Abstract: We focus on large blackouts in electric distribution systems caused by extreme winds. Such events have a large cost and impact on customers. To quantify resilience to these events, we formulate large event risk and show how to calculate it from the historical outage data routinely collected by utilities' outage management systems. Risk is defined using an event cost exceedance curve. The tail of this curve and the large event risk is described by the probability of a large cost event and the slope magnitude of the tail on a log-log plot. Resilience can be improved by planned investments to upgrade system components or speed up restoration. The benefits that these investments would have had if they had been made in the past can be quantified by "rerunning history" with the effects of the investment included, and then recalculating the large event risk to find the improvement in resilience. An example using utility data shows a 12% and 22% reduction in the probability of a large cost event due to 10% wind hardening and 10% faster restoration respectively. This new data-driven approach to quantify resilience and resilience investments is realistic and much easier to apply than complicated approaches based on modeling all the phases of resilience. Moreover, an appeal to improvements to past lived experience may well be persuasive to customers and regulators in making the case for resilience investments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10773v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arslan Ahmad, Ian Dobson</dc:creator>
    </item>
    <item>
      <title>Statistical Response of ENSO Complexity to Initial Condition and Model Parameter Perturbations</title>
      <link>https://arxiv.org/abs/2401.03281</link>
      <description>arXiv:2401.03281v3 Announce Type: replace-cross 
Abstract: Studying the response of a climate system to perturbations has practical significance. Standard methods in computing the trajectory-wise deviation caused by perturbations may suffer from the chaotic nature that makes the model error dominate the true response after a short lead time. Statistical response, which computes the return described by the statistics, provides a systematic way of reaching robust outcomes with an appropriate quantification of the uncertainty and extreme events. In this paper, information theory is applied to compute the statistical response and find the most sensitive perturbation direction of different El Ni\~no-Southern Oscillation (ENSO) events to initial value and model parameter perturbations. Depending on the initial phase and the time horizon, different state variables contribute to the most sensitive perturbation direction. While initial perturbations in sea surface temperature (SST) and thermocline depth usually lead to the most significant response of SST at short- and long-range, respectively, initial adjustment of the zonal advection can be crucial to trigger strong statistical responses at medium-range around 5 to 7 months, especially at the transient phases between El Ni\~no and La Ni\~na. It is also shown that the response in the variance triggered by external random forcing perturbations, such as the wind bursts, often dominates the mean response, making the resulting most sensitive direction very different from the trajectory-wise methods. Finally, despite the strong non-Gaussian climatology distributions, using Gaussian approximations in the information theory is efficient and accurate for computing the statistical response, allowing the method to be applied to sophisticated operational systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03281v3</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Andreou, Nan Chen</dc:creator>
    </item>
    <item>
      <title>A Virtual Solar Wind Monitor at Mars with Uncertainty Quantification using Gaussian Processes</title>
      <link>https://arxiv.org/abs/2402.01932</link>
      <description>arXiv:2402.01932v4 Announce Type: replace-cross 
Abstract: Single spacecraft missions do not measure the pristine solar wind continuously because of the spacecrafts' orbital trajectory. The infrequent spatiotemporal cadence of measurement fundamentally limits conclusions about solar wind-magnetosphere coupling throughout the solar system. At Mars, such single spacecraft missions result in limitations for assessing the solar wind's role in causing lower altitude observations such as auroral dynamics or atmospheric loss. In this work, we detail the development of a virtual solar wind monitor from the Mars Atmosphere and Volatile Evolution (MAVEN) mission; a single spacecraft. This virtual solar wind monitor provides a continuous estimate of the solar wind upstream from Mars with uncertainties. We specifically employ Gaussian process regression to estimate the upstream solar wind and uncertainty estimations that scale with the data sparsity of our real observations. This proxy enables continuous solar wind estimation at Mars with representative uncertainties for the majority of the time since since late 2014. We conclude by discussing suggested uses of this virtual solar wind monitor for statistical studies of the Mars space environment and heliosphere.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01932v4</guid>
      <category>physics.space-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1029/2024JH000155</arxiv:DOI>
      <dc:creator>A. R. Azari, E. Abrahams, F. Sapienza, J. Halekas, J. Biersteker, D. L. Mitchell, F. P\'erez, M. Marquette, M. J. Rutala, C. F. Bowers, C. M. Jackman, S. M. Curry</dc:creator>
    </item>
    <item>
      <title>Multivariate Bayesian models with flexible shared interactions for analyzing spatio-temporal patterns of rare cancers</title>
      <link>https://arxiv.org/abs/2403.10440</link>
      <description>arXiv:2403.10440v2 Announce Type: replace-cross 
Abstract: Rare cancers affect millions of people worldwide each year. However, estimating incidence or mortality rates associated with rare cancers presents important difficulties and poses new statistical methodological challenges. In this paper, we expand the collection of multivariate spatio-temporal models by introducing adaptable shared spatio-temporal components to enable a comprehensive analysis of both incidence and cancer mortality in rare cancer cases. These models allow the modulation of spatio-temporal effects between incidence and mortality, allowing for changes in their relationship over time. The new models have been implemented in INLA using r-generic constructions. We conduct a simulation study to evaluate the performance of the new spatio-temporal models. Our results show that multivariate spatio-temporal models incorporating a flexible shared spatio-temporal term outperform conventional multivariate spatio-temporal models that include specific spatio-temporal effects for each health outcome. We use these models to analyze incidence and mortality data for pancreatic cancer and leukaemia among males across 142 administrative health care districts of Great Britain over a span of nine biennial periods (2002-2019).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10440v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Garazi Retegui, Jaione Etxeberria, Mar\'ia Dolores Ugarte</dc:creator>
    </item>
    <item>
      <title>Process-based Inference for Spatial Energetics Using Bayesian Predictive Stacking</title>
      <link>https://arxiv.org/abs/2405.09906</link>
      <description>arXiv:2405.09906v2 Announce Type: replace-cross 
Abstract: Rapid developments in streaming data technologies have enabled real-time monitoring of human activity that can deliver high-resolution data on health variables over trajectories or paths carved out by subjects as they conduct their daily physical activities. Wearable devices, such as wrist-worn sensors that monitor gross motor activity, have become prevalent and have kindled the emerging field of ``spatial energetics'' in environmental health sciences. We devise a Bayesian inferential framework for analyzing such data while accounting for information available on specific spatial coordinates comprising a trajectory or path using a Global Positioning System (GPS) device embedded within the wearable device. We offer full probabilistic inference with uncertainty quantification using spatial-temporal process models adapted for data generated from ``actigraph'' units as the subject traverses a path or trajectory in their daily routine. Anticipating the need for fast inference for mobile health data, we pursue exact inference using conjugate Bayesian models and employ predictive stacking to assimilate inference across these individual models. This circumvents issues with iterative estimation algorithms such as Markov chain Monte Carlo. We devise Bayesian predictive stacking in this context for models that treat time as discrete epochs and that treat time as continuous. We illustrate our methods with simulation experiments and analysis of data from the Physical Activity through Sustainable Transport Approaches (PASTA-LA) study conducted by the Fielding School of Public Health at the University of California, Los Angeles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09906v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Sudipto Banerjee</dc:creator>
    </item>
  </channel>
</rss>
