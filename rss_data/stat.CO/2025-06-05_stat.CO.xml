<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Jun 2025 01:38:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Accelerating Randomized Algorithms for Low-Rank Matrix Approximation</title>
      <link>https://arxiv.org/abs/2506.03859</link>
      <description>arXiv:2506.03859v1 Announce Type: new 
Abstract: Randomized algorithms are overwhelming methods for low-rank approximation that can alleviate the computational expenditure with great reliability compared to deterministic algorithms. A crucial thought is generating a standard Gaussian matrix $\mathbf{G}$ and subsequently obtaining the orthonormal basis of the range of $\mathbf{AG}$ for a given matrix $\mathbf{A}$. Recently, the \texttt{farPCA} algorithm offers a framework for randomized algorithms, but the dense Gaussian matrix remains computationally expensive. Motivated by this, we introduce the standardized Bernoulli, sparse sign, and sparse Gaussian matrices to replace the standard Gaussian matrix in \texttt{farPCA} for accelerating computation. These three matrices possess a low computational expenditure in matrix-matrix multiplication and converge in distribution to a standard Gaussian matrix when multiplied by an orthogonal matrix under a mild condition. Therefore, the three corresponding proposed algorithms can serve as a superior alternative to fast adaptive randomized PCA (\texttt{farPCA}). Finally, we leverage random matrix theory (RMT) to derive a tighter error bound for \texttt{farPCA} without shifted techniques. Additionally, we extend this improved error bound to the error analysis of our three fast algorithms, ensuring that the proposed methods deliver more accurate approximations for large-scale matrices. Numerical experiments validate that the three algorithms achieve asymptotically the same performance as \texttt{farPCA} but with lower costs, offering a more efficient approach to low-rank matrix approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03859v1</guid>
      <category>stat.CO</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dandan Jiang, Bo Fu, Weiwei Xu</dc:creator>
    </item>
    <item>
      <title>Adaptive tuning of Hamiltonian Monte Carlo methods</title>
      <link>https://arxiv.org/abs/2506.04082</link>
      <description>arXiv:2506.04082v1 Announce Type: new 
Abstract: With the recently increased interest in probabilistic models, the efficiency of an underlying sampler becomes a crucial consideration. A Hamiltonian Monte Carlo (HMC) sampler is one popular option for models of this kind. Performance of HMC, however, strongly relies on a choice of parameters associated with an integration method for Hamiltonian equations, which up to date remains mainly heuristic or introduce time complexity. We propose a novel computationally inexpensive and flexible approach (we call it Adaptive Tuning or ATune) that, by analyzing the data generated during a burning stage of an HMC simulation, detects a system specific splitting integrator with a set of reliable HMC hyperparameters, including their credible randomization intervals, to be readily used in a production simulation. The method automatically eliminates those values of simulation parameters which could cause undesired extreme scenarios, such as resonance artifacts, low accuracy or poor sampling. The new approach is implemented in the in-house software package \textsf{HaiCS}, with no computational overheads introduced in a production simulation, and can be easily incorporated in any package for Bayesian inference with HMC. The tests on popular statistical models using original HMC and generalized Hamiltonian Monte Carlo (GHMC) reveal the superiority of adaptively tuned methods in terms of stability, performance and accuracy over conventional HMC tuned heuristically and coupled with the well-established integrators. We also claim that the generalized formulation of HMC, i.e. GHMC, is preferable for achieving high sampling performance. The efficiency of the new methodology is assessed in comparison with state-of-the-art samplers, e.g. the No-U-Turn-Sampler (NUTS), in real-world applications, such as endocrine therapy resistance in cancer, modeling of cell-cell adhesion dynamics and influenza epidemic outbreak.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04082v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Akhmatskaya, Lorenzo Nagar, Jose Antonio Carrillo, Leonardo Gavira Balmacz, Hristo Inouzhe, Mart\'in Parga Pazos, Mar\'ia Xos\'e Rodr\'iguez \'Alvarez</dc:creator>
    </item>
    <item>
      <title>Constrained mixtures of generalized normal distributions</title>
      <link>https://arxiv.org/abs/2506.03285</link>
      <description>arXiv:2506.03285v1 Announce Type: cross 
Abstract: This work introduces a family of univariate constrained mixtures of generalized normal distributions (CMGND) where the location, scale, and shape parameters can be constrained to be equal across any subset of mixture components. An expectation conditional maximisation (ECM) algorithm with Newton-Raphson updates is used to estimate the model parameters under the constraints. Simulation studies demonstrate that imposing correct constraints leads to more accurate parameter estimation compared to unconstrained mixtures, especially when components substantially overlap. Constrained models also exhibit competitive performance in capturing key characteristics of the marginal distribution, such as kurtosis. On a real dataset of daily stock index returns, CMGND models outperform constrained mixtures of normals and Student's t distributions based on the BIC criterion, highlighting their flexibility in modelling nonnormal features. The proposed constrained approach enhances interpretability and can improve parametric efficiency without compromising distributional flexibility for complex data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03285v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pierdomenico Duttilo, Stefano Antonio Gattone, Alfred Kume</dc:creator>
    </item>
    <item>
      <title>Urban Visibility Hotspots: Quantifying Building Vertex Visibility from Connected Vehicle Trajectories using Spatial Indexing</title>
      <link>https://arxiv.org/abs/2506.03365</link>
      <description>arXiv:2506.03365v1 Announce Type: cross 
Abstract: Effective placement of Out-of-Home advertising and street furniture requires accurate identification of locations offering maximum visual exposure to target audiences, particularly vehicular traffic. Traditional site selection methods often rely on static traffic counts or subjective assessments. This research introduces a data-driven methodology to objectively quantify location visibility by analyzing large-scale connected vehicle trajectory data (sourced from Compass IoT) within urban environments. We model the dynamic driver field-of-view using a forward-projected visibility area for each vehicle position derived from interpolated trajectories. By integrating this with building vertex locations extracted from OpenStreetMap, we quantify the cumulative visual exposure, or ``visibility count'', for thousands of potential points of interest near roadways. The analysis reveals that visibility is highly concentrated, identifying specific ``visual hotspots'' that receive disproportionately high exposure compared to average locations. The core technical contribution involves the construction of a BallTree spatial index over building vertices. This enables highly efficient (O(logN) complexity) radius queries to determine which vertices fall within the viewing circles of millions of trajectory points across numerous trips, significantly outperforming brute-force geometric checks. Analysis reveals two key findings: 1) Visibility is highly concentrated, identifying distinct 'visual hotspots' receiving disproportionately high exposure compared to average locations. 2) The aggregated visibility counts across vertices conform to a Log-Normal distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03365v1</guid>
      <category>eess.SY</category>
      <category>cs.CV</category>
      <category>cs.SY</category>
      <category>stat.CO</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Artur Grigorev, Adriana-Simona Mihaita</dc:creator>
    </item>
    <item>
      <title>N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion</title>
      <link>https://arxiv.org/abs/2506.04166</link>
      <description>arXiv:2506.04166v1 Announce Type: cross 
Abstract: Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix completion, offering strong empirical performance and recent theoretical guarantees, including entry-wise error bounds, confidence intervals, and minimax optimality. Despite their simplicity, recent work has shown that NN approaches are robust to a range of missingness patterns and effective across diverse applications. This paper introduces N$^2$, a unified Python package and testbed that consolidates a broad class of NN-based methods through a modular, extensible interface. Built for both researchers and practitioners, N$^2$ supports rapid experimentation and benchmarking. Using this framework, we introduce a new NN variant that achieves state-of-the-art results in several settings. We also release a benchmark suite of real-world datasets, from healthcare and recommender systems to causal inference and LLM evaluation, designed to stress-test matrix completion methods beyond synthetic scenarios. Our experiments demonstrate that while classical methods excel on idealized data, NN-based techniques consistently outperform them in real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04166v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caleb Chin, Aashish Khubchandani, Harshvardhan Maskara, Kyuseong Choi, Jacob Feitelberg, Albert Gong, Manit Paul, Tathagata Sadhukhan, Anish Agarwal, Raaz Dwivedi</dc:creator>
    </item>
    <item>
      <title>Least Squares Estimation For Hierarchical Data</title>
      <link>https://arxiv.org/abs/2404.13164</link>
      <description>arXiv:2404.13164v2 Announce Type: replace 
Abstract: The U.S. Census Bureau's 2020 Disclosure Avoidance System (DAS) bases its output on noisy measurements, which are population tabulations added to realizations of mean-zero random variables. These noisy measurements are observed for a set of hierarchical geographic units, e.g., the U.S. as a whole, states, counties, census tracts, and census blocks. The Census Bureau released the noisy measurements generated in the DAS executions for the two primary 2020 Census data products, in part to allow data users to assess uncertainty in 2020 Census tabulations introduced by disclosure avoidance. This paper describes an algorithm that can leverage a hierarchical structure of the input data in order to compute very high dimensional least squares estimates in a computationally efficient manner. Afterward, we show that this algorithm's output is equal to the generalized least squares estimator, describe how to find the variance of linear functions of this estimator, and provide a numerical experiment in which we compute confidence intervals of 2010 Census tabulations based on this estimator. We also describe an accompanying Census Bureau experimental data product that applies this estimator to the publicly available noisy measurements to provide data users with the inputs required to estimate confidence intervals for all tabulations that were included in one of the two main 2020 Census data products, i.e., the 2020 Redistricting Data Product, in the US, state, county, and census tract geographic levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13164v2</guid>
      <category>stat.CO</category>
      <category>cs.CR</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Cumings-Menon, Pavel Zhuravlev</dc:creator>
    </item>
  </channel>
</rss>
