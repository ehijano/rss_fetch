<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Oct 2024 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Block Vecchia Approximation for Scalable and Efficient Gaussian Process Computations</title>
      <link>https://arxiv.org/abs/2410.04477</link>
      <description>arXiv:2410.04477v1 Announce Type: new 
Abstract: Gaussian Processes (GPs) are vital for modeling and predicting irregularly-spaced, large geospatial datasets. However, their computations often pose significant challenges in large-scale applications. One popular method to approximate GPs is the Vecchia approximation, which approximates the full likelihood via a series of conditional probabilities. The classical Vecchia approximation uses univariate conditional distributions, which leads to redundant evaluations and memory burdens. To address this challenge, our study introduces block Vecchia, which evaluates each multivariate conditional distribution of a block of observations, with blocks formed using the K-means algorithm. The proposed GPU framework for the block Vecchia uses varying batched linear algebra operations to compute multivariate conditional distributions concurrently, notably diminishing the frequent likelihood evaluations. Diving into the factor affecting the accuracy of the block Vecchia, the neighbor selection criterion is investigated, where we found that the random ordering markedly enhances the approximated quality as the block count becomes large. To verify the scalability and efficiency of the algorithm, we conduct a series of numerical studies and simulations, demonstrating their practical utility and effectiveness compared to the exact GP. Moreover, we tackle large-scale real datasets using the block Vecchia method, i.e., high-resolution 3D profile wind speed with a million points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04477v1</guid>
      <category>stat.CO</category>
      <category>cs.CE</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qilong Pan, Sameh Abdulah, Marc G. Genton, Ying Sun</dc:creator>
    </item>
    <item>
      <title>Fast algorithm for sparse least trimmed squares via trimmed-regularized reformulation</title>
      <link>https://arxiv.org/abs/2410.04554</link>
      <description>arXiv:2410.04554v1 Announce Type: new 
Abstract: The least trimmed squares (LTS) is a reasonable formulation of robust regression whereas it suffers from high computational cost due to the nonconvexity and nonsmoothness of its objective function. The most frequently used FAST-LTS algorithm is particularly slow when a sparsity-inducing penalty such as the $\ell_1$ norm is added. This paper proposes a computationally inexpensive algorithm for the sparse LTS, which is based on the proximal gradient method with a reformulation technique. Proposed method is equipped with theoretical convergence preferred over existing methods. Numerical experiments show that our method efficiently yields small objective value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04554v1</guid>
      <category>stat.CO</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shotaro Yagishita</dc:creator>
    </item>
    <item>
      <title>Tables with Critical Values for the Meta-Analysis of Genuine and Fake $\boldsymbol{p}$-Values</title>
      <link>https://arxiv.org/abs/2410.04651</link>
      <description>arXiv:2410.04651v1 Announce Type: new 
Abstract: The classical theory for the meta-analysis of $p$-values is based on the assumption that if the overall null hypothesis is true, then all $p$-values used in a chosen combined test statistic are genuine, i.e., are observations from independent and identically distributed standard uniform random variables. However, the pressure felt by most researchers to publish, which is worsen by publication bias, can originate fake $p$-values to be reported, usually Beta(1,2) distributed. In general, the existence of fake $p$-values in a sample of $p$-values to be combined is unknown, and if, for some reason, there is information that they do exist, their number will most likely be unknown as well. Moreover, even if fake $p$-values are accounted for, the cumulative distribution function of classical combined test statistics does not have a closed-form expression that facilitates its practical usage. To overcome this problem, tables with estimated critical values are supplied for the commonly used combined tests for the meta-analysis of $p$-values when a few of them are fake ones, i.e., Beta(1,2) distributed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04651v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Santos, M. F\'atima Brilhante, Sandra Mendon\c{c}a</dc:creator>
    </item>
    <item>
      <title>Embrace rejection: Kernel matrix approximation by accelerated randomly pivoted Cholesky</title>
      <link>https://arxiv.org/abs/2410.03969</link>
      <description>arXiv:2410.03969v1 Announce Type: cross 
Abstract: Randomly pivoted Cholesky (RPCholesky) is an algorithm for constructing a low-rank approximation of a positive-semidefinite matrix using a small number of columns. This paper develops an accelerated version of RPCholesky that employs block matrix computations and rejection sampling to efficiently simulate the execution of the original algorithm. For the task of approximating a kernel matrix, the accelerated algorithm can run over $40\times$ faster. The paper contains implementation details, theoretical guarantees, experiments on benchmark data sets, and an application to computational chemistry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03969v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ethan N. Epperly, Joel A. Tropp, Robert J. Webber</dc:creator>
    </item>
    <item>
      <title>Nonparametric tests for interaction in two-way ANOVA with balanced replications</title>
      <link>https://arxiv.org/abs/2410.04700</link>
      <description>arXiv:2410.04700v1 Announce Type: cross 
Abstract: Nonparametric procedures are more powerful for detecting interaction in two-way ANOVA when the data are non-normal. In this paper, we compute null critical values for the aligned rank-based tests (APCSSA/APCSSM) where the levels of the factors are between 2 and 6. We compare the performance of these new procedures with the ANOVA F-test for interaction, the adjusted rank transform test (ART), Conover's rank transform procedure (RT), and a rank-based ANOVA test (raov) using Monte Carlo simulations. The new procedures APCSSA/APCSSM are comparable with existing competitors in all settings. Even though there is no single dominant test in detecting interaction effects for non-normal data, nonparametric procedure APCSSM is the most highly recommended procedure for Cauchy errors settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04700v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bao Khue Tran, Amy S. Wagaman, Andrew Nguyen, David Jacobson, Bradley Hartlaub</dc:creator>
    </item>
    <item>
      <title>Inference for the stochastic FitzHugh-Nagumo model from real action potential data via approximate Bayesian computation</title>
      <link>https://arxiv.org/abs/2405.17972</link>
      <description>arXiv:2405.17972v2 Announce Type: replace 
Abstract: The stochastic FitzHugh-Nagumo (FHN) model is a two-dimensional nonlinear stochastic differential equation with additive degenerate noise, whose first component, the only one observed, describes the membrane voltage evolution of a single neuron. Due to its low-dimensionality, its analytical and numerical tractability and its neuronal interpretation, it has been used as a case study to test the performance of different statistical methods in estimating the underlying model parameters. Existing methods, however, often require complete observations, non-degeneracy of the noise or a complex architecture (e.g., to estimate the transition density of the process, "recovering" the unobserved second component) and they may not (satisfactorily) estimate all model parameters simultaneously. Moreover, these studies lack real data applications for the stochastic FHN model. The proposed method tackles all challenges (non-globally Lipschitz drift, non-explicit solution, lack of available transition density, degeneracy of the noise and partial observations). It is an intuitive and easy-to-implement sequential Monte Carlo approximate Bayesian computation algorithm, which relies on a recent computationally efficient and structure-preserving numerical splitting scheme for synthetic data generation and on summary statistics exploiting the structural properties of the process. All model parameters are successfully estimated from simulated data and, more remarkably, real action potential data of rats. The presented novel real-data fit may broaden the scope and credibility of this classic and widely used neuronal model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17972v2</guid>
      <category>stat.CO</category>
      <category>math.DS</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adeline Samson, Massimiliano Tamborrino, Irene Tubikanec</dc:creator>
    </item>
    <item>
      <title>An engine to simulate insurance fraud network data</title>
      <link>https://arxiv.org/abs/2308.11659</link>
      <description>arXiv:2308.11659v2 Announce Type: replace-cross 
Abstract: Traditionally, the detection of fraudulent insurance claims relies on business rules and expert judgement which makes it a time-consuming and expensive process (\'Oskarsd\'ottir et al., 2022). Consequently, researchers have been examining ways to develop efficient and accurate analytic strategies to flag suspicious claims. Feeding learning methods with features engineered from the social network of parties involved in a claim is a particularly promising strategy (see for example Van Vlasselaer et al. (2016); Tumminello et al. (2023)). When developing a fraud detection model, however, we are confronted with several challenges. The uncommon nature of fraud, for example, creates a high class imbalance which complicates the development of well performing analytic classification models. In addition, only a small number of claims are investigated and get a label, which results in a large corpus of unlabeled data. Yet another challenge is the lack of publicly available data. This hinders not only the development of new methods, but also the validation of existing techniques. We therefore design a simulation machine that is engineered to create synthetic data with a network structure and available covariates similar to the real life insurance fraud data set analyzed in \'Oskarsd\'ottir et al. (2022). Further, the user has control over several data-generating mechanisms. We can specify the total number of policyholders and parties, the desired level of imbalance and the (effect size of the) features in the fraud generating model. As such, the simulation engine enables researchers and practitioners to examine several methodological challenges as well as to test their (development strategy of) insurance fraud detection models in a range of different settings. Moreover, large synthetic data sets can be generated to evaluate the predictive performance of (advanced) machine learning techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11659v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bavo D. C. Campo, Katrien Antonio</dc:creator>
    </item>
    <item>
      <title>A-Priori Reduction of Scenario Approximation for Automated Generation Control in High-Voltage Power Grids with Renewable Energy</title>
      <link>https://arxiv.org/abs/2310.02509</link>
      <description>arXiv:2310.02509v2 Announce Type: replace-cross 
Abstract: Renewable energy sources (RES) are increasingly integrated into power systems to support the United Nations' Sustainable Development Goals of decarbonization and energy security. However, their low inertia and high uncertainty pose challenges to grid stability and increase the risk of blackouts. Stochastic chance-constrained optimization, particularly data-driven methods, offers solutions but can be time-consuming, especially when handling multiple system snapshots. This paper addresses a dynamic joint chance-constrained Direct Current Optimal Power Flow (DC-OPF) problem with Automated Generation Control (AGC) to facilitate cost-effective power generation while ensuring that balance and security constraints are met. We propose an approach for a data-driven approximation that includes a priori sample reduction, maintaining solution reliability while reducing the size of the data-driven approximation. Both theoretical analysis and empirical results demonstrate the superiority of this approach in handling generation uncertainty, requiring up to twice less data while preserving solution reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02509v2</guid>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LCSYS.2024.3413367</arxiv:DOI>
      <arxiv:journal_reference>in IEEE Control Systems Letters, vol. 8, pp. 1613-1618, 2024</arxiv:journal_reference>
      <dc:creator>Aleksander Lukashevich, Aleksander Bulkin, Yury Maximov</dc:creator>
    </item>
    <item>
      <title>Granger Causality in High-Dimensional Networks of Time Series</title>
      <link>https://arxiv.org/abs/2406.02360</link>
      <description>arXiv:2406.02360v2 Announce Type: replace-cross 
Abstract: A novel approach is developed for discovering directed connectivity between specified pairs of nodes in a high-dimensional network (HDN) of brain signals. To accurately identify causal connectivity for such specified objectives, it is necessary to properly address the influence of all other nodes within the network. The proposed procedure herein starts with the estimation of a low-dimensional representation of the other nodes in the network utilizing (frequency-domain-based) spectral dynamic principal component analysis (sDPCA). The resulting scores can then be removed from the nodes of interest, thus eliminating the confounding effect of other nodes within the HDN. Accordingly, causal interactions can be dissected between nodes that are isolated from the effects of the network. Extensive simulations have demonstrated the effectiveness of this approach as a tool for causality analysis in complex time series networks. The proposed methodology has also been shown to be applicable to multichannel EEG networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02360v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
  </channel>
</rss>
