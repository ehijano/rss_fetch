<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Jun 2025 04:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Condensed Representation of RDF and its Application on Graph Versioning</title>
      <link>https://arxiv.org/abs/2506.21203</link>
      <description>arXiv:2506.21203v1 Announce Type: new 
Abstract: The study of the evolving phenomena in a domain helps to understand the relationships between entities at different points in time and predict future trends. These phenomena, often complex, can be represented using knowledge graphs, which have the capability to model heterogeneous data from multiple sources. Nowadays, a considerable amount of sources delivering periodic updates to knowledge graphs in various domains is openly available. The evolution of data is of interest to knowledge graph management systems, and therefore it is crucial to organize these constantly evolving data to make them easily accessible and exploitable for analyzes. In this article, we will present and formalize the condensed representation of these evolving graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21203v1</guid>
      <category>cs.DB</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jey Puget Gil, Emmanuel Coquery, John Samuel, Gilles Gesquiere</dc:creator>
    </item>
    <item>
      <title>Adaptive Hybrid Sort: Dynamic Strategy Selection for Optimal Sorting Across Diverse Data Distributions</title>
      <link>https://arxiv.org/abs/2506.20677</link>
      <description>arXiv:2506.20677v1 Announce Type: cross 
Abstract: Sorting is an essential operation in computer science with direct consequences on the performance of large scale data systems, real-time systems, and embedded computation. However, no sorting algorithm is optimal under all distributions of data. The new adaptive hybrid sorting paradigm proposed in this paper is the paradigm that automatically selects the most effective sorting algorithm Counting Sort, Radix Sort, or QuickSort based on real-time monitoring of patterns in input data. The architecture begins by having a feature extraction module to compute significant parameters such as data volume, value range and entropy. These parameters are sent to a decision engine involving Finite State Machine and XGBoost classifier to aid smart and effective in choosing the optimal sorting strategy. It implements Counting Sort on small key ranges, Radix Sort on large range structured input with low-entropy keys and QuickSort on general purpose sorting. The experimental findings of both synthetic and real life dataset confirm that the proposed solution is actually inclined to excel significantly by comparison in execution time, flexibility and the efficiency of conventional static sorting algorithms. The proposed framework provides a scalable, high perhaps and applicable to a wide range of data processing operations like big data analytics, edge computing, and systems with hardware limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20677v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.PF</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shrinivass Arunachalam Balasubramanian</dc:creator>
    </item>
    <item>
      <title>Practical and Accurate Local Edge Differentially Private Graph Algorithms</title>
      <link>https://arxiv.org/abs/2506.20828</link>
      <description>arXiv:2506.20828v1 Announce Type: cross 
Abstract: The rise of massive networks across diverse domains necessitates sophisticated graph analytics, often involving sensitive data and raising privacy concerns. This paper addresses these challenges using local differential privacy (LDP), which enforces privacy at the individual level, where no third-party entity is trusted, unlike centralized models that assume a trusted curator. We introduce novel LDP algorithms for two fundamental graph statistics: k-core decomposition and triangle counting. Our approach leverages input-dependent private graph properties, specifically the degeneracy and maximum degree of the graph, to improve theoretical utility. Unlike prior methods, our error bounds are determined by the maximum degree rather than the total number of edges, resulting in significantly tighter guarantees. For triangle counting, we improve upon the work of Imola, Murakami, and Chaudhury~\cite{IMC21locally, IMC21communication}, which bounds error in terms of edge count. Instead, our algorithm achieves bounds based on graph degeneracy by leveraging a private out-degree orientation, a refined variant of Eden et al.'s randomized response technique~\cite{ELRS23, and a novel analysis, yielding stronger guarantees than prior work. Beyond theoretical gains, we are the first to evaluate local DP algorithms in a distributed simulation, unlike prior work tested on a single processor. Experiments on real-world graphs show substantial accuracy gains: our k-core decomposition achieves errors within 3x of exact values, far outperforming the 131x error in the baseline of Dhulipala et al.~\cite{DLRSSY22}. Our triangle counting algorithm reduces multiplicative approximation errors by up to six orders of magnitude, while maintaining competitive runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20828v1</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pranay Mundra, Charalampos Papamanthou, Julian Shun, Quanquan C. Liu</dc:creator>
    </item>
    <item>
      <title>Generating Reliable Adverse event Profiles for Health through Automated Integrated Data (GRAPH-AID): A Semi-Automated Ontology Building Approach</title>
      <link>https://arxiv.org/abs/2506.20851</link>
      <description>arXiv:2506.20851v1 Announce Type: cross 
Abstract: As data and knowledge expand rapidly, adopting systematic methodologies for ontology generation has become crucial. With the daily increases in data volumes and frequent content changes, the demand for databases to store and retrieve information for the creation of knowledge graphs has become increasingly urgent. The previously established Knowledge Acquisition and Representation Methodology (KNARM) outlines a systematic approach to address these challenges and create knowledge graphs. However, following this methodology highlights the existing challenge of seamlessly integrating Neo4j databases with the Web Ontology Language (OWL). Previous attempts to integrate data from Neo4j into an ontology have been discussed, but these approaches often require an understanding of description logics (DL) syntax, which may not be familiar to many users. Thus, a more accessible method is necessary to bridge this gap. This paper presents a user-friendly approach that utilizes Python and its rdflib library to support ontology development. We showcase our novel approach through a Neo4j database we created by integrating data from the Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS) database. Using this dataset, we developed a Python script that automatically generates the required classes and their axioms, facilitating a smoother integration process. This approach offers a practical solution to the challenges of ontology generation in the context of rapidly growing adverse drug event datasets, supporting improved drug safety monitoring and public health decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20851v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Srikar Reddy Gadusu, Larry Callahan, Samir Lababidi, Arunasri Nishtala, Sophia Healey, Hande McGinty</dc:creator>
    </item>
    <item>
      <title>Reducing Biases in Record Matching Through Scores Calibration</title>
      <link>https://arxiv.org/abs/2411.01685</link>
      <description>arXiv:2411.01685v2 Announce Type: replace-cross 
Abstract: Record matching is the task of identifying records that refer to the same real-world entity across datasets. While most existing models optimize for accuracy, fairness has become an important concern due to the potential for unequal outcomes across demographic groups. Prior work typically focuses on binary outcomes evaluated at fixed decision thresholds. However, such evaluations can miss biases in matching scores--biases that persist across thresholds and affect downstream tasks. We propose a threshold-independent framework for measuring and reducing score bias, defined as disparities in the distribution of matching scores across groups. We show that several state-of-the-art matching methods exhibit substantial score bias, even when appearing fair under standard threshold-based metrics. To address this, we introduce two post-processing score calibration algorithms. The first, calib, aligns group-wise score distributions using the Wasserstein barycenter, targeting demographic parity. The second, ccalib, conditions on predicted labels to further reduce label-dependent biases, such as equal opportunity. Both methods are model-agnostic and require no access to model training data. calib also offers theoretical guarantees, ensuring reduced bias with minimal deviation from original scores. Experiments across real-world datasets and matching models confirm that calib and ccalib substantially reduce score bias while minimally impacting model accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01685v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Hossein Moslemi, Mostafa Milani</dc:creator>
    </item>
    <item>
      <title>Localized RETE for Incremental Graph Queries with Nested Graph Conditions</title>
      <link>https://arxiv.org/abs/2412.13918</link>
      <description>arXiv:2412.13918v2 Announce Type: replace-cross 
Abstract: The growing size of graph-based modeling artifacts in model-driven engineering calls for techniques that enable efficient execution of graph queries. Incremental approaches based on the RETE algorithm provide an adequate solution in many scenarios, but are generally designed to search for query results over the entire graph. However, in certain situations, a user may only be interested in query results for a subgraph, for instance when a developer is working on a large model of which only a part is loaded into their workspace. In this case, the global execution semantics can result in significant computational overhead.
  To mitigate the outlined shortcoming, in this article we propose an extension of the RETE approach that enables local, yet fully incremental execution of graph queries, while still guaranteeing completeness of results with respect to the relevant subgraph.
  We empirically evaluate the presented approach via experiments inspired by a scenario from software development and with queries and data from an independent social network benchmark. The experimental results indicate that the proposed technique can significantly improve performance regarding memory consumption and execution time in favorable cases, but may incur a noticeable overhead in unfavorable cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13918v2</guid>
      <category>cs.LO</category>
      <category>cs.DB</category>
      <pubDate>Fri, 27 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Barkowsky, Holger Giese</dc:creator>
    </item>
  </channel>
</rss>
