<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>SplitLLM: Hierarchical Split Learning for Large Language Model over Wireless Network</title>
      <link>https://arxiv.org/abs/2501.13318</link>
      <description>arXiv:2501.13318v1 Announce Type: new 
Abstract: Fine-tuning a large language model (LLM) using the local data of edge users can enable personalized services and applications. For privacy protection, the prevalent solution adopts distributed learning for fine-tuning and integrates low-rank adaptation (LoRA) to reduce users' computational load. However, as the number of users increases, numerous users simultaneously communicate with the server, and multiple server-side models concurrently execute on the server, leading to significant communication congestion and memory pressure. In this paper, we propose a split learning (SL) scheme for fine-tuning LLM in wireless networks, which involves one cloud server, a small number of edge servers, and multiple users. Specifically, the pre-trained model and LoRA adapters are divided into three parts and deployed across the cloud, edge, and user sides. The training process follows the sequence of user, edge, and cloud, with forward and backward propagation achieved by transmitting activation and gradient. In each round, all edge servers and an equivalent number of users train in parallel, and only the LoRA adapters are updated. At the end of each round, all edge-side and user-side LoRA adapters are uploaded to the cloud for aggregation. Extensive simulation demonstrates that the proposed scheme can reduce peak memory usage up to 74% compared to the state-of-the-art benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13318v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songge Zhang, Guoliang Cheng, Zuguang Li, Wen Wu</dc:creator>
    </item>
    <item>
      <title>Advancing ATLAS DCS Data Analysis with a Modern Data Platform</title>
      <link>https://arxiv.org/abs/2501.13543</link>
      <description>arXiv:2501.13543v1 Announce Type: new 
Abstract: This paper presents a modern and scalable framework for analyzing Detector Control System (DCS) data from the ATLAS experiment at CERN. The DCS data, stored in an Oracle database via the WinCC OA system, is optimized for transactional operations, posing challenges for large-scale analysis across extensive time periods and devices. To address these limitations, we developed a data pipeline using Apache Spark, CERN's Hadoop service, and the CERN SWAN platform. This framework integrates seamlessly with Python notebooks, providing an accessible and efficient environment for data analysis using industry-standard tools. The approach has proven effective in troubleshooting Data Acquisition (DAQ) links for the ATLAS New Small Wheel (NSW) detector, demonstrating the value of modern data platforms in enabling detector experts to quickly identify and resolve critical issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13543v1</guid>
      <category>cs.DC</category>
      <category>hep-ex</category>
      <category>physics.ins-det</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Canali, Andrea Formica, Michelle Ann Solis</dc:creator>
    </item>
    <item>
      <title>Accelerating Gaussian beam tracing method with dynamic parallelism on graphics processing units</title>
      <link>https://arxiv.org/abs/2501.13382</link>
      <description>arXiv:2501.13382v1 Announce Type: cross 
Abstract: This study presents a reconstruction of the Gaussian Beam Tracing solution using CUDA, with a particular focus on the utilisation of GPU acceleration as a means of overcoming the performance limitations of traditional CPU algorithms in complex acoustic simulations. The algorithm is implemented and optimised on the NVIDIA RTX A6000 GPU, resulting in a notable enhancement in the performance of the Gaussian Beam Summation (GBS) process. In particular, the GPU-accelerated GBS algorithm demonstrated a significant enhancement in performance, reaching up to 790 times faster in city enviroment and 188 times faster in open plane enviroment compared to the original CPU-based program. To address the challenges of acceleration, the study introduce innovative solutions for handling irregular loops and GPU memory limitations, ensuring the efficient processing of large quantities of rays beyond the GPU's single-process capacity. Furthermore, this work established performance evaluation strategies crucial for analysing and reconstructing similar algorithms. Additionally, the study explored future directions for further accelerating the algorithm, laying the groundwork for ongoing improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13382v1</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhang Sheng, Lishu Duan, Hanbo Jiang</dc:creator>
    </item>
    <item>
      <title>Billion-scale Similarity Search Using a Hybrid Indexing Approach with Advanced Filtering</title>
      <link>https://arxiv.org/abs/2501.13442</link>
      <description>arXiv:2501.13442v1 Announce Type: cross 
Abstract: This paper presents a novel approach for similarity search with complex filtering capabilities on billion-scale datasets, optimized for CPU inference. Our method extends the classical IVF-Flat index structure to integrate multi-dimensional filters. The proposed algorithm combines dense embeddings with discrete filtering attributes, enabling fast retrieval in high-dimensional spaces. Designed specifically for CPU-based systems, our disk-based approach offers a cost-effective solution for large-scale similarity search. We demonstrate the effectiveness of our method through a case study, showcasing its potential for various practical uses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13442v1</guid>
      <category>cs.IR</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.2478/cait-2024-0035</arxiv:DOI>
      <arxiv:journal_reference>Cybernetics and Information Technologies, Vol. 24, No 4 (2024), pp. 45-58</arxiv:journal_reference>
      <dc:creator>Simeon Emanuilov, Aleksandar Dimov</dc:creator>
    </item>
    <item>
      <title>Network Centrality as a New Perspective on Microservice Architecture</title>
      <link>https://arxiv.org/abs/2501.13520</link>
      <description>arXiv:2501.13520v1 Announce Type: cross 
Abstract: Context: Over the past decade, the adoption of Microservice Architecture (MSA) has led to the identification of various patterns and anti-patterns, such as Nano/Mega/Hub services. Detecting these anti-patterns often involves modeling the system as a Service Dependency Graph (SDG) and applying graph-theoretic approaches. Aim: While previous research has explored software metrics (SMs) such as size, complexity, and quality for assessing MSAs, the potential of graph-specific metrics like network centrality remains largely unexplored. This study investigates whether centrality metrics (CMs) can provide new insights into MSA quality and facilitate the detection of architectural anti-patterns, complementing or extending traditional SMs. Method: We analyzed 24 open-source MSA projects, reconstructing their architectures to study 53 microservices. We measured SMs and CMs for each microservice and tested their correlation to determine the relationship between these metric types. Results and Conclusion: Among 902 computed metric correlations, we found weak to moderate correlation in 282 cases. These findings suggest that centrality metrics offer a novel perspective for understanding MSA properties. Specifically, ratio-based centrality metrics show promise for detecting specific anti-patterns, while subgraph centrality needs further investigation for its applicability in architectural assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13520v1</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Bakhtin, Matteo Esposito, Valentina Lenarduzzi, Davide Taibi</dc:creator>
    </item>
    <item>
      <title>Compiler Support for Speculation in Decoupled Access/Execute Architectures</title>
      <link>https://arxiv.org/abs/2501.13553</link>
      <description>arXiv:2501.13553v1 Announce Type: cross 
Abstract: Irregular codes are bottlenecked by memory and communication latency. Decoupled access/execute (DAE) is a common technique to tackle this problem. It relies on the compiler to separate memory address generation from the rest of the program, however, such a separation is not always possible due to control and data dependencies between the access and execute slices, resulting in a loss of decoupling.
  In this paper, we present compiler support for speculation in DAE architectures that preserves decoupling in the face of control dependencies. We speculate memory requests in the access slice and poison mis-speculations in the execute slice without the need for replays or synchronization. Our transformation works on arbitrary, reducible control flow and is proven to preserve sequential consistency. We show that our approach applies to a wide range of architectural work on CPU/GPU prefetchers, CGRAs, and accelerators, enabling DAE on a wider range of codes than before.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13553v1</guid>
      <category>cs.PF</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3708493.3712695</arxiv:DOI>
      <dc:creator>Robert Szafarczyk, Syed Waqar Nabi, Wim Vanderbauwhede</dc:creator>
    </item>
    <item>
      <title>FedPref: Federated Learning Across Heterogeneous Multi-objective Preferences</title>
      <link>https://arxiv.org/abs/2501.13604</link>
      <description>arXiv:2501.13604v1 Announce Type: cross 
Abstract: Federated Learning (FL) is a distributed machine learning strategy, developed for settings where training data is owned by distributed devices and cannot be shared. FL circumvents this constraint by carrying out model training in distribution. The parameters of these local models are shared intermittently among participants and aggregated to enhance model accuracy. This strategy has been rapidly adopted by the industry in efforts to overcome privacy and resource constraints in model training. However, the application of FL to real-world settings brings additional challenges associated with heterogeneity between participants. Research into mitigating these difficulties in FL has largely focused on only two types of heterogeneity: the unbalanced distribution of training data, and differences in client resources. Yet more types of heterogeneity are becoming relevant as the capability of FL expands to cover more complex problems, from the tuning of LLMs to enabling machine learning on edge devices. In this work, we discuss a novel type of heterogeneity that is likely to become increasingly relevant in future applications: this is preference heterogeneity, emerging when clients learn under multiple objectives, with different importance assigned to each objective on different clients. In this work, we discuss the implications of this type of heterogeneity and propose FedPref, a first algorithm designed to facilitate personalised FL in this setting. We demonstrate the effectiveness of the algorithm across different problems, preference distributions and model architectures. In addition, we introduce a new analytical point of view, based on multi-objective metrics, for evaluating the performance of FL algorithms in this setting beyond the traditional client-focused metrics. We perform a second experimental analysis based in this view, and show that FedPref outperforms compared algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13604v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3708984</arxiv:DOI>
      <dc:creator>Maria Hartmann, Gr\'egoire Danoy, Pascal Bouvry</dc:creator>
    </item>
    <item>
      <title>Aggregating Digital Identities through Bridging. An Integration of Open Authentication Protocols for Web3 Identifiers</title>
      <link>https://arxiv.org/abs/2501.13770</link>
      <description>arXiv:2501.13770v1 Announce Type: cross 
Abstract: Web3's decentralised infrastructure has upended the standardised approach to digital identity established by protocols like OpenID Connect. Web2 and Web3 currently operate in silos, with Web2 leveraging selective disclosure JSON web tokens (SD-JWTs) and Web3 dApps being reliant on on-chain data and sometimes clinging to centralised system data. This fragmentation hinders user experience and the interconnectedness of the digital world. This paper explores the integration of Web3 within the OpenID Connect framework, scrutinising established authentication protocols for their adaptability to decentralised identities. The research examines the interplay between OpenID Connect and decentralised identity concepts, the limitations of existing protocols like OpenID Connect for verifiable credential issuance, OpenID Connect framework for verifiable presentations, and self-issued OpenID provider. As a result, a novel privacy-preserving digital identity bridge is proposed, which aims to answer the research question of whether authentication protocols should inherently support Web3 functionalities and the mechanisms for their integration. Through a Decentralised Autonomous Organisation (DAO) use case, the findings indicate that a privacy-centric bridge can mitigate existing fragmentation by aggregating different identities to provide a better user experience. While the digital identity bridge demonstrates a possible approach to harmonise digital identity across platforms for their use in Web3, the bridging is unidirectional and limits root trust of credentials. The bridge's dependence on centralised systems may further fuel the debate on (de-)centralised identities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13770v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ben Biedermann, Matthew Scerri, Victoria Kozlova, Joshua Ellul</dc:creator>
    </item>
    <item>
      <title>Affinity-aware Serverless Function Scheduling</title>
      <link>https://arxiv.org/abs/2407.14572</link>
      <description>arXiv:2407.14572v2 Announce Type: replace 
Abstract: Functions-as-a-Service (FaaS) is a Serverless Cloud paradigm where a platform manages the scheduling (e.g., resource allocation, runtime environments) of stateless functions. Recent work proposed using domain-specific languages to express per-function policies, e.g., policies that enforce the allocation on nodes that enjoy lower latencies to databases and services used by the function. Here, we focus on affinity-aware scenarios, i.e., where, for performance and functional requirements, the allocation of a function depends on the presence/absence of other functions on nodes.
  We present aAPP, an extension of a declarative, platform-agnostic language that captures affinity-aware scheduling at the FaaS level. We implement an aAPP-based prototype on Apache OpenWhisk. Besides proving that a FaaS platform can capture affinity awareness using aAPP and improve performance in affinity-aware scenarios, we use our prototype to show that aAPP imposes no noticeable overhead in scenarios without affinity constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14572v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe De Palma, Saverio Giallorenzo, Jacopo Mauro, Matteo Trentin, Gianluigi Zavattaro</dc:creator>
    </item>
    <item>
      <title>The Streaming Batch Model for Efficient and Fault-Tolerant Heterogeneous Execution</title>
      <link>https://arxiv.org/abs/2501.12407</link>
      <description>arXiv:2501.12407v2 Announce Type: replace 
Abstract: While ML model training and inference are both GPU-intensive, CPU-based data processing is often the bottleneck. Distributed data processing systems based on the batch or stream processing models assume homogeneous resource requirements. They excel at CPU-based computation but either under-utilize heterogeneous resources or impose high overheads on failure and reconfiguration. We introduce the streaming batch model, a hybrid of the two models that enables efficient and fault-tolerant heterogeneous execution. The key idea is to execute one partition at a time to allow lineage-based recovery with dynamic resource allocation. This enables memory-efficient pipelining across heterogeneous resources, similar to stream processing, but also offers the elasticity and fault tolerance properties of batch processing. We present Ray Data, an implementation of the streaming batch model that improves throughput on heterogeneous batch inference pipelines by 3--8$\times$ compared to traditional batch and stream processing systems. When training Stable Diffusion, Ray Data matches the throughput of single-node ML data loaders while additionally leveraging distributed heterogeneous clusters to further improve training throughput by 31%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12407v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Frank Sifei Luan, Ziming Mao, Ron Yifeng Wang, Charlotte Lin, Amog Kamsetty, Hao Chen, Cheng Su, Balaji Veeramani, Scott Lee, SangBin Cho, Clark Zinzow, Eric Liang, Ion Stoica, Stephanie Wang</dc:creator>
    </item>
    <item>
      <title>A Training-free Sub-quadratic Cost Transformer Model Serving Framework With Hierarchically Pruned Attention</title>
      <link>https://arxiv.org/abs/2406.09827</link>
      <description>arXiv:2406.09827v3 Announce Type: replace-cross 
Abstract: In modern large language models (LLMs), increasing the context length is crucial for improving comprehension and coherence in long-context, multi-modal, and retrieval-augmented language generation. While many recent transformer models attempt to extend their context length over a million tokens, they remain impractical due to the quadratic time and space complexities. Although recent works on linear and sparse attention mechanisms can achieve this goal, their real-world applicability is often limited by the need to re-train from scratch and significantly worse performance. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which reduces the time complexity of the attention mechanism to $O(T \log T)$ and the space complexity to $O(T)$, where $T$ is the sequence length. We notice a pattern in the attention scores of pretrained LLMs where tokens close together tend to have similar scores, which we call ``attention locality''. Based on this observation, we utilize a novel tree-search-like algorithm that estimates the top-$k$ key tokens for a given query on the fly, which is mathematically guaranteed to have better performance than random attention pruning. In addition to improving the time complexity of the attention mechanism, we further optimize GPU memory usage by implementing KV cache offloading, which stores only $O(\log T)$ tokens on the GPU while maintaining similar decoding throughput. Experiments on benchmarks show that HiP, with its training-free nature, significantly reduces both prefill and decoding latencies, as well as memory usage, while maintaining high-quality generation with minimal degradation. HiP enables pretrained LLMs to scale up to millions of tokens on commodity GPUs, potentially unlocking long-context LLM applications previously deemed infeasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09827v3</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Heejun Lee, Geon Park, Youngwan Lee, Jaduk Suh, Jina Kim, Wonyoung Jeong, Bumsik Kim, Hyemin Lee, Myeongjae Jeon, Sung Ju Hwang</dc:creator>
    </item>
    <item>
      <title>Ordered Momentum for Asynchronous SGD</title>
      <link>https://arxiv.org/abs/2407.19234</link>
      <description>arXiv:2407.19234v3 Announce Type: replace-cross 
Abstract: Distributed learning is essential for training large-scale deep models. Asynchronous SGD (ASGD) and its variants are commonly used distributed learning methods, particularly in scenarios where the computing capabilities of workers in the cluster are heterogeneous. Momentum has been acknowledged for its benefits in both optimization and generalization in deep model training. However, existing works have found that naively incorporating momentum into ASGD can impede the convergence. In this paper, we propose a novel method called ordered momentum (OrMo) for ASGD. In OrMo, momentum is incorporated into ASGD by organizing the gradients in order based on their iteration indexes. We theoretically prove the convergence of OrMo with both constant and delay-adaptive learning rates for non-convex problems. To the best of our knowledge, this is the first work to establish the convergence analysis of ASGD with momentum without dependence on the maximum delay. Empirical results demonstrate that OrMo can achieve better convergence performance compared with ASGD and other asynchronous methods with momentum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19234v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang-Wei Shi, Yi-Rui Yang, Wu-Jun Li</dc:creator>
    </item>
    <item>
      <title>IPMN Risk Assessment under Federated Learning Paradigm</title>
      <link>https://arxiv.org/abs/2411.05697</link>
      <description>arXiv:2411.05697v2 Announce Type: replace-cross 
Abstract: Accurate classification of Intraductal Papillary Mucinous Neoplasms (IPMN) is essential for identifying high-risk cases that require timely intervention. In this study, we develop a federated learning framework for multi-center IPMN classification utilizing a comprehensive pancreas MRI dataset. This dataset includes 652 T1-weighted and 655 T2-weighted MRI images, accompanied by corresponding IPMN risk scores from 7 leading medical institutions, making it the largest and most diverse dataset for IPMN classification to date. We assess the performance of DenseNet-121 in both centralized and federated settings for training on distributed data. Our results demonstrate that the federated learning approach achieves high classification accuracy comparable to centralized learning while ensuring data privacy across institutions. This work marks a significant advancement in collaborative IPMN classification, facilitating secure and high-accuracy model training across multiple centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05697v2</guid>
      <category>eess.IV</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongyi Pan, Ziliang Hong, Gorkem Durak, Elif Keles, Halil Ertugrul Aktas, Yavuz Taktak, Alpay Medetalibeyoglu, Zheyuan Zhang, Yury Velichko, Concetto Spampinato, Ivo Schoots, Marco J. Bruno, Pallavi Tiwari, Candice Bolan, Tamas Gonda, Frank Miller, Rajesh N. Keswani, Michael B. Wallace, Ziyue Xu, Ulas Bagci</dc:creator>
    </item>
    <item>
      <title>Pushing the Limit: Verified Performance-Optimal Causally-Consistent Database Transactions</title>
      <link>https://arxiv.org/abs/2411.07049</link>
      <description>arXiv:2411.07049v2 Announce Type: replace-cross 
Abstract: Modern web services crucially rely on high-performance distributed databases, where concurrent transactions are isolated from each other using concurrency control protocols. Relaxed isolation levels, which permit more complex concurrent behaviors than strong levels like serializability, are used in practice for higher performance and availability.
  In this paper, we present Eiger-PORT+, a concurrency control protocol that achieves a strong form of causal consistency, called TCCv (Transactional Causal Consistency with convergence). We show that Eiger-PORT+ also provides performance-optimal read transactions in the presence of transactional writes, thus refuting an open conjecture that this is impossible for TCCv. We also deductively verify that Eiger-PORT+ satisfies this isolation level by refining an abstract model of transactions. This yields the first deductive verification of a complex concurrency control protocol. Furthermore, we conduct a performance evaluation showing Eiger-PORT+'s superior performance over the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07049v2</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shabnam Ghasemirad, Christoph Sprenger, Si Liu, Luca Multazzu, David Basin</dc:creator>
    </item>
    <item>
      <title>FedGrAINS: Personalized SubGraph Federated Learning with Adaptive Neighbor Sampling</title>
      <link>https://arxiv.org/abs/2501.12592</link>
      <description>arXiv:2501.12592v2 Announce Type: replace-cross 
Abstract: Graphs are crucial for modeling relational and biological data. As datasets grow larger in real-world scenarios, the risk of exposing sensitive information increases, making privacy-preserving training methods like federated learning (FL) essential to ensure data security and compliance with privacy regulations. Recently proposed personalized subgraph FL methods have become the de-facto standard for training personalized Graph Neural Networks (GNNs) in a federated manner while dealing with the missing links across clients' subgraphs due to privacy restrictions. However, personalized subgraph FL faces significant challenges due to the heterogeneity in client subgraphs, such as degree distributions among the nodes, which complicate federated training of graph models. To address these challenges, we propose \textit{FedGrAINS}, a novel data-adaptive and sampling-based regularization method for subgraph FL. FedGrAINS leverages generative flow networks (GFlowNets) to evaluate node importance concerning clients' tasks, dynamically adjusting the message-passing step in clients' GNNs. This adaptation reflects task-optimized sampling aligned with a trajectory balance objective. Experimental results demonstrate that the inclusion of \textit{FedGrAINS} as a regularizer consistently improves the FL performance compared to baselines that do not leverage such regularization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12592v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.IR</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emir Ceyani, Han Xie, Baturalp Buyukates, Carl Yang, Salman Avestimehr</dc:creator>
    </item>
  </channel>
</rss>
