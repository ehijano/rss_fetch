<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Jun 2024 01:45:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multifractality in monitored single-particle dynamics</title>
      <link>https://arxiv.org/abs/2406.02386</link>
      <description>arXiv:2406.02386v1 Announce Type: cross 
Abstract: We study multifractal properties in time evolution of a single particle subject to repeated measurements. For quantum systems, we consider circuit models consisting of local unitary gates and local projective measurements. For classical systems, we consider models for estimating the trajectory of a particle evolved under local transition processes by partially measuring particle occupations. In both cases, multifractal behaviors appear in the ensemble of wave functions or probability distributions conditioned on measurement outcomes after a sufficiently long time. While the nature of particle transport (diffusive or ballistic) qualitatively affects the multifractal properties, they are even quantitatively robust to the measurement rate or specific protocols. On the other hand, multifractality is generically lost by generalized measurements allowing erroneous outcomes or by postselection of the outcomes with no particle detection. We demonstrate these properties by numerical simulations and also propose several simplified models, which allow us to analytically obtain multifractal properties in the monitored single-particle systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02386v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kohei Yajima, Hisanori Oshima, Ken Mochizuki, Yohei Fuji</dc:creator>
    </item>
    <item>
      <title>Symmetric Kernels with Non-Symmetric Data: A Data-Agnostic Learnability Bound</title>
      <link>https://arxiv.org/abs/2406.02663</link>
      <description>arXiv:2406.02663v1 Announce Type: cross 
Abstract: Kernel ridge regression (KRR) and Gaussian processes (GPs) are fundamental tools in statistics and machine learning with recent applications to highly over-parameterized deep neural networks. The ability of these tools to learn a target function is directly related to the eigenvalues of their kernel sampled on the input data. Targets having support on higher eigenvalues are more learnable. While kernels are often highly symmetric objects, the data is often not. Thus kernel symmetry seems to have little to no bearing on the above eigenvalues or learnability, making spectral analysis on real-world data challenging. Here, we show that contrary to this common lure, one may use eigenvalues and eigenfunctions associated with highly idealized data-measures to bound learnability on realistic data. As a demonstration, we give a theoretical lower bound on the sample complexity of copying heads for kernels associated with generic transformers acting on natural language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02663v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Itay Lavie, Zohar Ringel</dc:creator>
    </item>
    <item>
      <title>Feature learning in finite-width Bayesian deep linear networks with multiple outputs and convolutional layers</title>
      <link>https://arxiv.org/abs/2406.03260</link>
      <description>arXiv:2406.03260v1 Announce Type: cross 
Abstract: Deep linear networks have been extensively studied, as they provide simplified models of deep learning. However, little is known in the case of finite-width architectures with multiple outputs and convolutional layers. In this manuscript, we provide rigorous results for the statistics of functions implemented by the aforementioned class of networks, thus moving closer to a complete characterization of feature learning in the Bayesian setting. Our results include: (i) an exact and elementary non-asymptotic integral representation for the joint prior distribution over the outputs, given in terms of a mixture of Gaussians; (ii) an analytical formula for the posterior distribution in the case of squared error loss function (Gaussian likelihood); (iii) a quantitative description of the feature learning infinite-width regime, using large deviation theory. From a physical perspective, deep architectures with multiple outputs or convolutional layers represent different manifestations of kernel shape renormalization, and our work provides a dictionary that translates this physics intuition and terminology into rigorous Bayesian statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03260v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Bassetti, Marco Gherardi, Alessandro Ingrosso, Mauro Pastore, Pietro Rotondo</dc:creator>
    </item>
    <item>
      <title>Paths towards time evolution with larger neural-network quantum states</title>
      <link>https://arxiv.org/abs/2406.03381</link>
      <description>arXiv:2406.03381v1 Announce Type: cross 
Abstract: In recent years, the neural-network quantum states method has been investigated to study the ground state and the time evolution of many-body quantum systems. Here we expand on the investigation and consider a quantum quench from the paramagnetic to the anti-ferromagnetic phase in the tilted Ising model. We use two types of neural networks, a restricted Boltzmann machine and a feed-forward neural network. We show that for both types of networks, the projected time-dependent variational Monte Carlo (p-tVMC) method performs better than the non-projected approach. We further demonstrate that one can use K-FAC or minSR in conjunction with p-tVMC to reduce the computational complexity of the stochastic reconfiguration approach, thus allowing the use of these techniques for neural networks with more parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03381v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.quant-gas</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxuan Zhang, Bo Xing, Xiansong Xu, Dario Poletti</dc:creator>
    </item>
    <item>
      <title>Grokking Modular Polynomials</title>
      <link>https://arxiv.org/abs/2406.03495</link>
      <description>arXiv:2406.03495v1 Announce Type: cross 
Abstract: Neural networks readily learn a subset of the modular arithmetic tasks, while failing to generalize on the rest. This limitation remains unmoved by the choice of architecture and training strategies. On the other hand, an analytical solution for the weights of Multi-layer Perceptron (MLP) networks that generalize on the modular addition task is known in the literature. In this work, we (i) extend the class of analytical solutions to include modular multiplication as well as modular addition with many terms. Additionally, we show that real networks trained on these datasets learn similar solutions upon generalization (grokking). (ii) We combine these "expert" solutions to construct networks that generalize on arbitrary modular polynomials. (iii) We hypothesize a classification of modular polynomials into learnable and non-learnable via neural networks training; and provide experimental evidence supporting our claims.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03495v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>hep-th</category>
      <category>math.NT</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Darshil Doshi, Tianyu He, Aritra Das, Andrey Gromov</dc:creator>
    </item>
    <item>
      <title>The Copycat Perceptron: Smashing Barriers Through Collective Learning</title>
      <link>https://arxiv.org/abs/2308.03743</link>
      <description>arXiv:2308.03743v3 Announce Type: replace 
Abstract: We characterize the equilibrium properties of a model of $y$ coupled binary perceptrons in the teacher-student scenario, subject to a suitable cost function, with an explicit ferromagnetic coupling proportional to the Hamming distance between the students' weights. In contrast to recent works, we analyze a more general setting in which thermal noise is present that affects each student's generalization performance. In the nonzero temperature regime, we find that the coupling of replicas leads to a bend of the phase diagram towards smaller values of $\alpha$: This suggests that the free entropy landscape gets smoother around the solution with perfect generalization (i.e., the teacher) at a fixed fraction of examples, allowing standard thermal updating algorithms such as Simulated Annealing to easily reach the teacher solution and avoid getting trapped in metastable states as it happens in the unreplicated case, even in the computationally \textit{easy} regime of the inference phase diagram. These results provide additional analytic and numerical evidence for the recently conjectured Bayes-optimal property of Replicated Simulated Annealing (RSA) for a sufficient number of replicas. From a learning perspective, these results also suggest that multiple students working together (in this case reviewing the same data) are able to learn the same rule both significantly faster and with fewer examples, a property that could be exploited in the context of cooperative and federated learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.03743v3</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giovanni Catania, Aur\'elien Decelle, Beatriz Seoane</dc:creator>
    </item>
    <item>
      <title>The Spectral Boundary of Block Structured Random Matrices</title>
      <link>https://arxiv.org/abs/2312.11149</link>
      <description>arXiv:2312.11149v2 Announce Type: replace 
Abstract: Economic and ecological models can be extremely complex, with a large number of agents/species each featuring multiple interacting dynamical quantities. In an attempt to understand the generic stability properties of such systems, we define and study an interesting new matrix ensemble with extensive correlations, generalising the elliptic ensemble. We determine analytically the boundary of its eigenvalue spectrum in the complex plane, as a function of the correlations determined by the model at hand. We solve numerically our equations in several cases of interest, and show that the resulting spectra can take a surprisingly wide variety of shapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11149v2</guid>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nirbhay Patil, Fabian Aguirre-Lopez, Jean-Philippe Bouchaud</dc:creator>
    </item>
    <item>
      <title>Arbitrage equilibrium and the emergence of universal microstructure in deep neural networks</title>
      <link>https://arxiv.org/abs/2405.10955</link>
      <description>arXiv:2405.10955v2 Announce Type: replace 
Abstract: Despite the stunning progress recently in large-scale deep neural network applications, our understanding of their microstructure, 'energy' functions, and optimal design remains incomplete. Here, we present a new game-theoretic framework, called statistical teleodynamics, that reveals important insights into these key properties. The optimally robust design of such networks inherently involves computational benefit-cost trade-offs that are not adequately captured by physics-inspired models. These trade-offs occur as neurons and connections compete to increase their effective utilities under resource constraints during training. In a fully trained network, this results in a state of arbitrage equilibrium, where all neurons in a given layer have the same effective utility, and all connections to a given layer have the same effective utility. The equilibrium is characterized by the emergence of two lognormal distributions of connection weights and neuronal output as the universal microstructure of large deep neural networks. We call such a network the Jaynes Machine. Our theoretical predictions are shown to be supported by empirical data from seven large-scale deep neural networks. We also show that the Hopfield network and the Boltzmann Machine are the same special case of the Jaynes Machine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10955v2</guid>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Venkat Venkatasubramanian, N Sanjeevrajan, Manasi Khandekar, Abhishek Sivaram, Collin Szczepanski</dc:creator>
    </item>
    <item>
      <title>Stringlet Excitation Model of the Boson Peak</title>
      <link>https://arxiv.org/abs/2311.04230</link>
      <description>arXiv:2311.04230v3 Announce Type: replace-cross 
Abstract: The boson peak (BP), a low-energy excess in the vibrational density of states over the Debye contribution, is often identified as a characteristic of amorphous solid materials. Despite decades of efforts, its microscopic origin still remains a mystery. Recently, it has been proposed, and corroborated with simulations, that the BP might stem from intrinsic localized modes involving one-dimensional (1D) string-like excitations (``stringlets"). We build on a theory originally proposed by Lund that describes the localized modes as 1D vibrating strings, but we specify the stringlet size distribution to be exponential, as observed in simulations. We provide an analytical prediction for the BP frequency $\omega_{BP}$ in the temperature regime well below the observed glass transition temperature $T_g$. The prediction involves no free parameters and accords quantitatively with prior simulation observations in 2D and 3D model glasses based on inverse power law potentials. The comparison of the string model to observations is more uncertain when compared to simulations of an Al-Sm metallic glass material at temperatures well above $T_g$. Nonetheless, our stringlet model of the BP naturally reproduces the softening of the BP frequency upon heating and offers an analytical explanation for the experimentally observed scaling with the shear modulus in the glass state and changes in this scaling in simulations of glass-forming liquids. Finally, the theoretical analysis highlights the existence of a strong damping for the stringlet modes above $T_g$, which leads to a large low-frequency contribution to the 3D vibrational density of states, observed in both experiments and simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04230v3</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cunyuan Jiang, Matteo Baggioli, Jack F. Douglas</dc:creator>
    </item>
  </channel>
</rss>
