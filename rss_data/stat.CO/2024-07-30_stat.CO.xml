<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 31 Jul 2024 04:00:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 31 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Variational Inference Using Material Point Method</title>
      <link>https://arxiv.org/abs/2407.20287</link>
      <description>arXiv:2407.20287v1 Announce Type: cross 
Abstract: A new gradient-based particle sampling method, MPM-ParVI, based on material point method (MPM), is proposed for variational inference. MPM-ParVI simulates the deformation of a deformable body (e.g. a solid or fluid) under external effects driven by the target density; transient or steady configuration of the deformable body approximates the target density. The continuum material is modelled as an interacting particle system (IPS) using MPM, each particle carries full physical properties, interacts and evolves following conservation dynamics. This easy-to-implement ParVI method offers deterministic sampling and inference for a class of probabilistic models such as those encountered in Bayesian inference (e.g. intractable densities) and generative modelling (e.g. score-based).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20287v1</guid>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yongchao Huang</dc:creator>
    </item>
    <item>
      <title>Warped multifidelity Gaussian processes for data fusion of skewed environmental data</title>
      <link>https://arxiv.org/abs/2407.20295</link>
      <description>arXiv:2407.20295v1 Announce Type: cross 
Abstract: Understanding the dynamics of climate variables is paramount for numerous sectors, like energy and environmental monitoring. This study focuses on the critical need for a precise mapping of environmental variables for national or regional monitoring networks, a task notably challenging when dealing with skewed data. To address this issue, we propose a novel data fusion approach, the \textit{warped multifidelity Gaussian process} (WMFGP). The method performs prediction using multiple time-series, accommodating varying reliability and resolutions and effectively handling skewness. In an extended simulation experiment the benefits and the limitations of the methods are explored, while as a case study, we focused on the wind speed monitored by the network of ARPA Lombardia, one of the regional environmental agencies operting in Italy. ARPA grapples with data gaps, and due to the connection between wind speed and air quality, it struggles with an effective air quality management. We illustrate the efficacy of our approach in filling the wind speed data gaps through two extensive simulation experiments. The case study provides more informative wind speed predictions crucial for predicting air pollutant concentrations, enhancing network maintenance, and advancing understanding of relevant meteorological and climatic phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20295v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pietro Colombo, Claire Miller, Xiaochen Yang, Ruth O'Donnell, Paolo Maranzano</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification under Noisy Constraints, with Applications to Raking</title>
      <link>https://arxiv.org/abs/2407.20520</link>
      <description>arXiv:2407.20520v1 Announce Type: cross 
Abstract: We consider statistical inference problems under uncertain equality constraints, and provide asymptotically valid uncertainty estimates for inferred parameters. The proposed approach leverages the implicit function theorem and primal-dual optimality conditions for a particular problem class. The motivating application is multi-dimensional raking, where observations are adjusted to match marginals; for example, adjusting estimated deaths across race, county, and cause in order to match state all-race all-cause totals. We review raking from a convex optimization perspective, providing explicit primal-dual formulations, algorithms, and optimality conditions for a wide array of raking applications, which are then leveraged to obtain the uncertainty estimates. Empirical results show that the approach obtains, at the cost of a single solve, nearly the same uncertainty estimates as computationally intensive Monte Carlo techniques that pass thousands of observed and of marginal draws through the entire raking process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20520v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ariane Ducellier (Institute for Health Metrics and Evaluation, Seattle, WA), Alexander Hsu (Institute for Health Metrics and Evaluation, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA), Parkes Kendrick (Institute for Health Metrics and Evaluation, Seattle, WA), Bill Gustafson (Institute for Health Metrics and Evaluation, Seattle, WA), Laura Dwyer-Lindgren (Institute for Health Metrics and Evaluation, Seattle, WA), Christopher Murray (Institute for Health Metrics and Evaluation, Seattle, WA), Peng Zheng (Institute for Health Metrics and Evaluation, Seattle, WA), Aleksandr Aravkin (Institute for Health Metrics and Evaluation, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA)</dc:creator>
    </item>
    <item>
      <title>Persistent Sampling: Unleashing the Potential of Sequential Monte Carlo</title>
      <link>https://arxiv.org/abs/2407.20722</link>
      <description>arXiv:2407.20722v1 Announce Type: cross 
Abstract: Sequential Monte Carlo (SMC) methods are powerful tools for Bayesian inference but suffer from requiring many particles for accurate estimates, leading to high computational costs. We introduce persistent sampling (PS), an extension of SMC that mitigates this issue by allowing particles from previous iterations to persist. This generates a growing, weighted ensemble of particles distributed across iterations. In each iteration, PS utilizes multiple importance sampling and resampling from the mixture of all previous distributions to produce the next generation of particles. This addresses particle impoverishment and mode collapse, resulting in more accurate posterior approximations. Furthermore, this approach provides lower-variance marginal likelihood estimates for model comparison. Additionally, the persistent particles improve transition kernel adaptation for efficient exploration. Experiments on complex distributions show that PS consistently outperforms standard methods, achieving lower squared bias in posterior moment estimation and significantly reduced marginal likelihood errors, all at a lower computational cost. PS offers a robust, efficient, and scalable framework for Bayesian inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20722v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minas Karamanis, Uro\v{s} Seljak</dc:creator>
    </item>
    <item>
      <title>Zigzag path connects two Monte Carlo samplers: Hamiltonian counterpart to a piecewise deterministic Markov process</title>
      <link>https://arxiv.org/abs/2104.07694</link>
      <description>arXiv:2104.07694v4 Announce Type: replace 
Abstract: Zigzag and other piecewise deterministic Markov process samplers have attracted significant interest for their non-reversibility and other appealing properties for Bayesian posterior computation. Hamiltonian Monte Carlo is another state-of-the-art sampler, exploiting fictitious momentum to guide Markov chains through complex target distributions. We establish an important connection between the zigzag sampler and a variant of Hamiltonian Monte Carlo based on Laplace-distributed momentum. The position and velocity component of the corresponding Hamiltonian dynamics travels along a zigzag path paralleling the Markovian zigzag process; however, the dynamics is non-Markovian in this position-velocity space as the momentum component encodes non-immediate pasts. This information is partially lost during a momentum refreshment step, in which we preserve its direction but re-sample magnitude. In the limit of increasingly frequent momentum refreshments, we prove that Hamiltonian zigzag converges strongly to its Markovian counterpart. This theoretical insight suggests that, when retaining full momentum information, Hamiltonian zigzag can better explore target distributions with highly correlated parameters by suppressing the diffusive behavior of Markovian zigzag. We corroborate this intuition by comparing performance of the two zigzag cousins on high-dimensional truncated multivariate Gaussians, including a 11,235-dimensional target arising from a Bayesian phylogenetic multivariate probit modeling of HIV virus data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.07694v4</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akihiko Nishimura, Zhenyu Zhang, Marc A. Suchard</dc:creator>
    </item>
    <item>
      <title>Online Multivariate Changepoint Detection: Leveraging Links With Computational Geometry</title>
      <link>https://arxiv.org/abs/2311.01174</link>
      <description>arXiv:2311.01174v2 Announce Type: replace 
Abstract: The increasing volume of data streams poses significant computational challenges for detecting changepoints online. Likelihood-based methods are effective, but a naive sequential implementation becomes impractical online due to high computational costs. We develop an online algorithm that exactly calculates the likelihood ratio test for a single changepoint in $p$-dimensional data streams by leveraging fascinating connections with computational geometry. This connection straightforwardly allows us to recover sparse likelihood ratio statistics exactly: that is assuming only a subset of the dimensions are changing. Our algorithm is straightforward, fast, and apparently quasi-linear. A dyadic variant of our algorithm is provably quasi-linear, being $\mathcal{O}(n\log(n)^{p+1})$ for $n$ data points and $p$ less than $3$, but slower in practice. These algorithms are computationally impractical when $p$ is larger than $5$, and we provide an approximate algorithm suitable for such $p$ which is $\mathcal{O}(np\log(n)^{\tilde{p}+1}), $ for some user-specified $\tilde{p} \leq 5.$ We derive some statistical guarantees for the proposed procedures in the Gaussian case, and confirm the good computational and statistical performance, and usefulness, of the algorithms on both empirical data and on NBA data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01174v2</guid>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liudmila Pishchagina, Gaetano Romano, Paul Fearnhead, Vincent Runge, Guillem Rigaill</dc:creator>
    </item>
    <item>
      <title>New methods to compute the generalized chi-square distribution</title>
      <link>https://arxiv.org/abs/2404.05062</link>
      <description>arXiv:2404.05062v2 Announce Type: replace 
Abstract: We present several new mathematical methods (ray-trace, inverse Fourier transform and ellipse) and open-source software to compute the cdf, pdf and inverse cdf of the generalized chi-square distribution. Some methods are geared for speed, while others are designed to be accurate far into the tails, using which we can also measure large values of the discriminability index d' between multinormals. We characterize the performance and limitations of these and previous methods, and recommend the best methods to use for each part of each type of distribution. We also demonstrate the speed and accuracy of our new methods against previous methods across a wide sample of distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05062v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abhranil Das</dc:creator>
    </item>
    <item>
      <title>Monte Carlo inference for semiparametric Bayesian regression</title>
      <link>https://arxiv.org/abs/2306.05498</link>
      <description>arXiv:2306.05498v2 Announce Type: replace-cross 
Abstract: Data transformations are essential for broad applicability of parametric regression models. However, for Bayesian analysis, joint inference of the transformation and model parameters typically involves restrictive parametric transformations or nonparametric representations that are computationally inefficient and cumbersome for implementation and theoretical analysis, which limits their usability in practice. This paper introduces a simple, general, and efficient strategy for joint posterior inference of an unknown transformation and all regression model parameters. The proposed approach directly targets the posterior distribution of the transformation by linking it with the marginal distributions of the independent and dependent variables, and then deploys a Bayesian nonparametric model via the Bayesian bootstrap. Crucially, this approach delivers (1) joint posterior consistency under general conditions, including multiple model misspecifications, and (2) efficient Monte Carlo (not Markov chain Monte Carlo) inference for the transformation and all parameters for important special cases. These tools apply across a variety of data domains, including real-valued, positive, and compactly-supported data. Simulation studies and an empirical application demonstrate the effectiveness and efficiency of this strategy for semiparametric Bayesian analysis with linear models, quantile regression, and Gaussian processes. The R package SeBR is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.05498v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel R. Kowal, Bohan Wu</dc:creator>
    </item>
  </channel>
</rss>
