<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Oct 2024 20:49:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Dynamic Pricing based Near-Optimal Resource Allocation for Elastic Edge Offloading</title>
      <link>https://arxiv.org/abs/2409.18977</link>
      <description>arXiv:2409.18977v1 Announce Type: new 
Abstract: In mobile edge computing (MEC), task offloading can significantly reduce task execution latency and energy consumption of end user (EU). However, edge server (ES) resources are limited, necessitating efficient allocation to ensure the sustainable and healthy development for MEC systems. In this paper, we propose a dynamic pricing mechanism based near-optimal resource allocation for elastic edge offloading. First, we construct a resource pricing model and accordingly develop the utility functions for both EU and ES, the optimal pricing model parameters are derived by optimizing the utility functions. In the meantime, our theoretical analysis reveals that the EU's utility function reaches a local maximum within the search range, but exhibits barely growth with increased resource allocation beyond this point. To this end, we further propose the Dynamic Inertia and Speed-Constrained particle swarm optimization (DISC-PSO) algorithm, which efficiently identifies the near-optimal resource allocation. Comprehensive simulation results validate the effectiveness of DISC-PSO, demonstrating that it significantly outperforms existing schemes by reducing the average number of iterations to reach a near-optimal solution by 92.11\%, increasing the final user utility function value by 0.24\%, and decreasing the variance of results by 95.45\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18977v1</guid>
      <category>cs.NI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yun Xia, Hai Xue, Di Zhang, Shahid Mumtaz, Xiaolong Xu, Joel J. P. C. Rodrigues</dc:creator>
    </item>
    <item>
      <title>Trust, But Verify, Operator-Reported Geolocation</title>
      <link>https://arxiv.org/abs/2409.19109</link>
      <description>arXiv:2409.19109v1 Announce Type: new 
Abstract: Geolocation plays a critical role in understanding the Internet. In this work, we provide an in-depth analysis of operator-misreported geolocation. Using a bandwidth-efficient methodology, we find in May 2024 that only a small percentage (1.5%) of vantage points in the largest community-vantage point collection, RIPE Atlas, do not respond from their operator-reported geolocation. However, misreported geolocations disproportionately affect areas with limited coverage and cause entire countries to be left with no vantage points. Furthermore, the problem is escalating: within the past five years, the number of probes reporting the wrong location has increased ten-fold. To increase the accuracy of future methodologies and studies that rely upon operator-reported geolocation, we open source our methodology and release a continually updated dataset of RIPE Atlas vantage points that misreport geolocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19109v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katherine Izhikevich, Ben Du, Sumanth Rao, Alisha Ukani, Liz Izhikevich</dc:creator>
    </item>
    <item>
      <title>Towards Energy- and Cost-Efficient 6G Networks</title>
      <link>https://arxiv.org/abs/2409.19121</link>
      <description>arXiv:2409.19121v1 Announce Type: new 
Abstract: As the world enters the journey toward the 6th generation (6G) of wireless technology, the promises of ultra-high data rates, unprecedented low latency, and a massive surge in connected devices require crucial exploration of network energy saving (NES) solutions to minimize the carbon footprint and overall energy usage of future cellular networks. On the other hand, network-controlled repeaters (NCRs) have been introduced by 3rd generation partnership project (3GPP) as a cost-effective solution to improve network coverage. However, their impact on network power consumption and energy efficiency has not been thoroughly investigated. This paper studies NES schemes for next-generation 6G networks aided by NCRs and proposes optimal NES strategies aiming at maximizing the overall energy efficiency of the network. Repeaters are shown to allow for power savings at next-generation nodeB (gNB), and offer higher overall energy efficiency (EE) and spectral efficiency (SE), thus providing an energy-efficient and cost-efficient alternative to increase the performance of future 6G networks</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19121v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tommy Azzino, Aria HasanzadeZonuzy, Jianghong Luo, Navid Abedini, Tao Luo</dc:creator>
    </item>
    <item>
      <title>SAMBA: Scalable Approximate Forwarding For NDN Implicit FIB Aggregation</title>
      <link>https://arxiv.org/abs/2409.19154</link>
      <description>arXiv:2409.19154v1 Announce Type: new 
Abstract: The Internet landscape has witnessed a significant shift toward Information Centric Networking (ICN) due to the exponential growth of data-driven applications. Similar to routing tables in TCP/IP architectures, ICN uses Forward Information Base (FIB) tables. However, FIB tables can grow exponentially due to their URL-like naming scheme, introducing major delays in the prefix lookup process. Existing explicit FIB aggregation solutions are very complex to run, and ICN on-demand routing schemes, which use a discovery mechanism to help reduce the number of FIB records and thus have shorter lookup times, rely on flooding-based mechanisms and building routes for all requests, introducing additional scalability challenges. In this paper, we propose SAMBA, an Approximate Forwarding-based Self Learning, that uses the nearest FIB trie record to the given prefix for reducing the number of discoveries thus keeping the FIB table small. By choosing the nearest prefix to a given name prefix, SAMBA uses Implicit Prefix Aggregation (IPA) which implicitly aggregates the FIB records and reduces the number of Self Learning discoveries required. Coupled with the approximate forwarding, SAMBA can achieve efficient and scalable forwarding</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19154v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Esmaeili, Abderrahmen Mtibaa</dc:creator>
    </item>
    <item>
      <title>Sharing-Based Channel Access Procedure For Next Generation of Wireless LAN</title>
      <link>https://arxiv.org/abs/2409.19219</link>
      <description>arXiv:2409.19219v1 Announce Type: new 
Abstract: This paper proposes a new channel access procedure to mitigate the channel access contention in next generation of Wireless Local-Area Networks (WLANs) by allowing cooperation among devices belonging to same network, while maintaining high flexibility in terms of how each device may contend the medium. After introducing the details of the proposed procedure, which is here referred to as sharing-based protocol, an analytical analysis is provided to compare it with the two state-of-art protocols currently adopted in IEEE 802.11 standard, i.e, Enhanced Distributed Channel Access (EDCA)-based and trigger-based protocol. In this regards, closed form expressions are derived to evaluate the success probability of channel access for each protocol. In order to show the merit of the proposed procedure, a comprehensive system level analysis is also provided, which highlights that the proposed procedure outperforms the two state-of-art protocols in terms of mitigating the End-to-End (E2E) delay and allowing a better spectrum utilization by reducing the overall congestion in the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19219v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qing Xia, Salvatore Talarico</dc:creator>
    </item>
    <item>
      <title>Temporal Consistency of Data and Information in Cyber-Physical Systems</title>
      <link>https://arxiv.org/abs/2409.19309</link>
      <description>arXiv:2409.19309v1 Announce Type: new 
Abstract: In a large cyber-physical system, a temporal inconsistency of an output value can arise if there is a non-negligible delay between the instant when a sensor value is acquired from the environment and the instant when a setpoint, based on this sensor value, is used in the environment. Such a temporal inconsistency can be the cause of a critical malfunction of the cyber-physical system. This paper presents a solution of this temporal consistency problem that can best be implemented in a time-triggered architecture (TTA). In a TTA, the instants of sensor value acquisition, setpoint calculation, and actuation on the environment are statically configured, and the cyber-physical system implements software and hardware mechanisms to execute the respective actions tightly at these configured instants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19309v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hermann Kopetz (TU Wien, Austria), Wilfried Steiner (TTTech, Austria)</dc:creator>
    </item>
    <item>
      <title>CyclicSim: Comprehensive Evaluation of Cyclic Shapers in Time-Sensitive Networking</title>
      <link>https://arxiv.org/abs/2409.19792</link>
      <description>arXiv:2409.19792v1 Announce Type: new 
Abstract: Cyclic Queuing and Forwarding (CQF) is a key Time-Sensitive Networking (TSN) shaping mechanism that ensures bounded latency using a simple gate control list (GCL). Recently, variants of CQF, including Cycle Specific Queuing and Forwarding (CSQF) and Multi Cyclic Queuing and Forwarding (MCQF), have emerged. While popular TSN mechanisms such as the Time-Aware Shaper (TAS), Asynchronous Traffic Shaper (ATS), Credit-Based Shaper (CBS), and Strict Priority (SP) have been extensively studied, cyclic shapers have not been thoroughly evaluated. This paper presents a comprehensive analysis of CQF, CSQF, and MCQF, providing insights into their performance. We quantify delays through simulations and quantitative analysis on both synthetic and realistic networks. For the first time, we introduce an open-source OMNeT++ and INET4.4 based framework capable of modeling all three cyclic shaper variants. Our tool facilitates the validation of new algorithms and serves as a benchmark for cyclic shapers. Our evaluations reveal that MCQF supports diverse timing requirements, whereas CSQF, with its additional queue, often results in larger delays and jitter for some TT flows compared to CQF. Additionally, CSQF does not demonstrate significant advantages in TSN networks where propagation delays are less critical than in wide-area networks (WANs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19792v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rubi Debnath, Luxi Zhao, Mohammadreza Barzegaran, Sebastian Steinhorst</dc:creator>
    </item>
    <item>
      <title>Balancing Generalization and Specialization: Offline Metalearning for Bandwidth Estimation</title>
      <link>https://arxiv.org/abs/2409.19867</link>
      <description>arXiv:2409.19867v1 Announce Type: new 
Abstract: User experience in real-time video applications requires continuously adjusting video encoding bitrates to match available network capacity, which hinges on accurate bandwidth estimation (BWE). However, network heterogeneity prevents a one-size-fits-all solution to BWE, motivating the demand for personalized approaches. Although personalizing BWE algorithms offers benefits such as improved adaptability to individual network conditions, it faces the challenge of data drift -- where estimators degrade over time due to evolving network environments. To address this, we introduce Ivy, a novel method for BWE that leverages offline metalearning to tackle data drift and maximize end-user Quality of Experience (QoE). Our key insight is that dynamically selecting the most suitable BWE algorithm for current network conditions allows for more effective adaption to changing environments. Ivy is trained entirely offline using Implicit Q-learning, enabling it to learn from individual network conditions without a single, live videoconferencing interaction, thereby reducing deployment complexity and making Ivy more practical for real-world personalization. We implemented our method in a popular videoconferencing application and demonstrated that Ivy can enhance QoE by 5.9% to 11.2% over individual BWE algorithms and by 6.3% to 11.4% compared to existing online meta heuristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19867v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aashish Gottipati, Sami Khairy, Yasaman Hosseinkashi, Gabriel Mittag, Vishak Gopal, Francis Y. Yan, Ross Cutler</dc:creator>
    </item>
    <item>
      <title>Diagnosing and Repairing Distributed Routing Configurations Using Selective Symbolic Simulation</title>
      <link>https://arxiv.org/abs/2409.20306</link>
      <description>arXiv:2409.20306v1 Announce Type: new 
Abstract: Although substantial progress has been made in automatically verifying whether distributed routing configurations conform to certain requirements, diagnosing and repairing configuration errors remains manual and time-consuming. To fill this gap, we propose S^2Sim, a novel system for automatic routing configuration diagnosis and repair. Our key insight is that by selectively simulating variants of the given configuration in a symbolic way, we can find an intent-compliant variant, whose differences between the given configuration reveal the errors in the given configuration and suggest the patches. Building on this insight, we also design techniques to support complex scenarios (e.g., multiple protocol networks) and requirements (e.g., k-link failure tolerance). We implement a prototype of S^2Sim and evaluate its performance using networks of size O(10) ~ O(1000) with synthetic real-world configurations. Results show that S^2Sim diagnoses and repairs errors for 1) all WAN configurations within 10 s and 2) all DCN configurations within 20 minutes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20306v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rulan Yang, Hanyang Shao, Gao Han, Ziyi Wang, Xing Fang, Lizhao You, Qiao Xiang, Linghe Kong, Ruiting Zhou, Jiwu Shu</dc:creator>
    </item>
    <item>
      <title>Machine Learning-enabled Traffic Steering in O-RAN: A Case Study on Hierarchical Learning Approach</title>
      <link>https://arxiv.org/abs/2409.20391</link>
      <description>arXiv:2409.20391v1 Announce Type: new 
Abstract: Traffic Steering is a crucial technology for wireless networks, and multiple efforts have been put into developing efficient Machine Learning (ML)-enabled traffic steering schemes for Open Radio Access Networks (O-RAN). Given the swift emergence of novel ML techniques, conducting a timely survey that comprehensively examines the ML-based traffic steering schemes in O-RAN is critical. In this article, we provide such a survey along with a case study of hierarchical learning-enabled traffic steering in O-RAN. In particular, we first introduce the background of traffic steering in O-RAN and overview relevant state-of-the-art ML techniques and their applications. Then, we analyze the compatibility of the hierarchical learning framework in O-RAN and further propose a Hierarchical Deep-Q-Learning (h-DQN) framework for traffic steering. Compared to existing works, which focus on single-layer architecture with standalone agents, h-DQN decomposes the traffic steering problem into a bi-level architecture with hierarchical intelligence. The meta-controller makes long-term and high-level policies, while the controller executes instant traffic steering actions under high-level policies. Finally, the case study shows that the hierarchical learning approach can provide significant performance improvements over the baseline algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20391v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Arafat Habib, Hao Zhou, Pedro Enrique Iturria-Rivera, Yigit Ozcan, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Melike Erol-Kantarci</dc:creator>
    </item>
    <item>
      <title>Beacon based uplink transmission for lorawan direct to satellite internet of things</title>
      <link>https://arxiv.org/abs/2409.20408</link>
      <description>arXiv:2409.20408v1 Announce Type: new 
Abstract: Direct-to-satellite IoT DtS IoT communication structure is a promising solution to provide connectivity and extend the coverage of traditional low-power and long-range technologies, especially for isolated and remote areas where deploying traditional infrastructure is impracticable. Despite their bounded visibility, the Low Earth Orbit LEO satellites complement the terrestrial networks, offering broader gateway coverage and terrestrial network traffic offloading. However, the dynamics of LEO and the nature of such integration come with several challenges affecting the efficacy of the network. Therefore, this paper proposes Beacon based Uplink LoRaWAN BU LoRaWAN to enhance satellite-terrestrial communication efficiency. The proposed scheme exploits the LoRaWAN class B synchronization mechanism to provide efficient uplink transmission from LoRaWAN devices placed on the ground to satellite gateways. BU LoRaWAN proposes an uplink transmission slot approach to synchronize ground devices uplink traffic with LEO based orbiting gateways. It also uses a queue data structure to buffer end devices ready to send packets until the appropriate moment. BU LoRaWAN avoids possible transmission collision by optimizing a random transmission slot for an end device within the beacon window. The proposed system is implemented and evaluated using OMNeT network simulator and FLoRaSat framework. The result demonstrates the feasibility of the proposed system. BU-LoRaWAN achieves better performance compared to the standard LoRaWAN, which manages to deliver almost double the traffic delivered by the standard one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20408v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5121/ijcnc.2024.16503</arxiv:DOI>
      <dc:creator>Mohammad Al Mojamed</dc:creator>
    </item>
    <item>
      <title>Packet Aggregation May Harm Batched Network Coding</title>
      <link>https://arxiv.org/abs/2409.20501</link>
      <description>arXiv:2409.20501v1 Announce Type: new 
Abstract: Batched network coding (BNC) is a solution to multi-hop transmission on networks with packet loss. To be compatible with the existing infrastructure, BNC is usually implemented over UDP. A single error bit will probably result in discarding the packet. UDP-Lite is a variant of UDP that supports partial checksums. As long as the data covered by the checksum is correct, damaged payload will be delivered. With UDP-Lite, we can cope with other techniques such as payload aggregation of BNC packets to reduce the protocol overhead, and forward error correction to combat against bit errors. Unlike traditional transmissions, BNC has a loss resilience feature and there are dependencies between BNC packets. In this paper, we conduct a preliminary investigation on BNC over UDP-Lite. We show that aggregating as much as we can is not always the best strategy, and a hop-by-hop distributed efficiency optimization approach may lead to a worse throughput compared with the scheme without aggregation in a long network. These unnatural results caution that a casual integration of techniques with BNC can be harmful, and give us hints on future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20501v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoover H. F. Yin</dc:creator>
    </item>
    <item>
      <title>Jupyter Notebook Attacks Taxonomy: Ransomware, Data Exfiltration, and Security Misconfiguration</title>
      <link>https://arxiv.org/abs/2409.19456</link>
      <description>arXiv:2409.19456v1 Announce Type: cross 
Abstract: Open-science collaboration using Jupyter Notebooks may expose expensively trained AI models, high-performance computing resources, and training data to security vulnerabilities, such as unauthorized access, accidental deletion, or misuse. The ubiquitous deployments of Jupyter Notebooks (~11 million public notebooks on Github have transformed collaborative scientific computing by enabling reproducible research. Jupyter is the main HPC's science gateway interface between AI researchers and supercomputers at academic institutions, such as the National Center for Supercomputing Applications (NCSA), national labs, and the industry. An impactful attack targeting Jupyter could disrupt scientific missions and business operations.
  This paper describes the network-based attack taxonomy of Jupyter Notebooks, such as ransomware, data exfiltration, security misconfiguration, and resource abuse for cryptocurrency mining. The open nature of Jupyter (direct data access, arbitrary code execution in multiple programming languages kernels) and its vast attack interface (terminal, file browser, untrusted cells) also attract attacks attempting to misuse supercomputing resources and steal state-of-the-art research artifacts. Jupyter uses encrypted datagrams of rapidly evolving WebSocket protocols that challenge even the most state-of-the-art network observability tools, such as Zeek.
  We envisage even more sophisticated AI-driven attacks can be adapted to target Jupyter, where defenders have limited visibility. In addition, Jupyter's cryptographic design should be adapted to resist emerging quantum threats. On balance, this is the first paper to systematically describe the threat model against Jupyter Notebooks and lay out the design of auditing Jupyter to have better visibility against such attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19456v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Phuong Cao</dc:creator>
    </item>
    <item>
      <title>Age of Gossip with the Push-Pull Protocol</title>
      <link>https://arxiv.org/abs/2409.20490</link>
      <description>arXiv:2409.20490v1 Announce Type: cross 
Abstract: We consider a wireless network where a source generates packets and forwards them to a network containing $n$ nodes. The nodes in the network use the asynchronous push, pull or push-pull gossip communication protocols to maintain the most recent updates from the source. We use the version age of information metric to quantify the freshness of information in the network. Prior to this work, only the push gossiping protocol has been studied for age of information analysis. In this paper, we use the stochastic hybrid systems (SHS) framework to obtain recursive equations for the expected version age of sets of nodes in the time limit. We then show that the pull and push-pull protocols can achieve constant version age, while it is already known that the push protocol can only achieve logarithmic version age. We then show that the push-pull protocol performs better than the push and the pull protocol. Finally, we carry out numerical simulations to evaluate these results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20490v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arunabh Srivastava, Thomas Jacob Maranzatto, Sennur Ulukus</dc:creator>
    </item>
    <item>
      <title>A Survey on XAI for 5G and Beyond Security: Technical Aspects, Challenges and Research Directions</title>
      <link>https://arxiv.org/abs/2204.12822</link>
      <description>arXiv:2204.12822v3 Announce Type: replace 
Abstract: With the advent of 5G commercialization, the need for more reliable, faster, and intelligent telecommunication systems is envisaged for the next generation beyond 5G (B5G) radio access technologies. Artificial Intelligence (AI) and Machine Learning (ML) are immensely popular in service layer applications and have been proposed as essential enablers in many aspects of 5G and beyond networks, from IoT devices and edge computing to cloud-based infrastructures. However, existing 5G ML-based security surveys tend to emphasize AI/ML model performance and accuracy more than the models' accountability and trustworthiness. In contrast, this paper explores the potential of Explainable AI (XAI) methods, which would allow stakeholders in 5G and beyond to inspect intelligent black-box systems used to secure next-generation networks. The goal of using XAI in the security domain of 5G and beyond is to allow the decision-making processes of ML-based security systems to be transparent and comprehensible to 5G and beyond stakeholders, making the systems accountable for automated actions. In every facet of the forthcoming B5G era, including B5G technologies such as ORAN, zero-touch network management, and end-to-end slicing, this survey emphasizes the role of XAI in them that the general users would ultimately enjoy. Furthermore, we presented the lessons from recent efforts and future research directions on top of the currently conducted projects involving XAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.12822v3</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/COMST.2024.3437248</arxiv:DOI>
      <dc:creator>Thulitha Senevirathna, Vinh Hoa La, Samuel Marchal, Bartlomiej Siniarski, Madhusanka Liyanage, Shen Wang</dc:creator>
    </item>
    <item>
      <title>A QoS-Aware Uplink Spectrum and Power Allocation with Link Adaptation for Vehicular Communications in 5G networks</title>
      <link>https://arxiv.org/abs/2305.02667</link>
      <description>arXiv:2305.02667v3 Announce Type: replace 
Abstract: In this work, we have proposed link adaptation-based joint spectrum and power allocation algorithms for the uplink communication in 5G Cellular Vehicle-to-Everything (C-V2X) systems. In C-V2X, vehicle-to-vehicle (V2V) users share radio resources with vehicle-to-infrastructure (V2I) users. Existing works primarily focus on the optimal pairing of V2V and V2I users, assuming that each V2I user needs a single resource block (RB) while minimizing interference through power allocation. In contrast, in this work, we have considered that the number of RBs needed by the users is a function of their channel condition and Quality of Service (QoS) - a method called link adaptation. It effectively compensates for the frequent channel quality fluctuations at the high frequencies of 5G communication.5G uses a multi-numerology frame structure to support diverse QoS requirements, which has also been considered in this work.
  The first algorithm proposed in this article greedily allocates RBs to V2I users using link adaptation. It then uses the Hungarian algorithm to pair V2V with V2I users while minimizing interference through power allocation. The second proposed method groups RBs into resource chunks (RCs) and uses the Hungarian algorithm twice - first to allocate RCs to V2I users and then to pair V2I users with V2V users. Extensive simulations reveal that link adaptation increases the number of satisfied V2I users and their sum rate while also improving the QoS of V2I and V2V users, making it indispensable for 5G C-V2X systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.02667v3</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Krishna Pal Thakur, Basabdatta Palit</dc:creator>
    </item>
    <item>
      <title>Quantum Computing in Wireless Communications and Networking: A Tutorial-cum-Survey</title>
      <link>https://arxiv.org/abs/2406.02240</link>
      <description>arXiv:2406.02240v2 Announce Type: replace 
Abstract: Owing to its outstanding parallel computing capabilities, quantum computing (QC) has been a subject of continuous attention. With the gradual maturation of QC platforms, it has increasingly played a significant role in various fields such as transportation, pharmaceuticals, and industrial manufacturing,achieving unprecedented milestones. In modern society, wireless communication stands as an indispensable infrastructure, with its essence lying in optimization. Although artificial intelligence (AI) algorithms such as Reinforcement Learning (RL) and mathematical optimization have greatly enhanced the performance of wireless communication, the rapid attainment of optimal solutions for wireless communication problems remains an unresolved challenge. QC, however, presents a new alternative. This paper aims to elucidate the fundamentals of QC and explore its applications in wireless communications and networking. First, we will provide a tutorial on QC, covering its basics, characteristics, and popular QC algorithms. Next, we will introduce the applications of QC in communication and networking, followed by its applications in miscellaneous areas such as security and privacy, localization and tracking, and video streaming. Finally,we will discuss remaining open issues before concluding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02240v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Zhao, Tangjie Weng, Yue Ruan, Zhi Liu, Xuangou Wu, Xiao Zheng, Nei Kato</dc:creator>
    </item>
    <item>
      <title>Experimenting with Adaptive Bitrate Algorithms for Virtual Reality Streaming over Wi-Fi</title>
      <link>https://arxiv.org/abs/2407.15614</link>
      <description>arXiv:2407.15614v3 Announce Type: replace 
Abstract: Interactive Virtual Reality (VR) streaming over Wi-Fi networks encounters significant challenges due to bandwidth fluctuations caused by channel contention and user mobility. Adaptive BitRate (ABR) algorithms dynamically adjust the video encoding bitrate based on the available network capacity, aiming to maximize image quality while mitigating congestion and preserving the user's Quality of Experience (QoE). In this paper, we experiment with ABR algorithms for VR streaming using Air Light VR (ALVR), an open-source VR streaming solution. We extend ALVR with a comprehensive set of metrics that provide a robust characterization of the network's state, enabling more informed bitrate adjustments. To demonstrate the utility of these performance indicators, we develop and test the Network-aware Step-wise ABR algorithm for VR streaming (NeSt-VR). Results validate the accuracy of the newly implemented network performance metrics and demonstrate NeSt-VR's video bitrate adaptation capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15614v3</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ferran Maura, Miguel Casasnovas, Boris Bellalta</dc:creator>
    </item>
    <item>
      <title>FMLFS: A Federated Multi-Label Feature Selection Based on Information Theory in IoT Environment</title>
      <link>https://arxiv.org/abs/2405.00524</link>
      <description>arXiv:2405.00524v2 Announce Type: replace-cross 
Abstract: In certain emerging applications such as health monitoring wearable and traffic monitoring systems, Internet-of-Things (IoT) devices generate or collect a huge amount of multi-label datasets. Within these datasets, each instance is linked to a set of labels. The presence of noisy, redundant, or irrelevant features in these datasets, along with the curse of dimensionality, poses challenges for multi-label classifiers. Feature selection (FS) proves to be an effective strategy in enhancing classifier performance and addressing these challenges. Yet, there is currently no existing distributed multi-label FS method documented in the literature that is suitable for distributed multi-label datasets within IoT environments. This paper introduces FMLFS, the first federated multi-label feature selection method. Here, mutual information between features and labels serves as the relevancy metric, while the correlation distance between features, derived from mutual information and joint entropy, is utilized as the redundancy measure. Following aggregation of these metrics on the edge server and employing Pareto-based bi-objective and crowding distance strategies, the sorted features are subsequently sent back to the IoT devices. The proposed method is evaluated through two scenarios: 1) transmitting reduced-size datasets to the edge server for centralized classifier usage, and 2) employing federated learning with reduced-size datasets. Evaluation across three metrics - performance, time complexity, and communication cost - demonstrates that FMLFS outperforms five other comparable methods in the literature and provides a good trade-off on three real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00524v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Afsaneh Mahanipour, Hana Khamfroush</dc:creator>
    </item>
  </channel>
</rss>
