<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Oct 2024 04:00:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fast spatio-temporally varying coefficient modeling with reluctant interaction selection</title>
      <link>https://arxiv.org/abs/2410.07229</link>
      <description>arXiv:2410.07229v1 Announce Type: new 
Abstract: Spatially and temporally varying coefficient (STVC) models are currently attracting attention as a flexible tool to explore the spatio-temporal patterns in regression coefficients. However, these models often struggle with balancing computational efficiency and model flexibility. To address this challenge, this study develops a fast and flexible method for STVC modeling. For enhanced flexibility in modeling, we assume multiple processes in each varying coefficient, including purely spatial, purely temporal, and spatio-temporal interaction processes with or without time cyclicity. While considering multiple processes can be time consuming, we combine a pre-conditioning method with a model selection procedure, inspired by reluctant interaction modeling. This approach allows us to computationally efficiently select and specify the latent space-time structure. Monte Carlo experiments demonstrate that the proposed method outperforms alternatives in terms of coefficient estimation accuracy and computational efficiency. Finally, we apply the proposed method to crime analysis using a sample size of 279,360, confirming that the proposed method provides reasonable estimates of varying coefficients. The STVC model is implemented in an R package spmoran.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07229v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daisuke Murakami, Shinichiro Shirota, Seiji Kajita, Mami Kajita</dc:creator>
    </item>
    <item>
      <title>Dynamic borrowing from historical controls via the synthetic prior with covariates in randomized clinical trials</title>
      <link>https://arxiv.org/abs/2410.07242</link>
      <description>arXiv:2410.07242v1 Announce Type: new 
Abstract: Motivated by a rheumatoid arthritis clinical trial, we propose a new Bayesian method called SPx, standing for synthetic prior with covariates, to borrow information from historical trials to reduce the control group size in a new trial. The method involves a novel use of Bayesian model averaging to balance between multiple possible relationships between the historical and new trial data, allowing the historical data to be dynamically trusted or discounted as appropriate. We require only trial-level summary statistics, which are available more often than patient-level data. Through simulations and an application to the rheumatoid arthritis trial we show that SPx can substantially reduce the control group size while maintaining Frequentist properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07242v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel E. Schwartz, Yuan Ji, Li Wang</dc:creator>
    </item>
    <item>
      <title>Feature-centric nonlinear autoregressive models</title>
      <link>https://arxiv.org/abs/2410.07293</link>
      <description>arXiv:2410.07293v1 Announce Type: new 
Abstract: We propose a novel feature-centric approach to surrogate modeling of dynamical systems driven by time-varying exogenous excitations. This approach, named Functional Nonlinear AutoRegressive with eXogenous inputs (F-NARX), aims to approximate the system response based on temporal features of both the exogenous inputs and the system response, rather than on their values at specific time lags. This is a major step away from the discrete-time-centric approach of classical NARX models, which attempts to determine the relationship between selected time steps of the input/output time series. By modeling the system in a time-feature space instead of the original time axis, F-NARX can provide more stable long-term predictions and drastically reduce the reliance of the model performance on the time discretization of the problem. F-NARX, like NARX, acts as a framework and is not tied to a single implementation. In this work, we introduce an F-NARX implementation based on principal component analysis and polynomial basis functions. To further improve prediction accuracy and computational efficiency, we also introduce a strategy to identify and fit a sparse model structure, thanks to a modified hybrid least angle regression approach that minimizes the expected forecast error, rather than the one-step-ahead prediction error. Since F-NARX is particularly well-suited to modeling engineering structures typically governed by physical processes, we investigate the behavior and capabilities of our F-NARX implementation on two case studies: an eight-story building under wind loading and a three-story steel frame under seismic loading. Our results demonstrate that F-NARX has several favorable properties over classical NARX, making it well suited to emulate engineering systems with high accuracy over extended time periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07293v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Styfen Sch\"ar, Stefano Marelli, Bruno Sudret</dc:creator>
    </item>
    <item>
      <title>Penalized regression with negative-unlabeled data: An approach to developing a long COVID research index</title>
      <link>https://arxiv.org/abs/2410.07357</link>
      <description>arXiv:2410.07357v1 Announce Type: new 
Abstract: Moderate to severe post-acute sequelae of SARS-CoV-2 infection (PASC), also called long COVID, is estimated to impact as many as 10% of SARS-CoV-2 infected individuals, representing a chronic condition with a substantial global public health burden. An expansive literature has identified over 200 long-term and persistent symptoms associated with a history of SARS-CoV-2 infection; yet, there remains to be a clear consensus on a syndrome definition. Such a definition is a critical first step in future studies of risk and resiliency factors, mechanisms of disease, and interventions for both treatment and prevention. We recently applied a strategy for defining a PASC research index based on a Lasso-penalized logistic regression on history of SARS-CoV-2 infection. In the current paper we formalize and evaluate this approach through theoretical derivations and simulation studies. We demonstrate that this approach appropriately selects symptoms associated with PASC and results in a score that has high discriminatory power for detecting PASC. An application to data on participants enrolled in the RECOVER (Researching COVID to Enhance Recovery) Adult Cohort is presented to illustrate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07357v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison T. Reeder, Tanayott Thaweethai, Andrea S. Foulkes</dc:creator>
    </item>
    <item>
      <title>Predicting Dengue Outbreaks: A Dynamic Approach with Variable Length Markov Chains and Exogenous Factors</title>
      <link>https://arxiv.org/abs/2410.07374</link>
      <description>arXiv:2410.07374v1 Announce Type: new 
Abstract: Variable Length Markov Chains with Exogenous Covariates (VLMCX) are stochastic models that use Generalized Linear Models to compute transition probabilities, taking into account both the state history and time-dependent exogenous covariates. The beta-context algorithm selects a relevant finite suffix (context) for predicting the next symbol. This algorithm estimates flexible tree-structured models by aggregating irrelevant states in the process history and enables the model to incorporate exogenous covariates over time.
  This research uses data from multiple sources to extend the beta-context algorithm to incorporate time-dependent and time-invariant exogenous covariates. Within this approach, we have a distinct Markov chain for every data source, allowing for a comprehensive understanding of the process behavior across multiple situations, such as different geographic locations. Despite using data from different sources, we assume that all sources are independent and share identical parameters - we explore contexts within each data source and combine them to compute transition probabilities, deriving a unified tree. This approach eliminates the necessity for spatial-dependent structural considerations within the model. Furthermore, we incorporate modifications in the estimation procedure to address contexts that appear with low frequency.
  Our motivation was to investigate the impact of previous dengue rates, weather conditions, and socioeconomic factors on subsequent dengue rates across various municipalities in Brazil, providing insights into dengue transmission dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07374v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mar\'ilia Gabriela Rocha, Nancy L. Garcia</dc:creator>
    </item>
    <item>
      <title>Representation-Enhanced Neural Knowledge Integration with Application to Large-Scale Medical Ontology Learning</title>
      <link>https://arxiv.org/abs/2410.07454</link>
      <description>arXiv:2410.07454v1 Announce Type: new 
Abstract: A large-scale knowledge graph enhances reproducibility in biomedical data discovery by providing a standardized, integrated framework that ensures consistent interpretation across diverse datasets. It improves generalizability by connecting data from various sources, enabling broader applicability of findings across different populations and conditions. Generating reliable knowledge graph, leveraging multi-source information from existing literature, however, is challenging especially with a large number of node sizes and heterogeneous relations. In this paper, we propose a general theoretically guaranteed statistical framework, called RENKI, to enable simultaneous learning of multiple relation types. RENKI generalizes various network models widely used in statistics and computer science. The proposed framework incorporates representation learning output into initial entity embedding of a neural network that approximates the score function for the knowledge graph and continuously trains the model to fit observed facts. We prove nonasymptotic bounds for in-sample and out-of-sample weighted MSEs in relation to the pseudo-dimension of the knowledge graph function class. Additionally, we provide pseudo-dimensions for score functions based on multilayer neural networks with ReLU activation function, in the scenarios when the embedding parameters either fixed or trainable. Finally, we complement our theoretical results with numerical studies and apply the method to learn a comprehensive medical knowledge graph combining a pretrained language model representation with knowledge graph links observed in several medical ontologies. The experiments justify our theoretical findings and demonstrate the effect of weighting in the presence of heterogeneous relations and the benefit of incorporating representation learning in nonparametric models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07454v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suqi Liu, Tianxi Cai, Xiaoou Li</dc:creator>
    </item>
    <item>
      <title>Doubly robust estimation and sensitivity analysis with outcomes truncated by death in multi-arm clinical trials</title>
      <link>https://arxiv.org/abs/2410.07483</link>
      <description>arXiv:2410.07483v1 Announce Type: new 
Abstract: In clinical trials, the observation of participant outcomes may frequently be hindered by death, leading to ambiguity in defining a scientifically meaningful final outcome for those who die. Principal stratification methods are valuable tools for addressing the average causal effect among always-survivors, i.e., the average treatment effect among a subpopulation in the principal strata of those who would survive regardless of treatment assignment. Although robust methods for the truncation-by-death problem in two-arm clinical trials have been previously studied, its expansion to multi-arm clinical trials remains unknown. In this article, we study the identification of a class of survivor average causal effect estimands with multiple treatments under monotonicity and principal ignorability, and first propose simple weighting and regression approaches. As a further improvement, we then derive the efficient influence function to motivate doubly robust estimators for the survivor average causal effects in multi-arm clinical trials. We also articulate sensitivity methods under violations of key causal assumptions. Extensive simulations are conducted to investigate the finite-sample performance of the proposed methods, and a real data example is used to illustrate how to operationalize the proposed estimators and the sensitivity methods in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07483v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiaqi Tong, Chao Cheng, Guangyu Tong, Michael O. Harhay, Fan Li</dc:creator>
    </item>
    <item>
      <title>Learning associations of COVID-19 hospitalizations with wastewater viral signals by Markov modulated models</title>
      <link>https://arxiv.org/abs/2410.07487</link>
      <description>arXiv:2410.07487v1 Announce Type: new 
Abstract: Viral signal in wastewater offers a promising opportunity to assess and predict the burden of infectious diseases. That has driven the widespread adoption and development of wastewater monitoring tools by public health organizations. Recent research highlights a strong correlation between COVID-19 hospitalizations and wastewater viral signals, and validates that increases in wastewater measurements may offer early warnings of an increase in hospital admissions. Previous studies (e.g. Peng et al. 2023) utilize distributed lag models to explore associations of COVID-19 hospitalizations with lagged SARS-CoV-2 wastewater viral signals. However, the conventional distributed lag models assume the duration time of the lag to be fixed, which is not always plausible. This paper presents Markov-modulated models with distributed lasting time, treating the duration of the lag as a random variable defined by a hidden Markov chain. We evaluate exposure effects over the duration time and estimate the distribution of the lasting time using the wastewater data and COVID-19 hospitalization records from Ottawa, Canada during June 2020 to November 2022. The different COVID-19 waves are accommodated in the statistical learning. In particular, two strategies for comparing the associations over different time intervals are exemplified using the Ottawa data. Of note, the proposed Markov modulated models, an extension of distributed lag models, are potentially applicable to many different problems where the lag time is not fixed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07487v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. Ken Peng, Charmaine B. Dean, Robert Delatolla, X. Joan Hu</dc:creator>
    </item>
    <item>
      <title>A regression framework for studying relationships among attributes under network interference</title>
      <link>https://arxiv.org/abs/2410.07555</link>
      <description>arXiv:2410.07555v1 Announce Type: new 
Abstract: To understand how the interconnected and interdependent world of the twenty-first century operates and make model-based predictions, joint probability models for networks and interdependent outcomes are needed. We propose a comprehensive regression framework for networks and interdependent outcomes with multiple advantages, including interpretability, scalability, and provable theoretical guarantees. The regression framework can be used for studying relationships among attributes of connected units and captures complex dependencies among connections and attributes, while retaining the virtues of linear regression, logistic regression, and other regression models by being interpretable and widely applicable. On the computational side, we show that the regression framework is amenable to scalable statistical computing based on convex optimization of pseudo-likelihoods using minorization-maximization methods. On the theoretical side, we establish convergence rates for pseudo-likelihood estimators based on a single observation of dependent connections and attributes. We demonstrate the regression framework using simulations and an application to hate speech on the social media platform X in the six months preceding the insurrection at the U.S. Capitol on January 6, 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07555v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cornelius Fritz, Michael Schweinberger, Subhankar Bhadra, David R. Hunter</dc:creator>
    </item>
    <item>
      <title>panelPomp: Analysis of Panel Data via Partially Observed Markov Processes in R</title>
      <link>https://arxiv.org/abs/2410.07934</link>
      <description>arXiv:2410.07934v1 Announce Type: new 
Abstract: Panel data arise when time series measurements are collected from multiple, dynamically independent but structurally related systems. In such cases, each system's time series can be modeled as a partially observed Markov process (POMP), and the ensemble of these models is called a PanelPOMP. If the time series are relatively short, statistical inference for each time series must draw information from across the entire panel. Every time series has a name, called its unit label, which may correspond to an object on which that time series was collected. Differences between units may be of direct inferential interest or may be a nuisance for studying the commonalities. The R package panelPomp supports analysis of panel data via a general class of PanelPOMP models. This includes a suite of tools for manipulation of models and data that take advantage of the panel structure. The panelPomp package currently emphasizes recent advances enabling likelihood-based inference via simulation-based algorithms. However, the general framework provided by panelPomp supports development of additional, new inference methodology for panel data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07934v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carles Bret\'o, Jesse Wheeler, Aaron A. King, Edward L. Ionides</dc:creator>
    </item>
    <item>
      <title>Smoothed pseudo-population bootstrap methods with applications to finite population quantiles</title>
      <link>https://arxiv.org/abs/2410.07996</link>
      <description>arXiv:2410.07996v1 Announce Type: new 
Abstract: This paper introduces smoothed pseudo-population bootstrap methods for the purposes of variance estimation and the construction of confidence intervals for finite population quantiles. In an i.i.d. context, it has been shown that resampling from a smoothed estimate of the distribution function instead of the usual empirical distribution function can improve the convergence rate of the bootstrap variance estimator of a sample quantile. We extend the smoothed bootstrap to the survey sampling framework by implementing it in pseudo-population bootstrap methods for high entropy, single-stage survey designs, such as simple random sampling without replacement and Poisson sampling. Given a kernel function and a bandwidth, it consists of smoothing the pseudo-population from which bootstrap samples are drawn using the original sampling design. Given that the implementation of the proposed algorithms requires the specification of the bandwidth, we develop a plug-in selection method along with a grid search selection method based on a bootstrap estimate of the mean squared error. Simulation results suggest a gain in efficiency associated with the smoothed approach as compared to the standard pseudo-population bootstrap for estimating the variance of a quantile estimator together with mixed results regarding confidence interval coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07996v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vanessa McNealis, Christian L\'eger</dc:creator>
    </item>
    <item>
      <title>Negative Control Outcome Adjustment in Early-Phase Randomized Trials: Estimating Vaccine Effects on Immune Responses in HIV Exposed Uninfected Infants</title>
      <link>https://arxiv.org/abs/2410.08078</link>
      <description>arXiv:2410.08078v1 Announce Type: new 
Abstract: Adjustment for prognostic baseline covariates when estimating treatment effects in randomized trials can reduce bias due to covariate imbalance and yield guaranteed efficiency gain in large-samples. Gains in precision and reductions in finite-sample bias are arguably most important in the resource-limited setting of early-phase trials. Despite their favorable large-sample properties, the utility of covariate-adjusted estimators in early-phase trials is complicated by precision loss due to adjustment for weakly prognostic covariates and type I error rate inflation and undercoverage of asymptotic confidence intervals in finite samples. We propose adjustment for a valid negative control outcomes (NCO), or an auxiliary post-randomization outcome assumed completely unaffected by treatment but correlated with the outcome of interest. We articulate the causal assumptions that permit adjustment for NCOs, describe when NCO adjustment may improve upon adjustment for baseline covariates, illustrate performance and provide practical recommendations regarding model selection and finite-sample variance corrections in early-phase trials using numerical experiments, and demonstrate superior performance of NCO adjustment in the reanalysis of two early-phase vaccine trials in HIV exposed uninfected (HEU) infants. In early-phase studies without knowledge of baseline predictors of outcomes, we advocate for eschewing baseline covariates adjustment in favor of adjustment for NCOs believed to be unaffected by the experimental intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08078v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ethan Ashby, Bo Zhang, Genevieve G Fouda, Youyi Fong, Holly Janes</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Sensitivity Analysis of Multiple Comparisons Under Dependence</title>
      <link>https://arxiv.org/abs/2410.08080</link>
      <description>arXiv:2410.08080v1 Announce Type: new 
Abstract: This short communication introduces a sensitivity analysis method for Multiple Testing Procedures (MTPs), based on marginal $p$-values and the Dirichlet process prior distribution. The method measures each $p$-value's insensitivity towards a significance decision, with respect to the entire space of MTPs controlling either the family-wise error rate (FWER) or the false discovery rate (FDR) under arbitrary dependence between $p$-values, supported by this nonparametric prior. The sensitivity analysis method is illustrated through 1,081 hypothesis tests of the effects of the COVID-19 pandemic on educational processes for 15-year-old students, performed on a 2022 public dataset. Software code for the method is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08080v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Karabatsos</dc:creator>
    </item>
    <item>
      <title>Curb Your Attention: Causal Attention Gating for Robust Trajectory Prediction in Autonomous Driving</title>
      <link>https://arxiv.org/abs/2410.07191</link>
      <description>arXiv:2410.07191v1 Announce Type: cross 
Abstract: Trajectory prediction models in autonomous driving are vulnerable to perturbations from non-causal agents whose actions should not affect the ego-agent's behavior. Such perturbations can lead to incorrect predictions of other agents' trajectories, potentially compromising the safety and efficiency of the ego-vehicle's decision-making process. Motivated by this challenge, we propose $\textit{Causal tRajecTory predICtion}$ $\textbf{(CRiTIC)}$, a novel model that utilizes a $\textit{Causal Discovery Network}$ to identify inter-agent causal relations over a window of past time steps. To incorporate discovered causal relationships, we propose a novel $\textit{Causal Attention Gating}$ mechanism to selectively filter information in the proposed Transformer-based architecture. We conduct extensive experiments on two autonomous driving benchmark datasets to evaluate the robustness of our model against non-causal perturbations and its generalization capacity. Our results indicate that the robustness of predictions can be improved by up to $\textbf{54%}$ without a significant detriment to prediction accuracy. Lastly, we demonstrate the superior domain generalizability of the proposed model, which achieves up to $\textbf{29%}$ improvement in cross-domain performance. These results underscore the potential of our model to enhance both robustness and generalization capacity for trajectory prediction in diverse autonomous driving domains. Further details can be found on our project page: https://critic-model.github.io/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07191v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Ahmadi, Ray Mercurius, Soheil Alizadeh, Kasra Rezaee, Amir Rasouli</dc:creator>
    </item>
    <item>
      <title>A multivariate spatial regression model using signatures</title>
      <link>https://arxiv.org/abs/2410.07899</link>
      <description>arXiv:2410.07899v1 Announce Type: cross 
Abstract: We propose a spatial autoregressive model for a multivariate response variable and functional covariates. The approach is based on the notion of signature, which represents a function as an infinite series of its iterated integrals and presents the advantage of being applicable to a wide range of processes. We have provided theoretical guarantees for the choice of the signature truncation order, and we have shown in a simulation study that this approach outperforms existing approaches in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07899v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Camille Fr\'event, Issa-Mbenard Dabo</dc:creator>
    </item>
    <item>
      <title>General Covariance-Based Conditions for Central Limit Theorems with Dependent Triangular Arrays</title>
      <link>https://arxiv.org/abs/2308.12506</link>
      <description>arXiv:2308.12506v4 Announce Type: replace 
Abstract: We present a general central limit theorem with simple, easy-to-check covariance-based sufficient conditions for triangular arrays of random vectors when all variables could be interdependent. The result is constructed from Stein's method, but the conditions are distinct from related work. We show that these covariance conditions nest standard assumptions studied in the literature such as $M$-dependence, mixing random fields, non-mixing autoregressive processes, and dependency graphs, which themselves need not imply each other. This permits researchers to work with high-level but intuitive conditions based on overall correlation instead of more complicated and restrictive conditions such as strong mixing in random fields that may not have any obvious micro-foundation. As examples of the implications, we show how the theorem implies asymptotic normality in estimating: treatment effects with spillovers in more settings than previously admitted, covariance matrices, processes with global dependencies such as epidemic spread and information diffusion, and spatial process with Mat\'{e}rn dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12506v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arun G. Chandrasekhar, Matthew O. Jackson, Tyler H. McCormick, Vydhourie Thiyageswaran</dc:creator>
    </item>
    <item>
      <title>Addressing selection bias in cluster randomized experiments via weighting</title>
      <link>https://arxiv.org/abs/2309.07365</link>
      <description>arXiv:2309.07365v2 Announce Type: replace 
Abstract: In cluster randomized experiments, individuals are often recruited after the cluster treatment assignment, and data are typically only available for the recruited sample. Post-randomization recruitment can lead to selection bias, inducing systematic differences between the overall and the recruited populations, and between the recruited intervention and control arms. In this setting, we define causal estimands for the overall and the recruited populations. We prove, under the assumption of ignorable recruitment, that the average treatment effect on the recruited population can be consistently estimated from the recruited sample using inverse probability weighting. Generally we cannot identify the average treatment effect on the overall population. Nonetheless, we show, via a principal stratification formulation, that one can use weighting of the recruited sample to identify treatment effects on two meaningful subpopulations of the overall population: individuals who would be recruited into the study regardless of the assignment, and individuals who would be recruited into the study under treatment but not under control. We develop an estimation strategy and a sensitivity analysis approach for checking the ignorable recruitment assumption. The proposed methods are applied to the ARTEMIS cluster randomized trial, where removing co-payment barriers increases the persistence of P2Y12 inhibitor among the always-recruited population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07365v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Georgia Papadogeorgou, Bo Liu, Fan Li, Fan Li</dc:creator>
    </item>
    <item>
      <title>Sparse higher order partial least squares for simultaneous variable selection, dimension reduction, and tensor denoising</title>
      <link>https://arxiv.org/abs/2310.09428</link>
      <description>arXiv:2310.09428v2 Announce Type: replace 
Abstract: Partial Least Squares (PLS) regression emerged as an alternative to ordinary least squares for addressing multicollinearity in a wide range of scientific applications. As multidimensional tensor data is becoming more widespread, tensor adaptations of PLS have been developed. In this paper, we first establish the statistical behavior of Higher Order PLS (HOPLS) of Zhao et al. (2012), by showing that the consistency of the HOPLS estimator cannot be guaranteed as the tensor dimensions and the number of features increase faster than the sample size. To tackle this issue, we propose Sparse Higher Order Partial Least Squares (SHOPS) regression and an accompanying algorithm. SHOPS simultaneously accommodates variable selection, dimension reduction, and tensor response denoising. We further establish the asymptotic results of the SHOPS algorithm under a high-dimensional regime. The results also complete the unknown theoretic properties of SPLS algorithm (Chun and Kele\c{s}, 2010). We verify these findings through comprehensive simulation experiments, and application to an emerging high-dimensional biological data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09428v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kwangmoon Park, S\"und\"uz Kele\c{s}</dc:creator>
    </item>
    <item>
      <title>Euclidean mirrors and first-order changepoints in network time series</title>
      <link>https://arxiv.org/abs/2405.11111</link>
      <description>arXiv:2405.11111v3 Announce Type: replace 
Abstract: We describe a model for a network time series whose evolution is governed by an underlying stochastic process, known as the latent position process, in which network evolution can be represented in Euclidean space by a curve, called the Euclidean mirror. We define the notion of a first-order changepoint for a time series of networks, and construct a family of latent position process networks with underlying first-order changepoints. We prove that a spectral estimate of the associated Euclidean mirror localizes these changepoints, even when the graph distribution evolves continuously, but at a rate that changes. Simulated and real data examples on organoid networks show that this localization captures empirically significant shifts in network evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11111v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Chen, Zachary Lubberts, Avanti Athreya, Youngser Park, Carey E. Priebe</dc:creator>
    </item>
    <item>
      <title>Fast leave-one-cluster-out cross-validation using clustered Network Information Criterion (NICc)</title>
      <link>https://arxiv.org/abs/2405.20400</link>
      <description>arXiv:2405.20400v2 Announce Type: replace 
Abstract: For prediction models developed on clustered data that do not account for cluster heterogeneity in model parameterization, it is crucial to use cluster-based validation to assess model generalizability on unseen clusters. This paper introduces a clustered estimator of the Network Information Criterion (NICc) to approximate leave-one-cluster-out deviance for standard prediction models with twice differentiable log-likelihood functions. The NICc serves as a fast alternative to cluster-based cross-validation. Stone (1977) proved that the Akaike Information Criterion (AIC) is asymptotically equivalent to leave-one-observation-out cross-validation for true parametric models with independent and identically distributed observations. Ripley (1996) noted that the Network Information Criterion (NIC), derived from Stone's proof, is a better approximation when the model is misspecified. For clustered data, we derived NICc by substituting the Fisher information matrix in the NIC with a clustering-adjusted estimator. The NICc imposes a greater penalty when the data exhibits stronger clustering, thereby allowing the NICc to better prevent over-parameterization. In a simulation study and an empirical example, we used standard regression to develop prediction models for clustered data with Gaussian or binomial responses. Compared to the commonly used AIC and BIC for standard regression, NICc provides a much more accurate approximation to leave-one-cluster-out deviance and results in more accurate model size and variable selection, as determined by cluster-based cross-validation, especially when the data exhibit strong clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20400v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxing Qiu, Douglas E. Lake, Pavel Chernyavskiy, Teague R. Henry</dc:creator>
    </item>
    <item>
      <title>Chasing Shadows: How Implausible Assumptions Skew Our Understanding of Causal Estimands</title>
      <link>https://arxiv.org/abs/2409.11162</link>
      <description>arXiv:2409.11162v2 Announce Type: replace 
Abstract: The ICH E9 (R1) addendum on estimands, coupled with recent advancements in causal inference, has prompted a shift towards using model-free treatment effect estimands that are more closely aligned with the underlying scientific question. This represents a departure from traditional, model-dependent approaches where the statistical model often overshadows the inquiry itself. While this shift is a positive development, it has unintentionally led to the prioritization of an estimand's ability to perfectly answer the key scientific question over its practical learnability from data under plausible assumptions. We illustrate this by scrutinizing assumptions in the recent clinical trials literature on principal stratum estimands, demonstrating that some popular assumptions are not only implausible but often inevitably violated. We advocate for a more balanced approach to estimand formulation, one that carefully considers both the scientific relevance and the practical feasibility of estimation under realistic conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11162v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stijn Vansteelandt, Kelly Van Lancker</dc:creator>
    </item>
    <item>
      <title>Bayesian estimation for novel geometric INGARCH model</title>
      <link>https://arxiv.org/abs/2410.01283</link>
      <description>arXiv:2410.01283v2 Announce Type: replace 
Abstract: This paper introduces an integer-valued generalized autoregressive conditional heteroskedasticity (INGARCH) model based on the novel geometric distribution and discusses some of its properties. The parameter estimation problem of the models are studied by conditional maximum likelihood and Bayesian approach using Hamiltonian Monte Carlo (HMC) algorithm. The results of the simulation studies and real data analysis affirm the good performance of the estimators and the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01283v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Divya Kuttenchalil Andrews, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>When Does Interference Matter? Decision-Making in Platform Experiments</title>
      <link>https://arxiv.org/abs/2410.06580</link>
      <description>arXiv:2410.06580v2 Announce Type: replace 
Abstract: This paper investigates decision-making in A/B experiments for online platforms and marketplaces. In such settings, due to constraints on inventory, A/B experiments typically lead to biased estimators because of interference; this phenomenon has been well studied in recent literature. By contrast, there has been relatively little discussion of the impact of interference on decision-making. In this paper, we analyze a benchmark Markovian model of an inventory-constrained platform, where arriving customers book listings that are limited in supply; our analysis builds on a self-contained analysis of general A/B experiments for Markov chains. We focus on the commonly used frequentist hypothesis testing approach for making launch decisions based on data from customer-randomized experiments, and we study the impact of interference on (1) false positive probability and (2) statistical power.
  We obtain three main findings. First, we show that for monotone treatments -- i.e., those where the treatment changes booking probabilities in the same direction relative to control in all states -- the false positive probability of the na\"ive difference-in-means estimator with classical variance estimation is correctly controlled. This result stems from a novel analysis of A/A experiments with arbitrary dependence structures, which may be of independent interest. Second, we demonstrate that for monotone treatments, the statistical power of this na\"ive approach is higher than that of any similar pipeline using a debiased estimator. Taken together, these two findings suggest that platforms may be better off not debiasing when treatments are monotone. Finally, using simulations, we investigate false positive probability and statistical power when treatments are non-monotone, and we show that the performance of the na\"ive approach can be arbitrarily worse than a debiased approach in such cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06580v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramesh Johari, Hannah Li, Anushka Murthy, Gabriel Y. Weintraub</dc:creator>
    </item>
    <item>
      <title>On the role of surrogates in the efficient estimation of treatment effects with limited outcome data</title>
      <link>https://arxiv.org/abs/2003.12408</link>
      <description>arXiv:2003.12408v5 Announce Type: replace-cross 
Abstract: In many experimental and observational studies, the outcome of interest is often difficult or expensive to observe, reducing effective sample sizes for estimating average treatment effects (ATEs) even when identifiable. We study how incorporating data on units for which only surrogate outcomes not of primary interest are observed can increase the precision of ATE estimation. We refrain from imposing stringent surrogacy conditions, which permit surrogates as perfect replacements for the target outcome. Instead, we supplement the available, albeit limited, observations of the target outcome with abundant observations of surrogate outcomes, without any assumptions beyond unconfounded treatment assignment and missingness and corresponding overlap conditions. To quantify the potential gains, we derive the difference in efficiency bounds on ATE estimation with and without surrogates, both when an overwhelming or comparable number of units have missing outcomes. We develop robust ATE estimation and inference methods that realize these efficiency gains. We empirically demonstrate the gains by studying long-term-earning effects of job training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2003.12408v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Kallus, Xiaojie Mao</dc:creator>
    </item>
    <item>
      <title>Modeling Causal Mechanisms with Diffusion Models for Interventional and Counterfactual Queries</title>
      <link>https://arxiv.org/abs/2302.00860</link>
      <description>arXiv:2302.00860v3 Announce Type: replace-cross 
Abstract: We consider the problem of answering observational, interventional, and counterfactual queries in a causally sufficient setting where only observational data and the causal graph are available. Utilizing the recent developments in diffusion models, we introduce diffusion-based causal models (DCM) to learn causal mechanisms, that generate unique latent encodings. These encodings enable us to directly sample under interventions and perform abduction for counterfactuals. Diffusion models are a natural fit here, since they can encode each node to a latent representation that acts as a proxy for exogenous noise. Our empirical evaluations demonstrate significant improvements over existing state-of-the-art methods for answering causal queries. Furthermore, we provide theoretical results that offer a methodology for analyzing counterfactual estimation in general encoder-decoder models, which could be useful in settings beyond our proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.00860v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Chao, Patrick Bl\"obaum, Sapan Patel, Shiva Prasad Kasiviswanathan</dc:creator>
    </item>
    <item>
      <title>Balancing Application Relevant and Sparsity Revealing Excitation in Input Design</title>
      <link>https://arxiv.org/abs/2402.06048</link>
      <description>arXiv:2402.06048v2 Announce Type: replace-cross 
Abstract: The maximum absolute correlation between regressors, which is called mutual coherence, plays an essential role in sparse estimation. A regressor matrix whose columns are highly correlated may result from optimal input design, since there is no constraint on the mutual coherence, making it difficult to handle sparse estimation. This paper aims to tackle this issue for fixed denominator models, which include Laguerre, Kautz, and generalized orthonormal basis function expansion models, for example.
  The paper proposes an optimal input design method where the achieved Fisher information matrix is fitted to the desired Fisher matrix, together with a coordinate transformation designed to make the regressors in the transformed coordinates have low mutual coherence. The method can be used together with any sparse estimation method and any desired Fisher matrix. A numerical study shows its potential for alleviating the problem of model order selection when used in conjunction with, for example, classical methods such as the Akaike Information Criterion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06048v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.ME</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javad Parsa, Cristian R. Rojas, H{\aa}kan Hjalmarsson</dc:creator>
    </item>
    <item>
      <title>With random regressors, least squares inference is robust to correlated errors with unknown correlation structure</title>
      <link>https://arxiv.org/abs/2410.05567</link>
      <description>arXiv:2410.05567v2 Announce Type: replace-cross 
Abstract: Linear regression is arguably the most widely used statistical method. With fixed regressors and correlated errors, the conventional wisdom is to modify the variance-covariance estimator to accommodate the known correlation structure of the errors. We depart from the literature by showing that with random regressors, linear regression inference is robust to correlated errors with unknown correlation structure. The existing theoretical analyses for linear regression are no longer valid because even the asymptotic normality of the least-squares coefficients breaks down in this regime. We first prove the asymptotic normality of the t statistics by establishing their Berry-Esseen bounds based on a novel probabilistic analysis of self-normalized statistics. We then study the local power of the corresponding t tests and show that, perhaps surprisingly, error correlation can even enhance power in the regime of weak signals. Overall, our results show that linear regression is applicable more broadly than the conventional theory suggests, and further demonstrate the value of randomization to ensure robustness of inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05567v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zifeng Zhang, Peng Ding, Wen Zhou, Haonan Wang</dc:creator>
    </item>
  </channel>
</rss>
