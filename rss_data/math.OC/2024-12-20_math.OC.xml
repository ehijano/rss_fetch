<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Dec 2024 05:00:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Accelerating Reductions Using Graph Neural Networks and a New Concurrent Local Search for the Maximum Weight Independent Set Problem</title>
      <link>https://arxiv.org/abs/2412.14198</link>
      <description>arXiv:2412.14198v1 Announce Type: new 
Abstract: The Maximum Weight Independent Set problem is a fundamental NP-hard problem in combinatorial optimization with several real-world applications. Given an undirected vertex-weighted graph, the problem is to find a subset of the vertices with the highest possible weight under the constraint that no two vertices in the set can share an edge. An important part of solving this problem in both theory and practice is data reduction rules, which several state-of-the-art algorithms rely on. However, the most complicated rules are often not used in applications since the time needed to check them exhaustively becomes infeasible. In this work, we introduce three main results. First, we introduce several new data reduction rules and evaluate their effectiveness on real-world data. Second, we use a machine learning screening algorithm to speed up the reduction phase, thereby enabling more complicated rules to be applied. Our screening algorithm consults a Graph Neural Network oracle to decide if the probability of successfully reducing the graph is sufficiently large. For this task, we provide a dataset of labeled vertices for use in supervised learning. We also present the first results for this dataset using established Graph Neural Network architectures. Third, we present a new concurrent metaheuristic called Concurrent Difference-Core Heuristic. On the reduced instances, we use our new metaheuristic combined with iterated local search, called CHILS (Concurrent Hybrid Iterated Local Search). For this iterated local search, we provide a new implementation specifically designed to handle large graphs of varying densities. CHILS outperforms the current state-of-the-art on all commonly used benchmark instances, especially the largest ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14198v1</guid>
      <category>math.OC</category>
      <category>math.CO</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ernestine Gro{\ss}mann, Kenneth Langedal, Christian Schulz</dc:creator>
    </item>
    <item>
      <title>A universal convergence theorem for primal-dual penalty and augmented Lagrangian methods</title>
      <link>https://arxiv.org/abs/2412.14269</link>
      <description>arXiv:2412.14269v1 Announce Type: new 
Abstract: We present a so-called universal convergence theorem for inexact primal-dual penalty and augmented Lagrangian methods that can be applied to a large number of such methods and reduces their convergence analysis to verification of some simple conditions on sequences generated by these methods. If these conditions are verified, then both primal and dual convergence follow directly from the universal convergence theorem. This theorem allows one not only to derive standard convergence theorems for many existing primal-dual penalty and augmented Lagrangian methods in a unified and straightforward manner, but also to strengthen and generalize some of these theorems. In particular, we show how with the use of the universal convergence theorem one can significantly improve some existing results on convergence of a primal-dual rounded weighted $\ell_1$-penalty method, an augmented Lagrangian method for cone constrained optimization, and some other primal-dual methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14269v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. V. Dolgopolik</dc:creator>
    </item>
    <item>
      <title>Augmenting Subspace Optimization Methods with Linear Bandits</title>
      <link>https://arxiv.org/abs/2412.14278</link>
      <description>arXiv:2412.14278v1 Announce Type: new 
Abstract: We consider the framework of methods for unconstrained minimization that are, in each iteration, restricted to a model that is only a valid approximation to the objective function on some affine subspace containing an incumbent point. These methods are of practical interest in computational settings where derivative information is either expensive or impossible to obtain. Recent attention has been paid in the literature to employing randomized matrix sketching for generating the affine subspaces within this framework.
  We consider a relatively straightforward, deterministic augmentation of such a generic subspace optimization method. In particular, we consider a sequential optimization framework where actions consist of one-dimensional linear subspaces and rewards consist of (approximations to) the magnitudes of directional derivatives computed in the direction of the action subspace. Reward maximization in this context is consistent with maximizing lower bounds on descent guaranteed by first-order Taylor models. This sequential optimization problem can be analyzed through the lens of dynamic regret. We modify an existing linear upper confidence bound (UCB) bandit method and prove sublinear dynamic regret in the subspace optimization setting. We demonstrate the efficacy of employing this linear UCB method in a setting where forward-mode algorithmic differentiation can provide directional derivatives in arbitrary directions and in a derivative-free setting. For the derivative-free setting, we propose SS-POUNDers, an extension of the derivative-free optimization method POUNDers that employs the linear UCB mechanism to identify promising subspaces. Our numerical experiments suggest a preference, in either computational setting, for employing a linear UCB mechanism within a subspace optimization method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14278v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matt Menickelly</dc:creator>
    </item>
    <item>
      <title>Projected gradient methods for nonconvex and stochastic optimization: new complexities and auto-conditioned stepsizes</title>
      <link>https://arxiv.org/abs/2412.14291</link>
      <description>arXiv:2412.14291v1 Announce Type: new 
Abstract: We present a novel class of projected gradient (PG) methods for minimizing a smooth but not necessarily convex function over a convex compact set. We first provide a novel analysis of the "vanilla" PG method, achieving the best-known iteration complexity for finding an approximate stationary point of the problem. We then develop an "auto-conditioned" projected gradient (AC-PG) variant that achieves the same iteration complexity without requiring the input of the Lipschitz constant of the gradient or any line search procedure. The key idea is to estimate the Lipschitz constant using first-order information gathered from the previous iterations, and to show that the error caused by underestimating the Lipschitz constant can be properly controlled. We then generalize the PG methods to the stochastic setting, by proposing a stochastic projected gradient (SPG) method and a variance-reduced stochastic gradient (VR-SPG) method, achieving new complexity bounds in different oracle settings. We also present auto-conditioned stepsize policies for both stochastic PG methods and establish comparable convergence guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14291v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guanghui Lan, Tianjiao Li, Yangyang Xu</dc:creator>
    </item>
    <item>
      <title>Randomized Subspace Derivative-Free Optimization with Quadratic Models and Second-Order Convergence</title>
      <link>https://arxiv.org/abs/2412.14431</link>
      <description>arXiv:2412.14431v1 Announce Type: new 
Abstract: We consider model-based derivative-free optimization (DFO) for large-scale problems, based on iterative minimization in random subspaces. We provide the first worst-case complexity bound for such methods for convergence to approximate second-order critical points, and show that these bounds have significantly improved dimension dependence compared to standard full-space methods, provided low accuracy solutions are desired and/or the problem has low effective rank. We also introduce a practical subspace model-based method suitable for general objective minimization, based on iterative quadratic interpolation in subspaces, and show that it can solve significantly larger problems than state-of-the-art full-space methods, while also having comparable performance on medium-scale problems when allowed to use full-dimension subspaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14431v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Coralia Cartis, Lindon Roberts</dc:creator>
    </item>
    <item>
      <title>Instability of the Environment as a Necessary Condition for Optimal Control of an Economic Object</title>
      <link>https://arxiv.org/abs/2412.14448</link>
      <description>arXiv:2412.14448v1 Announce Type: new 
Abstract: A review of economic approaches showed the lack of a universal method for assessing management decisions in the face of an increasing volume of analyzed data and changing parameters of the external environment. The method of integral indicators is proposed. Integral indicators are one of the modern methods for researching the behavior of an enterprise. It provides an assessment of the impact of the external environment. It shows the ability of the enterprise to adapt to new conditions. The dynamics of the correlation indicator shows the reaction of the enterprise to the impact of external factors. The purpose of the scientific work was achieved: the optimal control of the enterprise was carried out in the conditions of changing the parameters of the external environment For this, the model of the economic object and the method of its analysis are formalized. The structure of an economic object (enterprise) is given. The characteristics of the parameters of the external environment are given. The state of an economic object (enterprise) is modeled taking into account the influence of the external environment. With the help of the software package created by the author, six optimal options for control decisions have been analyzed. The state of an economic object has been modeled depending on the state of the external environment by 5,000 parameters. The research showed significant changes in the values of the correlation of the parameters of the system and the intensity of business processes when the conditions for the functioning of the system change. The optimal control of an economic object (enterprise) is selected according to the integral indicator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14448v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2991/aebmr.k.210222.066</arxiv:DOI>
      <dc:creator>Sergey Masaev</dc:creator>
    </item>
    <item>
      <title>Stochastic first-order methods with multi-extrapolated momentum for highly smooth unconstrained optimization</title>
      <link>https://arxiv.org/abs/2412.14488</link>
      <description>arXiv:2412.14488v1 Announce Type: new 
Abstract: In this paper we consider an unconstrained stochastic optimization problem where the objective function exhibits a high order of smoothness. In particular, we propose a stochastic first-order method (SFOM) with multi-extrapolated momentum, in which multiple extrapolations are performed in each iteration, followed by a momentum step based on these extrapolations. We show that our proposed SFOM with multi-extrapolated momentum can accelerate optimization by exploiting the high-order smoothness of the objective function $f$. Specifically, assuming that the gradient and the $p$th-order derivative of $f$ are Lipschitz continuous for some $p\ge2$, and under some additional mild assumptions, we establish that our method achieves a sample complexity of $\widetilde{\mathcal{O}}(\epsilon^{-(3p+1)/p})$ for finding a point $x$ satisfying $\mathbb{E}[\|\nabla f(x)\|]\le\epsilon$. To the best of our knowledge, our method is the first SFOM to leverage arbitrary order smoothness of the objective function for acceleration, resulting in a sample complexity that strictly improves upon the best-known results without assuming the average smoothness condition. Finally, preliminary numerical experiments validate the practical performance of our method and corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14488v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chuan He</dc:creator>
    </item>
    <item>
      <title>Delayed Feedback in Online Non-Convex Optimization: A Non-Stationary Approach with Applications</title>
      <link>https://arxiv.org/abs/2412.14506</link>
      <description>arXiv:2412.14506v1 Announce Type: new 
Abstract: We study non-convex delayed-noise online optimization problems by evaluating dynamic regret in the non-stationary setting when the loss functions are quasar-convex. In particular, we consider scenarios involving quasar-convex functions either with a Lipschitz gradient or weakly smooth and, for each case, we ensure bounded dynamic regret in terms of cumulative path variation achieving sub-linear regret rates. Furthermore, we illustrate the flexibility of our framework by applying it to both theoretical settings such as zeroth-order (bandit) and also to practical applications with quadratic fractional functions. Moreover, we provide new examples of non-convex functions that are quasar-convex by proving that the class of differentiable strongly quasiconvex functions (Polyak 1966) are strongly quasar-convex on convex compact sets. Finally, several numerical experiments validate our theoretical findings, illustrating the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14506v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felipe Lara, Cristian Vega</dc:creator>
    </item>
    <item>
      <title>Complexities of Armijo-like algorithms in Deep Learning context</title>
      <link>https://arxiv.org/abs/2412.14637</link>
      <description>arXiv:2412.14637v1 Announce Type: new 
Abstract: The classical Armijo backtracking algorithm achieves the optimal complexity for smooth functions like gradient descent but without any hyperparameter tuning. However, the smoothness assumption is not suitable for Deep Learning optimization. In this work, we show that some variants of the Armijo optimizer achieves acceleration and optimal complexities under assumptions more suited for Deep Learning: the (L 0 , L 1 ) smoothness condition and analyticity. New dependences on the smoothness constants and the initial gap are established. The results theoretically highlight the powerful efficiency of Armijo-like conditions for highly non-convex problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14637v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bensaid Bilel (IMB)</dc:creator>
    </item>
    <item>
      <title>Computing All Shortest Passenger Routes with a Tropical Dijkstra Algorithm</title>
      <link>https://arxiv.org/abs/2412.14654</link>
      <description>arXiv:2412.14654v1 Announce Type: new 
Abstract: Given a public transportation network, which and how many passenger routes can potentially be shortest paths, when all possible timetables are taken into account? This question leads to shortest path problems on graphs with interval costs on their arcs and is closely linked to multi-objective optimization. We introduce a Dijkstra algorithm based on polynomials over the tropical semiring that computes complete or minimal sets of efficient paths. We demonstrate that this approach is computationally feasible by employing it on the public transport network of the city of Wuppertal and instances of the benchmarking set TimPassLib, and we evaluate the resulting sets of passenger routes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14654v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Berenike Masing, Niels Lindner, Enrico Bortoletto</dc:creator>
    </item>
    <item>
      <title>Analyzing the numerical correctness of branch-and-bound decisions for mixed-integer programming</title>
      <link>https://arxiv.org/abs/2412.14710</link>
      <description>arXiv:2412.14710v1 Announce Type: new 
Abstract: Most state-of-the-art branch-and-bound solvers for mixed-integer linear programming rely on limited-precision floating-point arithmetic and use numerical tolerances when reasoning about feasibility and optimality during their search. While the practical success of floating-point MIP solvers bears witness to their overall numerical robustness, it is well-known that numerically challenging input can lead them to produce incorrect results. Even when their final answer is correct, one critical question remains: Were the individual decisions taken during branch-and-bound justified, i.e., can they be verified in exact arithmetic? In this paper, we attempt a first such a posteriori analysis of a pure LP-based branch-and-bound solver by checking all intermediate decisions critical to the correctness of the result: accepting solutions as integer feasible, declaring the LP relaxation infeasible, and pruning subtrees as subopti mal. Our computational study in the academic MIP solver SCIP confirms the expectation that in the overwhelming majority of cases, all decisions are correct. When errors do occur on numerically challenging instances, they typically affect only a small, typically single-digit, amount of leaf nodes that would require further processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14710v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Hoen, Ambros Gleixner</dc:creator>
    </item>
    <item>
      <title>Provably Convergent Plug-and-play Proximal Block Coordinate Descent Method for Hyperspectral Anomaly Detection</title>
      <link>https://arxiv.org/abs/2412.14824</link>
      <description>arXiv:2412.14824v1 Announce Type: new 
Abstract: Hyperspectral anomaly detection refers to identifying pixels in the hyperspectral images that have spectral characteristics significantly different from the background. In this paper, we introduce a novel model that represents the background information using a low-rank representation. We integrate an implicit proximal denoiser prior, associated with a deep learning based denoiser, within a plug-and-play (PnP) framework to effectively remove noise from the eigenimages linked to the low-rank representation. Anomalies are characterized using a generalized group sparsity measure, denoted as $\|\cdot\|_{2,\psi}$. To solve the resulting orthogonal constrained nonconvex nonsmooth optimization problem, we develop a PnP-proximal block coordinate descent (PnP-PBCD) method, where the eigenimages are updated using a proximal denoiser within the PnP framework. We prove that any accumulation point of the sequence generated by the PnP-PBCD method is a stationary point. We evaluate the effectiveness of the PnP-PBCD method on hyperspectral anomaly detection in scenarios with and without Gaussian noise contamination. The results demonstrate that the proposed method can effectively detect anomalous objects, outperforming the competing methods that may mistakenly identify noise as anomalies or misidentify the anomalous objects due to noise interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14824v1</guid>
      <category>math.OC</category>
      <category>eess.IV</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoxia Liu, Shijie YU</dc:creator>
    </item>
    <item>
      <title>Surrogate-assisted multi-objective design of complex multibody systems</title>
      <link>https://arxiv.org/abs/2412.14854</link>
      <description>arXiv:2412.14854v1 Announce Type: new 
Abstract: The optimization of large-scale multibody systems is a numerically challenging task, in particular when considering multiple conflicting criteria at the same time. In this situation, we need to approximate the Pareto set of optimal compromises, which is significantly more expensive than finding a single optimum in single-objective optimization. To prevent large costs, the usage of surrogate models, constructed from a small but informative number of expensive model evaluations, is a very popular and widely studied approach. The central challenge then is to ensure a high quality (that is, near-optimality) of the solutions that were obtained using the surrogate model, which can be hard to guarantee with a single pre-computed surrogate. We present a back-and-forth approach between surrogate modeling and multi-objective optimization to improve the quality of the obtained solutions. Using the example of an expensive-to-evaluate multibody system, we compare different strategies regarding multi-objective optimization, sampling and also surrogate modeling, to identify the most promising approach in terms of computational efficiency and solution quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14854v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Augustina C. Amakor, Manuel B. Berkemeier, Meike Wohlleben, Walter Sextro, Sebastian Peitz</dc:creator>
    </item>
    <item>
      <title>Long Time Behavior and Stabilization for Displacement Monotone Mean Field Games</title>
      <link>https://arxiv.org/abs/2412.14903</link>
      <description>arXiv:2412.14903v1 Announce Type: new 
Abstract: This paper is devoted to the study of the long time behavior of Nash equilibria in Mean Field Games within the framework of displacement monotonicity. We first show that any two equilibria defined on the time horizon $[0,T]$ must be close as $T \to \infty$, in a suitable sense, independently of initial/terminal conditions. The way this stability property is made quantitative involves the $L^2$ distance between solutions of the associated Pontryagin system of FBSDEs that characterizes the equilibria. Therefore, this implies in particular the stability in the 2-Wasserstein distance for the two flows of probability measures describing the agent population density and the $L^2$ distance between the co-states of agents, that are related to the optimal feedback controls. We then prove that the value function of a typical agent converges as $T \to \infty$, and we describe this limit via an infinite horizon MFG system, involving an ergodic constant. All of our convergence results hold true in a unified way for deterministic and idiosyncratic noise driven Mean Field Games, in the case of strongly displacement monotone non-separable Hamiltonians. All these are quantitative at exponential rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14903v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marco Cirant, Alp\'ar R. M\'esz\'aros</dc:creator>
    </item>
    <item>
      <title>Computational Aspects of Lifted Cover Inequalities for Knapsacks with Few Different Weights</title>
      <link>https://arxiv.org/abs/2412.14919</link>
      <description>arXiv:2412.14919v1 Announce Type: new 
Abstract: Cutting planes are frequently used for solving integer programs. A common strategy is to derive cutting planes from building blocks or a substructure of the integer program. In this paper, we focus on knapsack constraints that arise from single row relaxations. Among the most popular classes derived from knapsack constraints are lifted minimal cover inequalities. The separation problem for these inequalities is NP-hard though, and one usually separates them heuristically, therefore not fully exploiting their potential. For many benchmarking instances however, it turns out that many knapsack constraints only have few different coefficients. This motivates the concept of sparse knapsacks where the number of different coefficients is a small constant, independent of the number of variables present. For such knapsacks, we observe that there are only polynomially many different classes of structurally equivalent minimal covers. This opens the door to specialized techniques for using lifted minimal cover inequalities. In this article we will discuss two such techniques, which are based on specialized sorting methods. On the one hand, we present new separation routines that separate equivalence classes of inequalities rather than individual inequalities. On the other hand, we derive compact extended formulations that express all lifted minimal cover inequalities by means of a polynomial number of constraints. These extended formulations are based on tailored sorting networks that express our separation algorithm by linear inequalities. We conclude the article by a numerical investigation of the different techniques for popular benchmarking instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14919v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Hojny, C\'edric Roy</dc:creator>
    </item>
    <item>
      <title>Local and Global Convergence of Greedy Parabolic Target-Following Methods for Linear Programming</title>
      <link>https://arxiv.org/abs/2412.14934</link>
      <description>arXiv:2412.14934v1 Announce Type: new 
Abstract: In the first part of this paper, we prove that, under some natural non-degeneracy assumptions, the Greedy Parabolic Target-Following Method, based on {\em universal tangent direction} has a favorable local behavior. In view of its global complexity bound of the order $O(\sqrt{n} \ln {1 \over \epsilon})$, this fact proves that the functional proximity measure, used for controlling the closeness to Greedy Central Path, is large enough for ensuring a local super-linear rate of convergence, provided that the proximity to the path is gradually reduced.
  This requirement is eliminated in our second algorithm based on a new auto-correcting predictor direction. This method, besides the best-known polynomial-time complexity bound, ensures an automatic switching onto the local quadratic convergence in a small neighborhood of solution.
  Our third algorithm approximates the path by quadratic curves. On the top of the best-known global complexity bound, this method benefits from an unusual local cubic rate of convergence. This amelioration needs no serious increase in the cost of one iteration. We compare the advantages of these local accelerations with possibilities of finite termination. The conditions allowing the optimal basis detection sometimes are even weaker than those required for the local superlinear convergence. Hence, it is important to endow the practical optimization schemes with both abilities.
  The proposed methods have a very interesting combination of favorable properties, which can be hardly found in the most of existing Interior-Point schemes. As all other parabolic target-following schemes, the new methods can start from an arbitrary strictly feasible primal-dual pair and go directly towards the optimal solution of the problem in a single phase. The preliminary computational experiments confirm the advantage of the second-order prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14934v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yurii Nesterov</dc:creator>
    </item>
    <item>
      <title>Effective Method with Compression for Distributed and Federated Cocoercive Variational Inequalities</title>
      <link>https://arxiv.org/abs/2412.14935</link>
      <description>arXiv:2412.14935v1 Announce Type: new 
Abstract: Variational inequalities as an effective tool for solving applied problems, including machine learning tasks, have been attracting more and more attention from researchers in recent years. The use of variational inequalities covers a wide range of areas - from reinforcement learning and generative models to traditional applications in economics and game theory. At the same time, it is impossible to imagine the modern world of machine learning without distributed optimization approaches that can significantly speed up the training process on large amounts of data. However, faced with the high costs of communication between devices in a computing network, the scientific community is striving to develop approaches that make computations cheap and stable. In this paper, we investigate the compression technique of transmitted information and its application to the distributed variational inequalities problem. In particular, we present a method based on advanced techniques originally developed for minimization problems. For the new method, we provide an exhaustive theoretical convergence analysis for cocoersive strongly monotone variational inequalities. We conduct experiments that emphasize the high performance of the presented technique and confirm its practical applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14935v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniil Medyakov, Gleb Molodtsov, Aleksandr Beznosikov</dc:creator>
    </item>
    <item>
      <title>Some Unified Theory for Variance Reduced Prox-Linear Methods</title>
      <link>https://arxiv.org/abs/2412.15008</link>
      <description>arXiv:2412.15008v1 Announce Type: new 
Abstract: This work considers the nonconvex, nonsmooth problem of minimizing a composite objective of the form $f(g(x))+h(x)$ where the inner mapping $g$ is a smooth finite summation or expectation amenable to variance reduction. In such settings, prox-linear methods can enjoy variance-reduced speed-ups despite the existence of nonsmoothness. We provide a unified convergence theory applicable to a wide range of common variance-reduced vector and Jacobian constructions. Our theory (i) only requires operator norm bounds on Jacobians (whereas prior works used potentially much larger Frobenius norms), (ii) provides state-of-the-art high probability guarantees, and (iii) allows inexactness in proximal computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15008v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Wu, Benjamin Grimmer</dc:creator>
    </item>
    <item>
      <title>Approximation Schemes for Age of Information Minimization in UAV Grid Patrols</title>
      <link>https://arxiv.org/abs/2412.14273</link>
      <description>arXiv:2412.14273v1 Announce Type: cross 
Abstract: Motivated by the critical need for unmanned aerial vehicles (UAVs) to patrol grid systems in hazardous and dynamically changing environments, this study addresses a routing problem aimed at minimizing the time-average Age of Information (AoI) for edges in general graphs. We establish a lower bound for all feasible patrol policies and demonstrate that this bound is tight when the graph contains an Eulerian cycle. For graphs without Eulerian cycles, it becomes challenging to identify the optimal patrol strategy due to the extensive range of feasible options. Our analysis shows that restricting the strategy to periodic sequences still results in an exponentially large number of possible strategies. To address this complexity, we introduce two polynomial-time approximation schemes, each involving a two-step process: constructing multigraphs first and then embedding Eulerian cycles within these multigraphs. We prove that both schemes achieve an approximation ratio of 2. Further, both analytical and numerical results suggest that evenly and sparsely distributing edge visits within a periodic route significantly reduces the average AoI compared to strategies that merely minimize the route travel distance. Building on this insight, we propose a heuristic method that not only maintains the approximation ratio of 2 but also ensures robust performance across varying random graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14273v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.adhoc.2024.103686</arxiv:DOI>
      <arxiv:journal_reference>Ad Hoc Networks Volume 166, 1 January 2025, 103686</arxiv:journal_reference>
      <dc:creator>Weiqi Wang, Jin Xu</dc:creator>
    </item>
    <item>
      <title>Balans: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for Mixed-Integer Programming Problem</title>
      <link>https://arxiv.org/abs/2412.14382</link>
      <description>arXiv:2412.14382v1 Announce Type: cross 
Abstract: Mixed-Integer Programming (MIP) is a powerful paradigm for modeling and solving various important combinatorial optimization problems. Recently, learning-based approaches have shown potential to speed up MIP solving via offline training that then guides important design decisions during search. However, a significant drawback of these methods is their heavy reliance on offline training, which requires collecting training datasets and computationally costly training epochs yet offering only limited generalization to unseen (larger) instances. In this paper, we propose Balans, an adaptive meta-solver for MIPs with online learning capability that does not require any supervision or apriori training. At its core, Balans is based on adaptive large-neighborhood search, operating on top of a MIP solver by successive applications of destroy and repair neighborhood operators. During the search, the selection among different neighborhood definitions is guided on the fly for the instance at hand via multi-armed bandit algorithms. Our extensive experiments on hard optimization instances show that Balans offers significant performance gains over the default MIP solver, is better than committing to any single best neighborhood, and improves over the state-of-the-art large-neighborhood search for MIPs. Finally, we release Balans as a highly configurable, MIP solver agnostic, open-source software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14382v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyang Cai, Serdar Kadioglu, Bistra Dilkina</dc:creator>
    </item>
    <item>
      <title>Multi-task Representation Learning for Mixed Integer Linear Programming</title>
      <link>https://arxiv.org/abs/2412.14409</link>
      <description>arXiv:2412.14409v1 Announce Type: cross 
Abstract: Mixed Integer Linear Programs (MILPs) are highly flexible and powerful tools for modeling and solving complex real-world combinatorial optimization problems. Recently, machine learning (ML)-guided approaches have demonstrated significant potential in improving MILP-solving efficiency. However, these methods typically rely on separate offline data collection and training processes, which limits their scalability and adaptability. This paper introduces the first multi-task learning framework for ML-guided MILP solving. The proposed framework provides MILP embeddings helpful in guiding MILP solving across solvers (e.g., Gurobi and SCIP) and across tasks (e.g., Branching and Solver configuration). Through extensive experiments on three widely used MILP benchmarks, we demonstrate that our multi-task learning model performs similarly to specialized models within the same distribution. Moreover, it significantly outperforms them in generalization across problem sizes and tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14409v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyang Cai, Taoan Huang, Bistra Dilkina</dc:creator>
    </item>
    <item>
      <title>Loss Minimization for Electrical Flows over Spanning Trees on Grids</title>
      <link>https://arxiv.org/abs/2412.14583</link>
      <description>arXiv:2412.14583v1 Announce Type: cross 
Abstract: We study the electrical distribution network reconfiguration problem, defined as follows. We are given an undirected graph with a root vertex, demand at each non-root vertex, and resistance on each edge. Then, we want to find a spanning tree of the graph that specifies the routing of power from the root to each vertex so that all the demands are satisfied and the energy loss is minimized. This problem is known to be NP-hard in general. When restricted to grids with uniform resistance and the root located at a corner, Gupta, Khodabaksh, Mortagy and Nikolova [Mathematical Programming 2022] invented the so-called Min-Min algorithm whose approximation factor is theoretically guaranteed. Our contributions are twofold. First, we prove that the problem is NP-hard even for grids; this resolves the open problem posed by Gupta et al. Second, we give a refined analysis of the Min-Min algorithm and improve its approximation factor under the same setup. In the analysis, we formulate the problem of giving an upper bound for the approximation factor as a non-linear optimization problem that maximizes a convex function over a polytope, which is less commonly employed in the analysis of approximation algorithms than linear optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14583v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takehiro Ito, Naonori Kakimura, Naoyuki Kamiyama, Yusuke Kobayashi, Yoshio Okamoto</dc:creator>
    </item>
    <item>
      <title>Solving Unbalanced Optimal Transport on Point Cloud by Tangent Radial Basis Function Method</title>
      <link>https://arxiv.org/abs/2412.14746</link>
      <description>arXiv:2412.14746v1 Announce Type: cross 
Abstract: In this paper, we solve unbalanced optimal transport (UOT) problem on surfaces represented by point clouds. Based on alternating direction method of multipliers algorithm, the original UOT problem can be solved by an iteration consists of three steps. The key ingredient is to solve a Poisson equation on point cloud which is solved by tangent radial basis function (TRBF) method. The proposed TRBF method requires only the point cloud and normal vectors to discretize the Poisson equation which simplify the computation significantly. Numerical experiments conducted on point clouds with varying geometry and topology demonstrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14746v1</guid>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiangong Pan, Wei Wan, Chenlong Bao, Zuoqiang Shi</dc:creator>
    </item>
    <item>
      <title>Admissibility theory in abstract Sobolev scales and transfer function growth at high frequencies</title>
      <link>https://arxiv.org/abs/2412.14786</link>
      <description>arXiv:2412.14786v1 Announce Type: cross 
Abstract: For strongly continous semigroups on Hilbert spaces, we investigate admissibility properties of control and observation operators shifted along continuous scales of spaces built by means of either interpolation and extrapolation or functional calculus. Our results show equivalence of admissibility in, on the one hand, a fractional domain of the generator and, on the other hand, a (different, in general) quadratic interpolation space of the same "Sobolev order". Furthermore, such properties imply quantified resolvent bounds in the original state space topology. When the semigroup is a group, the resulting frequency-domain estimates are in fact equivalent to the aforementioned time-domain properties. In the case of systems with both control and observation, we are able to translate input-output regularity properties into high-frequency growth rates of operator-valued transfer functions. As an application, based on results by Lasiecka, Triggiani and Tataru on interior and boundary regularity of the wave equation under Neumann control, we derive optimal asymptotics for the Neumann-to-Dirichlet wave transfer function. With that in hand, we establish non-uniform energy decay rates for the wave equation posed in a rectangle and subject to Neumann damping on an arbitrary open subset of the boundary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14786v1</guid>
      <category>math.AP</category>
      <category>math.FA</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lassi Paunonen, David Seifert, Nicolas Vanspranghe</dc:creator>
    </item>
    <item>
      <title>Novel Conditions for the Finite-Region Stability of 2D-Systems with Application to Iterative Learning Control</title>
      <link>https://arxiv.org/abs/2412.15078</link>
      <description>arXiv:2412.15078v1 Announce Type: cross 
Abstract: Some recent papers have extended the concept of finite-time stability (FTS) to the context of 2D linear systems, where it has been referred to as finite-region stability (FRS). FRS methodologies make even more sense than the classical FTS approach developed for 1D-systems, since, typically, at least one of the state variables of 2D-systems is a space coordinate, rather than a time variable. Since space coordinates clearly belong to finite intervals, FRS techniques are much more effective than the classical Lyapunov approach, which looks to the asymptotic behavior of the system over an infinite interval. To this regard, the novel contribution of this paper goes in several directions. First, we provide a novel sufficient condition for the FRS of linear time-varying (LTV) discrete-time 2D-systems, which turns out to be less conservative than those ones provided in the existing literature. Then, an interesting application of FRS to the context of iterative learning control (ILC) is investigated, by exploiting the previously developed theory. In particular, a new procedure is proposed so that the tracking errors of the ILC law converges within the desired bound in a finite number of iterations. Finally, a sufficient condition to solve the finite-region stabilization problem is proposed. All the results provided in the paper lead to optimization problems constrained by linear matrix inequalities (LMIs), that can be solved via widely available software. Numerical examples illustrate and validate the effectiveness of the proposed technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15078v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Liang, Carlo Cosentino, Alessio Merola, Maria Romano, Francesco Amato</dc:creator>
    </item>
    <item>
      <title>Projecting onto rectangular hyperbolic paraboloids in Hilbert space</title>
      <link>https://arxiv.org/abs/2206.04878</link>
      <description>arXiv:2206.04878v3 Announce Type: replace 
Abstract: In $\mathbb{R}^3$, a hyperbolic paraboloid is a classical saddle-shaped quadric surface. Recently, Elser has modeled problems arising in Deep Learning using rectangular hyperbolic paraboloids in $\mathbb{R}^n$. Motivated by his work, we provide a rigorous analysis of the associated projection. In some cases, finding this projection amounts to finding a certain root of a quintic or cubic polynomial. We also observe when the projection is not a singleton and point out connections to graphical and set convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.04878v3</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.23952/asvao.5.2023.2.04</arxiv:DOI>
      <arxiv:journal_reference>Applied Set-Valued Analysis and Optimization 5 (2023), No. 2, pp. 163-180</arxiv:journal_reference>
      <dc:creator>Heinz H. Bauschke, Manish Krishan Lal, Xianfu Wang</dc:creator>
    </item>
    <item>
      <title>Another Look at Partially Observed Optimal Stochastic Control: Existence, Ergodicity, and Approximations without Belief-Reduction</title>
      <link>https://arxiv.org/abs/2301.11244</link>
      <description>arXiv:2301.11244v3 Announce Type: replace 
Abstract: We present an alternative view for the study of optimal control of partially observed Markov Decision Processes (POMDPs). We first revisit the traditional (and by now standard) separated-design method of reducing the problem to fully observed MDPs (belief-MDPs), and present conditions for the existence of optimal policies. Then, rather than working with this standard method, we define a Markov chain taking values in an infinite dimensional product space with the history process serving as the controlled state process and a further refinement in which the control actions and the state process are causally conditionally independent given the measurement/information process. We provide new sufficient conditions for the existence of optimal control policies under the discounted cost and average cost infinite horizon criteria. For the discounted cost setup, we establish near optimality of finite window policies via a direct argument involving near optimality of quantized approximations for MDPs under weak Feller continuity, where finite truncations of memory can be viewed as quantizations of infinite memory with a uniform diameter in each finite window restriction under the product metric. For the average cost setup, we provide new existence conditions and also a general approach on how to initialize the randomness which we show to establish convergence to optimal cost. In the control-free case, our analysis leads to new and weak conditions for the existence and uniqueness of invariant probability measures for nonlinear filter processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.11244v3</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Serdar Y\"uksel</dc:creator>
    </item>
    <item>
      <title>Carbon-Aware Optimal Power Flow</title>
      <link>https://arxiv.org/abs/2308.03240</link>
      <description>arXiv:2308.03240v3 Announce Type: replace 
Abstract: To facilitate effective decarbonization of the electric power sector, this paper introduces the generic Carbon-aware Optimal Power Flow (C-OPF) method for power system decision-making that considers demand-side carbon accounting and emission management. Built upon the classic optimal power flow (OPF) model, the C-OPF method incorporates carbon emission flow equations and constraints, as well as carbon-related objectives, to jointly optimize power flow and carbon flow. In particular, this paper establishes the feasibility and solution uniqueness of the carbon emission flow equations, and proposes modeling and linearization techniques to address the issues of undetermined power flow directions and bilinear terms in the C-OPF model. Additionally, two novel carbon emission models, together with the carbon accounting schemes, for energy storage systems are developed and integrated into the C-OPF model. Numerical simulations demonstrate the characteristics and effectiveness of the C-OPF method, in comparison with OPF solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.03240v3</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xin Chen, Andy Sun, Wenbo Shi, Na Li</dc:creator>
    </item>
    <item>
      <title>Risk-Aware Control of Discrete-Time Stochastic Systems: Integrating Kalman Filter and Worst-case CVaR in Control Barrier Functions</title>
      <link>https://arxiv.org/abs/2312.15638</link>
      <description>arXiv:2312.15638v4 Announce Type: replace 
Abstract: This paper proposes control approaches for discrete-time linear systems subject to stochastic disturbances. It employs Kalman filter to estimate the mean and covariance of the state propagation, and the worst-case conditional value-at-risk (CVaR) to quantify the tail risk using the estimated mean and covariance. The quantified risk is then integrated into a control barrier function (CBF) to derive constraints for controller synthesis, addressing tail risks near safe set boundaries. Two optimization-based control methods are presented using the obtained constraints for half-space and ellipsoidal safe sets, respectively. The effectiveness of the obtained results is demonstrated using numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15638v4</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masako Kishida</dc:creator>
    </item>
    <item>
      <title>Linear-quadratic optimal control for infinite-dimensional input-state-output systems</title>
      <link>https://arxiv.org/abs/2401.11302</link>
      <description>arXiv:2401.11302v2 Announce Type: replace 
Abstract: We examine the minimization of a quadratic cost functional composed of the output and the final state of abstract infinite-dimensional evolution equations in view of existence of solutions and optimality conditions. While the initial value is prescribed, we are minimizing over all inputs within a specified convex subset of square integrable controls with values in a Hilbert space. The considered class of infinite-dimensional systems is based on the system node formulation. Thus, our developed approach includes optimal control of a wide variety of linear partial differential equations with boundary control and observation that are not well-posed in the sense that the output continuously depends on the input and the initial value. We provide an application of particular optimal control problems arising in energy-optimal control of port-Hamiltonian systems. Last, we illustrate the our abstract theory by two examples including a non-well-posed heat equation with Dirichlet boundary control and a wave equation on an L-shaped domain with boundary control of the stress in normal direction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11302v2</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Timo Reis, Manuel Schaller</dc:creator>
    </item>
    <item>
      <title>Decentralized real-time iterations for distributed NMPC</title>
      <link>https://arxiv.org/abs/2401.14898</link>
      <description>arXiv:2401.14898v2 Announce Type: replace 
Abstract: This article presents a Real-Time Iteration (RTI) scheme for distributed Nonlinear Model Predictive Control (NMPC). The scheme transfers the well-known RTI approach, a key enabler for many industrial real-time NMPC implementations, to the setting of cooperative distributed control. At each sampling instant, one outer iteration of a bi-level decentralized Sequential Quadratic Programming (dSQP) method is applied to a centralized optimal control problem. This ensures that real-time requirements are met and it facilitates cooperation between subsystems. Combining novel dSQP convergence results with RTI stability guarantees, we prove local exponential stability under standard assumptions on the MPC design with and without terminal constraints. The proposed scheme only requires neighbor-to-neighbor communication and avoids a central coordinator. A numerical example with coupled inverted pendulums demonstrates the efficacy of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14898v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G\"osta Stomberg, Alexander Engelmann, Moritz Diehl, Timm Faulwasser</dc:creator>
    </item>
    <item>
      <title>Unbalanced L1 optimal transport for vector valued measures and application to Full Waveform Inversion</title>
      <link>https://arxiv.org/abs/2403.02764</link>
      <description>arXiv:2403.02764v3 Announce Type: replace 
Abstract: Optimal transport has recently started to be successfully employed to define misfit or loss functions in inverse problems. However, it is a problem intrinsically defined for positive (probability) measures and therefore strategies are needed for its applications in more general settings of interest. In this paper we introduce an unbalanced optimal transport problem for vector valued measures starting from the $L^1$ optimal transport. By lifting data in a self-dual cone of a higher dimensional vector space, we show that one can recover a meaningful transport problem. We show that the favorable computational complexity of the $L^1$ problem, an advantage compared to other formulations of optimal transport, is inherited by our vector extension. We consider both a one-homogeneous and a two-homogeneous penalization for the imbalance of mass, the latter being potentially relevant for applications to physics based problems. In particular, we demonstrate the potential of our strategy for full waveform inversion, an inverse problem for high resolution seismic imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02764v3</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriele Todeschi (LIGM), Ludovic M\'etivier (ISTerre, EDP), Jean-Marie Mirebeau (CNRS, CB)</dc:creator>
    </item>
    <item>
      <title>Feature selection in linear SVMs via a hard cardinality constraint: a scalable SDP decomposition approach</title>
      <link>https://arxiv.org/abs/2404.10099</link>
      <description>arXiv:2404.10099v2 Announce Type: replace 
Abstract: In this paper, we study the embedded feature selection problem in linear Support Vector Machines (SVMs), in which a cardinality constraint is employed, leading to an interpretable classification model. The problem is NP-hard due to the presence of the cardinality constraint, even though the original linear SVM amounts to a problem solvable in polynomial time. To handle the hard problem, we first introduce two mixed-integer formulations for which novel semidefinite relaxations are proposed. Exploiting the sparsity pattern of the relaxations, we decompose the problems and obtain equivalent relaxations in a much smaller cone, making the conic approaches scalable. To make the best usage of the decomposed relaxations, we propose heuristics using the information of its optimal solution. Moreover, an exact procedure is proposed by solving a sequence of mixed-integer decomposed semidefinite optimization problems. Numerical results on classical benchmarking datasets are reported, showing the efficiency and effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10099v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Immanuel Bomze, Federico D'Onofrio, Laura Palagi, Bo Peng</dc:creator>
    </item>
    <item>
      <title>Double interdiction problem on trees on the sum of root-leaf distances by upgrading edges</title>
      <link>https://arxiv.org/abs/2407.13391</link>
      <description>arXiv:2407.13391v2 Announce Type: replace 
Abstract: The double interdiction problem on trees (DIT) for the sum of root-leaf distances (SRD) has significant implications in diverse areas such as transportation networks, military strategies, and counter-terrorism efforts. It aims to maximize the SRD by upgrading edge weights subject to two constraints. One gives an upper bound for the cost of upgrades under certain norm and the other specifies a lower bound for the shortest root-leaf distance (StRD). We utilize both weighted $l_\infty$ norm and Hamming distance to measure the upgrade cost and denote the corresponding (DIT) problem by (DIT$_{H\infty}$) and its minimum cost problem by (MCDIT$_{H\infty}$). We establish the $\mathcal{NP}$-hardness of problem (DIT$_{H\infty}$) by building a reduction from the 0-1 knapsack problem. We solve the problem (DIT$_{H\infty}$) by two scenarios based on the number $N$ of upgrade edges. When $N=1$, a greedy algorithm with $O(n)$ complexity is proposed. For the general case, an exact dynamic programming algorithm within a pseudo-polynomial time is proposed, which is established on a structure of left subtrees by maximizing a convex combination of the StRD and SRD. Furthermore, we confirm the $\mathcal{NP}$-hardness of problem (MCDIT$_{H\infty}$) by reducing from the 0-1 knapsack problem. To tackle problem (MCDIT$_{H\infty}$), a binary search algorithm with pseudo-polynomial time complexity is outlined, which iteratively solves problem (DIT$_{H\infty}$). We culminate our study with numerical experiments, showcasing effectiveness of the algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13391v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiao Li, Xiucui Guan, Junhua Jia, Panos M. Pardalos</dc:creator>
    </item>
    <item>
      <title>The Data-Driven Censored Newsvendor Problem</title>
      <link>https://arxiv.org/abs/2412.01763</link>
      <description>arXiv:2412.01763v2 Announce Type: replace 
Abstract: We study a censored variant of the data-driven newsvendor problem, where the decision-maker must select an ordering quantity that minimizes expected overage and underage costs based only on offline censored sales data, rather than historical demand realizations. Our goal is to understand how the degree of historical demand censoring affects the performance of any learning algorithm for this problem. To isolate this impact, we adopt a distributionally robust optimization framework, evaluating policies according to their worst-case regret over an ambiguity set of distributions. This set is defined by the largest historical order quantity (the observable boundary of the dataset), and contains all distributions matching the true demand distribution up to this boundary, while allowing them to be arbitrary afterwards. We demonstrate a spectrum of achievability under demand censoring by deriving a natural necessary and sufficient condition under which vanishing regret is an achievable goal. In regimes in which it is not, we exactly characterize the information loss due to censoring: an insurmountable lower bound on the performance of any policy, even when the decision-maker has access to infinitely many demand samples. We then leverage these sharp characterizations to propose a natural robust algorithm that adapts to the historical level of demand censoring. We derive finite-sample guarantees for this algorithm across all possible censoring regimes and show its near-optimality with matching lower bounds (up to polylogarithmic factors). We moreover demonstrate its robust performance via extensive numerical experiments on both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01763v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chamsi Hssaine, Sean R. Sinclair</dc:creator>
    </item>
    <item>
      <title>A subgradient splitting algorithm for optimization on nonpositively curved metric spaces</title>
      <link>https://arxiv.org/abs/2412.06730</link>
      <description>arXiv:2412.06730v2 Announce Type: replace 
Abstract: Many of the primal ingredients of convex optimization extend naturally from Euclidean to Hadamard spaces $\unicode{x2014}$ nonpositively curved metric spaces like Euclidean, Hilbert, and hyperbolic spaces, metric trees, and more general CAT(0) cubical complexes. Linear structure, however, and the duality theory it supports are absent. Nonetheless, we introduce a new type of subgradient for convex functions on Hadamard spaces, based on Busemann functions. This notion supports a splitting subgradient method with guaranteed complexity bounds. In particular, the algorithm solves $p$-mean problems in general Hadamard spaces: we illustrate by computing medians in BHV tree space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06730v2</guid>
      <category>math.OC</category>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel Goodwin, Adrian S. Lewis, Genaro L\'opez-Acedo, Adriana Nicolae</dc:creator>
    </item>
    <item>
      <title>Gauss-Newton Dynamics for Neural Networks: A Riemannian Optimization Perspective</title>
      <link>https://arxiv.org/abs/2412.14031</link>
      <description>arXiv:2412.14031v2 Announce Type: replace 
Abstract: We analyze the convergence of Gauss-Newton dynamics for training neural networks with smooth activation functions. In the underparameterized regime, the Gauss-Newton gradient flow induces a Riemannian gradient flow on a low-dimensional, smooth, embedded submanifold of the Euclidean output space. Using tools from Riemannian optimization, we prove \emph{last-iterate} convergence of the Riemannian gradient flow to the optimal in-class predictor at an \emph{exponential rate} that is independent of the conditioning of the Gram matrix, \emph{without} requiring explicit regularization. We further characterize the critical impacts of the neural network scaling factor and the initialization on the convergence behavior. In the overparameterized regime, we show that the Levenberg-Marquardt dynamics with an appropriately chosen damping factor yields robustness to ill-conditioned kernels, analogous to the underparameterized regime. These findings demonstrate the potential of Gauss-Newton methods for efficiently optimizing neural networks, particularly in ill-conditioned problems where kernel and Gram matrices have small singular values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14031v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ML</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Semih Cayci</dc:creator>
    </item>
    <item>
      <title>Coarse correlated equilibria for continuous time mean field games in open loop strategies</title>
      <link>https://arxiv.org/abs/2303.16728</link>
      <description>arXiv:2303.16728v3 Announce Type: replace-cross 
Abstract: In the framework of continuous time symmetric stochastic differential games in open loop strategies, we introduce a generalization of mean field game solution, called coarse correlated solution. This can be seen as the analogue of a coarse correlated equilibrium in the $N$-player game, where a moderator randomly generates a strategy profile and asks the players to pre-commit to such strategies before disclosing them privately to each one of them; such a profile is a coarse correlated equilibrium if no player has an incentive to unilaterally deviate. We justify our definition by showing that a coarse correlated solution for the mean field game induces a sequence of approximate coarse correlated equilibria with vanishing error for the underlying $N$-player games. Existence of coarse correlated solutions for the mean field game is proved by means of a minimax theorem. An example with explicit solutions is discussed as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.16728v3</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luciano Campi, Federico Cannerozzi, Markus Fischer</dc:creator>
    </item>
    <item>
      <title>Scalable Acceleration for Classification-Based Derivative-Free Optimization</title>
      <link>https://arxiv.org/abs/2309.11036</link>
      <description>arXiv:2309.11036v2 Announce Type: replace-cross 
Abstract: Derivative-free optimization algorithms play an important role in scientific and engineering design optimization problems, especially when derivative information is not accessible. In this paper, we study the framework of sequential classification-based derivative-free optimization algorithms. By introducing learning theoretic concept hypothesis-target shattering rate, we revisit the computational complexity upper bound of SRACOS (Hu, Qian, and Yu 2017). Inspired by the revisited upper bound, we propose an algorithm named RACE-CARS, which adds a random region-shrinking step compared with SRACOS. We further establish theorems showing the acceleration by region shrinking. Experiments on the synthetic functions as well as black-box tuning for language-model-as-a-service demonstrate empirically the efficiency of RACE-CARS. An ablation experiment on the introduced hyperparameters is also conducted, revealing the mechanism of RACE-CARS and putting forward an empirical hyper-parameter tuning guidance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.11036v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyi Han, Jingya Li, Zhipeng Guo, Yuan Jin</dc:creator>
    </item>
    <item>
      <title>Contractivity of neural ODEs: an eigenvalue optimization problem</title>
      <link>https://arxiv.org/abs/2402.13092</link>
      <description>arXiv:2402.13092v3 Announce Type: replace-cross 
Abstract: We propose a novel methodology to solve a key eigenvalue optimization problem which arises in the contractivity analysis of neural ODEs. When looking at contractivity properties of a one layer weight-tied neural ODE $\dot{u}(t)=\sigma(Au(t)+b)$ (with $u,b \in {\mathbb R}^n$, $A$ is a given $n \times n$ matrix, $\sigma : {\mathbb R} \to {\mathbb R}$ denotes an activation function and for a vector $z \in {\mathbb R}^n$, $\sigma(z) \in {\mathbb R}^n$ has to be interpreted entry-wise), we are led to study the logarithmic norm of a set of products of type $D A$, where $D$ is a diagonal matrix such that ${\mathrm{diag}}(D) \in \sigma'({\mathbb R}^n)$. Specifically, given a real number $c$ (usually $c=0$), the problem consists in finding the largest positive interval $\text{I}\subseteq \mathbb [0,\infty)$ such that the logarithmic norm $\mu(DA) \le c$ for all diagonal matrices $D$ with $D_{ii}\in \text{I}$. We propose a two-level nested methodology: an inner level where, for a given $\text{I}$, we compute an optimizer $D^\star(\text{I})$ by a gradient system approach, and an outer level where we tune $\text{I}$ so that the value $c$ is reached by $\mu(D^\star(\text{I})A)$. We extend the proposed two-level approach to the general multilayer, and possibly time-dependent, case $\dot{u}(t) = \sigma( A_k(t) \ldots \sigma ( A_{1}(t) u(t) + b_{1}(t) ) \ldots + b_{k}(t) )$ and we propose several numerical examples to illustrate its behaviour, including its stabilizing performance on a one-layer neural ODE applied to the classification of the MNIST handwritten digits dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13092v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicola Guglielmi, Arturo De Marinis, Anton Savostianov, Francesco Tudisco</dc:creator>
    </item>
    <item>
      <title>Remarks on potential mean field games</title>
      <link>https://arxiv.org/abs/2405.15921</link>
      <description>arXiv:2405.15921v2 Announce Type: replace-cross 
Abstract: In this expository article, we give an overview of the concept of potential mean field games of first order. We give a new proof that minimizers of the potential are equilibria by using a Lagrangian formulation. We also provide criteria to determine whether or not a game has a potential. Finally, we discuss in some depth the selection problem in mean field games, which consists in choosing one out of multiple Nash equilibria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15921v2</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P. Jameson Graber</dc:creator>
    </item>
  </channel>
</rss>
