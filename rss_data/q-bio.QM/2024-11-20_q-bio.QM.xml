<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Nov 2024 02:51:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>bia-binder: A web-native cloud compute service for the bioimage analysis community</title>
      <link>https://arxiv.org/abs/2411.12662</link>
      <description>arXiv:2411.12662v1 Announce Type: new 
Abstract: We introduce bia-binder (BioImage Archive Binder), an open-source, cloud-architectured, and web-based coding environment tailored to bioimage analysis that is freely accessible to all researchers. The service generates easy-to-use Jupyter Notebook coding environments hosted on EMBL-EBI's Embassy Cloud, which provides significant computational resources. The bia-binder architecture is free, open-source and publicly available for deployment. It features fast and direct access to images in the BioImage Archive, the Image Data Resource, and the BioStudies databases. We believe that this service can play a role in mitigating the current inequalities in access to scientific resources across academia. As bia-binder produces permanent links to compiled coding environments, we foresee the service to become widely-used within the community and enable exploratory research. bia-binder is built and deployed using helmsman and helm and released under the MIT licence. It can be accessed at binder.bioimagearchive.org and runs on any standard web browser.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12662v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Craig T. Russell, Jean-Marie Burel, Awais Athar, Simon Li, Ugis Sarkans, Jason Swedlow, Alvis Brazma, Matthew Hartley, Virginie Uhlmann</dc:creator>
    </item>
    <item>
      <title>Barttender: An approachable &amp; interpretable way to compare medical imaging and non-imaging data</title>
      <link>https://arxiv.org/abs/2411.12707</link>
      <description>arXiv:2411.12707v1 Announce Type: new 
Abstract: Imaging-based deep learning has transformed healthcare research, yet its clinical adoption remains limited due to challenges in comparing imaging models with traditional non-imaging and tabular data. To bridge this gap, we introduce Barttender, an interpretable framework that uses deep learning for the direct comparison of the utility of imaging versus non-imaging tabular data for tasks like disease prediction.
  Barttender converts non-imaging tabular features, such as scalar data from electronic health records, into grayscale bars, facilitating an interpretable and scalable deep learning based modeling of both data modalities. Our framework allows researchers to evaluate differences in utility through performance measures, as well as local (sample-level) and global (population-level) explanations. We introduce a novel measure to define global feature importances for image-based deep learning models, which we call gIoU. Experiments on the CheXpert and MIMIC datasets with chest X-rays and scalar data from electronic health records show that Barttender performs comparably to traditional methods and offers enhanced explainability using deep learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12707v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayush Singla, Shakson Isaac, Chirag J. Patel</dc:creator>
    </item>
    <item>
      <title>RatGene: Gene deletion-addition algorithms using growth to production ratio for growth-coupled production in constraint-based metabolic networks</title>
      <link>https://arxiv.org/abs/2411.12087</link>
      <description>arXiv:2411.12087v1 Announce Type: cross 
Abstract: In computational metabolic design, it is often necessary to modify the original constraint-based metabolic networks to lead to growth-coupled production, where cell growth forces target metabolite production. However, in genome-scale models, finding strategies to simultaneously delete and add genes to induce growth-coupled production is challenging. This is particularly true when heavy computation is necessary due to numerous gene deletions and additions. In this study, we mathematically defined related problems, proved NP-hardness and/or NP-completeness, and developed an algorithm named RatGene that (1) automatically integrates multiple constraint-based metabolic networks, (2) identifies gene deletion-addition strategies by a growth-to-production ratio-based approach, and (3) eliminates redundant gene additions and deletions. The results of computational experiments demonstrated that the RatGene-based approach can significantly improve the success ratio for identifying the strategies for growth-coupled production. RatGene can facilitate a more rational approach to computational metabolic design for the production of useful substances using microorganisms by concurrently considering both gene deletions and additions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12087v1</guid>
      <category>q-bio.MN</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yier Ma, Takeyuki Tamura</dc:creator>
    </item>
    <item>
      <title>Hierarchical Trait-State Model for Decoding Dyadic Social Interactions</title>
      <link>https://arxiv.org/abs/2411.12145</link>
      <description>arXiv:2411.12145v1 Announce Type: cross 
Abstract: Traits are patterns of brain signals and behaviors that are stable over time but differ across individuals, whereas states are phasic patterns that vary over time, are influenced by the environment, yet oscillate around the traits. The quality of a social interaction depends on the traits and states of the interacting agents. However, it remains unclear how to decipher both traits and states from the same set of brain signals. To explore the hidden neural traits and states in relation to the behavioral ones during social interactions, we developed a pipeline to extract latent dimensions of the brain from electroencephalogram (EEG) data collected during a team flow task. Our pipeline involved two stages of dimensionality reduction: first, non-negative matrix factorization (NMF), followed by linear discriminant analysis (LDA). This pipeline resulted in an interpretable, seven-dimensional EEG latent space that revealed a trait-state hierarchical structure, with macro-segregation capturing neural traits and micro-segregation capturing neural states. Out of the seven latent dimensions, we found that three that significantly contributed to variations across individuals and task states. Using representational similarity analysis, we mapped the EEG latent space to a skill-cognition space, establishing a connection between hidden neural signatures and social interaction behaviors. Our method demonstrates the feasibility of representing both traits and states within a single model that correlates with changes in social behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12145v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qianying Wu, Shigeki Nakauchi, Mohammad Shehata, Shinsuke Shimojo</dc:creator>
    </item>
    <item>
      <title>Rapid response to fast viral evolution using AlphaFold 3-assisted topological deep learning</title>
      <link>https://arxiv.org/abs/2411.12370</link>
      <description>arXiv:2411.12370v1 Announce Type: cross 
Abstract: The fast evolution of SARS-CoV-2 and other infectious viruses poses a grand challenge to the rapid response in terms of viral tracking, diagnostics, and design and manufacture of monoclonal antibodies (mAbs) and vaccines, which are both time-consuming and costly. This underscores the need for efficient computational approaches. Recent advancements, like topological deep learning (TDL), have introduced powerful tools for forecasting emerging dominant variants, yet they require deep mutational scanning (DMS) of viral surface proteins and associated three-dimensional (3D) protein-protein interaction (PPI) complex structures. We propose an AlphaFold 3 (AF3)-assisted multi-task topological Laplacian (MT-TopLap) strategy to address this need. MT-TopLap combines deep learning with topological data analysis (TDA) models, such as persistent Laplacians (PL) to extract detailed topological and geometric characteristics of PPIs, thereby enhancing the prediction of DMS and binding free energy (BFE) changes upon virus mutations. Validation with four experimental DMS datasets of SARS-CoV-2 spike receptor-binding domain (RBD) and the human angiotensin-converting enzyme-2 (ACE2) complexes indicates that our AF3 assisted MT-TopLap strategy maintains robust performance, with only an average 1.1% decrease in Pearson correlation coefficients (PCC) and an average 9.3% increase in root mean square errors (RMSE), compared with the use of experimental structures. Additionally, AF3-assisted MT-TopLap achieved a PCC of 0.81 when tested with a SARS-CoV-2 HK.3 variant DMS dataset, confirming its capability to accurately predict BFE changes and adapt to new experimental data, thereby showcasing its potential for rapid and effective response to fast viral evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12370v1</guid>
      <category>q-bio.BM</category>
      <category>q-bio.PE</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>JunJie Wee, Guo-Wei Wei</dc:creator>
    </item>
    <item>
      <title>Flat Cell Imaging</title>
      <link>https://arxiv.org/abs/2411.12656</link>
      <description>arXiv:2411.12656v1 Announce Type: cross 
Abstract: Recent advances in optical technology have significantly enhanced the resolution of imaging of living cells, achieving nanometer-scale precision. However, the crowded three-dimensional environment within cells presents a challenge for measuring the spatio-temporal dynamics of cellular components. One solution to this issue is expansion microscopy, which cannot be used for living cells. Here, we present a method for flattening live cells to a thickness of down to 200 nanometers by confining them between two surface-treated transparent plates. The anti-fouling coating on the surfaces restricts the cells to a quasi-two-dimensional space by exerting osmotic control and preventing surface adhesion. This technique increases the distance between cellular components, thereby enabling high-resolution imaging of their spatio-temporal dynamics. The viability and phenotype of various cell types are demonstrated to be unaltered upon release from flat-cell confinement. The flat cell imaging method is a robust and straightforward technique, making it a practical choice for optical microscopy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12656v1</guid>
      <category>physics.optics</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vahid Nasirimarekani, Zuzana Ditte, Eberhard Bodenschatz</dc:creator>
    </item>
    <item>
      <title>Learning Personalized Treatment Decisions in Precision Medicine: Disentangling Treatment Assignment Bias in Counterfactual Outcome Prediction and Biomarker Identification</title>
      <link>https://arxiv.org/abs/2410.00509</link>
      <description>arXiv:2410.00509v2 Announce Type: replace-cross 
Abstract: Precision medicine has the potential to tailor treatment decisions to individual patients using machine learning (ML) and artificial intelligence (AI), but it faces significant challenges due to complex biases in clinical observational data and the high-dimensional nature of biological data. This study models various types of treatment assignment biases using mutual information and investigates their impact on ML models for counterfactual prediction and biomarker identification. Unlike traditional counterfactual benchmarks that rely on fixed treatment policies, our work focuses on modeling different characteristics of the underlying observational treatment policy in distinct clinical settings. We validate our approach through experiments on toy datasets, semi-synthetic tumor cancer genome atlas (TCGA) data, and real-world biological outcomes from drug and CRISPR screens. By incorporating empirical biological mechanisms, we create a more realistic benchmark that reflects the complexities of real-world data. Our analysis reveals that different biases lead to varying model performances, with some biases, especially those unrelated to outcome mechanisms, having minimal effect on prediction accuracy. This highlights the crucial need to account for specific biases in clinical observational data in counterfactual ML model development, ultimately enhancing the personalization of treatment decisions in precision medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00509v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Vollenweider, Manuel Sch\"urch, Chiara Rohrer, Gabriele Gut, Michael Krauthammer, Andreas Wicki</dc:creator>
    </item>
    <item>
      <title>NACNet: A Histology Context-aware Transformer Graph Convolution Network for Predicting Treatment Response to Neoadjuvant Chemotherapy in Triple Negative Breast Cancer</title>
      <link>https://arxiv.org/abs/2411.09766</link>
      <description>arXiv:2411.09766v2 Announce Type: replace-cross 
Abstract: Neoadjuvant chemotherapy (NAC) response prediction for triple negative breast cancer (TNBC) patients is a challenging task clinically as it requires understanding complex histology interactions within the tumor microenvironment (TME). Digital whole slide images (WSIs) capture detailed tissue information, but their giga-pixel size necessitates computational methods based on multiple instance learning, which typically analyze small, isolated image tiles without the spatial context of the TME. To address this limitation and incorporate TME spatial histology interactions in predicting NAC response for TNBC patients, we developed a histology context-aware transformer graph convolution network (NACNet). Our deep learning method identifies the histopathological labels on individual image tiles from WSIs, constructs a spatial TME graph, and represents each node with features derived from tissue texture and social network analysis. It predicts NAC response using a transformer graph convolution network model enhanced with graph isomorphism network layers. We evaluate our method with WSIs of a cohort of TNBC patient (N=105) and compared its performance with multiple state-of-the-art machine learning and deep learning models, including both graph and non-graph approaches. Our NACNet achieves 90.0% accuracy, 96.0% sensitivity, 88.0% specificity, and an AUC of 0.82, through eight-fold cross-validation, outperforming baseline models. These comprehensive experimental results suggest that NACNet holds strong potential for stratifying TNBC patients by NAC response, thereby helping to prevent overtreatment, improve patient quality of life, reduce treatment cost, and enhance clinical outcomes, marking an important advancement toward personalized breast cancer treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09766v2</guid>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qiang Li, George Teodoro, Yi Jiang, Jun Kong</dc:creator>
    </item>
  </channel>
</rss>
