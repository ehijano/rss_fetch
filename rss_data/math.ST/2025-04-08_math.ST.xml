<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Apr 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Common Drivers in Sparsely Interacting Hawkes Processes</title>
      <link>https://arxiv.org/abs/2504.03916</link>
      <description>arXiv:2504.03916v1 Announce Type: new 
Abstract: We study a multivariate Hawkes process as a model for time-continuous relational event networks. The model does not assume the network to be known, it includes covariates, and it allows for both common drivers, parameters common to all the actors in the network, and also local parameters specific for each actor. We derive rates of convergence for all of the model parameters when both the number of actors and the time horizon tends to infinity. To prevent an exploding network, sparseness is assumed. We also discuss numerical aspects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03916v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Kreiss, Enno Mammen, Wolfgang Polonik</dc:creator>
    </item>
    <item>
      <title>Gaussian Mean Testing under Truncation</title>
      <link>https://arxiv.org/abs/2504.04682</link>
      <description>arXiv:2504.04682v1 Announce Type: new 
Abstract: We consider the task of Gaussian mean testing, that is, of testing whether a high-dimensional vector perturbed by white noise has large magnitude, or is the zero vector. This question, originating from the signal processing community, has recently seen a surge of interest from the machine learning and theoretical computer science community, and is by now fairly well understood. What is much less understood, and the focus of our work, is how to perform this task under truncation: that is, when the observations (i.i.d.\ samples from the underlying high-dimensional Gaussian) are only observed when they fall in an given subset of the domain $\R^d$. This truncation model, previously studied in the context of learning (instead of testing) the mean vector, has a range of applications, in particular in Economics and Social Sciences. As our work shows, sample truncations affect the complexity of the testing task in a rather subtle and surprising way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04682v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cl\'ement L. Canonne, Themis Gouleakis, Yuhao Wang, Joy Qiping Yang</dc:creator>
    </item>
    <item>
      <title>Extension of Yager's negation of probability distribution based on uncertainty measures</title>
      <link>https://arxiv.org/abs/2504.04762</link>
      <description>arXiv:2504.04762v1 Announce Type: new 
Abstract: Existing research on negations primarily focuses on entropy and extropy. Recently, new functions such as varentropy and varextropy have been developed, which can be considered as
  extensions of entropy and extropy. However, the impact of negation on these extended measures, particularly varentropy and varextropy, has not been extensively explored. To address
  this gap, this paper investigates the effect of negation on Shannon entropy, varentropy, and varextropy. We explore how the negation of a probability distribution influences these
  measures, showing that the negated distribution consistently leads to higher values of Shannon entropy, varentropy, and varextropy compared to the original distribution.
  Additionally, we prove that the negation of a probability distribution maximizes these measures during the process. The paper provides theoretical proofs and a detailed analysis of
  the behaviour of these measures, contributing to a better understanding of the interplay between probability distributions, negation, and information-theoretic quantities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04762v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santosh Kumar Chaudhary, Pradeep Kumar Sahu, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>Truncated sequential guaranteed estimation for the Cox-Ingersoll-Ross models</title>
      <link>https://arxiv.org/abs/2504.04923</link>
      <description>arXiv:2504.04923v1 Announce Type: new 
Abstract: The drift sequential parameter estimation problems for the Cox-Ingersoll-Ross (CIR) processes under the limited duration of observation are studied. Truncated sequential estimation methods for both scalar and {two}-dimensional parameter cases are proposed. In the non-asymptotic setting, for the proposed truncated estimators, the properties of guaranteed mean-square estimation accuracy are established. In the asymptotic formulation, when the observation time tends to infinity, it is shown that the proposed sequential procedures are asymptotically optimal among all possible sequential and non-sequential estimates with an average estimation time less than the fixed observation duration. It also turned out that asymptotically, without degrading the estimation quality, they significantly reduce the observation duration compared to classical non-sequential maximum likelihood estimations based on a fixed observation duration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04923v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Ben Alaya, Thi-Bao Tr\^am Ng\^o, Serguei Pergamenchtchikov</dc:creator>
    </item>
    <item>
      <title>Regression Discontinuity Design with Distribution-Valued Outcomes</title>
      <link>https://arxiv.org/abs/2504.03992</link>
      <description>arXiv:2504.03992v1 Announce Type: cross 
Abstract: This article introduces Regression Discontinuity Design (RDD) with Distribution-Valued Outcomes (R3D), extending the standard RDD framework to settings where the outcome is a distribution rather than a scalar. Such settings arise when treatment is assigned at a higher level of aggregation than the outcome-for example, when a subsidy is allocated based on a firm-level revenue cutoff while the outcome of interest is the distribution of employee wages within the firm. Since standard RDD methods cannot accommodate such two-level randomness, I propose a novel approach based on random distributions. The target estimand is a "local average quantile treatment effect", which averages across random quantiles. To estimate this target, I introduce two related approaches: one that extends local polynomial regression to random quantiles and another based on local Fr\'echet regression, a form of functional regression. For both estimators, I establish asymptotic normality and develop uniform, debiased confidence bands together with a data-driven bandwidth selection procedure. Simulations validate these theoretical properties and show existing methods to be biased and inconsistent in this setting. I then apply the proposed methods to study the effects of gubernatorial party control on within-state income distributions in the US, using a close-election design. The results suggest a classic equality-efficiency tradeoff under Democratic governorship, driven by reductions in income at the top of the distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03992v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Van Dijcke</dc:creator>
    </item>
    <item>
      <title>Balancing Complexity and Informativeness in LLM-Based Clustering: Finding the Goldilocks Zone</title>
      <link>https://arxiv.org/abs/2504.04314</link>
      <description>arXiv:2504.04314v1 Announce Type: cross 
Abstract: The challenge of clustering short text data lies in balancing informativeness with interpretability. Traditional evaluation metrics often overlook this trade-off. Inspired by linguistic principles of communicative efficiency, this paper investigates the optimal number of clusters by quantifying the trade-off between informativeness and cognitive simplicity. We use large language models (LLMs) to generate cluster names and evaluate their effectiveness through semantic density, information theory, and clustering accuracy. Our results show that Gaussian Mixture Model (GMM) clustering on embeddings generated by a LLM, increases semantic density compared to random assignment, effectively grouping similar bios. However, as clusters increase, interpretability declines, as measured by a generative LLM's ability to correctly assign bios based on cluster names. A logistic regression analysis confirms that classification accuracy depends on the semantic similarity between bios and their assigned cluster names, as well as their distinction from alternatives.
  These findings reveal a "Goldilocks zone" where clusters remain distinct yet interpretable. We identify an optimal range of 16-22 clusters, paralleling linguistic efficiency in lexical categorization. These insights inform both theoretical models and practical applications, guiding future research toward optimising cluster interpretability and usefulness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04314v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin Miller, Tristram Alexander</dc:creator>
    </item>
    <item>
      <title>On the bias of the Gini coefficient estimator for zero-truncated Poisson distributions</title>
      <link>https://arxiv.org/abs/2504.04518</link>
      <description>arXiv:2504.04518v1 Announce Type: cross 
Abstract: This paper analyzes the Gini coefficient estimator for zero-truncated Poisson populations, revealing the presence of bias, and provides a mathematical expression for the bias, along with a bias-corrected estimator, which is evaluated using Monte Carlo simulation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04518v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>SurvSurf: a partially monotonic neural network for first-hitting time prediction of intermittently observed discrete and continuous sequential events</title>
      <link>https://arxiv.org/abs/2504.04997</link>
      <description>arXiv:2504.04997v1 Announce Type: cross 
Abstract: We propose a neural-network based survival model (SurvSurf) specifically designed for direct and simultaneous probabilistic prediction of the first hitting time of sequential events from baseline. Unlike existing models, SurvSurf is theoretically guaranteed to never violate the monotonic relationship between the cumulative incidence functions of sequential events, while allowing nonlinear influence from predictors. It also incorporates implicit truths for unobserved intermediate events in model fitting, and supports both discrete and continuous time and events. We also identified a variant of the Integrated Brier Score (IBS) that showed robust correlation with the mean squared error (MSE) between the true and predicted probabilities by accounting for implied truths about the missing intermediate events. We demonstrated the superiority of SurvSurf compared to modern and traditional predictive survival models in two simulated datasets and two real-world datasets, using MSE, the more robust IBS and by measuring the extent of monotonicity violation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04997v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichen Kelly Chen, S\"oren Dittmer, Kinga Bernatowicz, Josep Ar\'us-Pous, Kamen Bliznashki, John Aston, James H. F. Rudd, Carola-Bibiane Sch\"onlieb, James Jones, Michael Roberts</dc:creator>
    </item>
    <item>
      <title>Detecting relevant dependencies under measurement error with applications to the analysis of planetary system evolution</title>
      <link>https://arxiv.org/abs/2504.05055</link>
      <description>arXiv:2504.05055v1 Announce Type: cross 
Abstract: Exoplanets play an important role in understanding the mechanics of planetary system formation and orbital evolution. In this context the correlations of different parameters of the planets and their host star are useful guides in the search for explanatory mechanisms. Based on a reanalysis of the data set from \cite{figueria14} we study the as of now still poorly understood correlation between planetary surface gravity and stellar activity of Hot Jupiters. Unfortunately, data collection often suffers from measurement errors due to complicated and indirect measurement setups, rendering standard inference techniques unreliable.
  We present new methods to estimate and test for correlations in a deconvolution framework and thereby improve the state of the art analysis of the data in two directions. First, we are now able to account for additive measurement errors which facilitates reliable inference. Second we test for relevant changes, i.e. we are testing for correlations exceeding a certain threshold $\Delta$. This reflects the fact that small nonzero correlations are to be expected for real life data almost always and that standard statistical tests will therefore always reject the null of no correlation given sufficient data. Our theory focuses on quantities that can be estimated by U-Statistics which contain a variety of correlation measures. We propose a bootstrap test and establish its theoretical validity. As a by product we also obtain confidence intervals. Applying our methods to the Hot Jupiter data set from \cite{figueria14}, we observe that taking into account the measurement errors yields smaller point estimates and the null of no relevant correlation is rejected only for very small $\Delta$. This demonstrates the importance of considering the impact of measurement errors to avoid misleading conclusions from the resulting statistical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05055v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Bastian, Nicolai Bissantz</dc:creator>
    </item>
    <item>
      <title>DDPM Score Matching and Distribution Learning</title>
      <link>https://arxiv.org/abs/2504.05161</link>
      <description>arXiv:2504.05161v1 Announce Type: cross 
Abstract: Score estimation is the backbone of score-based generative models (SGMs), especially denoising diffusion probabilistic models (DDPMs). A key result in this area shows that with accurate score estimates, SGMs can efficiently generate samples from any realistic data distribution (Chen et al., ICLR'23; Lee et al., ALT'23). This distribution learning result, where the learned distribution is implicitly that of the sampler's output, does not explain how score estimation relates to classical tasks of parameter and density estimation.
  This paper introduces a framework that reduces score estimation to these two tasks, with various implications for statistical and computational learning theory:
  Parameter Estimation: Koehler et al. (ICLR'23) demonstrate that a score-matching variant is statistically inefficient for the parametric estimation of multimodal densities common in practice. In contrast, we show that under mild conditions, denoising score-matching in DDPMs is asymptotically efficient.
  Density Estimation: By linking generation to score estimation, we lift existing score estimation guarantees to $(\epsilon,\delta)$-PAC density estimation, i.e., a function approximating the target log-density within $\epsilon$ on all but a $\delta$-fraction of the space. We provide (i) minimax rates for density estimation over H\"older classes and (ii) a quasi-polynomial PAC density estimation algorithm for the classical Gaussian location mixture model, building on and addressing an open problem from Gatmiry et al. (arXiv'24).
  Lower Bounds for Score Estimation: Our framework offers the first principled method to prove computational lower bounds for score estimation across general distributions. As an application, we establish cryptographic lower bounds for score estimation in general Gaussian mixture models, conceptually recovering Song's (NeurIPS'24) result and advancing his key open problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05161v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sinho Chewi, Alkis Kalavasis, Anay Mehrotra, Omar Montasser</dc:creator>
    </item>
    <item>
      <title>Dimension-Free Convergence of Diffusion Models for Approximate Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2504.05300</link>
      <description>arXiv:2504.05300v1 Announce Type: cross 
Abstract: Diffusion models are distinguished by their exceptional generative performance, particularly in producing high-quality samples through iterative denoising. While current theory suggests that the number of denoising steps required for accurate sample generation should scale linearly with data dimension, this does not reflect the practical efficiency of widely used algorithms like Denoising Diffusion Probabilistic Models (DDPMs). This paper investigates the effectiveness of diffusion models in sampling from complex high-dimensional distributions that can be well-approximated by Gaussian Mixture Models (GMMs). For these distributions, our main result shows that DDPM takes at most $\widetilde{O}(1/\varepsilon)$ iterations to attain an $\varepsilon$-accurate distribution in total variation (TV) distance, independent of both the ambient dimension $d$ and the number of components $K$, up to logarithmic factors. Furthermore, this result remains robust to score estimation errors. These findings highlight the remarkable effectiveness of diffusion models in high-dimensional settings given the universal approximation capability of GMMs, and provide theoretical insights into their practical success.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05300v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gen Li, Changxiao Cai, Yuting Wei</dc:creator>
    </item>
    <item>
      <title>Parametrization, Prior Independence, and the Semiparametric Bernstein-von Mises Theorem for the Partially Linear Model</title>
      <link>https://arxiv.org/abs/2306.03816</link>
      <description>arXiv:2306.03816v5 Announce Type: replace 
Abstract: I prove a semiparametric Bernstein-von Mises theorem for a partially linear regression model with independent priors for the low-dimensional parameter of interest and the infinite-dimensional nuisance parameters. My result avoids a challenging prior invariance condition that arises from a loss of information associated with not knowing the nuisance parameter. The key idea is to employ a feasible reparametrization of the partially linear regression model that reflects the semiparametric structure of the model. This allows a researcher to assume independent priors for the model parameters while automatically accounting for the loss of information associated with not knowing the nuisance parameters. The theorem is verified for uniform wavelet series priors and Mat\'{e}rn Gaussian process priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.03816v5</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher D. Walker</dc:creator>
    </item>
    <item>
      <title>On the best approximation by finite Gaussian mixtures</title>
      <link>https://arxiv.org/abs/2404.08913</link>
      <description>arXiv:2404.08913v2 Announce Type: replace 
Abstract: We consider the problem of approximating a general Gaussian location mixture by finite mixtures. The minimum order of finite mixtures that achieve a prescribed accuracy (measured by various $f$-divergences) is determined within constant factors for the family of mixing distributions with compactly support or appropriate assumptions on the tail probability including subgaussian and subexponential. While the upper bound is achieved using the technique of local moment matching, the lower bound is established by relating the best approximation error to the low-rank approximation of certain trigonometric moment matrices, followed by a refined spectral analysis of their minimum eigenvalue. In the case of Gaussian mixing distributions, this result corrects a previous lower bound in [Allerton Conference 48 (2010) 620-628].</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08913v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yun Ma, Yihong Wu, Pengkun Yang</dc:creator>
    </item>
    <item>
      <title>Minmax Trend Filtering: Generalizations of Total Variation Minmax Trend Filtering: Generalizations of Total Variation Denoising via a Local Minmax/Maxmin Formula</title>
      <link>https://arxiv.org/abs/2410.03041</link>
      <description>arXiv:2410.03041v2 Announce Type: replace 
Abstract: Total Variation Denoising (TVD) is a fundamental denoising and smoothing method. In this article, we identify a new local minmax/maxmin formula producing two estimators which sandwich the univariate TVD estimator at every point. Operationally, this formula gives a local definition of TVD as a minmax/maxmin of a simple function of local averages. Moreover we find that this minmax/maxmin formula is generalizeable and can be used to define other TVD like estimators. In this article we propose and study higher order polynomial versions of TVD which are defined pointwise lying between minmax and maxmin optimizations of penalized local polynomial regressions over intervals of different scales. These appear to be new nonparametric regression methods, different from usual Trend Filtering and any other existing method in the nonparametric regression toolbox. We call these estimators Minmax Trend Filtering (MTF). We show how the proposed local definition of TVD/MTF estimator makes it tractable to bound pointwise estimation errors in terms of a local bias variance like trade-off. This type of local analysis of TVD/MTF is new and arguably simpler than existing analyses of TVD/Trend Filtering. In particular, apart from minimax rate optimality over bounded variation and piecewise polynomial classes, our pointwise estimation error bounds also enable us to derive local rates of convergence for (locally) Holder Smooth signals. These local rates offer a new pointwise explanation of local adaptivity of TVD/MTF instead of global (MSE) based justifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03041v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabyasachi Chatterjee</dc:creator>
    </item>
    <item>
      <title>Asymptotics for estimating a diverging number of parameters -- with and without sparsity</title>
      <link>https://arxiv.org/abs/2411.17395</link>
      <description>arXiv:2411.17395v2 Announce Type: replace 
Abstract: We consider high-dimensional estimation problems where the number of parameters diverges with the sample size. General conditions are established for consistency, uniqueness, and asymptotic normality in both unpenalized and penalized estimation settings. The conditions are weak and accommodate a broad class of estimation problems, including ones with non-convex and group structured penalties. The wide applicability of the results is illustrated through diverse examples, including generalized linear models, multi-sample inference, and stepwise estimation procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17395v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jana Gauss, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>Universal coding, intrinsic volumes, and metric complexity</title>
      <link>https://arxiv.org/abs/2303.07279</link>
      <description>arXiv:2303.07279v2 Announce Type: replace-cross 
Abstract: We study sequential probability assignment in the Gaussian setting, where the goal is to predict, or equivalently compress, a sequence of real-valued observations almost as well as the best Gaussian distribution with mean constrained to a given subset of $\mathbb{R}^n$. First, in the case of a convex constraint set $K$, we express the hardness of the prediction problem (the minimax regret) in terms of the intrinsic volumes of $K$; specifically, it equals the logarithm of the Wills functional from convex geometry. We then establish a comparison inequality for the Wills functional in the general nonconvex case, which underlines the metric nature of this quantity and generalizes the Slepian-Sudakov-Fernique comparison principle for the Gaussian width. Motivated by this inequality, we characterize the exact order of magnitude of the considered functional for a general nonconvex set, in terms of global covering numbers and local Gaussian widths. This implies sharp estimates, of metric nature, on the log-Laplace transform of the intrinsic volume sequence of a convex body. As part of our analysis, we also characterize the minimax redundancy for a general constraint set. We finally relate and contrast our findings with classical asymptotic results in information theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07279v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaouad Mourtada</dc:creator>
    </item>
    <item>
      <title>Fused Extended Two-Way Fixed Effects for Difference-in-Differences With Staggered Adoptions</title>
      <link>https://arxiv.org/abs/2312.05985</link>
      <description>arXiv:2312.05985v4 Announce Type: replace-cross 
Abstract: To address the bias of the canonical two-way fixed effects estimator for difference-in-differences under staggered adoptions, Wooldridge (2021) proposed the extended two-way fixed effects estimator, which adds many parameters. However, this reduces efficiency. Restricting some of these parameters to be equal (for example, subsequent treatment effects within a cohort) helps, but ad hoc restrictions may reintroduce bias. We propose a machine learning estimator with a single tuning parameter, fused extended two-way fixed effects (FETWFE), that enables automatic data-driven selection of these restrictions. We prove that under an appropriate sparsity assumption FETWFE identifies the correct restrictions with probability tending to one, which improves efficiency. We also prove the consistency, oracle property, and asymptotic normality of FETWFE for several classes of heterogeneous marginal treatment effect estimators under either conditional or marginal parallel trends, and we prove the same results for conditional average treatment effects under conditional parallel trends. We provide an R package implementing fused extended two-way fixed effects, and we demonstrate FETWFE in simulation studies and an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05985v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Faletto</dc:creator>
    </item>
    <item>
      <title>Differentially Private Sliced Inverse Regression: Minimax Optimality and Algorithm</title>
      <link>https://arxiv.org/abs/2401.08150</link>
      <description>arXiv:2401.08150v2 Announce Type: replace-cross 
Abstract: Privacy preservation has become a critical concern in high-dimensional data analysis due to the growing prevalence of data-driven applications. Since its proposal, sliced inverse regression has emerged as a widely utilized statistical technique to reduce the dimensionality of covariates while maintaining sufficient statistical information. In this paper, we propose optimally differentially private algorithms specifically designed to address privacy concerns in the context of sufficient dimension reduction. We establish lower bounds for differentially private sliced inverse regression in low and high dimensional settings. Moreover, we develop differentially private algorithms that achieve the minimax lower bounds up to logarithmic factors. Through a combination of simulations and real data analysis, we illustrate the efficacy of these differentially private algorithms in safeguarding privacy while preserving vital information within the reduced dimension space. As a natural extension, we can readily offer analogous lower and upper bounds for differentially private sparse principal component analysis, a topic that may also be of potential interest to the statistics and machine learning community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08150v2</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xintao Xia, Linjun Zhang, Zhanrui Cai</dc:creator>
    </item>
    <item>
      <title>Distributed Tensor Principal Component Analysis with Data Heterogeneity</title>
      <link>https://arxiv.org/abs/2405.11681</link>
      <description>arXiv:2405.11681v3 Announce Type: replace-cross 
Abstract: As tensors become widespread in modern data analysis, Tucker low-rank Principal Component Analysis (PCA) has become essential for dimensionality reduction and structural discovery in tensor datasets. Motivated by the common scenario where large-scale tensors are distributed across diverse geographic locations, this paper investigates tensor PCA within a distributed framework where direct data pooling is impractical.
  We offer a comprehensive analysis of three specific scenarios in distributed Tensor PCA: a homogeneous setting in which tensors at various locations are generated from a single noise-affected model; a heterogeneous setting where tensors at different locations come from distinct models but share some principal components, aiming to improve estimation across all locations; and a targeted heterogeneous setting, designed to boost estimation accuracy at a specific location with limited samples by utilizing transferred knowledge from other sites with ample data.
  We introduce novel estimation methods tailored to each scenario, establish statistical guarantees, and develop distributed inference techniques to construct confidence regions. Our theoretical findings demonstrate that these distributed methods achieve sharp rates of accuracy by efficiently aggregating shared information across different tensors, while maintaining reasonable communication costs. Empirical validation through simulations and real-world data applications highlights the advantages of our approaches, particularly in managing heterogeneous tensor data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11681v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Elynn Chen, Xi Chen, Wenbo Jing, Yichen Zhang</dc:creator>
    </item>
    <item>
      <title>Comparing experiments in discounted problems</title>
      <link>https://arxiv.org/abs/2405.16458</link>
      <description>arXiv:2405.16458v3 Announce Type: replace-cross 
Abstract: This paper compares statistical experiments in discounted problems, ranging from the simplest ones where the state is fixed and the flow of information exogenous to more complex ones, where the decision-maker controls the flow of information or the state changes over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16458v3</guid>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ludovic Renou, Xavier Venel</dc:creator>
    </item>
    <item>
      <title>The Empirical Spectral Distribution of i.i.d. Random Matrices with Random Perturbations</title>
      <link>https://arxiv.org/abs/2410.21919</link>
      <description>arXiv:2410.21919v2 Announce Type: replace-cross 
Abstract: Consider a $d\times d$ random matrix $\mathbf{M} :=\frac{1}{\sqrt{d}}\mathbf{W}+\mathbf{U}\mathbf{\Lambda}\mathbf{U}^*$, where the entries of $\mathbf{W}$ are i.i.d.\ complex variables with zero mean, unit variance, and finite fourth moment; $\mathbf{\Lambda}=\operatorname{diag}(\lambda_1, \lambda_2, \ldots, \lambda_r)$ is a diagonal matrix with $\lambda_1\ge\lambda_2\ge \ldots\ge \lambda_r&gt;1$ and $\mathbf{U}$ is uniformly drawn from the Stiefel manifold $\operatorname{Stief}(d,r)$ or its complex counterpart $\operatorname{Stief}_\mathbb{C}(d,r)$. We propose three results: (i) We show that the top $r$ eigenvalues of $\mathbf{M}$ concentrate around $\lambda_1,\lambda_2,\ldots,\lambda_r$, respectively, and the magnitude of the $(r{+}1)$-th eigenvalue concentrates around $1$. (ii) For the case $r=1$ (i.e., $\mathbf{M}=\frac{1}{\sqrt{d}}\mathbf{W}+\lambda\mathbf{u}\mathbf{u}^*$), we prove that the top eigenvector (namely $\mathbf{v}_1(\mathbf{M})$) aligns with $\mathbf{u}$ in the sense that $\left|\langle\mathbf{v}_1(\mathbf{M}), \mathbf{u}\rangle\right|$ converges to a limit which depends on $\lambda$ (in probability). (iii) In applications, we present the first optimal \textit{query complexity} lower bound for approximating the top eigenvector of asymmetric matrices. We show that for every $\operatorname{gap}=\frac{\lambda-1}{\lambda}\in (0,1/2]$, then $\operatorname{gap}(\mathbf{M})=\Omega(\operatorname{gap})$ with high probability, and if a matrix-vector product algorithm can identify a vector $\hat{\mathbf{v}}$ which satisfies $\left\|\hat{\mathbf{v}}-\mathbf{v}_1(\mathbf{M}) \right\|_2^2\le \operatorname{const}\times \operatorname{gap}$, it needs at least $\mathcal{O}\left(\frac{\log d}{\operatorname{gap}}\right)$ queries of matrix-vector products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21919v2</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Chen, Zhihua Zhang</dc:creator>
    </item>
    <item>
      <title>Non-Bayesian Learning in Misspecified Models</title>
      <link>https://arxiv.org/abs/2503.18024</link>
      <description>arXiv:2503.18024v2 Announce Type: replace-cross 
Abstract: Deviations from Bayesian updating are traditionally categorized as biases, errors, or fallacies, thus implying their inherent ``sub-optimality.'' We offer a more nuanced view. We demonstrate that, in learning problems with misspecified models, non-Bayesian updating can outperform Bayesian updating.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18024v2</guid>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Bervoets, Mathieu Faure, Ludovic Renou</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for High-dimensional Time Series with a Directed Acyclic Graphical Structure</title>
      <link>https://arxiv.org/abs/2503.23563</link>
      <description>arXiv:2503.23563v3 Announce Type: replace-cross 
Abstract: In multivariate time series analysis, understanding the underlying causal relationships among variables is often of interest for various applications. Directed acyclic graphs (DAGs) provide a powerful framework for representing causal dependencies. This paper proposes a novel Bayesian approach for modeling multivariate time series where conditional independencies and causal structure are encoded by a DAG. The proposed model allows structural properties such as stationarity to be easily accommodated. Given the application, we further extend the model for matrix-variate time series. We take a Bayesian approach to inference, and a ``projection-posterior'' based efficient computational algorithm is developed. The posterior convergence properties of the proposed method are established along with two identifiability results for the unrestricted structural equation models. The utility of the proposed method is demonstrated through simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23563v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
  </channel>
</rss>
