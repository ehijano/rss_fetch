<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 May 2024 01:49:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Distance based prefetching algorithms for mining of the sporadic requests associations</title>
      <link>https://arxiv.org/abs/2405.18450</link>
      <description>arXiv:2405.18450v1 Announce Type: new 
Abstract: Modern storage systems intensively utilize data prefetching algorithms while processing sequences of the read requests. Performance of the prefetching algorithm (for instance increase of the cache hit ratio of the cache system - CHR) directly affects overall performance characteristics of the storage system (read latency, IOPS, etc.). There are widely known prefetching algorithms that are focused on the discovery of the sequential patterns in the stream of requests. This study examines a family of prefetching algorithms that is focused on mining of the pseudo random (sporadic) patterns between read requests - sporadic prefetching algorithms. The key contribution of this paper is that it discovers a new, lightweight family of distance-based sporadic prefetching algorithms (DBSP) that outperforms the best previously known results on MSR traces collection.Another important contribution of this paper is a thorough description of the procedure for comparing the performance of sporadic prefetchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18450v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vadim Voevodkin, Andrey Sokolov</dc:creator>
    </item>
    <item>
      <title>Learning from Uncertain Data: From Possible Worlds to Possible Models</title>
      <link>https://arxiv.org/abs/2405.18549</link>
      <description>arXiv:2405.18549v1 Announce Type: cross 
Abstract: We introduce an efficient method for learning linear models from uncertain data, where uncertainty is represented as a set of possible variations in the data, leading to predictive multiplicity. Our approach leverages abstract interpretation and zonotopes, a type of convex polytope, to compactly represent these dataset variations, enabling the symbolic execution of gradient descent on all possible worlds simultaneously. We develop techniques to ensure that this process converges to a fixed point and derive closed-form solutions for this fixed point. Our method provides sound over-approximations of all possible optimal models and viable prediction ranges. We demonstrate the effectiveness of our approach through theoretical and empirical analysis, highlighting its potential to reason about model and prediction uncertainty due to data quality issues in training data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18549v1</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <category>cs.SC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiongli Zhu, Su Feng, Boris Glavic, Babak Salimi</dc:creator>
    </item>
    <item>
      <title>GLOCON Database: Design Decisions and User Manual (v1.0)</title>
      <link>https://arxiv.org/abs/2405.18613</link>
      <description>arXiv:2405.18613v1 Announce Type: cross 
Abstract: GLOCON is a database of contentious events automatically extracted from national news sources from various countries in multiple languages. National news sources are utilized, and complete news archives are processed to create an event list for each source. Automation is achieved using a gold standard corpus sampled randomly from complete news archives (Y\"or\"uk et al. 2022) and all annotated by at least two domain experts based on the event definition provided in Duru\c{s}an et al. (2022).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18613v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali H\"urriyeto\u{g}lu, Osman Mutlu, F{\i}rat Duru\c{s}an, Erdem Y\"or\"uk</dc:creator>
    </item>
    <item>
      <title>Adapting Differentially Private Synthetic Data to Relational Databases</title>
      <link>https://arxiv.org/abs/2405.18670</link>
      <description>arXiv:2405.18670v1 Announce Type: cross 
Abstract: Existing differentially private (DP) synthetic data generation mechanisms typically assume a single-source table. In practice, data is often distributed across multiple tables with relationships across tables. In this paper, we introduce the first-of-its-kind algorithm that can be combined with any existing DP mechanisms to generate synthetic relational databases. Our algorithm iteratively refines the relationship between individual synthetic tables to minimize their approximation errors in terms of low-order marginal distributions while maintaining referential integrity. Finally, we provide both DP and theoretical utility guarantees for our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18670v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kaveh Alimohammadi, Hao Wang, Ojas Gulati, Akash Srivastava, Navid Azizan</dc:creator>
    </item>
    <item>
      <title>Navigable Graphs for High-Dimensional Nearest Neighbor Search: Constructions and Limits</title>
      <link>https://arxiv.org/abs/2405.18680</link>
      <description>arXiv:2405.18680v1 Announce Type: cross 
Abstract: There has been significant recent interest in graph-based nearest neighbor search methods, many of which are centered on the construction of navigable graphs over high-dimensional point sets. A graph is navigable if we can successfully move from any starting node to any target node using a greedy routing strategy where we always move to the neighbor that is closest to the destination according to a given distance function. The complete graph is navigable for any point set, but the important question for applications is if sparser graphs can be constructed. While this question is fairly well understood in low-dimensions, we establish some of the first upper and lower bounds for high-dimensional point sets. First, we give a simple and efficient way to construct a navigable graph with average degree $O(\sqrt{n \log n })$ for any set of $n$ points, in any dimension, for any distance function. We compliment this result with a nearly matching lower bound: even under the Euclidean metric in $O(\log n)$ dimensions, a random point set has no navigable graph with average degree $O(n^{\alpha})$ for any $\alpha &lt; 1/2$. Our lower bound relies on sharp anti-concentration bounds for binomial random variables, which we use to show that the near-neighborhoods of a set of random points do not overlap significantly, forcing any navigable graph to have many edges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18680v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haya Diwan, Jinrui Gou, Cameron Musco, Christopher Musco, Torsten Suel</dc:creator>
    </item>
    <item>
      <title>Disambiguate Entity Matching using Large Language Models through Relation Discovery</title>
      <link>https://arxiv.org/abs/2403.17344</link>
      <description>arXiv:2403.17344v2 Announce Type: replace 
Abstract: Entity matching is a critical challenge in data integration and cleaning, central to tasks like fuzzy joins and deduplication. Traditional approaches have focused on overcoming fuzzy term representations through methods such as edit distance, Jaccard similarity, and more recently, embeddings and deep neural networks, including advancements from large language models (LLMs) like GPT. However, the core challenge in entity matching extends beyond term fuzziness to the ambiguity in defining what constitutes a "match," especially when integrating with external databases. This ambiguity arises due to varying levels of detail and granularity among entities, complicating exact matches. We propose a novel approach that shifts focus from purely identifying semantic similarities to understanding and defining the "relations" between entities as crucial for resolving ambiguities in matching. By predefining a set of relations relevant to the task at hand, our method allows analysts to navigate the spectrum of similarity more effectively, from exact matches to conceptually related entities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17344v2</guid>
      <category>cs.DB</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zezhou Huang</dc:creator>
    </item>
    <item>
      <title>A Declarative System for Optimizing AI Workloads</title>
      <link>https://arxiv.org/abs/2405.14696</link>
      <description>arXiv:2405.14696v2 Announce Type: replace-cross 
Abstract: A long-standing goal of data management systems has been to build systems which can compute quantitative insights over large corpora of unstructured data in a cost-effective manner. Until recently, it was difficult and expensive to extract facts from company documents, data from scientific papers, or metrics from image and video corpora. Today's models can accomplish these tasks with high accuracy. However, a programmer who wants to answer a substantive AI-powered query must orchestrate large numbers of models, prompts, and data operations. For even a single query, the programmer has to make a vast number of decisions such as the choice of model, the right inference method, the most cost-effective inference hardware, the ideal prompt design, and so on. The optimal set of decisions can change as the query changes and as the rapidly-evolving technical landscape shifts. In this paper we present Palimpzest, a system that enables anyone to process AI-powered analytical queries simply by defining them in a declarative language. The system uses its cost optimization framework to implement the query plan with the best trade-offs between runtime, financial cost, and output data quality. We describe the workload of AI-powered analytics tasks, the optimization methods that Palimpzest uses, and the prototype system itself. We evaluate Palimpzest on tasks in Legal Discovery, Real Estate Search, and Medical Schema Matching. We show that even our simple prototype offers a range of appealing plans, including one that is 3.3x faster and 2.9x cheaper than the baseline method, while also offering better data quality. With parallelism enabled, Palimpzest can produce plans with up to a 90.3x speedup at 9.1x lower cost relative to a single-threaded GPT-4 baseline, while obtaining an F1-score within 83.5% of the baseline. These require no additional work by the user.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14696v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunwei Liu, Matthew Russo, Michael Cafarella, Lei Cao, Peter Baille Chen, Zui Chen, Michael Franklin, Tim Kraska, Samuel Madden, Gerardo Vitagliano</dc:creator>
    </item>
  </channel>
</rss>
