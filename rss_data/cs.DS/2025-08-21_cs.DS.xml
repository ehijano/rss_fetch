<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal Subspace Embeddings: Resolving Nelson-Nguyen Conjecture Up to Sub-Polylogarithmic Factors</title>
      <link>https://arxiv.org/abs/2508.14234</link>
      <description>arXiv:2508.14234v1 Announce Type: new 
Abstract: We give a proof of the conjecture of Nelson and Nguyen [FOCS 2013] on the optimal dimension and sparsity of oblivious subspace embeddings, up to sub-polylogarithmic factors: For any $n\geq d$ and $\epsilon\geq d^{-O(1)}$, there is a random $\tilde O(d/\epsilon^2)\times n$ matrix $\Pi$ with $\tilde O(\log(d)/\epsilon)$ non-zeros per column such that for any $A\in\mathbb{R}^{n\times d}$, with high probability, $(1-\epsilon)\|Ax\|\leq\|\Pi Ax\|\leq(1+\epsilon)\|Ax\|$ for all $x\in\mathbb{R}^d$, where $\tilde O(\cdot)$ hides only sub-polylogarithmic factors in $d$. Our result in particular implies a new fastest sub-current matrix multiplication time reduction of size $\tilde O(d/\epsilon^2)$ for a broad class of $n\times d$ linear regression tasks.
  A key novelty in our analysis is a matrix concentration technique we call iterative decoupling, which we use to fine-tune the higher-order trace moment bounds attainable via existing random matrix universality tools [Brailovskaya and van Handel, GAFA 2024].</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14234v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shabarish Chenakkod, Micha{\l} Derezi\'nski, Xiaoyu Dong</dc:creator>
    </item>
    <item>
      <title>Nearly Tight Bounds for the Online Sorting Problem</title>
      <link>https://arxiv.org/abs/2508.14287</link>
      <description>arXiv:2508.14287v1 Announce Type: new 
Abstract: In the online sorting problem, a sequence of $n$ numbers in $[0, 1]$ (including $\{0,1\}$) have to be inserted in an array of size $m \ge n$ so as to minimize the sum of absolute differences between pairs of numbers occupying consecutive non-empty cells. Previously, Aamand {\em et al.} (SODA 2023) gave a deterministic $2^{\sqrt{\log n} \sqrt{\log \log n + \log (1/\varepsilon)}}$-competitive algorithm when $m = (1+\varepsilon) n$ for any $\varepsilon \ge \Omega(\log n/n)$. They also showed a lower bound: with $m = \gamma n$ space, the competitive ratio of any deterministic algorithm is at least $\frac{1}{\gamma}\cdot\Omega(\log n / \log \log n)$. This left an exponential gap between the upper and lower bounds for the problem.
  In this paper, we bridge this exponential gap and almost completely resolve the online sorting problem. First, we give a deterministic $O(\log^2 n / \varepsilon)$-competitive algorithm with $m = (1+\varepsilon) n$, for any $\varepsilon \ge \Omega(\log n / n)$. Next, for $m = \gamma n$ where $\gamma = [O(1), O(\log^2 n)]$, we give a deterministic $O(\log^2 n / \gamma)$-competitive algorithm. In particular, this implies an $O(1)$-competitive algorithm with $O(n \log^2 n)$ space, which is within an $O(\log n\cdot \log \log n)$ factor of the lower bound of $\Omega(n \log n / \log \log n)$. Combined, the two results imply a close to optimal tradeoff between space and competitive ratio for the entire range of interest: specifically, an upper bound of $O(\log^2 n)$ on the product of the competitive ratio and $\gamma$ while the lower bound on this product is $\Omega(\log n / \log\log n)$. We also show that these results can be extended to the case when the range of the numbers is not known in advance, for an additional $O(\log n)$ factor in the competitive ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14287v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yossi Azar, Debmalya Panigrahi, Or Vardi</dc:creator>
    </item>
    <item>
      <title>Sublinear-Time Approximation for Graph Frequency Vectors in Hyperfinite Graphs</title>
      <link>https://arxiv.org/abs/2508.14324</link>
      <description>arXiv:2508.14324v1 Announce Type: new 
Abstract: In this work, we address the problem of approximating the $k$-disc distribution (``frequency vector") of a bounded-degree graph in sublinear-time under the assumption of hyperfiniteness. We revisit the partition-oracle framework of Hassidim, Kelner, Nguyen, and Onak \cite{hassidim2009local}, and provide a concise, self-contained analysis that explicitly separates the two sources of error: (i) the cut error, controlled by hyperfiniteness parameter $\phi$, which incurs at most $\varepsilon/2$ in $\ell_1$-distance by removing at most $\phi |V|$ edges; and (ii) the sampling error, controlled by the accuracy parameter $\varepsilon$, bounded by $\varepsilon/2$ via $N=\Theta(\varepsilon^{-2})$ random vertex queries and a Chernoff and union bound argument. Combining these yields an overall $\ell_1$-error of $\varepsilon$ with high probability. Algorithmically, we show that by sampling $N=\lceil C\varepsilon^{-2} \rceil$ vertices and querying the local partition oracle, one can in time $poly(d,k,\varepsilon^{-1})$ construct a summary graph $H$ of size $|H|=poly(d^k,1/\varepsilon)$ whose $k$-disc frequency vector approximates that of the original graph within $\varepsilon$ in $\ell_1$-distance. Our approach clarifies the dependence of both runtime and summary-size on the parameter $d$,$k$, and $\varepsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14324v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Moroie</dc:creator>
    </item>
    <item>
      <title>Improved Online Sorting</title>
      <link>https://arxiv.org/abs/2508.14361</link>
      <description>arXiv:2508.14361v1 Announce Type: new 
Abstract: We study the online sorting problem, where $n$ real numbers arrive in an online fashion, and the algorithm must immediately place each number into an array of size $(1+\varepsilon) n$ before seeing the next number. After all $n$ numbers are placed into the array, the cost is defined as the sum over the absolute differences of all $n-1$ pairs of adjacent numbers in the array, ignoring empty array cells. Aamand, Abrahamsen, Beretta, and Kleist introduced the problem and obtained a deterministic algorithm with cost $2^{O\left(\sqrt{\log n \cdot\log\log n +\log \varepsilon^{-1}}\right)}$, and a lower bound of $\Omega(\log n / \log\log n)$ for deterministic algorithms. We obtain a deterministic algorithm with quasi-polylogarithmic cost $\left(\varepsilon^{-1}\log n\right)^{O\left(\log \log n\right)}$.
  Concurrent and independent work by Azar, Panigrahi, and Vardi achieves polylogarithmic cost $O(\varepsilon^{-1}\log^2 n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14361v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jubayer Nirjhor, Nicole Wein</dc:creator>
    </item>
    <item>
      <title>Compact representation of maximal palindromes</title>
      <link>https://arxiv.org/abs/2508.14384</link>
      <description>arXiv:2508.14384v1 Announce Type: new 
Abstract: Palindromes are strings that read the same forward and backward. The computation of palindromic structures within strings is a fundamental problem in string algorithms, being motivated by potential applications in formal language theory and bioinformatics. Although the number of palindromic factors in a string of length $n$ can be quadratic, they can be implicitly represented in $O(n \log n)$ bits of space by storing the lengths of all maximal palindromes in an integer array, which can be computed in $O(n)$ time [Manacher, 1975]. In this paper, we propose a novel $O(n)$-bit representation of all maximal palindromes in a string, which enables $O(1)$-time retrieval of the length of the maximal palindrome centered at any given position. Since Manacher's algorithm and the notion of maximal palindromes are widely utilized for solving numerous problems involving palindromic structures, our compact representation will accelerate the development of more space-efficient solutions. Indeed, as the first application of our compact representation of maximal palindromes, we present a data structure of size $O(n)$ bits that can compute the longest palindrome appearing in any given factor of the string in $O(\log n)$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14384v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Mieno</dc:creator>
    </item>
    <item>
      <title>Incremental-Decremental Maximization</title>
      <link>https://arxiv.org/abs/2508.14516</link>
      <description>arXiv:2508.14516v1 Announce Type: new 
Abstract: We introduce a framework for incremental-decremental maximization that captures the gradual transformation or renewal of infrastructures. In our model, an initial solution is transformed one element at a time and the utility of an intermediate solution is given by the sum of the utilities of the transformed and untransformed parts. We propose a simple randomized and a deterministic algorithm that both find an order in which to transform the elements while maintaining a large utility during all stages of transformation, relative to an optimum solution for the current stage. More specifically, our algorithms yield competitive solutions for utility functions of bounded curvature and/or generic submodularity ratio, and, in particular, for submodular functions, and gross substitute functions. Our results exhibit that incremental-decremental maximization is substantially more difficult than incremental maximization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14516v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yann Disser, Max Klimm, Annette Lutz, Lea Strubberg</dc:creator>
    </item>
    <item>
      <title>A $(4/3+\varepsilon)$-Approximation for Preemptive Scheduling with Batch Setup Times</title>
      <link>https://arxiv.org/abs/2508.14528</link>
      <description>arXiv:2508.14528v1 Announce Type: new 
Abstract: We consider the $\mathcal{NP}$-hard problem $\mathrm{P} \mathbf{\vert} \mathrm{pmtn, setup=s_i} \mathbf{\vert} \mathrm{C_{\max}}$, the problem of scheduling $n$ jobs, which are divided into $c$ classes, on $m$ identical parallel machines while allowing preemption. For each class $i$ of the $c$ classes, we are given a setup time $s_i$ that is required to be scheduled whenever a machine switches from processing a job of one class to a job from another class. The goal is to find a schedule that minimizes the makespan.
  We give a $(4/3+\varepsilon)$-approximate algorithm with run time in $\mathcal{O}(n^2 \log(1/\varepsilon))$. For any $\varepsilon &lt; 1/6$, this improves upon the previously best known approximation ratio of $3/2$ for this problem.
  Our main technical contributions are as follows. We first partition any instance into an "easy" and a "hard" part, such that a $4/3 T$-approximation for the former is easy to compute for some given makespan $T$. We then proceed to show our main structural result, namely that there always exists a $4/3 T$-approximation for any instance that has a solution with makespan $T$, where the hard part has some easy to compute properties. Finally, we obtain an algorithm that computes a $(4/3+\varepsilon)$-approximation in time n $\mathcal{O}(n^2 \log(1/\varepsilon))$ for general instances by computing solutions with the previously shown structural properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14528v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Max A. Deppert, David Fischer, Klaus Jansen</dc:creator>
    </item>
    <item>
      <title>Explainable Information Design</title>
      <link>https://arxiv.org/abs/2508.14196</link>
      <description>arXiv:2508.14196v1 Announce Type: cross 
Abstract: The optimal signaling schemes in information design (Bayesian persuasion) problems often involve non-explainable randomization or disconnected partitions of state space, which are too intricate to be audited or communicated. We propose explainable information design in the context of information design with a continuous state space, restricting the information designer to use $K$-partitional signaling schemes defined by deterministic and monotone partitions of the state space, where a unique signal is sent for all states in each part. We first prove that the price of explainability (PoE) -- the ratio between the performances of the optimal explainable signaling scheme and unrestricted signaling scheme -- is exactly $1/2$ in the worst case, meaning that partitional signaling schemes are never worse than arbitrary signaling schemes by a factor of 2.
  We then study the complexity of computing optimal explainable signaling schemes. We show that the exact optimization problem is NP-hard in general. But for Lipschitz utility functions, an $\varepsilon$-approximately optimal explainable signaling scheme can be computed in polynomial time. And for piecewise constant utility functions, we provide an efficient algorithm to find an explainable signaling scheme that provides a $1/2$ approximation to the optimal unrestricted signaling scheme, which matches the worst-case PoE bound.
  A technical tool we develop is a conversion from any optimal signaling scheme (which satisfies a bi-pooling property) to a partitional signaling scheme that achieves $1/2$ fraction of the expected utility of the former. We use this tool in the proofs of both our PoE result and algorithmic result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14196v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiling Chen, Tao Lin, Wei Tang, Jamie Tucker-Foltz</dc:creator>
    </item>
    <item>
      <title>Lagrangian Simulation Volume-Based Contour Tree Simplification</title>
      <link>https://arxiv.org/abs/2508.14339</link>
      <description>arXiv:2508.14339v1 Announce Type: cross 
Abstract: Many scientific and engineering problems are modelled by simulating scalar fields defined either on space-filling meshes (Eulerian) or as particles (Lagrangian). For analysis and visualization, topological primitives such as contour trees can be used, but these often need simplification to filter out small-scale features. For parcel-based convective cloud simulations, simplification of the contour tree requires a volumetric measure rather than persistence. Unlike for cubic meshes, volume cannot be approximated by counting regular vertices. Typically, this is addressed by resampling irregular data onto a uniform grid. Unfortunately, the spatial proximity of parcels requires a high sampling frequency, resulting in a massive increase in data size for processing. We therefore extend volume-based contour tree simplification to parcel-in-cell simulations with a graph adaptor in Viskores (VTK-m), using Delaunay tetrahedralization of the parcel centroids as input. Instead of relying on a volume approximation by counting regular vertices -- as was done for cubic meshes -- we adapt the 2D area splines reported by Bajaj et al. 10.1145/259081.259279, and Zhou et al. 10.1109/TVCG.2018.2796555. We implement this in Viskores (formerly called VTK-m) as prefix-sum style hypersweeps for parallel efficiency and show how it can be generalized to compute any integrable property. Finally, our results reveal that contour trees computed directly on the parcels are orders of magnitude faster than computing them on a resampled grid, while also arguably offering better quality segmentation, avoiding interpolation artifacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14339v1</guid>
      <category>cs.CG</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Domantas Dilys, Hamish Carr, Steven Boeing</dc:creator>
    </item>
    <item>
      <title>Properties of Egalitarian Sequences of Committees: Theory and Experiments</title>
      <link>https://arxiv.org/abs/2508.14439</link>
      <description>arXiv:2508.14439v1 Announce Type: cross 
Abstract: We study the task of electing egalitarian sequences of $\tau$ committees given a set of agents with additive utilities for candidates available on each of $\tau$ levels. We introduce several rules for electing an egalitarian committee sequence as well as properties for such rules. We settle the computational complexity of finding a winning sequence for our rules and classify them against our properties. Additionally, we transform sequential election data from existing election data from the literature. Using this data set, we compare our rules empirically and test them experimentally against our properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14439v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paula B\"ohm, Robert Bredereck, Till Fluschnik</dc:creator>
    </item>
    <item>
      <title>Auditable Shared Objects: From Registers to Synchronization Primitives</title>
      <link>https://arxiv.org/abs/2508.14506</link>
      <description>arXiv:2508.14506v1 Announce Type: cross 
Abstract: Auditability allows to track operations performed on a shared object, recording who accessed which information. This gives data owners more control on their data. Initially studied in the context of single-writer registers, this work extends the notion of auditability to other shared objects, and studies their properties.
  We start by moving from single-writer to multi-writer registers, and provide an implementation of an auditable $n$-writer $m$-reader read / write register, with $O(n+m)$ step complexity. This implementation uses $(m+n)$-sliding registers, which have consensus number $m+n$. We show that this consensus number is necessary. The implementation extends naturally to support an auditable load-linked / store-conditional (LL/SC) shared object. LL/SC is a primitive that supports efficient implementation of many shared objects. Finally, we relate auditable registers to other access control objects, by implementing an anti-flickering deny list from auditable registers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14506v1</guid>
      <category>cs.DC</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>DISC 2025</arxiv:journal_reference>
      <dc:creator>Hagit Attiya, Antonio Fern\'andez Anta, Alessia Milani, Alexandre Rapetti, Corentin Travers</dc:creator>
    </item>
    <item>
      <title>$TIME[t] \subseteq SPACE[O(\sqrt{t})]$ via Tree Height Compression</title>
      <link>https://arxiv.org/abs/2508.14831</link>
      <description>arXiv:2508.14831v1 Announce Type: cross 
Abstract: We prove a square-root space simulation for deterministic multitape Turing machines, showing $\TIME[t] \subseteq \SPACE[O(\sqrt{t})]$. The key step is a Height Compression Theorem that uniformly (and in logspace) reshapes the canonical left-deep succinct computation tree for a block-respecting run into a binary tree whose evaluation-stack depth along any DFS path is $O(\log T)$ for $T = \lceil t/b \rceil$, while preserving $O(b)$ work at leaves, $O(1)$ at internal nodes, and edges that are logspace-checkable; semantic correctness across merges is witnessed by an exact $O(b)$ window replay at the unique interface. The proof uses midpoint (balanced) recursion, a per-path potential that bounds simultaneously active interfaces by $O(\log T)$, and an indegree-capping replacement of multiway merges by balanced binary combiners. Algorithmically, an Algebraic Replay Engine with constant-degree maps over a constant-size field, together with pointerless DFS and index-free streaming, ensures constant-size per-level tokens and eliminates wide counters, yielding the additive tradeoff $S(b)=O(b + \log(t/b))$ for block sizes $b \ge b_0$ with $b_0 = \Theta(\log t)$, which at the canonical choice $b = \Theta(\sqrt{t})$ gives $O(\sqrt{t})$ space; the $b_0$ threshold rules out degenerate blocks where addressing scratch would dominate the window footprint. The construction is uniform, relativizes, and is robust to standard model choices. Consequences include branching-program upper bounds $2^{O(\sqrt{s})}$ for size-$s$ bounded-fan-in circuits, tightened quadratic-time lower bounds for $\SPACE[n]$-complete problems via the standard hierarchy argument, and $O(\sqrt{t})$-space certifying interpreters; under explicit locality assumptions, the framework extends to geometric $d$-dimensional models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14831v1</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Logan Nye</dc:creator>
    </item>
    <item>
      <title>The Kikuchi Hierarchy and Tensor PCA</title>
      <link>https://arxiv.org/abs/1904.03858</link>
      <description>arXiv:1904.03858v3 Announce Type: replace 
Abstract: For the tensor PCA (principal component analysis) problem, we propose a new hierarchy of increasingly powerful algorithms with increasing runtime. Our hierarchy is analogous to the sum-of-squares (SOS) hierarchy but is instead inspired by statistical physics and related algorithms such as belief propagation and AMP (approximate message passing). Our level-$\ell$ algorithm can be thought of as a linearized message-passing algorithm that keeps track of $\ell$-wise dependencies among the hidden variables. Specifically, our algorithms are spectral methods based on the Kikuchi Hessian, which generalizes the well-studied Bethe Hessian to the higher-order Kikuchi free energies.
  It is known that AMP, the flagship algorithm of statistical physics, has substantially worse performance than SOS for tensor PCA. In this work we 'redeem' the statistical physics approach by showing that our hierarchy gives a polynomial-time algorithm matching the performance of SOS. Our hierarchy also yields a continuum of subexponential-time algorithms, and we prove that these achieve the same (conjecturally optimal) tradeoff between runtime and statistical power as SOS. Our proofs are much simpler than prior work, and also apply to the related problem of refuting random $k$-XOR formulas. The results we present here apply to tensor PCA for tensors of all orders, and to $k$-XOR when $k$ is even.
  Our methods suggest a new avenue for systematically obtaining optimal algorithms for Bayesian inference problems, and our results constitute a step toward unifying the statistical physics and sum-of-squares approaches to algorithm design.</description>
      <guid isPermaLink="false">oai:arXiv.org:1904.03858v3</guid>
      <category>cs.DS</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander S. Wein, Ahmed El Alaoui, Cristopher Moore</dc:creator>
    </item>
    <item>
      <title>First Order Logic on Pathwidth Revisited Again</title>
      <link>https://arxiv.org/abs/2210.09899</link>
      <description>arXiv:2210.09899v2 Announce Type: replace 
Abstract: Courcelle's celebrated theorem states that all MSO-expressible properties can be decided in linear time on graphs of bounded treewidth. Unfortunately, the hidden constant implied by this theorem is a tower of exponentials whose height increases with each quantifier alternation in the formula. More devastatingly, this cannot be improved, under standard assumptions, even if we consider the much more restricted problem of deciding FO-expressible properties on trees.
  In this paper we revisit this well-studied topic and identify a natural special case where the dependence of Courcelle's theorem can, in fact, be improved. Specifically, we show that all FO-expressible properties can be decided with an elementary dependence on the input formula, if the input graph has bounded pathwidth (rather than treewidth). This is a rare example of treewidth and pathwidth having different complexity behaviors. Our result is also in sharp contrast with MSO logic on graphs of bounded pathwidth, where it is known that the dependence has to be non-elementary, under standard assumptions. Our work builds upon, and generalizes, a corresponding meta-theorem by Gajarsk{\'{y}} and Hlin{\v{e}}n{\'{y}} for the more restricted class of graphs of bounded tree-depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.09899v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lampis</dc:creator>
    </item>
    <item>
      <title>Coupling without Communication and Drafter-Invariant Speculative Decoding</title>
      <link>https://arxiv.org/abs/2408.07978</link>
      <description>arXiv:2408.07978v4 Announce Type: replace 
Abstract: Suppose Alice has a distribution $P$ and Bob has a distribution $Q$. Alice wants to draw a sample $a\sim P$ and Bob a sample $b \sim Q$ such that $a = b$ with as high of probability as possible. It is well-known that, by sampling from an optimal coupling between the distributions, Alice and Bob can achieve $\Pr[a = b] = 1 - D_{TV}(P,Q)$, where $D_{TV}(P,Q)$ is the total variation distance between $P$ and $Q$. What if Alice and Bob must solve this same problem \emph{without communicating at all?} Perhaps surprisingly, with access to public randomness, they can still achieve $\Pr[a = b] \geq \frac{1 - D_{TV}(P,Q)}{1 + D_{TV}(P,Q)} \geq 1-2D_{TV}(P,Q)$ using a simple protocol based on the Weighted MinHash algorithm. This bound was shown to be optimal in the worst-case by [Bavarian et al., 2020]. In this work, we revisit the communication-free coupling problem. We provide a simpler proof of the optimality result from [Bavarian et al., 2020]. We show that, while the worst-case success probability of Weighted MinHash cannot be improved, an equally simple protocol based on Gumbel sampling offers a Pareto improvement: for every pair of distributions $P, Q$, Gumbel sampling achieves an equal or higher value of $\Pr[a = b]$ than Weighted MinHash. Importantly, this improvement translates to practice. We demonstrate an application of communication-free coupling to \emph{speculative decoding}, a recent method for accelerating autoregressive large language models [Leviathan, Kalman, Matias, ICML 2023]. We show that communication-free protocols can be used to contruct \emph{\CSD{}} schemes, which have the desirable property that their output is fixed given a fixed random seed, regardless of what drafter is used for speculation. In experiments on a language generation task, Gumbel sampling outperforms Weighted MinHash. Code is available at https://github.com/majid-daliri/DISD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07978v4</guid>
      <category>cs.DS</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Majid Daliri, Christopher Musco, Ananda Theertha Suresh</dc:creator>
    </item>
    <item>
      <title>Balanced connected partitions of edge-weighted graphs: Hardness and solving methods</title>
      <link>https://arxiv.org/abs/2504.02421</link>
      <description>arXiv:2504.02421v2 Announce Type: replace 
Abstract: The balanced connected $k$-partition problem (\textsc{bcp}) is a classic problem, which consists in partitioning the set of vertices of a vertex-weighted connected graph into a collection of~$k$ classes such that each class induces a connected subgraph of \emph{roughly} the same weight. In this study, we investigate edge-weighted variants of $\textsc{bcp}$, where we are given a connected graph $G$, $k \in \Z_\ge$, and an edge-weight function $w \colon E(G)\to\Q_\ge$, and the goal is to compute a spanning $k$-forest~$\mathcal{T}$ of $G$ (i.e., a forest with exactly $k$ trees) that minimizes the weight of the heaviest tree in~$\mathcal{T}$ in the min-max version, or maximizes the weight of the lightest tree in~$\mathcal{T}$ in the max-min version. We show that both versions of this problem are $\NP$-hard on complete graphs with $k=2$, unweighted split graphs, and unweighted bipartite graphs with $k\geq 2$ fixed. Moreover, we prove that these problems do not admit subexponential-time algorithms, unless the Exponential-Time Hypothesis fails. We focus on the min-max version and devise a tight $k$-approximation algorithm, compact and non-compact integer linear programming formulations, branch and cut, and branch and price algorithms. Finally, we present the outcomes of an experimental study on the performances of different solution methods. The source code of the complete implementation of the proposed algorithms is also available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02421v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Morteza Davari, Phablo F. S. Moura, Hande Yaman</dc:creator>
    </item>
    <item>
      <title>Minimum Stable Cut and Treewidth</title>
      <link>https://arxiv.org/abs/2104.13097</link>
      <description>arXiv:2104.13097v3 Announce Type: replace-cross 
Abstract: A stable or locally-optimal cut of a graph is a cut whose weight cannot be increased by changing the side of a single vertex. In this paper we study Minimum Stable Cut, the problem of finding a stable cut of minimum weight. Since this problem is NP-hard, we study its complexity on graphs of low treewidth, low degree, or both. We begin by showing that the problem remains weakly NP-hard on severely restricted trees, so bounding treewidth alone cannot make it tractable. We match this hardness with a pseudo-polynomial DP algorithm solving the problem in time $(\Delta\cdot W)^{O(tw)}n^{O(1)}$, where $tw$ is the treewidth, $\Delta$ the maximum degree, and $W$ the maximum weight. On the other hand, bounding $\Delta$ is also not enough, as the problem is NP-hard for unweighted graphs of bounded degree. We therefore parameterize Minimum Stable Cut by both $tw$ and $\Delta$ and obtain an FPT algorithm running in time $2^{O(\Delta tw)}(n+\log W)^{O(1)}$. Our main result for the weighted problem is to provide a reduction showing that both aforementioned algorithms are essentially optimal, even if we replace treewidth by pathwidth: if there exists an algorithm running in $(nW)^{o(pw)}$ or $2^{o(\Delta pw)}(n+\log W)^{O(1)}$, then the ETH is false. Complementing this, we show that we can, however, obtain an FPT approximation scheme parameterized by treewidth, if we consider almost-stable solutions, that is, solutions where no single vertex can unilaterally increase the weight of its incident cut edges by more than a factor of $(1+\varepsilon)$.
  Motivated by these mostly negative results, we consider Unweighted Minimum Stable Cut. Here our results already imply a much faster exact algorithm running in time $\Delta^{O(tw)}n^{O(1)}$. We show that this is also probably essentially optimal: an algorithm running in $n^{o(pw)}$ would contradict the ETH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.13097v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lampis</dc:creator>
    </item>
    <item>
      <title>Exact threshold for approximate ellipsoid fitting of random points</title>
      <link>https://arxiv.org/abs/2310.05787</link>
      <description>arXiv:2310.05787v2 Announce Type: replace-cross 
Abstract: We consider the problem $(\rm P)$ of exactly fitting an ellipsoid (centered at $0$) to $n$ standard Gaussian random vectors in $\mathbb{R}^d$, as $n, d \to \infty$ with $n / d^2 \to \alpha &gt; 0$. This problem is conjectured to undergo a sharp transition: with high probability, $(\rm P)$ has a solution if $\alpha &lt; 1/4$, while $(\rm P)$ has no solutions if $\alpha &gt; 1/4$. So far, only a trivial bound $\alpha &gt; 1/2$ is known to imply the absence of solutions, while the sharpest results on the positive side assume $\alpha \leq \eta$ (for $\eta &gt; 0$ a small constant) to prove that $(\rm P)$ is solvable. In this work we show a universality property for the minimal fitting error achievable by ellipsoids: we show that, to leading order, it coincides with the minimal error in a so-called "Gaussian equivalent" problem, for which the satisfiability transition can be rigorously analyzed. Our main results follow from this finding, and they are twofold. On the positive side, we prove that if $\alpha &lt; 1/4$, there exists an ellipsoid fitting all the points up to a small error, and that the lengths of its principal axes are bounded above and below. On the other hand, for $\alpha &gt; 1/4$, we show that achieving small fitting error is not possible if the length of the ellipsoid's shortest axis does not approach $0$ as $d \to \infty$ (and in particular there does not exist any ellipsoid fit whose shortest axis length is bounded away from $0$ as $d \to \infty$). To the best of our knowledge, our work is the first rigorous result characterizing the expected phase transition in ellipsoid fitting at $\alpha = 1/4$. In a companion non-rigorous work, the second author and D. Kunisky give a general analysis of ellipsoid fitting using the replica method of statistical physics, which inspired the present work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05787v2</guid>
      <category>math.PR</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/25-EJP1378</arxiv:DOI>
      <arxiv:journal_reference>Electron. J. Probab. 30: 1-46 (2025)</arxiv:journal_reference>
      <dc:creator>Afonso S. Bandeira, Antoine Maillard</dc:creator>
    </item>
  </channel>
</rss>
