<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Jun 2025 01:36:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Near-real-time ship grounding damage assessment using Bayesian networks</title>
      <link>https://arxiv.org/abs/2506.06493</link>
      <description>arXiv:2506.06493v1 Announce Type: new 
Abstract: In a post-grounding event, the rapid assessment of hull girder residual strength is crucial for making informed decisions, such as determining whether the vessel can safely reach the closest yard. One of the primary challenges in this assessment is the uncertainty in the estimation of the extent of structural damage. Although classification societies have developed rapid response damage assessment tools, primarily relying on 2D Smith-based models, these tools are based on deterministic methods and conservative estimates of damage extent. To enhance this assessment, we propose a probabilistic framework for rapid grounding damage assessment of ship structures using Bayesian networks (BNs). The proposed BN model integrates multiple information sources, including underwater inspection results, hydrostatic and bathymetric data, crashworthiness models, and hydraulic models for flooding and oil spill monitoring. By systematically incorporating these parameters and their associated uncertainties within a causal framework, the BN allows for dynamic updates as new evidence emerges during an incident. Two case studies demonstrate the effectiveness of this methodology, highlighting its potential as a practical decision support tool to improve operational safety during grounding events. The results indicate that combining models with on-site observations can even replace costly underwater inspections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06493v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dimitris G. Georgiadis, Manolis S. Samuelides, Daniel Straub</dc:creator>
    </item>
    <item>
      <title>Comparing methods for handling missing data in electronic health records for dynamic risk prediction of central-line associated bloodstream infection</title>
      <link>https://arxiv.org/abs/2506.06707</link>
      <description>arXiv:2506.06707v1 Announce Type: new 
Abstract: Electronic health records (EHR) often contain varying levels of missing data. This study compared different imputation strategies to identify the most suitable approach for predicting central line-associated bloodstream infection (CLABSI) in the presence of competing risks using EHR data. We analyzed 30862 catheter episodes at University Hospitals Leuven (2012-2013) to predict 7-day CLABSI risk using a landmark cause-specific supermodel, accounting for competing risks of hospital discharge and death. Imputation methods included simple methods (median/mode, last observation carried forward), multiple imputation, regression-based and mixed-effects models leveraging longitudinal structure, and random forest imputation to capture interactions and non-linearities. Missing indicators were also assessed alone and in combination with other imputation methods. Model performance was evaluated dynamically at daily landmarks up to 14 days post-catheter placement. The missing indicator approach showed the highest discriminative ability, achieving a mean AUROC of up to 0.782 and superior overall performance based on the scaled Brier score. Combining missing indicators with other methods slightly improved performance, with the mixed model approach combined with missing indicators achieving the highest AUROC (0.783) at day 4, and the missForestPredict approach combined with missing indicators yielding the best scaled Brier scores at earlier landmarks. This suggests that in EHR data, the presence or absence of information may hold valuable insights for patient risk prediction. However, the use of missing indicators requires caution, as shifts in EHR data over time can alter missing data patterns, potentially impacting model transportability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06707v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shan Gao, Elena Albu, Pieter Stijnen, Frank Rademakers, Veerle Cossey, Yves Debaveye, Christel Janssens, Ben Van Calster, Laure Wynants</dc:creator>
    </item>
    <item>
      <title>Advancing Waterfall Plots for Cancer Treatment Response Assessment through Adjustment of Incomplete Follow-Up Time</title>
      <link>https://arxiv.org/abs/2506.07365</link>
      <description>arXiv:2506.07365v1 Announce Type: new 
Abstract: Waterfall plots are a key tool in early phase oncology clinical studies for visualizing individual patients' tumor size changes and provide efficacy assessment. However, comparing waterfall plots from ongoing studies with limited follow-up to those from completed studies with long follow-up is challenging due to underestimation of tumor response in ongoing patients. To address this, we propose a novel adjustment method that projects the waterfall plot of an ongoing study to approximate its appearance with sufficient follow-up. Recognizing that waterfall plots are simply rotated survival functions of best tumor size reduction from the baseline (in percentage), we frame the problem in a survival analysis context and adjust weight of each ongoing patients in an interim look Kaplan-Meier curve by leveraging the probability of potential tumor response improvement (i.e., "censoring"). The probability of improvement is quantified through an incomplete multinomial model to estimate the best tumor size change occurrence at each scan time. The adjusted waterfall plots of experimental treatments from ongoing studies are suitable for comparison with historical controls from completed studies, without requiring individual-level data of those controls. A real-data example demonstrates the utility of this method for robust efficacy evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07365v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Zhe (April),  Wang, Linda Z. Sun, Cong Chen</dc:creator>
    </item>
    <item>
      <title>Modelling Nonstationary Time Series using Trend-Stationary Hypothesis</title>
      <link>https://arxiv.org/abs/2506.07987</link>
      <description>arXiv:2506.07987v1 Announce Type: new 
Abstract: This paper challenges the prevalence of unit root models by introducing the Linear Trend-Stationary Trigonometric ARMA (LTSTA), a novel framework for modelling nonstationary time series under the trend-stationary hypothesis. LTSTA decomposes series into three components: (1) a deterministic trend (modelled via continuous piecewise linear functions with structural breaks), (2) a Fourier-based deterministic seasonality component, and (3) a stochastic ARMA error term. We propose a heuristic approach to determine the optimal number of structural breaks, with parameter estimation performed through an iterative scheme that integrates a modified dynamic programming algorithm for break detection and a standard regression procedure with ARMA errors. The model's performance is evaluated through a case study on US Real GDP (2002-2025), where it accurately identifies breaks corresponding to major economic events (e.g., the 2008 financial crisis and COVID-19 shocks). Additionally, LTSTA outperforms well-established univariate statistical models (SES, Theta, TBATS, ETS, ARIMA, and Prophet) on the CIF 2016 forecasting competition dataset across MAE, RMSE, sMAPE, and MASE metrics. The LTSTA model provides an interpretable alternative to unit root approaches, particularly suited for time series with predominant deterministic properties where structural break detection is critical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07987v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhandos Abdikhadir</dc:creator>
    </item>
    <item>
      <title>When Tukey meets Chauvenet: a new boxplot criterion for outlier detection</title>
      <link>https://arxiv.org/abs/2506.06491</link>
      <description>arXiv:2506.06491v1 Announce Type: cross 
Abstract: The box-and-whisker plot, introduced by Tukey (1977), is one of the most popular graphical methods in descriptive statistics. On the other hand, however, Tukey's boxplot is free of sample size, yielding the so-called "one-size-fits-all" fences for outlier detection. Although improvements on the sample size adjusted boxplots do exist in the literature, most of them are either not easy to implement or lack justification. As another common rule for outlier detection, Chauvenet's criterion uses the sample mean and standard derivation to perform the test, but it is often sensitive to the included outliers and hence is not robust. In this paper, by combining Tukey's boxplot and Chauvenet's criterion, we introduce a new boxplot, namely the Chauvenet-type boxplot, with the fence coefficient determined by an exact control of the outside rate per observation. Our new outlier criterion not only maintains the simplicity of the boxplot from a practical perspective, but also serves as a robust Chauvenet's criterion. Simulation study and a real data analysis on the civil service pay adjustment in Hong Kong demonstrate that the Chauvenet-type boxplot performs extremely well regardless of the sample size, and can therefore be highly recommended for practical use to replace both Tukey's boxplot and Chauvenet's criterion. Lastly, to increase the visibility of the work, a user-friendly R package named `ChauBoxplot' has also been officially released on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06491v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Computational and Graphical Statistics, 2025</arxiv:journal_reference>
      <dc:creator>Hongmei Lin, Riquan Zhang, Tiejun Tong</dc:creator>
    </item>
    <item>
      <title>Drift Optimization of Regulated Stochastic Models Using Sample Average Approximation</title>
      <link>https://arxiv.org/abs/2506.06723</link>
      <description>arXiv:2506.06723v1 Announce Type: cross 
Abstract: This paper introduces a drift optimization model of stochastic optimization problems driven by regulated stochastic processes. A broad range of problems across operations research, machine learning, and statistics can be viewed as optimizing the "drift" associated with a process by minimizing a cost functional, while respecting path constraints imposed by a Lipschitz continuous regulator. Towards an implementable solution to such infinite-dimensional problems, we develop the fundamentals of a Sample Average Approximation (SAA) method that incorporates (i) path discretization, (ii) function-space discretization, and (iii) Monte Carlo sampling, and that is solved using an optimization recursion such as mirror descent. We start by constructing pathwise directional derivatives for use within the SAA method, followed by consistency and complexity calculations. The characterized complexity is expressed as a function of the number of optimization steps, and the computational effort involved in (i)--(iii), leading to guidance on how to trade-off the computational effort allocated to optimization steps versus the "dimension reduction" steps in (i)--(iii).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06723v1</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihe Zhou, Harsha Honnappa, Raghu Pasupathy</dc:creator>
    </item>
    <item>
      <title>The Currents of Conflict: Decomposing Conflict Trends with Gaussian Processes</title>
      <link>https://arxiv.org/abs/2506.06828</link>
      <description>arXiv:2506.06828v1 Announce Type: cross 
Abstract: I present a novel approach to estimating the temporal and spatial patterns of violent conflict. I show how we can use highly temporally and spatially disaggregated data on conflict events in tandem with Gaussian processes to estimate temporospatial conflict trends. These trends can be studied to gain insight into conflict traps, diffusion and tempo-spatial conflict exposure in general; they can also be used to control for such phenomenons given other estimation tasks; lastly, the approach allow us to extrapolate the estimated tempo-spatial conflict patterns into future temporal units, thus facilitating powerful, stat-of-the-art, conflict forecasts. Importantly, these results are achieved via a relatively parsimonious framework using only one data source: past conflict patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06828v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon P. von der Maase</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework for Model Selection in LSTM Networks</title>
      <link>https://arxiv.org/abs/2506.06840</link>
      <description>arXiv:2506.06840v1 Announce Type: cross 
Abstract: Long Short-Term Memory (LSTM) neural network models have become the cornerstone for sequential data modeling in numerous applications, ranging from natural language processing to time series forecasting. Despite their success, the problem of model selection, including hyperparameter tuning, architecture specification, and regularization choice remains largely heuristic and computationally expensive. In this paper, we propose a unified statistical framework for systematic model selection in LSTM networks. Our framework extends classical model selection ideas, such as information criteria and shrinkage estimation, to sequential neural networks. We define penalized likelihoods adapted to temporal structures, propose a generalized threshold approach for hidden state dynamics, and provide efficient estimation strategies using variational Bayes and approximate marginal likelihood methods. Several biomedical data centric examples demonstrate the flexibility and improved performance of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06840v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fahad Mostafa</dc:creator>
    </item>
    <item>
      <title>Efficient and Robust Block Designs for Order-of-Addition Experiments</title>
      <link>https://arxiv.org/abs/2506.07096</link>
      <description>arXiv:2506.07096v1 Announce Type: cross 
Abstract: Designs for Order-of-Addition (OofA) experiments have received growing attention due to their impact on responses based on the sequence of component addition. In certain cases, these experiments involve heterogeneous groups of units, which necessitates the use of blocking to manage variation effects. Despite this, the exploration of block OofA designs remains limited in the literature. As experiments become increasingly complex, addressing this gap is essential to ensure that the designs accurately reflect the effects of the addition sequence and effectively handle the associated variability. Motivated by this, this paper seeks to address the gap by expanding the indicator function framework for block OofA designs. We propose the use of the word length pattern as a criterion for selecting robust block OofA designs. To improve search efficiency and reduce computational demands, we develop algorithms that employ orthogonal Latin squares for design construction and selection, minimizing the need for exhaustive searches. Our analysis, supported by correlation plots, reveals that the algorithms effectively manage confounding and aliasing between effects. Additionally, simulation studies indicate that designs based on our proposed criterion and algorithms achieve power and type I error rates comparable to those of full block OofA designs. This approach offers a practical and efficient method for constructing block OofA designs and may provide valuable insights for future research and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07096v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chang-Yun Lin</dc:creator>
    </item>
    <item>
      <title>Analyzing Breast Cancer Survival Disparities by Race and Demographic Location: A Survival Analysis Approach</title>
      <link>https://arxiv.org/abs/2506.07191</link>
      <description>arXiv:2506.07191v1 Announce Type: cross 
Abstract: This study employs a robust analytical framework to uncover patterns in survival outcomes among breast cancer patients from diverse racial and geographical backgrounds. This research uses the SEER 2021 dataset to analyze breast cancer survival outcomes to identify and comprehend dissimilarities. Our approach integrates exploratory data analysis (EDA), through this we identify key variables that influence survival rates and employ survival analysis techniques, including the Kaplan-Meier estimator and log-rank test and the advanced modeling Cox Proportional Hazards model to determine how survival rates vary across racial groups and countries. Model validation and interpretation are undertaken to ensure the reliability of our findings, which are documented comprehensively to inform policymakers and healthcare professionals. The outcome of this paper is a detailed version of statistical analysis that not just highlights disparities in breast cancer treatment and care but also serves as a foundational tool for developing targeted interventions to address the inequalities effectively. Through this research, our aim is to contribute to the global efforts to improve breast cancer outcomes and reduce treatment disparities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07191v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramisa Farha, Joshua O. Olukoya</dc:creator>
    </item>
    <item>
      <title>Impact of Label Noise from Large Language Models Generated Annotations on Evaluation of Diagnostic Model Performance</title>
      <link>https://arxiv.org/abs/2506.07273</link>
      <description>arXiv:2506.07273v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to generate labels from radiology reports to enable large-scale AI evaluation. However, label noise from LLMs can introduce bias into performance estimates, especially under varying disease prevalence and model quality. This study quantifies how LLM labeling errors impact downstream diagnostic model evaluation. We developed a simulation framework to assess how LLM label errors affect observed model performance. A synthetic dataset of 10,000 cases was generated across different prevalence levels. LLM sensitivity and specificity were varied independently between 90% and 100%. We simulated diagnostic models with true sensitivity and specificity ranging from 90% to 100%. Observed performance was computed using LLM-generated labels as the reference. We derived analytical performance bounds and ran 5,000 Monte Carlo trials per condition to estimate empirical uncertainty. Observed performance was highly sensitive to LLM label quality, with bias strongly influenced by disease prevalence. In low-prevalence settings, small reductions in LLM specificity led to substantial underestimation of sensitivity. For example, at 10% prevalence, an LLM with 95% specificity yielded an observed sensitivity of ~53% despite a perfect model. In high-prevalence scenarios, reduced LLM sensitivity caused underestimation of model specificity. Monte Carlo simulations consistently revealed downward bias, with observed performance often falling below true values even when within theoretical bounds. LLM-generated labels can introduce systematic, prevalence-dependent bias into model evaluation. Specificity is more critical in low-prevalence tasks, while sensitivity dominates in high-prevalence settings. These findings highlight the importance of prevalence-aware prompt design and error characterization when using LLMs for post-deployment model assessment in clinical AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07273v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammadreza Chavoshi, Hari Trivedi, Janice Newsome, Aawez Mansuri, Chiratidzo Rudado Sanyika, Rohan Satya Isaac, Frank Li, Theo Dapamede, Judy Gichoya</dc:creator>
    </item>
    <item>
      <title>Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models</title>
      <link>https://arxiv.org/abs/2506.07275</link>
      <description>arXiv:2506.07275v1 Announce Type: cross 
Abstract: Machine learning approaches, such as contextual multi-armed bandit (cMAB) algorithms, offer a promising strategy to reduce sedentary behavior by delivering personalized interventions to encourage physical activity. However, cMAB algorithms typically require large participant samples to learn effectively and may overlook key psychological factors that are not explicitly encoded in the model. In this study, we propose a hybrid approach that combines cMAB for selecting intervention types with large language models (LLMs) to personalize message content. We evaluate four intervention types: behavioral self-monitoring, gain-framed, loss-framed, and social comparison, each delivered as a motivational message aimed at increasing motivation for physical activity and daily step count. Message content is further personalized using dynamic contextual factors including daily fluctuations in self-efficacy, social influence, and regulatory focus. Over a seven-day trial, participants receive daily messages assigned by one of four models: cMAB alone, LLM alone, combined cMAB with LLM personalization (cMABxLLM), or equal randomization (RCT). Outcomes include daily step count and message acceptance, assessed via ecological momentary assessments (EMAs). We apply a causal inference framework to evaluate the effects of each model. Our findings offer new insights into the complementary roles of LLM-based personalization and cMAB adaptation in promoting physical activity through personalized behavioral messaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07275v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haochen Song, Dominik Hofer, Rania Islambouli, Laura Hawkins, Ananya Bhattacharjee, Meredith Franklin, Joseph Jay Williams</dc:creator>
    </item>
    <item>
      <title>Capability demonstration of a JEDI-based system for TEMPO assimilation: system description and evaluation</title>
      <link>https://arxiv.org/abs/2506.07321</link>
      <description>arXiv:2506.07321v1 Announce Type: cross 
Abstract: The launch of the Tropospheric Emissions: Monitoring of Pollution (TEMPO) mission in 2023 marked a new era in air quality monitoring by providing high-frequency, geostationary observations of column NO2 across North America. In this study, we present the first implementation of a TEMPO NO2 data assimilation system using the Joint Effort for Data assimilation Integration (JEDI) framework. Leveraging a four-dimensional ensemble variational (4DEnVar) approach and an Ensemble of Data Assimilations (EDA), we demonstrate a novel capability to assimilate hourly NO2 retrievals from TEMPO alongside polar-orbiting TROPOMI data into NASA's GEOS Composition Forecast (GEOS-CF) model. The system is evaluated over the CONUS region for August 2023, using a suite of independent measurements including Pandora spectrometers, AirNow surface stations, and aircraft-based observations from AEROMMA and STAQS field campaigns. Results show that the assimilation system successfully integrates geostationary NO2 observations, improves model performance in the column, and captures diurnal variability. However, assimilation also leads to systematic reductions in surface NO2 levels, improving agreement with some datasets (e.g., Pandora, AEROMMA) but degrading comparisons with others (e.g., AirNow). These findings highlight the importance of joint evaluation across platforms and motivate further development of dual-concentration emission assimilation schemes. While the system imposes high computational costs, primarily from the forecast model, ongoing efforts to integrate AI-based model emulators offer a promising path toward scalable, real-time assimilation of geostationary atmospheric composition data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07321v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maryam Abdi-Oskouei, J\'er\^ome Barr\'e</dc:creator>
    </item>
    <item>
      <title>Scalable Spatiotemporal Modeling for Bicycle Count Prediction</title>
      <link>https://arxiv.org/abs/2506.07582</link>
      <description>arXiv:2506.07582v1 Announce Type: cross 
Abstract: We propose a novel sparse spatiotemporal dynamic generalized linear model for efficient inference and prediction of bicycle count data. Assuming Poisson distributed counts with spacetime-varying rates, we model the log-rate using spatiotemporal intercepts, dynamic temporal covariates, and site-specific effects additively. Spatiotemporal dependence is modeled using a spacetime-varying intercept that evolves smoothly over time with spatially correlated errors, and coefficients of some temporal covariates including seasonal harmonics also evolve dynamically over time. Inference is performed following the Bayesian paradigm, and uncertainty quantification is naturally accounted for when predicting bicycle counts for unobserved locations and future times of interest. To address the challenges of high-dimensional inference of spatiotemporal data in a Bayesian setting, we develop a customized hybrid Markov Chain Monte Carlo (MCMC) algorithm. To address the computational burden of dense covariance matrices, we extend our framework to high-dimensional spatial settings using the sparse SPDE approach of Lindgren et al. (2011), demonstrating its accuracy and scalability on both synthetic data and Montreal Island bicycle datasets. The proposed approach naturally provides missing value imputations, kriging, future forecasting, spatiotemporal predictions, and inference of model components. Moreover, it provides ways to predict average annual daily bicycles (AADB), a key metric often sought when designing bicycle networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07582v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishikesh Yadav, Alexandra M. Schmidt, Aurelie Labbe, Pratheepa Jeganathan, Luis F. Miranda-Moreno</dc:creator>
    </item>
    <item>
      <title>Mediation Analysis for Sparse and Irregularly Spaced Longitudinal Outcomes with Application to the MrOS Sleep Study</title>
      <link>https://arxiv.org/abs/2506.07953</link>
      <description>arXiv:2506.07953v1 Announce Type: cross 
Abstract: Mediation analysis has become a widely used method for identifying the pathways through which an independent variable influences a dependent variable via intermediate mediators. However, limited research addresses the case where mediators are high-dimensional and the outcome is represented by sparse, irregularly spaced longitudinal data. To address these challenges, we propose a mediation analysis approach for scalar exposures, high-dimensional mediators, and sparse longitudinal outcomes. This approach effectively identifies significant mediators by addressing two key issues: (i) the underlying correlation structure within the sparse and irregular cognitive measurements, and (ii) adjusting mediation effects to handle the high-dimensional set of candidate mediators. In the MrOS Sleep study, our primary objective is to explore lipid pathways that may mediate the relationship between rest-activity rhythms and longitudinal cognitive decline in older men. Our findings suggest a potential mechanism involving rest-activity rhythms, lipid metabolites, and cognitive decline, and highlight significant mediators identified through multiple testing procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07953v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Ren, Haoyi Yang, Qian Xiao, Lingzhou Xue, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>Scalable Inference for Bayesian Multinomial Logistic-Normal Dynamic Linear Models</title>
      <link>https://arxiv.org/abs/2410.05548</link>
      <description>arXiv:2410.05548v2 Announce Type: replace 
Abstract: Many scientific fields collect longitudinal count compositional data. Each observation is a multivariate count vector, where the total counts are arbitrary, and the information lies in the relative frequency of the counts. Multiple authors have proposed Bayesian Multinomial Logistic-Normal Dynamic Linear Models (MLN-DLMs) as a flexible approach to modeling these data. However, adoption of these methods has been limited by computational challenges. This article develops an efficient and accurate approach to posterior state estimation, called $\textit{Fenrir}$. Our approach relies on a novel algorithm for MAP estimation and an accurate approximation to a key posterior marginal of the model. As there are no equivalent methods against which we can compare, we also develop an optimized Stan implementation of MLN-DLMs. Our experiments suggest that Fenrir can be three orders of magnitude more efficient than Stan and can even be incorporated into larger sampling schemes for joint inference of model hyperparameters. Our methods are made available to the community as a user-friendly software library written in C++ with an R interface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05548v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manan Saxena, Tinghua Chen, Justin D. Silverman</dc:creator>
    </item>
    <item>
      <title>VISTA-SSM: Varying and Irregular Sampling Time-series Analysis via State Space Models</title>
      <link>https://arxiv.org/abs/2410.21527</link>
      <description>arXiv:2410.21527v2 Announce Type: replace 
Abstract: We introduce VISTA, a clustering approach for multivariate and irregularly sampled time series based on a parametric state space mixture model. VISTA is specifically designed for the unsupervised identification of groups in datasets originating from healthcare and psychology where such sampling issues are commonplace. Our approach adapts linear Gaussian state space models (LGSSMs) to provide a flexible parametric framework for fitting a wide range of time series dynamics. The clustering approach itself is based on the assumption that the population can be represented as a mixture of a fixed number of LGSSMs. VISTA's model formulation allows for an explicit derivation of the log-likelihood function, from which we develop an expectation-maximization scheme for fitting model parameters to the observed data samples. Our algorithmic implementation is designed to handle populations of multivariate time series that can exhibit large changes in sampling rate as well as irregular sampling. We evaluate the versatility and accuracy of our approach on simulated and real-world datasets, including demographic trends, wearable sensor data, epidemiological time series, and ecological momentary assessments. Our results indicate that VISTA outperforms most comparable standard times series clustering methods. We provide an open-source implementation of VISTA in Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21527v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Brindle, Thomas Derrick Hull, Matteo Malgaroli, Nicolas Charon</dc:creator>
    </item>
    <item>
      <title>Hierarchical models for small area estimation using zero-inflated forest inventory variables: comparison and implementation</title>
      <link>https://arxiv.org/abs/2503.22103</link>
      <description>arXiv:2503.22103v2 Announce Type: replace 
Abstract: National Forest Inventory (NFI) data are typically limited to sparse networks of sample locations due to cost constraints. While traditional design-based estimators provide reliable forest parameter estimates for large areas, there is increasing interest in model-based small area estimation (SAE) methods to improve precision for smaller spatial, temporal, or biophysical domains. SAE methods can be broadly categorized into area- and unit-level models, with unit-level models offering greater flexibility -- making them the focus of this study. Ensuring valid inference requires satisfying model distributional assumptions, which is particularly challenging for NFI variables that exhibit positive support and zero inflation, such as forest biomass, carbon, and volume. Here, we evaluate a class of two-stage unit-level hierarchical Bayesian models for estimating forest biomass at the county-level in Washington and Nevada, United States. We compare these models to simpler Bayesian single-stage and two-stage frequentist approaches. To assess estimator performance, we employ simulated populations and cross-validation techniques. Results indicate that small area estimators that incorporate a two-stage approach to account for zero inflation, county-specific random intercepts and residual variances, and spatial random effects provide the most reliable county-level estimates. We illustrate the usefulness of simulated populations and cross-validation for assessing qualities of the various estimators considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22103v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grayson W. White, Andrew O. Finley, Josh K. Yamamoto, Jennifer L. Green, Tracey S. Frescino, David. W. MacFarlane, Hans-Erik Andersen</dc:creator>
    </item>
    <item>
      <title>Mechanistic models for panel data: Analysis of ecological experiments with four interacting species</title>
      <link>https://arxiv.org/abs/2506.04508</link>
      <description>arXiv:2506.04508v2 Announce Type: replace 
Abstract: In an ecological context, panel data arise when time series measurements are made on a collection of ecological processes. Each process may correspond to a spatial location for field data, or to an experimental ecosystem in a designed experiment. Statistical models for ecological panel data should capture the high levels of nonlinearity, stochasticity, and measurement uncertainty inherent in ecological systems. Furthermore, the system dynamics may depend on unobservable variables. This study applies iterated particle filtering techniques to explore new possibilities for likelihood-based statistical analysis of these complex systems. We analyze data from a mesocosm experiment in which two species of the freshwater planktonic crustacean genus, Daphnia, coexist with an alga and a fungal parasite. Time series data were collected on replicated mesocosms under six treatment conditions. Iterated filtering enables maximization of the likelihood for scientifically motivated nonlinear partially observed Markov process models, providing access to standard likelihood-based methods for parameter estimation, confidence intervals, hypothesis testing, model selection and diagnostics. This toolbox allows scientists to propose and evaluate scientifically motivated stochastic dynamic models for panel data, constrained only by the requirement to write code to simulate from the model and to specify a measurement distribution describing how the system state is observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04508v2</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Yang, Jesse Wheeler, Meghan A. Duffy, Aaron A. King, Edward L. Ionides</dc:creator>
    </item>
    <item>
      <title>Integrating Project Spatial Coordinates into Pavement Management Prioritization</title>
      <link>https://arxiv.org/abs/1811.03437</link>
      <description>arXiv:1811.03437v2 Announce Type: replace-cross 
Abstract: To date, pavement management software products and studies on optimizing the prioritization of pavement maintenance and rehabilitation (M&amp;R) have been mainly focused on three parameters; the pre-treatment pavement condition, the rehabilitation cost, and the available budget. Yet, the role of the candidate projects' spatial characteristics in the decision-making process has not been deeply considered. Such a limitation, predominately, allows the recommended M&amp;R projects' schedule to involve simultaneously running but spatially scattered construction sites, which are very challenging to monitor and manage. This study introduces a novel approach to integrate pavement segments' spatial coordinates into the M&amp;R prioritization analysis. The introduced approach aims at combining the pavement segments with converged spatial coordinates to be repaired in the same timeframe without compromising the allocated budget levels or the overall target Pavement Condition Index (PCI). Such a combination would result in minimizing the routing of crews, materials and other equipment among the construction sites and would provide better collaborations and communications between the pavement maintenance teams. Proposed herein is a novel spatial clustering algorithm that automatically finds the projects within a certain budget and spatial constrains. The developed algorithm was successfully validated using 1,800 pavement maintenance projects from two real-life examples of the City of Milton, GA and the City of Tyler, TX.</description>
      <guid isPermaLink="false">oai:arXiv.org:1811.03437v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shadi Hanandeh, Omar Elbagalati, Mustafa Hajij</dc:creator>
    </item>
    <item>
      <title>GEEPERs: Principal Stratification using Principal Scores and Stacked Estimating Equations</title>
      <link>https://arxiv.org/abs/2212.10406</link>
      <description>arXiv:2212.10406v2 Announce Type: replace-cross 
Abstract: Principal stratification is a framework for making sense of causal effects conditioned on variables that themselves may have been affected by treatment. For instance, one component of an educational computer application is the availability of ``bottom-out'' hints that provide the answer. In evaluating a recent experimental evaluation against alternative programs without bottom-out hints, researchers may be interested in estimating separate average treatment effects for students who, if given the opportunity, would request bottom-out hints frequently, and for students who would not. Most principal stratification estimators rely on strong structural or modeling assumptions, and many require advanced statistical training to fit and check. In this paper, we introduce a new M-estimation principal effect estimator for one-way noncompliance based on a binary indicator. Estimates may be computed using conventional regressions (though the standard errors require a specialized sandwich formula) and do not rely on distributional assumptions. We present a simulation study that demonstrates the novel method's greater robustness compared to popular alternatives and illustrate the method through two real-data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.10406v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam C. Sales, Kirk P. Vanacore, Erin R. Ottmar</dc:creator>
    </item>
    <item>
      <title>The inverse Kalman filter</title>
      <link>https://arxiv.org/abs/2407.10089</link>
      <description>arXiv:2407.10089v4 Announce Type: replace-cross 
Abstract: We introduce the inverse Kalman filter, which enables exact matrix-vector multiplication between a covariance matrix from a dynamic linear model and any real-valued vector with linear computational cost. We integrate the inverse Kalman filter with the conjugate gradient algorithm, which substantially accelerates the computation of matrix inversion for a general form of covariance matrix, where other approximation approaches may not be directly applicable. We demonstrate the scalability and efficiency of the proposed approach through applications in nonparametric estimation of particle interaction functions, using both simulations and cell trajectories from microscopy data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10089v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Fang, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Predicting Bad Goods Risk Scores with ARIMA Time Series: A Novel Risk Assessment Approach</title>
      <link>https://arxiv.org/abs/2502.16520</link>
      <description>arXiv:2502.16520v3 Announce Type: replace-cross 
Abstract: The increasing complexity of supply chains and the rising costs associated with defective or substandard goods (bad goods) highlight the urgent need for advanced predictive methodologies to mitigate risks and enhance operational efficiency. This research presents a novel framework that integrates Time Series ARIMA (AutoRegressive Integrated Moving Average) models with a proprietary formula specifically designed to calculate bad goods after time series forecasting. By leveraging historical data patterns, including sales, returns, and capacity, the model forecasts potential quality failures, enabling proactive decision-making. ARIMA is employed to capture temporal trends in time series data, while the newly developed formula quantifies the likelihood and impact of defects with greater precision. Experimental results, validated on a dataset spanning 2022-2024 for Organic Beer-G 1 Liter, demonstrate that the proposed method outperforms traditional statistical models, such as Exponential Smoothing and Holt-Winters, in both prediction accuracy and risk evaluation. This study advances the field of predictive analytics by bridging time series forecasting, ARIMA, and risk management in supply chain quality control, offering a scalable and practical solution for minimizing losses due to bad goods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16520v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bishwajit Prasad Gond</dc:creator>
    </item>
    <item>
      <title>Fast Two-photon Microscopy by Neuroimaging with Oblong Random Acquisition (NORA)</title>
      <link>https://arxiv.org/abs/2503.15487</link>
      <description>arXiv:2503.15487v2 Announce Type: replace-cross 
Abstract: Advances in neural imaging have enabled neuroscientists to study how large neural populations conspire to produce perception, behavior and cognition. Despite many advances in optical methods, there exists a fundamental tradeoff between imaging speed, field of view, and resolution that limits the scope of neural imaging, especially for the raster-scanning multi-photon imaging needed to image deeper into the brain. One approach to overcoming this trade-off is computational imaging: the co-development of optics designed to encode the target images into fewer measurements that are faster to acquire, with algorithms that compensate by inverting the optical coding to recover a larger or higher resolution image. We present here one such approach for raster-scanning two-photon imaging: Neuroimaging with Oblong Random Acquisition (NORA). NORA quickly acquires each frame in a microscopy video by subsampling only a fraction of the fast scanning lines, ignoring large portions of each frame. NORA mitigates the loss of information by 1) extending the point-spread function in the slow-scan direction to effectively integrate the fluorescence of several lines into a single set of measurements and 2) imaging different, randomly selected, lines at each frame. Rather than reconstruct the video frame-by-frame, NORA recovers full video sequences via nuclear-norm minimization on the pixels-by-time matrix, for which we prove theoretical guarantees on recovery. We simulated NORA imaging using the Neural Anatomy and Optical Microscopy (NAOMi) biophysical simulator, and used the simulations to demonstrate that NORA can accurately recover 400 um X 400 um fields of view at subsampling rates up to 20X, despite realistic noise and motion conditions. As NORA requires minimal changes to current microscopy systems, our results indicate that NORA can provide a promising avenue towards fast imaging of neural circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15487v2</guid>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>physics.optics</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Esther Whang, Skyler Thomas, Ji Yi, Adam S. Charles</dc:creator>
    </item>
    <item>
      <title>Alpha-Beta HMM: Hidden Markov Model Filtering with Equal Exit Probabilities and a Step-Size Parameter</title>
      <link>https://arxiv.org/abs/2504.01759</link>
      <description>arXiv:2504.01759v3 Announce Type: replace-cross 
Abstract: The hidden Markov model (HMM) provides a powerful framework for inference in time-varying environments, where the underlying state evolves according to a Markov chain. To address the optimal filtering problem in general dynamic settings, we propose the $\alpha\beta$-HMM algorithm, which simplifies the state transition model to a Markov chain with equal exit probabilities and introduces a step-size parameter to balance the influence of observational data and the model. By analyzing the algorithm's dynamics in stationary environments, we uncover a fundamental trade-off between inference accuracy and adaptation capability, highlighting how key parameters and observation quality impact performance. A comprehensive theoretical analysis of the nonlinear dynamical system governing the evolution of the log-belief ratio, along with supporting numerical experiments, demonstrates that the proposed approach effectively balances adaptability and inference performance in dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01759v3</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dongyan Sui, Haotian Pu, Siyang Leng, Stefan Vlaski</dc:creator>
    </item>
    <item>
      <title>A CRISP approach to QSP: XAI enabling fit-for-purpose models</title>
      <link>https://arxiv.org/abs/2505.02750</link>
      <description>arXiv:2505.02750v3 Announce Type: replace-cross 
Abstract: Quantitative Systems Pharmacology (QSP) promises to accelerate drug development, enable personalized medicine, and improve the predictability of clinical outcomes. Realizing this potential requires effectively managing the complexity of mathematical models representing biological systems. Here, we present and validate a novel QSP workflow--CRISP (Contextualized Reduction for Identifiability and Scientific Precision)--that addresses a central challenge in QSP: the problem of complexity and over-parameterization, in which models contain irrelevant parameters that obscure interpretation and hinder predictive reliability. The CRISP workflow begins with a literature-derived model, constructed to be comprehensive and unbiased by integrating prior mechanistic insights. At the core of the workflow is the Manifold Boundary Approximation Method (MBAM), a reduction technique that simplifies models while preserving mechanistic structure and predictive fidelity. By applying MBAM in a context-specific manner, CRISP links parsimonious models directly to predictions of interest, clarifying causal structure and enhancing interpretability. The resulting models are computationally efficient and well-suited to key QSP tasks, including virtual population generation, experimental design, toxicology, and target discovery. We demonstrate the utility of CRISP on case studies involving the coagulation cascade and SHIV infection, and identify promising directions for improving the efficacy of bNAb therapies for HIV. Together, these results establish CRISP as a general-purpose QSP workflow for turning complex mechanistic models into tools for precise scientific reasoning to guide pharmacological and regulatory decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02750v3</guid>
      <category>q-bio.QM</category>
      <category>q-bio.MN</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noah DeTal, Christian N. K. Anderson, Mark K. Transtrum</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Richardson-Lucy Deconvolution and Data Unfolding with Mean Integrated Square Error Optimization</title>
      <link>https://arxiv.org/abs/2505.10283</link>
      <description>arXiv:2505.10283v2 Announce Type: replace-cross 
Abstract: Two maximum likelihood-based algorithms for unfolding or deconvolution are considered: the Richardson-Lucy method and the Data Unfolding method with Mean Integrated Square Error (MISE) optimization [10]. Unfolding is viewed as a procedure for estimating an unknown probability density function. Both external and internal quality assessment methods can be applied for this purpose. In some cases, external criteria exist to evaluate deconvolution quality. A typical example is the deconvolution of a blurred image, where the sharpness of the restored image serves as an indicator of quality. However, defining such external criteria can be challenging, particularly when a measurement has not been performed previously. In such instances, internal criteria are necessary to assess the quality of the result independently of external information. The article discusses two internal criteria: MISE for the unfolded distribution and the condition number of the correlation matrix of the unfolded distribution. These internal quality criteria are applied to a comparative analysis of the two methods using identical numerical data. The results of the analysis demonstrate the superiority of the Data Unfolding method with MISE optimization over the Richardson-Lucy method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10283v2</guid>
      <category>physics.data-an</category>
      <category>astro-ph.IM</category>
      <category>hep-ex</category>
      <category>nucl-ex</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolay D. Gagunashvili</dc:creator>
    </item>
    <item>
      <title>Two-step dimensionality reduction of human mobility data: From potential landscapes to spatiotemporal insights</title>
      <link>https://arxiv.org/abs/2505.20929</link>
      <description>arXiv:2505.20929v3 Announce Type: replace-cross 
Abstract: Understanding the spatiotemporal patterns of human mobility is crucial for addressing societal challenges, such as epidemic control and urban transportation optimization. Despite advancements in data collection, the complexity and scale of mobility data continue to pose significant analytical challenges. Existing methods often result in losing location-specific details and fail to fully capture the intricacies of human movement. This study proposes a two-step dimensionality reduction framework to overcome existing limitations. First, we construct a potential landscape of human flow from origin-destination (OD) matrices using combinatorial Hodge theory, preserving essential spatial and structural information while enabling an intuitive visualization of flow patterns. Second, we apply principal component analysis (PCA) to the potential landscape, systematically identifying major spatiotemporal patterns. By implementing this two-step reduction method, we reveal significant shifts during a pandemic, characterized by an overall declines in mobility and stark contrasts between weekdays and holidays. These findings underscore the effectiveness of our framework in uncovering complex mobility patterns and provide valuable insights into urban planning and public health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20929v3</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 10 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunhan Du, Takaaki Aoki, Naoya Fujiwara</dc:creator>
    </item>
  </channel>
</rss>
