<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Oct 2025 02:45:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>General Divergence Regularized Optimal Transport: Sample Complexity and Central Limit Theorems</title>
      <link>https://arxiv.org/abs/2510.02489</link>
      <description>arXiv:2510.02489v1 Announce Type: new 
Abstract: Optimal transport has emerged as a fundamental methodology with applications spanning multiple research areas in recent years. However, the convergence rate of the empirical estimator to its population counterpart suffers from the curse of dimensionality, which prevents its application in high-dimensional spaces. While entropic regularization has been proven to effectively mitigate the curse of dimensionality and achieve a parametric convergence rate under mild conditions, these statistical guarantees have not been extended to general regularizers. Our work bridges this gap by establishing analogous results for a broader family of regularizers. Specifically, under boundedness constraints, we prove a convergence rate of order $n^{-1/2} with respect to sample size n. Furthermore, we derive several central limit theorems for divergence regularized optimal transport.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02489v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaping Yang, Yunxin Zhang</dc:creator>
    </item>
    <item>
      <title>Robustified Gaussian quasi-likelihood inference for volatility</title>
      <link>https://arxiv.org/abs/2510.02666</link>
      <description>arXiv:2510.02666v1 Announce Type: new 
Abstract: We consider statistical inference for a class of continuous regression models contaminated by finite-activity jumps and spike noises. We propose an $M$-estimator through some easy-to-implement one-parameter robustifications of the conventional Gaussian quasi-likelihood function, and prove its asymptotic mixed normality at the standard rate $\sqrt{n}$. It is theoretically shown that the estimator is simultaneously robust against the contaminations in both the covariate process and the objective process. Additionally, we prove that, under suitable design conditions on the tuning parameter, the proposed estimators can enjoy the same asymptotic distribution as in the case of no contamination. Some illustrative simulation results are presented, highlighting the estimator's insensitivity to fine-tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02666v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shoichi Eguchi, Hiroki Masuda</dc:creator>
    </item>
    <item>
      <title>New M-estimator of the leading principal component</title>
      <link>https://arxiv.org/abs/2510.02799</link>
      <description>arXiv:2510.02799v1 Announce Type: new 
Abstract: We study the minimization of the non-convex and non-differentiable objective function $v \mapsto \mathrm{E} ( \| X - v \| \| X + v \| - \| X \|^2 )$ in $\mathbb{R}^p$. In particular, we show that its minimizers recover the first principal component direction of elliptically symmetric $X$ under specific conditions. The stringency of these conditions is studied in various scenarios, including a diverging number of variables $p$. We establish the consistency and asymptotic normality of the sample minimizer. We propose a Weiszfeld-type algorithm for optimizing the objective and show that it is guaranteed to converge in a finite number of steps. The results are illustrated with two simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02799v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joni Virta, Una Radojicic, Marko Voutilainen</dc:creator>
    </item>
    <item>
      <title>Gradient-enhanced global sensitivity analysis with Poincar{\'e} chaos expansions</title>
      <link>https://arxiv.org/abs/2510.03056</link>
      <description>arXiv:2510.03056v1 Announce Type: new 
Abstract: Chaos expansions are widely used in global sensitivity analysis (GSA), as they leverage orthogonal bases of L2 spaces to efficiently compute Sobol' indices, particularly in data-scarce settings. When derivatives are available, we argue that a desirable property is for the derivatives of the basis functions to also form an orthogonal basis. We demonstrate that the only basis satisfying this property is the one associated with weighted Poincar{\'e} inequalities and Sturm-Liouville eigenvalue problems, which we refer to as the Poincar{\'e} basis. We then introduce a comprehensive framework for gradient-enhanced GSA that integrates recent advances in sparse, gradient-enhanced regression for surrogate modeling with the construction of weighting schemes for derivative-based sensitivity analysis. The proposed methodology is applicable to a broad class of probability measures and supports various choices of weights. We illustrate the effectiveness of the approach on a challenging flood modeling case study, where Sobol' indices are accurately estimated using limited data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03056v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>O Roustant (INSA Toulouse, IMT, RT-UQ, ANITI), N L\"uthen (INSA Toulouse, IMT), D Heredia (INSA Toulouse, IMT), B Sudret</dc:creator>
    </item>
    <item>
      <title>Rates of Convergence of Generalised Variational Inference Posteriors under Prior Misspecification</title>
      <link>https://arxiv.org/abs/2510.03109</link>
      <description>arXiv:2510.03109v1 Announce Type: new 
Abstract: We prove rates of convergence and robustness to prior misspecification within a Generalised Variational Inference (GVI) framework with bounded divergences. This addresses a significant open challenge for GVI and Federated GVI that employ a different divergence to the Kullback--Leibler under prior misspecification, operate within a subset of possible probability measures, and result in intractable posteriors. Our theoretical contributions cover severe prior misspecification while relying on our ability to restrict the space of possible GVI posterior measures, and infer properties based on this space. In particular, we are able to establish sufficient conditions for existence and uniqueness of GVI posteriors on arbitrary Polish spaces, prove that the GVI posterior measure concentrates on a neighbourhood of loss minimisers, and extend this to rates of convergence regardless of the prior measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03109v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Terje Mildner, Paris Giampouras, Theodoros Damoulas</dc:creator>
    </item>
    <item>
      <title>Nonparametric Vector Quantile Autoregression</title>
      <link>https://arxiv.org/abs/2510.03166</link>
      <description>arXiv:2510.03166v1 Announce Type: new 
Abstract: Prediction is a key issue in time series analysis. Just as classical mean regression models, classical autoregressive methods, yielding L$^2$ point-predictions, provide rather poor predictive summaries; a much more informative approach is based on quantile (auto)regression, where the whole distribution of future observations conditional on the past is consistently recovered. Since their introduction by Koenker and Xiao in 2006, autoregressive quantile autoregression methods have become a popular and successful alternative to the traditional L$^2$ ones. Due to the lack of a widely accepted concept of multivariate quantiles, however, quantile autoregression methods so far have been limited to univariate time series. Building upon recent measure-transportation-based concepts of multivariate quantiles, we develop here a nonparametric vector quantile autoregressive approach to the analysis and prediction of (nonlinear as well as linear) multivariate time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03166v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Gonz\'alez-Sanz, Marc Hallin, Yisha Yao</dc:creator>
    </item>
    <item>
      <title>Apply Bayes Theorem to Optimize IVR Authentication Process</title>
      <link>https://arxiv.org/abs/2510.02378</link>
      <description>arXiv:2510.02378v1 Announce Type: cross 
Abstract: This paper introduces a Bayesian approach to improve Interactive Voice Response (IVR) authentication processes used by financial institutions. Traditional IVR systems authenticate users through a static sequence of credentials, assuming uniform effectiveness among them. However, fraudsters exploit this predictability, selectively bypassing strong credentials. This study applies Bayes' Theorem and conditional probability modeling to evaluate fraud risk dynamically and adapt credential verification paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02378v1</guid>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jingrong Xie, Yumin Li</dc:creator>
    </item>
    <item>
      <title>Orthogonal Procrustes problem preserves correlations in synthetic data</title>
      <link>https://arxiv.org/abs/2510.02405</link>
      <description>arXiv:2510.02405v1 Announce Type: cross 
Abstract: This work introduces the application of the Orthogonal Procrustes problem to the generation of synthetic data. The proposed methodology ensures that the resulting synthetic data preserves important statistical relationships among features, specifically the Pearson correlation. An empirical illustration using a large, real-world, tabular dataset of energy consumption demonstrates the effectiveness of the approach and highlights its potential for application in practical synthetic data generation. Our approach is not meant to replace existing generative models, but rather as a lightweight post-processing step that enforces exact Pearson correlation to an already generated synthetic dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02405v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oussama Ounissi, Nicklas J\"averg\r{a}rd, Adrian Muntean</dc:creator>
    </item>
    <item>
      <title>Higher-arity PAC learning, VC dimension and packing lemma</title>
      <link>https://arxiv.org/abs/2510.02420</link>
      <description>arXiv:2510.02420v1 Announce Type: cross 
Abstract: The aim of this note is to overview some of our work in Chernikov, Towsner'20 (arXiv:2010.00726) developing higher arity VC theory (VC$_n$ dimension), including a generalization of Haussler packing lemma, and an associated tame (slice-wise) hypergraph regularity lemma; and to demonstrate that it characterizes higher arity PAC learning (PAC$_n$ learning) in $n$-fold product spaces with respect to product measures introduced by Kobayashi, Kuriyama and Takeuchi'15. We also point out how some of the recent results in arXiv:2402.14294, arXiv:2505.15688, arXiv:2509.20404 follow from our work in arXiv:2010.00726.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02420v1</guid>
      <category>stat.ML</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <category>math.LO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Artem Chernikov, Henry Towsner</dc:creator>
    </item>
    <item>
      <title>Predictive inference for time series: why is split conformal effective despite temporal dependence?</title>
      <link>https://arxiv.org/abs/2510.02471</link>
      <description>arXiv:2510.02471v1 Announce Type: cross 
Abstract: We consider the problem of uncertainty quantification for prediction in a time series: if we use past data to forecast the next time point, can we provide valid prediction intervals around our forecasts? To avoid placing distributional assumptions on the data, in recent years the conformal prediction method has been a popular approach for predictive inference, since it provides distribution-free coverage for any iid or exchangeable data distribution. However, in the time series setting, the strong empirical performance of conformal prediction methods is not well understood, since even short-range temporal dependence is a strong violation of the exchangeability assumption. Using predictors with "memory" -- i.e., predictors that utilize past observations, such as autoregressive models -- further exacerbates this problem. In this work, we examine the theoretical properties of split conformal prediction in the time series setting, including the case where predictors may have memory. Our results bound the loss of coverage of these methods in terms of a new "switch coefficient", measuring the extent to which temporal dependence within the time series creates violations of exchangeability. Our characterization of the coverage probability is sharp over the class of stationary, $\beta$-mixing processes. Along the way, we introduce tools that may prove useful in analyzing other predictive inference methods for dependent data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02471v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rina Foygel Barber, Ashwin Pananjady</dc:creator>
    </item>
    <item>
      <title>Batched Nonparametric Contextual Bandits</title>
      <link>https://arxiv.org/abs/2402.17732</link>
      <description>arXiv:2402.17732v4 Announce Type: replace 
Abstract: We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. We establish a minimax regret lower bound for this setting and propose a novel batch learning algorithm that achieves the optimal regret (up to logarithmic factors). In essence, our procedure dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. Our theoretical results suggest that for nonparametric contextual bandits, a nearly constant number of policy updates can attain optimal regret in the fully online setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17732v4</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rong Jiang, Cong Ma</dc:creator>
    </item>
    <item>
      <title>Improved thresholds for e-values</title>
      <link>https://arxiv.org/abs/2408.11307</link>
      <description>arXiv:2408.11307v2 Announce Type: replace 
Abstract: The rejection threshold used for e-values and e-processes is by default set to $1/\alpha$ for a guaranteed type-I error control at $\alpha$, based on Markov's and Ville's inequalities. This threshold can be wasteful in practical applications. We discuss how this threshold can be improved under additional distributional assumptions on the e-values; some of these assumptions are naturally plausible and empirically observable, without knowing explicitly the form or model of the e-values. For small values of $\alpha$, the threshold can roughly be improved (divided) by a factor of $2$ for decreasing or unimodal densities, and by a factor of $e$ for decreasing or unimodal-symmetric densities of log-transformed e-values. Moreover, we propose to use the supremum of comonotonic e-values, which is shown to preserve the type-I error guarantee. We also propose some preliminary methods to boost e-values in the e-BH procedure under some distributional assumptions while controlling the false discovery rate. Through a series of simulation studies, we demonstrate the effectiveness of our proposed methods in various testing scenarios, showing enhanced power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11307v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Blier-Wong, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Efficient adjustment sets for time-dependent treatment effect estimation in nonparametric causal graphical model</title>
      <link>https://arxiv.org/abs/2410.01000</link>
      <description>arXiv:2410.01000v2 Announce Type: replace 
Abstract: Criteria for identifying optimal adjustment sets yielding consistent estimation with minimal asymptotic variance of average treatment effects in parametric and nonparametric models have recently been established. In a single treatment time point setting, it has been shown that the optimal adjustment set can be identified based on a causal directed acyclic graph alone. In a time-dependent treatment setting, previous work has established graphical rules to compare the asymptotic variance of estimators based on nested time-dependent adjustment sets. However, these rules do not always permit the identification of an optimal time-dependent adjustment set based on a causal graph alone. We extend those results by exploiting conditional independencies that can be read from the graph and demonstrate theoretically and empirically that our results can yield estimators with lower asymptotic variance than those allowed by previous results. We further show how our results allow for the identification of optimal adjustment sets based on a directed acyclic graph alone in the time-dependent treatment setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01000v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Adenyo, Mireille E Schnitzer, David Berger, Jason R Guertin, Denis Talbot</dc:creator>
    </item>
    <item>
      <title>Testing LRD in the spectral domain for functional time series in manifolds</title>
      <link>https://arxiv.org/abs/2411.07731</link>
      <description>arXiv:2411.07731v4 Announce Type: replace 
Abstract: A statistical hypothesis test for long range dependence (LRD) is formulated in the spectral domain for functional time series in manifolds. The elements of the spectral density operator family are assumed to be invariant with respect to the group of isometries of the manifold. The proposed test statistic is based on the weighted periodogram operator. A Central Limit Theorem is derived to obtain the asymptotic Gaussian distribution of the proposed test statistic operator under the null hypothesis. The rate of convergence to zero, in the Hilbert--Schmidt operator norm, of the bias of the integrated empirical second and fourth order cumulant spectral density operators is obtained under the alternative hypothesis. The consistency of the test follows from the consistency of the integrated weighted periodogram operator under LRD. Practical implementation of our testing approach is based on the random projection methodology. A simulation study illustrates, in the context of spherical functional time series, the asymptotic normality of the test statistic under the null hypothesis, and its consistency under the alternative. The empirical size and power properties are also computed for different functional sample sizes, and under different scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07731v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. D. Ruiz-Medina, R. M. Crujeiras</dc:creator>
    </item>
    <item>
      <title>Dynamical local Fr\'echet curve regression in manifolds</title>
      <link>https://arxiv.org/abs/2505.05168</link>
      <description>arXiv:2505.05168v2 Announce Type: replace 
Abstract: The present paper solves the problem of local linear approximation of the Fr\'echet conditional mean in an extrinsic and intrinsic way from time correlated bivariate curve data evaluated in a manifold (see Torres et al, 2025, on global Fr\'echet functional regression in manifolds). The extrinsic local linear Fr\'echet functional regression predictor is obtained in the time-varying tangent space by projection into an orthornormal eigenfunction basis in the ambient Hilbert space. The conditions assumed ensure the existence and uniqueness of this predictor, and its computation via exponential and logarithmic maps. A weighted Fr\'echet mean approach is adopted in the computation of an intrinsic local linear Fr\'echet functional regression predictor. The asymptotic optimality of this intrinsic local approximation is also proved. The finite sample size performance of the empirical version of both, extrinsic and intrinsic local functional predictors, and of a Nadaraya-Watson type Fr\'echet curve predictor is illustrated in the simulation study undertaken. As motivating real data application, we consider the prediction problem of the Earth's magnetic field from the time-varying geocentric latitude and longitude of the satellite NASA's MAGSAT spacecraft.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05168v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. D. Ruiz-Medina, A. Torres-Signes</dc:creator>
    </item>
    <item>
      <title>Restricted Spectral Gap Decomposition for Simulated Tempering Targeting Mixture Distributions</title>
      <link>https://arxiv.org/abs/2505.15059</link>
      <description>arXiv:2505.15059v2 Announce Type: replace 
Abstract: Simulated tempering is a widely used strategy for sampling from multimodal distributions. In this paper, we consider simulated tempering combined with an arbitrary local Markov chain Monte Carlo sampler and present a new decomposition theorem that provides a lower bound on the restricted spectral gap of the algorithm for sampling from mixture distributions. By working with the restricted spectral gap, the applicability of our results is extended to broader settings such as when the usual spectral gap is difficult to bound or becomes degenerate. We demonstrate the application of our theoretical results by analyzing simulated tempering combined with random walk Metropolis--Hastings for sampling from mixtures of Gaussian distributions. Our complexity bound scales polynomially with the separation between modes, logarithmically with $1/\varepsilon$, where $\varepsilon$ denotes the target accuracy in total variation distance, and exponentially with the dimension $d$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15059v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jhanvi Garg, Krishna Balasubramanian, Quan Zhou</dc:creator>
    </item>
    <item>
      <title>Optimal structure learning and conditional independence testing</title>
      <link>https://arxiv.org/abs/2507.05689</link>
      <description>arXiv:2507.05689v2 Announce Type: replace 
Abstract: We establish a fundamental connection between optimal structure learning and optimal conditional independence testing by showing that the minimax optimal rate for structure learning problems is determined by the minimax rate for conditional independence testing in these problems. This is accomplished by establishing a general reduction between these two problems in the case of poly-forests, and demonstrated by deriving optimal rates for several examples, including Bernoulli, Gaussian and nonparametric models. Furthermore, we show that the optimal algorithm in these settings is a suitable modification of the PC algorithm. This theoretical finding provides a unified framework for analyzing the statistical complexity of structure learning through the lens of minimax testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05689v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ming Gao, Yuhao Wang, Bryon Aragam</dc:creator>
    </item>
    <item>
      <title>Extending Mean-Field Variational Inference via Entropic Regularization: Theory and Computation</title>
      <link>https://arxiv.org/abs/2404.09113</link>
      <description>arXiv:2404.09113v3 Announce Type: replace-cross 
Abstract: Variational inference (VI) has emerged as a popular method for approximate inference for high-dimensional Bayesian models. In this paper, we propose a novel VI method that extends the naive mean field via entropic regularization, referred to as $\Xi$-variational inference ($\Xi$-VI). $\Xi$-VI has a close connection to the entropic optimal transport problem and benefits from the computationally efficient Sinkhorn algorithm. We show that $\Xi$-variational posteriors effectively recover the true posterior dependency, where the dependence is downweighted by the regularization parameter. We analyze the role of dimensionality of the parameter space on the accuracy of $\Xi$-variational approximation and how it affects computational considerations, providing a rough characterization of the statistical-computational trade-off in $\Xi$-VI. We also investigate the frequentist properties of $\Xi$-VI and establish results on consistency, asymptotic normality, high-dimensional asymptotics, and algorithmic stability. We provide sufficient criteria for achieving polynomial-time approximate inference using the method. Finally, we demonstrate the practical advantage of $\Xi$-VI over mean-field variational inference on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09113v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bohan Wu, David Blei</dc:creator>
    </item>
    <item>
      <title>Tropical combinatorics of max-linear Bayesian networks</title>
      <link>https://arxiv.org/abs/2411.10394</link>
      <description>arXiv:2411.10394v5 Announce Type: replace-cross 
Abstract: A polytrope is a tropical polyhedron that is also classically convex. We study the tropical combinatorial types of polytropes associated to weighted directed acyclic graphs (DAGs). This family of polytropes arises in algebraic statistics when describing the model class of max-linear Bayesian networks. We show how the edge weights of a network directly relate to the facet structure of the corresponding polytrope. We also give a classification of polytropes from weighted DAGs at different levels of equivalence. These results give insight on the statistical problem of identifiability for a max-linear Bayesian network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10394v5</guid>
      <category>math.CO</category>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Am\'endola, Kamillo Ferry</dc:creator>
    </item>
    <item>
      <title>A Heavily Right Strategy for Statistical Inference with Dependent Studies in Any Dimension</title>
      <link>https://arxiv.org/abs/2501.01065</link>
      <description>arXiv:2501.01065v2 Announce Type: replace-cross 
Abstract: We leverage recent advances in heavy-tail approximations for global hypothesis testing with dependent studies to construct approximate confidence regions without modeling or estimating their dependence structures. A non-rejection region is a confidence region but it may not be convex. Convexity is appealing because it ensures any one-dimensional linear projection of the region is a confidence interval, easy to compute and interpret. We show why convexity fails for nearly all heavy-tail combination tests proposed in recent years, including the influential Cauchy combination test. These insights motivate a \textit{heavily right} strategy: truncating the left half of the Cauchy distribution to obtain the Half-Cauchy combination test. The harmonic mean test also corresponds to a heavily right distribution with a Cauchy-like tail, namely a Pareto distribution with unit power. We prove that both approaches guarantee convexity when individual studies are summarized by Hotelling $T^2$ or $\chi^{2}$ statistics (regardless of the validity of this summary) and provide efficient, \textit{exact} algorithms for implementation. Applying these methods, we develop a divide-and-combine strategy for mean estimation in any dimension and construct simultaneous confidence intervals in a network meta-analysis for treatment effect comparisons across multiple clinical trials. We also present many open problems and conclude with epistemic reflections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01065v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianle Liu, Xiao-Li Meng, Natesh S. Pillai</dc:creator>
    </item>
    <item>
      <title>Non-Bayesian Learning in Misspecified Models</title>
      <link>https://arxiv.org/abs/2503.18024</link>
      <description>arXiv:2503.18024v3 Announce Type: replace-cross 
Abstract: Deviations from Bayesian updating are traditionally categorized as biases, errors, or fallacies, thus implying their inherent ``sub-optimality.'' We offer a more nuanced view. We demonstrate that, in learning problems with misspecified models, non-Bayesian updating can outperform Bayesian updating.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18024v3</guid>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Bervoets, Mathieu Faure, Ludovic Renou</dc:creator>
    </item>
    <item>
      <title>Iteratively reweighted kernel machines efficiently learn sparse functions</title>
      <link>https://arxiv.org/abs/2505.08277</link>
      <description>arXiv:2505.08277v2 Announce Type: replace-cross 
Abstract: The impressive practical performance of neural networks is often attributed to their ability to learn low-dimensional data representations and hierarchical structure directly from data. In this work, we argue that these two phenomena are not unique to neural networks, and can be elicited from classical kernel methods. Namely, we show that the derivative of the kernel predictor can detect the influential coordinates with low sample complexity. Moreover, by iteratively using the derivatives to reweight the data and retrain kernel machines, one is able to efficiently learn hierarchical polynomials with finite leap complexity. Numerical experiments illustrate the developed theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08277v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Libin Zhu, Damek Davis, Dmitriy Drusvyatskiy, Maryam Fazel</dc:creator>
    </item>
    <item>
      <title>Semiparametric Causal Inference for Right-Censored Outcomes with Many Weak Invalid Instruments</title>
      <link>https://arxiv.org/abs/2509.13176</link>
      <description>arXiv:2509.13176v2 Announce Type: replace-cross 
Abstract: We propose a semiparametric framework for causal inference with right-censored survival outcomes and many weak invalid instruments, motivated by Mendelian randomization in biobank studies where classical methods may fail. We adopt an accelerated failure time model and construct a moment condition based on augmented inverse probability of censoring weighting, incorporating both uncensored and censored observations. Under a heteroscedasticity-based condition on the treatment model, we establish point identification of the causal effect despite censoring and invalid instruments. We propose GEL-NOW (Generalized Empirical Likelihood with Non-Neyman Orthogonal and Weak moments) for valid inference under these conditions. A divergent number of Neyman orthogonal nuisance functions is estimated using deep neural networks. A key challenge is that the conditional censoring distribution is a non-Neyman orthogonal nuisance, contributing to the first-order asymptotics of the estimator for the target causal effect parameter. We derive the asymptotic distribution and explicitly incorporate this additional uncertainty into the asymptotic variance formula. We also introduce a censoring-adjusted over-identification test that accounts for this new variance component. Simulation studies and UK Biobank applications demonstrate the method's robustness and practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13176v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiushi Bu, Wen Su, Xingqiu Zhao, Zhonghua Liu</dc:creator>
    </item>
    <item>
      <title>Exactly or Approximately Wasserstein Distributionally Robust Estimation According to Wasserstein Radii Being Small or Large</title>
      <link>https://arxiv.org/abs/2510.01763</link>
      <description>arXiv:2510.01763v2 Announce Type: replace-cross 
Abstract: This paper primarily considers the robust estimation problem under Wasserstein distance constraints on the parameter and noise distributions in the linear measurement model with additive noise, which can be formulated as an infinite-dimensional nonconvex minimax problem. We prove that the existence of a saddle point for this problem is equivalent to that for a finite-dimensional minimax problem, and give a counterexample demonstrating that the saddle point may not exist. Motivated by this observation, we present a verifiable necessary and sufficient condition whose parameters can be derived from a convex problem and its dual. Additionally, we also introduce a simplified sufficient condition, which intuitively indicates that when the Wasserstein radii are small enough, the saddle point always exists. In the absence of the saddle point, we solve an finite-dimensional nonconvex minimax problem, obtained by restricting the estimator to be linear. Its optimal value establishes an upper bound on the robust estimation problem, while its optimal solution yields a robust linear estimator. Numerical experiments are also provided to validate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01763v2</guid>
      <category>eess.SP</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiao Ding, Enbin Song, Dunbiao Niu, Zhujun Cao, Qingjiang Shi</dc:creator>
    </item>
  </channel>
</rss>
