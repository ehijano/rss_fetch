<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Feb 2026 05:00:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Quantum walk inspired JPEG compression of images</title>
      <link>https://arxiv.org/abs/2602.12306</link>
      <description>arXiv:2602.12306v1 Announce Type: new 
Abstract: This work proposes a quantum inspired adaptive quantization framework that enhances the classical JPEG compression by introducing a learned, optimized Qtable derived using a Quantum Walk Inspired Optimization (QWIO) search strategy. The optimizer searches a continuous parameter space of frequency band scaling factors under a unified rate distortion objective that jointly considers reconstruction fidelity and compression efficiency. The proposed framework is evaluated on MNIST, CIFAR10, and ImageNet subsets, using Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM), Bits Per Pixel (BPP), and error heatmap visual analysis as evaluation metrics. Experimental results show average gains ranging from 3 to 6 dB PSNR, along with better structural preservation of edges, contours, and luminance transitions, without modifying decoder compatibility. The structure remains JPEG compliant and can be implemented using accessible scientific packages making it ideal for deployment and practical research use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12306v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Abhishek Verma, Sahil Tomar, Sandeep Kumar</dc:creator>
    </item>
    <item>
      <title>Visible and Hyperspectral Imaging for Quality Assessment of Milk: Property Characterisation and Identification</title>
      <link>https://arxiv.org/abs/2602.12313</link>
      <description>arXiv:2602.12313v1 Announce Type: new 
Abstract: Rapid and non-destructive assessment of milk quality is crucial to ensuring both nutritional value and food safety. In this study, we investigated the potential of visible and hyperspectral imaging as cost-effective and quick-response alternatives to conventional chemical analyses for characterizing key properties of cow\'s milk. A total of 52 milk samples were analysed to determine their biochemical composition (polyphenols, antioxidant capacity, and fatty acids) using spectrophotometer methods and standard gas-liquid and high-performance liquid chromatography (GLC/HPLC). Concurrently, visible (RGB) images were captured using a standard smartphone, and hyperspectral data were acquired in the near-infrared range. A comprehensive analytical framework, including eleven different machine learning algorithms, was employed to correlate imaging features with biochemical measurements. Analysis of visible images accurately distinguished between fresh samples and those stored for 12 days (100 percent accuracy) and achieved perfect discrimination between antibiotic-treated and untreated groups (100 percent accuracy). Moreover, image-derived features enabled perfect prediction of the polyphenols content and the antioxidant capacity using an XGBoost model. Hyperspectral imaging further achieved classification accuracies exceeding 95 percent for several individual fatty acids and 94.8 percent for treatment groups using a Random Forest model. These findings demonstrate that both visible and hyperspectral imaging, when coupled with machine learning, are powerful, non-invasive tools for the rapid assessment of milk\'s chemical and nutritional profiles, highlighting the strong potential of imaging-based approaches for milk quality assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12313v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Massimo Martinelli, Elena Tomassi, Nafiou Arouna, Morena Gabriele, Laryssa Perez Fabbri, Luisa Pozzo, Giuseppe Conte, Davide Moroni, Laura Pucci</dc:creator>
    </item>
    <item>
      <title>Conference Proceedings of the Inaugural Conference of the International Society for Tractography (IST 2025 Bordeaux)</title>
      <link>https://arxiv.org/abs/2602.12410</link>
      <description>arXiv:2602.12410v1 Announce Type: new 
Abstract: This collection comprises the abstracts presented during poster, power pitch and oral sessions at the Inaugural Conference of the International Society for Tractography (IST Conference 2025), held in Bordeaux, France, from October 13-16, 2025. The conference was designed to foster meaningful exchange and collaboration between disparate fields. The overall focus was on advancing research, innovation, and community in the common fields of interest: neuroanatomy, tractography methods and scientific/clinical applications of tractography. The included abstracts cover the latest advancements in tractography, Diffusion MRI, and related fields including new work on; neurological and psychiatric disorders, deep brain stimulation targeting, and brain development. This landmark event brought together world-leading experts to discuss critical challenges and chart the future direction of the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12410v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flavio Dell Acqua, Maxime Descoteaux, Graham Little, Laurent Petit, Dogu Baran Aydogan, Stephanie Forkel, Alexander Leemans, Simona Schiavi, Michel Thiebaut de Schotten</dc:creator>
    </item>
    <item>
      <title>Lung nodule classification on CT scan patches using 3D convolutional neural networks</title>
      <link>https://arxiv.org/abs/2602.12750</link>
      <description>arXiv:2602.12750v1 Announce Type: new 
Abstract: Lung cancer remains one of the most common and deadliest forms of cancer worldwide. The likelihood of successful treatment depends strongly on the stage at which the disease is diagnosed. Therefore, early detection of lung cancer represents a critical medical challenge. However, this task poses significant difficulties for thoracic radiologists due to the large number of studies to review, the presence of multiple nodules within the lungs, and the small size of many nodules, which complicates visual assessment. Consequently, the development of automated systems that incorporate highly accurate and computationally efficient lung nodule detection and classification modules is essential. This study introduces three methodological improvements for lung nodule classification: (1) an advanced CT scan cropping strategy that focuses the model on the target nodule while reducing computational cost; (2) target filtering techniques for removing noisy labels; (3) novel augmentation methods to improve model robustness. The integration of these techniques enables the development of a robust classification subsystem within a comprehensive Clinical Decision Support System for lung cancer detection, capable of operating across diverse acquisition protocols, scanner types, and upstream models (segmentation or detection). The multiclass model achieved a Macro ROC AUC of 0.9176 and a Macro F1-score of 0.7658, while the binary model reached a Binary ROC AUC of 0.9383 and a Binary F1-score of 0.8668 on the LIDC-IDRI dataset. These results outperform several previously reported approaches and demonstrate state-of-the-art performance for this task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12750v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.32782/tnv-tech.2025.5.1.42</arxiv:DOI>
      <arxiv:journal_reference>Tavriiskyi Naukovyi Visnyk. Seriia: Tekhnichni Nauky, 1(5):399-412, 2025</arxiv:journal_reference>
      <dc:creator>Volodymyr Sydorskyi</dc:creator>
    </item>
    <item>
      <title>VineetVC: Adaptive Video Conferencing Under Severe Bandwidth Constraints Using Audio-Driven Talking-Head Reconstruction</title>
      <link>https://arxiv.org/abs/2602.12758</link>
      <description>arXiv:2602.12758v1 Announce Type: new 
Abstract: Intense bandwidth depletion within consumer and constrained networks has the potential to undermine the stability of real-time video conferencing: encoder rate management becomes saturated, packet loss escalates, frame rates deteriorate, and end-to-end latency significantly increases. This work delineates an adaptive conferencing system that integrates WebRTC media delivery with a supplementary audio-driven talking-head reconstruction pathway and telemetry-driven mode regulation. The system consists of a WebSocket signaling service, an optional SFU for multi-party transmission, a browser client capable of real-time WebRTC statistics extraction and CSV telemetry export, and an AI REST service that processes a reference face image and recorded audio to produce a synthesized MP4; the browser can substitute its outbound camera track with the synthesized stream with a median bandwidth of 32.80 kbps. The solution incorporates a bandwidth-mode switching strategy and a client-side mode-state logger.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12758v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vineet Kumar Rakesh, Soumya Mazumdar, Tapas Samanta, Hemendra Kumar Pandey, Amitabha Das, Sarbajit Pal</dc:creator>
    </item>
    <item>
      <title>3DLAND: 3D Lesion Abdominal Anomaly Localization Dataset</title>
      <link>https://arxiv.org/abs/2602.12820</link>
      <description>arXiv:2602.12820v1 Announce Type: new 
Abstract: Existing medical imaging datasets for abdominal CT often lack three-dimensional annotations, multi-organ coverage, or precise lesion-to-organ associations, hindering robust representation learning and clinical applications. To address this gap, we introduce 3DLAND, a large-scale benchmark dataset comprising over 6,000 contrast-enhanced CT volumes with over 20,000 high-fidelity 3D lesion annotations linked to seven abdominal organs: liver, kidneys, pancreas, spleen, stomach, and gallbladder. Our streamlined three-phase pipeline integrates automated spatial reasoning, prompt-optimized 2D segmentation, and memory-guided 3D propagation, validated by expert radiologists with surface dice scores exceeding 0.75. By providing diverse lesion types and patient demographics, 3DLAND enables scalable evaluation of anomaly detection, localization, and cross-organ transfer learning for medical AI. Our dataset establishes a new benchmark for evaluating organ-aware 3D segmentation models, paving the way for advancements in healthcare-oriented AI. To facilitate reproducibility and further research, the 3DLAND dataset and implementation code are publicly available at https://mehrn79.github.io/3DLAND.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12820v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mehran Advand, Zahra Dehghanian, Navid Faraji, Reza Barati, Seyed Amir Ahmad Safavi-Naini, Hamid R. Rabiee</dc:creator>
    </item>
    <item>
      <title>Dual-Phase Cross-Modal Contrastive Learning for CMR-Guided ECG Representations for Cardiovascular Disease Assessment</title>
      <link>https://arxiv.org/abs/2602.12883</link>
      <description>arXiv:2602.12883v1 Announce Type: new 
Abstract: Cardiac magnetic resonance imaging (CMR) offers detailed evaluation of cardiac structure and function, but its limited accessibility restricts use to selected patient populations. In contrast, the electrocardiogram (ECG) is ubiquitous and inexpensive, and provides rich information on cardiac electrical activity and rhythm, yet offers limited insight into underlying cardiac structure and mechanical function. To address this, we introduce a contrastive learning framework that improves the extraction of clinically relevant cardiac phenotypes from ECG by learning from paired ECG-CMR data. Our approach aligns ECG representations with 3D CMR volumes at end-diastole (ED) and end-systole (ES), with a dual-phase contrastive loss to anchor each ECG jointly with both cardiac phases in a shared latent space. Unlike prior methods limited to 2D CMR representations with or without a temporal component, our framework models 3D anatomy at both ED and ES phases as distinct latent representations, enabling flexible disentanglement of structural and functional cardiac properties. Using over 34,000 ECG-CMR pairs from the UK Biobank, we demonstrate improved extraction of image-derived phenotypes from ECG, particularly for functional parameters ($\uparrow$ 9.2\%), while improvements in clinical outcome prediction remained modest ($\uparrow$ 0.7\%). This strategy could enable scalable and cost-effective extraction of image-derived traits from ECG. The code for this research is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12883v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Alvarez-Florez, Angel Bujalance-Gomez, Femke Raijmakers, Samuel Ruiperez-Campillo, Maarten Z. H. Kolk, Jesse Wiers, Julia Vogt, Erik J. Bekkers, Ivana I\v{s}gum, Fleur V. Y. Tjong</dc:creator>
    </item>
    <item>
      <title>Efficient Plug-and-Play method for Dynamic Imaging Via Kalman Smoothing</title>
      <link>https://arxiv.org/abs/2602.13043</link>
      <description>arXiv:2602.13043v1 Announce Type: new 
Abstract: State-space models (SSM) are common in signal processing, where Kalman smoothing (KS) methods are state-of-the-art. However, traditional KS techniques lack expressivity as they do not incorporate spatial prior information. Recently, [1] proposed an ADMM algorithm that handles the state-space fidelity term with KS while regularizing the object via a sparsity-based prior with proximity operators. Plug-and-Play (PnP) methods are a popular type of iterative algorithms that replace proximal operators encoding prior knowledge with powerful denoisers such as deep neural networks. These methods are widely used in image processing, achieving state-of-the-art results. In this work, we build on the KS-ADMM method, combining it with deep learning to achieve higher expressivity. We propose a PnP algorithm based on KS-ADMM iterations, efficiently handling the SSM through KS, while enabling the use of powerful denoising networks. Simulations on a 2D+t imaging problem show that the proposed PnP-KS-ADMM algorithm improves the computational efficiency over standard PnP-ADMM for large numbers of timesteps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13043v1</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Hawkes, Mike Davies, Victor Elvira, Audrey Repetti</dc:creator>
    </item>
    <item>
      <title>MedXIAOHE: A Comprehensive Recipe for Building Medical MLLMs</title>
      <link>https://arxiv.org/abs/2602.12705</link>
      <description>arXiv:2602.12705v1 Announce Type: cross 
Abstract: We present MedXIAOHE, a medical vision-language foundation model designed to advance general-purpose medical understanding and reasoning in real-world clinical applications. MedXIAOHE achieves state-of-the-art performance across diverse medical benchmarks and surpasses leading closed-source multimodal systems on multiple capabilities. To achieve this, we propose an entity-aware continual pretraining framework that organizes heterogeneous medical corpora to broaden knowledge coverage and reduce long-tail gaps (e.g., rare diseases). For medical expert-level reasoning and interaction, MedXIAOHE incorporates diverse medical reasoning patterns via reinforcement learning and tool-augmented agentic training, enabling multi-step diagnostic reasoning with verifiable decision traces. To improve reliability in real-world use, MedXIAOHE integrates user-preference rubrics, evidence-grounded reasoning, and low-hallucination long-form report generation, with improved adherence to medical instructions. We release this report to document our practical design choices, scaling insights, and evaluation framework, hoping to inspire further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12705v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baorong Shi, Bo Cui, Boyuan Jiang, Deli Yu, Fang Qian, Haihua Yang, Huichao Wang, Jiale Chen, Jianfei Pan, Jieqiong Cao, Jinghao Lin, Kai Wu, Lin Yang, Shengsheng Yao, Tao Chen, Xiaojun Xiao, Xiaozhong Ji, Xu Wang, Yijun He, Zhixiong Yang</dc:creator>
    </item>
    <item>
      <title>A Wavefield Correlation Approach to Improve Sound Speed Estimation in Ultrasound Autofocusing</title>
      <link>https://arxiv.org/abs/2602.12805</link>
      <description>arXiv:2602.12805v1 Announce Type: cross 
Abstract: Aberration often degrades ultrasound image quality when beamforming does not account for wavefront distortions. In the past decade, local sound speed estimators have been developed for distributed aberration correction throughout a medium. Recently, iterative sound speed optimization approaches have achieved more accurate estimates than earlier approaches, but these newer methods still struggle with decreased accuracy for media with reverberation clutter and large sound speed changes. To address these challenges, we propose using a wavefield correlation (WFC) beamforming approach when performing sound speed optimization. WFC correlates simulated forward-propagated transmit wavefields and backwards-propagated receive wavefields in order to form images. This process more accurately models wave propagation in heterogeneous media and can decrease diffuse clutter due to its spatiotemporal matched filtering effect. This beamformer is implemented using auto-differentiation software to then perform gradient descent optimization, using a total-variation regularized common midpoint phase focus metric loss, on the local sound speed map used during beamforming. This approach is compared to using delay and sum (DAS) with straight-ray time delay calculations in the same sound speed optimization approach on a variety of simulated, phantom, and in vivo data with large sound speed changes and clutter. Results show that using WFC decreases sound speed estimation error, and using the estimates for aberration correction improves image resolution and contrast. These promising results have potential to improve pulse-echo imaging for challenging clinical scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12805v1</guid>
      <category>physics.med-ph</category>
      <category>cs.SD</category>
      <category>eess.IV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Louise Zhuang, Samuel Beuret, Ben Frey, Saachi Munot, Jeremy J. Dahl</dc:creator>
    </item>
    <item>
      <title>Model-Aware Rate-Distortion Limits for Task-Oriented Source Coding</title>
      <link>https://arxiv.org/abs/2602.12866</link>
      <description>arXiv:2602.12866v1 Announce Type: cross 
Abstract: Task-Oriented Source Coding (TOSC) has emerged as a paradigm for efficient visual data communication in machine-centric inference systems, where bitrate, latency, and task performance must be jointly optimized under resource constraints. While recent works have proposed rate-distortion bounds for coding for machines, these results often rely on strong assumptions on task identifiability and neglect the impact of deployed task models. In this work, we revisit the fundamental limits of single-TOSC through the lens of indirect rate-distortion theory. We highlight the conditions under which existing rate-distortion bounds are achievable and show their limitations in realistic settings. We then introduce task model-aware rate-distortion bounds that account for task model suboptimality and architectural constraints. Experiments on standard classification benchmarks confirm that current learned TOSC schemes operate far from these limits, highlighting transmitter-side complexity as a key bottleneck.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12866v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andriy Enttsel, Vincent Corlay</dc:creator>
    </item>
    <item>
      <title>A Plug-and-Play Method for Guided Multi-contrast MRI Reconstruction based on Content/Style Modeling</title>
      <link>https://arxiv.org/abs/2409.13477</link>
      <description>arXiv:2409.13477v5 Announce Type: replace 
Abstract: Since multiple MRI contrasts of the same anatomy contain redundant information, one contrast can guide the reconstruction of an undersampled subsequent contrast. To this end, several end-to-end learning-based guided reconstruction methods have been proposed. However, a key challenge is the requirement of large paired training datasets comprising raw data and aligned reference images. We propose a modular two-stage approach that does not require any k-space training data, relying solely on image-domain datasets, a large part of which can be unpaired. Additionally, our approach provides an explanatory framework for the multi-contrast problem based on the shared and non-shared generative factors underlying two given contrasts. A content/style model of two-contrast image data is learned from a largely unpaired image-domain dataset and is subsequently applied as a plug-and-play operator in iterative reconstruction. The disentanglement of content and style allows explicit representation of contrast-independent and contrast-specific factors. Consequently, incorporating prior information into the reconstruction reduces to a simple replacement of the aliased content of the reconstruction iterate with high-quality content derived from the reference scan. Combining this component with a data consistency step and introducing a general corrective process for the content yields an iterative scheme. We name this novel approach PnP-CoSMo. Various aspects like interpretability and convergence are explored via simulations. Furthermore, its practicality is demonstrated on the public NYU fastMRI DICOM dataset, showing improved generalizability compared to end-to-end methods, and on two in-house multi-coil raw datasets, offering up to 32.6\% more acceleration over learning-based non-guided reconstruction for a given SSIM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13477v5</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chinmay Rao, Matthias van Osch, Nicola Pezzotti, Jeroen de Bresser, Mark van Buchem, Laurens Beljaards, Jakob Meineke, Elwin de Weerdt, Huangling Lu, Mariya Doneva, Marius Staring</dc:creator>
    </item>
    <item>
      <title>A Synthetic Data-Driven Radiology Foundation Model for Pan-tumor Clinical Diagnosis</title>
      <link>https://arxiv.org/abs/2502.06171</link>
      <description>arXiv:2502.06171v3 Announce Type: replace 
Abstract: AI-assisted imaging made substantial advances in tumor diagnosis and management. However, a major barrier to developing robust oncology foundation models is the scarcity of large-scale, high-quality annotated datasets, which are limited by privacy restrictions and the high cost of manual labeling. To address this gap, we present PASTA, a pan-tumor radiology foundation model built on PASTA-Gen, a synthetic data framework that generated 30,000 3D CT scans with pixel-level lesion masks and structured reports of tumors across ten organ systems. Leveraging this resource, PASTA achieves state-of-the-art performance on 45 of 46 oncology tasks, including non-contrast CT tumor screening, lesion segmentation, structured reporting, tumor staging, survival prediction, and MRI-modality transfer. To assess clinical applicability, we developed PASTA-AID, a clinical decision support system, and ran a retrospective simulated clinical trial across two scenarios. For pan-tumor screening on plain CT with fixed reading time, PASTA-AID increased radiologists' throughput by 11.1-25.1% and improved sensitivity by 17.0-31.4% and precision by 10.5-24.9%; additionally, in a diagnosis-aid workflow, it reduced segmentation time by up to 78.2% and reporting time by up to 36.5%. Beyond gains in accuracy and efficiency, PASTA-AID narrowed the expertise gap, enabling less-experienced radiologists to approach expert-level performance. Together, this work establishes an end-to-end, synthetic data-driven pipeline spanning data generation, model development, and clinical validation, thereby demonstrating substantial potential for pan-tumor research and clinical translation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06171v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhui Lei, Hanyu Chen, Zitian Zhang, Luyang Luo, Qiong Xiao, Yannian Gu, Peng Gao, Yankai Jiang, Ci Wang, Guangtao Wu, Tongjia Xu, Yingjie Zhang, Pranav Rajpurkar, Xiaofan Zhang, Shaoting Zhang, Zhenning Wang</dc:creator>
    </item>
    <item>
      <title>LesionDiffusion: Towards Text-controlled General Lesion Synthesis</title>
      <link>https://arxiv.org/abs/2503.00741</link>
      <description>arXiv:2503.00741v5 Announce Type: replace 
Abstract: Fully-supervised lesion recognition methods in medical imaging face challenges due to the reliance on large annotated datasets, which are expensive and difficult to collect. To address this, synthetic lesion generation has become a promising approach. However, existing models struggle with scalability, fine-grained control over lesion attributes, and the generation of complex structures. We propose LesionDiffusion, a text-controllable lesion synthesis framework for 3D CT imaging that generates both lesions and corresponding masks. By utilizing a structured lesion report template, our model provides greater control over lesion attributes and supports a wider variety of lesion types. We introduce a dataset of 1,505 annotated CT scans with paired lesion masks and structured reports, covering 14 lesion types across 8 organs. LesionDiffusion consists of two components: a lesion mask synthesis network (LMNet) and a lesion inpainting network (LINet), both guided by lesion attributes and image features. Extensive experiments demonstrate that LesionDiffusion significantly improves segmentation performance, with strong generalization to unseen lesion types and organs, outperforming current state-of-the-art models. Code is available at https://github.com/HengruiTianSJTU/LesionDiffusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00741v5</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhui Lei, Henrui Tian, Linrui Dai, Hanyu Chen, Xiaofan Zhang</dc:creator>
    </item>
    <item>
      <title>Active Sampling for MRI-based Sequential Decision Making</title>
      <link>https://arxiv.org/abs/2505.04586</link>
      <description>arXiv:2505.04586v2 Announce Type: replace 
Abstract: Despite the superior diagnostic capability of Magnetic Resonance Imaging (MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and complexity. To enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. Previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. Such work shows that single diagnostic decisions can be made, but if we aspire to see MRI as a true PoC, multiple and sequential decisions are necessary while minimizing the number of samples acquired. We present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. Our approach during inference actively adapts to sequential decisions to optimally sample. To achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. We evaluate our approach in two sequential knee pathology assessment tasks: ACL sprain detection and cartilage thickness loss assessment. Our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. Our approach paves the way for the future of MRI as a comprehensive and affordable PoC device. Our code is publicly available at https://github.com/vios-s/MRI_Sequential_Active_Sampling</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04586v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuning Du, Jingshuai Liu, Rohan Dharmakumar, Sotirios A. Tsaftaris</dc:creator>
    </item>
    <item>
      <title>Direct Kernel Optimization: Efficient Design for Opto-Electronic Convolutional Neural Networks</title>
      <link>https://arxiv.org/abs/2511.02065</link>
      <description>arXiv:2511.02065v2 Announce Type: replace 
Abstract: Hybrid opto-electronic neural networks combine optical front-ends with electronic back-ends to perform vision tasks, but joint end-to-end (E2E) optimization of optical and electronic components is computationally expensive due to large parameter spaces and repeated optical convolutions. We propose Direct Kernel Optimization (DKO), a two-stage training framework that first trains a conventional electronic CNN and then synthesizes optical kernels to replicate the first-layer convolutional filters, reducing optimization dimensionality and avoiding hefty simulated optical convolutions during optimization. We evaluate DKO in simulation on a monocular depth estimation model and show that it achieves twice the accuracy of E2E training under equal computational budgets while reducing training time. Given the substantial computational challenges of optimizing hybrid opto-electronic systems, our results position DKO as a scalable optimization approach to train and realize these systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02065v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Almuallem, Harshana Weligampola, Abhiram Gnanasambandam, Wei Xu, Dilshan Godaliyadda, Hamid R. Sheikh, Stanley H. Chan, Qi Guo</dc:creator>
    </item>
    <item>
      <title>Towards a Unified Theoretical Framework for Self-Supervised MRI Reconstruction</title>
      <link>https://arxiv.org/abs/2601.04775</link>
      <description>arXiv:2601.04775v2 Announce Type: replace 
Abstract: The demand for high-resolution, non-invasive imaging continues to drive innovation in magnetic resonance imaging (MRI), yet prolonged acquisition times hinder accessibility and real-time applications. While deep learning-based reconstruction methods have accelerated MRI, their predominant supervised paradigm depends on fully-sampled reference data that are challenging to acquire. Recently, self-supervised learning (SSL) approaches have emerged as promising alternatives, but most are empirically designed and fragmented. Therefore, we introduce UNITS (Unified Theory for Self-supervision), a general framework for self-supervised MRI reconstruction. UNITS unifies prior SSL strategies within a common formalism, enabling consistent interpretation and systematic benchmarking. We prove that SSL can achieve the same expected performance as supervised learning. Under this theoretical guarantee, we introduce sampling stochasticity and flexible data utilization, which improve network generalization under out-of-domain distributions and stabilize training. Together, these contributions establish UNITS as a theoretical foundation and a practical paradigm for interpretable, generalizable, and clinically applicable self-supervised MRI reconstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04775v2</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siying Xu, Kerstin Hammernik, Daniel Rueckert, Sergios Gatidis, Thomas K\"ustner</dc:creator>
    </item>
    <item>
      <title>Toward Agentic AI: Task-Oriented Communication for Hierarchical Planning of Long-Horizon Tasks</title>
      <link>https://arxiv.org/abs/2601.13685</link>
      <description>arXiv:2601.13685v2 Announce Type: replace 
Abstract: Agentic artificial intelligence (AI) is an AI paradigm that can perceive the environment, reason over observations, and execute actions to achieve specific goals. Task-oriented communication supports agentic AI by transmitting only the task-related information instead of full raw data in order to reduce the bandwidth requirement. In real-world scenarios, AI agents often need to perform a sequence of actions to complete complex tasks. Completing these long-horizon tasks requires a hierarchical agentic AI architecture, where a high-level planner module decomposes a task into subtasks, and a low-level actor module executes each subtask sequentially. Since each subtask has a distinct goal, the existing task-oriented communication schemes are not designed to handle different goals for different subtasks. To address this challenge, in this paper, we develop a hierarchical task-oriented communication (HiTOC) framework. We consider a system with an edge server and a robot as an edge device. The high-level planner and low-level actor modules reside on the edge server. The robot transmits only the environmental observation that is relevant to the current subtask to the edge server. We propose a conditional variational information bottleneck (cVIB) approach to train the HiTOC framework to adaptively transmit minimal information required for each subtask. Simulations conducted on the AI2-THOR platform demonstrate that the proposed HiTOC framework outperforms three state-of-the-art schemes in terms of the success rate on MAP-THOR benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13685v2</guid>
      <category>eess.IV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sin-Yu Huang, Lele Wang, Vincent W. S. Wong</dc:creator>
    </item>
    <item>
      <title>Hallucinating 360{\deg}: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting</title>
      <link>https://arxiv.org/abs/2507.06971</link>
      <description>arXiv:2507.06971v3 Announce Type: replace-cross 
Abstract: Panoramic perception holds significant potential for autonomous driving, enabling vehicles to acquire a comprehensive 360{\deg} surround view in a single shot. However, autonomous driving is a data-driven task. Complete panoramic data acquisition requires complex sampling systems and annotation pipelines, which are time-consuming and labor-intensive. Although existing street view generation models have demonstrated strong data regeneration capabilities, they can only learn from the fixed data distribution of existing datasets and cannot leverage stitched pinhole images as a supervisory signal. In this paper, we propose the first panoramic generation method Percep360 for autonomous driving. Percep360 enables coherent generation of panoramic data with control signals based on the stitched panoramic data. Percep360 focuses on two key aspects: coherence and controllability. Specifically, to overcome the inherent information loss caused by the pinhole sampling process, we propose the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama generation as a spatially continuous diffusion process, bridging the gaps between different data distributions. Additionally, to achieve the controllable generation of panoramic images, we propose a Probabilistic Prompting Method (PPM). PPM dynamically selects the most relevant control cues, enabling controllable panoramic image generation. We evaluate the effectiveness of the generated images from three perspectives: image quality assessment (i.e., no-reference and with reference), controllability, and their utility in real-world Bird's Eye View (BEV) segmentation. Notably, the generated data consistently outperforms the original stitched images in no-reference quality metrics and enhances downstream perception models. The source code will be publicly available at https://github.com/FeiT-FeiTeng/Percep360.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06971v3</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Teng, Kai Luo, Sheng Wu, Siyu Li, Pujun Guo, Jiale Wei, Jiaming Zhang, Kunyu Peng, Kailun Yang</dc:creator>
    </item>
    <item>
      <title>Algebraic Connectivity Reveals Modulated High-Order Functional Networks in Alzheimer's Disease</title>
      <link>https://arxiv.org/abs/2508.01252</link>
      <description>arXiv:2508.01252v2 Announce Type: replace-cross 
Abstract: Functional MRI is a neuroimaging technique that analyzes the functional activity of the brain by measuring blood-oxygen-level-dependent signals throughout the brain. The derived functional features can be used for investigating brain alterations in neurological and psychiatric disorders. In this work, we employed a hypergraph to model high-order functional relations across brain regions, introducing algebraic connectivity (a(G)) for estimating the hyperedge weights. The hypergraph structure was derived from healthy controls to build a common topology across individuals. The considered cohort for subsequent analyses included subjects covering the Alzheimer's disease (AD) continuum, encompassing both mild cognitive impairment and AD patients. Statistical analysis and three classification tasks: HC vs AD, MCI vs AD, and HC vs MCI, were performed to assess differences across the three groups and the potential of the hyperedge weights as functional features. Furthermore, a mediation analysis was performed to evaluate the reliability of the a(G) values, representing functional information as the mediator between tau-PET levels, a key biomarker of AD, and cognitive scores. The proposed approach identified a larger number of hyperedges statistically different across groups compared to state-of-the-art methods. The a(G) hyperedge weights also demonstrated a higher discriminative power in all three binary classifications. Finally, two hyperedges belonging to salience/ventral attention and somatomotor networks showed a partial mediation effect between the tau biomarker and cognitive decline. These results suggested that a(G) can be an effective approach for extracting the hyperedge weights, including important functional information that resides in the brain areas forming the hyperedges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01252v2</guid>
      <category>q-bio.NC</category>
      <category>eess.IV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgio Dolci, Silvia Saglia, Lorenza Brusini, Vince D. Calhoun, Ilaria Boscolo Galazzo, Gloria Menegaz</dc:creator>
    </item>
    <item>
      <title>Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach</title>
      <link>https://arxiv.org/abs/2602.07015</link>
      <description>arXiv:2602.07015v2 Announce Type: replace-cross 
Abstract: Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07015v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Subreena, Mohammad Amzad Hossain, Mirza Raquib, Saydul Akbar Murad, Farida Siddiqi Prity, Muhammad Hanif, Nick Rahimi</dc:creator>
    </item>
  </channel>
</rss>
