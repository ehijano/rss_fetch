<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.RO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.RO</link>
    <description>cs.RO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.RO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Jul 2024 04:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Language-Driven 6-DoF Grasp Detection Using Negative Prompt Guidance</title>
      <link>https://arxiv.org/abs/2407.13842</link>
      <description>arXiv:2407.13842v1 Announce Type: new 
Abstract: 6-DoF grasp detection has been a fundamental and challenging problem in robotic vision. While previous works have focused on ensuring grasp stability, they often do not consider human intention conveyed through natural language, hindering effective collaboration between robots and users in complex 3D environments. In this paper, we present a new approach for language-driven 6-DoF grasp detection in cluttered point clouds. We first introduce Grasp-Anything-6D, a large-scale dataset for the language-driven 6-DoF grasp detection task with 1M point cloud scenes and more than 200M language-associated 3D grasp poses. We further introduce a novel diffusion model that incorporates a new negative prompt guidance learning strategy. The proposed negative prompt strategy directs the detection process toward the desired object while steering away from unwanted ones given the language input. Our method enables an end-to-end framework where humans can command the robot to grasp desired objects in a cluttered scene using natural language. Intensive experimental results show the effectiveness of our method in both benchmarking experiments and real-world scenarios, surpassing other baselines. In addition, we demonstrate the practicality of our approach in real-world robotic applications. Our project is available at https://airvlab.github.io/grasp-anything.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13842v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Toan Nguyen, Minh Nhat Vu, Baoru Huang, An Vuong, Quan Vuong, Ngan Le, Thieu Vo, Anh Nguyen</dc:creator>
    </item>
    <item>
      <title>Simultaneous Localization and Affordance Prediction for Tasks in Egocentric Video</title>
      <link>https://arxiv.org/abs/2407.13856</link>
      <description>arXiv:2407.13856v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have shown great success as foundational models for downstream vision and natural language applications in a variety of domains. However, these models lack the spatial understanding necessary for robotics applications where the agent must reason about the affordances provided by the 3D world around them. We present a system which trains on spatially-localized egocentric videos in order to connect visual input and task descriptions to predict a task's spatial affordance, that is the location where a person would go to accomplish the task. We show our approach outperforms the baseline of using a VLM to map similarity of a task's description over a set of location-tagged images. Our learning-based approach has less error both on predicting where a task may take place and on predicting what tasks are likely to happen at the current location. The resulting system enables robots to use egocentric sensing to navigate to physical locations of novel tasks specified in natural language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13856v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zachary Chavis, Hyun Soo Park, Stephen J. Guy</dc:creator>
    </item>
    <item>
      <title>A New Tightly-Coupled Dual-VIO for a Mobile Manipulator With Dynamic Locomotion</title>
      <link>https://arxiv.org/abs/2407.13878</link>
      <description>arXiv:2407.13878v1 Announce Type: new 
Abstract: This paper introduces a new dual monocular visualinertial odometry (dual-VIO) strategy for a mobile manipulator operating under dynamic locomotion, i.e. coordinated movement involving both the base platform and the manipulator arm. Our approach has been motivated by challenges arising from inaccurate estimation due to coupled excitation when the mobile manipulator is engaged in dynamic locomotion in cluttered environments. The technique maintains two independent monocular VIO modules, with one at the mobile base and the other at the end-effector (EE), which are tightly coupled at the low level of the factor graph. The proposed method treats each monocular VIO with respect to each other as a positional anchor through arm-kinematics. These anchor points provide a soft geometric constraint during the VIO pose optimization. This allows us to stabilize both estimators in case of instability of one estimator in highly dynamic locomotions. The performance of our approach has been demonstrated through extensive experimental testing with a mobile manipulator tested in comparison to running dual VINS-Mono in parallel. We envision that our method can also provide a foundation towards active-SLAM (ASLAM) with a new perspective on multi-VIO fusion and system redundancy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13878v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TMECH.2024.3400918</arxiv:DOI>
      <arxiv:journal_reference>IEEE/ASME Transactions on Mechatronics (2024)</arxiv:journal_reference>
      <dc:creator>Jianxiang Xu, Soo Jeon</dc:creator>
    </item>
    <item>
      <title>Multi-agent Coverage Control: From Discrete Assignments to Continuous Multi-agent Distribution Matching</title>
      <link>https://arxiv.org/abs/2407.13890</link>
      <description>arXiv:2407.13890v1 Announce Type: new 
Abstract: The multi-agent spatial coverage control problem encompasses a broad research domain, dealing with both dynamic and static deployment strategies, discrete-task assignments, and spatial distribution-matching deployment. Coverage control may involve the deployment of a finite number of agents or a continuum through centralized or decentralized, locally-interacting schemes. All these problems can be solved via a different taxonomy of deployment algorithms for multiple agents. Depending on the application scenario, these problems involve from purely discrete descriptions of tasks (finite loads) and agents (finite resources), to a mixture of discrete and continuous elements, to fully continuous descriptions of the same. Yet, it is possible to find common features that underline all the above formulations, which we aim to illustrate here. By doing so, we aim to point the reader to novel references related to these problems. The short article outline is the following: Static coverage via concurrent area partitioning and assignment; Static coverage as a discrete task assignment; and Continuum task assignment for large-scale swarms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13890v1</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Summer 2024 DSCD Newsletter</arxiv:journal_reference>
      <dc:creator>Solmaz Kia, Sonia Martinez</dc:creator>
    </item>
    <item>
      <title>Optimization-Based Outlier Accommodation for Tightly Coupled RTK-Aided Inertial Navigation Systems in Urban Environments</title>
      <link>https://arxiv.org/abs/2407.13912</link>
      <description>arXiv:2407.13912v1 Announce Type: new 
Abstract: Global Navigation Satellite Systems (GNSS) aided Inertial Navigation System (INS) is a fundamental approach for attaining continuously available absolute vehicle position and full state estimates at high bandwidth. For transportation applications, stated accuracy specifications must be achieved, unless the navigation system can detect when it is violated. In urban environments, GNSS measurements are susceptible to outliers, which motivates the important problem of accommodating outliers while either achieving a performance specification or communicating that it is not feasible. Risk-Averse Performance-Specified (RAPS) is designed to optimally select measurements to address this problem. Existing RAPS approaches lack a method applicable to carrier phase measurements, which have the benefit of measurement errors at the centimeter level along with the challenge of being biased by integer ambiguities. This paper proposes a RAPS framework that combines Real-time Kinematic (RTK) GNSS in a tightly coupled INS for urban navigation applications. Experimental results demonstrate the effectiveness of this RAPS-INS-RTK framework, achieving 84.05% and 89.84% of horizontal and vertical errors less than 1.5 meters and 3 meters, respectively, using a deep-urban dataset. This performance not only surpasses the Society of Automotive Engineers (SAE) requirements, but also shows a 10% improvement compared to traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13912v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wang Hu, Yingjie Hu, Mike Stas, Jay A. Farrell</dc:creator>
    </item>
    <item>
      <title>MSSP : A Versatile Multi-Scenario Adaptable Intelligent Robot Simulation Platform Based on LIDAR-Inertial Fusion</title>
      <link>https://arxiv.org/abs/2407.14102</link>
      <description>arXiv:2407.14102v1 Announce Type: new 
Abstract: This letter presents a multi-scenario adaptable intelligent robot simulation platform based on LIDAR-inertial fusion, with three main features: (1 The platform includes an versatile robot model that can be freely controlled through manual control or autonomous tracking. This model is equipped with various types of LIDAR and Inertial Measurement Unit (IMU), providing ground truth information with absolute accuracy. (2 The platform provides a collection of simulation environments with diverse characteristic information and supports developers in customizing and modifying environments according to their needs. (3 The platform supports evaluation of localization performance for SLAM frameworks. Ground truth with absolute accuracy eliminates the inherent errors of global positioning sensors present in real experiments, facilitating detailed analysis and evaluation of the algorithms. By utilizing the simulation platform, developers can overcome the limitations of real environments and datasets, enabling fine-grained analysis and evaluation of mainstream SLAM algorithms in various environments. Experiments conducted in different environments and with different LIDARs demonstrate the wide applicability and practicality of our simulation platform. The implementation of the simulation platform is open-sourced on Github.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14102v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyan Li, Chang Wu, Yifei Yuan, Yuan You</dc:creator>
    </item>
    <item>
      <title>Efficient and Safe Contact-rich pHRI via Subtask Detection and Motion Estimation using Deep Learning</title>
      <link>https://arxiv.org/abs/2407.14161</link>
      <description>arXiv:2407.14161v1 Announce Type: new 
Abstract: This paper proposes an adaptive admittance controller for improving efficiency and safety in physical human-robot interaction (pHRI) tasks in small-batch manufacturing that involve contact with stiff environments, such as drilling, polishing, cutting, etc. We aim to minimize human effort and task completion time while maximizing precision and stability during the contact of the machine tool attached to the robot's end-effector with the workpiece. To this end, a two-layered learning-based human intention recognition mechanism is proposed, utilizing only the kinematic and kinetic data from the robot and two force sensors. A ``subtask detector" recognizes the human intent by estimating which phase of the task is being performed, e.g., \textit{Idle}, \textit{Tool-Attachment}, \textit{Driving}, and \textit{Contact}. Simultaneously, a ``motion estimator" continuously quantifies intent more precisely during the \textit{Driving} to predict when \textit{Contact} will begin. The controller is adapted online according to the subtask while allowing early adaptation before the \textit{Contact} to maximize precision and safety and prevent potential instabilities. Three sets of pHRI experiments were performed with multiple subjects under various conditions. Spring compression experiments were performed in virtual environments to train the data-driven models and validate the proposed adaptive system, and drilling experiments were performed in the physical world to test the proposed methods' efficacy in real-life scenarios. Experimental results show subtask classification accuracy of 84\% and motion estimation R\textsuperscript{2} score of 0.96. Furthermore, 57\% lower human effort was achieved during \textit{Driving} as well as 53\% lower oscillation amplitude at \textit{Contact} as a result of the proposed system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14161v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pouya P. Niaz, Engin Erzin, Cagatay Basdogan</dc:creator>
    </item>
    <item>
      <title>Stochastic Model Predictive Control with Optimal Linear Feedback for Mobile Robots in Dynamic Environments</title>
      <link>https://arxiv.org/abs/2407.14220</link>
      <description>arXiv:2407.14220v1 Announce Type: new 
Abstract: Robot navigation around humans can be a challenging problem since human movements are hard to predict. Stochastic model predictive control (MPC) can account for such uncertainties and approximately bound the probability of a collision to take place. In this paper, to counteract the rapidly growing human motion uncertainty over time, we incorporate state feedback in the stochastic MPC. This allows the robot to more closely track reference trajectories. To this end the feedback policy is left as a degree of freedom in the optimal control problem. The stochastic MPC with feedback is validated in simulation experiments and is compared against nominal MPC and stochastic MPC without feedback. The added computation time can be limited by reducing the number of additional variables for the feedback law with a small compromise in control performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14220v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunfan Gao, Florian Messerer, Niels van Duijkeren, Moritz Diehl</dc:creator>
    </item>
    <item>
      <title>Words2Contact: Identifying Support Contacts from Verbal Instructions Using Foundation Models</title>
      <link>https://arxiv.org/abs/2407.14229</link>
      <description>arXiv:2407.14229v1 Announce Type: new 
Abstract: This paper presents Words2Contact, a language-guided multi-contact placement pipeline leveraging large language models and vision language models. Our method is a key component for language-assisted teleoperation and human-robot cooperation, where human operators can instruct the robots where to place their support contacts before whole-body reaching or manipulation using natural language. Words2Contact transforms the verbal instructions of a human operator into contact placement predictions; it also deals with iterative corrections, until the human is satisfied with the contact location identified in the robot's field of view. We benchmark state-of-the-art LLMs and VLMs for size and performance in contact prediction. We demonstrate the effectiveness of the iterative correction process, showing that users, even naive, quickly learn how to instruct the system to obtain accurate locations. Finally, we validate Words2Contact in real-world experiments with the Talos humanoid robot, instructed by human operators to place support contacts on different locations and surfaces to avoid falling when reaching for distant objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14229v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dionis Totsila, Quentin Rouxel, Jean-Baptiste Mouret, Serena Ivaldi</dc:creator>
    </item>
    <item>
      <title>Neuromuscular Modeling for Locomotion with Wearable Assistive Robots -- A primer</title>
      <link>https://arxiv.org/abs/2407.14289</link>
      <description>arXiv:2407.14289v1 Announce Type: new 
Abstract: Wearable assistive robots (WR) for the lower extremity are extensively documented in literature. Various interfaces have been designed to control these devices during gait and balance activities. However, achieving seamless and intuitive control requires accurate modeling of the human neuromusculoskeletal (NMSK) system. Such modeling enables WR to anticipate user intentions and determine the necessary joint assistance. Despite the existence of controllers interfacing with the NMSK system, robust and generalizable techniques across different tasks remain scarce. Designing these novel controllers necessitates the combined expertise of neurophysiologists, who understand the physiology of movement initiation and generation, and biomechatronic engineers, who design and control devices that assist movement. This paper aims to bridge the gaps between these fields by presenting a primer on key concepts and the current state of the science in each area. We present three main sections: the neuromechanics of locomotion, neuromechanical models of movement, and existing neuromechanical controllers used in WR. Through these sections, we provide a comprehensive overview of seminal studies in the field, facilitating collaboration between neurophysiologists and biomechatronic engineers for future advances in wearable robotics for locomotion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14289v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohamed Irfan Refai, Huawei Wang, Antonio Gogeascoechea, Rafael Ornelas Kobayashi, Lucas A. Gaudio, Federica Damonte, Guillaume Durandau, Herman van der Kooij, Utku S. Yavuz, Massimo Sartori</dc:creator>
    </item>
    <item>
      <title>Foundation Models for Autonomous Robots in Unstructured Environments</title>
      <link>https://arxiv.org/abs/2407.14296</link>
      <description>arXiv:2407.14296v1 Announce Type: new 
Abstract: Automating activities through robots in unstructured environments, such as construction sites, has been a long-standing desire. However, the high degree of unpredictable events in these settings has resulted in far less adoption compared to more structured settings, such as manufacturing, where robots can be hard-coded or trained on narrowly defined datasets. Recently, pretrained foundation models, such as Large Language Models (LLMs), have demonstrated superior generalization capabilities by providing zero-shot solutions for problems do not present in the training data, proposing them as a potential solution for introducing robots to unstructured environments. To this end, this study investigates potential opportunities and challenges of pretrained foundation models from a multi-dimensional perspective. The study systematically reviews application of foundation models in two field of robotic and unstructured environment and then synthesized them with deliberative acting theory. Findings showed that linguistic capabilities of LLMs have been utilized more than other features for improving perception in human-robot interactions. On the other hand, findings showed that the use of LLMs demonstrated more applications in project management and safety in construction, and natural hazard detection in disaster management. Synthesizing these findings, we located the current state-of-the-art in this field on a five-level scale of automation, placing them at conditional automation. This assessment was then used to envision future scenarios, challenges, and solutions toward autonomous safe unstructured environments. Our study can be seen as a benchmark to track our progress toward that future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14296v1</guid>
      <category>cs.RO</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hossein Naderi, Alireza Shojaei</dc:creator>
    </item>
    <item>
      <title>Equivariant Symmetries for Aided Inertial Navigation</title>
      <link>https://arxiv.org/abs/2407.14297</link>
      <description>arXiv:2407.14297v1 Announce Type: new 
Abstract: Respecting the geometry of the underlying system and exploiting its symmetry have been driving concepts in deriving modern geometric filters for inertial navigation systems (INSs). Despite their success, the explicit treatment of inertial measurement unit (IMU) biases remains challenging, unveiling a gap in the current theory of filter design. In response to this gap, this dissertation builds upon the recent theory of equivariant systems to address and overcome the limitations in existing methodologies. The goal is to identify new symmetries of inertial navigation systems that include a geometric treatment of IMU biases and exploit them to design filtering algorithms that outperform state-of-the-art solutions in terms of accuracy, convergence rate, robustness, and consistency. This dissertation leverages the semi-direct product rule and introduces the tangent group for inertial navigation systems as the first equivariant symmetry that properly accounts for IMU biases. Based on that, we show that it is possible to derive an equivariant filter (EqF) algorithm with autonomous navigation error dynamics. The resulting filter demonstrates superior to state-of-the-art solutions. Through a comprehensive analysis of various symmetries of inertial navigation systems, we formalized the concept that every filter can be derived as an EqF with a specific choice of symmetry. This underlines the fundamental role of symmetry in determining filter performance. This dissertation advances the understanding of equivariant symmetries in the context of inertial navigation systems and serves as a basis for the next generation of equivariant estimators, marking a significant leap toward more reliable navigation solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14297v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Fornasier</dc:creator>
    </item>
    <item>
      <title>Complementary Learning for Real-World Model Failure Detection</title>
      <link>https://arxiv.org/abs/2407.14306</link>
      <description>arXiv:2407.14306v1 Announce Type: new 
Abstract: In real-world autonomous driving, deep learning models can experience performance degradation due to distributional shifts between the training data and the driving conditions encountered. As is typical in machine learning, it is difficult to acquire a large and potentially representative labeled test set to validate models in preparation for deployment in the wild. In this work, we introduce complementary learning, where we use learned characteristics from different training paradigms to detect model errors. We demonstrate our approach by learning semantic and predictive motion labels in point clouds in a supervised and self-supervised manner and detect and classify model discrepancies subsequently. We perform a large-scale qualitative analysis and present LidarCODA, the first dataset with labeled anomalies in lidar point clouds, for an extensive quantitative analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14306v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Bogdoll, Finn Sartoris, Vincent Geppert, Svetlana Pavlitska, J. Marius Z\"ollner</dc:creator>
    </item>
    <item>
      <title>Bio-inspired Soft Grippers for Biological Applications</title>
      <link>https://arxiv.org/abs/2407.14324</link>
      <description>arXiv:2407.14324v1 Announce Type: new 
Abstract: The field of bio-inspired soft grippers has emerged as a transformative area of research with profound implications for biomedical applications. This book chapter provides a comprehensive overview of the principles, developments, challenges, and prospects of soft grippers that draw inspiration from biological systems. Bio-inspired soft grippers have gained prominence due to their unique characteristics, including compliance, adaptability, and biocompatibility. They have revolutionized the way we approach biomedical tasks, offering safer interactions with delicate tissues and enabling complex operations that were once inconceivable with rigid tools. The chapter delves into the fundamental importance of soft grippers in biomedical contexts. It outlines their significance in surgeries, diagnostics, tissue engineering, and various medical interventions. Soft grippers have the capacity to mimic the intricate movements of biological organisms, facilitating minimally invasive procedures and enhancing patient outcomes. A historical perspective traces the evolution of soft grippers in biomedical research, highlighting key milestones and breakthroughs. From early attempts to emulate the dexterity of octopus tentacles to the latest advancements in soft lithography and biomaterials, the journey has been marked by ingenuity and collaboration across multiple disciplines. Motivations for adopting soft grippers in biomedical applications are explored, emphasizing their ability to reduce invasiveness, increase precision, and provide adaptability to complex anatomical structures. The requirements and challenges in designing grippers fit for medical contexts are outlined, encompassing biocompatibility, sterilization, control, and integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14324v1</guid>
      <category>cs.RO</category>
      <category>physics.med-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>CRC Press, a Taylor and Francis Group 2024</arxiv:journal_reference>
      <dc:creator>Rekha Raja, Ali Leylavi Shoushtari</dc:creator>
    </item>
    <item>
      <title>Deep Domain Adaptation Regression for Force Calibration of Optical Tactile Sensors</title>
      <link>https://arxiv.org/abs/2407.14380</link>
      <description>arXiv:2407.14380v1 Announce Type: new 
Abstract: Optical tactile sensors provide robots with rich force information for robot grasping in unstructured environments. The fast and accurate calibration of three-dimensional contact forces holds significance for new sensors and existing tactile sensors which may have incurred damage or aging. However, the conventional neural-network-based force calibration method necessitates a large volume of force-labeled tactile images to minimize force prediction errors, with the need for accurate Force/Torque measurement tools as well as a time-consuming data collection process. To address this challenge, we propose a novel deep domain-adaptation force calibration method, designed to transfer the force prediction ability from a calibrated optical tactile sensor to uncalibrated ones with various combinations of domain gaps, including marker presence, illumination condition, and elastomer modulus. Experimental results show the effectiveness of the proposed unsupervised force calibration method, with lowest force prediction errors of 0.102N (3.4\% in full force range) for normal force, and 0.095N (6.3\%) and 0.062N (4.1\%) for shear forces along the x-axis and y-axis, respectively. This study presents a promising, general force calibration methodology for optical tactile sensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14380v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuo Chen, Ni Ou, Jiaqi Jiang, Shan Luo</dc:creator>
    </item>
    <item>
      <title>Online learning of Koopman operator using streaming data from different dynamical regimes</title>
      <link>https://arxiv.org/abs/2407.13940</link>
      <description>arXiv:2407.13940v1 Announce Type: cross 
Abstract: The paper presents a framework for online learning of the Koopman operator using streaming data. Many complex systems for which data-driven modeling and control are sought provide streaming sensor data, the abundance of which can present computational challenges but cannot be ignored. Streaming data can intermittently sample dynamically different regimes or rare events which could be critical to model and control. Using ideas from subspace identification, we present a method where the Grassmannian distance between the subspace of an extended observability matrix and the streaming segment of data is used to assess the `novelty' of the data. If this distance is above a threshold, it is added to an archive and the Koopman operator is updated if not it is discarded. Therefore, our method identifies data from segments of trajectories of a dynamical system that are from different dynamical regimes, prioritizes minimizing the amount of data needed in updating the Koopman model and furthermore reduces the number of basis functions by learning them adaptively. Therefore, by dynamically adjusting the amount of data used and learning basis functions, our method optimizes the model's accuracy and the system order.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13940v1</guid>
      <category>eess.SY</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kartik Loya, Phanindra Tallapragada</dc:creator>
    </item>
    <item>
      <title>Component Selection for Craft Assembly Tasks</title>
      <link>https://arxiv.org/abs/2407.14001</link>
      <description>arXiv:2407.14001v1 Announce Type: cross 
Abstract: Inspired by traditional handmade crafts, where a person improvises assemblies based on the available objects, we formally introduce the Craft Assembly Task. It is a robotic assembly task that involves building an accurate representation of a given target object using the available objects, which do not directly correspond to its parts. In this work, we focus on selecting the subset of available objects for the final craft, when the given input is an RGB image of the target in the wild. We use a mask segmentation neural network to identify visible parts, followed by retrieving labelled template meshes. These meshes undergo pose optimization to determine the most suitable template. Then, we propose to simplify the parts of the transformed template mesh to primitive shapes like cuboids or cylinders. Finally, we design a search algorithm to find correspondences in the scene based on local and global proportions. We develop baselines for comparison that consider all possible combinations, and choose the highest scoring combination for common metrics used in foreground maps and mask accuracy. Our approach achieves comparable results to the baselines for two different scenes, and we show qualitative results for an implementation in a real-world scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14001v1</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vitor Hideyo Isume (Osaka University), Takuya Kiyokawa (Osaka University), Natsuki Yamanobe (AIST), Yukiyasu Domae (AIST), Weiwei Wan (Osaka University), Kensuke Harada (Osaka University, AIST)</dc:creator>
    </item>
    <item>
      <title>A Learning-based Adaptive Compliance Method for Symmetric Bi-manual Manipulation</title>
      <link>https://arxiv.org/abs/2303.15262</link>
      <description>arXiv:2303.15262v2 Announce Type: replace 
Abstract: Symmetric bi-manual manipulation is an essential skill in on-orbit operations due to its potent load capacity. Previous works have applied compliant control to maintain the stability of manipulations. However, traditional methods have viewed motion planning and compliant control as two separate modules, which can lead to conflicts with the simultaneous change of the desired trajectory and impedance parameters in the presence of external forces and disturbances. Additionally, the joint usage of these two modules requires experts to manually adjust parameters. To achieve high efficiency while enhancing adaptability, we propose a novel Learning-based Adaptive Compliance algorithm (LAC) that improves the efficiency and robustness of symmetric bi-manual manipulation. Specifically, the algorithm framework integrates desired trajectory generation and impedance-parameter adjustment under a unified framework to mitigate contradictions and improve efficiency. Second, we introduce a centralized Actor-Critic framework with LSTM networks preprocessing the force states, enhancing the synchronization of bi-manual manipulation. When evaluated in dual-arm peg-in-hole assembly experiments, our method outperforms baseline algorithms in terms of optimality and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.15262v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxue Cao, Wenbo Zhao, Shengjie Wang, Xiang Zheng, Wenke Ma, Zhaolei Wang, Tao Zhang</dc:creator>
    </item>
    <item>
      <title>Safety-Oriented Calibration and Evaluation of the Intelligent Driver Model</title>
      <link>https://arxiv.org/abs/2310.04259</link>
      <description>arXiv:2310.04259v2 Announce Type: replace 
Abstract: Many car-following models like the Intelligent Driver Model (IDM) incorporate important aspects of safety in their definitions, such as collision-free driving and keeping safe distances, implying that drivers are safety conscious when driving. Despite their safety-oriented nature, when calibrating and evaluating these models, the main objective of most studies is to find model parameters that minimize the error in observed measurements like spacing and speed while studies specifically focused on calibrating and evaluating unobserved safe behavior captured by the parameters of the model are scarce. Most studies on calibration and evaluation of the IDM do not check if the observed driving behavior (i.e. spacing) are within the model estimated unobserved safety thresholds (i.e. desired safety spacing) or what parameters are important for safety. This limits their application for safety driven traffic simulations. To fill this gap, this paper first proposes a simple metric to evaluate driver compliance with the safety thresholds of the IDM model. Specifically, we evaluate driver compliance to their desired safety spacing, speed and safe time gap. Next, a method to enforce compliance to the safety threshold during model calibration is proposed. The proposed compliance metric and the calibration approach is tested using Dutch highway trajectory data obtained from a driving simulator experiment and two drones. The results show that compliance to the IDM safety threshold greatly depends on braking capability with a median compliance between 38% and 90% of driving time, indicating that drivers can only partially follow the IDM safety threshold in reality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04259v2</guid>
      <category>cs.RO</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kingsley Adjenughwure, Arturo Tejada, Pedro F. V. Oliveira, Jeroen Hogema, Gerdien Klunder</dc:creator>
    </item>
    <item>
      <title>Multimodal Active Measurement for Human Mesh Recovery in Close Proximity</title>
      <link>https://arxiv.org/abs/2310.08116</link>
      <description>arXiv:2310.08116v3 Announce Type: replace 
Abstract: For physical human-robot interactions (pHRI), a robot needs to estimate the accurate body pose of a target person. However, in these pHRI scenarios, the robot cannot fully observe the target person's body with equipped cameras because the target person must be close to the robot for physical interaction. This close distance leads to severe truncation and occlusions and thus results in poor accuracy of human pose estimation. For better accuracy in this challenging environment, we propose an active measurement and sensor fusion framework of the equipped cameras with touch and ranging sensors such as 2D LiDAR. Touch and ranging sensor measurements are sparse but reliable and informative cues for localizing human body parts. In our active measurement process, camera viewpoints and sensor placements are dynamically optimized to measure body parts with higher estimation uncertainty, which is closely related to truncation or occlusion. In our sensor fusion process, assuming that the measurements of touch and ranging sensors are more reliable than the camera-based estimations, we fuse the sensor measurements to the camera-based estimated pose by aligning the estimated pose towards the measured points. Our proposed method outperformed previous methods on the standard occlusion benchmark with simulated active measurement. Furthermore, our method reliably estimated human poses using a real robot, even with practical constraints such as occlusion by blankets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08116v3</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takahiro Maeda, Keisuke Takeshita, Norimichi Ukita, Kazuhito Tanaka</dc:creator>
    </item>
    <item>
      <title>Osprey: Multi-Session Autonomous Aerial Mapping with LiDAR-based SLAM and Next Best View Planning</title>
      <link>https://arxiv.org/abs/2311.03484</link>
      <description>arXiv:2311.03484v3 Announce Type: replace 
Abstract: Aerial mapping systems are important for many surveying applications (e.g., industrial inspection or agricultural monitoring). Aerial platforms that can fly GPS-guided preplanned missions semi-autonomously are already widely available but fully autonomous systems can significantly improve efficiency. Autonomously mapping complex 3D structures requires a system that performs online mapping and mission planning. This paper presents Osprey, an autonomous aerial mapping system with state-of-the-art multi-session LiDAR-based mapping capabilities. It enables a non-expert operator to specify a bounded target area that the aerial platform can then map autonomously over multiple flights. Field experiments with Osprey demonstrate that this system can achieve greater map coverage of large industrial sites than manual surveys with a pilot-flown aerial platform or a terrestrial laser scanner (TLS). Three sites, with a total ground coverage of $2528$ m$^2$ and a maximum height of $27$ m, were mapped in separate missions using $112$ minutes of autonomous flight time. True colour maps were created from images captured by Osprey using pointcloud and NeRF reconstruction methods. These maps provide useful data for structural inspection tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03484v3</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TFR.2024.3432031</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Field Robotics (2024)</arxiv:journal_reference>
      <dc:creator>Rowan Border, Nived Chebrolu, Yifu Tao, Jonathan D. Gammell, Maurice Fallon</dc:creator>
    </item>
    <item>
      <title>R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robotic Ecosystems via Proposal Refinement</title>
      <link>https://arxiv.org/abs/2403.11567</link>
      <description>arXiv:2403.11567v3 Announce Type: replace 
Abstract: We introduce a novel approach for scalable domain adaptation in cloud robotics scenarios where robots rely on third-party AI inference services powered by large pre-trained deep neural networks. Our method is based on a downstream proposal-refinement stage running locally on the robots, exploiting a new lightweight DNN architecture, R2SNet. This architecture aims to mitigate performance degradation from domain shifts by adapting the object detection process to the target environment, focusing on relabeling, rescoring, and suppression of bounding-box proposals. Our method allows for local execution on robots, addressing the scalability challenges of domain adaptation without incurring significant computational costs. Real-world results on mobile service robots performing door detection show the effectiveness of the proposed method in achieving scalable domain adaptation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11567v3</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Antonazzi, Matteo Luperto, N. Alberto Borghese, Nicola Basilico</dc:creator>
    </item>
    <item>
      <title>Frontier-Based Exploration for Multi-Robot Rendezvous in Communication-Restricted Unknown Environments</title>
      <link>https://arxiv.org/abs/2403.11617</link>
      <description>arXiv:2403.11617v3 Announce Type: replace 
Abstract: Multi-robot rendezvous and exploration are fundamental challenges in the domain of mobile robotic systems. This paper addresses multi-robot rendezvous within an initially unknown environment where communication is only possible after the rendezvous. Traditionally, exploration has been focused on rapidly mapping the environment, often leading to suboptimal rendezvous performance in later stages. We adapt a standard frontier-based exploration technique to integrate exploration and rendezvous into a unified strategy, with a mechanism that allows robots to re-visit previously explored regions thus enhancing rendezvous opportunities. We validate our approach in 3D realistic simulations using ROS, showcasing its effectiveness in achieving faster rendezvous times compared to exploration strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11617v3</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauro Tellaroli, Matteo Luperto, Michele Antonazzi, Nicola Basilico</dc:creator>
    </item>
    <item>
      <title>How Generalizable Is My Behavior Cloning Policy? A Statistical Approach to Trustworthy Performance Evaluation</title>
      <link>https://arxiv.org/abs/2405.05439</link>
      <description>arXiv:2405.05439v2 Announce Type: replace 
Abstract: With the rise of stochastic generative models in robot policy learning, end-to-end visuomotor policies are increasingly successful at solving complex tasks by learning from human demonstrations. Nevertheless, since real-world evaluation costs afford users only a small number of policy rollouts, it remains a challenge to accurately gauge the performance of such policies. This is exacerbated by distribution shifts causing unpredictable changes in performance during deployment. To rigorously evaluate behavior cloning policies, we present a framework that provides a tight lower-bound on robot performance in an arbitrary environment, using a minimal number of experimental policy rollouts. Notably, by applying the standard stochastic ordering to robot performance distributions, we provide a worst-case bound on the entire distribution of performance (via bounds on the cumulative distribution function) for a given task. We build upon established statistical results to ensure that the bounds hold with a user-specified confidence level and tightness, and are constructed from as few policy rollouts as possible. In experiments we evaluate policies for visuomotor manipulation in both simulation and hardware. Specifically, we (i) empirically validate the guarantees of the bounds in simulated manipulation settings, (ii) find the degree to which a learned policy deployed on hardware generalizes to new real-world environments, and (iii) rigorously compare two policies tested in out-of-distribution settings. Our experimental data, code, and implementation of confidence bounds are open-source.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05439v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph A. Vincent, Haruki Nishimura, Masha Itkina, Paarth Shah, Mac Schwager, Thomas Kollar</dc:creator>
    </item>
    <item>
      <title>Talk2Radar: Bridging Natural Language with 4D mmWave Radar for 3D Referring Expression Comprehension</title>
      <link>https://arxiv.org/abs/2405.12821</link>
      <description>arXiv:2405.12821v2 Announce Type: replace 
Abstract: Embodied perception is essential for intelligent vehicles and robots in interactive environmental understanding. However, these advancements primarily focus on vision, with limited attention given to using 3D modeling sensors, restricting a comprehensive understanding of objects in response to prompts containing qualitative and quantitative queries. Recently, as a promising automotive sensor with affordable cost, 4D millimeter-wave radars provide denser point clouds than conventional radars and perceive both semantic and physical characteristics of objects, thereby enhancing the reliability of perception systems. To foster the development of natural language-driven context understanding in radar scenes for 3D visual grounding, we construct the first dataset, Talk2Radar, which bridges these two modalities for 3D Referring Expression Comprehension (REC). Talk2Radar contains 8,682 referring prompt samples with 20,558 referred objects. Moreover, we propose a novel model, T-RadarNet, for 3D REC on point clouds, achieving State-Of-The-Art (SOTA) performance on the Talk2Radar dataset compared to counterparts. Deformable-FPN and Gated Graph Fusion are meticulously designed for efficient point cloud feature modeling and cross-modal fusion between radar and text features, respectively. Comprehensive experiments provide deep insights into radar-based 3D REC. We release our project at https://github.com/GuanRunwei/Talk2Radar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12821v2</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runwei Guan, Ruixiao Zhang, Ningwei Ouyang, Jianan Liu, Ka Lok Man, Xiaohao Cai, Ming Xu, Jeremy Smith, Eng Gee Lim, Yutao Yue, Hui Xiong</dc:creator>
    </item>
    <item>
      <title>HPHS: Hierarchical Planning based on Hybrid Frontier Sampling for Unknown Environments Exploration</title>
      <link>https://arxiv.org/abs/2407.10660</link>
      <description>arXiv:2407.10660v2 Announce Type: replace 
Abstract: Rapid sampling from the environment to acquire available frontier points and timely incorporating them into subsequent planning to reduce fragmented regions are critical to improve the efficiency of autonomous exploration. We propose HPHS, a fast and effective method for the autonomous exploration of unknown environments. In this work, we efficiently sample frontier points directly from the LiDAR data and the local map around the robot, while exploiting a hierarchical planning strategy to provide the robot with a global perspective. The hierarchical planning framework divides the updated environment into multiple subregions and arranges the order of access to them by considering the overall revenue of the global path. The combination of the hybrid frontier sampling method and hierarchical planning strategy reduces the complexity of the planning problem and mitigates the issue of region remnants during the exploration process. Detailed simulation and real-world experiments demonstrate the effectiveness and efficiency of our approach in various aspects. The source code will be released to benefit the further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10660v2</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shijun Long, Ying Li, Chenming Wu, Bin Xu, Wei Fan</dc:creator>
    </item>
    <item>
      <title>HEROS: Hierarchical Exploration with Online Subregion Updating for 3D Environment Coverage</title>
      <link>https://arxiv.org/abs/2407.11326</link>
      <description>arXiv:2407.11326v2 Announce Type: replace 
Abstract: We present an autonomous exploration system for efficient coverage of unknown environments. First, a rapid environment preprocessing method is introduced to provide environmental information for subsequent exploration planning. Then, the whole exploration space is divided into multiple subregion cells, each with varying levels of detail. The subregion cells are capable of decomposition and updating online, effectively characterizing dynamic unknown regions with variable resolution. Finally, the hierarchical planning strategy treats subregions as basic planning units and computes an efficient global coverage path. Guided by the global path, the local path that sequentially visits the viewpoint set is refined to provide an executable path for the robot. This hierarchical planning from coarse to fine steps reduces the complexity of the planning scheme while improving exploration efficiency. The proposed method is compared with state-of-art methods in benchmark environments. Our approach demonstrates superior efficiency in completing exploration while using lower computational resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11326v2</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shijun Long, Ying Li, Chenming Wu, Bin Xu, Wei Fan</dc:creator>
    </item>
    <item>
      <title>Hyp2Nav: Hyperbolic Planning and Curiosity for Crowd Navigation</title>
      <link>https://arxiv.org/abs/2407.13567</link>
      <description>arXiv:2407.13567v2 Announce Type: replace 
Abstract: Autonomous robots are increasingly becoming a strong fixture in social environments. Effective crowd navigation requires not only safe yet fast planning, but should also enable interpretability and computational efficiency for working in real-time on embedded devices. In this work, we advocate for hyperbolic learning to enable crowd navigation and we introduce Hyp2Nav. Different from conventional reinforcement learning-based crowd navigation methods, Hyp2Nav leverages the intrinsic properties of hyperbolic geometry to better encode the hierarchical nature of decision-making processes in navigation tasks. We propose a hyperbolic policy model and a hyperbolic curiosity module that results in effective social navigation, best success rates, and returns across multiple simulation settings, using up to 6 times fewer parameters than competitor state-of-the-art models. With our approach, it becomes even possible to obtain policies that work in 2-dimensional embedding spaces, opening up new possibilities for low-resource crowd navigation and model interpretability. Insightfully, the internal hyperbolic representation of Hyp2Nav correlates with how much attention the robot pays to the surrounding crowds, e.g. due to multiple people occluding its pathway or to a few of them showing colliding plans, rather than to its own planned route.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13567v2</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guido Maria D'Amely di Melendugno, Alessandro Flaborea, Pascal Mettes, Fabio Galasso</dc:creator>
    </item>
    <item>
      <title>Introduction to Online Nonstochastic Control</title>
      <link>https://arxiv.org/abs/2211.09619</link>
      <description>arXiv:2211.09619v3 Announce Type: replace-cross 
Abstract: This text presents an introduction to an emerging paradigm in control of dynamical systems and differentiable reinforcement learning called online nonstochastic control. The new approach applies techniques from online convex optimization and convex relaxations to obtain new methods with provable guarantees for classical settings in optimal and robust control.
  The primary distinction between online nonstochastic control and other frameworks is the objective. In optimal control, robust control, and other control methodologies that assume stochastic noise, the goal is to perform comparably to an offline optimal strategy. In online nonstochastic control, both the cost functions as well as the perturbations from the assumed dynamical model are chosen by an adversary. Thus the optimal policy is not defined a priori. Rather, the target is to attain low regret against the best policy in hindsight from a benchmark class of policies.
  This objective suggests the use of the decision making framework of online convex optimization as an algorithmic methodology. The resulting methods are based on iterative mathematical optimization algorithms, and are accompanied by finite-time regret and computational complexity guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.09619v3</guid>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elad Hazan, Karan Singh</dc:creator>
    </item>
    <item>
      <title>FoundPose: Unseen Object Pose Estimation with Foundation Features</title>
      <link>https://arxiv.org/abs/2311.18809</link>
      <description>arXiv:2311.18809v2 Announce Type: replace-cross 
Abstract: We propose FoundPose, a model-based method for 6D pose estimation of unseen objects from a single RGB image. The method can quickly onboard new objects using their 3D models without requiring any object- or task-specific training. In contrast, existing methods typically pre-train on large-scale, task-specific datasets in order to generalize to new objects and to bridge the image-to-model domain gap. We demonstrate that such generalization capabilities can be observed in a recent vision foundation model trained in a self-supervised manner. Specifically, our method estimates the object pose from image-to-model 2D-3D correspondences, which are established by matching patch descriptors from the recent DINOv2 model between the image and pre-rendered object templates. We find that reliable correspondences can be established by kNN matching of patch descriptors from an intermediate DINOv2 layer. Such descriptors carry stronger positional information than descriptors from the last layer, and we show their importance when semantic information is ambiguous due to object symmetries or a lack of texture. To avoid establishing correspondences against all object templates, we develop an efficient template retrieval approach that integrates the patch descriptors into the bag-of-words representation and can promptly propose a handful of similarly looking templates. Additionally, we apply featuremetric alignment to compensate for discrepancies in the 2D-3D correspondences caused by coarse patch sampling. The resulting method noticeably outperforms existing RGB methods for refinement-free pose estimation on the standard BOP benchmark with seven diverse datasets and can be seamlessly combined with an existing render-and-compare refinement method to achieve RGB-only state-of-the-art results. Project page: evinpinar.github.io/foundpose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18809v2</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evin P{\i}nar \"Ornek, Yann Labb\'e, Bugra Tekin, Lingni Ma, Cem Keskin, Christian Forster, Tomas Hodan</dc:creator>
    </item>
    <item>
      <title>Scene-Graph ViT: End-to-End Open-Vocabulary Visual Relationship Detection</title>
      <link>https://arxiv.org/abs/2403.14270</link>
      <description>arXiv:2403.14270v2 Announce Type: replace-cross 
Abstract: Visual relationship detection aims to identify objects and their relationships in images. Prior methods approach this task by adding separate relationship modules or decoders to existing object detection architectures. This separation increases complexity and hinders end-to-end training, which limits performance. We propose a simple and highly efficient decoder-free architecture for open-vocabulary visual relationship detection. Our model consists of a Transformer-based image encoder that represents objects as tokens and models their relationships implicitly. To extract relationship information, we introduce an attention mechanism that selects object pairs likely to form a relationship. We provide a single-stage recipe to train this model on a mixture of object and relationship detection data. Our approach achieves state-of-the-art relationship detection performance on Visual Genome and on the large-vocabulary GQA benchmark at real-time inference speeds. We provide ablations, real-world qualitative examples, and analyses of zero-shot performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14270v2</guid>
      <category>cs.CV</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Salzmann, Markus Ryll, Alex Bewley, Matthias Minderer</dc:creator>
    </item>
    <item>
      <title>Diffusion Models for Offline Multi-agent Reinforcement Learning with Safety Constraints</title>
      <link>https://arxiv.org/abs/2407.00741</link>
      <description>arXiv:2407.00741v4 Announce Type: replace-cross 
Abstract: In recent advancements in Multi-agent Reinforcement Learning (MARL), its application has extended to various safety-critical scenarios. However, most methods focus on online learning, which presents substantial risks when deployed in real-world settings. Addressing this challenge, we introduce an innovative framework integrating diffusion models within the MARL paradigm. This approach notably enhances the safety of actions taken by multiple agents through risk mitigation while modeling coordinated action. Our framework is grounded in the Centralized Training with Decentralized Execution (CTDE) architecture, augmented by a Diffusion Model for prediction trajectory generation. Additionally, we incorporate a specialized algorithm to further ensure operational safety. We evaluate our model against baselines on the DSRL benchmark. Experiment results demonstrate that our model not only adheres to stringent safety constraints but also achieves superior performance compared to existing methodologies. This underscores the potential of our approach in advancing the safety and efficacy of MARL in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00741v4</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianuo Huang</dc:creator>
    </item>
    <item>
      <title>An efficient algorithm for solving linear equality-constrained LQR problems</title>
      <link>https://arxiv.org/abs/2407.05433</link>
      <description>arXiv:2407.05433v3 Announce Type: replace-cross 
Abstract: We present a new algorithm for solving linear-quadratic regulator (LQR) problems with linear equality constraints, also known as constrained LQR (CLQR) problems. Our method's sequential runtime is linear in the number of stages and constraints, and its parallel runtime is logarithmic in the number of stages. The main technical contribution of this paper is the derivation of parallelizable techniques for eliminating the linear equality constraints while preserving the standard positive (semi-)definiteness requirements of LQR problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05433v3</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jo\~ao Sousa-Pinto, Dominique Orban</dc:creator>
    </item>
    <item>
      <title>Exploring the Causality of End-to-End Autonomous Driving</title>
      <link>https://arxiv.org/abs/2407.06546</link>
      <description>arXiv:2407.06546v2 Announce Type: replace-cross 
Abstract: Deep learning-based models are widely deployed in autonomous driving areas, especially the increasingly noticed end-to-end solutions. However, the black-box property of these models raises concerns about their trustworthiness and safety for autonomous driving, and how to debug the causality has become a pressing concern. Despite some existing research on the explainability of autonomous driving, there is currently no systematic solution to help researchers debug and identify the key factors that lead to the final predicted action of end-to-end autonomous driving. In this work, we propose a comprehensive approach to explore and analyze the causality of end-to-end autonomous driving. First, we validate the essential information that the final planning depends on by using controlled variables and counterfactual interventions for qualitative analysis. Then, we quantitatively assess the factors influencing model decisions by visualizing and statistically analyzing the response of key model inputs. Finally, based on the comprehensive study of the multi-factorial end-to-end autonomous driving system, we have developed a strong baseline and a tool for exploring causality in the close-loop simulator CARLA. It leverages the essential input sources to obtain a well-designed model, resulting in highly competitive capabilities. As far as we know, our work is the first to unveil the mystery of end-to-end autonomous driving and turn the black box into a white one. Thorough close-loop experiments demonstrate that our method can be applied to end-to-end autonomous driving solutions for causality debugging. Code will be available at https://github.com/bdvisl/DriveInsight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06546v2</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiankun Li, Hao Li, Jiangjiang Liu, Zhikang Zou, Xiaoqing Ye, Fan Wang, Jizhou Huang, Hua Wu, Haifeng Wang</dc:creator>
    </item>
    <item>
      <title>Fisheye-Calib-Adapter: An Easy Tool for Fisheye Camera Model Conversion</title>
      <link>https://arxiv.org/abs/2407.12405</link>
      <description>arXiv:2407.12405v2 Announce Type: replace-cross 
Abstract: The increasing necessity for fisheye cameras in fields such as robotics and autonomous driving has led to the proposal of various fisheye camera models. While the evolution of camera models has facilitated the development of diverse systems in the field, the lack of adaptation between different fisheye camera models means that recalibration is always necessary, which is cumbersome. This paper introduces a conversion tool for various previously proposed fisheye camera models. It is user-friendly, simple, yet extremely fast and accurate, offering conversion capabilities for a broader range of models compared to existing tools. We have verified that models converted using our system perform correctly in applications such as SLAM. By utilizing our system, researchers can obtain output parameters directly from input parameters without the need for an image set and any recalibration processes, thus serving as a bridge across different fisheye camera models in various research fields. We provide our system as an open source tool available at: https://github.com/eowjd0512/fisheye-calib-adapter</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12405v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sangjun Lee</dc:creator>
    </item>
    <item>
      <title>The need of a self for self-driving cars a theoretical model applying homeostasis to self driving</title>
      <link>https://arxiv.org/abs/2407.12795</link>
      <description>arXiv:2407.12795v2 Announce Type: replace-cross 
Abstract: This paper explores the concept of creating a "self" for self-driving cars through a homeostatic architecture designed to enhance their autonomy, safety, and efficiency. The proposed system integrates inward focused sensors to monitor the car's internal state, such as the condition of its metal bodywork, wheels, engine, and battery, establishing a baseline homeostatic state representing optimal functionality. Outward facing sensors, like cameras and LIDAR, are then interpreted via their impact on the car's homeostatic state by quantifying deviations from homeostasis. This contrasts with the approach of trying to make cars "see" reality in a similar way to humans and identify elements in their reality in the same way humans. Virtual environments would be leveraged to accelerate training. Additionally, cars are programmed to communicate and share experiences via blockchain technology, learning from each other's mistakes while maintaining individualized training models. A dedicated language for self-driving cars is proposed to enable nuanced interpretation and response to environmental data. This architecture allows self-driving cars to dynamically adjust their behavior based on internal and external feedback, promoting cooperation and continuous improvement. The study concludes by discussing the broader implications for AI development, potential real-world applications, and future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12795v2</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Schmalzried</dc:creator>
    </item>
  </channel>
</rss>
