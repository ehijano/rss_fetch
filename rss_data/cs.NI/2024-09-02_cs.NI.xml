<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Sep 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>AdapShare: An RL-Based Dynamic Spectrum Sharing Solution for O-RAN</title>
      <link>https://arxiv.org/abs/2408.16842</link>
      <description>arXiv:2408.16842v1 Announce Type: new 
Abstract: The Open Radio Access Network (O-RAN) initiative, characterized by open interfaces and AI/ML-capable RAN Intelligent Controller (RIC), facilitates effective spectrum sharing among RANs. In this context, we introduce AdapShare, an ORAN-compatible solution leveraging Reinforcement Learning (RL) for intent-based spectrum management, with the primary goal of minimizing resource surpluses or deficits in RANs. By employing RL agents, AdapShare intelligently learns network demand patterns and uses them to allocate resources. We demonstrate the efficacy of AdapShare in the spectrum sharing scenario between LTE and NR networks, incorporating real-world LTE resource usage data and synthetic NR usage data to demonstrate its practical use. We use the average surplus or deficit and fairness index to measure the system's performance in various scenarios. AdapShare outperforms a quasi-static resource allocation scheme based on long-term network demand statistics, particularly when available resources are scarce or exceed the aggregate demand from the networks. Lastly, we present a high-level O-RAN compatible architecture using RL agents, which demonstrates the seamless integration of AdapShare into real-world deployment scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16842v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sneihil Gopal, David Griffith, Richard A. Rouil, Chunmei Liu</dc:creator>
    </item>
    <item>
      <title>Characterizing User Platforms for Video Streaming in Broadband Networks</title>
      <link>https://arxiv.org/abs/2408.16995</link>
      <description>arXiv:2408.16995v1 Announce Type: new 
Abstract: Internet Service Providers (ISPs) bear the brunt of being the first port of call for poor video streaming experience. ISPs can benefit from knowing the user's device type (e.g., Android, iOS) and software agent (e.g., native app, Chrome) to troubleshoot platform-specific issues, plan capacity and create custom bundles. Unfortunately, encryption and NAT have limited ISPs' visibility into user platforms across video streaming providers. We develop a methodology to identify user platforms for video streams from four popular providers, namely YouTube, Netflix, Disney, and Amazon, by analyzing network traffic in real-time. First, we study the anatomy of the connection establishment process to show how TCP/QUIC and TLS handshakes vary across user platforms. We then develop a classification pipeline that uses 62 attributes extracted from the handshake messages to determine the user device and software agent of video flows with over 96% accuracy. Our method is evaluated and deployed in a large campus network (mimicking a residential broadband network) serving users including dormitory residents. Analysis of 100+ million video streams over a four-month period reveals insights into the mix of user platforms across the video providers, variations in bandwidth consumption across operating systems and browsers, and differences in peak hours of usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16995v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3646547.3688435</arxiv:DOI>
      <dc:creator>Yifan Wang, Minzhao Lyu, Vijay Sivaraman</dc:creator>
    </item>
    <item>
      <title>Deadline and Priority Constrained Immersive Video Streaming Transmission Scheduling</title>
      <link>https://arxiv.org/abs/2408.17028</link>
      <description>arXiv:2408.17028v1 Announce Type: new 
Abstract: Deadline-aware transmission scheduling in immersive video streaming is crucial. The objective is to guarantee that at least a certain block in multi-links is fully delivered within their deadlines, which is referred to as delivery ratio. Compared with existing models that focus on maximizing throughput and ultra-low latency, which makes bandwidth resource allocation and user satisfaction locally optimized, immersive video streaming needs to guarantee more high-priority block delivery within personalized deadlines. In this paper, we propose a deadline and priority-constrained immersive video streaming transmission scheduling scheme. It builds an accurate bandwidth prediction model that can sensitively assist scheduling decisions. It divides video streaming into various media elements and performs scheduling based on the user's personalized latency sensitivity thresholds and the media element's priority. We evaluate our scheme via trace-driven simulations. Compared with existing models, the results further demonstrate the superiority of our scheme with 12{\%}-31{\%} gains in quality of experience (QoE).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17028v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tongtong Feng, Qi Qi, Bo He, Jingyu Wang</dc:creator>
    </item>
    <item>
      <title>PIB: Prioritized Information Bottleneck Framework for Collaborative Edge Video Analytics</title>
      <link>https://arxiv.org/abs/2408.17047</link>
      <description>arXiv:2408.17047v1 Announce Type: new 
Abstract: Collaborative edge sensing systems, particularly in collaborative perception systems in autonomous driving, can significantly enhance tracking accuracy and reduce blind spots with multi-view sensing capabilities. However, their limited channel capacity and the redundancy in sensory data pose significant challenges, affecting the performance of collaborative inference tasks. To tackle these issues, we introduce a Prioritized Information Bottleneck (PIB) framework for collaborative edge video analytics. We first propose a priority-based inference mechanism that jointly considers the signal-to-noise ratio (SNR) and the camera's coverage area of the region of interest (RoI). To enable efficient inference, PIB reduces video redundancy in both spatial and temporal domains and transmits only the essential information for the downstream inference tasks. This eliminates the need to reconstruct videos on the edge server while maintaining low latency. Specifically, it derives compact, task-relevant features by employing the deterministic information bottleneck (IB) method, which strikes a balance between feature informativeness and communication costs. Given the computational challenges caused by IB-based objectives with high-dimensional data, we resort to variational approximations for feasible optimization. Compared to TOCOM-TEM, JPEG, and HEVC, PIB achieves an improvement of up to 15.1\% in mean object detection accuracy (MODA) and reduces communication costs by 66.7% when edge cameras experience poor channel conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17047v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengru Fang, Senkang Hu, Liyan Yang, Yiqin Deng, Xianhao Chen, Yuguang Fang</dc:creator>
    </item>
    <item>
      <title>Reasoning AI Performance Degradation in 6G Networks with Large Language Models</title>
      <link>https://arxiv.org/abs/2408.17097</link>
      <description>arXiv:2408.17097v1 Announce Type: new 
Abstract: The integration of Artificial Intelligence (AI) within 6G networks is poised to revolutionize connectivity, reliability, and intelligent decision-making. However, the performance of AI models in these networks is crucial, as any decline can significantly impact network efficiency and the services it supports. Understanding the root causes of performance degradation is essential for maintaining optimal network functionality. In this paper, we propose a novel approach to reason about AI model performance degradation in 6G networks using the Large Language Models (LLMs) empowered Chain-of-Thought (CoT) method. Our approach employs an LLM as a ''teacher'' model through zero-shot prompting to generate teaching CoT rationales, followed by a CoT ''student'' model that is fine-tuned by the generated teaching data for learning to reason about performance declines. The efficacy of this model is evaluated in a real-world scenario involving a real-time 3D rendering task with multi-Access Technologies (mATs) including WiFi, 5G, and LiFi for data transmission. Experimental results show that our approach achieves over 97% reasoning accuracy on the built test questions, confirming the validity of our collected dataset and the effectiveness of the LLM-CoT method. Our findings highlight the potential of LLMs in enhancing the reliability and efficiency of 6G networks, representing a significant advancement in the evolution of AI-native network infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17097v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liming Huang, Yulei Wu, Dimitra Simeonidou</dc:creator>
    </item>
    <item>
      <title>Time varying channel estimation for RIS assisted network with outdated CSI: Looking beyond coherence time</title>
      <link>https://arxiv.org/abs/2408.17128</link>
      <description>arXiv:2408.17128v1 Announce Type: new 
Abstract: The channel estimation (CE) overhead for unstructured multipath-rich channels increases linearly with the number of reflective elements of reconfigurable intelligent surface (RIS). This results in a significant portion of the channel coherence time being spent on CE, reducing data communication time. Furthermore, due to the mobility of the user equipment (UE) and the time consumed during CE, the estimated channel state information (CSI) may become outdated during actual data communication. In recent studies, the timing for CE has been primarily determined based on the coherence time interval, which is dependent on the velocity of the UE. However, the effect of the current channel condition and pathloss of the UEs can also be utilized to control the duration between successive CE to reduce the overhead while still maintaining the quality of service. Furthermore, for muti-user systems, the appropriate coherence time intervals of different users may be different depending on their velocities. Therefore CE carried out ignoring the difference in coherence time of different UEs may result in the estimated CSI being detrimentally outdated for some users. In contrast, others may not have sufficient time for data communication. To this end, based on the throughput analysis on outdated CSI, an algorithm has been designed to dynamically predict the next time instant for CE after the current CSI acquisition. In the first step, optimal RIS phase shifts to maximise channel gain is computed. Based on this and the amount of degradation of SINR due to outdated CSI, transmit powers are allocated for the UEs and finally the next time instant for CE is predicted such that the aggregated throughput is maximized. Simulation results confirm that our proposed algorithm outperforms the coherence time-based strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17128v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Souvik Deb, Sasthi C. Ghosh</dc:creator>
    </item>
    <item>
      <title>Secure Integration of 5G in Industrial Networks: State of the Art, Challenges and Opportunities</title>
      <link>https://arxiv.org/abs/2408.16833</link>
      <description>arXiv:2408.16833v1 Announce Type: cross 
Abstract: The industrial landscape is undergoing a significant transformation, moving away from traditional wired fieldbus networks to cutting-edge 5G mobile networks. This transition, extending from local applications to company-wide use and spanning multiple factories, is driven by the promise of low-latency communication and seamless connectivity for various devices in industrial settings. However, besides these tremendous benefits, the integration of 5G as the communication infrastructure in industrial networks introduces a new set of risks and threats to the security of industrial systems. The inherent complexity of 5G systems poses unique challenges for ensuring a secure integration, surpassing those encountered with any technology previously utilized in industrial networks. Most importantly, the distinct characteristics of industrial networks, such as real-time operation, required safety guarantees, and high availability requirements, further complicate this task. As the industrial transition from wired to wireless networks is a relatively new concept, a lack of guidance and recommendations on securely integrating 5G renders many industrial systems vulnerable and exposed to threats associated with 5G. To address this situation, in this paper, we summarize the state-of-the-art and derive a set of recommendations for the secure integration of 5G into industrial networks based on a thorough analysis of the research landscape. Furthermore, we identify opportunities to utilize 5G to further enhance security and indicate remaining challenges, potentially identifying future academic potential</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16833v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sotiris Michaelides, Thomas Vogt, Martin Henze</dc:creator>
    </item>
    <item>
      <title>Coverage Analysis of Multi-Environment Q-Learning Algorithms for Wireless Network Optimization</title>
      <link>https://arxiv.org/abs/2408.16882</link>
      <description>arXiv:2408.16882v1 Announce Type: cross 
Abstract: Q-learning is widely used to optimize wireless networks with unknown system dynamics. Recent advancements include ensemble multi-environment hybrid Q-learning algorithms, which utilize multiple Q-learning algorithms across structurally related but distinct Markovian environments and outperform existing Q-learning algorithms in terms of accuracy and complexity in large-scale wireless networks. We herein conduct a comprehensive coverage analysis to ensure optimal data coverage conditions for these algorithms. Initially, we establish upper bounds on the expectation and variance of different coverage coefficients. Leveraging these bounds, we present an algorithm for efficient initialization of these algorithms. We test our algorithm on two distinct real-world wireless networks. Numerical simulations show that our algorithm can achieve %50 less policy error and %40 less runtime complexity than state-of-the-art reinforcement learning algorithms. Furthermore, our algorithm exhibits robustness to changes in network settings and parameters. We also numerically validate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16882v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Talha Bozkus, Urbashi Mitra</dc:creator>
    </item>
    <item>
      <title>Manipulating OpenFlow Link Discovery Packet Forwarding for Topology Poisoning</title>
      <link>https://arxiv.org/abs/2408.16940</link>
      <description>arXiv:2408.16940v1 Announce Type: cross 
Abstract: Software-defined networking (SDN) is a centralized, dynamic, and programmable network management technology that enables flexible traffic control and scalability. SDN facilitates network administration through a centralized view of the underlying physical topology; tampering with this topology view can result in catastrophic damage to network management and security. To underscore this issue, we introduce Marionette, a new topology poisoning technique that manipulates OpenFlow link discovery packet forwarding to alter topology information. Our approach exposes an overlooked yet widespread attack vector, distinguishing itself from traditional link fabrication attacks that tamper, spoof, or relay discovery packets at the data plane. Unlike localized attacks observed in existing methods, our technique introduces a globalized topology poisoning attack that leverages control privileges. Marionette implements a reinforcement learning algorithm to compute a poisoned topology target, and injects flow entries to achieve a long-lived stealthy attack. Our evaluation shows that Marionette successfully attacks five open-source controllers and nine OpenFlow-based discovery protocols. Marionette overcomes the state-of-the-art topology poisoning defenses, showcasing a new class of topology poisoning that initiates on the control plane. This security vulnerability was ethically disclosed to OpenDaylight, and CVE-2024-37018 has been assigned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16940v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingming Chen, Thomas La Porta, Teryl Taylor, Frederico Araujo, Trent Jaeger</dc:creator>
    </item>
    <item>
      <title>Hybridizing Base-Line 2D-CNN Model with Cat Swarm Optimization for Enhanced Advanced Persistent Threat Detection</title>
      <link>https://arxiv.org/abs/2408.17307</link>
      <description>arXiv:2408.17307v1 Announce Type: cross 
Abstract: In the realm of cyber-security, detecting Advanced Persistent Threats (APTs) remains a formidable challenge due to their stealthy and sophisticated nature. This research paper presents an innovative approach that leverages Convolutional Neural Networks (CNNs) with a 2D baseline model, enhanced by the cutting-edge Cat Swarm Optimization (CSO) algorithm, to significantly improve APT detection accuracy. By seamlessly integrating the 2D-CNN baseline model with CSO, we unlock the potential for unprecedented accuracy and efficiency in APT detection. The results unveil an impressive accuracy score of $98.4\%$, marking a significant enhancement in APT detection across various attack stages, illuminating a path forward in combating these relentless and sophisticated threats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17307v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ali M. Bakhiet, Salah A. Aly</dc:creator>
    </item>
    <item>
      <title>Beam-align: distributed user association for mmWave networks with multi-connectivity</title>
      <link>https://arxiv.org/abs/2206.13166</link>
      <description>arXiv:2206.13166v2 Announce Type: replace 
Abstract: Since the spectrum below 6 GHz bands is insufficient to meet the high bandwidth requirements of 5G use cases, 5G networks expand their operation to mmWave bands. However, operation at these bands has to cope with a high penetration loss and susceptibility to blocking objects. Beamforming and multi-connectivity (MC) can together mitigate these challenges. But, to design such an optimal user association scheme leveraging these two features is non-trivial and computationally expensive. Previous studies either considered a fixed MC degree for all users or overlooked beamforming. Driven by the question what is the optimal degree of MC for each user in a mmWave network, we formulate a user association scheme that maximizes throughput considering beam formation and MC. Our numerical analysis shows that there is no one-size-fits-all degree of optimal MC; it depends on the number of users, their rate requirements, locations, and the maximum number of active beams at a BS.Based on the optimal association, we design BEAM-ALIGN: an efficient heuristic with polynomial-time complexity O(|U|log|U|), where |U| is the number of users. Moreover, BEAM-ALIGN only uses local BS information - i.e. the received signal quality at the user. Differing from prior works, BEAM-ALIGN considers beamforming, multiconnectivity and line-of-sight probability. Via simulations, we show that BEAM-ALIGN performs close to optimal in terms of per-user capacity and satisfaction while it outperforms frequently-used signal-to-interference-and-noise-ratio based association schemes. We then show that BEAM-ALIGN has a robust performance under various challenging scenarios: the presence of blockers, rain, and clustered users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.13166v2</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lotte Weedage, Clara Stegehuis, Suzan Bayhan</dc:creator>
    </item>
    <item>
      <title>Machine Learning &amp; Wi-Fi: Unveiling the Path Towards AI/ML-Native IEEE 802.11 Networks</title>
      <link>https://arxiv.org/abs/2405.11504</link>
      <description>arXiv:2405.11504v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) and machine learning (ML) are nowadays mature technologies considered essential for driving the evolution of future communications systems. Simultaneously, Wi-Fi technology has constantly evolved over the past three decades and incorporated new features generation after generation, thus gaining in complexity. As such, researchers have observed that AI/ML functionalities may be required to address the upcoming Wi-Fi challenges that will be otherwise difficult to solve with traditional approaches. This paper discusses the role of AI/ML in current and future Wi-Fi networks and depicts the ways forward. A roadmap towards AI/ML-native Wi-Fi, key challenges, standardization efforts, and major enablers are also discussed. An exemplary use case is provided to showcase the potential of AI/ML in Wi-Fi at different adoption stages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11504v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesc Wilhelmi, Szymon Szott, Katarzyna Kosek-Szott, Boris Bellalta</dc:creator>
    </item>
    <item>
      <title>LCDN: Providing Network Determinism with Low-Cost Switches</title>
      <link>https://arxiv.org/abs/2408.10171</link>
      <description>arXiv:2408.10171v2 Announce Type: replace 
Abstract: The demands on networks are increasing at a fast pace. In particular, real-time applications have very strict network requirements. However, building a network that hosts real-time applications is a cost-intensive endeavor, especially for experimental systems such as testbeds. Systems that provide guaranteed real-time networking capabilities usually work with expensive software-defined switches. In contrast, real-time networking systems based on low-cost hardware face the limitation of lower link speeds. This paper fills this gap and presents Low-Cost Deterministic Networking (LCDN), a system designed to work with inexpensive, common off-the-shelf switches and devices. LCDN works at Gigabit speed and enables powerful testbeds to host real-time applications with strict delay guarantees. This paper also provides an evaluation of the determinism of the switch and a Raspberry Pi used as an end device to demonstrate the applicability of LCDN on inexpensive low-power reduced capacity apparatus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10171v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Diederich, Yash Deshpande, Laura Becker, Wolfgang Kellerer</dc:creator>
    </item>
  </channel>
</rss>
