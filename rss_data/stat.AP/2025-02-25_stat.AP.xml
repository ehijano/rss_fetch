<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Feb 2025 02:57:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Rashomon perspective for measuring uncertainty in the survival predictive maintenance models</title>
      <link>https://arxiv.org/abs/2502.15772</link>
      <description>arXiv:2502.15772v1 Announce Type: new 
Abstract: The prediction of the Remaining Useful Life of aircraft engines is a critical area in high-reliability sectors such as aerospace and defense. Early failure predictions help ensure operational continuity, reduce maintenance costs, and prevent unexpected failures. Traditional regression models struggle with censored data, which can lead to biased predictions. Survival models, on the other hand, effectively handle censored data, improving predictive accuracy in maintenance processes. This paper introduces a novel approach based on the Rashomon perspective, which considers multiple models that achieve similar performance rather than relying on a single best model. This enables uncertainty quantification in survival probability predictions and enhances decision-making in predictive maintenance. The Rashomon survival curve was introduced to represent the range of survival probability estimates, providing insights into model agreement and uncertainty over time. The results on the CMAPSS dataset demonstrate that relying solely on a single model for RUL estimation may increase risk in some scenarios. The censoring levels significantly impact prediction uncertainty, with longer censoring times leading to greater variability in survival probabilities. These findings underscore the importance of incorporating model multiplicity in predictive maintenance frameworks to achieve more reliable and robust failure predictions. This paper contributes to uncertainty quantification in RUL prediction and highlights the Rashomon perspective as a powerful tool for predictive modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15772v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yigitcan Yardimci, Mustafa Cavus</dc:creator>
    </item>
    <item>
      <title>Resident Turnover and Community Satisfaction in Active Lifestyle Communities</title>
      <link>https://arxiv.org/abs/2502.15789</link>
      <description>arXiv:2502.15789v1 Announce Type: new 
Abstract: An analysis of Tellico Village, a non-age-restricted active lifestyle community, reveals complex patterns in resident tenure and satisfaction. Longitudinal surveys (2018-2024) and property records show consistently high satisfaction levels (93%), yet a decline in median tenure from 13 years pre-COVID to 11 years post-COVID (p &lt; 0.001), reflecting a broader nationwide trend in homeownership duration.
  Kaplan-Meier survival analysis identifies departure risk peaks at years 3, 5, 7, 11, 16, 22, and 26, corresponding to life transitions and market cycles. Satisfaction follows a U-shaped trajectory, lowest between years 6-9 (3-12). Key predictors include financial attitudes, recreational engagement, and openness to growth.
  While aggregate willingness to pay higher POA fees strongly correlates with satisfaction, explaining 94% of the variance, this relationship weakens at the neighborhood (44%) and household (5%) levels. Machine learning models - including Support Vector Machines, Random Forests, and XGBoost - and hedonic price analysis provided limited predictive power, suggesting the influence of unmeasured variables.
  The study advances understanding of housing tenure dynamics in lifestyle communities while highlighting the need for more sophisticated longitudinal tracking and instrumental variable approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15789v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Irene S. Gabashvili, Christopher K. Allsup</dc:creator>
    </item>
    <item>
      <title>One citation, one vote! A new approach for analysing check-all-that-apply (CATA) data in sensometrics, using L1 norm methods</title>
      <link>https://arxiv.org/abs/2502.15945</link>
      <description>arXiv:2502.15945v1 Announce Type: new 
Abstract: A unified framework is provided for analysing check-all-that-apply (CATA) product data following the ``one citation, one vote" principle. CATA data arise from studies where A consumers evaluate P products by describing samples by checking all of the T terms that apply. Giving every citation the same weight, regardless of the assessor, product, or term, leads to analyses based on the L1 norm where the median absolute deviation is the measure of dispersion. Five permutation tests are proposed to answer the following questions. Do any products differ? For which terms do products differ? Within each of the terms, which products differ? Which product pairs differ? On which terms does each product pair differ? Additionally, we show how products and terms can be clustered following the ``one citation, one vote" principle and how L1-norm principal component analysis (L1-norm PCA) can be applied to visualize CATA results in few dimensions. Together, the permutation tests, clustering methods, and L1-norm PCA provide a unified approach. The proposed methods are illustrated using a data set in which 100 consumers evaluated 11 products using 34 CATA terms.R code is provided to perform the analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15945v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carolina Chaya, John Castura, Michael Greenacre</dc:creator>
    </item>
    <item>
      <title>Breaking the Balance: Asymmetric Negative Voting in the 2020 Presidential Election</title>
      <link>https://arxiv.org/abs/2502.16081</link>
      <description>arXiv:2502.16081v1 Announce Type: new 
Abstract: While voters from opposing parties have traditionally exhibited symmetric levels of hostility toward out-party candidates, our analysis of the 2016 and 2020 Nationscape data reveals a notable departure from this pattern. In 2016, negative voting was relatively balanced, with similar levels of hostility directed at Hillary Clinton and Donald Trump. However, by 2020, asymmetric negative voting had emerged. As an incumbent seeking re-election amid a rapidly declining economy, the COVID-19 pandemic, and widespread uncertainty, Trump faced heightened negative perceptions fueled by dissatisfaction with his handling of the economy, race relations, the pandemic, and his leadership style. These factors galvanized younger, educated Democrats and Independents to vote against him in unprecedented numbers. In contrast, Republicans expressed less animosity toward Biden in 2020 than they had toward Clinton in 2016. This shift disrupted the balance in the typical pattern of symmetric negative voting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16081v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bang Quan Zheng</dc:creator>
    </item>
    <item>
      <title>A Bayesian mixed-effects model to evaluate the determinants of COVID-19 vaccine uptake in the US</title>
      <link>https://arxiv.org/abs/2502.16130</link>
      <description>arXiv:2502.16130v1 Announce Type: new 
Abstract: The COVID-19 pandemic has adversely affected US public health, resulting in over a hundred million cases and more than one million deaths. Vaccination is the key intervention against the COVID-19 pandemic. Multiple COVID-19 vaccines are now available for human use. However, a number of factors, including socio-demographic variables, impact the uptake of COVID-19 vaccines. In this study, we apply a Bayesian mixed-effects model to assess different socio-demographic and spatial factors that influence the acceptance of COVID-19 vaccines in the US. The fitted mixed-effects model provides the probabilistic inference about the vaccine acceptance determinants with uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16130v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asim K. Dey</dc:creator>
    </item>
    <item>
      <title>A multilevel model with heterogeneous variances for snap timing in the National Football League</title>
      <link>https://arxiv.org/abs/2502.16313</link>
      <description>arXiv:2502.16313v1 Announce Type: new 
Abstract: Player tracking data have provided great opportunities to generate novel insights into understudied areas of American football, such as pre-snap motion. Using a Bayesian multilevel model with heterogeneous variances, we provide an assessment of NFL quarterbacks and their ability to synchronize the timing of the ball snap with pre-snap movement from their teammates. We focus on passing plays with receivers in motion at the snap and running a route, and define the snap timing as the time between the moment a receiver begins motioning and the ball snap event. We assume a Gamma distribution for the play-level snap timing and model the mean parameter with player and team random effects, along with relevant fixed effects such as the motion type identified via a Gaussian mixture model. Most importantly, we model the shape parameter with quarterback random effects, which enables us to estimate the differences in snap timing variability among NFL quarterbacks. We demonstrate that higher variability in snap timing is beneficial for the passing game, as it relates to facing less havoc created by the opposing defense. We also obtain a quarterback leaderboard based on our snap timing variability measure, and Patrick Mahomes stands out as the top player.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16313v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quang Nguyen, Ronald Yurko</dc:creator>
    </item>
    <item>
      <title>Including an infrequently measured time-dependent error-prone covariate in survival analyses: a simulation-based comparison of methods</title>
      <link>https://arxiv.org/abs/2502.16362</link>
      <description>arXiv:2502.16362v1 Announce Type: new 
Abstract: Epidemiologic studies often evaluate the association between an exposure and an event risk. When time-varying, exposure updates usually occur at discrete visits although changes are in continuous time and survival models require values to be constantly known. Moreover, exposures are likely measured with error, and their observation truncated at the event time. We aimed to quantify in a Cox regression the bias in the association resulting from intermittent measurements of an error-prone exposure. Using simulations under various scenarios, we compared five methods: last observation carried-forward (LOCF), classical two-stage regression-calibration using measurements up to the event (RC) or also after (PE-RC), multiple imputation (MI) and joint modeling of the exposure and the event (JM). The LOCF, and to a lesser extent the classical RC, showed substantial bias in almost all 43 scenarios. The RC bias was avoided when considering post-event information. The MI performed relatively well, as did the JM. Illustrations exploring the association of Body Mass Index and Executive Functioning with dementia risk showed consistent conclusions. Accounting for measurement error and discrete updates is critical when studying time-varying exposures. MI and JM techniques may be applied in this context, while classical RC should be avoided due to the informative truncation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16362v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Viviane Philipps (on behalf of Measurement Error,Misclassification Topic Group), Laurence Freedman (on behalf of Measurement Error,Misclassification Topic Group), Veronika Deffner (on behalf of Measurement Error,Misclassification Topic Group), Catherine Helmer (on behalf of Measurement Error,Misclassification Topic Group), Hendriek Boshuizen (on behalf of Measurement Error,Misclassification Topic Group), Anne C. M. Thi\'ebaut (on behalf of Measurement Error,Misclassification Topic Group), C\'ecile Proust-Lima (on behalf of Measurement Error,Misclassification Topic Group)</dc:creator>
    </item>
    <item>
      <title>Adjustment for Inconsistency in Adaptive Phase 2/3 Designs with Dose Optimization</title>
      <link>https://arxiv.org/abs/2502.16591</link>
      <description>arXiv:2502.16591v1 Announce Type: new 
Abstract: Adaptive Phase 2/3 designs hold great promise in contemporary oncology drug development, especially when limited data from Phase 1 dose-finding is insufficient for identifying an optimal dose. However, there is a general concern about inconsistent results before and after the adaptation. The imperfection in dose selection further complicates the issue. In this paper, we explicitly incorporate the concerns about inconsistency into the statistical analysis under three hypothesis testing strategies. This investigation paves the way for further research in a less explored area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16591v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cong Chen, Mo Huang</dc:creator>
    </item>
    <item>
      <title>Financial fraud detection system based on improved random forest and gradient boosting machine (GBM)</title>
      <link>https://arxiv.org/abs/2502.15822</link>
      <description>arXiv:2502.15822v1 Announce Type: cross 
Abstract: This paper proposes a financial fraud detection system based on improved Random Forest (RF) and Gradient Boosting Machine (GBM). Specifically, the system introduces a novel model architecture called GBM-SSRF (Gradient Boosting Machine with Simplified and Strengthened Random Forest), which cleverly combines the powerful optimization capabilities of the gradient boosting machine (GBM) with improved randomization. The computational efficiency and feature extraction capabilities of the Simplified and Strengthened Random Forest (SSRF) forest significantly improve the performance of financial fraud detection. Although the traditional random forest model has good classification capabilities, it has high computational complexity when faced with large-scale data and has certain limitations in feature selection. As a commonly used ensemble learning method, the GBM model has significant advantages in optimizing performance and handling nonlinear problems. However, GBM takes a long time to train and is prone to overfitting problems when data samples are unbalanced. In response to these limitations, this paper optimizes the random forest based on the structure, reducing the computational complexity and improving the feature selection ability through the structural simplification and enhancement of the random forest. In addition, the optimized random forest is embedded into the GBM framework, and the model can maintain efficiency and stability with the help of GBM's gradient optimization capability. Experiments show that the GBM-SSRF model not only has good performance, but also has good robustness and generalization capabilities, providing an efficient and reliable solution for financial fraud detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15822v1</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>q-fin.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tianzuo Hu</dc:creator>
    </item>
    <item>
      <title>A non-parametric optimal design algorithm for population pharmacokinetics</title>
      <link>https://arxiv.org/abs/2502.15848</link>
      <description>arXiv:2502.15848v1 Announce Type: cross 
Abstract: This paper introduces a non-parametric estimation algorithm designed to effectively estimate the joint distribution of model parameters with application to population pharmacokinetics. Our research group has previously developed the non-parametric adaptive grid (NPAG) algorithm, which while accurate, explores parameter space using an ad-hoc method to suggest new support points. In contrast, the non-parametric optimal design (NPOD) algorithm uses a gradient approach to suggest new support points, which reduces the amount of time spent evaluating non-relevant points and by this the overall number of cycles required to reach convergence. In this paper, we demonstrate that the NPOD algorithm achieves similar solutions to NPAG across two datasets, while being significantly more efficient in both the number of cycles required and overall runtime. Given the importance of developing robust and efficient algorithms for determining drug doses quickly in pharmacokinetics, the NPOD algorithm represents a valuable advancement in non-parametric modeling. Further analysis is needed to determine which algorithm performs better under specific conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15848v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Hovd, Alona Kryshchenko, Michael N. Neely, Julian Otalvaro, Alan Schumitzky, Walter M. Yamada</dc:creator>
    </item>
    <item>
      <title>Improving ex ante accuracy assessment in predicting house price dispersion: evidence from the USA</title>
      <link>https://arxiv.org/abs/2502.15905</link>
      <description>arXiv:2502.15905v1 Announce Type: cross 
Abstract: The study focuses on improving the ex ante prediction accuracy assessment in the case of forecasting various house price dispersion measures in the USA. It addresses a critical gap in real estate market forecasting by proposing a novel method for assessing ex ante prediction accuracy under unanticipated shocks. The proposal is based on a parametric bootstrap approach under a misspecified model, allowing for the simulation of future values and estimation of prediction errors in case of unexpected price changes. The study highlights the limitations of the traditional approach that fails to account for unforeseen market events and provides a more in-depth understanding of how prediction accuracy changes under unexpected scenarios. The proposed methods offers valuable insights for real estate market management by enabling more robust risk assessment and decision-making in the face of unexpected market fluctuations. Real data application is based on longitudinal U.S. data on real estate transactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15905v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adam Chwila, Monika Hada\'s-Dyduch, Ma{\l}gorzata Krzciuk, Tomasz Stachurski, Alicja Wolny-Dominiak, Tomasz \.Z\k{a}d{\l}o</dc:creator>
    </item>
    <item>
      <title>Empirical Best Prediction of Poverty Indicators via Nested Error Regression with High Dimensional Parameters</title>
      <link>https://arxiv.org/abs/2502.15933</link>
      <description>arXiv:2502.15933v1 Announce Type: cross 
Abstract: This paper extends the Nested Error Regression Model with High-Dimensional Parameters (NERHDP) to address challenges in small area poverty estimation. Building on the NERHDP framework, we develop a robust and flexible approach to derive empirical best predictors (EBPs) of small area poverty indicators, while accommodating heterogeneity in regression coefficients and sampling variances across areas. To overcome computational limitations in the existing algorithm, we introduce an efficient method that significantly reduces computation time, enhancing scalability for large datasets. Additionally, we propose a novel method for generating area-specific poverty estimates for out-of-sample areas, improving the reliability of synthetic estimates. To quantify uncertainty, we introduce a parametric bootstrap method tailored to the extended model. Through design-based simulation studies, we demonstrate that the proposed method has better performance in terms of relative bias and relative root mean squared prediction error compared to existing approaches. Furthermore, the proposed method is applied to household survey data from the 2002 Albania Living Standards Measurement Survey to estimate poverty indicators for 374 municipalities using auxiliary information from the 2001 census.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15933v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuting Chen, Partha Lahiri, Nicola Salvati</dc:creator>
    </item>
    <item>
      <title>Local False Sign Rate and the Role of Prior Covariance Rank in Multivariate Empirical Bayes Multiple Testing</title>
      <link>https://arxiv.org/abs/2502.16118</link>
      <description>arXiv:2502.16118v1 Announce Type: cross 
Abstract: We study the relationship between the rank of the prior covariance matrix and the local false sign rate in a multivariate empirical Bayes normal mean model. It has been observed that the false sign rate is inflated when the prior assigns weight to low-rank covariance matrices. We show that this issue arises due to the rank deficiency of prior covariance matrices and propose an adjustment to mitigate it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16118v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dongyue Xie</dc:creator>
    </item>
    <item>
      <title>Subspace Recovery in Winsorized PCA: Insights into Accuracy and Robustness</title>
      <link>https://arxiv.org/abs/2502.16391</link>
      <description>arXiv:2502.16391v1 Announce Type: cross 
Abstract: In this paper, we explore the theoretical properties of subspace recovery using Winsorized Principal Component Analysis (WPCA), utilizing a common data transformation technique that caps extreme values to mitigate the impact of outliers. Despite the widespread use of winsorization in various tasks of multivariate analysis, its theoretical properties, particularly for subspace recovery, have received limited attention. We provide a detailed analysis of the accuracy of WPCA, showing that increasing the number of samples while decreasing the proportion of outliers guarantees the consistency of the sample subspaces from WPCA with respect to the true population subspace. Furthermore, we establish perturbation bounds that ensure the WPCA subspace obtained from contaminated data remains close to the subspace recovered from pure data. Additionally, we extend the classical notion of breakdown points to subspace-valued statistics and derive lower bounds for the breakdown points of WPCA. Our analysis demonstrates that WPCA exhibits strong robustness to outliers while maintaining consistency under mild assumptions. A toy example is provided to numerically illustrate the behavior of the upper bounds for perturbation bounds and breakdown points, emphasizing winsorization's utility in subspace recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16391v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sangil Han, Kyoowon Kim, Sungkyu Jung</dc:creator>
    </item>
    <item>
      <title>Beyond Words: How Large Language Models Perform in Quantitative Management Problem-Solving</title>
      <link>https://arxiv.org/abs/2502.16556</link>
      <description>arXiv:2502.16556v1 Announce Type: cross 
Abstract: This study examines how Large Language Models (LLMs) perform when tackling quantitative management decision problems in a zero-shot setting. Drawing on 900 responses generated by five leading models across 20 diverse managerial scenarios, our analysis explores whether these base models can deliver accurate numerical decisions under varying presentation formats, scenario complexities, and repeated attempts. Contrary to prior findings, we observed no significant effects of text presentation format (direct, narrative, or tabular) or text length on accuracy. However, scenario complexity -- particularly in terms of constraints and irrelevant parameters -- strongly influenced performance, often degrading accuracy. Surprisingly, the models handled tasks requiring multiple solution steps more effectively than expected. Notably, only 28.8\% of responses were exactly correct, highlighting limitations in precision. We further found no significant ``learning effect'' across iterations: performance remained stable across repeated queries. Nonetheless, significant variations emerged among the five tested LLMs, with some showing superior binary accuracy. Overall, these findings underscore both the promise and the pitfalls of harnessing LLMs for complex quantitative decision-making, informing managers and researchers about optimal deployment strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16556v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Kuzmanko</dc:creator>
    </item>
    <item>
      <title>A tutorial on optimal dynamic treatment regimes</title>
      <link>https://arxiv.org/abs/2502.16988</link>
      <description>arXiv:2502.16988v1 Announce Type: cross 
Abstract: A dynamic treatment regime is a sequence of treatment decision rules tailored to an individual's evolving status over time. In precision medicine, much focus has been placed on finding an optimal dynamic treatment regime which, if followed by everyone in the population, would yield the best outcome on average; and extensive investigation has been conducted from both methodological and applications standpoints. The aim of this tutorial is to provide readers who are interested in optimal dynamic treatment regimes with a systematic, detailed but accessible introduction, including the formal definition and formulation of this topic within the framework of causal inference, identification assumptions required to link the causal quantity of interest to the observed data, existing statistical models and estimation methods to learn the optimal regime from data, and application of these methods to both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16988v1</guid>
      <category>stat.OT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chunyu Wang, Brian DM Tom</dc:creator>
    </item>
    <item>
      <title>Sustainable Greenhouse Management: A Comparative Analysis of Recurrent and Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2502.17371</link>
      <description>arXiv:2502.17371v1 Announce Type: cross 
Abstract: The integration of photovoltaic (PV) systems into greenhouses not only optimizes land use but also enhances sustainable agricultural practices by enabling dual benefits of food production and renewable energy generation. However, accurate prediction of internal environmental conditions is crucial to ensure optimal crop growth while maximizing energy production. This study introduces a novel application of Spatio-Temporal Graph Neural Networks (STGNNs) to greenhouse microclimate modeling, comparing their performance with traditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal pattern recognition, they cannot explicitly model the directional relationships between environmental variables. Our STGNN approach addresses this limitation by representing these relationships as directed graphs, enabling the model to capture both spatial dependencies and their directionality. Using high-frequency data collected at 15-minute intervals from a greenhouse in Volos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter conditions (R^2 = 0.985) but show limitations during summer cooling system operation. Though STGNNs currently show lower performance (winter R^2 = 0.947), their architecture offers greater potential for integrating additional variables such as PV generation and crop growth indicators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17371v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiliano Seri, Marcello Petitta, Cristina Cornaro</dc:creator>
    </item>
    <item>
      <title>Efficient evaluation of risk allocations</title>
      <link>https://arxiv.org/abs/2207.02654</link>
      <description>arXiv:2207.02654v3 Announce Type: replace 
Abstract: Expectations of marginals conditional on the total risk of a portfolio are crucial in risk-sharing and allocation. However, computing these conditional expectations may be challenging, especially in critical cases where the marginal risks have compound distributions or when the risks are dependent. We introduce a generating function method to compute these conditional expectations. We provide efficient algorithms to compute the conditional expectations of marginals given the total risk for a portfolio of risks with lattice-type support. We show that the ordinary generating function of unconditional expected allocations is a function of the multivariate probability generating function of the portfolio. The generating function method allows us to develop recursive and transform-based techniques to compute the unconditional expected allocations. We illustrate our method to large-scale risk-sharing and risk allocation problems, including cases where the marginal risks have compound distributions, where the portfolio is composed of dependent risks, and where the risks have heavy tails, leading in some cases to computational gains of several orders of magnitude. Our approach is useful for risk-sharing in peer-to-peer insurance and risk allocation based on Euler's rule.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.02654v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Blier-Wong, H\'el\`ene Cossette, Etienne Marceau</dc:creator>
    </item>
    <item>
      <title>Digital N-of-1 Trials and their Application in Experimental Physiology</title>
      <link>https://arxiv.org/abs/2412.15076</link>
      <description>arXiv:2412.15076v2 Announce Type: replace 
Abstract: Traditionally, studies in experimental physiology have been conducted in small groups of human participants, animal models or cell lines. Identifying optimal study designs that achieve sufficient power for drawing proper statistical inferences to detect group level effects with small sample sizes has been challenging. Moreover, average effects derived from traditional group-level inference do not necessarily apply to individual participants. Here, we introduce N-of-1 trials as an innovative study design that can be used to draw valid statistical inference about the effects of interventions on individual participants and can be aggregated across multiple study participants to provide population-level inferences more efficiently than standard group randomized trials. In this manuscript, we introduce the key components and design features of N-of-1 trials, describe statistical analysis and interpretations of the results, and describe some available digital tools to facilitate their use using examples from experimental physiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15076v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefan Konigorski, Mathias Ried-Larsen, Christopher H Schmid</dc:creator>
    </item>
    <item>
      <title>A Tutorial on Markov Renewal and Semi-Markov Proportional Hazards Model</title>
      <link>https://arxiv.org/abs/2502.03479</link>
      <description>arXiv:2502.03479v4 Announce Type: replace 
Abstract: Transition probability estimation plays a critical role in multi-state modeling, especially in clinical research. This paper investigates the application of semi-Markov and Markov renewal frameworks to the EBMT dataset, focusing on six clinical states encountered during hematopoietic stem cell transplantation. By comparing Aalen-Johansen (AJ) and Dabrowska-Sun-Horowitz (DSH) estimators, we demonstrate that semi-Markov models, which incorporate sojourn times, provide a more nuanced and temporally sensitive depiction of patient trajectories compared to memoryless Markov models. The DSH estimator consistently yields smoother probability curves, particularly for transitions involving prolonged states. We use empirical process theory and Burkholder-Davis-Gundy inequality to show weak convergence of the estimator. Future work includes extending the framework to accommodate advanced covariate structures and non-Markovian dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03479v4</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eliuvish Cuicizion, Itsugo Ri, Elaine Holmes, Jawad Chern</dc:creator>
    </item>
    <item>
      <title>Angular Combining of Forecasts of Probability Distributions</title>
      <link>https://arxiv.org/abs/2305.16735</link>
      <description>arXiv:2305.16735v2 Announce Type: replace-cross 
Abstract: When multiple forecasts are available for a probability distribution, forecast combining enables a pragmatic synthesis of the information to extract the wisdom of the crowd. The linear opinion pool has been widely used, whereby the combining is applied to the probabilities of the distributional forecasts. However, it has been argued that this will tend to deliver overdispersed distributions, prompting the combination to be applied, instead, to the quantiles of the distributional forecasts. Results from different applications are mixed, leaving it as an empirical question whether to combine probabilities or quantiles. In this paper, we present an alternative approach. Looking at the distributional forecasts, combining the probabilities can be viewed as vertical combining, with quantile combining seen as horizontal combining. Our proposal is to allow combining to take place on an angle between the extreme cases of vertical and horizontal combining. We term this angular combining. The angle is a parameter that can be optimized using a proper scoring rule. For implementation, we provide a pragmatic numerical approach and a simulation algorithm. Among our theoretical results, we show that, as with vertical and horizontal averaging, angular averaging results in a distribution with mean equal to the average of the means of the distributions that are being combined. We also show that angular averaging produces a distribution with lower variance than vertical averaging, and, under certain assumptions, greater variance than horizontal averaging. We provide empirical results for distributional forecasts of Covid mortality, macroeconomic survey data, and electricity prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.16735v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James W. Taylor, Xiaochun Meng</dc:creator>
    </item>
    <item>
      <title>UAVDB: Trajectory-Guided Adaptable Bounding Boxes for UAV Detection</title>
      <link>https://arxiv.org/abs/2409.06490</link>
      <description>arXiv:2409.06490v5 Announce Type: replace-cross 
Abstract: The widespread deployment of Unmanned Aerial Vehicles (UAVs) in surveillance, security, and airspace management has created an urgent demand for precise, scalable, and efficient UAV detection. However, existing datasets often suffer from limited scale diversity and inaccurate annotations, hindering robust model development. This paper introduces UAVDB, a high-resolution UAV detection dataset constructed using Patch Intensity Convergence (PIC). This novel technique automatically generates high-fidelity bounding box annotations from UAV trajectory data~\cite{li2020reconstruction}, eliminating the need for manual labeling. UAVDB features single-class annotations with a fixed-camera setup and consists of RGB frames capturing UAVs across various scales, from large-scale UAVs to near-single-pixel representations, along with challenging backgrounds that pose difficulties for modern detectors. We first validate the accuracy and efficiency of PIC-generated bounding boxes by comparing Intersection over Union (IoU) performance and runtime against alternative annotation methods, demonstrating that PIC achieves higher annotation accuracy while being more efficient. Subsequently, we benchmark UAVDB using state-of-the-art (SOTA) YOLO-series detectors, establishing UAVDB as a valuable resource for advancing long-range and high-resolution UAV detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06490v5</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu-Hsi Chen</dc:creator>
    </item>
    <item>
      <title>Compactly-supported nonstationary kernels for computing exact Gaussian processes on big data</title>
      <link>https://arxiv.org/abs/2411.05869</link>
      <description>arXiv:2411.05869v2 Announce Type: replace-cross 
Abstract: The Gaussian process (GP) is a widely used probabilistic machine learning method with implicit uncertainty characterization for stochastic function approximation, stochastic modeling, and analyzing real-world measurements of nonlinear processes. Traditional implementations of GPs involve stationary kernels (also termed covariance functions) that limit their flexibility, and exact methods for inference that prevent application to data sets with more than about ten thousand points. Modern approaches to address stationarity assumptions generally fail to accommodate large data sets, while all attempts to address scalability focus on approximating the Gaussian likelihood, which can involve subjectivity and lead to inaccuracies. In this work, we explicitly derive an alternative kernel that can discover and encode both sparsity and nonstationarity. We embed the kernel within a fully Bayesian GP model and leverage high-performance computing resources to enable the analysis of massive data sets. We demonstrate the favorable performance of our novel kernel relative to existing exact and approximate GP methods across a variety of synthetic data examples. Furthermore, we conduct space-time prediction based on more than one million measurements of daily maximum temperature and verify that our results outperform state-of-the-art methods in the Earth sciences. More broadly, having access to exact GPs that use ultra-scalable, sparsity-discovering, nonstationary kernels allows GP methods to truly compete with a wide variety of machine learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05869v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark D. Risser, Marcus M. Noack, Hengrui Luo, Ronald Pandolfi</dc:creator>
    </item>
    <item>
      <title>Biomarker combination based on the Youden index with and without gold standard</title>
      <link>https://arxiv.org/abs/2412.17471</link>
      <description>arXiv:2412.17471v2 Announce Type: replace-cross 
Abstract: In clinical practice, multiple biomarkers are often measured on the same subject for disease diagnosis, and combining them can improve diagnostic accuracy. Existing studies typically combine multiple biomarkers by maximizing the Area Under the ROC Curve (AUC), assuming a gold standard exists or that biomarkers follow a multivariate normal distribution. However, practical diagnostic settings require both optimal combination coefficients and an effective cutoff value, and the reference test may be imperfect. In this paper, we propose a two-stage method for identifying the optimal linear combination and cutoff value based on the Youden index. First, it maximizes an approximation of the empirical AUC to estimate the optimal linear coefficients for combining multiple biomarkers. Then, it maximizes the empirical Youden index to determine the optimal cutoff point for disease classification. Under the semiparametric single index model and regularity conditions, the estimators for the linear coefficients, cutoff point, and Youden index are consistent. This method is also applicable when the reference standard is imperfect. We demonstrate the performance of our method through simulations and apply it to construct a diagnostic scale for Chinese medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17471v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ao Sun, Yanting Li, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Spherical Double K-Means: a co-clustering approach for text data analysis</title>
      <link>https://arxiv.org/abs/2501.04562</link>
      <description>arXiv:2501.04562v3 Announce Type: replace-cross 
Abstract: In text analysis, Spherical K-means (SKM) is a specialized k-means clustering algorithm widely utilized for grouping documents represented in high-dimensional, sparse term-document matrices, often normalized using techniques like TF-IDF. Researchers frequently seek to cluster not only documents but also the terms associated with them into coherent groups. To address this dual clustering requirement, we introduce Spherical Double K-Means (SDKM), a novel methodology that simultaneously clusters documents and terms. This approach offers several advantages: first, by integrating the clustering of documents and terms, SDKM provides deeper insights into the relationships between content and vocabulary, enabling more effective topic identification and keyword extraction. Additionally, the two-level clustering assists in understanding both overarching themes and specific terminologies within document clusters, enhancing interpretability. SDKM effectively handles the high dimensionality and sparsity inherent in text data by utilizing cosine similarity, leading to improved computational efficiency. Moreover, the method captures dynamic changes in thematic content over time, making it well-suited for applications in rapidly evolving fields. Ultimately, SDKM presents a comprehensive framework for advancing text mining efforts, facilitating the uncovering of nuanced patterns and structures that are critical for robust data analysis. We apply SDKM to the corpus of US presidential inaugural addresses, spanning from George Washington in 1789 to Joe Biden in 2021. Our analysis reveals distinct clusters of words and documents that correspond to significant historical themes and periods, showcasing the method's ability to facilitate a deeper understanding of the data. Our findings demonstrate the efficacy of SDKM in uncovering underlying patterns in textual data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04562v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilaria Bombelli, Domenica Fioredistella Iezzi, Emiliano Seri, Maurizio Vichi</dc:creator>
    </item>
    <item>
      <title>Hypergraph Representations of scRNA-seq Data for Improved Clustering with Random Walks</title>
      <link>https://arxiv.org/abs/2501.11760</link>
      <description>arXiv:2501.11760v2 Announce Type: replace-cross 
Abstract: Analysis of single-cell RNA sequencing data is often conducted through network projections such as coexpression networks, primarily due to the abundant availability of network analysis tools for downstream tasks. However, this approach has several limitations: loss of higher-order information, inefficient data representation caused by converting a sparse dataset to a fully connected network, and overestimation of coexpression due to zero-inflation. To address these limitations, we propose conceptualizing scRNA-seq expression data as hypergraphs, which are generalized graphs in which the hyperedges can connect more than two vertices. In the context of scRNA-seq data, the hypergraph nodes represent cells and the edges represent genes. Each hyperedge connects all cells where its corresponding gene is actively expressed and records the expression of the gene across different cells. This hypergraph conceptualization enables us to explore multi-way relationships beyond the pairwise interactions in coexpression networks without loss of information. We propose two novel clustering methods: (1) the Dual-Importance Preference Hypergraph Walk (DIPHW) and (2) the Coexpression and Memory-Integrated Dual-Importance Preference Hypergraph Walk (CoMem-DIPHW). They outperform established methods on both simulated and real scRNA-seq datasets. The improvement brought by our proposed methods is especially significant when data modularity is weak. Furthermore, CoMem-DIPHW incorporates the gene coexpression network, cell coexpression network, and the cell-gene expression hypergraph from the single-cell abundance counts data altogether for embedding computation. This approach accounts for both the local level information from single-cell level gene expression and the global level information from the pairwise similarity in the two coexpression networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11760v2</guid>
      <category>q-bio.QM</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wan He, Daniel I. Bolnick, Samuel V. Scarpino, Tina Eliassi-Rad</dc:creator>
    </item>
    <item>
      <title>Sequential Methods for Error Correction of Probabilistic Wind Power Forecasts</title>
      <link>https://arxiv.org/abs/2501.14805</link>
      <description>arXiv:2501.14805v3 Announce Type: replace-cross 
Abstract: Reliable probabilistic production forecasts are required to better manage the uncertainty that the rapid build-out of wind power capacity adds to future energy systems. In this article, we consider sequential methods to correct errors in wind power production forecast ensembles derived from numerical weather predictions. We propose combining neural networks with time-adaptive quantile regression to enhance the accuracy of wind power forecasts. We refer to this approach as Neural Adaptive Basis for (time-adaptive) Quantile Regression or NABQR. First, we use NABQR to correct power production ensembles with neural networks. We find that Long Short-Term Memory networks are the most effective architecture for this purpose. Second, we apply time-adaptive quantile regression to the corrected ensembles to obtain optimal median predictions along with quantiles of the forecast distribution. With the suggested method, we beat state-of-the-art methods and achieve accuracy improvements up to 40% in mean absolute terms in an application to day-ahead forecasting of on- and offshore wind power production in Denmark. In addition, we explore the value of our method for applications in energy trading. We have implemented the NABQR method as an open-source Python package to support applications in renewable energy forecasting and future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14805v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bastian Schmidt J{\o}rgensen, Jan Kloppenborg M{\o}ller, Peter Nystrup, Henrik Madsen</dc:creator>
    </item>
  </channel>
</rss>
