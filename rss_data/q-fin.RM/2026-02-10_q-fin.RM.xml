<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.RM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.RM</link>
    <description>q-fin.RM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.RM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Feb 2026 03:00:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>LLM as a Risk Manager: LLM Semantic Filtering for Lead-Lag Trading in Prediction Markets</title>
      <link>https://arxiv.org/abs/2602.07048</link>
      <description>arXiv:2602.07048v1 Announce Type: new 
Abstract: Prediction markets provide a unique setting where event-level time series are directly tied to natural-language descriptions, yet discovering robust lead-lag relationships remains challenging due to spurious statistical correlations. We propose a hybrid two-stage causal screener to address this challenge: (i) a statistical stage that uses Granger causality to identify candidate leader-follower pairs from market-implied probability time series, and (ii) an LLM-based semantic stage that re-ranks these candidates by assessing whether the proposed direction admits a plausible economic transmission mechanism based on event descriptions. Because causal ground truth is unobserved, we evaluate the ranked pairs using a fixed, signal-triggered trading protocol that maps relationship quality into realized profit and loss (PnL). On Kalshi Economics markets, our hybrid approach consistently outperforms the statistical baseline. Across rolling evaluations, the win rate increases from 51.4% to 54.5%. Crucially, the average magnitude of losing trades decreases substantially from 649 USD to 347 USD. This reduction is driven by the LLM's ability to filter out statistically fragile links that are prone to large losses, rather than relying on rare gains. These improvements remain stable across different trading configurations, indicating that the gains are not driven by specific parameter choices. Overall, the results suggest that LLMs function as semantic risk managers on top of statistical discovery, prioritizing lead-lag relationships that generalize under changing market conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07048v1</guid>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sumin Kim, Minjae Kim, Jihoon Kwon, Yoon Kim, Nicole Kagan, Joo Won Lee, Oscar Levy, Alejandro Lopez-Lira, Yongjae Lee, Chanyeol Choi</dc:creator>
    </item>
    <item>
      <title>Algorithmic Monitoring: Measuring Market Stress with Machine Learning</title>
      <link>https://arxiv.org/abs/2602.07066</link>
      <description>arXiv:2602.07066v1 Announce Type: new 
Abstract: I construct a Market Stress Probability Index (MSPI) that estimates the probability of high stress in the U.S. equity market one month ahead using information from the cross-section of individual stocks. Using CRSP daily data, each month is summarized by a set of interpretable cross-sectional fragility signals and mapped into a forward-looking stress probability via an L1-regularized logistic regression in a real-time expanding-window design. Out of sample, MSPI tracks major stress episodes and improves discrimination and accuracy relative to a parsimonious benchmark based on lagged market return and realized volatility, delivering calibrated stress probabilities on an economically meaningful scale. Further, I illustrate how MSPI can be used as a probability-based measurement object in financial econometrics. The resulting index provides a transparent and easily updated measure of near-term equity-market stress risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07066v1</guid>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Schmitt</dc:creator>
    </item>
    <item>
      <title>Perfectly Fitting CDO Prices Across Tranches: A Theoretical Framework with Efficient Algorithms</title>
      <link>https://arxiv.org/abs/2602.08039</link>
      <description>arXiv:2602.08039v1 Announce Type: new 
Abstract: This paper addresses a key challenge in CDO modeling: achieving a perfect fit to market prices across all tranches using a single, consistent model. The existence of such a perfect-fit model implies the absence of arbitrage among CDO tranches and is thus essential for unified risk management and the pricing of nonstandard credit derivatives. To address this central challenge, we face three primary difficulties: standard parametric models typically fail to achieve a perfect fit; the calibration of standard parametric models inherently relies on computationally intensive simulation-based optimization; and there is a lack of formal theory to determine when a perfect-fit model exists and, if it exists, how to construct it. We propose a theoretical framework to overcome these difficulties. We first introduce and define two compatibility levels of market prices: weak compatibility and strong compatibility. Specifically, market prices across all tranches are said to be weakly (resp. strongly) compatible if there exists a single model (resp. a single conditionally i.i.d. model) that perfectly fits these market prices. We then derive sufficient and necessary conditions for both levels of compatibility by establishing a relationship between compatibility and LP problems. Furthermore, under either condition, we construct a corresponding concrete copula model that achieves a perfect fit. Notably, our framework not only allows for efficient verification of weak compatibility and strong compatibility through LP problems but also facilitates the construction of the corresponding copula models that achieve a perfect fit, eliminating the need for simulation-based optimization. The practical applications of our framework are demonstrated in risk management and the pricing of nonstandard credit derivatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08039v1</guid>
      <category>q-fin.RM</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lan Bu, Ning Cai, Chenxi Xia, Jingping Yang</dc:creator>
    </item>
    <item>
      <title>Eliciting reference measures of law-invariant functionals</title>
      <link>https://arxiv.org/abs/2507.13763</link>
      <description>arXiv:2507.13763v2 Announce Type: replace 
Abstract: Law-invariant functionals are central to risk management and assign identical values to random prospects sharing the same distribution under an atomless reference probability measure. This measure is typically assumed fixed. Here, we adopt the reverse perspective: given only observed functional values, we aim to either recover the reference measure or identify a candidate measure to test for law invariance when that property is not {\em a priori} satisfied. Our approach is based on a key observation about law-invariant functionals defined on law-invariant domains. These functionals define lower (upper) supporting sets in dual spaces of signed measures, and the suprema (infima) of these supporting sets -- if existent -- are scalar multiples of the reference measure. In specific cases, this observation can be formulated as a sandwich theorem. We illustrate the methodology through a detailed analysis of prominent examples: the entropic risk measure, Expected Shortfall, and Value-at-Risk. For the latter, our elicitation procedure initially fails due to the triviality of supporting set extrema. We therefore develop a suitable modification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13763v2</guid>
      <category>q-fin.RM</category>
      <category>math.PR</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix-Benedikt Liebrich, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>When Indemnity Insurance Fails: Parametric Coverage under Binding Budget and Risk Constraints</title>
      <link>https://arxiv.org/abs/2512.21973</link>
      <description>arXiv:2512.21973v5 Announce Type: replace-cross 
Abstract: In high-risk environments, traditional indemnity insurance is often unaffordable or ineffective, despite its well-known optimality under expected utility. We compare excess-of-loss indemnity insurance with parametric insurance within a common mean-variance framework, allowing for fixed costs, heterogeneous premium loadings, and binding budget constraints. Motivated by the disaster insurance and risk-sharing literature, we show that, once these realistic frictions are introduced, parametric insurance can yield higher welfare for risk-averse individuals, even under the same utility objective and without relying on behavioral assumptions. The welfare advantage arises precisely when indemnity insurance becomes impractical, and disappears once both contracts are unconstrained. Our results help reconcile classical insurance theory with the growing use of parametric risk transfer in high-risk settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21973v5</guid>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Avanzi, Debbie Kusch Falden, Mogens Steffensen</dc:creator>
    </item>
  </channel>
</rss>
