<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Dec 2025 05:02:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Analysis of a Discontinuous Galerkin Method for Diffusion Problems on Intersecting Domains</title>
      <link>https://arxiv.org/abs/2512.11111</link>
      <description>arXiv:2512.11111v1 Announce Type: new 
Abstract: The interior penalty discontinuous Galerkin method is applied to solve elliptic equations on either networks of segments or networks of planar surfaces, with arbitrary but fixed number of bifurcations. Stability is obtained by proving a discrete Poincar\'e's inequality on the hypergraphs. Convergence of the scheme is proved for $H^r$ regularity solution with $1 &lt; r \leq 2$. In the low regularity case ($r \leq 3/2$), a weak consistency result is obtained via generalized lifting operators for Sobolev spaces defined on hypergraphs. Numerical experiments confirm the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11111v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Miroslav Kuchta, Rami Masri, Beatrice Riviere</dc:creator>
    </item>
    <item>
      <title>An Optimal Weighted Least-Squares Method for Operator Learning</title>
      <link>https://arxiv.org/abs/2512.11168</link>
      <description>arXiv:2512.11168v1 Announce Type: new 
Abstract: We consider the problem of learning an unknown, possibly nonlinear operator between separable Hilbert spaces from supervised data. Inputs are drawn from a prescribed probability measure on the input space, and outputs are (possibly noisy) evaluations of the target operator. We regard admissible operators as square-integrable maps with respect to a fixed approximation measure, and we measure reconstruction error in the corresponding Bochner norm. For a finite-dimensional approximation space $V$ of dimension $N$, we study weighted least squares estimators in $V$ and establish probabilistic stability and accuracy bounds in the Bochner norm. We show that there exist sampling measures and weights - defined via an operator-level Christoffel function - that yield uniformly well-conditioned Gram matrices and near-optimal sample complexity, with a number of training samples $M$ on the order of $N \log N$. We complement the analysis by constructing explicit operator approximation spaces in cases of interest: rank-one linear operators that are dense in the class of bounded linear operators, and rank-one polynomial operators that are dense in the Bochner space under mild assumptions on the approximation measure. For both families we describe implementable procedures for sampling from the associated optimal measures. Finally, we demonstrate the effectiveness of this framework on several benchmark problems, including learning solution operators for the Poisson equation, viscous Burgers' equation, and the incompressible Navier-Stokes equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11168v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Turnage, Matthew Lowery, John Jakeman, Zachary Morrow, Akil Narayan, Varun Shankar</dc:creator>
    </item>
    <item>
      <title>On the Jacobi formula for Bivariate Pade Approximants of Rectangular Type</title>
      <link>https://arxiv.org/abs/2512.11238</link>
      <description>arXiv:2512.11238v1 Announce Type: new 
Abstract: In this paper a recursive algorithm is presented for evaluating multivariate Pad\'e approximants (of the rectangular type described in the work of Lutterodt) which is analogous to the Jacobi formula for univariate Pad\'e approximants. This algorithm is then applied to a (singular) Riccati differential equation to generate fast and accurate approximate solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11238v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gareth Hegarty</dc:creator>
    </item>
    <item>
      <title>Deconvolution of inclined channel elutriation data to infer particle size distribution</title>
      <link>https://arxiv.org/abs/2512.11318</link>
      <description>arXiv:2512.11318v1 Announce Type: new 
Abstract: In this paper we investigate the application of optimisation techniques in the deconvolution of mineral fractionation data obtained from a mathematical model for the operation of a fluidised bed with a set of inclined parallel channels mounted above. The model involved the transport equation with a stochastic source function and a linearly increasing fluidisation rate, with the overflow solids being collected in a finite number of increments (bags). Deconvolution of this data is an ill-posed problem and regularisation is required to provide feasible solutions. Deconvolution with regularisation is applied to a synthetic feed consisting of particles of constant density that vary in size only. It was found that the feed size distribution could be successfully deconvolved from the bag weights, with an accuracy that improved as the rate acceleration of the fluidisation rate was decreased. The deconvolution error only grew linearly with error in the measured bag masses. It was also shown that combining data from two different liquids can improve the accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11318v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jeffrey A. Hogan, Simon Iveson, Jason Mackellar, Kevin Galvin</dc:creator>
    </item>
    <item>
      <title>Projected Sobolev Natural Gradient Descent for Neural Variational Monte Carlo Solution of the Gross-Pitaevskii Equation</title>
      <link>https://arxiv.org/abs/2512.11339</link>
      <description>arXiv:2512.11339v1 Announce Type: new 
Abstract: This paper proposes a neural variational Monte Carlo method based on deep neural networks to solve the Gross-Pitaevskii equation (GPE) via projected Sobolev natural gradient descent (NGD). Adopting an "optimize-then-discretize" strategy, we first apply a constraint-preserving continuous Riemannian gradient flow on an infinite-dimensional Riemannian manifold, which is subsequently mapped to the neural network parameter space via Galerkin projection. This process naturally induces a Sobolev energy metric that incorporates physical information, effectively mitigating stiffness during optimization. To address the explicit dependence on the normalization constant caused by the nonlinear interaction term in the GPE, we design a hybrid sampling strategy combining an integration stream and a MCMC stream to achieve precise estimation of the generalized Gram matrix and energy gradients. Numerical experiments on benchmark cases, including the harmonic oscillator potential in the strong interaction limit and multi-scale optical lattice potentials, demonstrate the high accuracy of the proposed method. Furthermore, it achieves an order-of-magnitude acceleration in convergence compared to standard optimizers like Adam, exhibiting superior robustness in handling strong nonlinearities and complex geometric constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11339v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenglong Bao, Chen Cui, Kai Jiang, Shi Shu</dc:creator>
    </item>
    <item>
      <title>A meshless MUSCL method for the BGK-Boltzmann equation</title>
      <link>https://arxiv.org/abs/2512.11598</link>
      <description>arXiv:2512.11598v1 Announce Type: new 
Abstract: We present a numerical method for simulating rarefied gases that interact with moving boundaries and rigid bodies. The gas is described by the BGK equation in Lagrangian form and solved using an Arbitrary Lagrangian-Eulerian method, in which grid points move with the local mean velocity of the gas. The main advantage of the moving grid is that the algorithm can deal well with cases where the domain boundaries are time-dependent and the simulation domain contains rigid objects. Due to the irregular nature of the grid, we use a novel meshless MUSCL-like Moving Least Squares Method (MLS) for spatial discretisation coupled with a higher-order Implicit-Explicit Runge-Kutta method. To avoid spurious oscillations at discontinuities, we use the so-called Multi-dimensional Optimal Order Detection (MOOD) method with an adapted criterion to relax the discrete maximum property. Finally, we employ a new implementation of the boundary conditions that requires no iterative or extrapolation procedure. The method achieves fourth-order in 1D and second-order in 2D for simulations with moving boundaries. We demonstrate the method's effectiveness on classical test cases such as the driven square cavity, shear layer, and shock tube.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11598v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Klaas Willems, Axel Klar, Giovanni Russo, Giovanni Samaey, Sudarshan Tiwari</dc:creator>
    </item>
    <item>
      <title>A Fully Discrete Surface Finite Element Method for the Navier--Stokes equations on Evolving Surfaces with prescribed Normal Velocity</title>
      <link>https://arxiv.org/abs/2512.11737</link>
      <description>arXiv:2512.11737v1 Announce Type: new 
Abstract: We analyze two fully time-discrete numerical schemes for the incompressible Navier-Stokes equations posed on evolving surfaces in $\mathbb{R}^3$ with prescribed normal velocity using the evolving surface finite element method (ESFEM). We employ generalized Taylor-Hood finite elements $\mathrm{\mathbf{P}}_{k_u}$-- $\mathrm{P}_{k_{pr}}$-- $\mathrm{P}_{k_\lambda}$, $k_u=k_{pr}+1 \geq 2$, $k_\lambda\geq 1$, for the spatial discretization, where the normal velocity constraint is enforced weakly via a Lagrange multiplier $\lambda$, and a backward Euler discretization for the time-stepping procedure. Depending on the approximation order of $\lambda$ and weak formulation of the Navier-Stokes equations, we present stability and error analysis for two different discrete schemes, whose difference lies in the geometric information needed. We establish optimal velocity $L^{2}_{a_h}$-norm error bounds ($a_h$ an energy norm) for both schemes when $k_\lambda=k_u$, but only for the more information intensive one when $k_\lambda=k_u-1$, using iso-parametric and super-parametric discretizations, respectively, with the help of a newly derived surface Ritz-Stokes projection. Similarly, stability and optimal convergence for the pressures is established in an $L^2_{L^2}\times L^2_{H_h^{-1}}$-norm ($H_h^{-1}$ a discrete dual space) when $k_\lambda=k_u$, using a novel Leray time-projection to ensure weakly divergence conformity for our discrete velocity solution at two different time-steps (surfaces). Assuming further regularity conditions for the more information intensive scheme, along with an almost weak divergence conformity result at two different time-steps, we establish optimal $L^2_{L^2}\times L^2_{L^2}$-norm pressure error bounds when $k_\lambda=k_u-1$, using super-parametric approximation. Simulations verifying our results are provided, along with a comparison test against a penalty approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11737v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles M. Elliott, Achilleas Mavrakis</dc:creator>
    </item>
    <item>
      <title>Uplink Rate Maximization for Pinching Antenna- Assisted Covert Backscatter Communication</title>
      <link>https://arxiv.org/abs/2512.10970</link>
      <description>arXiv:2512.10970v1 Announce Type: cross 
Abstract: The emerging pinching antenna (PA) technology enables flexible antenna positioning for creating line-of-sight (LoS) links, thus offering substantial potential to facilitate ambient signal-based backscatter communication (BSC). This paper investigates PA-assisted BSC for enhanced communication and covertness in the presence of a randomly distributed eavesdropper. An optimization problem is formulated to maximize the uplink covert transmission rate by jointly optimizing the transmit power and antenna positions while satisfying both communication reliability and covertness constraints. An alternative optimization (AO)-based framework is proposed to solve this problem. Numerical results demonstrate that the proposed PA-BSC effectively mitigates the double near-far problem, where energy harvesting and backscatter transmission degrade simultaneously due to distance disparities, thereby improving downlink energy harvesting and uplink data transmission while maintaining covertness performance under practical deployment scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10970v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yulei Wang, Yalin Liu, Yaru Fu, Yuanwei Liu</dc:creator>
    </item>
    <item>
      <title>Data-Driven Model Reduction using WeldNet: Windowed Encoders for Learning Dynamics</title>
      <link>https://arxiv.org/abs/2512.11090</link>
      <description>arXiv:2512.11090v1 Announce Type: cross 
Abstract: Many problems in science and engineering involve time-dependent, high dimensional datasets arising from complex physical processes, which are costly to simulate. In this work, we propose WeldNet: Windowed Encoders for Learning Dynamics, a data-driven nonlinear model reduction framework to build a low-dimensional surrogate model for complex evolution systems. Given time-dependent training data, we split the time domain into multiple overlapping windows, within which nonlinear dimension reduction is performed by auto-encoders to capture latent codes. Once a low-dimensional representation of the data is learned, a propagator network is trained to capture the evolution of the latent codes in each window, and a transcoder is trained to connect the latent codes between adjacent windows. The proposed windowed decomposition significantly simplifies propagator training by breaking long-horizon dynamics into multiple short, manageable segments, while the transcoders ensure consistency across windows. In addition to the algorithmic framework, we develop a mathematical theory establishing the representation power of WeldNet under the manifold hypothesis, justifying the success of nonlinear model reduction via deep autoencoder-based architectures. Our numerical experiments on various differential equations indicate that WeldNet can capture nonlinear latent structures and their underlying dynamics, outperforming both traditional projection-based approaches and recently developed nonlinear model reduction methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11090v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biraj Dahal, Jiahui Cheng, Hao Liu, Rongjie Lai, Wenjing Liao</dc:creator>
    </item>
    <item>
      <title>Parametric Numerical Integration with (Differential) Machine Learning</title>
      <link>https://arxiv.org/abs/2512.11530</link>
      <description>arXiv:2512.11530v1 Announce Type: cross 
Abstract: In this work, we introduce a machine/deep learning methodology to solve parametric integrals. Besides classical machine learning approaches, we consider a differential learning framework that incorporates derivative information during training, emphasizing its advantageous properties. Our study covers three representative problem classes: statistical functionals (including moments and cumulative distribution functions), approximation of functions via Chebyshev expansions, and integrals arising directly from differential equations. These examples range from smooth closed-form benchmarks to challenging numerical integrals. Across all cases, the differential machine learning-based approach consistently outperforms standard architectures, achieving lower mean squared error, enhanced scalability, and improved sample efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11530v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>\'Alvaro Leitao, Jonatan R\'afales</dc:creator>
    </item>
    <item>
      <title>Gradient Descent as a Perceptron Algorithm: Understanding Dynamics and Implicit Acceleration</title>
      <link>https://arxiv.org/abs/2512.11587</link>
      <description>arXiv:2512.11587v1 Announce Type: cross 
Abstract: Even for the gradient descent (GD) method applied to neural network training, understanding its optimization dynamics, including convergence rate, iterate trajectories, function value oscillations, and especially its implicit acceleration, remains a challenging problem. We analyze nonlinear models with the logistic loss and show that the steps of GD reduce to those of generalized perceptron algorithms (Rosenblatt, 1958), providing a new perspective on the dynamics. This reduction yields significantly simpler algorithmic steps, which we analyze using classical linear algebra tools. Using these tools, we demonstrate on a minimalistic example that the nonlinearity in a two-layer model can provably yield a faster iteration complexity $\tilde{O}(\sqrt{d})$ compared to $\Omega(d)$ achieved by linear models, where $d$ is the number of features. This helps explain the optimization dynamics and the implicit acceleration phenomenon observed in neural networks. The theoretical results are supported by extensive numerical experiments. We believe that this alternative view will further advance research on the optimization of neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11587v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Tyurin</dc:creator>
    </item>
    <item>
      <title>A theoretical analysis on the inversion of matrices via Neural Networks designed with Strassen algorithm</title>
      <link>https://arxiv.org/abs/2501.06539</link>
      <description>arXiv:2501.06539v3 Announce Type: replace 
Abstract: We construct a Neural Network that approximates the matrix multiplication operator for any activation function such that there exists a Neural Network which can approximate the scalar multiplication function. In particular, we use the Strassen algorithm to reduce the number of weights and layers needed for such Neural Networks. This allows us to define another Neural Network for approximating the inverse matrix operator. Also, by relying on the Galerkin method, we apply those Neural Networks to solve parametric elliptic PDEs for a whole set of parameters. Finally, we discuss improvements with respect to the prior results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06539v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gonzalo Romera, Jon Asier B\'arcena-Petisco</dc:creator>
    </item>
    <item>
      <title>A parameterized block-splitting preconditioner for indefinite least squares problem</title>
      <link>https://arxiv.org/abs/2507.16938</link>
      <description>arXiv:2507.16938v3 Announce Type: replace 
Abstract: We present a stationary iteration based upon a block splitting for a class of indefinite least squares problem. Convergence of the proposed method is investigated and optimal value of the involving parameter is used. The induced preconditioner is applied to accelerate the convergence of the GMRES method for solving the problem. We also analysed the eigenpair distribution of the preconditioned matrix. {We assess the efficiency of the proposed preconditioner by presenting results from a numerical comparison with several existing preconditioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16938v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Davod Khojasteh Salkuyeh</dc:creator>
    </item>
    <item>
      <title>Error Analysis of Krylov Subspace approximation Based on IDR($s$) Method for Matrix Function Bilinear Forms</title>
      <link>https://arxiv.org/abs/2509.08563</link>
      <description>arXiv:2509.08563v3 Announce Type: replace 
Abstract: The bilinear form of a matrix function, namely $\mathbf{u}^\top f(A) \mathbf{v}$, appears in many scientific computing problems, where $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$, $A \in \mathbb{R}^{n \times n}$, and $f(z)$ is a given analytic function. The Induced Dimension Reduction IDR($s$) method was originally proposed to solve a large-scale linear system, and effectively reduces the complexity and storage requirement by dimension reduction techniques while maintaining the numerical stability of the algorithm. In fact, the IDR($s$) method can generate an interesting Hessenberg decomposition, our study just applies this fact to establish the numerical algorithm and a posteriori error estimate for the bilinear form of a matrix function $\mathbf{u}^{\top} f(A) \mathbf{v}$. Through the error analysis of the IDR($s$) algorithm, the corresponding error expansion is derived, and it is verified that the leading term of the error expansion serves as a reliable posteriori error estimate. Based on this, in this paper, a corresponding stopping criterion is proposed. Numerical examples are reported to support our theoretical findings and show the utility of our proposed method and its stopping criterion over the traditional Arnoldi-based method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08563v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianqian Xue, Xiaoqiang Yue, Xian-Ming Gu</dc:creator>
    </item>
    <item>
      <title>Fast and accurate computation of classical Gaussian quadratures</title>
      <link>https://arxiv.org/abs/2509.16716</link>
      <description>arXiv:2509.16716v2 Announce Type: replace 
Abstract: Algorithms for computing the classical Gaussian quadrature rules (Gauss--Jacobi, Gauss--Laguerre, and Gauss--Hermite) are presented, based on globally convergent fourth-order iterative methods combined with asymptotic approximations, which are applied in complementary regions of the parameter space. This approach yields methods that improve upon existing algorithms in speed, accuracy, and computational range. The MATLAB algorithm for Gauss--Jacobi is faster than previous methods and lifts the upper restrictions on the parameters imposed by those methods ($\alpha,\beta\le 5$); for example, for degrees up to $10^6$ all nodes and weights can be computed within the underflow limit for $-1&lt;\alpha,\beta\le 30$, and the computable range of parameters is much larger for smaller degrees, limited only by intrinsic overflow/underflow constraints. For the particular case of Gauss--Legendre quadrature ($\alpha=\beta=0$), a specific asymptotic approach is considered, which yields the most efficient MATLAB implementation available so far. The Gauss--Laguerre and Gauss--Hermite algorithms incorporate subsampling, and scaling is also available in order to extend the computational range. Gauss--Radau and Gauss--Lobatto variants are also considered, along with the computation of the associated barycentric weights. Additionally, arbitrary-precision algorithms (in Maple) are offered for the symmetric cases (Gauss--Gegenbauer and Gauss--Hermite), which can be used to compute thousands of nodes with hundreds of digits in a matter of seconds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16716v2</guid>
      <category>math.NA</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.CA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Gil, J. Segura, N. M. Temme</dc:creator>
    </item>
    <item>
      <title>On the optimality of dimension truncation error rates for a class of parametric partial differential equations</title>
      <link>https://arxiv.org/abs/2511.01492</link>
      <description>arXiv:2511.01492v2 Announce Type: replace 
Abstract: In uncertainty quantification for parametric partial differential equations (PDEs), it is common to model uncertain random field inputs using countably infinite sequences of independent and identically distributed random variables. The lognormal random field is a prime example of such a model. While there have been many studies assessing the error in the PDE response that occurs when an infinite-dimensional random field input is replaced with a finite-dimensional random field, there do not seem to be any analyses in the existing literature discussing the sharpness of these bounds. This work seeks to remedy the situation. Specifically, we investigate two model problems where the existing dimension truncation error rates can be shown to be sharp.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01492v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp A. Guth, Vesa Kaarnioja</dc:creator>
    </item>
    <item>
      <title>Spurious resonances for substructured FEM-BEM coupling</title>
      <link>https://arxiv.org/abs/2511.04501</link>
      <description>arXiv:2511.04501v2 Announce Type: replace 
Abstract: We are interested in time-harmonic acoustic scattering by an impenetrable obstacle in a medium where the wavenumber is constant in an exterior unbounded subdomain and is possibly heterogeneous in a bounded subdomain. The associated Helmholtz boundary value problem can be solved by coupling the Finite Element Method (FEM) in the heterogeneous subdomain with the Boundary Element Method (BEM) in the homogeneous subdomain. Recently, we designed and analyzed a new substructured FEM-BEM formulation, called Generalized Optimized Schwarz Method (GOSM). Unfortunately, it is well known that, even when the initial boundary value problem is well-posed, the variational formulation of classical FEM-BEM couplings can be ill-posed for certain wavenumbers, called spurious resonances. In this paper, we focus on the Johnson-N\'ed\'elec and Costabel couplings and show that the GOSM derived from both is not immune to that issue. In particular, we give an explicit expression of the kernel of the local operator associated with the interface between the FEM and BEM subdomains. That kernel and the one of classical FEM-BEM couplings are simultaneously non-trivial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04501v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonin Boisneault, Marcella Bonazzoli, Pierre Marchand, Xavier Claeys</dc:creator>
    </item>
    <item>
      <title>Numerical Analysis of 2D Stochastic Navier--Stokes Equations with Transport Noise: Regularity and Spatial Semidiscretization</title>
      <link>https://arxiv.org/abs/2512.03483</link>
      <description>arXiv:2512.03483v3 Announce Type: replace 
Abstract: This paper establishes strong convergence rates for the spatial finite element discretization of a two-dimensional stochastic Navier--Stokes
  system with transport noise and no-slip boundary conditions on a convex polygonal domain.
  The main challenge arises from the lack of spatial \(D(A)\)-regularity of the solution (where \(A\) is the Stokes operator),
  which prevents the application of standard error analysis techniques. Under a small-noise assumption, we prove that the weak solution satisfies \[ u \in L^2\bigl(\Omega; C([0,T]; \dot{H}_{\sigma}^{\varrho}) \cap L^2(0,T; \dot{H}_{\sigma}^{1+\varrho})\bigr) \] for some \(\varrho \in (0,\tfrac{1}{2})\). To address the low regularity in the numerical analysis, we introduce a novel smoothing operator \(J_{h,\alpha} = A_h^{\alpha}\mathcal{P}_h A^{-\alpha}\) with \(\alpha \in (0,1)\), where \(A_h\) is the discrete Stokes operator and \(\mathcal{P}_h\) the discrete Helmholtz projection. This tool enables a complete error analysis for a MINI-element spatial semidiscretization, yielding the mean-square convergence estimate \[ \|u - u_h\|_{L^2(\Omega; C([0,T]; L^2(\mathcal O;\mathbb{R}^2)))}
  + \|\nabla(u - u_h)\|_{L^2(\Omega \times (0,T); L^2(\mathcal{O};\mathbb{R}^{2\times2}))} \leqslant c\, h^{\varrho} \log\big(1 + \frac{1}{h}\big). \] The framework can be extended to broader stochastic fluid models with rough noise and Dirichlet boundary conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03483v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Binjie Li, Qin Zhou</dc:creator>
    </item>
    <item>
      <title>A New Class of General Linear Method with Inherent Quadratic Stability for Solving Stiff Differential Systems</title>
      <link>https://arxiv.org/abs/2512.05486</link>
      <description>arXiv:2512.05486v2 Announce Type: replace 
Abstract: This article proposes a new class of general linear method with $p=q$ and $r=s=p+1$. The construction of the present method is carried out using order conditions and error minimization subject to $A$- stability constraints. The proposed time integration schemes are $A$- and $L$-stable general linear methods (GLMs) equipped with inherent quadratic stability (IQS) criteria. We construct implicit GLMs of orders up to four with $p = q$ and $s = r$ along with the Nordsieck input vector assumption. Further, we test these schemes on three real-world problems: the van der Pol oscillator and two partial differential equations consisting of diffusion (Burgers' equation and the Gray-Scott model), and numerical results are presented. Computational results confirm that our proposed schemes are competitive with the existing GLMs and can be recognized as an alternative time integration scheme. We demonstrate the order of accuracy and convergence for the proposed schemes through observed order computation and error versus step size plots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05486v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Gautam, Ram K. Pandey</dc:creator>
    </item>
    <item>
      <title>Gaussian Process Regression for Uncertainty Quantification: An Introductory Tutorial</title>
      <link>https://arxiv.org/abs/2502.03090</link>
      <description>arXiv:2502.03090v3 Announce Type: replace-cross 
Abstract: Uncertainty Quantification (UQ) is essential for the reliable application of computational models in engineering and science. Among surrogate modeling techniques, Gaussian Process Regression (GPR) is particularly valuable for its non-parametric flexibility and inherent probabilistic output. This paper presents an introductory review of GPR-based methodologies within the context of UQ. We begin with an introduction to UQ and outline its key tasks, including uncertainty propagation, risk estimation, optimization under uncertainty, parameter estimation, and sensitivity analysis. We then introduce Gaussian Processes as a surrogate modeling technique, detailing their formulation, choice of covariance kernels, hyperparameter estimation, and active learning strategies for efficient data acquisition. The tutorial further explores how GPR can be applied to different UQ tasks, including Bayesian quadrature for uncertainty propagation, active learning-based risk estimation, Bayesian optimization for optimization under uncertainty, and surrogate-based sensitivity analysis. Throughout, we emphasize how to leverage the unique formulation of GP for these UQ tasks, rather than simply using it as a standard surrogate model. This work offers a comprehensive guide and unified framework for researchers seeking to rigorously apply probabilistic modeling to complex computational systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03090v3</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinglai Li, Hongqiao Wang</dc:creator>
    </item>
    <item>
      <title>Two Datasets Are Better Than One: Method of Double Moments for 3-D Reconstruction in Cryo-EM</title>
      <link>https://arxiv.org/abs/2511.07438</link>
      <description>arXiv:2511.07438v2 Announce Type: replace-cross 
Abstract: Cryo-electron microscopy (cryo-EM) is a powerful imaging technique for reconstructing three-dimensional molecular structures from noisy tomographic projection images of randomly oriented particles. We introduce a new data fusion framework, termed the method of double moments (MoDM), which reconstructs molecular structures from two instances of the second-order moment of projection images obtained under distinct orientation distributions: one uniform, the other non-uniform and unknown. We prove that these moments generically uniquely determine the underlying structure, up to a global rotation and reflection, and we develop a convex-relaxation-based algorithm that achieves accurate recovery using only second-order statistics. Our results demonstrate the advantage of collecting and modeling multiple datasets under different experimental conditions, illustrating that leveraging dataset diversity can substantially enhance reconstruction quality in computational imaging tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07438v2</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joe Kileel, Oscar Mickelin, Amit Singer, Sheng Xu</dc:creator>
    </item>
  </channel>
</rss>
