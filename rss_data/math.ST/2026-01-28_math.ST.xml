<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Jan 2026 05:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Asymptotic inference for skewed stable Ornstein-Uhlenbeck process</title>
      <link>https://arxiv.org/abs/2601.19071</link>
      <description>arXiv:2601.19071v1 Announce Type: new 
Abstract: We consider the parametric estimation of the Ornstein-Uhlenbeck process driven by a non-Gaussian $\alpha$-stable L\'{e}vy process with the stable index $\alpha&gt;1$ and possibly skewed jumps, based on a discrete-time sample over a fixed period. By employing a suitable non-diagonal normalizing matrix, we present the following: the parametric family satisfies the local asymptotic mixed normality with a non-degenerate Fisher information matrix; there exists a local maximum of the log-likelihood function which is asymptotically mixed-normal; the local maximum is asymptotically efficient in the sense that it has maximal concentration around the true value over symmetric convex Borel subsets. In the proof, we prove the asymptotic equivalence between the genuine likelihood and the much simpler Euler-type quasi-likelihood. Furthermore, we propose a simple moment-based method to estimate the parameters of the driving stable L\'{e}vy process, which serves as an initial estimator for numerical search of the (quasi-)likelihood, reducing the computational burden of the optimization to a large extent. We also present simulation results, which illustrate the theoretical results and highlight the advantages and disadvantages of the genuine and quasi-likelihood approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19071v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eitaro Kawamo, Hiroki Masuda</dc:creator>
    </item>
    <item>
      <title>Sufficient Conditions for Some Stochastic Orders of Discrete Random Variables with Applications in Reliability</title>
      <link>https://arxiv.org/abs/2601.19389</link>
      <description>arXiv:2601.19389v1 Announce Type: new 
Abstract: In this paper we focus on providing sufficient conditions for some well-known stochastic orders in reliability but dealing with the discrete versions of them, filling a gap in the literature. In particular, we find conditions based on the unimodality of the likelihood ratio for the comparison in some stochastic orders of two discrete random variables. These results have interest in comparing discrete random variables because the sufficient conditions are easy to check when there are no closed expressions for the survival functions, which occurs in many cases. In addition, the results are applied to compare several parametric families of discrete distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19389v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/math10010147</arxiv:DOI>
      <arxiv:journal_reference>Mathematics (2022), 10, 147</arxiv:journal_reference>
      <dc:creator>F. Belzunce, C. Mart\'inez-Riquelme, M. Pereda</dc:creator>
    </item>
    <item>
      <title>Semi-supervised learning in unmatched linear regression using an empirical likelihood approach</title>
      <link>https://arxiv.org/abs/2601.19649</link>
      <description>arXiv:2601.19649v1 Announce Type: new 
Abstract: Knowing the link between observed predictive variables and outcomes is crucial for making inference in any regression model. When this link is missing, partially or completely, classical estimation methods fail in recovering the true regression function. Deconvolution approaches have been proposed and studied in detail in the unmatched setting where the predictive variables and responses are allowed to be independent. In this work, we consider linear regression in a semi-supervised learning setting where, beside a small sample of matched data, we have access to a relatively large unmatched sample. Using maximum likelihood estimation, we show that under some mild assumptions the semi-supervised learning empirical maximum likelihood estimator (SSLEMLE) is asymptotically normal and give explicitly its asymptotic covariance matrix as a function of the ratio of the matched/unmatched sample sizes and other parameters. Furthermore, we quantify the statistical gain achieved by having the additional large unmatched sample over having only the small matched sample. To illustrate the theory, we present the results of an extensive simulation study and apply our methodology to the "combined cycle power plant" data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19649v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fadoua Balabdaoui, Jinyu Chen</dc:creator>
    </item>
    <item>
      <title>Normalized Fractional Order Entropy-Based Decision-Making Models under Risk</title>
      <link>https://arxiv.org/abs/2601.19715</link>
      <description>arXiv:2601.19715v1 Announce Type: new 
Abstract: Constructing efficient portfolios requires balancing expected returns with risk through optimal stock selection, while accounting for investor preferences. In a recent work by Paul and Kundu (2026), the fractional-order entropy due to Ubriaco was introduced as an uncertainty measure to capture varying investor attitudes toward risk. Building on this foundation, we introduce a novel normalized fractional order entropy aligned with investors' risk preferences that combines normalized fractional entropy with expected utility and variance. Risk sensitivity is modeled through the fractional parameter, interpolating between conservative or risk aversion and adventurous or high risk tolerance attitudes. Furthermore, the robustness and statistical significance of the fractional order entropy-based risk measure, termed normalized expected utility-fractional entropy (NEU-FE) and normalized expected utility-fractional entropy-variance (NEU-FEV) risk measures are explained with the help of machine learning tools, including Random forest, Ridge regression, Lasso Regression and artificial neural networks by using Indian stock market (NIFTY50). The results confirm that the proposed decision models support investors in making high-quality portfolio investments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19715v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Poulami Paul, Chanchal Kundu</dc:creator>
    </item>
    <item>
      <title>Estimating ordered variance of two scale mixture of normal distributions</title>
      <link>https://arxiv.org/abs/2601.19868</link>
      <description>arXiv:2601.19868v1 Announce Type: new 
Abstract: This study investigates component wise estimation of ordered variances of scale mixture of two normal distributions. For this study two special loss functions are considered namely squared error loss function and entropy loss function. We have derived the general improvement results and based on these results the estimators that outperform BAEE are obtained. Moreover under certain sufficient conditions a class of improved estimators is proposed for both loss functions. As a special case of scale mixture of normal distribution the results are applied to the multivariate t-distribution and obtained the improvement results. For this case a detailed numerical comparison is carried out which validates our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19868v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shrajal Bajpai, Lakshmi Kanta Patra</dc:creator>
    </item>
    <item>
      <title>Asymptotic Distribution of Robust Effect Size Index</title>
      <link>https://arxiv.org/abs/2601.19004</link>
      <description>arXiv:2601.19004v1 Announce Type: cross 
Abstract: The Robust Effect Size Index (RESI) is a recently proposed standardized effect size to quantify association strength across models. However, its confidence interval construction has relied on computationally intensive bootstrap procedures. We establish a general theorem for the asymptotic distribution of the RESI using a Taylor expansion that accommodates a broad class of models. Simulations under various linear and logistic regression settings show that RESI and its CI have smaller bias and more reliable coverage than commonly used effect sizes such as Cohen's d and f. Combining with robust covariance estimation yields valid inference under model misspecification. We use the methods to investigate associations of depression and behavioral problems with sex and diagnosis in Autism spectrum disorders, and demonstrate that the asymptotic approach achieves up to a 50-fold speedup over the bootstrap. Our work provides a scalable and reliable alternative to bootstrap inference, greatly enhancing the applicability of RESI to high-dimensional studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19004v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinyu Zhang, Rachael Muscatello, Megan Jones, Blythe Corbett, Simon Vandekar</dc:creator>
    </item>
    <item>
      <title>Average-Case Reductions for $k$-XOR and Tensor PCA</title>
      <link>https://arxiv.org/abs/2601.19016</link>
      <description>arXiv:2601.19016v1 Announce Type: cross 
Abstract: We study two canonical planted average-case problems -- noisy $k\mathsf{\text{-}XOR}$ and Tensor PCA -- and relate their computational properties via poly-time average-case reductions. In fact, we consider a \emph{family of problems} that interpolates between $k\mathsf{\text{-}XOR}$ and Tensor PCA, allowing intermediate densities and signal levels. We introduce two \emph{densifying} reductions that increase the number of observed entries while controlling the decrease in signal, and, in particular, reduce any $k\mathsf{\text{-}XOR}$ instance at the computational threshold to Tensor PCA at the computational threshold. Additionally, we give new order-reducing maps (e.g., $5\to 4$ $k\mathsf{\text{-}XOR}$ and $7\to 4$ Tensor PCA) at fixed entry density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19016v1</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Bresler, Alina Harbuzova</dc:creator>
    </item>
    <item>
      <title>Positive autocorrelation at unit lag for stationary random walk Metropolis-Hastings in ${\mathbb R}^d$</title>
      <link>https://arxiv.org/abs/2601.19323</link>
      <description>arXiv:2601.19323v1 Announce Type: cross 
Abstract: It is often asserted in the literature that one should expect positive autocorrelation for random walk Metropolis-Hastings (RWMH), especially if the typical proposal step-size is small relative to the variability in the target density. In this paper, we consider a stationary RWMH chain ${\bf X}$ taking values in $d$-dimensional Euclidean space and (subject only to the existence of densities with respect to Lebesgue measure) with general target distribution having finite second moment and general proposal random walk step-distribution. We prove, for any nonzero vector ${\bf c}$, strict positivity of the autocorrelation function at unit lag for the stochastic process $\langle{\bf c},{\bf X}\rangle$, that is, \[{\operatorname{Corr}}(\langle{\bf c},{\bf X}_0\rangle,\langle{\bf c},{\bf X}_1\rangle)&gt;0,\] and we establish the same result, but with weak inequality (which can in some cases be equality) when the state space for ${\bf X}$ is changed to the integer grid ${\mathbb Z}^d$. Further, for ${\bf c}\neq{\bf 0}$ we establish the sharp lower bound \[{\operatorname{Corr}}(\langle{\bf c},{\bf X}_0\rangle,\langle{\bf c},{\bf X}_1\rangle)&gt;\tfrac19\] on autocorrelation when we assume both that (i) the target density $\pi$ is spherically symmetric and unimodal in the specific sense that $\pi({\bf x})=\hat{\pi}(\|{\bf x}\|)$ for some nonincreasing function $\hat{\pi}$ on $[0,\infty)$ and that (ii) the proposal step-density is symmetric about ${\bf 0}$.
  We study the autocorrelation indirectly, by considering the incremental variance function (or incremental second-moment function) at unit lag. The same approach allows us also for $r\in[2,\infty)$ to upper-bound the incremental $r$th-absolute-moment function at unit lag.
  We give also closely related inequalities for the total variation distance between two distributions on ${\mathbb R}^d$ differing only by a location shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19323v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Allen Fill, Svante Janson</dc:creator>
    </item>
    <item>
      <title>Direct Doubly Robust Estimation of Conditional Quantile Contrasts</title>
      <link>https://arxiv.org/abs/2601.19666</link>
      <description>arXiv:2601.19666v1 Announce Type: cross 
Abstract: Within heterogeneous treatment effect (HTE) analysis, various estimands have been proposed to capture the effect of a treatment conditional on covariates. Recently, the conditional quantile comparator (CQC) has emerged as a promising estimand, offering quantile-level summaries akin to the conditional quantile treatment effect (CQTE) while preserving some interpretability of the conditional average treatment effect (CATE). It achieves this by summarising the treated response conditional on both the covariates and the untreated response. Despite these desirable properties, the CQC's current estimation is limited by the need to first estimate the difference in conditional cumulative distribution functions and then invert it. This inversion obscures the CQC estimate, hampering our ability to both model and interpret it. To address this, we propose the first direct estimator of the CQC, allowing for explicit modelling and parameterisation. This explicit parameterisation enables better interpretation of our estimate while also providing a means to constrain and inform the model. We show, both theoretically and empirically, that our estimation error depends directly on the complexity of the CQC itself, improving upon the existing estimation procedure. Furthermore, it retains the desirable double robustness property with respect to nuisance parameter estimation. We further show our method to outperform existing procedures in estimation accuracy across multiple data scenarios while varying sample size and nuisance error. Finally, we apply it to real-world data from an employment scheme, uncovering a reduced range of potential earnings improvement as participant age increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19666v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josh Givens, Song Liu, Henry W J Reeve, Katarzyna Reluga</dc:creator>
    </item>
    <item>
      <title>On randomized step sizes in Metropolis-Hastings algorithms</title>
      <link>https://arxiv.org/abs/2601.19710</link>
      <description>arXiv:2601.19710v1 Announce Type: cross 
Abstract: The performance of Metropolis-Hastings algorithms is highly sensitive to the choice of step size, and miss-specification can lead to severe loss of efficiency. We study algorithms with randomized step sizes, considering both auxiliary-variable and marginalized constructions. We show that algorithms with a randomized step size inherit weak Poincar\'e inequalities/spectral gaps from their fixed-step-size counterparts under minimal conditions, and that the marginalized kernel should always be preferred in terms of asymptotic variance to the auxiliary-variable choice if it is implementable. In addition we show that both types of randomization make an algorithm robust to tuning, meaning that spectral gaps decay polynomially as the step size is increasingly poorly chosen. We further show that step-size randomization often preserves high-dimensional scaling limits and algorithmic complexity, while increasing the optimal acceptance rate for Langevin and Hamiltonian samplers when an Exponential or Uniform distribution is chosen to randomize the step size. Theoretical results are complemented with a numerical study on challenging benchmarks such as Poisson regression, Neal's funnel and the Rosenbrock (banana) distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19710v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastiano Grazzi, Samuel Livingstone, Lionel Riou-Durand</dc:creator>
    </item>
    <item>
      <title>Zeroth-order parallel sampling</title>
      <link>https://arxiv.org/abs/2601.19722</link>
      <description>arXiv:2601.19722v1 Announce Type: cross 
Abstract: Finding effective ways to exploit parallel computing to accelerate Markov chain Monte Carlo methods is an important problem in Bayesian computation and related disciplines. In this paper, we consider the zeroth-order setting where the unnormalized target distribution can be evaluated but its gradient is unavailable for theoretical, practical, or computational reasons. We also assume access to $m$ parallel processors to accelerate convergence. The proposed approach is inspired by modern zeroth-order optimization methods, which mimic gradient-based schemes by replacing the gradient with a zeroth-order stochastic gradient estimator. Our contribution is twofold. First, we show that a naive application of popular zeroth-order stochastic gradient estimators within Markov chain Monte Carlo methods leads to algorithms with poor dependence on $m$, both for unadjusted and Metropolis-adjusted schemes. We then propose a simple remedy to this problem, based on a random-slice perspective, as opposed to a stochastic gradient one, obtaining a new class of zeroth-order samplers that provably achieve a polynomial speed-up in $m$. Theoretical findings are supported by numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19722v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Pozza, Giacomo Zanella</dc:creator>
    </item>
    <item>
      <title>Towards a complete characterization of indicator variograms and madograms</title>
      <link>https://arxiv.org/abs/2601.19800</link>
      <description>arXiv:2601.19800v1 Announce Type: cross 
Abstract: Indicator variograms and madograms are structural tools used in many disciplines of the natural sciences and engineering to describe random sets and random fields. To date, several necessary conditions are known for a function to be a valid indicator variogram but, except for intractable corner-positive inequalities, a complete characterization of indicator variograms is missing. Likewise, only partial characterizations of madograms are known. This paper provides novel necessary and sufficient conditions for a given function to be the variogram of an indicator random field with constant mean value or to be the madogram of a random field, and establishes under which conditions these two families of functions coincide. Our results apply to any set of points where the random field is defined and rely on distance geometry and Gaussian random field theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19800v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xavier Emery, Christian Lantu\'ejoul, Nadia Mery, Mohammad Maleki</dc:creator>
    </item>
    <item>
      <title>Revisiting Incremental Stochastic Majorization-Minimization Algorithms with Applications to Mixture of Experts</title>
      <link>https://arxiv.org/abs/2601.19811</link>
      <description>arXiv:2601.19811v1 Announce Type: cross 
Abstract: Processing high-volume, streaming data is increasingly common in modern statistics and machine learning, where batch-mode algorithms are often impractical because they require repeated passes over the full dataset. This has motivated incremental stochastic estimation methods, including the incremental stochastic Expectation-Maximization (EM) algorithm formulated via stochastic approximation. In this work, we revisit and analyze an incremental stochastic variant of the Majorization-Minimization (MM) algorithm, which generalizes incremental stochastic EM as a special case. Our approach relaxes key EM requirements, such as explicit latent-variable representations, enabling broader applicability and greater algorithmic flexibility. We establish theoretical guarantees for the incremental stochastic MM algorithm, proving consistency in the sense that the iterates converge to a stationary point characterized by a vanishing gradient of the objective. We demonstrate these advantages on a softmax-gated mixture of experts (MoE) regression problem, for which no stochastic EM algorithm is available. Empirically, our method consistently outperforms widely used stochastic optimizers, including stochastic gradient descent, root mean square propagation, adaptive moment estimation, and second-order clipped stochastic optimization. These results support the development of new incremental stochastic algorithms, given the central role of softmax-gated MoE architectures in contemporary deep neural networks for heterogeneous data modeling. Beyond synthetic experiments, we also validate practical effectiveness on two real-world datasets, including a bioinformatics study of dent maize genotypes under drought stress that integrates high-dimensional proteomics with ecophysiological traits, where incremental stochastic MM yields stable gains in predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.19811v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>TrungKhang Tran, TrungTin Nguyen, Gersende Fort, Tung Doan, Hien Duy Nguyen, Binh T. Nguyen, Florence Forbes, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Test for independence of long-range dependent time series using distance covariance</title>
      <link>https://arxiv.org/abs/2107.03041</link>
      <description>arXiv:2107.03041v3 Announce Type: replace 
Abstract: We apply the concept of distance covariance for testing independence of two long-range dependent time series. As test statistic we propose a linear combination of empirical distance cross-covariances. We derive the asymptotic distribution of the test statistic, and we show consistency against a very general class of alternatives. The asymptotic theory developed in this paper is based on a novel non-central limit theorem for stochastic processes with values in an $L^2$-Hilbert space. This limit theorem is of general theoretical interest which goes beyond the context of this article. Subject to the dependence in the data, the standardization and the limit distributions of the proposed test statistic vary. Since the limit distributions are unknown, we propose a subsampling procedure to determine the critical values for the proposed test, and we provide a proof for the validity of subsampling. In a simulation study, we investigate the finite-sample behavior of our test, and we compare its performance to tests based on the empirical cross-covariances. As an application of our results we analyze the cross-dependencies between mean monthly discharges of three rivers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.03041v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annika Betken, Herold Dehling</dc:creator>
    </item>
    <item>
      <title>Robust Signal Detection with Quadratically Convex Orthosymmetric Constraints</title>
      <link>https://arxiv.org/abs/2308.13036</link>
      <description>arXiv:2308.13036v2 Announce Type: replace 
Abstract: This paper is concerned with robust signal detection in Gaussian noise under quadratically convex orthosymmetric (QCO) constraints. Specifically, the null hypothesis $H _{0}$ assumes no signal, whereas the alternative $H _{1}$ considers signal that is separated in Euclidean norm from zero and belongs to a set $K$ satisfying the QCO constraints. In addition, an adversary is allowed to inspect all the $N$ samples and replace up to $\epsilon N$ of them with arbitrary values, where $0 &lt; \epsilon &lt; c_0 &lt; \frac{1}{2}$ is the corruption rate. Our main results establish the minimax rate of the separation radius $\rho _{\text{critical}}$ between $H _{0}$ and $H _{1}$ purely in terms of the geometry of $K$, the corruption rate $\epsilon$ (up to logarithmic factors in $\frac{1}{\epsilon }$) and the scale of the noise $\sigma $. We argue that the Kolmogorov widths of the constraint play a central role in determining the minimax rate. This indicates similarity with the (uncorrupted) estimation problem under QCO constraints, which was first established by Donoho et al. (1990). Moreover, the minimax lower bound reveals interesting phase transitions of the testing problem regarding $\epsilon $. Consistent with classic belief about testing and estimation, the testing problem is ``easier'' even when one compares the results with recent papers studying the constrained robust estimation problem. In addition to the main results above, where the upper bound is achieved with an intractable algorithm, inspired by Canonne et al. (2023), we develop a polynomial time algorithm which also nearly (up to logarithmic factors) achieves the minimax lower bound. In contrast to Canonne et al. (2023), our algorithm works for signals of arbitrary Euclidean length, and respects the QCO constraint. Finally, all the results above are naturally extended to the $\ell _{p}$ norm testing problem for $1 \le p&lt;2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13036v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yikun Li, Matey Neykov</dc:creator>
    </item>
    <item>
      <title>Statistical Hypothesis Testing for Information Value (IV)</title>
      <link>https://arxiv.org/abs/2309.13183</link>
      <description>arXiv:2309.13183v3 Announce Type: replace 
Abstract: Information Value (IV) is a widely used technique for feature selection prior to the modeling phase, particularly in credit scoring and related domains. However, conventional IV-based practices rely on fixed empirical thresholds, which lack statistical justification and may be sensitive to characteristics such as class imbalance. In this work, we develop a formal statistical framework for IV by establishing its connection with Jeffreys divergence and propose a novel nonparametric hypothesis test, referred to as the J-Divergence test. Our method provides rigorous asymptotic guarantees and enables interpretable decisions based on \(p\)-values. Numerical experiments, including synthetic and real-world data, demonstrate that the proposed test is more reliable than traditional IV thresholding, particularly under strong imbalance. The test is model-agnostic, computationally efficient, and well-suited for the pre-modeling phase in high-dimensional or imbalanced settings. An open-source Python library is provided for reproducibility and practical adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13183v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helder Rojas, Cirilo Alvarez, Nilton Rojas</dc:creator>
    </item>
    <item>
      <title>Minimax-Optimal Spectral Clustering with Covariance Projection for High-Dimensional Anisotropic Mixtures</title>
      <link>https://arxiv.org/abs/2502.02580</link>
      <description>arXiv:2502.02580v3 Announce Type: replace 
Abstract: In mixture models, anisotropic noise within each cluster is widely present in real-world data. This work investigates both computationally efficient procedures and fundamental statistical limits for clustering in high-dimensional anisotropic mixtures. We propose a new clustering method, Covariance Projected Spectral Clustering (COPO), which adapts to a wide range of dependent noise structures. We first project the data onto a low-dimensional space via eigen-decomposition of a diagonal-deleted Gram matrix. Our central methodological idea is to sharpen clustering in this embedding space by a covariance-aware reassignment step, using quadratic distances induced by estimated projected covariances. Through a novel row-wise analysis of the subspace estimation step in weak-signal regimes, which is of independent interest, we establish tight performance guarantees and algorithmic upper bounds for COPO, covering both Gaussian noise with flexible covariance and general noise with local dependence. To characterize the fundamental difficulty of clustering high-dimensional anisotropic Gaussian mixtures, we further establish two distinct and complementary minimax lower bounds, each highlighting different covariance-driven barriers. Our results show that COPO attains minimax-optimal misclustering rates in Gaussian settings. Extensive simulation studies across diverse noise structures, along with a real data application, demonstrate the superior empirical performance of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02580v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengzhu Huang, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>Safety of particle filters: Some results on the time evolution of particle filter estimates</title>
      <link>https://arxiv.org/abs/2503.21334</link>
      <description>arXiv:2503.21334v5 Announce Type: replace 
Abstract: Particle filters (PFs) form a class of Monte Carlo algorithms that propagate over time a set of $N\geq 1$ particles which can be used to estimate, in an online fashion, the sequence of filtering distributions $(\hat{\eta}_t)_{t\geq 1}$ defined by a state-space model. Despite the popularity of PFs, the study of the time evolution of their estimates has received barely any attention in the literature. Denoting by $(\hat{\eta}_t^N)_{t\geq 1}$ the PF estimate of $(\hat{\eta}_t)_{t\geq 1}$ and letting $\kappa\in (0,1/2)$, in this work we first show that for any number of particles $N$ it holds that, with probability one, we have $\|\hat{\eta}_t^N- \hat{\eta}_t\|\geq \kappa$ for infinitely many time instants $t\geq 1$, with $\|\cdot\|$ the Kolmogorov distance between probability distributions. Considering a simple filtering problem we then provide reassuring results concerning the ability of PFs to estimate jointly a finite set $\{\hat{\eta}_t\}_{t=1}^T$ of filtering distributions by studying the probability $\mathbb{P}(\sup_{t\in\{1,\dots,T\}}\|\hat{\eta}_t^{N}-\hat{\eta}_t\|\geq \kappa)$. Finally, on the same toy filtering problem, we prove that sequential quasi-Monte Carlo, a randomized quasi-Monte Carlo version of PF algorithms, offers greater safety guarantees than PFs in the sense that, for this algorithm, it holds that $\lim_{N\rightarrow\infty}\sup_{t\geq 1}\|\hat{\eta}_t^N-\hat{\eta}_t\|=0$ with probability one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21334v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathieu Gerber</dc:creator>
    </item>
    <item>
      <title>Sharp Asymptotic Minimaxity for Multiple Testing Using One-Group Shrinkage Priors</title>
      <link>https://arxiv.org/abs/2505.16428</link>
      <description>arXiv:2505.16428v4 Announce Type: replace 
Abstract: This paper investigates asymptotic minimaxity properties of Bayesian multiple testing rules in the sparse Gaussian sequence model using a broad class of global-local scale mixtures of normals as priors for the means. Minimaxity is studied under standard misclassification loss and the composite loss given by the sum of the false discovery proportion (FDP) and false non-discovery proportion (FNP). When the sparsity level is known, we show that by suitably choosing the global shrinkage parameter based on the sparsity level, our proposed testing rule achieves the exact minimax risk asymptotically for both losses under the ''beta-min'' separation condition. When the sparsity level is unknown, both empirical Bayes and fully Bayesian adaptations of the same rule are shown to achieve exact minimax risk asymptotically under suitable assumptions on sparsity. Our results reveal that minimaxity is attained for ''horseshoe-type'' priors that are broad enough to include the horseshoe, Strawderman-Berger, standard double Pareto, and certain inverse-gamma priors, among others. For non-''horseshoe-type'' priors, minimaxity fails to hold for either loss function. To the best of our knowledge, these are the first results of their kind for multiple hypothesis testing based on global-local shrinkage priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16428v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayantan Paul, Prasenjit Ghosh, Arijit Chakrabarti</dc:creator>
    </item>
    <item>
      <title>Polynomial Log-Marginals and Tweedie's Formula : When Is Bayes Possible?</title>
      <link>https://arxiv.org/abs/2509.05823</link>
      <description>arXiv:2509.05823v3 Announce Type: replace 
Abstract: Motivated by Tweedie's formula for the Compound Decision problem, we examine the theoretical foundations of empirical Bayes estimators that directly model the marginal density $m(y)$. Our main result shows that polynomial log-marginals of degree $k \ge 3 $ cannot arise from any valid prior distribution in exponential family models, while quadratic forms correspond exactly to Gaussian priors. This provides theoretical justification for why certain empirical Bayes decision rules, while practically useful, do not correspond to any formal Bayes procedures. We also strengthen the diagnostic by showing that a marginal is a Gaussian convolution only if it extends to a bounded solution of the heat equation in a neighborhood of the smoothing parameter, beyond the convexity of $c(y)=\tfrac12 y^2+\log m(y)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05823v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jyotishka Datta, Nicholas G. Polson</dc:creator>
    </item>
    <item>
      <title>Testing Imprecise Hypotheses</title>
      <link>https://arxiv.org/abs/2510.20717</link>
      <description>arXiv:2510.20717v2 Announce Type: replace 
Abstract: Many scientific applications involve testing theories that are only partially specified. This task often amounts to testing the goodness-of-fit of a candidate distribution while allowing for reasonable deviations from it. The tolerant testing framework provides a systematic way of constructing such tests. Rather than testing the simple null hypothesis that data was drawn from a candidate distribution, a tolerant test assesses whether the data is consistent with any distribution that lies within a given neighborhood of the candidate. As this neighborhood grows, the tolerance to misspecification increases, while the power of the test decreases. In this work, we characterize the information-theoretic trade-off between the size of the neighborhood and the power of the test, in several canonical models. On the one hand, we characterize the optimal trade-off for tolerant testing in the Gaussian sequence model, under deviations measured in both smooth and non-smooth norms. On the other hand, we study nonparametric analogues of this problem in smooth regression and density models. Along the way, we establish the sub-optimality of the classical chi-squared statistic for tolerant testing, and study simple alternative hypothesis tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20717v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Kania, Tudor Manole, Larry Wasserman, Sivaraman Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Beyond the noise: intrinsic dimension estimation with optimal neighbourhood identification</title>
      <link>https://arxiv.org/abs/2405.15132</link>
      <description>arXiv:2405.15132v4 Announce Type: replace-cross 
Abstract: The Intrinsic Dimension (ID) is a key concept in unsupervised learning and feature selection, as it is a lower bound to the number of variables which are necessary to describe a system. However, in almost any real-world dataset the ID depends on the scale at which the data are analysed. Quite typically at a small scale, the ID is very large, as the data are affected by measurement errors. At large scale, the ID can also be erroneously large, due to the curvature and the topology of the manifold containing the data. In this work, we introduce an automatic protocol to select the sweet spot, namely the correct range of scales in which the ID is meaningful and useful. This protocol is based on imposing that for distances smaller than the correct scale the density of the data is constant. In the presented framework, to estimate the density it is necessary to know the ID, therefore, this condition is imposed self-consistently. We illustrate the usefulness and robustness of this procedure to noise by benchmarks on artificial and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15132v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Di Noia, Iuri Macocco, Aldo Glielmo, Alessandro Laio, Antonietta Mira</dc:creator>
    </item>
    <item>
      <title>Incremental effects for continuous exposures</title>
      <link>https://arxiv.org/abs/2409.11967</link>
      <description>arXiv:2409.11967v3 Announce Type: replace-cross 
Abstract: Causal inference problems often involve continuous treatments, such as dose, duration, or frequency. However, identifying and estimating standard dose-response estimands requires that everyone has some chance of receiving any level of the exposure (i.e., positivity). To avoid this assumption, we consider stochastic interventions based on exponentially tilting the treatment distribution by some parameter $\delta$ (an incremental effect); this increases or decreases the likelihood a unit receives a given treatment level. We derive the efficient influence function and semiparametric efficiency bound for these incremental effects under continuous exposures. We then show estimation depends on the size of the tilt, as measured by $\delta$. In particular, we derive new minimax lower bounds illustrating how the best possible root mean squared error scales with an effective sample size of $n / \delta$, instead of $n$. Further, we establish new convergence rates and bounds on the bias of double machine learning-style estimators. Our novel analysis gives a better dependence on $\delta$ compared to standard analyses by using mixed supremum and $L_2$ norms. Finally, we define a "reflected" exponential tilt around any interior point and show that taking $\delta \to \infty$ yields a new estimator of the dose-response curve across the treatment support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11967v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Schindl, Shuying Shen, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>Testing independence and conditional independence in high dimensions via coordinatewise Gaussianization</title>
      <link>https://arxiv.org/abs/2504.02233</link>
      <description>arXiv:2504.02233v2 Announce Type: replace-cross 
Abstract: We propose new statistical tests, in high-dimensional settings, for testing the independence of two random vectors and their conditional independence given a third random vector. The key idea is simple, i.e., we first transform each component variable to the standard normal via its marginal empirical distribution, and we then test for independence and conditional independence of the transformed random vectors using appropriate $L_\infty$-type test statistics. While we are testing some necessary conditions of the independence or the conditional independence, the new tests outperform the 13 frequently used testing methods in a large scale simulation comparison. The advantage of the new tests can be summarized as follows: (i) they do not require any moment conditions, (ii) they allow arbitrary dependence structures of the components among the random vectors, and (iii) they allow the dimensions of random vectors to diverge at the exponential rates of the sample size. The critical values of the proposed tests are determined by a computationally efficient multiplier bootstrap procedure. Theoretical analysis shows that the sizes of the proposed tests can be well controlled by the nominal significance level, and the proposed tests are also consistent under certain local alternatives. The finite sample performance of the new tests is illustrated via extensive simulation studies and a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02233v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Yue Du, Jing He, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification of synchrosqueezing transform under complicated nonstationary noise</title>
      <link>https://arxiv.org/abs/2506.00779</link>
      <description>arXiv:2506.00779v2 Announce Type: replace-cross 
Abstract: We propose a bootstrapping framework to quantify uncertainty in time-frequency representations (TFRs) generated by the short-time Fourier transform (STFT) and the STFT-based synchrosqueezing transform (SST) for oscillatory signals with time-varying amplitude and frequency contaminated by complex nonstationary noise. To this end, we leverage a recent high-dimensional Gaussian approximation technique to establish a sequential Gaussian approximation for nonstationary processes under mild assumptions. This result is of independent interest and provides a theoretical basis for characterizing the approximate Gaussianity of STFT-induced TFRs as random fields. Building on this foundation, we establish the robustness of SST-based signal decomposition in the presence of nonstationary noise. Furthermore, assuming locally stationary noise, we develop a Gaussian autoregressive bootstrap for uncertainty quantification of SST-based TFRs and provide theoretical justification. We validate the proposed methods with simulations and illustrate their practical utility by analyzing spindle activity in electroencephalogram recordings. Our work bridges time-frequency analysis in signal processing and nonlinear spectral analysis of time series in statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00779v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hau-Tieng Wu, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Maximum-likelihood estimation of the Mat\'ern covariance structure of isotropic spatial random fields on finite, sampled grids</title>
      <link>https://arxiv.org/abs/2509.06223</link>
      <description>arXiv:2509.06223v2 Announce Type: replace-cross 
Abstract: We present a statistically and computationally efficient spectral-domain maximum-likelihood procedure to solve for the structure of Gaussian spatial random fields within the Matern covariance hyperclass. For univariate, stationary, and isotropic fields, the three controlling parameters are the process variance, smoothness, and range. The debiased Whittle likelihood maximization explicitly treats discretization and edge effects for finite sampled regions in parameter estimation and uncertainty quantification. As even the best parameter estimate may not be good enough, we provide a test for whether the model specification itself warrants rejection. Our results are practical and relevant for the study of a variety of geophysical fields, and for spatial interpolation, out-of-sample extension, kriging, machine learning, and feature detection of geological data. We present procedural details and high-level results on real-world examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06223v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederik J. Simons, Olivia L. Walbert, Arthur P. Guillaumin, Gabriel L. Eggers, Kevin W. Lewis, Sofia C. Olhede</dc:creator>
    </item>
    <item>
      <title>A Generalized Adaptive Joint Learning Framework for High-Dimensional Time-Varying Models</title>
      <link>https://arxiv.org/abs/2601.04499</link>
      <description>arXiv:2601.04499v2 Announce Type: replace-cross 
Abstract: In modern biomedical and econometric studies, longitudinal processes are often characterized by complex time-varying associations and abrupt regime shifts that are shared across correlated outcomes. Standard functional data analysis (FDA) methods, which prioritize smoothness, often fail to capture these dynamic structural features, particularly in high-dimensional settings. This article introduces Adaptive Joint Learning (AJL), a hierarchical regularization framework designed to integrate functional variable selection with structural changepoint detection in multivariate time-varying coefficient models. Unlike standard simultaneous estimation approaches, we propose a theoretically grounded two-stage screening-and-refinement procedure. This framework first synergizes adaptive group-wise penalization with sure screening principles to robustly identify active predictors, followed by a refined fused regularization step that effectively borrows strength across multiple outcomes to detect local regime shifts. We provide a rigorous theoretical analysis of the estimator in the ultra-high-dimensional regime (p &gt;&gt; n). Crucially, we establish the sure screening consistency of the first stage, which serves as the foundation for proving that the refined estimator achieves the oracle property-performing as well as if the true active set and changepoint locations were known a priori. A key theoretical contribution is the explicit handling of approximation bias via undersmoothing conditions to ensure valid asymptotic inference. The proposed method is validated through comprehensive simulations and an application to Sleep-EDF data, revealing novel dynamic patterns in physiological states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04499v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baolin Chen, Mengfei Ran</dc:creator>
    </item>
  </channel>
</rss>
