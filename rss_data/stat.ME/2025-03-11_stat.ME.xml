<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Mar 2025 02:15:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sample size determination for machine learning in medical research</title>
      <link>https://arxiv.org/abs/2503.05809</link>
      <description>arXiv:2503.05809v1 Announce Type: new 
Abstract: Machine learning (ML) methods are being increasingly used across various domains of medicine research. However, despite advancements in the use of ML in medicine, clear and definitive guidelines for determining sample sizes in medical ML research are lacking. This article proposes a method for determining sample sizes for medical research utilizing ML methods, beginning with the determination of the testing set sample size, followed with the determination of the training set and total sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05809v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wan Nor Arifin, Najib Majdi Yaacob</dc:creator>
    </item>
    <item>
      <title>Model-based bi-clustering using multivariate Poisson-lognormal with general block-diagonal covariance matrix and its applications</title>
      <link>https://arxiv.org/abs/2503.05961</link>
      <description>arXiv:2503.05961v1 Announce Type: new 
Abstract: While several Gaussian mixture models-based biclustering approaches currently exist in the literature for continuous data, approaches to handle discrete data have not been well researched. A multivariate Poisson-lognormal (MPLN) model-based bi-clustering approach that utilizes a block-diagonal covariance structure is introduced to allow for a more flexible structure of the covariance matrix. Two variations of the algorithm are developed where the number of column clusters: 1) are assumed equal across groups or 2) can vary across groups. Variational Gaussian approximation is utilized for parameter estimation, and information criteria are used for model selection. The proposed models are investigated in the context of clustering multivariate count data. Using simulated data the models display strong accuracy and computational efficiency and is applied to breast cancer RNA-sequence data from The Cancer Genome Atlas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05961v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caitlin Kral, Evan Chance, Ryan Browne, Sanjeena Dang</dc:creator>
    </item>
    <item>
      <title>Shiny-MAGEC: A Bayesian R Shiny Application for Meta-analysis of Censored Adverse Events</title>
      <link>https://arxiv.org/abs/2503.05982</link>
      <description>arXiv:2503.05982v1 Announce Type: new 
Abstract: Accurate assessment of adverse event (AE) incidence is critical in clinical cancer research for drug safety evaluation and regulatory approval. While meta-analysis serves as an essential tool to comprehensively synthesize the evidence across multiple studies, incomplete AE reporting in clinical trials remains a persistent challenge. In particular, AEs occurring below study-specific reporting thresholds are often omitted from publications, leading to left-censored data. Failure to account for these censored AE counts can result in biased AE incidence estimates. We present an R Shiny application that implements a one-stage Bayesian meta-analysis model specifically designed to incorporate censored AE data into the estimation process. This interactive tool provides a user-friendly interface for researchers to conduct AE meta-analyses and estimate the AE incidence probability following the bias-correction methods proposed by Qi et al. (2024). It also enables direct comparisons between models that either incorporate or ignore censoring, highlighting the biases introduced by conventional approaches. This tutorial demonstrates the Shiny application's functionality through an illustrative example on meta-analysis of PD-1/PD-L1 inhibitor safety and highlights the importance of this tool in improving AE risk assessment. Ultimately, the new Shiny app facilitates more accurate and transparent drug safety evaluations. The Shiny-MAGEC app is available at: https://zihanzhou98.shinyapps.io/Shiny-MAGEC/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05982v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihan Zhou, Zizhong Tian, Christine B. Peterson, Le Bao, Shouhao Zhou</dc:creator>
    </item>
    <item>
      <title>Randomized Quasi-Monte Carlo Features for Kernel Approximation</title>
      <link>https://arxiv.org/abs/2503.06041</link>
      <description>arXiv:2503.06041v1 Announce Type: new 
Abstract: We investigate the application of randomized quasi-Monte Carlo (RQMC) methods in random feature approximations for kernel-based learning. Compared to the classical Monte Carlo (MC) approach \citep{rahimi2007random}, RQMC improves the deterministic approximation error bound from $O_P(1/\sqrt{n})$ to $O(1/M)$ (up to logarithmic factors), matching the rate achieved by quasi-Monte Carlo (QMC) methods \citep{huangquasi}. Beyond the deterministic error bound guarantee, we further establish additional average error bounds for RQMC features: some requiring weaker assumptions and others significantly reducing the exponent of the logarithmic factor. In the context of kernel ridge regression, we show that RQMC features offer computational advantages over MC features while preserving the same statistical error rate. Empirical results further show that RQMC methods maintain stable performance in both low and moderately high-dimensional settings, unlike QMC methods, which suffer from significant performance degradation as dimension increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06041v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yian Huang, Zhen Huang</dc:creator>
    </item>
    <item>
      <title>Bayesian Machine Learning for Estimating Optimal Dynamic Treatment Regimes with Ordinal Outcomes</title>
      <link>https://arxiv.org/abs/2503.06199</link>
      <description>arXiv:2503.06199v1 Announce Type: new 
Abstract: Dynamic treatment regimes (DTRs) are sequences of decision rules designed to tailor treatment based on patients' treatment history and evolving disease status. Ordinal outcomes frequently serve as primary endpoints in clinical trials and observational studies. However, constructing optimal DTRs for ordinal outcomes has been underexplored. This paper introduces a Bayesian machine learning (BML) framework to handle ordinal outcomes in the DTR setting. To deal with potential nonlinear associations between outcomes and predictors, we first introduce ordinal Bayesian additive regression trees (OBART), a new model that integrates the latent variable framework within the traditional Bayesian additive regression trees (BART). We then incorporate OBART into the BML to estimate optimal DTRs based on ordinal data and quantify the associated uncertainties. Extensive simulation studies are conducted to evaluate the performance of the proposed approach against existing methods. We demonstrate the application of the proposed approach using data from a smoking cessation trial and provide the OBART R package along with R code for implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06199v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinru Wang, Tanujit Chakraborty, Bibhas Chakraborty</dc:creator>
    </item>
    <item>
      <title>Gaussian mixture copulas for flexible dependence modelling in the body and tails of joint distributions</title>
      <link>https://arxiv.org/abs/2503.06255</link>
      <description>arXiv:2503.06255v1 Announce Type: new 
Abstract: Fully describing the entire data set is essential in multivariate risk assessment, since moderate levels of one variable can influence another, potentially leading it to be extreme. Additionally, modelling both non-extreme and extreme events within a single framework avoids the need to select a threshold vector used to determine an extremal region, or the requirement to add flexibility to bridge between separate models for the body and tail regions. We propose a copula model, based on a mixture of Gaussian distributions, as this model avoids the need to define an extremal region, it is scalable to dimensions beyond the bivariate case, and it can handle both asymptotic dependent and asymptotic independent extremal dependence structures. We apply the proposed model through simulations and to a 5-dimensional seasonal air pollution data set, previously analysed in the multivariate extremes literature. Through pairwise, trivariate and 5-dimensional analyses, we show the flexibility of the Gaussian mixture copula in capturing different joint distributional behaviours and its ability to identify potential graphical structure features, both of which can vary across the body and tail regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06255v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\'idia M. Andr\'e, Jonathan A. Tawn</dc:creator>
    </item>
    <item>
      <title>Parameter Estimation and Inference in a Continuous Piecewise Linear Regression Model</title>
      <link>https://arxiv.org/abs/2503.06303</link>
      <description>arXiv:2503.06303v1 Announce Type: new 
Abstract: The estimation of regression parameters in one dimensional broken stick models is a research area of statistics with an extensive literature. We are interested in extending such models by aiming to recover two or more intersecting (hyper)planes in multiple dimensions. In contrast to approaches aiming to recover a given number of piecewise linear components using either a grid search or local smoothing around the change points, we show how to use Nesterov smoothing to obtain a smooth and everywhere differentiable approximation to a piecewise linear regression model with a uniform error bound. The parameters of the smoothed approximation are then efficiently found by minimizing a least squares objective function using a quasi-Newton algorithm. Our main contribution is threefold: We show that the estimates of the Nesterov smoothed approximation of the broken plane model are also $\sqrt{n}$ consistent and asymptotically normal, where $n$ is the number of data points on the two planes. Moreover, we show that as the degree of smoothing goes to zero, the smoothed estimates converge to the unsmoothed estimates and present an algorithm to perform parameter estimation. We conclude by presenting simulation results on simulated data together with some guidance on suitable parameter choices for practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06303v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georg Hahn, Moulinath Banerjee, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>On a fast consistent selection of nested models with possibly unnormalised probability densities</title>
      <link>https://arxiv.org/abs/2503.06331</link>
      <description>arXiv:2503.06331v1 Announce Type: new 
Abstract: Models with unnormalized probability density functions are ubiquitous in statistics, artificial intelligence and many other fields. However, they face significant challenges in model selection if the normalizing constants are intractable. Existing methods to address this issue often incur high computational costs, either due to numerical approximations of normalizing constants or evaluation of bias corrections in information criteria. In this paper, we propose a novel and fast selection criterion, T-GIC, for nested models, allowing direct data sampling from a possibly unnormalized probability density function. T-GIC gives a consistent selection under mild regularity conditions and is computationally efficient, benefiting from a multiplying factor that depends only on the sample size and the model complexity. Extensive simulation studies and real-data applications demonstrate the efficacy of T-GIC in the selection of nested models with unnormalized probability densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06331v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rong Bian, Kung-Sik Chan, Bing Cheng, Howell Tong</dc:creator>
    </item>
    <item>
      <title>Fairness-aware organ exchange and kidney paired donation</title>
      <link>https://arxiv.org/abs/2503.06431</link>
      <description>arXiv:2503.06431v1 Announce Type: new 
Abstract: The kidney paired donation (KPD) program provides an innovative solution to overcome incompatibility challenges in kidney transplants by matching incompatible donor-patient pairs and facilitating kidney exchanges. To address unequal access to transplant opportunities, there are two widely used fairness criteria: group fairness and individual fairness. However, these criteria do not consider protected patient features, which refer to characteristics legally or ethically recognized as needing protection from discrimination, such as race and gender. Motivated by the calibration principle in machine learning, we introduce a new fairness criterion: the matching outcome should be conditionally independent of the protected feature, given the sensitization level. We integrate this fairness criterion as a constraint within the KPD optimization framework and propose a computationally efficient solution. Theoretically, we analyze the associated price of fairness using random graph models. Empirically, we compare our fairness criterion with group fairness and individual fairness through both simulations and a real-data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06431v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingrui Zhang, Xiaowu Dai, Lexin Li</dc:creator>
    </item>
    <item>
      <title>Statistical Inference of the Matthews Correlation Coefficient for Multiclass Classification</title>
      <link>https://arxiv.org/abs/2503.06450</link>
      <description>arXiv:2503.06450v1 Announce Type: new 
Abstract: Classification problems are essential statistical tasks that form the foundation of decision-making across various fields, including patient prognosis and treatment strategies for critical conditions. Consequently, evaluating the performance of classification models is of significant importance, and numerous evaluation metrics have been proposed. Among these, the Matthews correlation coefficient (MCC), also known as the phi coefficient, is widely recognized as a reliable metric that provides balanced measurements even in the presence of class imbalance. However, with the increasing prevalence of multiclass classification problems involving three or more classes, macro-averaged and micro-averaged extensions of MCC have been employed, despite a lack of clear definitions or established references for these extensions. In the present study, we propose a formal framework for MCC tailored to multiclass classification problems using macro-averaged and micro-averaged approaches. Moreover, discussions on the use of these extended MCCs for multiclass problems often rely solely on point estimates, potentially overlooking the statistical significance and reliability of the results. To address this gap, we introduce several methods for constructing asymptotic confidence intervals for the proposed metrics. Furthermore, we extend these methods to include the construction of asymptotic confidence intervals for differences in the proposed metrics, specifically for paired study designs. The utility of our methods is evaluated through comprehensive simulations and real-world data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06450v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Tamura, Yuki Itaya, Kenichi Hayashi, Kouji Yamamoto</dc:creator>
    </item>
    <item>
      <title>Bayesian Synthetic Control with a Soft Simplex Constraint</title>
      <link>https://arxiv.org/abs/2503.06454</link>
      <description>arXiv:2503.06454v1 Announce Type: new 
Abstract: Whether the synthetic control method should be implemented with the simplex constraint and how to implement it in a high-dimensional setting have been widely discussed. To address both issues simultaneously, we propose a novel Bayesian synthetic control method that integrates a soft simplex constraint with spike-and-slab variable selection. Our model is featured by a hierarchical prior capturing how well the data aligns with the simplex assumption, which enables our method to efficiently adapt to the structure and information contained in the data by utilizing the constraint in a more flexible and data-driven manner. A unique computational challenge posed by our model is that conventional Markov chain Monte Carlo sampling algorithms for Bayesian variable selection are no longer applicable, since the soft simplex constraint results in an intractable marginal likelihood. To tackle this challenge, we propose to update the regression coefficients of two predictors simultaneously from their full conditional posterior distribution, which has an explicit but highly complicated characterization. This novel Gibbs updating scheme leads to an efficient Metropolis-within-Gibbs sampler that enables effective posterior sampling from our model and accurate estimation of the average treatment effect. Simulation studies demonstrate that our method performs well across a wide range of settings, in terms of both variable selection and treatment effect estimation, even when the true data-generating process does not adhere to the simplex constraint. Finally, application of our method to two empirical examples in the economic literature yields interesting insights into the impact of economic policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06454v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Xu, Quan Zhou</dc:creator>
    </item>
    <item>
      <title>Extremes of structural causal models</title>
      <link>https://arxiv.org/abs/2503.06536</link>
      <description>arXiv:2503.06536v1 Announce Type: new 
Abstract: The behavior of extreme observations is well-understood for time series or spatial data, but little is known if the data generating process is a structural causal model (SCM). We study the behavior of extremes in this model class, both for the observational distribution and under extremal interventions. We show that under suitable regularity conditions on the structure functions, the extremal behavior is described by a multivariate Pareto distribution, which can be represented as a new SCM on an extremal graph. Importantly, the latter is a sub-graph of the graph in the original SCM, which means that causal links can disappear in the tails. We further introduce a directed version of extremal graphical models and show that an extremal SCM satisfies the corresponding Markov properties. Based on a new test of extremal conditional independence, we propose two algorithms for learning the extremal causal structure from data. The first is an extremal version of the PC-algorithm, and the second is a pruning algorithm that removes edges from the original graph to consistently recover the extremal graph. The methods are illustrated on river data with known causal ground truth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06536v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Engelke, Nicola Gnecco, Frank R\"ottger</dc:creator>
    </item>
    <item>
      <title>Association measures for two-way contingency tables based on multi-categorical proportional reduction in error</title>
      <link>https://arxiv.org/abs/2503.06538</link>
      <description>arXiv:2503.06538v1 Announce Type: new 
Abstract: In two-way contingency tables under an asymmetric situation, where the row and column variables are defined as explanatory and response variables respectively, quantifying the extent to which the explanatory variable contributes to predicting the response variable is important. One quantification method is the association measure, which indicates the degree of association in a range from $0$ to $1$. Among various measures, those based on proportional reduction in error (PRE) are particularly notable for their simplicity and intuitive interpretation. These measures, including Goodman-Kruskal's lambda proposed in 1954, are widely implemented in statistical software such as R and SAS and remain extensively used. However, a known limitation of PRE measures is their potential to return a value of $0$ despite no independence. This issue arises because the measures are constructed based solely on the maximum joint and marginal probabilities, failing to utilize the information available in the contingency table fully. To address this problem, we propose new association measures designed for the proportional reduction in error with multiple categories. The properties of the proposed measure are examined and their utility is demonstrated through simulations and real data analyses. The results suggest their potential as practical tools in applied statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06538v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wataru Urasaki, Kouji Tahata, Sadao Tomizawa</dc:creator>
    </item>
    <item>
      <title>How to improve the regression factor score predictor when individuals have different factor loadings</title>
      <link>https://arxiv.org/abs/2503.06742</link>
      <description>arXiv:2503.06742v1 Announce Type: new 
Abstract: Previous research has shown that ignoring individual differences of factor loadings in conventional factor models may reduce the determinacy of factor score predictors. Therefore, the aim of the present study is to propose a heterogeneous regression factor score with larger determinacy than the conventional regression factor score when individuals have different factor loadings. First, a method for the estimation of individual loadings is proposed. The individual loading estimates are used to compute the heterogeneity-based regression factor score predictor. Then, a binomial test for loading heterogeneity of a factor is recommended to compute the heterogeneity-based regression factor score predictor only when the test is significant. Otherwise, the conventional regression factor score predictor should be used. A simulation study reveals that the heterogeneity-based regression factor score predictor has larger determinacy than the conventional regression factor score predictor in populations with substantial loading heterogeneity. An empirical example based on subsamples drawn randomly from a large sample of Big Five Markers indicates that the determinacy can be improved for the factor emotional stability when the heterogeneity-based regression factor score is computed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06742v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e Beauducel, Norbert Hilger, Anneke C. Weide</dc:creator>
    </item>
    <item>
      <title>Robust local empirical Bayes correction for Bayesian modeling</title>
      <link>https://arxiv.org/abs/2503.06837</link>
      <description>arXiv:2503.06837v1 Announce Type: new 
Abstract: This paper investigates a robust empirical Bayes correction for Bayesian modeling. We show the application of the model on income distribution. Income shock includes temporal and permanent shocks. We aim to eliminate temporal shock and permanent shock using two-step local empirical correction method. Our results show that only 6.7% of the observed income shocks were permanent shock, and the posterior (permanent) mean weekly income was reduced from the observed income 415 pounds to 202 pounds for the United Kingdom using the Living Costs and Food Survey in 2021-2022. Keywords: Empirical Bayes correction; Outliers; Bayesian modeling</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06837v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
    <item>
      <title>Doubly robust omnibus sensitivity analysis of externally controlled trials with intercurrent events</title>
      <link>https://arxiv.org/abs/2503.06864</link>
      <description>arXiv:2503.06864v1 Announce Type: new 
Abstract: Externally controlled trials are crucial in clinical development when randomized controlled trials are unethical or impractical. These trials consist of a full treatment arm with the experimental treatment and a full external control arm. However, they present significant challenges in learning the treatment effect due to the lack of randomization and a parallel control group. Besides baseline incomparability, outcome mean non-exchangeability, caused by differences in conditional outcome distributions between external controls and counterfactual concurrent controls, is infeasible to test and may introduce biases in evaluating the treatment effect. Sensitivity analysis of outcome mean non-exchangeability is thus critically important to assess the robustness of the study's conclusions against such assumption violations. Moreover, intercurrent events, which are ubiquitous and inevitable in clinical studies, can further confound the treatment effect and hinder the interpretation of the estimated treatment effects. This paper establishes a semi-parametric framework for externally controlled trials with intercurrent events, offering doubly robust and locally optimal estimators for primary and sensitivity analyses. We develop an omnibus sensitivity analysis that accounts for both outcome mean non-exchangeability and the impacts of intercurrent events simultaneously, ensuring root-n consistency and asymptotic normality under specified conditions. The performance of the proposed sensitivity analysis is evaluated in simulation studies and a real-data problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06864v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chenyin Gao, Xiang Zhang, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Data-Driven Sequential Sampling for Tail Risk Mitigation</title>
      <link>https://arxiv.org/abs/2503.06913</link>
      <description>arXiv:2503.06913v1 Announce Type: new 
Abstract: Given a finite collection of stochastic alternatives, we study the problem of sequentially allocating a fixed sampling budget to identify the optimal alternative with a high probability, where the optimal alternative is defined as the one with the smallest value of extreme tail risk. We particularly consider a situation where these alternatives generate heavy-tailed losses whose probability distributions are unknown and may not admit any specific parametric representation. In this setup, we propose data-driven sequential sampling policies that maximize the rate at which the likelihood of falsely selecting suboptimal alternatives decays to zero. We rigorously demonstrate the superiority of the proposed methods over existing approaches, which is further validated via numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06913v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dohyun Ahn, Taeho Kim</dc:creator>
    </item>
    <item>
      <title>Confidence distributions for the parameters in an autoregressive process</title>
      <link>https://arxiv.org/abs/2503.07064</link>
      <description>arXiv:2503.07064v1 Announce Type: new 
Abstract: We suggest how to construct joint confidence distributions for several parameters and apply these ideas to an autoregressive process of general order. The implied non informative prior for the parameters, i.e. the ratio between the confidence density and the likelihood function, is proved to be asymptotically flat in the stationary case. However, in the presence of a unit root, the implied prior needs to be adjusted. The results are illustrated by simulation studies and empirical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07064v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rolf Larsson</dc:creator>
    </item>
    <item>
      <title>Representative dietary behavior patterns and associations with cardiometabolic outcomes in Puerto Rico using a Bayesian latent class analysis for non-probability samples</title>
      <link>https://arxiv.org/abs/2503.07240</link>
      <description>arXiv:2503.07240v1 Announce Type: new 
Abstract: There is limited understanding of how dietary behaviors cluster together and influence cardiometabolic health at a population level in Puerto Rico. Data availability is scarce, particularly outside of urban areas, and is often limited to non-probability sample (NPS) data where sample inclusion mechanisms are unknown. In order to generalize results to the broader Puerto Rican population, adjustments are necessary to account for selection bias but are difficult to implement for NPS data. Although Bayesian latent class models enable summaries of dietary behavior variables through underlying patterns, they have not yet been adapted to the NPS setting. We propose a novel Weighted Overfitted Latent Class Analysis for Non-probability samples (WOLCAN). WOLCAN utilizes a quasi-randomization framework to (1) model pseudo-weights for an NPS using Bayesian additive regression trees (BART) and a reference probability sample, and (2) integrate the pseudo-weights within a weighted pseudo-likelihood approach for Bayesian latent class analysis, while propagating pseudo-weight uncertainty into parameter estimation. A stacked sample approach is used to allow shared individuals between the NPS and the reference sample. We evaluate model performance through simulations and apply WOLCAN to data from the Puerto Rico Observational Study of Psychosocial, Environmental, and Chronic Disease Trends (PROSPECT). We identify dietary behavior patterns for adults in Puerto Rico aged 30 to 75 and examine their associations with type 2 diabetes, hypertension, and hypercholesterolemia. Our findings suggest that an out-of-home eating pattern is associated with a higher likelihood of these cardiometabolic outcomes compared to a nutrition-sensitive pattern. WOLCAN effectively reveals generalizable dietary behavior patterns and demonstrates relevant applications in studying diet-disease relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07240v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephanie M. Wu, Abrania Marrero, Matthew R. Williams, Terrance D. Savitsky, Josiemer Mattei, Jos\'e Rodr\'iguez-Orengo, Briana J. K. Stephenson</dc:creator>
    </item>
    <item>
      <title>A right-truncated Poisson mixture model for analyzing count data</title>
      <link>https://arxiv.org/abs/2503.07254</link>
      <description>arXiv:2503.07254v1 Announce Type: new 
Abstract: In this paper, we investigate right-truncated count data models incorporating cavariates into the parameters. A regression method is proposed to model
  right-truncated count data exibiting high heterogeneity. The study encompasses the formulation of the proposed model, parameter estimation using an Expectation-Maximisation (EM) algorithm, and the properties of these estimators. We also discuss model selection procedures for the proposed method. Furthermore, a Monte Carlo simulation study is presented to assess the performance of the proposed method and the model selection process. Results express accuracy under regularity conditions of the model. The method is used to analyze the determinants of the degree of adherence to preventive
  measures during teh COVID-19 pandemic. in northern Benin. The results show that a right-truncated Poisson mixture model is adequate to analyze these
  data. Using this model, we conclude that age, education level, and household size determine an individual's degree of adherence to preventive measures during COVID-19 in this region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07254v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Babagnid\'e Fran\c{c}ois Koladjo, Ricardo Anderson Donte, Epiphane Sodjinou</dc:creator>
    </item>
    <item>
      <title>Robust Multilinear Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2503.07327</link>
      <description>arXiv:2503.07327v1 Announce Type: new 
Abstract: Multilinear Principal Component Analysis (MPCA) is an important tool for analyzing tensor data. It performs dimension reduction similar to PCA for multivariate data. However, standard MPCA is sensitive to outliers. It is highly influenced by observations deviating from the bulk of the data, called casewise outliers, as well as by individual outlying cells in the tensors, so-called cellwise outliers. This latter type of outlier is highly likely to occur in tensor data, as tensors typically consist of many cells. This paper introduces a novel robust MPCA method that can handle both types of outliers simultaneously, and can cope with missing values as well. This method uses a single loss function to reduce the influence of both casewise and cellwise outliers. The solution that minimizes this loss function is computed using an iteratively reweighted least squares algorithm with a robust initialization. Graphical diagnostic tools are also proposed to identify the different types of outliers that have been found by the new robust MPCA method. The performance of the method and associated graphical displays is assessed through simulations and illustrated on two real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07327v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mehdi Hirari, Fabio Centofanti, Mia Hubert, Stefan Van Aelst</dc:creator>
    </item>
    <item>
      <title>Change-plane analysis in functional response quantile regression</title>
      <link>https://arxiv.org/abs/2503.07332</link>
      <description>arXiv:2503.07332v1 Announce Type: new 
Abstract: Change-plane analysis is a pivotal tool for identifying subgroups within a heterogeneous population, yet it presents challenges when applied to functional data. In this paper, we consider a change-plane model within the framework of functional response quantile regression, capable of identifying and testing subgroups in non-Gaussian functional responses with scalar predictors. The proposed model naturally extends the change-plane method to account for the heterogeneity in functional data. To detect the existence of subgroups, we develop a weighted average of the squared score test statistic, which has a closed form and thereby reduces the computational stress. An alternating direction method of multipliers algorithm is formulated to estimate the functional coefficients and the grouping parameters. We establish the asymptotic theory for the estimates based on the reproducing kernel Hilbert space and derive the asymptotic distributions of the proposed test statistic under both null and alternative hypotheses. Simulation studies are conducted to evaluate the performance of the proposed approach in subgroup identification and hypothesis test. The proposed methods are also applied to two datasets, one from a study on China stocks and another from the COVID-19 pandemic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07332v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Guan, Yiyuan Li, Xu Liu, Jinhong You</dc:creator>
    </item>
    <item>
      <title>The Multi-Trip Time-Dependent Mix Vehicle Routing Problem for Hybrid Autonomous Shared Delivery Location and Traditional Door-to-Door Delivery Modes</title>
      <link>https://arxiv.org/abs/2503.05842</link>
      <description>arXiv:2503.05842v1 Announce Type: cross 
Abstract: Rising labor costs and increasing logistical demands pose significant challenges to modern delivery systems. Automated Electric Vehicles (AEVs) could reduce reliance on delivery personnel and increase route flexibility, but their adoption is limited due to varying customer acceptance and integration complexities. Shared Distribution Locations (SDLs) offer an alternative to door-to-door (D2D) delivery by providing a wider delivery window and serving multiple community customers, thereby improving last-mile logistics through reduced delivery time, lower costs, and higher customer satisfaction.This paper introduces the Multi-Trip Time-Dependent Hybrid Vehicle Routing Problem (MTTD-MVRP), a challenging variant of the Vehicle Routing Problem (VRP) that combines Autonomous Electric Vehicles (AEVs) with conventional vehicles. The problem's complexity arises from factors such as time-dependent travel speeds, strict time windows, battery limitations, and driver labor constraints, while integrating both SDLs and D2D deliveries. To solve the MTTD-MVRP efficiently, we develop a tailored meta-heuristic based on Adaptive Large Neighborhood Search (ALNS) augmented with column generation (CG). This approach intensively explores the solution space using problem-specific operators and adaptively refines solutions, balancing high-quality outcomes with computational effort. Extensive experiments show that the proposed method delivers near-optimal solutions for large-scale instances within practical time limits.From a managerial perspective, our findings highlight the importance of integrating autonomous and human-driven vehicles in last-mile logistics. Decision-makers can leverage SDLs to reduce operational costs and carbon footprints while still accommodating customers who require or prefer D2D services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05842v1</guid>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyi Zhao, Jiayu Yang, Haoxiang Yang</dc:creator>
    </item>
    <item>
      <title>Black Box Causal Inference: Effect Estimation via Meta Prediction</title>
      <link>https://arxiv.org/abs/2503.05985</link>
      <description>arXiv:2503.05985v1 Announce Type: cross 
Abstract: Causal inference and the estimation of causal effects plays a central role in decision-making across many areas, including healthcare and economics. Estimating causal effects typically requires an estimator that is tailored to each problem of interest. But developing estimators can take significant effort for even a single causal inference setting. For example, algorithms for regression-based estimators, propensity score methods, and doubly robust methods were designed across several decades to handle causal estimation with observed confounders. Similarly, several estimators have been developed to exploit instrumental variables (IVs), including two-stage least-squares (TSLS), control functions, and the method-of-moments. In this work, we instead frame causal inference as a dataset-level prediction problem, offloading algorithm design to the learning process. The approach we introduce, called black box causal inference (BBCI), builds estimators in a black-box manner by learning to predict causal effects from sampled dataset-effect pairs. We demonstrate accurate estimation of average treatment effects (ATEs) and conditional average treatment effects (CATEs) with BBCI across several causal inference problems with known identification, including problems with less developed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05985v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucius E. J. Bynum, Aahlad Manas Puli, Diego Herrero-Quevedo, Nhi Nguyen, Carlos Fernandez-Granda, Kyunghyun Cho, Rajesh Ranganath</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization for Robust Identification of Ornstein-Uhlenbeck Model</title>
      <link>https://arxiv.org/abs/2503.06381</link>
      <description>arXiv:2503.06381v1 Announce Type: cross 
Abstract: This paper deals with the identification of the stochastic Ornstein-Uhlenbeck (OU) process error model, which is characterized by an inverse time constant, and the unknown variances of the process and observation noises. Although the availability of the explicit expression of the log-likelihood function allows one to obtain the maximum likelihood estimator (MLE), this entails evaluating the nontrivial gradient and also often struggles with local optima. To address these limitations, we put forth a sample-efficient global optimization approach based on the Bayesian optimization (BO) framework, which relies on a Gaussian process (GP) surrogate model for the objective function that effectively balances exploration and exploitation to select the query points. Specifically, each evaluation of the objective is implemented efficiently through the Kalman filter (KF) recursion. Comprehensive experiments on various parameter settings and sampling intervals corroborate that BO-based estimator consistently outperforms MLE implemented by the steady-state KF approximation and the expectation-maximization algorithm (whose derivation is a side contribution) in terms of root mean-square error (RMSE) and statistical consistency, confirming the effectiveness and robustness of the BO for identification of the stochastic OU process. Notably, the RMSE values produced by the BO-based estimator are smaller than the classical Cram\'{e}r-Rao lower bound, especially for the inverse time constant, estimating which has been a long-standing challenge. This seemingly counterintuitive result can be explained by the data-driven prior for the learning parameters indirectly injected by BO through the GP prior over the objective function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06381v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinwen Xu, Qin Lu, Yaakov Bar-Shalom</dc:creator>
    </item>
    <item>
      <title>fastfrechet: An R package for fast implementation of Fr\'echet regression with distributional responses</title>
      <link>https://arxiv.org/abs/2503.06401</link>
      <description>arXiv:2503.06401v1 Announce Type: cross 
Abstract: Distribution-as-response regression problems are gaining wider attention, especially within biomedical settings where observation-rich patient specific data sets are available, such as feature densities in CT scans (Petersen et al., 2021) actigraphy (Ghosal et al., 2023), and continuous glucose monitoring (Coulter et al., 2024; Matabuena et al., 2021). To accommodate the complex structure of such problems, Petersen and M\"uller (2019) proposed a regression framework called Fr\'echet regression which allows non-Euclidean responses, including distributional responses. This regression framework was further extended for variable selection by Tucker et al. (2023), and Coulter et al. (2024) (arXiv:2403.00922 [stat.AP]) developed a fast variable selection algorithm for the specific setting of univariate distributional responses equipped with the 2-Wasserstein metric (2-Wasserstein space). We present "fastfrechet", an R package providing fast implementation of these Fr\'echet regression and variable selection methods in 2-Wasserstein space, with resampling tools for automatic variable selection. "fastfrechet" makes distribution-based Fr\'echet regression with resampling-supplemented variable selection readily available and highly scalable to large data sets, such as the UK Biobank (Doherty et al., 2017).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06401v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Coulter, Rebecca Lee, Irina Gaynanova</dc:creator>
    </item>
    <item>
      <title>Censoring-Aware Tree-Based Reinforcement Learning for Estimating Dynamic Treatment Regimes with Censored Outcomes</title>
      <link>https://arxiv.org/abs/2503.06690</link>
      <description>arXiv:2503.06690v1 Announce Type: cross 
Abstract: Dynamic Treatment Regimes (DTRs) provide a systematic approach for making sequential treatment decisions that adapt to individual patient characteristics, particularly in clinical contexts where survival outcomes are of interest. Censoring-Aware Tree-Based Reinforcement Learning (CA-TRL) is a novel framework to address the complexities associated with censored data when estimating optimal DTRs. We explore ways to learn effective DTRs, from observational data. By enhancing traditional tree-based reinforcement learning methods with augmented inverse probability weighting (AIPW) and censoring-aware modifications, CA-TRL delivers robust and interpretable treatment strategies. We demonstrate its effectiveness through extensive simulations and real-world applications using the SANAD epilepsy dataset, where it outperformed the recently proposed ASCL method in key metrics such as restricted mean survival time (RMST) and decision-making accuracy. This work represents a step forward in advancing personalized and data-driven treatment strategies across diverse healthcare settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06690v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Animesh Kumar Paul, Russell Greiner</dc:creator>
    </item>
    <item>
      <title>A Unified View of Optimal Kernel Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2503.07084</link>
      <description>arXiv:2503.07084v1 Announce Type: cross 
Abstract: This paper provides a unifying view of optimal kernel hypothesis testing across the MMD two-sample, HSIC independence, and KSD goodness-of-fit frameworks. Minimax optimal separation rates in the kernel and $L^2$ metrics are presented, with two adaptive kernel selection methods (kernel pooling and aggregation), and under various testing constraints: computational efficiency, differential privacy, and robustness to data corruption. Intuition behind the derivation of the power results is provided in a unified way accross the three frameworks, and open problems are highlighted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07084v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Antonin Schrab</dc:creator>
    </item>
    <item>
      <title>Complexity Analysis of Environmental Time Series</title>
      <link>https://arxiv.org/abs/2503.07582</link>
      <description>arXiv:2503.07582v1 Announce Type: cross 
Abstract: Small, forested catchments are prototypes of terrestrial ecosystems and have been studied in several disciplines of environmental sciences since several decades. Time series of water and matter fluxes and nutrient concentrations from these systems exhibit a bewildering diversity of spatio-temporal patterns, indicating the intricate nature of processes acting on a large range of time scales. Nonlinear dynamics is an obvious framework to investigate catchment time series. We analyze selected long-term data from three headwater catchments in the Bramke valley, Harz mountains, Lower Saxony in Germany at common biweekly resolution for the period 1991 to 2023. For every time series, we perform gap filling, detrending and removal of the annual cycle using Singular System Analysis (SSA), and then calculate metrics based on ordinal pattern statistics: the permutation entropy, permutation complexity and Fisher information, as well as their generalized versions (q-entropy and {\alpha}-entropy). Further, the position of each variable in Tarnopolski diagrams is displayed and compared to reference stochastic processes, like fractional Brownian motion, fractional Gaussian noise, and \b{eta} noise. Still another way of distinguishing deterministic chaos and structured noise, and quantifying the latter, is provided by the complexity from ordinal pattern positioned slopes (COPPS). We also construct Horizontal Visibility Graphs and estimate the exponent of the decay of the degree distribution. Taken together, the analyses create a characterization of the dynamics of these systems which can be scrutinized for universality, either across variables or between the three geographically very close catchments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07582v1</guid>
      <category>nlin.CD</category>
      <category>nlin.AO</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Holger Lange, Michael Hauhs</dc:creator>
    </item>
    <item>
      <title>Extremal Dependence Concepts</title>
      <link>https://arxiv.org/abs/1512.03232</link>
      <description>arXiv:1512.03232v3 Announce Type: replace 
Abstract: The probabilistic characterization of the relationship between two or more random variables calls for a notion of dependence. Dependence modeling leads to mathematical and statistical challenges, and recent developments in extremal dependence concepts have drawn a lot of attention to probability and its applications in several disciplines. The aim of this paper is to review various concepts of extremal positive and negative dependence, including several recently established results, reconstruct their history, link them to probabilistic optimization problems, and provide a list of open questions in this area. While the concept of extremal positive dependence is agreed upon for random vectors of arbitrary dimensions, various notions of extremal negative dependence arise when more than two random variables are involved. We review existing popular concepts of extremal negative dependence given in literature and introduce a novel notion, which in a general sense includes the existing ones as particular cases. Even if much of the literature on dependence is focused on positive dependence, we show that negative dependence plays an equally important role in the solution of many optimization problems. While the most popular tool used nowadays to model dependence is that of a copula function, in this paper we use the equivalent concept of a set of rearrangements. This is not only for historical reasons. Rearrangement functions describe the relationship between random variables in a completely deterministic way, allow a deeper understanding of dependence itself, and have several advantages on the approximation of solutions in a broad class of optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:1512.03232v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/15-STS525</arxiv:DOI>
      <arxiv:journal_reference>Statistical Science 2015, Vol. 30, No. 4, 485-517</arxiv:journal_reference>
      <dc:creator>Giovanni Puccetti, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Change-point regression with a smooth additive disturbance</title>
      <link>https://arxiv.org/abs/2112.03878</link>
      <description>arXiv:2112.03878v2 Announce Type: replace 
Abstract: We assume a nonparametric regression model where the signal is given by the sum of a piecewise constant function and a smooth function. To detect the change-points and estimate the regression functions, we propose PCpluS, a combination of the fused Lasso and kernel smoothing. In contrast to existing approaches, it explicitly uses the additive decomposition of the signal when detecting change-points. This is motivated by several applications and by theoretical results about partial linear model. We show how the use of the Epanechnikov kernel in the linear smoother results in very fast computation. Simulations demonstrate that our approach has a small mean squared error and detects change-points well. We also apply the methodology to genome sequencing data to detect copy number variations. Finally, we demonstrate its flexibility by proposing extensions to multivariate and filtered data. An R-package called PCpluS is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.03878v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Pein, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>Adaptive truncation of infinite sums: applications to Statistics</title>
      <link>https://arxiv.org/abs/2202.06121</link>
      <description>arXiv:2202.06121v2 Announce Type: replace 
Abstract: It is often the case in Statistics that one needs to compute sums of infinite series, especially in marginalising over discrete latent variables. This has become more relevant with the popularization of gradient-based techniques (e.g. Hamiltonian Monte Carlo) in the Bayesian inference context, for which discrete latent variables are hard or impossible to deal with. For many commonly used infinite series, custom algorithms have been developed which exploit specific features of each problem. General techniques, suitable for a large class of problems with limited input from the user are less established. We employ basic results from the theory of infinite series to investigate general, problem-agnostic algorithms to truncate infinite sums within an arbitrary tolerance $\varepsilon &gt; 0$ and provide robust computational implementations with provable guarantees. We compare three tentative solutions to estimating the infinite sum of interest: (i) a "naive" approach that sums terms until the terms are below the threshold $\varepsilon$; (ii) a `bounding pair' strategy based on trapping the true value between two partial sums; and (iii) a `batch' strategy that computes the partial sums in regular intervals and stops when their difference is less than $\varepsilon$. We show under which conditions each strategy guarantees the truncated sum is within the required tolerance and compare the error achieved by each approach, as well as the number of function evaluations necessary for each one. A detailed discussion of numerical issues in practical implementations is also provided. The paper provides some theoretical discussion of a variety of statistical applications, including raw and factorial moments and count models with observation error. Finally, detailed illustrations in the form noisy MCMC for Bayesian inference and maximum marginal likelihood estimation are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.06121v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luiz Max Carvalho, Wellington J. Silva, Guido A. Moreira</dc:creator>
    </item>
    <item>
      <title>Double Machine Learning Methods for Estimating Average Treatment Effects: A Comparative Study</title>
      <link>https://arxiv.org/abs/2204.10969</link>
      <description>arXiv:2204.10969v5 Announce Type: replace 
Abstract: Observational cohort studies are increasingly being used for comparative effectiveness research to assess the safety of therapeutics. Recently, various doubly robust methods have been proposed for average treatment effect estimation by combining the treatment model and the outcome model via different vehicles, such as matching, weighting, and regression. The key advantage of doubly robust estimators is that they require either the treatment model or the outcome model to be correctly specified to obtain a consistent estimator of average treatment effects, and therefore lead to a more accurate and often more precise inference. However, little work has been done to understand how doubly robust estimators differ due to their unique strategies of using the treatment and outcome models and how machine learning techniques can be combined to boost their performance, which we call double machine learning estimators. Here we examine multiple popular doubly robust methods and compare their performance using different treatment and outcome modeling via extensive simulations and a real-world application. We found that incorporating machine learning with doubly robust estimators such as the targeted maximum likelihood estimator gives the best overall performance. Practical guidance on how to apply doubly robust estimators is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10969v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoqing Tan, Shu Yang, Wenyu Ye, Douglas E. Faries, Ilya Lipkovich, Zbigniew Kadziola</dc:creator>
    </item>
    <item>
      <title>lsirm12pl: An R package for latent space item response modeling</title>
      <link>https://arxiv.org/abs/2205.06989</link>
      <description>arXiv:2205.06989v3 Announce Type: replace 
Abstract: The item response model in latent space (LSIRM; Jeon et al., 2021) uncovers unobserved interactions between respondents and items in the item response data by embedding both in a shared latent metric space. The R package lsirm12pl implements Bayesian estimation of the LSIRM and its extensions for various response types, base model specifications, and missing data handling. Furthermore, lsirm12pl package provides methods to improve model utilization and interpretation, such as clustering item positions on an estimated interaction map. The package also offers convenient summary and plotting options to evaluate and process the estimated results. In this paper, we provide an overview of the LSIRM's methodological foundation and describe several extensions included in the package. We then demonstrate the use of the package with real data examples contained within it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.06989v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongyoung Go, Gwanghee Kim, Jina Park, Junyong Park, Minjeong Jeon, Ick Hoon Jin</dc:creator>
    </item>
    <item>
      <title>Predicting Distributions of Physical Activity Profiles in the NHANES Database Using a Partially Linear Fr\'echet Single Index Model</title>
      <link>https://arxiv.org/abs/2302.07692</link>
      <description>arXiv:2302.07692v2 Announce Type: replace 
Abstract: Object-oriented data analysis is a fascinating and evolving field in modern statistical science, with the potential to make significant contributions to biomedical applications. This statistical framework facilitates the development of new methods to analyze complex data objects that capture more information than traditional clinical biomarkers. This paper applies the object-oriented framework to analyze physical activity levels, measured by accelerometers, as response objects in a regression model. Unlike traditional summary metrics, we utilize a recently proposed representation of physical activity data as a distributional object, providing a more nuanced and complete profile of individual energy expenditure across all ranges of monitoring intensity. A novel hybrid Fr\'echet regression model is proposed and applied to US population accelerometer data from National Health and Nutrition Examination Survey (NHANES) 2011-2014. The semi-parametric nature of the model allows for the inclusion of nonlinear effects for critical variables, such as age, which are biologically known to have subtle impacts on physical activity. Simultaneously, the inclusion of linear effects preserves interpretability for other variables, particularly categorical covariates such as ethnicity and sex. The results obtained are valuable from a public health perspective and could lead to new strategies for optimizing physical activity interventions in specific American subpopulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07692v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena, Aritra Ghosal, Wendy Meiring, Alexander Petersen</dc:creator>
    </item>
    <item>
      <title>Design of Bayesian A/B Tests Controlling False Discovery Rates and Power</title>
      <link>https://arxiv.org/abs/2312.10814</link>
      <description>arXiv:2312.10814v3 Announce Type: replace 
Abstract: Online controlled experiments (i.e., A/B tests) are a critical tool used by businesses with digital operations to optimize their products and services. These experiments routinely track information related to various business metrics, each of which summarizes a different aspect of how users interact with an online platform. Although multiple metrics are commonly tracked, this information is often not well utilized; multiple metrics are often aggregated into a single composite measure, losing valuable information, or strict family-wise error rate adjustments are imposed, leading to reduced power. In this paper, we propose an economical framework to design Bayesian A/B tests while controlling both power and the false discovery rate (FDR). Selecting optimal decision thresholds to control power and the FDR typically relies on intensive simulation at each sample size considered. Our framework efficiently recommends optimal sample sizes and decision thresholds for Bayesian A/B tests that satisfy criteria for the FDR and average power. Our approach is efficient because we leverage new theoretical results to obtain these recommendations using simulations conducted at only two sample sizes. Our methodology is illustrated using an example based on a real A/B test involving several metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10814v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Hagar, Nathaniel T. Stevens</dc:creator>
    </item>
    <item>
      <title>Channelling Multimodality Through a Unimodalizing Transport: Warp-U Sampler and Stochastic Bridge Sampling Estimator</title>
      <link>https://arxiv.org/abs/2401.00667</link>
      <description>arXiv:2401.00667v2 Announce Type: replace 
Abstract: Monte Carlo integration is a powerful tool for scientific and statistical computation, but faces significant challenges when the integrand is a multi-modal distribution, even when the mode locations are known. This work introduces novel Monte Carlo sampling and integration estimation strategies for the multi-modal context by leveraging a generalized version of the stochastic Warp-U transformation Wang et al. [2022]. We propose two flexible classes of Warp-U transformations, one based on a general location-scale-skew mixture model and a second using neural ordinary differential equations. We develop an efficient sampling strategy called Warp-U sampling, which applies a Warp-U transformation to map a multi-modal density into a uni-modal one, then inverts the transformation with injected stochasticity. In high dimensions, our approach relies on information about the mode locations, but requires minimal tuning and demonstrates better mixing properties than conventional methods with identical mode information. To improve normalizing constant estimation once samples are obtained, we propose a stochastic Warp-U bridge sampling estimator, which we demonstrate has higher asymptotic precision per CPU second compared to the original approach proposed by Wang et al. [2022]. We also establish the ergodicity of our sampling algorithm. The effectiveness and current limitations of our methods are illustrated through simulation studies and an application to exoplanet detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00667v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fei Ding, Shiyuan He, David E. Jones, Xiao-Li Meng</dc:creator>
    </item>
    <item>
      <title>Bayesian changepoint detection via logistic regression and the topological analysis of image series</title>
      <link>https://arxiv.org/abs/2401.02917</link>
      <description>arXiv:2401.02917v3 Announce Type: replace 
Abstract: We present a Bayesian method for multivariate changepoint detection that allows for simultaneous inference on the location of a changepoint and the coefficients of a logistic regression model for distinguishing pre-changepoint data from post-changepoint data. In contrast to many methods for multivariate changepoint detection, the proposed method is applicable to data of mixed type and avoids strict assumptions regarding the distribution of the data and the nature of the change. The regression coefficients provide an interpretable description of a potentially complex change. For posterior inference, the model admits a simple Gibbs sampling algorithm based on P\'olya-gamma data augmentation. We establish conditions under which the proposed method is guaranteed to recover the true underlying changepoint. As a testing ground for our method, we consider the problem of detecting topological changes in time series of images. We demonstrate that our proposed method BCLR, combined with a topological feature embedding, performs well on both simulated and real image data. The method also successfully recovers the location and nature of changes in more traditional changepoint tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.02917v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew M. Thomas, Michael Jauch, David S. Matteson</dc:creator>
    </item>
    <item>
      <title>Asymptotic Online FWER Control for Dependent Test Statistics</title>
      <link>https://arxiv.org/abs/2401.09559</link>
      <description>arXiv:2401.09559v2 Announce Type: replace 
Abstract: In online multiple testing, an a priori unknown number of hypotheses are tested sequentially, i.e. at each time point a test decision for the current hypothesis has to be made using only the data available so far. Although many powerful test procedures have been developed for online error control in recent years, most of them are designed solely for independent or at most locally dependent test statistics. In this work, we provide a new framework for deriving online multiple test procedures which ensure asymptotical (with respect to the sample size) control of the familywise error rate (FWER), regardless of the dependence structure between test statistics. In this context, we give a few concrete examples of such test procedures and discuss their properties. Furthermore, we conduct a simulation study in which the type I error control of these test procedures is also confirmed for a finite sample size and a gain in power is indicated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09559v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Vincent Jankovic, Lasse Fischer, Werner Brannath</dc:creator>
    </item>
    <item>
      <title>Optimal Cut-Point Estimation for Functional Digital Biomarkers: Application to Diabetes Risk Stratification via Continuous Glucose Monitoring</title>
      <link>https://arxiv.org/abs/2404.09716</link>
      <description>arXiv:2404.09716v2 Announce Type: replace 
Abstract: Establishing optimal cut-offs for clinical biomarkers is a fundamental statistical problem in epidemiology, clinical trials, and drug discovery. While there is extensive literature regarding the definition of optimal cut-offs for scalar biomarkers, methodologies for analyzing random statistical objects in the more complex spaces associated with random functions and graphs - something increasingly required in the field of modern digital health applications - are lacking. This paper proposes a new, general, simple methodology for defining optimal cut-offs for random objects residing in separable Hilbert spaces. Its underlying motivation is the need to create new, digital health rules for the detection of diabetes mellitus, and thus better exploit the continuous high-dimensional functional information provided by continuous glucose monitors (CGM). A functional cut-off for identifying diabetes is offered, based on glucose distributional representations from CGM time series. This work may be a valuable resource for researchers interested in defining and validating new digital biomarkers for biosensor time series</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09716v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Lado-Baleato, Carla D\'iaz-Louza, Francisco Gude, Marcos Matabuena</dc:creator>
    </item>
    <item>
      <title>Power-Enhanced Two-Sample Mean Tests for High-Dimensional Compositional Data with Application to Microbiome Data Analysis</title>
      <link>https://arxiv.org/abs/2405.02551</link>
      <description>arXiv:2405.02551v2 Announce Type: replace 
Abstract: Testing differences in mean vectors is a fundamental task in the analysis of high-dimensional compositional data. Existing methods may suffer from low power if the underlying signal pattern is in a situation that does not favor the deployed test. In this work, we develop two-sample power-enhanced mean tests for high-dimensional compositional data based on the combination of $p$-values, which integrates strengths from two popular types of tests: the maximum-type test and the quadratic-type test. We provide rigorous theoretical guarantees on the proposed tests, showing accurate Type-I error rate control and enhanced testing power. Our method boosts the testing power towards a broader alternative space, which yields robust performance across a wide range of signal pattern settings. Our theory also contributes to the literature on power enhancement and Gaussian approximation for high-dimensional hypothesis testing. We demonstrate the performance of our method on both simulated data and real-world microbiome data, showing that our proposed approach improves the testing power substantially compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02551v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danning Li, Lingzhou Xue, Haoyi Yang, Xiufan Yu</dc:creator>
    </item>
    <item>
      <title>Byzantine-tolerant distributed learning of finite mixture models</title>
      <link>https://arxiv.org/abs/2407.13980</link>
      <description>arXiv:2407.13980v2 Announce Type: replace 
Abstract: Traditional statistical methods need to be updated to work with modern distributed data storage paradigms. A common approach is the split-and-conquer framework, which involves learning models on local machines and averaging their parameter estimates. However, this does not work for the important problem of learning finite mixture models, because subpopulation indices on each local machine may be arbitrarily permuted (the "label switching problem"). Zhang and Chen (2022) proposed Mixture Reduction (MR) to address this issue, but MR remains vulnerable to Byzantine failure, whereby a fraction of local machines may transmit arbitrarily erroneous information. This paper introduces Distance Filtered Mixture Reduction (DFMR), a Byzantine tolerant adaptation of MR that is both computationally efficient and statistically sound. DFMR leverages the densities of local estimates to construct a robust filtering mechanism. By analysing the pairwise L2 distances between local estimates, DFMR identifies and removes severely corrupted local estimates while retaining the majority of uncorrupted ones. We provide theoretical justification for DFMR, proving its optimal convergence rate and asymptotic equivalence to the global maximum likelihood estimate under standard assumptions. Numerical experiments on simulated and real-world data validate the effectiveness of DFMR in achieving robust and accurate aggregation in the presence of Byzantine failure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13980v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiong Zhang, Yan Shuo Tan, Jiahua Chen</dc:creator>
    </item>
    <item>
      <title>Highly Multivariate Large-scale Spatial Stochastic Processes -- A Cross-Markov Random Field Approach</title>
      <link>https://arxiv.org/abs/2408.10396</link>
      <description>arXiv:2408.10396v2 Announce Type: replace 
Abstract: Key challenges in the analysis of highly multivariate large-scale spatial stochastic processes, where both the number of components (p) and spatial locations (n) can be large, include achieving maximal sparsity in the joint precision matrix, ensuring efficient computational cost for its generation, accommodating asymmetric cross-covariance in the joint covariance matrix, and delivering scientific interpretability. We propose a cross-MRF model class, consisting of a mixed spatial graphical model framework and cross-MRF theory, to collectively address these challenges in one unified framework across two modelling stages. The first stage exploits scientifically informed conditional independence (CI) among p component fields and allows for a step-wise parallel generation of joint covariance and precision matrix, enabling a simultaneous accommodation of asymmetric cross-covariance in joint covariance matrix and sparsity in joint precision matrix. The second stage extends the first-stage CI to doubly CI among both p and n and unearths the cross-MRF via an extended Hammersley-Clifford theorem for multivariate spatial stochastic processes. This results in the sparsest possible representation of the joint precision matrix and ensures its lowest generation complexity. We demonstrate with 1D simulated comparative studies and 2D real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10396v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaoqing Chen, Peter Diggle, James V. Zidek, Gavin Shaddick</dc:creator>
    </item>
    <item>
      <title>Climate Change in Austria: Precipitation and Dry Spells over the last 50 years</title>
      <link>https://arxiv.org/abs/2408.11497</link>
      <description>arXiv:2408.11497v2 Announce Type: replace 
Abstract: We propose a statistical model for precipitation patterns that resolves small-scale local effects in the Austrian Alpine region. Despite the significance of accounting for elevation-dependent precipitation changes in the Alpine region, they have not been extensively explored in regional climate studies. We investigate changes in precipitation patterns between two 10-year periods over the past 50 years in Austria. Specifically, we analyse real precipitation data for three scenarios: monthly mean, monthly maximum precipitation, and the monthly maximum length of a dry spell. We compute temporal difference maps to visualise these changes by comparing the average monthly precipitation scenario across the two decades 1973-1982 and 2013-2022. Our findings are essential for detecting fine-scale precipitation changes in Austria, identifying thresholds across space and time and creating the basis for political decision-makers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11497v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Corinna Perchtold</dc:creator>
    </item>
    <item>
      <title>Causal Data Fusion for Panel Data without Pre-Intervention Period</title>
      <link>https://arxiv.org/abs/2410.16391</link>
      <description>arXiv:2410.16391v2 Announce Type: replace 
Abstract: Traditional panel data causal inference frameworks, such as difference-in-differences and synthetic control methods, rely on pre-intervention data to estimate counterfactuals. However, such data may not be available in real-world settings when interventions are implemented in response to sudden events, such as public health crises or epidemiological shocks. In this paper, we introduce two data fusion methods for causal inference from panel data in scenarios where pre-intervention data is unavailable. These methods leverage auxiliary reference domains with related panel data to estimate causal effects in the target domain, overcoming the limitations imposed by the absence of pre-intervention data. We show the efficacy of these methods by obtaining converging bounds on the bias as well as through a simulation study. Our proposed methodology renders causal inference feasible in urgent and data-constrained environments where the assumptions of the existing causal inference frameworks are not met. As an application of the proposed methodology, we study the causal effect of the community organization activity on the COVID-19 vaccination rate among the Hispanic sub-population in the city of Chelsea, Massachusetts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16391v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zou Yang, Seung Hee Lee, Julia R. K\"ohler, AmirEmad Ghassami</dc:creator>
    </item>
    <item>
      <title>Posterior SBC: Simulation-Based Calibration Checking Conditional on Data</title>
      <link>https://arxiv.org/abs/2502.03279</link>
      <description>arXiv:2502.03279v2 Announce Type: replace 
Abstract: Simulation-based calibration checking (SBC) refers to the validation of an inference algorithm and model implementation through repeated inference on data simulated from a generative model. In the original and commonly used approach, the generative model uses parameters drawn from the prior, and thus the approach is testing whether the inference works for simulated data generated with parameter values plausible under that prior. This approach is natural and desirable when we want to test whether the inference works for a wide range of datasets we might observe. However, after observing data, we are interested in answering whether the inference works conditional on that particular data. In this paper, we propose posterior SBC and demonstrate how it can be used to validate the inference conditionally on observed data. We illustrate the utility of posterior SBC in three case studies: (1) A simple multilevel model; (2) a model that is governed by differential equations; and (3) a joint integrative neuroscience model which is approximated via amortized Bayesian inference with neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03279v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teemu S\"ailynoja, Marvin Schmitt, Paul-Christian B\"urkner, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>Kernel-based estimators for functional causal effects</title>
      <link>https://arxiv.org/abs/2503.05024</link>
      <description>arXiv:2503.05024v2 Announce Type: replace 
Abstract: We propose causal effect estimators based on empirical Fr\'{e}chet means and operator-valued kernels, tailored to functional data spaces. These methods address the challenges of high-dimensionality, sequential ordering, and model complexity while preserving robustness to treatment misspecification. Using structural assumptions, we obtain compact representations of potential outcomes, enabling scalable estimation of causal effects over time and across covariates. We provide both theoretical, regarding the consistency of functional causal effects, as well as empirical comparison of a range of proposed causal effect estimators.
  Applications to binary treatment settings with functional outcomes illustrate the framework's utility in biomedical monitoring, where outcomes exhibit complex temporal dynamics. Our estimators accommodate scenarios with registered covariates and outcomes, aligning them to the Fr\'{e}chet means, as well as cases requiring higher-order representations to capture intricate covariate-outcome interactions. These advancements extend causal inference to dynamic and non-linear domains, offering new tools for understanding complex treatment effects in functional data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05024v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yordan P. Raykov, Hengrui Luo, Justin D. Strait, Wasiur R. KhudaBukhsh</dc:creator>
    </item>
    <item>
      <title>lpcde: Estimation and Inference for Local Polynomial Conditional Density Estimators</title>
      <link>https://arxiv.org/abs/2204.10375</link>
      <description>arXiv:2204.10375v3 Announce Type: replace-cross 
Abstract: This paper discusses the R package lpcde, which stands for local polynomial conditional density estimation. It implements the kernel-based local polynomial smoothing methods introduced in Cattaneo, Chandak, Jansson, Ma (2024) for statistical estimation and inference of conditional distributions, densities, and derivatives thereof. The package offers mean square error optimal bandwidth selection and associated point estimators, as well as uncertainty quantification based on robust bias correction both pointwise (e.g., confidence intervals) and uniformly (e.g., confidence bands) over evaluation points. The methods implemented are boundary adaptive whenever the data is compactly supported. The package also implements regularized conditional density estimation methods, ensuring the resulting density estimate is non-negative and integrates to one. We contrast the functionalities of lpcde with existing open-source packages for conditional density estimation, and showcase its main features using simulated and real datasets. An abbreviated version of this article is published in Cattaneo, Chandak, Jansson, Ma (2025 JOSS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10375v3</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rajita Chandak, Michael Jansson, Xinwei Ma</dc:creator>
    </item>
    <item>
      <title>Conditionality principle under unconstrained randomness</title>
      <link>https://arxiv.org/abs/2402.04859</link>
      <description>arXiv:2402.04859v2 Announce Type: replace-cross 
Abstract: A very simple example demonstrates that Fisher's application of the conditionality principle to regression ("fixed-$x$ regression"), endorsed by Sprott and many other followers, makes prediction impossible in the context of statistical learning theory. On the other hand, relaxing the requirement of conditionality makes it possible via, e.g., conformal prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04859v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Vovk</dc:creator>
    </item>
    <item>
      <title>Choosing alpha post hoc: the danger of multiple standard significance thresholds</title>
      <link>https://arxiv.org/abs/2410.02306</link>
      <description>arXiv:2410.02306v2 Announce Type: replace-cross 
Abstract: A fundamental assumption of classical hypothesis testing is that the significance threshold $\alpha$ is chosen independently from the data. The validity of confidence intervals likewise relies on choosing $\alpha$ beforehand. We point out that the independence of $\alpha$ is guaranteed in practice because, in most fields, there exists one standard $\alpha$ that everyone uses -- so that $\alpha$ is automatically independent of everything. However, there have been recent calls to decrease $\alpha$ from $0.05$ to $0.005$. We note that this may lead to multiple accepted standard thresholds within one scientific field. For example, different journals may require different significance thresholds. As a consequence, some researchers may be tempted to conveniently choose their $\alpha$ based on their p-value. We use examples to illustrate that this severely invalidates hypothesis tests, and mention some potential solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02306v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Hemerik, Nick W Koning</dc:creator>
    </item>
    <item>
      <title>Free Anytime Validity by Sequentializing a Test and Optional Continuation with Tests as Future Significance Levels</title>
      <link>https://arxiv.org/abs/2501.03982</link>
      <description>arXiv:2501.03982v4 Announce Type: replace-cross 
Abstract: Anytime valid sequential tests permit us to stop and continue testing based on the current data, without invalidating the inference. Given a maximum number of observations $N$, one may believe this must come at the cost of power when compared to a conventional test that waits until all $N$ observations have arrived. Our first contribution is to show that this is false: for any valid test based on $N$ observations, we derive an anytime valid sequential test that matches it after $N$ observations. Our second contribution is that the outcome of a continuously-interpreted test can be used as a significance level in subsequent testing, leading to an overall procedure that is valid at the original significance level. This shows anytime validity and optional continuation are readily available in traditional testing, without requiring explicit use of e-values. We illustrate this by deriving the anytime valid sequentialized $z$-test and $t$-test, which at time $N$ coincide with the traditional $z$-test and $t$-test. Lastly, we show the popular log-optimal sequential $z$-test can be interpreted as desiring a rejection by the traditional $z$-test at some tiny significance level in the distant future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03982v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning, Sam van Meer</dc:creator>
    </item>
    <item>
      <title>Causal bandits with backdoor adjustment on unknown Gaussian DAGs</title>
      <link>https://arxiv.org/abs/2502.02020</link>
      <description>arXiv:2502.02020v2 Announce Type: replace-cross 
Abstract: The causal bandit problem aims to sequentially learn the intervention that maximizes the expectation of a reward variable within a system governed by a causal graph. Most existing approaches assume prior knowledge of the graph structure, or impose unrealistically restrictive conditions on the graph. In this paper, we assume a Gaussian linear directed acyclic graph (DAG) over arms and the reward variable, and study the causal bandit problem when the graph structure is unknown. We identify backdoor adjustment sets for each arm using sequentially generated experimental and observational data during the decision process, which allows us to estimate causal effects and construct upper confidence bounds. By integrating estimates from both data sources, we develop a novel bandit algorithm, based on modified upper confidence bounds, to sequentially determine the optimal intervention. We establish both case-dependent and case-independent upper bounds on the cumulative regret for our algorithm, which improve upon the bounds of the standard multi-armed bandit algorithms. Our empirical study demonstrates its advantage over existing methods with respect to cumulative regret and computation time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02020v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yijia Zhao, Qing Zhou</dc:creator>
    </item>
    <item>
      <title>Post-detection inference for sequential changepoint localization</title>
      <link>https://arxiv.org/abs/2502.06096</link>
      <description>arXiv:2502.06096v2 Announce Type: replace-cross 
Abstract: This paper addresses a fundamental but largely unexplored challenge in sequential changepoint analysis: conducting inference following a detected change. We study the problem of localizing the changepoint using only the data observed up to a data-dependent stopping time at which a sequential detection algorithm $\mathcal A$ declares a change. We first construct confidence sets for the unknown changepoint when pre- and post-change distributions are assumed to be known. We then extend our framework to composite pre- and post-change scenarios. We impose no conditions on the observation space or on $\mathcal A$ -- we only need to be able to run $\mathcal A$ on simulated data sequences. In summary, this work offers both theoretically sound and practically effective tools for sequential changepoint localization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06096v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aytijhya Saha, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Principles for Open Data Curation: A Case Study with the New York City 311 Service Request Data</title>
      <link>https://arxiv.org/abs/2502.08649</link>
      <description>arXiv:2502.08649v2 Announce Type: replace-cross 
Abstract: In the early 21st century, the open data movement began to transform societies and governments by promoting transparency, innovation, and public engagement. The City of New York (NYC) has been at the forefront of this movement since the enactment of the Open Data Law in 2012, creating the NYC Open Data portal. The portal currently hosts 2,700 datasets, serving as a crucial resource for research across various domains, including health, urban development, and transportation. However, the effective use of open data relies heavily on data quality and usability, challenges that remain insufficiently addressed in the literature. This paper examines these challenges via a case study of the NYC 311 Service Request dataset, identifying key issues in data validity, consistency, and curation efficiency. We propose a set of data curation principles, tailored for government-released open data, to address these challenges. Our findings highlight the importance of harmonized field definitions, streamlined storage, and automated quality checks, offering practical guidelines for improving the reliability and utility of open datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08649v2</guid>
      <category>cs.DB</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Tussey, Jun Yan</dc:creator>
    </item>
    <item>
      <title>Age Group Sensitivity Analysis of Epidemic Models: Investigating the Impact of Contact Matrix Structure</title>
      <link>https://arxiv.org/abs/2502.19206</link>
      <description>arXiv:2502.19206v2 Announce Type: replace-cross 
Abstract: Understanding the role of different age groups in disease transmission is crucial for designing effective intervention strategies. A key parameter in age-structured epidemic models is the contact matrix, which defines the interaction structure between age groups. However, accurately estimating contact matrices is challenging, as different age groups respond differently to surveys and are accessible through different channels. This variability introduces significant epistemic uncertainty in epidemic models.
  In this study, we introduce the Age Group Sensitivity Analysis (AGSA) method, a novel framework for assessing the impact of age-structured contact patterns on epidemic outcomes. Our approach integrates age-stratified epidemic models with Latin Hypercube Sampling (LHS) and the Partial Rank Correlation Coefficient (PRCC) method, enabling a systematic sensitivity analysis of age-specific interactions. Additionally, we propose a new sensitivity aggregation technique that quantifies the contribution of each age group to key epidemic parameters.
  By identifying the age groups to which the model is most sensitive, AGSA helps pinpoint those that introduce the greatest epistemic uncertainty. This allows for targeted data collection efforts, focusing surveys and empirical studies on the most influential age groups to improve model accuracy. As a result, AGSA can enhance epidemic forecasting and inform the design of more effective and efficient public health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19206v2</guid>
      <category>q-bio.QM</category>
      <category>math.DS</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zsolt Vizi, Evans Kiptoo Korir, Norbert Bogya, Csaba Roszt\'oczy, G\'eza Makay, P\'eter Boldog</dc:creator>
    </item>
  </channel>
</rss>
