<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Jan 2025 05:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Fault Localization via Fine-tuning Large Language Models with Mutation Generated Stack Traces</title>
      <link>https://arxiv.org/abs/2501.18005</link>
      <description>arXiv:2501.18005v1 Announce Type: new 
Abstract: Abrupt and unexpected terminations of software are termed as software crashes. They can be challenging to analyze. Finding the root cause requires extensive manual effort and expertise to connect information sources like stack traces, source code, and logs. Typical approaches to fault localization require either test failures or source code. Crashes occurring in production environments, such as that of SAP HANA, provide solely crash logs and stack traces. We present a novel approach to localize faults based only on the stack trace information and no additional runtime information, by fine-tuning large language models (LLMs). We address complex cases where the root cause of a crash differs from the technical cause, and is not located in the innermost frame of the stack trace. As the number of historic crashes is insufficient to fine-tune LLMs, we augment our dataset by leveraging code mutators to inject synthetic crashes into the code base. By fine-tuning on 64,369 crashes resulting from 4.1 million mutations of the HANA code base, we can correctly predict the root cause location of a crash with an accuracy of 66.9\% while baselines only achieve 12.6% and 10.6%. We substantiate the generalizability of our approach by evaluating on two additional open-source databases, SQLite and DuckDB, achieving accuracies of 63% and 74%, respectively. Across all our experiments, fine-tuning consistently outperformed prompting non-finetuned LLMs for localizing faults in our datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18005v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Neetha Jambigi, Bartosz Bogacz, Moritz Mueller, Thomas Bach, Michael Felderer</dc:creator>
    </item>
    <item>
      <title>Utilizing API Response for Test Refinement</title>
      <link>https://arxiv.org/abs/2501.18145</link>
      <description>arXiv:2501.18145v1 Announce Type: new 
Abstract: Most of the web services are offered in the form of RESTful APIs. This has led to an active research interest in API testing to ensure the reliability of these services. While most of the testing techniques proposed in the past rely on the API specification to generate the test cases, a major limitation of such an approach is that in the case of an incomplete or inconsistent specification, the test cases may not be realistic in nature and would result in a lot of 4xx response due to invalid input. This is indicative of poor test quality. Learning-based approaches may learn about valid inputs but often require a large number of request-response pairs to learn the constraints, making it infeasible to be readily used in the industry. To address this limitation, this paper proposes a dynamic test refinement approach that leverages the response message. The response is used to infer the point in the API testing flow where a test scenario fix is required. Using an intelligent agent, the approach adds constraints to the API specification that are further used to generate a test scenario accounting for the learned constraint from the response. Following a greedy approach, the iterative learning and refinement of test scenarios are obtained from the API testing system. The proposed approach led to a decrease in the number of 4xx responses, taking a step closer to generating more realistic test cases with high coverage that would aid in functional testing. A high coverage was obtained from a lesser number of API requests, as compared with the state-of-the-art search-based API Testing tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18145v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Devika Sondhi, Ananya Sharma, Diptikalyan Saha</dc:creator>
    </item>
    <item>
      <title>The Dilemma of Building Do-It-Yourself (DIY) Solutions for Workplace Accessibility</title>
      <link>https://arxiv.org/abs/2501.18148</link>
      <description>arXiv:2501.18148v1 Announce Type: new 
Abstract: Existing commercial and in-house software development tools are often inaccessible to Blind and Low Vision Software Professionals (BLVSPs), hindering their participation and career growth at work. Building on existing research on Do-It-Yourself (DIY) Assistive Technologies and customized tools made by programmers, we shed light on the currently unexplored intersection of how DIY tools built and used by BLVSPs support accessible software development. Through semi-structured interviews with 30 BLVSPs, we found that such tools serve many different purposes and are driven by motivations such as desiring to maintain a professional image and a sense of dignity at work. These tools had significant impacts on workplace accessibility and revealed a need for a more centralized community for sharing tools, tips, and tricks. Based on our findings, we introduce the "Double Hacker Dilemma" and highlight a need for developing more effective peer and organizational platforms that support DIY tool sharing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18148v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713302</arxiv:DOI>
      <dc:creator>Yoonha Cha, Victoria Jackson, Karina Kohl, Rafael Prikladnicki, Andr\'e van der Hoek, Stacy M. Branham</dc:creator>
    </item>
    <item>
      <title>RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing</title>
      <link>https://arxiv.org/abs/2501.18160</link>
      <description>arXiv:2501.18160v1 Announce Type: new 
Abstract: Code auditing is a code review process with the goal of finding bugs. Large Language Models (LLMs) have shown substantial potential in this task, offering the ability to analyze programs without compilation and enabling customized bug detection following specified prompts. However, applying LLMs to repository-level code auditing presents notable challenges. The inherent context limits and hallucinations of LLMs can lead to the low quality of bug reports. Meanwhile, the large size of software repositories introduces substantial time and token costs, hindering efficiency and scalability in real-world scenarios.
  This work introduces an autonomous LLM-agent, RepoAudit, designed to enable precise and efficient repository-level code auditing. Equipped with the agent memory, RepoAudit explores the code repository on demand, analyzing data-flow facts along different feasible program paths in individual functions. It also introduces the validator to check the data-flow facts for hallucination mitigation and examine the satisfiability of path conditions of potential buggy paths, which enables RepoAudit to discard false positives in the code auditing. Our experiment shows that RepoAudit powered by Claude 3.5 Sonnet successfully finds 38 true bugs in 15 real-world systems, consuming 0.44 hours and $2.54 per project on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18160v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinyao Guo, Chengpeng Wang, Xiangzhe Xu, Zian Su, Xiangyu Zhang</dc:creator>
    </item>
    <item>
      <title>Toward Bundler-Independent Module Federations: Enabling Typed Micro-Frontend Architectures</title>
      <link>https://arxiv.org/abs/2501.18225</link>
      <description>arXiv:2501.18225v1 Announce Type: new 
Abstract: Modern web applications demand scalable and modular architectures, driving the adoption of micro-frontends. This paper introduces Bundler-Independent Module Federation (BIMF) as a New Idea, enabling runtime module loading without relying on traditional bundlers, thereby enhancing flexibility and team collaboration. This paper presents the initial implementation of BIMF, emphasizing benefits such as shared dependency management and modular performance optimization. We address key challenges, including debugging, observability, and performance bottlenecks, and propose solutions such as distributed tracing, server-side rendering, and intelligent prefetching. Future work will focus on evaluating observability tools, improving developer experience, and implementing performance optimizations to fully realize BIMF's potential in micro-frontend architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18225v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Billy Lando, Wilhelm Hasselbring</dc:creator>
    </item>
    <item>
      <title>Fast and Efficient What-If Analyses of Invocation Overhead and Transactional Boundaries to Support the Migration to Microservices</title>
      <link>https://arxiv.org/abs/2501.18230</link>
      <description>arXiv:2501.18230v1 Announce Type: new 
Abstract: Improving agility and maintainability are common drivers for companies to adopt a microservice architecture for their existing software systems. However, the existing software often relies heavily on the fact that it is executed within a single process space. Therefore, decomposing existing software into out-of-process components like microservices can have a severe impact on non-functional properties, such as overall performance due to invocation overhead or data consistency.
  To minimize this impact, it is important to consider non-functional properties already as part of the design process of the service boundaries. A useful method for such considerations are what-if analyses, which allow to explore different scenarios and to develop the service boundaries in an iterative and incremental way. Experience from an industrial case study suggests that for these analyses, ease of use and speed tend to be more important than precision.
  In this paper, we present emerging results for an approach for what-if analyses based on trace rewriting that is (i) specifically designed for analyzing the impact on non-functional properties due to decomposition into out-of-process components and (ii) deliberately prefers ease of use and analysis speed over precision of the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18230v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Holger Knoche, Wilhelm Hasselbring</dc:creator>
    </item>
    <item>
      <title>RESMETRIC: Analyzing Resilience to Enable Research on Antifragility</title>
      <link>https://arxiv.org/abs/2501.18245</link>
      <description>arXiv:2501.18245v1 Announce Type: new 
Abstract: A key feature in self-adaptive systems is resilience, which is an ongoing research topic. Recently, the community started to explore antifragility, which describes the improvement of resilience over time. While there are model-agnostic resilience metrics, there is currently no out-of-the-box tool for researchers and practitioners to determine to which degree their system is resilient. To facilitate research on antifragility, we present ResMetric, a model-agnostic tool that calculates and visualizes various resilience metrics based on the quality of service over time. With ResMetric, researchers can evaluate their definition of resilience and antifragility. This paper highlights how ResMetric can be employed by demonstrating its use in a case study on gas detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18245v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ferdinand K\"onig, Marc Carwehl, Calum Imrie</dc:creator>
    </item>
    <item>
      <title>DATCloud: A Model-Driven Framework for Multi-Layered Data-Intensive Architectures</title>
      <link>https://arxiv.org/abs/2501.18257</link>
      <description>arXiv:2501.18257v1 Announce Type: new 
Abstract: The complexity of multi-layered, data-intensive systems demands frameworks that ensure flexibility, scalability, and efficiency. DATCloud is a model-driven framework designed to facilitate the modeling, validation, and refinement of multi-layered architectures, addressing scalability, modularity, and real-world requirements. By adhering to ISO/IEC/IEEE 42010 standards, DATCloud leverages structural and behavioral meta-models and graphical domain-specific languages (DSLs) to enhance reusability and stakeholder communication. Initial validation through the VASARI system at the Uffizi Gallery demonstrates a 40% reduction in modeling time and a 32% improvement in flexibility compared to manual methods. While effective, DATCloud is a work in progress, with plans to integrate advanced code generation, simulation tools, and domain-specific extensions to further enhance its capabilities for applications in healthcare, smart cities, and other data-intensive domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18257v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Moamin Abughazala, Henry Muccini</dc:creator>
    </item>
    <item>
      <title>PyExamine A Comprehensive, UnOpinionated Smell Detection Tool for Python</title>
      <link>https://arxiv.org/abs/2501.18327</link>
      <description>arXiv:2501.18327v1 Announce Type: new 
Abstract: The growth of Python adoption across diverse domains has led to increasingly complex codebases, presenting challenges in maintaining code quality. While numerous tools attempt to address these challenges, they often fall short in providing comprehensive analysis capabilities or fail to consider Python-specific contexts. PyExamine addresses these critical limitations through an approach to code smell detection that operates across multiple levels of analysis.
  PyExamine architecture enables detailed examination of code quality through three distinct but interconnected layers: architectural patterns, structural relationships, and code-level implementations. This approach allows for the detection and analysis of 49 distinct metrics, providing developers with an understanding of their codebase's health. The metrics span across all levels of code organization, from high-level architectural concerns to granular implementation details.
  Through evaluation on 7 diverse projects, PyExamine achieved detection accuracy rates: 91.4\% for code-level smells, 89.3\% for structural smells, and 80.6\% for architectural smells. These results were further validated through extensive user feedback and expert evaluations, confirming PyExamine's capability to identify potential issues across all levels of code organization with high recall accuracy.
  In additional to this, we have also used PyExamine to analysis the prevalence of different type of smells, across 183 diverse Python projects ranging from small utilities to large-scale enterprise applications.
  PyExamine's distinctive combination of comprehensive analysis, Python-specific detection, and high customizability makes it a valuable asset for both individual developers and large teams seeking to enhance their code quality practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18327v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik Shivashankar, Antonio Martini</dc:creator>
    </item>
    <item>
      <title>o3-mini vs DeepSeek-R1: Which One is Safer?</title>
      <link>https://arxiv.org/abs/2501.18438</link>
      <description>arXiv:2501.18438v1 Announce Type: new 
Abstract: The irruption of DeepSeek-R1 constitutes a turning point for the AI industry in general and the LLMs in particular. Its capabilities have demonstrated outstanding performance in several tasks, including creative thinking, code generation, maths and automated program repair, at apparently lower execution cost. However, LLMs must adhere to an important qualitative property, i.e., their alignment with safety and human values. A clear competitor of DeepSeek-R1 is its American counterpart, OpenAI's o3-mini model, which is expected to set high standards in terms of performance, safety and cost. In this paper we conduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b version) and OpenAI's o3-mini (beta version). To this end, we make use of our recently released automated safety testing tool, named ASTRAL. By leveraging this tool, we automatically and systematically generate and execute a total of 1260 unsafe test inputs on both models. After conducting a semi-automated assessment of the outcomes provided by both LLMs, the results indicate that DeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our evaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts whereas o3-mini only to 1.19%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18438v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aitor Arrieta, Miriam Ugarte, Pablo Valle, Jos\'e Antonio Parejo, Sergio Segura</dc:creator>
    </item>
    <item>
      <title>ExeCoder: Empowering Large Language Models with Executability Representation for Code Translation</title>
      <link>https://arxiv.org/abs/2501.18460</link>
      <description>arXiv:2501.18460v1 Announce Type: new 
Abstract: Code translation is a crucial activity in the software development and maintenance process, and researchers have recently begun to focus on using pre-trained large language models (LLMs) for code translation. However, existing LLMs only learn the contextual semantics of code during pre-training, neglecting executability information closely related to the execution state of the code, which results in unguaranteed code executability and unreliable automated code translation. To address this issue, we propose ExeCoder, an LLM specifically designed for code translation, aimed at utilizing executability representations such as functional semantics, syntax structures, and variable dependencies to enhance the capabilities of LLMs in code translation. To evaluate the effectiveness of ExeCoder, we manually enhanced the widely used benchmark TransCoder-test, resulting in a benchmark called TransCoder-test-X that serves LLMs. Evaluation of TransCoder-test-X indicates that ExeCoder achieves state-of-the-art performance in code translation, surpassing existing open-source code LLMs by over 10.88% to 38.78% and over 27.44% to 42.97% on two metrics, and even outperforms the renowned closed-source LLM GPT-4o. Website: https://execoder4trans.github.io/</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18460v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghua He, Fangkai Yang, Pu Zhao, Wenjie Yin, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang</dc:creator>
    </item>
    <item>
      <title>A Tool for In-depth Analysis of Code Execution Reasoning of Large Language Models</title>
      <link>https://arxiv.org/abs/2501.18482</link>
      <description>arXiv:2501.18482v1 Announce Type: new 
Abstract: Code Executing Reasoning is becoming a new non-functional metric that assesses the ability of large language models (LLMs) in programming tasks. State-of-the-art frameworks (CodeMind or REval) and benchmarks (CruxEval) usually focus on LLM's prediction of a given code's input/output or intermediate variable states/values on limited programs. However, there is no tool for more in-depth analysis of the results. Without such a tool, the observations about LLM's code execution reasoning cannot be generalized to more datasets, preventing the research community and practitioners from devising the next generation of LLMs with better code execution reasoning abilities. This paper introduces ExeRScope, a series of tools and heuristics to analyze the result of code execution reasoning frameworks to understand better the impact of code properties in the studied benchmarks on the code execution reasoning. With such tooling, analysis can be generalized to code with similar properties without the urgent need to design more benchmarks, which is a cumbersome effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18482v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changshu Liu, Reyhaneh Jabbarvand</dc:creator>
    </item>
    <item>
      <title>An Empirical Study of Dotfiles Repositories Containing User-Specific Configuration Files</title>
      <link>https://arxiv.org/abs/2501.18555</link>
      <description>arXiv:2501.18555v1 Announce Type: new 
Abstract: Storing user-specific configuration files in a "dotfiles" repository is a common practice among software developers, with hundreds of thousands choosing to publicly host their repositories on GitHub. This practice not only provides developers with a simple backup mechanism for their essential configuration files, but also facilitates sharing ideas and learning from others on how best to configure applications that are key to their daily workflows. However, our current understanding of these repository sharing practices is limited and mostly anecdotal. To address this gap, we conducted a study to delve deeper into this phenomenon. Beginning with collecting and analyzing publicly-hosted dotfiles repositories on GitHub, we discovered that maintaining dotfiles is widespread among developers. Notably, we found that 25.8% of the top 500 most-starred GitHub users maintain some form of publicly accessible dotfiles repository. Among these, configurations for text editors like Vim and shells such as bash and zsh are the most commonly tracked. Our analysis reveals that updating dotfiles is primarily driven by the need to adjust configurations (63.3%) and project meta-management (25.4%). Surprisingly, we found no significant difference in the types of dotfiles observed across code churn history patterns, suggesting that the frequency of dotfile modifications depends more on the developer than the properties of the specific dotfile and its associated application. Finally, we discuss the challenges associated with managing dotfiles, including the necessity for a reliable and effective deployment mechanism, and how the insights gleaned from dotfiles can inform tool designers by offering real-world usage information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18555v1</guid>
      <category>cs.SE</category>
      <category>cs.SI</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhan Zhu, Michael W. Godfrey</dc:creator>
    </item>
    <item>
      <title>Docling: An Efficient Open-Source Toolkit for AI-driven Document Conversion</title>
      <link>https://arxiv.org/abs/2501.17887</link>
      <description>arXiv:2501.17887v1 Announce Type: cross 
Abstract: We introduce Docling, an easy-to-use, self-contained, MIT-licensed, open-source toolkit for document conversion, that can parse several types of popular document formats into a unified, richly structured representation. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. Docling is released as a Python package and can be used as a Python API or as a CLI tool. Docling's modular architecture and efficient document representation make it easy to implement extensions, new features, models, and customizations. Docling has been already integrated in other popular open-source frameworks (e.g., LangChain, LlamaIndex, spaCy), making it a natural fit for the processing of documents and the development of high-end applications. The open-source community has fully engaged in using, promoting, and developing for Docling, which gathered 10k stars on GitHub in less than a month and was reported as the No. 1 trending repository in GitHub worldwide in November 2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17887v1</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikolaos Livathinos, Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Kasper Dinkla, Yusik Kim, Shubham Gupta, Rafael Teixeira de Lima, Valery Weber, Lucas Morin, Ingmar Meijer, Viktor Kuropiatnyk, Peter W. J. Staar</dc:creator>
    </item>
    <item>
      <title>Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2501.18071</link>
      <description>arXiv:2501.18071v1 Announce Type: cross 
Abstract: Diabetes mellitus (DM) is a global health issue of significance that must be diagnosed as early as possible and managed well. This study presents a framework for diabetes prediction using Machine Learning (ML) models, complemented with eXplainable Artificial Intelligence (XAI) tools, to investigate both the predictive accuracy and interpretability of the predictions from ML models. Data Preprocessing is based on the Synthetic Minority Oversampling Technique (SMOTE) and feature scaling used on the Diabetes Binary Health Indicators dataset to deal with class imbalance and variability of clinical features. The ensemble model provided high accuracy, with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General Health, Income, and Physical Activity were the most influential predictors obtained from the model explanations. The results of this study suggest that ML combined with XAI is a promising means of developing accurate and computationally transparent tools for use in healthcare systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18071v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pir Bakhsh Khokhar, Viviana Pentangelo, Fabio Palomba, Carmine Gravino</dc:creator>
    </item>
    <item>
      <title>Stream-Based Monitoring of Algorithmic Fairness</title>
      <link>https://arxiv.org/abs/2501.18331</link>
      <description>arXiv:2501.18331v1 Announce Type: cross 
Abstract: Automatic decision and prediction systems are increasingly deployed in applications where they significantly impact the livelihood of people, such as for predicting the creditworthiness of loan applicants or the recidivism risk of defendants. These applications have given rise to a new class of algorithmic-fairness specifications that require the systems to decide and predict without bias against social groups. Verifying these specifications statically is often out of reach for realistic systems, since the systems may, e.g., employ complex learning components, and reason over a large input space. In this paper, we therefore propose stream-based monitoring as a solution for verifying the algorithmic fairness of decision and prediction systems at runtime. Concretely, we present a principled way to formalize algorithmic fairness over temporal data streams in the specification language RTLola and demonstrate the efficacy of this approach on a number of benchmarks. Besides synthetic scenarios that particularly highlight its efficiency on streams with a scaling amount of data, we notably evaluate the monitor on real-world data from the recidivism prediction tool COMPAS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18331v1</guid>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Baumeister, Bernd Finkbeiner, Frederik Scheerer, Julian Siber, Tobias Wagenpfeil</dc:creator>
    </item>
    <item>
      <title>Design and Validation of Learning Aware HMI For Learning-Enabled Increasingly Autonomous Systems</title>
      <link>https://arxiv.org/abs/2501.18506</link>
      <description>arXiv:2501.18506v1 Announce Type: cross 
Abstract: With the rapid advancements in Artificial Intelligence (AI), autonomous agents are increasingly expected to manage complex situations where learning-enabled algorithms are vital. However, the integration of these advanced algorithms poses significant challenges, especially concerning safety and reliability. This research emphasizes the importance of incorporating human-machine collaboration into the systems engineering process to design learning-enabled increasingly autonomous systems (LEIAS). Our proposed LEIAS architecture emphasizes communication representation and pilot preference learning to boost operational safety. Leveraging the Soar cognitive architecture, the system merges symbolic decision logic with numeric decision preferences enhanced through reinforcement learning. A core aspect of this approach is transparency; the LEIAS provides pilots with a comprehensive, interpretable view of the system's state, encompassing detailed evaluations of sensor reliability, including GPS, IMU, and LIDAR data. This multi-sensor assessment is critical for diagnosing discrepancies and maintaining trust. Additionally, the system learns and adapts to pilot preferences, enabling responsive, context-driven decision-making. Autonomy is incrementally escalated based on necessity, ensuring pilots retain control in standard scenarios and receive assistance only when required. Simulation studies conducted in Microsoft's XPlane simulation environment to validate this architecture's efficacy, showcasing its performance in managing sensor anomalies and enhancing human-machine collaboration, ultimately advancing safety in complex operational environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18506v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parth Ganeriwala, Michael Matessa, Siddhartha Bhattacharyya, Randolph M. Jones, Jennifer Davis, Parneet Kaur, Simone Fulvio Rollini, Natasha Neogi</dc:creator>
    </item>
    <item>
      <title>Automating Physics-Based Reasoning for SysML Model Validation</title>
      <link>https://arxiv.org/abs/2501.18514</link>
      <description>arXiv:2501.18514v1 Announce Type: cross 
Abstract: System and software design benefits greatly from formal modeling, allowing for automated analysis and verification early in the design phase. Current methods excel at checking information flow and component interactions, ensuring consistency, and identifying dependencies within Systems Modeling Language (SysML) models. However, these approaches often lack the capability to perform physics-based reasoning about a system's behavior represented in SysML models, particularly in the electromechanical domain. This significant gap critically hinders the ability to automatically and effectively verify the correctness and consistency of the model's behavior against well-established underlying physical principles. Therefore, this paper presents an approach that leverages existing research on function representation, including formal languages, graphical representations, and reasoning algorithms, and integrates them with physics-based verification techniques. Four case studies (coffeemaker, vacuum cleaner, hairdryer, and wired speaker) are inspected to illustrate the model's practicality and effectiveness in performing physics-based reasoning on systems modeled in SysML. This automated physics-based reasoning is broken into two main categories: (i) structural, which is performed on BDD and IBD, and (ii) functional, which is then performed on activity diagrams. This work advances the field of automated reasoning by providing a framework for verifying structural and functional correctness and consistency with physical laws within SysML models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18514v1</guid>
      <category>eess.SY</category>
      <category>cs.ET</category>
      <category>cs.SE</category>
      <category>cs.SY</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Candice Chambers, Summer Mueller, Parth Ganeriwala, Chiradeep Sen, Siddhartha Bhattacharyya</dc:creator>
    </item>
    <item>
      <title>An Industry Interview Study of Software Signing for Supply Chain Security</title>
      <link>https://arxiv.org/abs/2406.08198</link>
      <description>arXiv:2406.08198v2 Announce Type: replace 
Abstract: Many software products are composed of components integrated from other teams or external parties. Each additional link in a software product's supply chain increases the risk of the injection of malicious behavior. To improve supply chain provenance, many cybersecurity frameworks, standards, and regulations recommend the use of software signing. However, recent surveys and measurement studies have found that the adoption rate and quality of software signatures are low. We lack in-depth industry perspectives on the challenges and practices of software signing.
  To understand software signing in practice, we interviewed 18 experienced security practitioners across 13 organizations. We study the challenges that affect the effective implementation of software signing in practice. We also provide possible impacts of experienced software supply chain failures, security standards, and regulations on software signing adoption. To summarize our findings: (1) We present a refined model of the software supply chain factory model highlighting practitioner's signing practices; (2) We highlight the different challenges-technical, organizational, and human-that hamper software signing implementation; (3) We report that experts disagree on the importance of signing; and (4) We describe how internal and external events affect the adoption of software signing. Our work describes the considerations for adopting software signing as one aspect of the broader goal of improved software supply chain security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08198v2</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kelechi G. Kalu, Tanya Singla, Chinenye Okafor, Santiago Torres-Arias, James C. Davis</dc:creator>
    </item>
    <item>
      <title>Pushing the Boundary: Specialising Deep Configuration Performance Learning</title>
      <link>https://arxiv.org/abs/2407.02706</link>
      <description>arXiv:2407.02706v2 Announce Type: replace 
Abstract: Software systems often have numerous configuration options that can be adjusted to meet different performance requirements. However, understanding the combined impact of these options on performance is often challenging, especially with limited real-world data. To tackle this issue, deep learning techniques have gained popularity due to their ability to capture complex relationships even with limited samples. This thesis begins with a systematic literature review of deep learning techniques in configuration performance modeling, analyzing 85 primary papers out of 948 searched papers. It identifies knowledge gaps and sets three objectives for the thesis. The first knowledge gap is the lack of understanding about which encoding scheme is better and in what circumstances. To address this, the thesis conducts an empirical study comparing three popular encoding schemes. Actionable suggestions are provided to support more reliable decisions. Another knowledge gap is the sparsity inherited from the configuration landscape. To handle this, the thesis proposes a model-agnostic and sparsity-robust framework called DaL, which uses a "divide-and-learn" approach. DaL outperforms state-of-the-art approaches in accuracy improvement across various real-world systems. The thesis also addresses the limitation of predicting under static environments by proposing a sequential meta-learning framework called SeMPL. Unlike traditional meta-learning frameworks, SeMPL trains meta-environments in a specialized order, resulting in significantly improved prediction accuracy in multi-environment scenarios. Overall, the thesis identifies and addresses critical knowledge gaps in deep performance learning, significantly advancing the accuracy of performance prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02706v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingzhi Gong</dc:creator>
    </item>
    <item>
      <title>Deciphering Refactoring Branch Dynamics in Modern Code Review: An Empirical Study on Qt</title>
      <link>https://arxiv.org/abs/2410.04678</link>
      <description>arXiv:2410.04678v2 Announce Type: replace 
Abstract: Context: Modern code review is a widely employed technique in both industrial and open-source projects, serving to enhance software quality, share knowledge, and ensure compliance with coding standards and guidelines. While code review is extensively studied for its general challenges, best practices, outcomes, and socio-technical aspects, little attention has been paid to how refactoring is reviewed and what developers prioritize when reviewing refactored code in the Refactor branch. Objective: The goal is to understand the review process for refactoring changes in the Refactor branch and to identify what developers care about when reviewing code in this branch. Method: In this study, we present a quantitative and qualitative examination to understand the main criteria developers use to decide whether to accept or reject refactored code submissions and identify the challenges inherent in this process. Results: Analyzing 2,154 refactoring and non-refactoring reviews across Qt open-source projects, we find that reviews involving refactoring from the Refactor branch take significantly less time to resolve in terms of code review efforts. Additionally, documentation of developer intent is notably sparse within the Refactor branch compared to other branches. Furthermore, through thematic analysis of a substantial sample of refactoring code review discussions, we construct a comprehensive taxonomy consisting of 12 refactoring review criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04678v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Eman Abdullah AlOmar</dc:creator>
    </item>
    <item>
      <title>Automated Test-Case Generation for REST APIs Using Model Inference Search Heuristic</title>
      <link>https://arxiv.org/abs/2412.03420</link>
      <description>arXiv:2412.03420v2 Announce Type: replace 
Abstract: The rising popularity of the microservice architectural style has led to a growing demand for automated testing approaches tailored to these systems. EvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) to automatically generate test cases for microservices' REST APIs. One limitation of these EAs is the use of unit-level search heuristics, such as branch distances, which focus on fine-grained code coverage and may not effectively capture the complex, interconnected behaviors characteristic of system-level testing. To address this limitation, we propose a new search heuristic (MISH) that uses real-time automaton learning to guide the test case generation process. We capture the sequential call patterns exhibited by a test case by learning an automaton from the stream of log events outputted by different microservices within the same system. Therefore, MISH learns a representation of the systemwide behavior, allowing us to define the fitness of a test case based on the path it traverses within the inferred automaton. We empirically evaluate MISH's effectiveness on six real-world benchmark microservice applications and compare it against a state-of-the-art technique, MOSA, for testing REST APIs. Our evaluation shows promising results for using MISH to guide the automated test case generation within EvoMaster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03420v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clinton Cao, Annibale Panichella, Sicco Verwer</dc:creator>
    </item>
    <item>
      <title>"Estimating software project effort using analogies": Reflections after 28 years</title>
      <link>https://arxiv.org/abs/2501.14582</link>
      <description>arXiv:2501.14582v2 Announce Type: replace 
Abstract: Background: This invited paper is the result of an invitation to write a retrospective article on a "TSE most influential paper" as part of the journal's 50th anniversary. Objective: To reflect on the progress of software engineering prediction research using the lens of a selected, highly cited research paper and 28 years of hindsight. Methods: The paper examines (i) what was achieved, (ii) what has endured and (iii) what could have been done differently with the benefit of retrospection. Conclusions: While many specifics of software project effort prediction have evolved, key methodological issues remain relevant. The original study emphasised empirical validation with benchmarks, out-of-sample testing and data/tool sharing. Four areas for improvement are identified: (i) stronger commitment to Open Science principles, (ii) focus on effect sizes and confidence intervals, (iii) reporting variability alongside typical results and (iv) more rigorous examination of threats to validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14582v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TSE.2025.3534032</arxiv:DOI>
      <dc:creator>Martin Shepperd</dc:creator>
    </item>
    <item>
      <title>Ultraverse: A System-Centric Framework for Efficient What-If Analysis for Database-Intensive Web Applications</title>
      <link>https://arxiv.org/abs/2211.05327</link>
      <description>arXiv:2211.05327v4 Announce Type: replace-cross 
Abstract: Existing what-if analysis systems are predominantly tailored to operate on either only the application layer or only the database layer of software. This isolated approach limits their effectiveness in scenarios where intensive interaction between applications and database systems occurs. To address this gap, we introduce Ultraverse, a what-if analysis framework that seamlessly integrates both application and database layers. Ultraverse employs dynamic symbolic execution to effectively translate application code into compact SQL procedure representations, thereby synchronizing application semantics at both SQL and application levels during what-if replays. A novel aspect of Ultraverse is its use of advanced query dependency analysis, which serves two key purposes: (1) it eliminates the need to replay irrelevant transactions that do not influence the outcome, and (2) it facilitates parallel replay of mutually independent transactions, significantly enhancing the analysis efficiency. Ultraverse is applicable to existing unmodified database systems and legacy application codes. Our extensive evaluations of the framework have demonstrated remarkable improvements in what-if analysis speed, achieving performance gains ranging from 7.7x to 291x across diverse benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.05327v4</guid>
      <category>cs.DB</category>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronny Ko, Chuan Xiao, Makoto Onizuka, Yihe Huang, Zhiqiang Lin</dc:creator>
    </item>
    <item>
      <title>SoK: A Literature and Engineering Review of Regular Expression Denial of Service</title>
      <link>https://arxiv.org/abs/2406.11618</link>
      <description>arXiv:2406.11618v3 Announce Type: replace-cross 
Abstract: Regular Expression Denial of Service (ReDoS) is a vulnerability class that has become prominent in recent years. Attackers can weaponize such weaknesses as part of asymmetric cyberattacks that exploit the slow worst-case matching time of regular expression (regex) engines. In the past, problematic regular expressions have led to outages at Cloudflare and Stack Overflow, showing the severity of the problem. While ReDoS has drawn significant research attention, there has been no systematization of knowledge to delineate the state of the art and identify opportunities for further research. In this paper, we describe the existing knowledge on ReDoS. We first provide a systematic literature review, discussing approaches for detecting, preventing, and mitigating ReDoS vulnerabilities. Then, our engineering review surveys the latest regex engines to examine whether and how ReDoS defenses have been realized. Combining our findings, we observe that (1) in the literature, almost no studies evaluate whether and how ReDoS vulnerabilities can be weaponized against real systems, making it difficult to assess their real-world impact; and (2) from an engineering view, many mainstream regex engines now have ReDoS defenses, rendering many threat models obsolete. We conclude with an extensive discussion, highlighting avenues for future work. The open challenges in ReDoS research are to evaluate emerging defenses and support engineers in migrating to defended engines. We also highlight the parallel between performance bugs and asymmetric DoS, and we argue that future work should capitalize more on this similarity and adopt a more systematic view on ReDoS-like vulnerabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11618v3</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masudul Hasan Masud Bhuiyan, Berk \c{C}akar, Ethan H. Burmane, James C. Davis, Cristian-Alexandru Staicu</dc:creator>
    </item>
    <item>
      <title>Establishing Provenance Before Coding: Traditional and Next-Gen Software Signing</title>
      <link>https://arxiv.org/abs/2407.03949</link>
      <description>arXiv:2407.03949v2 Announce Type: replace-cross 
Abstract: Software engineers integrate third-party components into their applications. The resulting software supply chain is vulnerable. To reduce the attack surface, we can verify the origin of components (provenance) before adding them. Cryptographic signatures enable this. This article describes traditional signing, its challenges, and the changes introduced by next-generation signing platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03949v2</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taylor R. Schorlemmer, Ethan H. Burmane, Kelechi G. Kalu, Santiago Torres-Arias, James C. Davis</dc:creator>
    </item>
  </channel>
</rss>
