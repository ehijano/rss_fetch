<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Mar 2025 02:04:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Neurosymbolic Decision Trees</title>
      <link>https://arxiv.org/abs/2503.08762</link>
      <description>arXiv:2503.08762v1 Announce Type: cross 
Abstract: Neurosymbolic (NeSy) AI studies the integration of neural networks (NNs) and symbolic reasoning based on logic. Usually, NeSy techniques focus on learning the neural, probabilistic and/or fuzzy parameters of NeSy models. Learning the symbolic or logical structure of such models has, so far, received less attention. We introduce neurosymbolic decision trees (NDTs), as an extension of decision trees together with a novel NeSy structure learning algorithm, which we dub NeuID3. NeuID3 adapts the standard top-down induction of decision tree algorithms and combines it with a neural probabilistic logic representation, inherited from the DeepProbLog family of models. The key advantage of learning NDTs with NeuID3 is the support of both symbolic and subsymbolic data (such as images), and that they can exploit background knowledge during the induction of the tree structure, In our experimental evaluation we demonstrate the benefits of NeSys structure learning over more traditonal approaches such as purely data-driven learning with neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08762v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias M\"oller, Arvid Norlander, Pedro Zuidberg Dos Martires, Luc De Raedt</dc:creator>
    </item>
    <item>
      <title>Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework</title>
      <link>https://arxiv.org/abs/2503.09504</link>
      <description>arXiv:2503.09504v1 Announce Type: cross 
Abstract: The Mixture-of-Experts (MoE) model has succeeded in deep learning (DL). However, its complex architecture and advantages over dense models in image classification remain unclear. In previous studies, MoE performance has often been affected by noise and outliers in the input space. Some approaches incorporate input clustering for training MoE models, but most clustering algorithms lack access to labeled data, limiting their effectiveness. This paper introduces the Double-stage Feature-level Clustering and Pseudo-labeling-based Mixture of Experts (DFCP-MoE) framework, which consists of input feature extraction, feature-level clustering, and a computationally efficient pseudo-labeling strategy. This approach reduces the impact of noise and outliers while leveraging a small subset of labeled data to label a large portion of unlabeled inputs. We propose a conditional end-to-end joint training method that improves expert specialization by training the MoE model on well-labeled, clustered inputs. Unlike traditional MoE and dense models, the DFCP-MoE framework effectively captures input space diversity, leading to competitive inference results. We validate our approach on three benchmark datasets for multi-class classification tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09504v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LO</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bakary Badjie, Jos\'e Cec\'ilio, Ant\'onio Casimiro</dc:creator>
    </item>
    <item>
      <title>Taming denumerable Markov decision processes with decisiveness</title>
      <link>https://arxiv.org/abs/2008.10426</link>
      <description>arXiv:2008.10426v2 Announce Type: replace 
Abstract: Decisiveness has proven to be an elegant concept for denumerable Markov chains: it is general enough to encompass several natural classes of denumerable Markov chains, and is a sufficient condition for simple qualitative and approximate quantitative model checking algorithms to exist. In this paper, we explore how to extend the notion of decisiveness to Markov decision processes. Compared to Markov chains, the extra non-determinism can be resolved in an adversarial or cooperative way, yielding two natural notions of decisiveness. We then explore whether these notions yield model checking procedures concerning the infimum and supremum probabilities of reachability properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.10426v2</guid>
      <category>cs.LO</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathalie Bertrand, Patricia Bouyer, Thomas Brihaye, Paulin Fournier, Pierre Vandenhove</dc:creator>
    </item>
    <item>
      <title>The structure of polynomial growth for tree automata/transducers and MSO set queries</title>
      <link>https://arxiv.org/abs/2501.10270</link>
      <description>arXiv:2501.10270v2 Announce Type: replace-cross 
Abstract: Given an $\mathbb{N}$-weighted tree automaton, we give a decision procedure for exponential vs polynomial growth (with respect to the input size) in quadratic time, and an algorithm that computes the exact polynomial degree of growth in cubic time. As a special case, they apply to the growth of the ambiguity of a nondeterministic tree automaton, i.e. the number of distinct accepting runs over a given input. Our time complexities match the recent fine-grained lower bounds for these problems restricted to ambiguity of word automata.
  We deduce analogous decidability results (ignoring complexity) for the growth of the number of results of set queries in Monadic Second-Order logic (MSO) over ranked trees. In the case of polynomial growth of degree $k$, we also prove a reparameterization theorem for such queries: their results can be mapped to $k$-tuples of input nodes in a finite-to-one and MSO-definable fashion.
  This property of MSO set queries leads directly to a generalization of the dimension minimization theorem for string-to-string polyregular functions. Our generalization applies to MSO set interpretations from trees, which subsume (as we show) tree-walking tree transducers and invisible pebble tree-to-string transducers. Finally, with a bit more work we obtain the following:
  * a new, short and conceptual proof that macro tree transducers (MTTs) of linear growth compute only tree-to-tree MSO transductions;
  * a procedure to decide polynomial size-to-height increase for MTTs and compute the degree.
  The paper concludes with a survey of a wide range of related work, with over a hundred references.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10270v2</guid>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Thu, 13 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Gallot, Nathan Lhote, L\^e Th\`anh D\~ung Nguy\^en</dc:creator>
    </item>
  </channel>
</rss>
