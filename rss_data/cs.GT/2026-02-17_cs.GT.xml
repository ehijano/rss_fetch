<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 18 Feb 2026 02:34:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Complexity of Tournament Fixing: Subset FAS Number and Acyclic Neighborhoods</title>
      <link>https://arxiv.org/abs/2602.13422</link>
      <description>arXiv:2602.13422v1 Announce Type: new 
Abstract: The \textsc{Tournament Fixing Problem} (TFP) asks whether a knockout tournament can be scheduled to guarantee that a given player $v^*$ wins. Although TFP is NP-hard in general, it is known to be \emph{fixed-parameter tractable} (FPT) when parameterized by the feedback arc/vertex set number, or the in/out-degree of $v^*$ (AAAI 17; IJCAI 18; AAAI 23; AAAI 26). However, it remained open whether TFP is FPT with respect to the \emph{subset FAS number of $v^*$} -- the minimum number of arcs intersecting all cycles containing $v^*$ -- a parameter that is never larger than the aforementioned ones (AAAI 26). In this paper, we resolve this question negatively by proving that TFP stays NP-hard even when the subset FAS number of $v^*$ is constant $\geq 1$ and either the subgraph induced by the in-neighbors $D[N_{\mathrm{in}}(v^*)]$ or the out-neighbors $D[N_{\mathrm{out}}(v^*)]$ is acyclic. Conversely, when both $D[N_{\mathrm{in}}(v^*)]$ and $D[N_{\mathrm{out}}(v^*)]$ are acyclic, we show that TFP becomes FPT parameterized by the subset FAS number of $v^*$. Furthermore, we provide sufficient conditions under which $v^*$ can win even when this parameter is unbounded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13422v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxi Liu, Junqiang Peng, Mingyu Xiao</dc:creator>
    </item>
    <item>
      <title>Personalization Aids Pluralistic Alignment Under Competition</title>
      <link>https://arxiv.org/abs/2602.13451</link>
      <description>arXiv:2602.13451v1 Announce Type: new 
Abstract: Can competition among misaligned AI providers yield aligned outcomes for a diverse population of users, and what role does model personalization play? We study a setting where multiple competing AI providers interact with multiple users who must make downstream decisions but differ in preferences. Providers have their own objectives over users' actions and strategically deploy AI models to advance them. We model the interaction as a Stackelberg game with multiple leaders (providers) and followers (users): providers commit to conversational policies, and users choose which model to use, how to converse, and how to act. With user-specific personalization, we show that under a Weak Market Alignment condition, every equilibrium gives each user outcomes comparable to those from a perfectly aligned common model -- so personalization can induce pluralistically aligned outcomes, even when providers are self-interested. In contrast, when providers must deploy a single anonymous policy, there exist equilibria with uninformative behavior under the same condition. We then give a stronger alignment condition that guarantees each user their optimal utility in the anonymous setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13451v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalie Collina, Surbhi Goel, Aaron Roth, Mirah Shi</dc:creator>
    </item>
    <item>
      <title>Revenue-Optimal Pricing for Budget-Constrained Buyers in Data Markets</title>
      <link>https://arxiv.org/abs/2602.13897</link>
      <description>arXiv:2602.13897v1 Announce Type: new 
Abstract: We study revenue-optimal pricing in data markets with rational, budget-constrained buyers. Such a market offers multiple datasets for sale, and buyers aim to improve the accuracy of their prediction tasks by acquiring data bundles. For each dataset, the market sets a pricing function, which maps the number of records purchased from the dataset to a non-negative price. The market's objective is to set these pricing functions to maximize total revenue, considering that buyers with quasi-linear utilities choose their bundles optimally under budget constraints.
  We analyze optimal pricing when each dataset's pricing function is only required to be monotone and lower-continuous. Surprisingly, even with this generality, optimal pricing has a highly structured form: it is piecewise linear and convex (PLC) and can be computed efficiently via an LP. Moreover, the total number of kinks across all pricing functions is bounded by the number of buyers. Thus, when datasets far outnumber buyers, most pricing functions are effectively linear.
  This motivates studying linear pricing, where each record in a dataset is priced uniformly. Although competitive equilibrium gives revenue-optimal linear prices in rivalrous markets with quasi-linear buyers, we show that revenue maximization under linear pricing in data markets is APX-hard. Hence, a striking computational dichotomy emerges: fully general (nonlinear) pricing admits a polynomial-time algorithm, while the simpler linear scheme is APX-hard.
  Despite the hardness, we design a 2-approximation algorithm when datasets arrive online, and a $(1-1/e)^{-1}$-approximation algorithm for the offline setting. Our framework lays the groundwork for exploring more general pricing schemes, richer utility models, and a deeper understanding of how market structure -- rivalrous versus non-rivalrous -- shapes revenue-optimal pricing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13897v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bhaskar Ray Chaudhury, Jugal Garg, Eklavya Sharma, Jiaxin Song</dc:creator>
    </item>
    <item>
      <title>Truthful Reporting of Competence with Minimal Verification</title>
      <link>https://arxiv.org/abs/2602.14076</link>
      <description>arXiv:2602.14076v1 Announce Type: new 
Abstract: Suppose you run a home exam, where students should report their own scores but can cheat freely. You can, if needed, call a limited number of students to class and verify their actual performance against their reported score. We consider the class of mechanisms where truthful reporting is a dominant strategy, and truthful agents are never penalized -- even off-equilibrium.
  How many students do we need to verify, in expectation, if we want to minimize the bias, i.e., the difference between agents' competence and their expected grade? When perfect verification is available, we characterize the best possible tradeoff between these requirements and provide a simple parametrized mechanism that is optimal in the class for any distribution of agents' types. When verification is noisy, the task becomes much more challenging. We show how proper scoring rules can be leveraged in different ways to construct truthful mechanisms with a good (though not necessarily optimal) tradeoff.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14076v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.65109/CZFV1739</arxiv:DOI>
      <dc:creator>Reshef Meir, Jonathan Wagner, Omer Ben-Porat</dc:creator>
    </item>
    <item>
      <title>Evaluating the Performance of Approximation Mechanisms under Budget Constraints</title>
      <link>https://arxiv.org/abs/2602.14120</link>
      <description>arXiv:2602.14120v1 Announce Type: new 
Abstract: We study revenue maximization in a buyer-seller setting where the seller has a single object and the buyer has both a private valuation and a private budget. The presence of private budgets complicates the classic single-product monopoly problem, making optimal mechanisms difficult to analyze. To overcome this, we evaluate the robust performance of approximation mechanisms relative to optimal mechanisms. We work with three measures of performance: the guaranteed fraction of optimal revenue (GFOR) for restricted classes of mechanisms, the maximal value of relaxation (MVR) for relaxed classes, and a revenue non-monotonicity gap for either relaxed or restricted classes. Our analysis reveals sharp contrasts. On the positive side, we show that for distributions with bounded support, simple mechanisms with poly-logarithmic menu size can approximate optimal revenue arbitrarily well, regardless of correlation between valuations and budgets. On the negative side, we establish strong impossibility results: for distributions with unbounded support, or even bounded distributions concentrated in the unit square, no simple mechanism - or indeed any mechanism with a finite or sublinear menu - can guarantee a positive fraction of the optimal revenue. We also demonstrate unbounded revenue gains from certain relaxations when valuations and budgets are negatively correlated, and highlight cases of revenue non-monotonicity. Taken together, our results underscore the fragility of approximation approaches in the presence of private budgets: except for a narrow set of conditions, approximation mechanisms incur large revenue losses, pointing to fundamental limits of simplicity and robustness in mechanism design. Our analysis highlights that approximation results are highly sensitive to details of the design environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14120v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Carlos Carbajal, Ahuva Mualem</dc:creator>
    </item>
    <item>
      <title>Pareto and Bowley Reinsurance Games in Peer-to-Peer Insurance</title>
      <link>https://arxiv.org/abs/2602.14223</link>
      <description>arXiv:2602.14223v1 Announce Type: new 
Abstract: We propose a peer-to-peer (P2P) insurance scheme comprising a risk-sharing pool and a reinsurer. A plan manager determines how risks are allocated among members and ceded to the reinsurer, while the reinsurer sets the reinsurance loading. Our work focuses on the strategic interaction between the plan manager and the reinsurer, and this focus leads to two game-theoretic contract designs: a Pareto design and a Bowley design, for which we derive closed-form optimal contracts. In the Pareto design, cooperation between the reinsurer and the plan manager leads to multiple Pareto-optimal contracts, which are further refined by introducing the notion of coalitional stability. In contrast, the Bowley design yields a unique optimal contract through a leader-follower framework, and we provide a rigorous verification of the individual rationality constraints via pointwise comparisons of payoff vectors. Comparing the two designs, we prove that the Bowley-optimal contract is never Pareto optimal and typically yields lower total welfare. In our numerical examples, the presence of reinsurance improves welfare, especially with Pareto designs and a less risk-averse reinsurer. We further analyze the impact of the single-loading restriction, which disproportionately favors members with riskier losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14223v1</guid>
      <category>cs.GT</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tim J. Boonen, Kenneth Tsz Hin Ng, Tak Wa Ng, Thai Nguyen</dc:creator>
    </item>
    <item>
      <title>Characterizing Robustness of Strategies to Novelty in Zero-Sum Open Worlds</title>
      <link>https://arxiv.org/abs/2602.14278</link>
      <description>arXiv:2602.14278v1 Announce Type: new 
Abstract: In open-world environments, artificial agents must often contend with novel conditions that deviate from their training or design assumptions. This paper studies the robustness of fixed-strategy agents to such novelty within the setting of two-player zero-sum games. We present a general framework for characterizing the impact of environmental novelties, such as changes in payoff structure or action constraints, on agent performance in two distinct domains: Iterated Prisoner's Dilemma (IPD) and heads-up Texas Hold'em Poker. Novelty is operationalized as a perturbation of the game's rules or scoring mechanics, while agent behavior remains fixed. To measure the effects, we introduce two metrics: per-agent robustness, quantifying the relative performance shift of each strategy across novelties, and global impact, summarizing the population-wide disruption caused by a novelty. Our experiments, comprising 30 IPD agents across 20 payoff matrix novelties and 10 Poker agents across 5 rule-based novelties, reveal systematic patterns in robustness and highlight certain novelties that induce severe destabilization. The results offer insights into agent generalizability under perturbation and provide a quantitative basis for designing safer and more resilient autonomous systems in adversarial and dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14278v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mayank Kejriwal, Shilpa Thomas, Hongyu Li</dc:creator>
    </item>
    <item>
      <title>Offline Learning of Nash Stable Coalition Structures with Possibly Overlapping Coalitions</title>
      <link>https://arxiv.org/abs/2602.14321</link>
      <description>arXiv:2602.14321v1 Announce Type: new 
Abstract: Coalition formation concerns strategic collaborations of selfish agents that form coalitions based on their preferences. It is often assumed that coalitions are disjoint and preferences are fully known, which may not hold in practice. In this paper, we thus present a new model of coalition formation with possibly overlapping coalitions under partial information, where selfish agents may be part of multiple coalitions simultaneously and their full preferences are initially unknown. Instead, information about past interactions and associated utility feedback is stored in a fixed offline dataset, and we aim to efficiently infer the agents' preferences from this dataset. We analyze the impact of diverse dataset information constraints by studying two types of utility feedback that can be stored in the dataset: agent- and coalition-level utility feedback. For both feedback models, we identify assumptions under which the dataset covers sufficient information for an offline learning algorithm to infer preferences and use them to recover a partition that is (approximately) Nash stable, in which no agent can improve her utility by unilaterally deviating. Our additional goal is devising algorithms with low sample complexity, requiring only a small dataset to obtain a desired approximation to Nash stability. Under agent-level feedback, we provide a sample-efficient algorithm proven to obtain an approximately Nash stable partition under a sufficient and necessary assumption on the information covered by the dataset. However, under coalition-level feedback, we show that only under a stricter assumption is sufficient for sample-efficient learning. Still, in multiple cases, our algorithms' sample complexity bounds have optimality guarantees up to logarithmic factors. Finally, extensive experiments show that our algorithm converges to a low approximation level to Nash stability across diverse settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14321v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saar Cohen</dc:creator>
    </item>
    <item>
      <title>A Bayesian Framework for Human-AI Collaboration: Complementarity and Correlation Neglect</title>
      <link>https://arxiv.org/abs/2602.14331</link>
      <description>arXiv:2602.14331v1 Announce Type: new 
Abstract: We develop a decision-theoretic model of human-AI interaction to study when AI assistance improves or impairs human decision-making. A human decision-maker observes private information and receives a recommendation from an AI system, but may combine these signals imperfectly. We show that the effect of AI assistance decomposes into two main forces: the marginal informational value of the AI beyond what the human already knows, and a behavioral distortion arising from how the human uses the AI's recommendation. Central to our analysis is a micro-founded measure of informational overlap between human and AI knowledge. We study an empirically relevant form of imperfect decision-making -- correlation neglect -- whereby humans treat AI recommendations as independent of their own information despite shared evidence. Under this model, we characterize how overlap and AI capabilities shape the Human-AI interaction regime between augmentation, impairment, complementarity, and automation, and draw key insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14331v1</guid>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <category>econ.TH</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saurabh Amin, Amine Bennouna, Daniel Huttenlocher, Dingwen Kong, Liang Lyu, Asuman Ozdaglar</dc:creator>
    </item>
    <item>
      <title>Truthful Reverse Auctions for Adaptive Selection via Contextual Multi-Armed Bandits</title>
      <link>https://arxiv.org/abs/2602.14476</link>
      <description>arXiv:2602.14476v1 Announce Type: new 
Abstract: We study the problem of selecting large language models (LLMs) for user queries in settings where multiple LLM providers submit the cost of solving a query. From the users' perspective, choosing an optimal model is a sequential, query-dependent decision problem: high-capacity models offer more reliable outputs but are costlier, while lightweight models are faster and cheaper. We formalize this interaction as a reverse auction design problem with contextual online learning, where the user adaptively discovers which model performs best while eliciting costs from competing LLM providers. Existing multi-armed bandit (MAB) mechanisms focus on forward auctions and social welfare, leaving open the challenges of reverse auctions, provider-optimal outcomes, and contextual adaptation. We address these gaps by designing a resampling-based procedure that generalizes truthful forward MAB mechanisms to reverse auctions and prove that any monotone allocation rule with this procedure is truthful. Using this, we propose a contextual MAB algorithm that learns query-dependent model quality with sublinear regret. Our framework unifies mechanism design and adaptive learning, enabling efficient, truthful, and query-aware LLM selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14476v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pronoy Patra, Sankarshan Damle, Manisha Padala, Sujit Gujar</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Best-of-Both-Worlds Fairness for Few Agents</title>
      <link>https://arxiv.org/abs/2602.14668</link>
      <description>arXiv:2602.14668v1 Announce Type: new 
Abstract: We consider the problem of fair allocation of indivisible goods among agents with additive valuations, aiming for Best-of-Both-Worlds (BoBW) fairness: a distribution over allocations that is ex-ante fair, and additionally, it is supported only on deterministic allocations that are ex-post fair. We focus on BoBW for few agents, and our main result is the design of the first BoBW algorithms achieving near-optimal fairness for three agents. For three agents, we prove the existence of an ex-ante proportional distribution whose every allocation is Epistemic EFX (EEFX) and guarantees each agent at least $\tfrac{9}{10}$ of her MMS. As MMS allocations do not exist for three additive agents, in every allocation at least one agent might not be getting her MMS. To compensate such an agent, we also guarantee that if an agent is not getting her MMS then she is EFX-satisfied - giving her the strongest achievable envy-based guarantee. Additionally, using an FPTAS for near-MMS partitions, we present an FPTAS to compute a BoBW distribution preserving all envy-based guarantees, and also preserving all value-based guarantees up to $(1-\varepsilon)$. We further show that exact ex-ante proportionality can be restored when dropping EEFX. To do so, we first design, for two agents and any $\varepsilon &gt; 0$, a Fully Polynomial-Time Approximation Scheme (FPTAS) that outputs a distribution which is ex-ante envy-free (and thus proportional) and ex-post envy-free up to any good (EFX), while guaranteeing each agent at least a $(1-\varepsilon)$-fraction of her maximin share (MMS). We then leverage this two-agent FPTAS algorithm as a subroutine to obtain, for three agents, the FPTAS guaranteeing exact ex-ante proportionality. We note that our result for two agents essentially matches the strongest fairness and efficiency guarantees achievable in polynomial time, and thus might be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14668v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moshe Babaioff, Gefen Frosh</dc:creator>
    </item>
    <item>
      <title>Revenue Guarantees in Autobidding Platforms</title>
      <link>https://arxiv.org/abs/2602.14815</link>
      <description>arXiv:2602.14815v1 Announce Type: new 
Abstract: Motivated by autobidding systems in online advertising, we study revenue maximization in markets with divisible goods and budget-constrained buyers with linear valuations. Our aim is to compute a single price for each good and an allocation that maximizes total revenue. We show that the First-Price Pacing Equilibrium (FPPE) guarantees at least half of the optimal revenue, even when compared to the maximal revenue of buyer-specific prices. This guarantee is particularly striking in light of our hardness result: we prove that revenue maximization under individual rationality and single-price-per-good constraints is APX-hard.
  We further extend our analysis in two directions: first, we introduce an online analogue of FPPE and show that it achieves a constant-factor revenue guarantee, specifically a $1/4$-approximation; second, we consider buyers with concave valuation functions, characterizing an FPPE-type outcome as the solution to an Eisenberg-Gale-style convex program and showing that the revenue approximation degrades gracefully with the degree of nonlinearity of the valuations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14815v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Caragiannis, Anders Bo Ipsen, Stratis Skoulakis</dc:creator>
    </item>
    <item>
      <title>Fair Allocation with Initial Utilities</title>
      <link>https://arxiv.org/abs/2602.14850</link>
      <description>arXiv:2602.14850v1 Announce Type: new 
Abstract: The problem of allocating indivisible resources to agents arises in a wide range of domains, including treatment distribution and social support programs. An important goal in algorithm design for this problem is fairness, where the focus in previous work has been on ensuring that the computed allocation provides equal treatment to everyone. However, this perspective disregards that agents may start from unequal initial positions, which is crucial to consider in settings where fairness is understood as equality of outcome. In such settings, the goal is to create an equal final outcome for everyone by leveling initial inequalities through the allocated resources. To close this gap, focusing on agents with additive utilities, we extend the classic model by assigning each agent an initial utility and study the existence and computational complexity of several new fairness notions following the principle of equality of outcome. Among others, we show that complete allocations satisfying a direct analog of envy-freeness up to one resource (EF1) may fail to exist and are computationally hard to find, forming a contrast to the classic setting without initial utilities. We propose a new, always satisfiable fairness notion, called minimum-EF1-init and design a polynomial-time algorithm based on an extended round-robin procedure to compute complete allocations satisfying this notion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14850v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niclas Boehmer, Luca Kreisel</dc:creator>
    </item>
    <item>
      <title>Thermal Min-Max Games: Unifying Bounded Rationality and Typical-Case Equilibrium</title>
      <link>https://arxiv.org/abs/2602.14858</link>
      <description>arXiv:2602.14858v1 Announce Type: new 
Abstract: Strategic-form min-max game theory examines the existence, multiplicity, selection of equilibria, and the worst-case computational complexity under perfect rationality. However, in many applications, games are drawn from an ensemble, and players exhibit bounded rationality. We introduce thermal min-max games, a thermodynamic relaxation that unifies bounded and perfect rationality by assigning each player a temperature to regulate their rationality level. To analyze typical behavior in the large-strategy limit, we develop a nested replica framework for this relaxation. This theory provides tractable predictions for typical equilibrium values and mixed-strategy statistics as functions of rationality strength, strategy-count aspect ratio, and payoff randomness. Numerical experiments demonstrate that these asymptotic predictions accurately align with the equilibrium of finite games of moderate size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14858v1</guid>
      <category>cs.GT</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuma Ichikawa</dc:creator>
    </item>
    <item>
      <title>The Distortion of Stable Matching</title>
      <link>https://arxiv.org/abs/2602.14961</link>
      <description>arXiv:2602.14961v1 Announce Type: new 
Abstract: We initiate the study of distortion in stable matching. Concretely, we aim to design algorithms that have limited access to the agents' cardinal preferences and compute stable matchings of high quality with respect to some aggregate objective, e.g., the social welfare. Our first result is a strong impossibility: the classic Deferred Acceptance (DA) algorithm of Gale and Shapley [1962], as well as any deterministic algorithm that relies solely on ordinal information about the agents' preferences, has unbounded distortion.
  To circumvent this impossibility, we consider algorithms that either (a) use randomization or (b) perform a small number of value queries to the agents' cardinal preferences. In the former case, we prove that a simple randomized version of the DA algorithm achieves a distortion of $2$, and that this is optimal among all randomized stable matching algorithms. For the latter case, we prove that the same bound of $2$ can be achieved with only $1$ query per agent, and improving upon this bound requires $\Omega(\log n)$ queries per agent. We further show that this query bound is asymptotically optimal for any constant approximation: for any $\varepsilon &gt;0$, there exists an algorithm which uses $O(\log n /\varepsilon^2)$ queries, and achieves a distortion of $1+\varepsilon$. Moreover, under natural structural restrictions on the instances of the problem, we provide improved upper bounds on the number of queries required for a $(1+\varepsilon)$-approximation.
  We complement our main findings above with theoretical and empirical results on the average-case performance of stable matching algorithms, when the preferences of the agents are drawn i.i.d. from a given distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14961v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aris Filos-Ratsikas, Georgios Kalantzis</dc:creator>
    </item>
    <item>
      <title>Robust Value Maximization in Challenge the Champ Tournaments with Probabilistic Outcomes</title>
      <link>https://arxiv.org/abs/2602.14966</link>
      <description>arXiv:2602.14966v1 Announce Type: new 
Abstract: Challenge the Champ is a simple tournament format, where an ordering of the players -- called a seeding -- is decided. The first player in this order is the initial champ, and faces the next player. The outcome of each match decides the current champion, who faces the next player in the order. Each player also has a popularity, and the value of each match is the popularity of the winner. Value maximization in tournaments has been previously studied when each match has a deterministic outcome. However, match outcomes are often probabilistic, rather than deterministic. We study robust value maximization in Challenge the Champ tournaments, when the winner of a match may be probabilistic. That is, we seek to maximize the total value that is obtained, irrespective of the outcome of probabilistic matches. We show that even in simple binary settings, for non-adaptive algorithms, the optimal robust value -- which we term the \textsc{VnaR}, or the value not at risk -- is hard to approximate. However, if we allow adaptive algorithms that determine the order of challengers based on the outcomes of previous matches, or restrict the matches with probabilistic outcomes, we can obtain good approximations to the optimal \textsc{VnaR}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14966v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Umang Bhaskar, Juhi Chaudhary, Sushmita Gupta, Pallavi Jain, Sanjay Seetharaman</dc:creator>
    </item>
    <item>
      <title>Robust Mean-Field Games with Risk Aversion and Bounded Rationality</title>
      <link>https://arxiv.org/abs/2602.13353</link>
      <description>arXiv:2602.13353v1 Announce Type: cross 
Abstract: Recent advances in mean-field game literature enable the reduction of large-scale multi-agent problems to tractable interactions between a representative agent and a population distribution. However, existing approaches typically assume a fixed initial population distribution and fully rational agents, limiting robustness under distributional uncertainty and cognitive constraints. We address these limitations by introducing risk aversion with respect to the initial population distribution and by incorporating bounded rationality to model deviations from fully rational decision-making agents. The combination of these two elements yields a new and more general equilibrium concept, which we term the mean-field risk-averse quantal response equilibrium (MF-RQE). We establish existence results and prove convergence of fixed-point iteration and fictitious play to MF-RQE. Building on these insights, we develop a scalable reinforcement learning algorithm for scenarios with large state-action spaces. Numerical experiments demonstrate that MF-RQE policies achieve improved robustness relative to classical mean-field approaches that optimize expected cumulative rewards under a fixed initial distribution and are restricted to entropy-based regularizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13353v1</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bhavini Jeloka, Yue Guan, Panagiotis Tsiotras</dc:creator>
    </item>
    <item>
      <title>Endogenous Epistemic Weighting under Heterogeneous Information: Beyond Majority Rule</title>
      <link>https://arxiv.org/abs/2602.13499</link>
      <description>arXiv:2602.13499v1 Announce Type: cross 
Abstract: Collective decision-making can be viewed as the problem of aggregating multiple noisy information channels about an unknown state of the world. Classical epistemic justifications of majority rule rely on restrictive assumptions about the homogeneity and symmetry of these channels, which are often violated in realistic environments. This paper introduces the Epistemic Shared-Choice Mechanism (ESCM), a lightweight and auditable procedure that endogenously estimates issue-specific signal reliability and assigns bounded, decision-specific voting weights. Using central limit approximations, the paper provides an analytical comparison between ESCM and unweighted majority rule, showing how their relative epistemic performance depends on the distributional structure of information in the population, including unimodal competence distributions and segmented environments with informed minorities. The results indicate that endogenous and bounded epistemic weighting can improve collective accuracy by merging procedural and epistemic requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13499v1</guid>
      <category>econ.GN</category>
      <category>cs.GT</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Enrico Manfredi</dc:creator>
    </item>
    <item>
      <title>Stochastic variance reduced extragradient methods for solving hierarchical variational inequalities</title>
      <link>https://arxiv.org/abs/2602.13510</link>
      <description>arXiv:2602.13510v1 Announce Type: cross 
Abstract: We are concerned with optimization in a broad sense through the lens of solving variational inequalities (VIs) -- a class of problems that are so general that they cover as particular cases minimization of functions, saddle-point (minimax) problems, Nash equilibrium problems, and many others. The key challenges in our problem formulation are the two-level hierarchical structure and finite-sum representation of the smooth operators in each level. For this setting, we are the first to prove convergence rates and complexity statements for variance-reduced stochastic algorithms approaching the solution of hierarchical VIs in Euclidean and Bregman setups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13510v1</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pavel Dvurechensky, Andrea Ebner, Johannes Carl Schnebel, Shimrit Shtern, Mathias Staudigl</dc:creator>
    </item>
    <item>
      <title>NFT Games: an Empirical Look into the Play-to-Earn Model</title>
      <link>https://arxiv.org/abs/2602.13882</link>
      <description>arXiv:2602.13882v1 Announce Type: cross 
Abstract: The past decade has witnessed the burgeoning and continuous development of blockchain and its applications. Besides various cryptocurrencies, an industry that has quickly embraced this trend is gaming. Thanks to the support of blockchain, games have started to incorporate non-fungible tokens (NFTs) that can enable a new gaming model, play-to-earn (P2E), which incentivizes users to participate and play. While recent studies looked at several NFT games qualitatively and individually, an in-depth understanding is still missing, particularly on how the P2E model has transformed traditional games. In this work, we set to conduct a measurement study of NFT games, aiming to gain a comprehensive understanding of the effectiveness of P2E in practice. For this purpose, we collect and analyze relevant NFT transaction data from the underlying blockchain (e.g., Ethereum) of 12 games, supplemented with various data scraped from their websites. Our study shows that (1) a few top wallets control unproportionally high percentage of NFTs, and the majority of wallets own only one or two NFTs and do not actively trade; (2) promotion events do boost the trade amount and the NFT price for some games, but their effect does not sustain; and (3) few players actually earned a profit, and players in 9 out of 12 games who traded NFTs have a negative profit on average. Motivated by these findings, we further investigate effective incentive mechanisms based on game theory to improve the trading profits that players can earn from these NFT games. Both modeling and simulation results confirm the effectiveness of the proposed incentive mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13882v1</guid>
      <category>cs.CE</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiao Gao, Fei Li, Ruizhe Shi, Ruizhi Cheng, Jean Zhang, Bo Han, Songqing Chen</dc:creator>
    </item>
    <item>
      <title>Existence of Fair Resolute Voting Rules</title>
      <link>https://arxiv.org/abs/2602.13894</link>
      <description>arXiv:2602.13894v1 Announce Type: cross 
Abstract: Among two-candidate elections that treat the candidates symmetrically and never result in a tie, which voting rules are fair? A natural requirement is that each voter exerts an equal influence over the outcome, i.e., is equally likely to swing the election one way or the other. A voter's influence has been formalized in two canonical ways: the Shapley-Shubik (1954) index and the Banzhaf (1964) index. We consider both indices, and ask: Which electorate sizes admit a fair voting rule (under the respective index)?
  For an odd number $n$ of voters, simple majority rule is an example of a fair voting rule. However, when $n$ is even, fair voting rules can be challenging to identify, and a diverse literature has studied this problem under different notions of fairness. Our main results completely characterize which values of $n$ admit fair voting rules under the two canonical indices we consider. For the Shapley-Shubik index, a fair voting rule exists for $n&gt;1$ if and only if $n$ is not a power of $2$. For the Banzhaf index, a fair voting rule exists for all $n$ except $2$, $4$, and $8$. Along the way, we show how the Shapley-Shubik and Banzhaf indices relate to the winning coalitions of the voting rule, and compare these indices to previously considered notions of fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13894v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manik Dhar, Kunal Mittal, Clayton Thomas</dc:creator>
    </item>
    <item>
      <title>Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems</title>
      <link>https://arxiv.org/abs/2602.14471</link>
      <description>arXiv:2602.14471v1 Announce Type: cross 
Abstract: Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent's private objective and an estimate of group welfare via a social weight $\lambda\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $\beta$, we show that SWA induces a critical threshold $\lambda^*=(n-\beta)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14471v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Furkan Mumcu, Yasin Yilmaz</dc:creator>
    </item>
    <item>
      <title>Governing AI Forgetting: Auditing for Machine Unlearning Compliance</title>
      <link>https://arxiv.org/abs/2602.14553</link>
      <description>arXiv:2602.14553v1 Announce Type: cross 
Abstract: Despite legal mandates for the right to be forgotten, AI operators routinely fail to comply with data deletion requests. While machine unlearning (MU) provides a technical solution to remove personal data's influence from trained models, ensuring compliance remains challenging due to the fundamental gap between MU's technical feasibility and regulatory implementation. In this paper, we introduce the first economic framework for auditing MU compliance, by integrating certified unlearning theory with regulatory enforcement. We first characterize MU's inherent verification uncertainty using a hypothesis-testing interpretation of certified unlearning to derive the auditor's detection capability, and then propose a game-theoretic model to capture the strategic interactions between the auditor and the operator. A key technical challenge arises from MU-specific nonlinearities inherent in the model utility and the detection probability, which create complex strategic couplings that traditional auditing frameworks do not address and that also preclude closed-form solutions. We address this by transforming the complex bivariate nonlinear fixed-point problem into a tractable univariate auxiliary problem, enabling us to decouple the system and establish the equilibrium existence, uniqueness, and structural properties without relying on explicit solutions. Counterintuitively, our analysis reveals that the auditor can optimally reduce the inspection intensity as deletion requests increase, since the operator's weakened unlearning makes non-compliance easier to detect. This is consistent with recent auditing reductions in China despite growing deletion requests. Moreover, we prove that although undisclosed auditing offers informational advantages for the auditor, it paradoxically reduces the regulatory cost-effectiveness relative to disclosed auditing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14553v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Qinqi Lin, Ningning Ding, Lingjie Duan, Jianwei Huang</dc:creator>
    </item>
    <item>
      <title>AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises</title>
      <link>https://arxiv.org/abs/2602.14740</link>
      <description>arXiv:2602.14740v1 Announce Type: cross 
Abstract: Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act.
  Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making.
  Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence.
  We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14740v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenneth Payne</dc:creator>
    </item>
    <item>
      <title>Majoritarian Assignment Rules</title>
      <link>https://arxiv.org/abs/2602.14816</link>
      <description>arXiv:2602.14816v1 Announce Type: cross 
Abstract: A central problem in multiagent systems is the fair assignment of objects to agents. In this paper, we initiate the analysis of classic majoritarian social choice functions in assignment. Exploiting the special structure of the assignment domain, we show a number of surprising results with no counterparts in general social choice. In particular, we establish a near one-to-one correspondence between preference profiles and majority graphs. This correspondence implies that key properties of assignments -- such as Pareto-optimality, least unpopularity, and mixed popularity -- can be determined solely by the associated majority graph. We further show that all Pareto-optimal assignments are semi-popular and belong to the top cycle. Elements of the top cycle can thus easily be found via serial dictatorships. Our main result is a complete characterization of the top cycle, which implies the top cycle can only consist of one, two, all but two, all but one, or all assignments. By contrast, we find that the uncovered set contains only very few assignments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14816v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Brandt, Haoyuan Chen, Chris Dong, Patrick Lederer, Alexander Schlenga</dc:creator>
    </item>
    <item>
      <title>Auctioning Time to Mitigate Latency Races: Theory and Evidence from Blockchains</title>
      <link>https://arxiv.org/abs/2512.10094</link>
      <description>arXiv:2512.10094v3 Announce Type: replace 
Abstract: High-frequency trading, in both traditional and decentralized markets, induces latency races and redundant order flow as traders spend resources to win time-sensitive opportunities. We show that auctioning artificial time priority can redirect resources away from wasteful speed races toward auction payments. While such waste is difficult to measure in traditional markets, blockchain transactions provide transparent records of these competitive costs through observable duplicate submissions. We study the introduction of Timeboost, a time-priority auction mechanism on Arbitrum, a blockchain that batches transactions before settlement on Ethereum, as a natural experiment. We find that redundant transactions decrease and platform revenue increases relative to comparable networks, consistent with our theoretical predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10094v3</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agostino Capponi, Brian Zhu</dc:creator>
    </item>
    <item>
      <title>Adaptive Agents in Spatial Double-Auction Markets: Modeling the Emergence of Industrial Symbiosis</title>
      <link>https://arxiv.org/abs/2512.17979</link>
      <description>arXiv:2512.17979v2 Announce Type: replace 
Abstract: Industrial symbiosis fosters circularity by enabling firms to repurpose residual resources, yet its emergence is constrained by socio-spatial frictions that shape costs, matching opportunities, and market efficiency. Existing models often overlook the interaction between spatial structure, market design, and adaptive firm behavior, limiting our understanding of where and how symbiosis arises. We develop an agent-based model where heterogeneous firms trade byproducts through a spatially embedded double-auction market, with prices and quantities emerging endogenously from local interactions. Leveraging reinforcement learning, firms adapt their bidding strategies to maximize profit while accounting for transport costs, disposal penalties, and resource scarcity. Simulation experiments reveal the economic and spatial conditions under which decentralized exchanges converge toward stable and efficient outcomes. Counterfactual regret analysis shows that sellers' strategies approach a near Nash equilibrium, while sensitivity analysis highlights how spatial structures and market parameters jointly govern circularity. Our model provides a basis for exploring policy interventions that seek to align firm incentives with sustainability goals, and more broadly demonstrates how decentralized coordination can emerge from adaptive agents in spatially constrained markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17979v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.65109/EXII2056</arxiv:DOI>
      <arxiv:journal_reference>AAMAS 2026, Paphos, IFAAMAS, 10 pages</arxiv:journal_reference>
      <dc:creator>Matthieu Mastio, Paul Saves, Benoit Gaudou, Nicolas Verstaevel</dc:creator>
    </item>
    <item>
      <title>Metric Hedonic Games on the Line</title>
      <link>https://arxiv.org/abs/2602.05888</link>
      <description>arXiv:2602.05888v2 Announce Type: replace 
Abstract: Hedonic games are fundamental models for investigating the formation of coalitions among a set of strategic agents, where every agent has a certain utility for every possible coalition of agents it can be part of. To avoid the intractability of defining exponentially many utilities for all possible coalitions, many variants with succinct representations of the agents' utility functions have been devised and analyzed, e.g., modified fractional hedonic games by Monaco et al. [JAAMAS 2020]. We extend this by studying a novel succinct variant that is related to modified fractional hedonic games. In our model, each agent has a fixed type-value and an agent's cost for some given coalition is based on the differences between its value and those of the other members of its coalition. This allows to model natural situations like athletes forming training groups with similar performance levels or voters that partition themselves along a political spectrum.
  In particular, we investigate natural variants where an agent's cost is defined by distance thresholds, or by the maximum or average value difference to the other agents in its coalition. For these settings, we study the existence of stable coalition structures, their properties, and their quality in terms of the price of anarchy and the price of stability. Further, we investigate the impact of limiting the maximum number of coalitions. Despite the simple setting with metric distances on a line, we uncover a rich landscape of models, partially with counter-intuitive behavior. Also, our focus on both swap stability and jump stability allows us to study the influence of fixing the number and the size of the coalitions. Overall, we find that stable coalition structures always exist but that their properties and quality can vary widely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.05888v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Merlin de la Haye, Pascal Lenzner, Farehe Soheil, Marcus Wunderlich</dc:creator>
    </item>
    <item>
      <title>Characterizations of voting rules based on majority margins</title>
      <link>https://arxiv.org/abs/2501.08595</link>
      <description>arXiv:2501.08595v3 Announce Type: replace-cross 
Abstract: In the context of voting with ranked ballots, an important class of voting rules is the class of margin-based rules (also called pairwise rules). A voting rule is margin-based if whenever two elections generate the same head-to-head margins of victory or loss between candidates, the voting rule yields the same outcome in both elections. Although this is a mathematically natural invariance property to consider, whether it should be regarded as a normative axiom on voting rules is less clear. In this paper, we address this question for voting rules with any kind of output, whether a set of candidates, a ranking, a probability distribution, etc. We prove that a voting rule is margin-based if and only if it satisfies some axioms with clearer normative content. A key axiom is what we call Preferential Equality, stating that if two voters both rank a candidate $x$ immediately above a candidate $y$, then either voter switching to rank $y$ immediately above $x$ will have the same effect on the election outcome as if the other voter made the switch, so each voter's preference for $y$ over $x$ is treated equally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08595v3</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifeng Ding, Wesley H. Holliday, Eric Pacuit</dc:creator>
    </item>
    <item>
      <title>Policy Design in Long-Run Welfare Dynamics</title>
      <link>https://arxiv.org/abs/2503.00632</link>
      <description>arXiv:2503.00632v2 Announce Type: replace-cross 
Abstract: Improving social welfare is a complex challenge requiring policymakers to optimize objectives across multiple time horizons. Evaluating the impact of such policies presents a fundamental challenge, as those that appear suboptimal in the short run may yield significant long-term benefits. We tackle this challenge by analyzing the long-term dynamics of two prominent policy frameworks: Rawlsian policies, which prioritize those with the greatest need, and utilitarian policies, which maximize immediate welfare gains. Conventional wisdom suggests these policies are at odds, as Rawlsian policies are assumed to come at the cost of reducing the average social welfare, which their utilitarian counterparts directly optimize. We challenge this assumption by analyzing these policies in a sequential decision-making framework where individuals' welfare levels stochastically decay over time, and policymakers can intervene to prevent this decay. Under reasonable assumptions, we prove that interventions following Rawlsian policies can outperform utilitarian policies in the long run, even when the latter dominate in the short run. We characterize the exact conditions under which Rawlsian policies can outperform utilitarian policies. We further illustrate our theoretical findings using simulations, which highlight the risks of evaluating policies based solely on their short-term effects. Our results underscore the necessity of considering long-term horizons in designing and evaluating welfare policies; the true efficacy of even well-established policies may only emerge over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00632v2</guid>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiduan Wu, Rediet Abebe, Moritz Hardt, Ana-Andreea Stoica</dc:creator>
    </item>
    <item>
      <title>Winning Criteria for Open Games: A Game-Theoretic Approach to Prefix Codes</title>
      <link>https://arxiv.org/abs/2601.17521</link>
      <description>arXiv:2601.17521v2 Announce Type: replace-cross 
Abstract: We study two-player games with alternating moves played on infinite trees. Our main focus is on the case where the trees are full (regular) and the winning set is open (with respect to the product topology on the tree). Gale and Stewart showed that in this setting one of the players always has a winning strategy, though it is not known in advance which player. We present simple necessary conditions for the first player to have a winning strategy, and establish an equivalence between winning sets that guarantee a win for the first player and maximal prefix codes. Using this equivalence, we derive a necessary algebraic condition for winning, and exhibit a family of games for which this algebraic condition is in fact equivalent to winning. We introduce the concept of coverings, and show that by covering the tree of the game with an infinite labeled tree corresponding to the free group, we can use "game-theoretic tools" to derive a simple trait of maximal prefix codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17521v2</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dean Kraizberg</dc:creator>
    </item>
  </channel>
</rss>
