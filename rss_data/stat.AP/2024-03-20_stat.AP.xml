<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Mar 2024 04:00:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bayesian analysis of verbal autopsy data using factor models with age- and sex-dependent associations between symptoms</title>
      <link>https://arxiv.org/abs/2403.12288</link>
      <description>arXiv:2403.12288v1 Announce Type: new 
Abstract: Verbal autopsies (VAs) are extensively used to investigate the population-level distributions of deaths by cause in low-resource settings without well-organized vital statistics systems. Computer-based methods are often adopted to assign causes of death to deceased individuals based on the interview responses of their family members or caregivers. In this article, we develop a new Bayesian approach that extracts information about cause-of-death distributions from VA data considering the age- and sex-related variation in the associations between symptoms. Its performance is compared with that of existing approaches using gold-standard data from the Population Health Metrics Research Consortium. In addition, we compute the relevance of predictors to causes of death based on information-theoretic measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12288v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tsuyoshi Kunihama, Zehang Richard Li, Samuel J. Clark, Tyler H. McCormick</dc:creator>
    </item>
    <item>
      <title>Settlement Mapping for Population Density Modelling in Disease Risk Spatial Analysis</title>
      <link>https://arxiv.org/abs/2403.12858</link>
      <description>arXiv:2403.12858v1 Announce Type: new 
Abstract: In disease risk spatial analysis, many researchers especially in Indonesia are still modelling population density as the ratio of total population to administrative area extent. This model oversimplifies the problem, because it covers large uninhabited areas, while the model should focus on inhabited areas. This study uses settlement mapping against satellite imagery to focus the model and calculate settlement area extent. As far as our search goes, we did not find any specific studies comparing the use of settlement mapping with administrative area to model population density in computing its correlation to a disease case rate. This study investigates the comparison of both models using data on Tuberculosis (TB) case rate in Central and East Java Indonesia. Our study shows that using administrative area density the Spearman's $\rho$ was considered as "Fair" (0.566, p&lt;0.01) and using settlement density was "Moderately Strong" (0.673, p&lt;0.01). The difference is significant according to Hotelling's t test. By this result we are encouraging researchers to use settlement mapping to improve population density modelling in disease risk spatial analysis. Resources used by and resulting from this work are publicly available at https://github.com/mirzaalimm/PopulationDensityVsDisease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12858v1</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IWBIS53353.2021.9631853</arxiv:DOI>
      <dc:creator>Mirza Alim Mutasodirin, Rafi Dwi Rizqullah, Andie Setiyoko, Aniati Murni Arymurthy</dc:creator>
    </item>
    <item>
      <title>Student t-L\'evy regression model in YUIMA</title>
      <link>https://arxiv.org/abs/2403.12078</link>
      <description>arXiv:2403.12078v1 Announce Type: cross 
Abstract: The aim of this paper is to discuss an estimation and a simulation method in the \textsf{R} package YUIMA for a linear regression model driven by a Student-$t$ L\'evy process with constant scale and arbitrary degrees of freedom. This process finds applications in several fields, for example finance, physic, biology, etc. The model presents two main issues. The first is related to the simulation of a sample path at high-frequency level. Indeed, only the $t$-L\'evy increments defined on an unitary time interval are Student-$t$ distributed. In YUIMA, we solve this problem by means of the inverse Fourier transform for simulating the increments of a Student-$t$ L\'{e}vy defined on a interval with any length. A second problem is due to the fact that joint estimation of trend, scale, and degrees of freedom does not seem to have been investigated as yet. In YUIMA, we develop a two-step estimation procedure that efficiently deals with this issue. Numerical examples are given in order to explain methods and classes used in the YUIMA package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12078v1</guid>
      <category>stat.CO</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroki Masuda, Lorenzo Mercuri, Yuma Uehara</dc:creator>
    </item>
    <item>
      <title>Beyond Beats: A Recipe to Song Popularity? A machine learning approach</title>
      <link>https://arxiv.org/abs/2403.12079</link>
      <description>arXiv:2403.12079v1 Announce Type: cross 
Abstract: Music popularity prediction has garnered significant attention in both industry and academia, fuelled by the rise of data-driven algorithms and streaming platforms like Spotify. This study aims to explore the predictive power of various machine learning models in forecasting song popularity using a dataset comprising 30,000 songs spanning different genres from 1957 to 2020. Methods: We employ Ordinary Least Squares (OLS), Multivariate Adaptive Regression Splines (MARS), Random Forest, and XGBoost algorithms to analyse song characteristics and their impact on popularity. Results: Ordinary Least Squares (OLS) regression analysis reveals genre as the primary influencer of popularity, with notable trends over time. MARS modelling highlights the complex relationship between variables, particularly with features like instrumentalness and duration. Random Forest and XGBoost models underscore the importance of genre, especially EDM, in predicting popularity. Despite variations in performance, Random Forest emerges as the most effective model, improving prediction accuracy by 7.1% compared to average scores. Despite the importance of genre, predicting song popularity remains challenging, as observed variations in music-related features suggest complex interactions between genre and other factors. Consequently, while certain characteristics like loudness and song duration may impact popularity scores, accurately predicting song success remains elusive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12079v1</guid>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niklas Sebastian,  Jung, Florian Mayer</dc:creator>
    </item>
    <item>
      <title>Does AI help humans make better decisions? A methodological framework for experimental evaluation</title>
      <link>https://arxiv.org/abs/2403.12108</link>
      <description>arXiv:2403.12108v1 Announce Type: cross 
Abstract: The use of Artificial Intelligence (AI) based on data-driven algorithms has become ubiquitous in today's society. Yet, in many cases and especially when stakes are high, humans still make final decisions. The critical question, therefore, is whether AI helps humans make better decisions as compared to a human alone or AI an alone. We introduce a new methodological framework that can be used to answer experimentally this question with no additional assumptions. We measure a decision maker's ability to make correct decisions using standard classification metrics based on the baseline potential outcome. We consider a single-blinded experimental design, in which the provision of AI-generated recommendations is randomized across cases with a human making final decisions. Under this experimental design, we show how to compare the performance of three alternative decision-making systems--human-alone, human-with-AI, and AI-alone. We apply the proposed methodology to the data from our own randomized controlled trial of a pretrial risk assessment instrument. We find that AI recommendations do not improve the classification accuracy of a judge's decision to impose cash bail. Our analysis also shows that AI-alone decisions generally perform worse than human decisions with or without AI assistance. Finally, AI recommendations tend to impose cash bail on non-white arrestees more often than necessary when compared to white arrestees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12108v1</guid>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eli Ben-Michael, D. James Greiner, Melody Huang, Kosuke Imai, Zhichao Jiang, Sooahn Shin</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: I. Mean</title>
      <link>https://arxiv.org/abs/2403.12110</link>
      <description>arXiv:2403.12110v1 Announce Type: cross 
Abstract: As the most fundamental problem in statistics, robust location estimation has many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges Lehmann estimator, Huber M estimator, and median of means. Recent studies suggest that their maximum biases concerning the mean can be quite different, but the underlying mechanisms largely remain unclear. This study exploited a semiparametric method to classify distributions by the asymptotic orderliness of quantile combinations with varying breakdown points, showing their interrelations and connections to parametric distributions. Further deductions explain why the Winsorized mean typically has smaller biases compared to the trimmed mean; two sequences of semiparametric robust mean estimators emerge, particularly highlighting the superiority of the median Hodges Lehmann mean. This article sheds light on the understanding of the common nature of probability distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12110v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuobang Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization Sequential Surrogate (BOSS) Algorithm: Fast Bayesian Inference for a Broad Class of Bayesian Hierarchical Models</title>
      <link>https://arxiv.org/abs/2403.12250</link>
      <description>arXiv:2403.12250v1 Announce Type: cross 
Abstract: Approximate Bayesian inference based on Laplace approximation and quadrature methods have become increasingly popular for their efficiency at fitting latent Gaussian models (LGM), which encompass popular models such as Bayesian generalized linear models, survival models, and spatio-temporal models. However, many useful models fall under the LGM framework only if some conditioning parameters are fixed, as the design matrix would vary with these parameters otherwise. Such models are termed the conditional LGMs with examples in change-point detection, non-linear regression, etc. Existing methods for fitting conditional LGMs rely on grid search or Markov-chain Monte Carlo (MCMC); both require a large number of evaluations of the unnormalized posterior density of the conditioning parameters. As each evaluation of the density requires fitting a separate LGM, these methods become computationally prohibitive beyond simple scenarios. In this work, we introduce the Bayesian optimization sequential surrogate (BOSS) algorithm, which combines Bayesian optimization with approximate Bayesian inference methods to significantly reduce the computational resources required for fitting conditional LGMs. With orders of magnitude fewer evaluations compared to grid or MCMC methods, Bayesian optimization provides us with sequential design points that capture the majority of the posterior mass of the conditioning parameters, which subsequently yields an accurate surrogate posterior distribution that can be easily normalized. We illustrate the efficiency, accuracy, and practical utility of the proposed method through extensive simulation studies and real-world applications in epidemiology, environmental sciences, and astrophysics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12250v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dayi Li, Ziang Zhang</dc:creator>
    </item>
    <item>
      <title>The Wreaths of KHAN: Uniform Graph Feature Selection with False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2403.12284</link>
      <description>arXiv:2403.12284v1 Announce Type: cross 
Abstract: Graphical models find numerous applications in biology, chemistry, sociology, neuroscience, etc. While substantial progress has been made in graph estimation, it remains largely unexplored how to select significant graph signals with uncertainty assessment, especially those graph features related to topological structures including cycles (i.e., wreaths), cliques, hubs, etc. These features play a vital role in protein substructure analysis, drug molecular design, and brain network connectivity analysis. To fill the gap, we propose a novel inferential framework for general high dimensional graphical models to select graph features with false discovery rate controlled. Our method is based on the maximum of $p$-values from single edges that comprise the topological feature of interest, thus is able to detect weak signals. Moreover, we introduce the $K$-dimensional persistent Homology Adaptive selectioN (KHAN) algorithm to select all the homological features within $K$ dimensions with the uniform control of the false discovery rate over continuous filtration levels. The KHAN method applies a novel discrete Gram-Schmidt algorithm to select statistically significant generators from the homology group. We apply the structural screening method to identify the important residues of the SARS-CoV-2 spike protein during the binding process to the ACE2 receptors. We score the residues for all domains in the spike protein by the $p$-value weighted filtration level in the network persistent homology for the closed, partially open, and open states and identify the residues crucial for protein conformational changes and thus being potential targets for inhibition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12284v1</guid>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiajun Liang, Yue Liu, Doudou Zhou, Sinian Zhang, Junwei Lu</dc:creator>
    </item>
    <item>
      <title>Large-scale metric objects filtering for binary classification with application to abnormal brain connectivity detection</title>
      <link>https://arxiv.org/abs/2403.12624</link>
      <description>arXiv:2403.12624v1 Announce Type: cross 
Abstract: The classification of random objects within metric spaces without a vector structure has attracted increasing attention. However, the complexity inherent in such non-Euclidean data often restricts existing models to handle only a limited number of features, leaving a gap in real-world applications. To address this, we propose a data-adaptive filtering procedure to identify informative features from large-scale random objects, leveraging a novel Kolmogorov-Smirnov-type statistic defined on the metric space. Our method, applicable to data in general metric spaces with binary labels, exhibits remarkable flexibility. It enjoys a model-free property, as its implementation does not rely on any specified classifier. Theoretically, it controls the false discovery rate while guaranteeing the sure screening property. Empirically, equipped with a Wasserstein metric, it demonstrates superior sample performance compared to Euclidean competitors. When applied to analyze a dataset on autism, our method identifies significant brain regions associated with the condition. Moreover, it reveals distinct interaction patterns among these regions between individuals with and without autism, achieved by filtering hundreds of thousands of covariance matrices representing various brain connectivities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12624v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuaida He, Jiaqi Li, Xin Chen</dc:creator>
    </item>
    <item>
      <title>Tests for categorical data beyond Pearson: A distance covariance and energy distance approach</title>
      <link>https://arxiv.org/abs/2403.12711</link>
      <description>arXiv:2403.12711v1 Announce Type: cross 
Abstract: Categorical variables are of uttermost importance in biomedical research. When two of them are considered, it is often the case that one wants to test whether or not they are statistically dependent. We show weaknesses of classical methods -- such as Pearson's and the G-test -- and we propose testing strategies based on distances that lack those drawbacks. We first develop this theory for classical two-dimensional contingency tables, within the context of distance covariance, an association measure that characterises general statistical independence of two variables. We then apply the same fundamental ideas to one-dimensional tables, namely to the testing for goodness of fit to a discrete distribution, for which we resort to an analogous statistic called energy distance. We prove that our methodology has desirable theoretical properties, and we show how we can calibrate the null distribution of our test statistics without resorting to any resampling technique. We illustrate all this in simulations, as well as with some real data examples, demonstrating the adequate performance of our approach for biostatistical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12711v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fernando Castro-Prado, Wenceslao Gonz\'alez-Manteiga, Javier Costas, Fernando Facal, Dominic Edelmann</dc:creator>
    </item>
    <item>
      <title>Bayesian uncertainty evaluation applied to the tilted-wave interferometer</title>
      <link>https://arxiv.org/abs/2403.12715</link>
      <description>arXiv:2403.12715v1 Announce Type: cross 
Abstract: The tilted-wave interferometer is a promising technique for the development of a reference measurement system for the highly accurate form measurement of aspheres and freeform surfaces. The technique combines interferometric measurements, acquired with a special setup, and sophisticated mathematical evaluation procedures. To determine the form of the surface under test, a computational model is required that closely mimics the measurement process of the physical measurement instruments. The parameters of the computational model, comprising the surface under test sought, are then tuned by solving an inverse problem. Due to this embedded structure of the real experiment and computational model and the overall complexity, a thorough uncertainty evaluation is challenging. In this work, a Bayesian approach is proposed to tackle the inverse problem, based on a statistical model derived from the computational model of the tilted-wave interferometer. Such a procedure naturally allows for uncertainty quantification to be made. We present an approximate inference scheme to efficiently sample quantities of the posterior using Monte Carlo sampling involving the statistical model. In particular, the methodology derived is applied to the tilted-wave interferometer to obtain an estimate and corresponding uncertainty of the pixel-by-pixel form of the surface under test for two typical surfaces taking into account a number of key influencing factors. A statistical analysis using experimental design is employed to identify main influencing factors and a subsequent analysis confirms the efficacy of the method derived.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12715v1</guid>
      <category>physics.optics</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manuel Marschall, Ines Fortmeier, Manuel Stavridis, Finn Hughes, Clemens Elster</dc:creator>
    </item>
    <item>
      <title>Predicting COVID-19 hospitalisation using a mixture of Bayesian predictive syntheses</title>
      <link>https://arxiv.org/abs/2308.06134</link>
      <description>arXiv:2308.06134v2 Announce Type: replace 
Abstract: This paper proposes a novel methodology called the mixture of Bayesian predictive syntheses (MBPS) for multiple time series count data for the challenging task of predicting the numbers of COVID-19 inpatients and isolated cases in Japan and Korea at the subnational-level. MBPS combines a set of predictive models and partitions the multiple time series into clusters based on their contribution to predicting the outcome. In this way, MBPS leverages the shared information within each cluster and is suitable for predicting COVID-19 inpatients since the data exhibit similar dynamics over multiple areas. Also, MBPS avoids using a multivariate count model, which is generally cumbersome to develop and implement. Our Japanese and Korean data analyses demonstrate that the proposed MBPS methodology has improved predictive accuracy and uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.06134v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Genya Kobayashi, Shonosuke Sugasawa, Yuki Kawakubo, Dongu Han, Taeryon Choi</dc:creator>
    </item>
    <item>
      <title>Copula Conformal Prediction for Multi-step Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2212.03281</link>
      <description>arXiv:2212.03281v4 Announce Type: replace-cross 
Abstract: Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper, we propose a Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has finite sample validity guarantee. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.03281v4</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICLR 2024</arxiv:journal_reference>
      <dc:creator>Sophia Sun, Rose Yu</dc:creator>
    </item>
    <item>
      <title>Early-Phase Local-Area Model for Pandemics Using Limited Data: A SARS-CoV-2 Application</title>
      <link>https://arxiv.org/abs/2212.08282</link>
      <description>arXiv:2212.08282v2 Announce Type: replace-cross 
Abstract: The emergence of novel infectious agents presents challenges to statistical models of disease transmission. These challenges arise from limited, poor-quality data and an incomplete understanding of the agent. Moreover, outbreaks manifest differently across regions due to various factors, making it imperative for models to factor in regional specifics. In this work, we offer a model that effectively utilizes constrained data resources to estimate disease transmission rates at the local level, especially during the early outbreak phase when primarily infection counts and aggregated local characteristics are accessible. This model merges a pathogen transmission methodology based on daily infection numbers with regression techniques, drawing correlations between disease transmission and local-area factors, such as demographics, health policies, behavior, and even climate, to estimate and forecast daily infections. We incorporate the quasi-score method and an error term to navigate potential data concerns and mistaken assumptions. Additionally, we introduce an online estimator that facilitates real-time data updates, complemented by an iterative algorithm for parameter estimation. This approach facilitates real-time analysis of disease transmission when data quality is suboptimal and knowledge of the infectious pathogen is limited. It is particularly useful in the early stages of outbreaks, providing support for local decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.08282v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiasheng Shi, Jeffrey S. Morris, David M. Rubin, Jing Huang</dc:creator>
    </item>
    <item>
      <title>Democratizing Uncertainty Quantification</title>
      <link>https://arxiv.org/abs/2402.13768</link>
      <description>arXiv:2402.13768v3 Announce Type: replace-cross 
Abstract: Uncertainty Quantification (UQ) is vital to safety-critical model-based analyses, but the widespread adoption of sophisticated UQ methods is limited by technical complexity. In this paper, we introduce UM-Bridge (the UQ and Modeling Bridge), a high-level abstraction and software protocol that facilitates universal interoperability of UQ software with simulation codes. It breaks down the technical complexity of advanced UQ applications and enables separation of concerns between experts. UM-Bridge democratizes UQ by allowing effective interdisciplinary collaboration, accelerating the development of advanced UQ methods, and making it easy to perform UQ analyses from prototype to High Performance Computing (HPC) scale.
  In addition, we present a library of ready-to-run UQ benchmark problems, all easily accessible through UM-Bridge. These benchmarks support UQ methodology research, enabling reproducible performance comparisons. We demonstrate UM-Bridge with several scientific applications, harnessing HPC resources even using UQ codes not designed with HPC support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13768v3</guid>
      <category>cs.MS</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Linus Seelinger, Anne Reinarz, Mikkel B. Lykkegaard, Amal M. A. Alghamdi, David Aristoff, Wolfgang Bangerth, Jean B\'en\'ezech, Matteo Diez, Kurt Frey, John D. Jakeman, Jakob S. J{\o}rgensen, Ki-Tae Kim, Massimiliano Martinelli, Matthew Parno, Riccardo Pellegrini, Noemi Petra, Nicolai A. B. Riis, Katherine Rosenfeld, Andrea Serani, Lorenzo Tamellini, Umberto Villa, Tim J. Dodwell, Robert Scheichl</dc:creator>
    </item>
    <item>
      <title>Classical and Bayesian statistical methods for low-level metrology</title>
      <link>https://arxiv.org/abs/2403.00385</link>
      <description>arXiv:2403.00385v3 Announce Type: replace-cross 
Abstract: This document presents the statistical methods used to process low-level measurements in the presence of noise. These methods can be classical or Bayesian. The question is placed in the general framework of the problem of nuisance parameters, one of the canonical problems of statistical inference. By using a simple criterion proposed by Bolstad (2007), it is possible to define statistically significant results during a measurement process (act of measuring in the vocabulary of metrology). This result is similar for a classic paradigm (called ``frequentist'') or Bayesian: the presence of zero in the interval considered (confidence or credibility). It is shown that in the case of homoskedastic Gaussians, the commonly used results are found. The case of Poisson distributions is then considered. In the case of heteroscedastic Gaussians, which is that of radioactivity measurement, we can consider them as Poisson laws in the limit of large counts. The results are different from those commonly used, and in particular those from standards (ISO 11929). Their statistical performances, characterized by simulation, are better and are well verified experimentally. This is confirmed theoretically by the use of the Neyman-Pearson lemma which makes it possible to formally determine the statistical tests with the best performances. These results also make it possible to understand the paradox of the possible divergence of the detection limit. It is also formally shown that the confidence intervals thus calculated by getting rid of the nuisance parameter according to established methods result in the commonly used confidence interval. To our knowledge, this constitutes the first formal derivation of these confidence intervals.This method is based on keeping the measurement results whether they are significant or not (not censoring them). This is recommended in several standards or documents, is compatible with the ISO 11929 standard and is in line with recent proposals in the field of statistics. On the other hand, all the information necessary to determine whether a measurement result is significant or not remains available. The conservation and restitution of all results is currently applied in the USA. The textbook case of the WIPP incident makes it possible to ensure favorable public perception.The implications and applications of this method in different fields are finally discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00385v3</guid>
      <category>physics.ins-det</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Manificat (IRSN), Salima Helali (IRSN), Patrick Bouisset (IRSN)</dc:creator>
    </item>
    <item>
      <title>Bayesian multi-exposure image fusion for robust high dynamic range ptychography</title>
      <link>https://arxiv.org/abs/2403.11344</link>
      <description>arXiv:2403.11344v2 Announce Type: replace-cross 
Abstract: Image information is restricted by the dynamic range of the detector, which can be addressed using multi-exposure image fusion (MEF). The conventional MEF approach employed in ptychography is often inadequate under conditions of low signal-to-noise ratio (SNR) or variations in illumination intensity. To address this, we developed a Bayesian approach for MEF based on a modified Poisson noise model that considers the background and saturation. Our method outperforms conventional MEF under challenging experimental conditions, as demonstrated by the synthetic and experimental data. Furthermore, this method is versatile and applicable to any imaging scheme requiring high dynamic range (HDR).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11344v2</guid>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shantanu Kodgirwar, Lars Loetgering, Chang Liu, Aleena Joseph, Leona Licht, Daniel S. Penagos Molina, Wilhelm Eschen, Jan Rothhardt, Michael Habeck</dc:creator>
    </item>
  </channel>
</rss>
