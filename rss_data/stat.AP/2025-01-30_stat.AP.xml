<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Jan 2025 05:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Constructing Simultaneous Confidence Bands for Errors-in-variables Curves with Application to the Lorenz Curve</title>
      <link>https://arxiv.org/abs/2501.17264</link>
      <description>arXiv:2501.17264v1 Announce Type: new 
Abstract: Errors-in-variables curves are curves where errors exist not only in the independent variable but also in the dependent variable. We address the challenge of constructing simultaneous confidence bands (SCBs) for such curves. Our method finds application in the Lorenz curve, which represents the concentration of income or wealth. Unlike ordinary regression curves, the Lorenz curve incorporates errors in its explanatory variable and requires a fundamentally different treatment. To the best of our knowledge, the development of SCBs for such curves has not been explored in previous research. Using the Lorenz curve as a case study, this paper proposes a novel approach to address this challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17264v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziqing Dong, Francesco Bartolucci, Satoshi Kuriki, Antonietta Mira</dc:creator>
    </item>
    <item>
      <title>Modelling a storage system of a wind farm with a ramp-rate limitation: a semi-Markov modulated Brownian bridge approach</title>
      <link>https://arxiv.org/abs/2501.17464</link>
      <description>arXiv:2501.17464v1 Announce Type: new 
Abstract: We propose a new methodology to simulate the discounted penalty applied to a wind-farm operator by violating ramp-rate limitation policies. It is assumed that the operator manages a wind turbine plugged into a battery, which either provides or stores energy on demand to avoid ramp-up and ramp-down events. The battery stages, namely charging, discharging, or neutral, are modeled as a semi-Markov process. During each charging/discharging period, the energy stored/supplied is assumed to follow a modified Brownian bridge that depends on three parameters. We prove the validity of our methodology by testing the model on 10 years of real wind-power data and comparing real versus simulated results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17464v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10479-024-06236-6</arxiv:DOI>
      <arxiv:journal_reference>Ann Oper Res 345, 39-57 (2025)</arxiv:journal_reference>
      <dc:creator>Abel Azze, Guglielmo D'Amico, Bernardo D'Auria, Salvatore Vergine</dc:creator>
    </item>
    <item>
      <title>Improving LLM Leaderboards with Psychometrical Methodology</title>
      <link>https://arxiv.org/abs/2501.17200</link>
      <description>arXiv:2501.17200v1 Announce Type: cross 
Abstract: The rapid development of large language models (LLMs) has necessitated the creation of benchmarks to evaluate their performance. These benchmarks resemble human tests and surveys, as they consist of sets of questions designed to measure emergent properties in the cognitive behavior of these systems. However, unlike the well-defined traits and abilities studied in social sciences, the properties measured by these benchmarks are often vaguer and less rigorously defined. The most prominent benchmarks are often grouped into leaderboards for convenience, aggregating performance metrics and enabling comparisons between models. Unfortunately, these leaderboards typically rely on simplistic aggregation methods, such as taking the average score across benchmarks. In this paper, we demonstrate the advantages of applying contemporary psychometric methodologies - originally developed for human tests and surveys - to improve the ranking of large language models on leaderboards. Using data from the Hugging Face Leaderboard as an example, we compare the results of the conventional naive ranking approach with a psychometrically informed ranking. The findings highlight the benefits of adopting psychometric techniques for more robust and meaningful evaluation of LLM performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17200v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denis Federiakin</dc:creator>
    </item>
    <item>
      <title>Dilemmas and trade-offs in the diffusion of conventions</title>
      <link>https://arxiv.org/abs/2501.17300</link>
      <description>arXiv:2501.17300v1 Announce Type: cross 
Abstract: Outside ideal settings, conventions are shaped by heterogeneous competing processes that can challenge the emergence of universal norms. This paper identifies three trade-offs challenging the diffusion of conventions and explores each of them empirically using observational behavioral data. The first trade-off (I) concerns the imperatives of social, sequential, and contextual consistency that individuals must balance when choosing between competing conventions. The second trade-off (II) involves the balance between local and global coordination, depending on whether individuals coordinate their behavior via interactions throughout a social network or external factors transcending the network. The third trade-off (III) is the balance between decision optimality (e.g., collective satisfaction) and decision costs when collectives with conflicting preferences choose one convention. We develop a utilitarian account of conventions which we translate into a broadly applicable statistical physics framework for measuring each of these trade-offs. We then apply this framework to a sign convention in physics using textual and network data. Our analysis suggests that the purpose of conventions may exceed coordination, and that multiple infrastructures (including prior cultural traits and social networks) concurrently shape individual preferences towards conventions. Additionally, we confirm the role of seniority in resolving conflicting preferences in collaborations, resulting in suboptimal outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17300v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Gautheron</dc:creator>
    </item>
    <item>
      <title>Gradient-free Importance Sampling Scheme for Efficient Reliability Estimation</title>
      <link>https://arxiv.org/abs/2501.17401</link>
      <description>arXiv:2501.17401v1 Announce Type: cross 
Abstract: This work presents a novel gradient-free importance sampling-based framework for precisely and efficiently estimating rare event probabilities, often encountered in reliability analyses of engineering systems. The approach is formulated around our foundational Approximate Sampling Target with Post-processing Adjustment (ASTPA) methodology. ASTPA uniquely constructs and directly samples an unnormalized target distribution, relaxing the optimal importance sampling distribution (ISD). The target's normalizing constant is then estimated using our inverse importance sampling (IIS) scheme, employing an ISD fitted based on the obtained samples. In this work, a gradient-free sampling method within ASTPA is developed through a guided dimension-robust preconditioned Crank-Nicolson (pCN) algorithm, particularly suitable for black-box computational models where analytical gradient information is not available. To boost the sampling efficiency of pCN in our context, a computationally effective, general discovery stage for the rare event domain is devised, providing (multi-modal) rare event samples used in initializing the pCN chains. A series of diverse test functions and engineering problems involving high dimensionality and strong nonlinearity is presented, demonstrating the advantages of the proposed framework compared to several state-of-the-art sampling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17401v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elsayed Eshra, Konstantinos G. Papakonstantinou</dc:creator>
    </item>
    <item>
      <title>SIGN: A Statistically-Informed Gaze Network for Gaze Time Prediction</title>
      <link>https://arxiv.org/abs/2501.17422</link>
      <description>arXiv:2501.17422v1 Announce Type: cross 
Abstract: We propose a first version of SIGN, a Statistically-Informed Gaze Network, to predict aggregate gaze times on images. We develop a foundational statistical model for which we derive a deep learning implementation involving CNNs and Visual Transformers, which enables the prediction of overall gaze times. The model enables us to derive from the aggregate gaze times the underlying gaze pattern as a probability map over all regions in the image, where each region's probability represents the likelihood of being gazed at across all possible scan-paths. We test SIGN's performance on AdGaze3500, a dataset of images of ads with aggregate gaze times, and on COCO-Search18, a dataset with individual-level fixation patterns collected during search. We demonstrate that SIGN (1) improves gaze duration prediction significantly over state-of-the-art deep learning benchmarks on both datasets, and (2) can deliver plausible gaze patterns that correspond to empirical fixation patterns in COCO-Search18. These results suggest that the first version of SIGN holds promise for gaze-time predictions and deserves further development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17422v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianping Ye, Michel Wedel</dc:creator>
    </item>
    <item>
      <title>nabqr: Python package for improving probabilistic forecasts</title>
      <link>https://arxiv.org/abs/2501.17604</link>
      <description>arXiv:2501.17604v1 Announce Type: cross 
Abstract: We introduce the open-source Python package NABQR: Neural Adaptive Basis for (time-adaptive) Quantile Regression that provides reliable probabilistic forecasts. NABQR corrects ensembles (scenarios) with LSTM networks and then applies time-adaptive quantile regression to the corrected ensembles to obtain improved and more reliable forecasts. With the suggested package, accuracy improvements of up to 40% in mean absolute terms can be achieved in day-ahead forecasting of onshore and offshore wind power production in Denmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17604v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bastian Schmidt J{\o}rgensena, Jan Kloppenborg M{\o}ller, Peter Nystrup, Henrik Madsen</dc:creator>
    </item>
    <item>
      <title>A Bayesian Integrative Mixed Modeling Framework for Analysis of the Adolescent Brain and Cognitive Development Study</title>
      <link>https://arxiv.org/abs/2501.17705</link>
      <description>arXiv:2501.17705v1 Announce Type: cross 
Abstract: Integrating high-dimensional, heterogeneous data from multi-site cohort studies with complex hierarchical structures poses significant feature selection and prediction challenges. We extend the Bayesian Integrative Analysis and Prediction (BIP) framework to enable simultaneous feature selection and outcome modeling in data of nested hierarchical structure. We apply the proposed Bayesian Integrative Mixed Modeling (BIPmixed) framework to the Adolescent Brain Cognitive Development (ABCD) Study, leveraging multi-view data, including structural and functional MRI and early life adversity (ELA) metrics, to identify relevant features and predict the behavioral outcome. BIPmixed incorporates 2-level nested random effects, to enhance interpretability and make predictions in hierarchical data settings. Simulation studies illustrate BIPmixed's robustness in distinct random effect settings, highlighting its use for complex study designs. Our findings suggest that BIPmixed effectively integrates multi-view data while accounting for nested sampling, making it a valuable tool for analyzing large-scale studies with hierarchical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17705v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aidan Neher, Apostolos Stamenos, Mark Fiecas, Sandra Safo, Thierry Chekouo</dc:creator>
    </item>
    <item>
      <title>An Estimator-Robust Design for Augmenting Randomized Controlled Trial with External Real-World Data</title>
      <link>https://arxiv.org/abs/2501.17835</link>
      <description>arXiv:2501.17835v1 Announce Type: cross 
Abstract: Augmenting randomized controlled trials (RCTs) with external real-world data (RWD) has the potential to improve the finite sample efficiency of treatment effect estimators. We describe using adaptive targeted maximum likelihood estimation (A-TMLE) for estimating the average treatment effect (ATE) by decomposing the ATE estimand into two components: a pooled-ATE estimand that combines data from both the RCT and external sources, and a bias estimand that captures the conditional effect of RCT enrollment on the outcome. This approach views the RCT data as the reference and corrects for inconsistencies of any kind between the RCT and the external data source. Given the growing abundance of external RWD from modern electronic health records, determining the optimal strategy to select candidate external patients for data integration remains an open yet critical problem. In this work, we begin by analyzing the robustness property of the A-TMLE estimator and then propose a matching-based sampling strategy that improves the robustness of the estimator with respect to the target estimand. Our proposed strategy is outcome-blind and involves matching based on two one-dimensional scores: the trial enrollment score and the propensity score in the external data. We demonstrate in simulations that our sampling strategy improves the coverage and shortens the widths of confidence intervals produced by A-TMLE. We illustrate our method with a case study of augmenting the DEVOTE cardiovascular safety trial by using the Optum Clinformatics claims database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17835v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sky Qiu, Jens Tarp, Andrew Mertens, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>WATCH: A Workflow to Assess Treatment Effect Heterogeneity in Drug Development for Clinical Trial Sponsors</title>
      <link>https://arxiv.org/abs/2405.00859</link>
      <description>arXiv:2405.00859v2 Announce Type: replace 
Abstract: This paper proposes a Workflow for Assessing Treatment effeCt Heterogeneity (WATCH) in clinical drug development targeted at clinical trial sponsors. WATCH is designed to address the challenges of investigating treatment effect heterogeneity (TEH) in randomized clinical trials, where sample size and multiplicity limit the reliability of findings. The proposed workflow includes four steps: Analysis Planning, Initial Data Analysis and Analysis Dataset Creation, TEH Exploration, and Multidisciplinary Assessment. The workflow offers a general overview of how treatment effects vary by baseline covariates in the observed data, and guides interpretation of the observed findings based on external evidence and best scientific understanding. The workflow is exploratory and not inferential/confirmatory in nature, but should be pre-planned before data-base lock and analysis start. It is focused on providing a general overview rather than a single specific finding or subgroup with differential effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00859v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/pst.2463</arxiv:DOI>
      <arxiv:journal_reference>Pharmaceutical Statistics 2025</arxiv:journal_reference>
      <dc:creator>Konstantinos Sechidis, Sophie Sun, Yao Chen, Jiarui Lu, Cong Zhang, Mark Baillie, David Ohlssen, Marc Vandemeulebroecke, Rob Hemmings, Stephen Ruberg, Bj\"orn Bornkamp</dc:creator>
    </item>
    <item>
      <title>Hazard and Beyond: Exploring Five Distributional Representations of Accelerometry Data for Disability Discrimination in Multiple Sclerosis</title>
      <link>https://arxiv.org/abs/2410.20620</link>
      <description>arXiv:2410.20620v2 Announce Type: replace 
Abstract: Research on modeling the distributional aspects in sensor-based digital health (sDHT) data has grown significantly in recent years. Most existing approaches focus on using individual-specific density or quantile functions. However, there has been limited exploration to assess the practical utility of alternative distributional representations in clinical contexts collecting sDHT data. This study is motivated by accelerometry data collected on 246 individuals with multiple sclerosis (MS)representing a wide range of disability (Expanded Disability Status Scale, EDSS: 0-7). We consider five different individual-level distributional representations of minute-level activity counts: density, survival, hazard, quantile, and total time on test functions. For each of the five distributional representations, scalar-on-function regression fits linear discriminators for binary and continuously measured MS disability, and cross-validated discriminatory performance of these linear discriminators is compared across. The results show that individual-level hazard functions provide the highest discriminatory accuracy, more than double the accuracy compared to density functions. Individual-level quantile functions provided the second-highest discriminatory accuracy. These findings highlight the importance of focusing on distributional representations that capture the tail behavior of distributions when analyzing digital health data, especially in clinical contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20620v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratim Guha Niyogi, Muraleetharan Sanjayan, Dmitri Volfson, Kathryn C. Fitzgerald, Ellen M. Mowry, Vadim Zipunnikov</dc:creator>
    </item>
    <item>
      <title>Controlling FDR in selecting group-level simultaneous signals from multiple data sources with application to the National Covid Collaborative Cohort data</title>
      <link>https://arxiv.org/abs/2303.01599</link>
      <description>arXiv:2303.01599v2 Announce Type: replace-cross 
Abstract: One challenge in exploratory association studies using observational data is that the associations between the predictors and the outcome are potentially weak and rare, and the candidate predictors have complex correlation structures. False discovery rate (FDR) controlling procedures can provide important statistical guarantees for replicability in predictor identification in exploratory research. In the recently established National COVID Collaborative Cohort (N3C), electronic health record (EHR) data on the same set of candidate predictors are independently collected in multiple different sites, offering opportunities to identify true associations by combining information from different sources. This paper presents a general knockoff-based variable selection algorithm to identify associations from unions of group-level conditional independence tests (simultaneous signals) with exact FDR control guarantees under finite sample settings. This algorithm can work with general regression settings, allowing heterogeneity of both the predictors and the outcomes across multiple data sources. We demonstrate the performance of this method with extensive numerical studies and an application to the N3C data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.01599v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Runqiu Wang (on behalf of N3C consortium), Ran Dai (on behalf of N3C consortium), Hongying Dai (on behalf of N3C consortium), Evan French (on behalf of N3C consortium), Cheng Zheng (on behalf of N3C consortium)</dc:creator>
    </item>
    <item>
      <title>Bayesian Variable Selection in Distributed Lag Models: A Focus on Binary Quantile and Count Data Regressions</title>
      <link>https://arxiv.org/abs/2403.03646</link>
      <description>arXiv:2403.03646v2 Announce Type: replace-cross 
Abstract: Distributed Lag Models (DLMs) and similar regression approaches such as MIDAS have been used for many decades in econometrics and more recently to investigate how poor air quality adversely affects human health. In this paper we describe how to expand the utility of these models for Bayesian inference by leveraging latent variables. In particular we explain how to perform binary regression to better handle imbalanced data, how to incorporate negative binomial regression, and how to estimate the probability of predictor inclusion. Extra parameters introduced through the DLM framework may require calibration for the MCMC algorithm, but this will not be the case in DLM-based analyses often seen in pollution exposure literature. In these cases, the parameters are inferred through a fully automatic Gibbs sampling procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03646v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Dempsey, Jason Wyse</dc:creator>
    </item>
    <item>
      <title>Breaking New Ground, Reinforcing Old Gaps: Gender Disparities in Access to Emerging Research Frontiers</title>
      <link>https://arxiv.org/abs/2404.04707</link>
      <description>arXiv:2404.04707v3 Announce Type: replace-cross 
Abstract: This study exploits COVID-19 as an exogenous shock in biomedical research to show how the emergence of an unexpected new research topic exacerbates gender bias in key authorship positions of scientific publications relevant to new research topics (e.g. Vaccines, Epidemiology). We determine author's gender based on the names listed on their scientific publications and analyze the changes in the composition of the scientific teams after the COVID-19 outbreak. Using a Difference-in-Differences approach, we find that although the share of female authorship has increased overall, women are less likely to be first or last authors (the most prestigious positions) on COVID-19-related research papers and more likely to be found in middle author positions. Stay-at-home mandates, the journal importance and funding opportunities do not fully account for the decline of women in key author positions. The main difference in first authorship is due to the composition of the team and the experience of the lead authors in COVID-19 related research. First authorship by women declined after teams of novices emerged, where lead authors have no prior experience in COVID-related research. Discretionality in first-author appointments for newcomers, combined with high pressure to publish quickly, may have led to discriminatory biases. Conversely, there may also be differences in risk-taking attitudes in doing research in unfamiliar domains. Monitoring gender inequality in scientific production is crucial for reducing gender inequalities and for implementing timely policies that ensure equal access to emerging research topics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04707v3</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carolina Biliotti, Luca Verginer, Massimo Riccaboni</dc:creator>
    </item>
    <item>
      <title>The 2020 United States Decennial Census Is More Private Than You (Might) Think</title>
      <link>https://arxiv.org/abs/2410.09296</link>
      <description>arXiv:2410.09296v2 Announce Type: replace-cross 
Abstract: The U.S. Decennial Census serves as the foundation for many high-profile policy decision-making processes, including federal funding allocation and redistricting. In 2020, the Census Bureau adopted differential privacy to protect the confidentiality of individual responses through a disclosure avoidance system that injects noise into census data tabulations. The Bureau subsequently posed an open question: Could stronger privacy guarantees be obtained for the 2020 U.S. Census compared to their published guarantees, or equivalently, had the privacy budgets been fully utilized?
  In this paper, we address this question affirmatively by demonstrating that the 2020 U.S. Census provides significantly stronger privacy protections than its nominal guarantees suggest at each of the eight geographical levels, from the national level down to the block level. This finding is enabled by our precise tracking of privacy losses using $f$-differential privacy, applied to the composition of private queries across these geographical levels. Our analysis reveals that the Census Bureau introduced unnecessarily high levels of noise to meet the specified privacy guarantees for the 2020 Census. Consequently, we show that noise variances could be reduced by $15.08\%$ to $24.82\%$ while maintaining nearly the same level of privacy protection for each geographical level, thereby improving the accuracy of privatized census statistics. We empirically demonstrate that reducing noise injection into census statistics mitigates distortion caused by privacy constraints in downstream applications of private census data, illustrated through a study examining the relationship between earnings and education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09296v2</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Buxin Su, Weijie J. Su, Chendi Wang</dc:creator>
    </item>
  </channel>
</rss>
