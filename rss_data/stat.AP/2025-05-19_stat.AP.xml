<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 May 2025 04:00:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Framing Causal Questions in Sports Analytics: A Case Study of Crossing in Soccer</title>
      <link>https://arxiv.org/abs/2505.11841</link>
      <description>arXiv:2505.11841v1 Announce Type: new 
Abstract: Causal inference has become an accepted analytic framework in settings where experimentation is impossible, which is frequently the case in sports analytics, particularly for studying in-game tactics. However, subtle differences in implementation can lead to important differences in interpretation. In this work, we provide a case study to demonstrate the utility and the nuance of these approaches. Motivated by a case study of crossing in soccer, two causal questions are considered: the overall impact of crossing on shot creation (Average Treatment Effect, ATE) and its impact in plays where crossing was actually attempted (Average Treatment Effect on the Treated, ATT). Using data from Shandong Taishan Luneng Football Club's 2017 season, we demonstrate how distinct matching strategies are used for different estimation targets - the ATE and ATT - though both aim to eliminate any spurious relationship between crossing and shot creation. Results suggest crossing yields a 1.6% additive increase in shot probability overall compared to not crossing (ATE), whereas the ATT is 5.0%. We discuss what insights can be gained from each estimand, and provide examples where one may be preferred over the alternative. Understanding and clearly framing analytics questions through a causal lens ensure rigorous analyses of complex questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11841v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shomoita Alam, Erica E. M. Moodie, Lucas Y. Wu, Tim B. Swartz</dc:creator>
    </item>
    <item>
      <title>Integrative Analysis and Imputation of Multiple Data Streams via Deep Gaussian Processes</title>
      <link>https://arxiv.org/abs/2505.12076</link>
      <description>arXiv:2505.12076v1 Announce Type: new 
Abstract: Healthcare data, particularly in critical care settings, presents three key challenges for analysis. First, physiological measurements come from different sources but are inherently related. Yet, traditional methods often treat each measurement type independently, losing valuable information about their relationships. Second, clinical measurements are collected at irregular intervals, and these sampling times can carry clinical meaning. Finally, the prevalence of missing values. Whilst several imputation methods exist to tackle this common problem, they often fail to address the temporal nature of the data or provide estimates of uncertainty in their predictions. We propose using deep Gaussian process emulation with stochastic imputation, a methodology initially conceived to deal with computationally expensive models and uncertainty quantification, to solve the problem of handling missing values that naturally occur in critical care data. This method leverages longitudinal and cross-sectional information and provides uncertainty estimation for the imputed values. Our evaluation of a clinical dataset shows that the proposed method performs better than conventional methods, such as multiple imputations with chained equations (MICE), last-known value imputation, and individually fitted Gaussian Processes (GPs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12076v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Akbar Septiandri, Deyu Ming, F. Alejandro DiazDelaO, Takoua Jendoubi, Samiran Ray</dc:creator>
    </item>
    <item>
      <title>SIMBA -- A Bayesian Decision Framework for the Identification of Optimal Biomarker Subgroups for Cancer Basket Clinical Trials</title>
      <link>https://arxiv.org/abs/2505.13202</link>
      <description>arXiv:2505.13202v1 Announce Type: new 
Abstract: We consider basket trials in which a biomarker-targeting drug may be efficacious for patients across different disease indications. Patients are enrolled if their cells exhibit some levels of biomarker expression. The threshold level is allowed to vary by indication. The proposed SIMBA method uses a decision framework to identify optimal biomarker subgroups (OBS) defined by an optimal biomarker threshold for each indication. The optimality is achieved through minimizing a posterior expected loss that balances estimation accuracy and investigator preference for broadly effective therapeutics. A Bayesian hierarchical model is proposed to adaptively borrow information across indications and enhance the accuracy in the estimation of the OBS. The operating characteristics of SIMBA are assessed via simulations and compared against a simplified version and an existing alternative method, both of which do not borrow information. SIMBA is expected to improve the identification of patient sub-populations that may benefit from a biomarker-driven therapeutics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13202v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Yuan, Jiaxin Liu, Zhihua Gong, Xia Qin, Crystal Qin, Yuan Ji, Peter M\"uller</dc:creator>
    </item>
    <item>
      <title>Modeling Innovation Ecosystem Dynamics through Interacting Reinforced Bernoulli Processes</title>
      <link>https://arxiv.org/abs/2505.13364</link>
      <description>arXiv:2505.13364v1 Announce Type: new 
Abstract: Understanding how capabilities evolve into core capabilities-and how core capabilities may ossify into rigidities-is central to innovation strategy [https://www.jstor.org/stable/2486355, https://www.barnesandnoble.com/w/dynamic-capabilities-and-strategic-management-david-j-teece/1102436798].
  To address this, we propose a novel formal model based on interacting reinforced Bernoulli processes. This framework captures how patent successes propagate across technological categories and how these categories co-evolve. The model is able to jointly account for several stylized facts in the empirical innovation literature, including sublinear success growth (success-probability decay), convergence of success shares across fields, and diminishing cross-category correlations over time.
  Empirical validation using GLOBAL PATSTAT (1980-2018) supports the theoretical predictions. We estimate the structural parameters of the interaction matrix and we also propose a statistical procedure to make inference on the intensity of cross-category interactions under the mean-field assumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13364v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giacomo Aletti, Irene Crimaldi, Andrea Ghiglietti, Federico Nutarelli</dc:creator>
    </item>
    <item>
      <title>Predicting temperatures in Brazilian states capitals via Machine Learning</title>
      <link>https://arxiv.org/abs/2505.11511</link>
      <description>arXiv:2505.11511v1 Announce Type: cross 
Abstract: Climate change refers to substantial long-term variations in weather patterns. In this work, we employ a Machine Learning (ML) technique, the Random Forest (RF) algorithm, to forecast the monthly average temperature for Brazilian's states capitals (27 cities) and the whole country, from January 1961 until December 2022. To forecast the temperature at $k$-month, we consider as features in RF: $i)$ global emissions of carbon dioxide (CO$_2$), methane (CH$_4$), and nitrous oxide (N$_2$O) at $k$-month; $ii)$ temperatures from the previous three months, i.e., $(k-1)$, $(k-2)$ and $(k-3)$-month; $iii)$ combination of $i$ and $ii$. By investigating breakpoints in the time series, we discover that 24 cities and the gases present breakpoints in the 80's and 90's. After the breakpoints, we find an increase in the temperature and the gas emission. Thereafter, we separate the cities according to their geographical position and employ the RF algorithm to forecast the temperature from 2010-08 until 2022-12. Based on $i$, $ii$, and $iii$, we find that the three inputs result in a very precise forecast, with a normalized root mean squared error (NMRSE) less than 0.083 for the considered cases. From our simulations, the better forecasted region is Northeast through $iii$ (NMRSE = 0.012). Furthermore, we also investigate the forecasting of anomalous temperature data by removing the annual component of each time series. In this case, the best forecasting is obtained with strategy $i$, with the best region being Northeast (NRMSE = 0.090).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11511v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sidney T. da Silva, Enrique C. Gabrick, Ana Luiza R. de Moraes, Ricardo L. Viana, Antonio M. Batista, Iber\^e L. Caldas, J\"urgen Kurths</dc:creator>
    </item>
    <item>
      <title>Grid Topology Estimation using an Information Theoretic Approach</title>
      <link>https://arxiv.org/abs/2505.11517</link>
      <description>arXiv:2505.11517v1 Announce Type: cross 
Abstract: The topology of a power grid is estimated using an information theoretic approach. By modeling the grid as a graph and using voltage magnitude data of individual nodes in the grid, the mutual information between pairs of nodes is computed using different approximation methods. Using the well-known Chow-Liu algorithm, a maximum spanning tree based on mutual information is computed to estimate the power grid topology. Experiments and results are presented to optimize this approach with success shown for IEEE networks generated with MATPOWER and data generated using GridLAB-D. The algorithm is then cross-validated on IEEE networks generated by the European Union Joint Research Council.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11517v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CE</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel T. Speckhard</dc:creator>
    </item>
    <item>
      <title>BLOG: Bayesian Longitudinal Omics with Group Constraints</title>
      <link>https://arxiv.org/abs/2505.11673</link>
      <description>arXiv:2505.11673v1 Announce Type: cross 
Abstract: Clinical investigators are increasingly interested in discovering computational biomarkers from short-term longitudinal omics data sets. This work focuses on Bayesian regression and variable selection for longitudinal omics datasets, which can quantify uncertainty and control false discovery. In our univariate approach, Zellner's $g$ prior is used with two different options of the tuning parameter $g$: $g=\sqrt{n}$ and a $g$ that minimizes Stein's unbiased risk estimate (SURE). Bayes Factors were used to quantify uncertainty and control for false discovery. In the multivariate approach, we use Bayesian Group LASSO with a spike and slab prior for group variable selection. In both approaches, we use the first difference ($\Delta$) scale of longitudinal predictor and the response. These methods work together to enhance our understanding of biomarker identification, improving inference and prediction. We compare our method against commonly used linear mixed effect models on simulated data and real data from a Tuberculosis (TB) study on metabolite biomarker selection. With an automated selection of hyperparameters, the Zellner's $g$ prior approach correctly identifies target metabolites with high specificity and sensitivity across various simulation and real data scenarios. The Multivariate Bayesian Group Lasso spike and slab approach also correctly selects target metabolites across various simulation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11673v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Livia Popa, Sumanta Basu, Myung Hee Lee, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization of Pythia8 Tunes</title>
      <link>https://arxiv.org/abs/2505.11675</link>
      <description>arXiv:2505.11675v1 Announce Type: cross 
Abstract: A new tune (set of model parameters) is found for the six most important parameters of the Pythia8 final state parton shower and hadronization model using Bayesian optimization. The tune fits the LEPI data from ALEPH better than the default tune in Pythia8. To the best of our knowledge, we present the most comprehensive application of Bayesian optimization to the tuning of a parton shower and hadronization model using the LEPI data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11675v1</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Al Kadhim, Harrison B Prosper, Stephen Mrenna</dc:creator>
    </item>
    <item>
      <title>Logarithmic resilience risk metrics that address the huge variations in blackout cost</title>
      <link>https://arxiv.org/abs/2505.12016</link>
      <description>arXiv:2505.12016v1 Announce Type: cross 
Abstract: Resilience risk metrics must address the customer cost of the largest blackouts of greatest impact. However, there are huge variations in blackout cost in observed distribution utility data that make it impractical to properly estimate the mean large blackout cost and the corresponding risk. These problems are caused by the heavy tail observed in the distribution of customer costs. To solve these problems, we propose resilience metrics that describe large blackout risk using the mean of the logarithm of the cost of large-cost blackouts, the slope index of the heavy tail, and the frequency of large-cost blackouts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12016v1</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arslan Ahmad, Ian Dobson</dc:creator>
    </item>
    <item>
      <title>Exploring the interplay between population profile and optimal routes in U.S. cities</title>
      <link>https://arxiv.org/abs/2505.12510</link>
      <description>arXiv:2505.12510v1 Announce Type: cross 
Abstract: Cities have developed over time alongside advancements in civilization, focusing on efficient travel and reducing costs. Many studies have examined the distinctive features of urban road networks, such as their length, efficiency, connection to population density, and other properties. However, the relationship between car routes and population in city structures remains unclear. In this study, we used the center of mass for each city tract, defined by the US Census, as the origins and destinations for our itineraries. We calculated travel time, and both Euclidean and travel distances for sixty major cities. We discovered that the total sum of all routes adheres to an urban law. The distribution of these car journeys follows Weibull functions, suggesting that the urban center plays a crucial role in optimizing routes across multiple cities. We also developed a simple point pattern model for the population, which aligns with the well-known decreasing exponential density expression. Our findings show that the interplay between population and path optimization influences city structure through its center. This study offers a new perspective on the fundamental principles that shape urban design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12510v1</guid>
      <category>physics.soc-ph</category>
      <category>nlin.AO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41598-025-00308-8</arxiv:DOI>
      <arxiv:journal_reference>Scientific Reports 15, (2025) 16094</arxiv:journal_reference>
      <dc:creator>Diego Ortega, Elka Korutcheva</dc:creator>
    </item>
    <item>
      <title>Double machine learning to estimate the effects of multiple treatments and their interactions</title>
      <link>https://arxiv.org/abs/2505.12617</link>
      <description>arXiv:2505.12617v1 Announce Type: cross 
Abstract: Causal inference literature has extensively focused on binary treatments, with relatively fewer methods developed for multi-valued treatments. In particular, methods for multiple simultaneously assigned treatments remain understudied despite their practical importance. This paper introduces two settings: (1) estimating the effects of multiple treatments of different types (binary, categorical, and continuous) and the effects of treatment interactions, and (2) estimating the average treatment effect across categories of multi-valued regimens. To obtain robust estimates for both settings, we propose a class of methods based on the Double Machine Learning (DML) framework. Our methods are well-suited for complex settings of multiple treatments/regimens, using machine learning to model confounding relationships while overcoming regularization and overfitting biases through Neyman orthogonality and cross-fitting. To our knowledge, this work is the first to apply machine learning for robust estimation of interaction effects in the presence of multiple treatments. We further establish the asymptotic distribution of our estimators and derive variance estimators for statistical inference. Extensive simulations demonstrate the performance of our methods. Finally, we apply the methods to study the effect of three treatments on HIV-associated kidney disease in an adult HIV cohort of 2455 participants in Nigeria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12617v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qingyan Xiang, Yubai Yuan, Dongyuan Song, Usman J. Wudil, Muktar H. Aliyu, C. William Wester, Bryan E. Shepherd</dc:creator>
    </item>
    <item>
      <title>How to optimise tournament draws: The case of the 2022 FIFA World Cup</title>
      <link>https://arxiv.org/abs/2505.13106</link>
      <description>arXiv:2505.13106v1 Announce Type: cross 
Abstract: The organisers of major sports competitions use different policies with respect to constraints in the group draw. Our paper aims to rationalise these choices by analysing the trade-off between attractiveness (the number of games played by teams from the same geographic zone) and fairness (the departure of the draw mechanism from a uniform distribution). A parametric optimisation model is formulated and applied to the 2022 FIFA World Cup draw. A flaw of the draw procedure is identified: the pre-assignment of the host to a group implies additional but unnecessary distortions. All Pareto efficient sets of draw constraints are determined via simulations. The proposed framework can be used to find the optimal draw rules of a tournament and justify the distortion of the draw procedure for the stakeholders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13106v1</guid>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o</dc:creator>
    </item>
    <item>
      <title>Discretion in the Loop: Human Expertise in Algorithm-Assisted College Advising</title>
      <link>https://arxiv.org/abs/2505.13325</link>
      <description>arXiv:2505.13325v1 Announce Type: cross 
Abstract: In higher education, many institutions use algorithmic alerts to flag at-risk students and deliver advising at scale. While much research has focused on evaluating algorithmic predictions, relatively little is known about how discretionary interventions by human experts shape outcomes in algorithm-assisted settings. We study this question using rich quantitative and qualitative data from a randomized controlled trial of an algorithm-assisted advising program at Georgia State University. Taking a mixed-methods approach, we examine whether and how advisors use context unavailable to an algorithm to guide interventions and influence student success. We develop a causal graphical framework for human expertise in the interventional setting, extending prior work on discretion in purely predictive settings. We then test a necessary condition for discretionary expertise using structured advisor logs and student outcomes data, identifying several interventions that meet the criterion for statistical significance. Accordingly, we estimate that 2 out of 3 interventions taken by advisors in the treatment arm were plausibly "expertly targeted" to students using non-algorithmic context. Systematic qualitative analysis of advisor notes corroborates these findings, showing that advisors incorporate diverse forms of contextual information--such as personal circumstances, financial issues, and student engagement--into their decisions. Finally, we explore the broader implications of human discretion for long-term outcomes and equity, using heterogeneous treatment effect estimation. Our results offer theoretical and practical insight into the real-world effectiveness of algorithm-supported college advising, and underscore the importance of accounting for human expertise in the design, evaluation, and implementation of algorithmic decision systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13325v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sofiia Druchyna, Kara Schechtman, Benjamin Brandon, Jenise Stafford, Hannah Li, Lydia T. Liu</dc:creator>
    </item>
    <item>
      <title>Starting Seatwork Earlier as a Valid Measure of Student Engagement</title>
      <link>https://arxiv.org/abs/2505.13341</link>
      <description>arXiv:2505.13341v1 Announce Type: cross 
Abstract: Prior work has developed a range of automated measures ("detectors") of student self-regulation and engagement from student log data. These measures have been successfully used to make discoveries about student learning. Here, we extend this line of research to an underexplored aspect of self-regulation: students' decisions about when to start and stop working on learning software during classwork. In the first of two analyses, we build on prior work on session-level measures (e.g., delayed start, early stop) to evaluate their reliability and predictive validity. We compute these measures from year-long log data from Cognitive Tutor for students in grades 8-12 (N = 222). Our findings show that these measures exhibit moderate to high month-to-month reliability (G &gt; .75), comparable to or exceeding gaming-the-system behavior. Additionally, they enhance the prediction of final math scores beyond prior knowledge and gaming-the-system behaviors. The improvement in learning outcome predictions beyond time-on-task suggests they capture a broader motivational state tied to overall learning. The second analysis demonstrates the cross-system generalizability of these measures in i-Ready, where they predict state test scores for grade 7 students (N = 818). By leveraging log data, we introduce system-general naturally embedded measures that complement motivational surveys without extra instrumentation or disruption of instruction time. Our findings demonstrate the potential of session-level logs to mine valid and generalizable measures with broad applications in the predictive modeling of learning outcomes and analysis of learner self-regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13341v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashish Gurung, Jionghao Lin, Zhongtian Huang, Conrad Borchers, Ryan S. Baker, Vincent Aleven, Kenneth R. Koedinger</dc:creator>
    </item>
    <item>
      <title>Machine learning the first stage in 2SLS: Practical guidance from bias decomposition and simulation</title>
      <link>https://arxiv.org/abs/2505.13422</link>
      <description>arXiv:2505.13422v1 Announce Type: cross 
Abstract: Machine learning (ML) primarily evolved to solve "prediction problems." The first stage of two-stage least squares (2SLS) is a prediction problem, suggesting potential gains from ML first-stage assistance. However, little guidance exists on when ML helps 2SLS$\unicode{x2014}$or when it hurts. We investigate the implications of inserting ML into 2SLS, decomposing the bias into three informative components. Mechanically, ML-in-2SLS procedures face issues common to prediction and causal-inference settings$\unicode{x2014}$and their interaction. Through simulation, we show linear ML methods (e.g., post-Lasso) work well, while nonlinear methods (e.g., random forests, neural nets) generate substantial bias in second-stage estimates$\unicode{x2014}$potentially exceeding the bias of endogenous OLS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13422v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Connor Lennon, Edward Rubin, Glen Waddell</dc:creator>
    </item>
    <item>
      <title>Inverse Probability Weighting-based Mediation Analysis for Microbiome Data</title>
      <link>https://arxiv.org/abs/2110.02440</link>
      <description>arXiv:2110.02440v4 Announce Type: replace 
Abstract: Mediation analysis is an important tool for studying causal associations in biomedical and other scientific areas and has recently gained attention in microbiome studies. Using a microbiome study of acute myeloid leukemia (AML) patients, we investigate whether the effect of induction chemotherapy intensity levels on infection status is mediated by microbial taxa abundance. The unique characteristics of the microbial mediators -- high-dimensionality, zero-inflation, and dependence -- call for new methodological developments in mediation analysis. The presence of an exposure-induced mediator-outcome confounder, antibiotic use, further requires a delicate treatment in the analysis. To address these unique challenges in our motivating AML microbiome study, we propose a novel nonparametric identification formula for the interventional indirect effect (IIE), a recently developed measure for assessing mediation effects. We develop a corresponding estimation algorithm using the inverse probability weighting method. We also test the presence of mediation effects via constructing the standard normal bootstrap confidence intervals. Simulation studies demonstrate that the proposed method has good finite-sample performance in terms of IIE estimation accuracy and the type-I error rate and power of the corresponding tests. In the AML microbiome study, our findings suggest that the effect of induction chemotherapy intensity levels on infection is mainly mediated by patients' gut microbiome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.02440v4</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuexia Zhang, Jian Wang, Jiayi Shen, Jessica Galloway-Pena, Samuel Shelburne, Linbo Wang, Jianhua Hu</dc:creator>
    </item>
    <item>
      <title>Continuously Optimizing Radar Placement with Model Predictive Path Integrals</title>
      <link>https://arxiv.org/abs/2405.18999</link>
      <description>arXiv:2405.18999v3 Announce Type: replace 
Abstract: Continuously optimizing sensor placement is essential for precise target localization in various military and civilian applications. While information theory has shown promise in optimizing sensor placement, many studies oversimplify sensor measurement models or neglect dynamic constraints of mobile sensors. To address these challenges, we employ a range measurement model that incorporates radar parameters and radar-target distance, coupled with Model Predictive Path Integral (MPPI) control to manage complex environmental obstacles and dynamic constraints. We compare the proposed approach against stationary radars or simplified range measurement models based on the root mean squared error (RMSE) of the Cubature Kalman Filter (CKF) estimator for the targets' state. Additionally, we visualize the evolving geometry of radars and targets over time, highlighting areas of highest measurement information gain, demonstrating the strengths of the approach. The proposed strategy outperforms stationary radars and simplified range measurement models in target localization, achieving a 38-74% reduction in mean RMSE and a 33-79% reduction in the upper tail of the 90% Highest Density Interval (HDI) over 500 Monte Carl (MC) trials across all time steps.
  Code will be made publicly available upon acceptance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18999v3</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TAES.2025.3528397</arxiv:DOI>
      <dc:creator>Michael Potter, Shuo Tang, Paul Ghanem, Milica Stojanovic, Pau Closas, Murat Akcakaya, Ben Wright, Marius Necsoiu, Deniz Erdogmus, Michael Everett, Tales Imbiriba</dc:creator>
    </item>
    <item>
      <title>The ICML 2023 Ranking Experiment: Examining Author Self-Assessment in ML/AI Peer Review</title>
      <link>https://arxiv.org/abs/2408.13430</link>
      <description>arXiv:2408.13430v2 Announce Type: replace 
Abstract: We conducted an experiment during the review process of the 2023 International Conference on Machine Learning (ICML), asking authors with multiple submissions to rank their papers based on perceived quality. In total, we received 1,342 rankings, each from a different author, covering 2,592 submissions. In this paper, we present an empirical analysis of how author-provided rankings could be leveraged to improve peer review processes at machine learning conferences. We focus on the Isotonic Mechanism, which calibrates raw review scores using the author-provided rankings. Our analysis shows that these ranking-calibrated scores outperform the raw review scores in estimating the ground truth ``expected review scores'' in terms of both squared and absolute error metrics. Furthermore, we propose several cautious, low-risk applications of the Isotonic Mechanism and author-provided rankings in peer review, including supporting senior area chairs in overseeing area chairs' recommendations, assisting in the selection of paper awards, and guiding the recruitment of emergency reviewers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13430v2</guid>
      <category>stat.AP</category>
      <category>cs.DL</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Buxin Su, Jiayao Zhang, Natalie Collina, Yuling Yan, Didong Li, Kyunghyun Cho, Jianqing Fan, Aaron Roth, Weijie Su</dc:creator>
    </item>
    <item>
      <title>Principal stratification with continuous treatments and continuous post-treatment variables</title>
      <link>https://arxiv.org/abs/2309.14486</link>
      <description>arXiv:2309.14486v2 Announce Type: replace-cross 
Abstract: Principal stratification (PS) is a commonly used approach for understanding the mechanisms through which a treatment affects an outcome. The goal of this work is to extend the PS framework to studies with continuous treatments, which introduces a number of both challenges and opportunities in terms of defining causal effects and performing inference. This manuscript provides multiple key methodological contributions: 1) we introduce principal causal estimands for continuous treatments that provide insights into different causal mechanisms, 2) we show that nonparametric identification is possible under a principal ignorability assumption, but only under a restrictive assumption on the joint distribution of potential mediators, which can be dropped under mild parametric assumptions, 3) we utilize nonparametric Bayesian models for the joint distribution of the potential mediating variables to ensure our approach is robust to model misspecification, and 4) we provide theoretical justification for utilizing an outcome model to identify the joint distribution of the potential mediating variables, and show that this is only possible if a principal ignorability assumption is violated. Lastly, we apply our methodology to a novel study of the relationship between the economy and arrest rates, and how this is potentially mediated by police capacity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14486v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Antonelli, Minxuan Wu, Fabrizia Mealli, Brenden Beck, Alessandra Mattei</dc:creator>
    </item>
    <item>
      <title>Sharp variance estimator and causal bootstrap in stratified randomized experiments</title>
      <link>https://arxiv.org/abs/2401.16667</link>
      <description>arXiv:2401.16667v3 Announce Type: replace-cross 
Abstract: Randomized experiments are the gold standard for estimating treatment effects, and randomization serves as a reasoned basis for inference. In widely used stratified randomized experiments, randomization-based finite-population asymptotic theory enables valid inference for the average treatment effect, relying on normal approximation and a Neyman-type conservative variance estimator. However, when the sample size is small or the outcomes are skewed, the Neyman-type variance estimator may become overly conservative, and the normal approximation can fail. To address these issues, we propose a sharp variance estimator and two causal bootstrap methods to more accurately approximate the sampling distribution of the weighted difference-in-means estimator in stratified randomized experiments. The first causal bootstrap procedure is based on rank-preserving imputation and we prove its second-order refinement over normal approximation. The second causal bootstrap procedure is based on constant-treatment-effect imputation and is further applicable in paired experiments. In contrast to traditional bootstrap methods, where randomness originates from hypothetical super-population sampling, our analysis for the proposed causal bootstrap is randomization-based, relying solely on the randomness of treatment assignment in randomized experiments. Numerical studies and two real data applications demonstrate advantages of our proposed methods in finite samples. The \texttt{R} package \texttt{CausalBootstrap} implementing our method is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16667v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 20 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyang Yu, Ke Zhu, Hanzhong Liu</dc:creator>
    </item>
  </channel>
</rss>
