<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 Dec 2024 05:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>LINKs: Large Language Model Integrated Management for 6G Empowered Digital Twin NetworKs</title>
      <link>https://arxiv.org/abs/2412.19811</link>
      <description>arXiv:2412.19811v1 Announce Type: new 
Abstract: In the rapidly evolving landscape of digital twins (DT) and 6G networks, the integration of large language models (LLMs) presents a novel approach to network management. This paper explores the application of LLMs in managing 6G-empowered DT networks, with a focus on optimizing data retrieval and communication efficiency in smart city scenarios. The proposed framework leverages LLMs for intelligent DT problem analysis and radio resource management (RRM) in fully autonomous way without any manual intervention. Our proposed framework -- LINKs, builds up a lazy loading strategy which can minimize transmission delay by selectively retrieving the relevant data. Based on the data retrieval plan, LLMs transform the retrieval task into an numerical optimization problem and utilizing solvers to build an optimal RRM, ensuring efficient communication across the network. Simulation results demonstrate the performance improvements in data planning and network management, highlighting the potential of LLMs to enhance the integration of DT and 6G technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19811v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shufan Jiang, Bangyan Lin, Yue Wu, Yuan Gao</dc:creator>
    </item>
    <item>
      <title>A Survey on Large Language Models for Communication, Network, and Service Management: Application Insights, Challenges, and Future Directions</title>
      <link>https://arxiv.org/abs/2412.19823</link>
      <description>arXiv:2412.19823v1 Announce Type: new 
Abstract: The rapid evolution of communication networks in recent decades has intensified the need for advanced Network and Service Management (NSM) strategies to address the growing demands for efficiency, scalability, enhanced performance, and reliability of these networks. Large Language Models (LLMs) have received tremendous attention due to their unparalleled capabilities in various Natural Language Processing (NLP) tasks and generating context-aware insights, offering transformative potential for automating diverse communication NSM tasks. Contrasting existing surveys that consider a single network domain, this survey investigates the integration of LLMs across different communication network domains, including mobile networks and related technologies, vehicular networks, cloud-based networks, and fog/edge-based networks. First, the survey provides foundational knowledge of LLMs, explicitly detailing the generic transformer architecture, general-purpose and domain-specific LLMs, LLM model pre-training and fine-tuning, and their relation to communication NSM. Under a novel taxonomy of network monitoring and reporting, AI-powered network planning, network deployment and distribution, and continuous network support, we extensively categorize LLM applications for NSM tasks in each of the different network domains, exploring existing literature and their contributions thus far. Then, we identify existing challenges and open issues, as well as future research directions for LLM-driven communication NSM, emphasizing the need for scalable, adaptable, and resource-efficient solutions that align with the dynamic landscape of communication networks. We envision that this survey serves as a holistic roadmap, providing critical insights for leveraging LLMs to enhance NSM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19823v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Gordon Owusu Boateng, Hani Sami, Ahmed Alagha, Hanae Elmekki, Ahmad Hammoud, Rabeb Mizouni, Azzam Mourad, Hadi Otrok, Jamal Bentahar, Sami Muhaidat, Chamseddine Talhi, Zbigniew Dziong, Mohsen Guizani</dc:creator>
    </item>
    <item>
      <title>High-Accuracy and Efficient DV-Hop Localization for IoT Using Hop Loss</title>
      <link>https://arxiv.org/abs/2412.19827</link>
      <description>arXiv:2412.19827v1 Announce Type: new 
Abstract: Accurate localization is critical for Internet of Things (IoT) applications. Using hop loss in DV-Hop-based algorithms is a promising approach. Nevertheless, challenges lie in overcoming the computational complexity caused by re-calculating the predicted hop-counts, and how to further optimize the modeling for better accuracy. In this paper, a novel hop loss modeling, distance-based connectivity consistency (DCC), is proposed. By focusing on the first order connectivity, DCC avoids computing predicted hop-counts, and significantly reduces the time complexity. We also provide a proof to theoretically guarantee that this design achieves a full coverage of all hop errors. In addition, by computing a continuous loss function instead of the discrete hop-count errors, DCC further improves the localization accuracy. In the evaluations, DCC demonstrates notable improvements in accuracy over other highly regarded algorithms, and reduces 30% to 40% total computation time compared with the baseline algorithm using hop loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19827v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengdi Shen, Qiran Wang</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Context-Aware IoT Management and State-of-the-Art IoT Traffic Anomaly Detection</title>
      <link>https://arxiv.org/abs/2412.19830</link>
      <description>arXiv:2412.19830v1 Announce Type: new 
Abstract: The rapid expansion of Internet of Things (IoT) ecosystems has introduced growing complexities in device management and network security. To address these challenges, we present a unified framework that combines context-driven large language models (LLMs) for IoT administrative tasks with a fine-tuned anomaly detection module for network traffic analysis. The framework streamlines administrative processes such as device management, troubleshooting, and security enforcement by harnessing contextual knowledge from IoT manuals and operational data. The anomaly detection model achieves state-of-the-art performance in identifying irregularities and threats within IoT traffic, leveraging fine-tuning to deliver exceptional accuracy. Evaluations demonstrate that incorporating relevant contextual information significantly enhances the precision and reliability of LLM-based responses for diverse IoT administrative tasks. Additionally, resource usage metrics such as execution time, memory consumption, and response efficiency demonstrate the framework's scalability and suitability for real-world IoT deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19830v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Adu Worae, Athar Sheikh, Spyridon Mastorakis</dc:creator>
    </item>
    <item>
      <title>Hierarchical Blockchain Radio Access Networks: Architecture, Modelling, and Performance Assessment</title>
      <link>https://arxiv.org/abs/2412.19838</link>
      <description>arXiv:2412.19838v1 Announce Type: new 
Abstract: Demands for secure, ubiquitous, and always-available connectivity have been identified as the pillar design parameters of the next generation radio access networks (RANs). Motivated by this, the current contribution introduces a network architecture that leverages blockchain technologies to augment security in RANs, while enabling dynamic coverage expansion through the use of intermediate commercial or private wireless nodes. To assess the efficiency and limitations of the architecture, we employ Markov chain theory in order to extract a theoretical model with increased engineering insights. Building upon this model, we quantify the latency as well as the security capabilities in terms of probability of successful attack, for three scenarios, namely fixed topology fronthaul network, advanced coverage expansion and advanced mobile node connectivity, which reveal the scalability of the blockchain-RAN architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19838v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasileios Kouvakis, Stylianos E. Trevlakis, Alexandros-Apostolos A. Boulogeorgos, Hongwu Liu, Waqas Khalid, Theodoros Tsiftsis, Octavia A. Dobre</dc:creator>
    </item>
    <item>
      <title>Interference Management Strategies in HAPS-Enabled vHetNets in Urban Deployments</title>
      <link>https://arxiv.org/abs/2412.19865</link>
      <description>arXiv:2412.19865v1 Announce Type: new 
Abstract: Next-generation wireless networks are evolving towards architectures that integrate terrestrial and non-terrestrial networks (NTN), unitedly known as vertical heterogeneous networks (vHetNets). This integration is vital to address the increasing demand for coverage, capacity, and new services in urban environments. In vHetNets, various tiers can operate within the same frequency band, creating a harmonized spectrum-integrated network. Although this harmonization significantly enhances spectral efficiency, it also introduces challenges, with interference being a primary concern. This paper investigates vHetNets comprising high altitude platform stations (HAPS) and terrestrial macro base stations (MBSs) operating in a shared spectrum, where interference becomes a critical issue. The unique constraints of HAPS-enabled vHetNets further complicate the interference management problem. To address these challenges, we explore various strategies to manage interference in HAPS-enabled vHetNets. Accordingly, we discuss centralized and distributed approaches that leverage tools based on mathematical optimization and artificial intelligence (AI) to solve interference management problems. Preliminarily numerical evaluations reveal that distributed approaches not only achieve lower complexity but also deliver superior scalability compared to centralized methods, primarily due to their reduced dependence on global information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19865v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Afsoon Alidadi Shamsabadi, Animesh Yadav, Halim Yanikomeroglu</dc:creator>
    </item>
    <item>
      <title>Scalability Assurance in SFC provisioning via Distributed Design for Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2412.19995</link>
      <description>arXiv:2412.19995v1 Announce Type: new 
Abstract: High-quality Service Function Chaining (SFC) provisioning is provided by the timely execution of Virtual Network Functions (VNFs) in a defined sequence. Advanced Deep Reinforcement Learning (DRL) solutions are utilized in many studies to contribute to fast and reliable autonomous SFC provisioning. However, under a large-scale network environment, centralized solutions might struggle to provide efficient outcomes when handling massive demands with stringent End-to-End (E2E) delay constraints. Therefore, in this paper, a novel distributed SFC provisioning framework is proposed, where the network is divided into several clusters. Each cluster has a dedicated local agent with a DRL module to handle the SFC provisioning of demands in that cluster. Also, there is a general agent that can communicate with local agents to handle the requests beyond their capacity. The DRL module of local agents can be applied under different configurations of clusters independent of different numbers of data centers and logical links in each cluster. Simulation results demonstrate that utilizing the proposed distributed framework offers up to 60% improvements in the acceptance ratio of service requests in comparison to the centralized approach while minimizing the E2E delay of accepted requests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19995v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Emil Janulewicz</dc:creator>
    </item>
    <item>
      <title>Embodied AI-empowered Low Altitude Economy: Integrated Sensing, Communications, Computation, and Control (ISC3)</title>
      <link>https://arxiv.org/abs/2412.19996</link>
      <description>arXiv:2412.19996v1 Announce Type: new 
Abstract: Low altitude economy (LAE) holds immense potential to drive urban development across various sectors. However, LAE also faces challenges in data collection and processing efficiency, flight control precision, and network performance. The challenges could be solved by realizing an integration of sensing, communications, computation, and control (ISC3) for LAE. In this regard, embodied artificial intelligence (EAI), with its unique perception, planning, and decision-making capabilities, offers a promising solution to realize ISC3. Specifically, this paper investigates an application of EAI into ISC3 to support LAE, exploring potential research focuses, solutions, and case study. We begin by outlining rationales and benefits of introducing EAI into LAE, followed by reviewing research directions and solutions for EAI in ISC3. We then propose a framework of an EAI-enabled ISC3 for LAE. The framework's effectiveness is evaluated through a case study of express delivery utilizing an EAI-enabled UAV. Finally, we discuss several future research directions for advancing EAI-enabled LAE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19996v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaoqi Yang, Yong Chen, Jiacheng Wang, Geng Sun, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>A Time-Triggered Communication Method Based on Urgency-Based Scheduler in Time-Sensitive Networking</title>
      <link>https://arxiv.org/abs/2412.20077</link>
      <description>arXiv:2412.20077v1 Announce Type: new 
Abstract: The development of the automotive industry and automation has led to a growing demand for time-critical systems to have low latency and jitter for critical traffic. To address this issue, the IEEE 802.1 Time-Sensitive Networking (TSN) task group proposed the Time-Aware Shaper (TAS) to implement Time-Triggered (TT) communication, enabling deterministic transmission by assigning specific time windows to each stream. However, the deployment of TAS requires a solution for the particular deployment scenario. Scheduling algorithms with Fixed-Routing and Waiting-Allowed (FR-WA) mechanisms while providing flexible solutions still have room for optimization in terms of their solution time, reducing network design efficiency and online scheduling deployment. Firstly, the paper overviews the key mechanisms to implement TT communication in TSN, including TAS and FR-WA scheduling algorithms. Secondly, building on this overview of these mechanisms, potential problems with the current implementation of the TAS mechanism are analyzed, including the increasing constraint number as the network scales and potential issues that may arise in traffic anomalies. Then, a method for implementing TT communication using Urgency-Based Scheduler (TT-UBS) is proposed to improve solution efficiency and deterministic transmission in the presence of traffic anomalies. The effectiveness of this method is also analyzed. We propose a scheduling algorithm for solving the parameters of the TT-UBS mechanism. Finally, a simulation-based assessment of the TT-UBS mechanism and the scheduling algorithm is presented. In addition, we extend the method of modifying the scheduling algorithm to other scheduling algorithms and explore the solution efficiency of the extended algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20077v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zitong Wang, Mingzhi Wu, Feng Luo, Yi Ren, Xiaoxian Zhang</dc:creator>
    </item>
    <item>
      <title>Contention-Aware Microservice Deployment in Collaborative Mobile Edge Networks</title>
      <link>https://arxiv.org/abs/2412.20151</link>
      <description>arXiv:2412.20151v1 Announce Type: new 
Abstract: As an emerging computing paradigm, mobile edge computing (MEC) provides processing capabilities at the network edge, aiming to reduce latency and improve user experience. Meanwhile, the advancement of containerization technology facilitates the deployment of microservice-based applications via edge node collaboration, ensuring highly efficient service delivery. However, existing research overlooks the resource contention among microservices in MEC. This neglect potentially results in inadequate resources for microservices constituting latency-sensitive applications, leading to increased response time and ultimately compromising quality of service (QoS). To solve this problem, we propose the Contention-Aware Multi-Application Microservice Deployment (CAMD) algorithm for collaborative MEC, balancing rapid response for applications with low-latency requirements and overall processing efficiency. The CAMD algorithm decomposes the overall deployment problem into manageable sub-problems, each focusing on a single microservice, then employs a heuristic approach to optimize these sub-problems, and ultimately arrives at an optimized deployment scheme through an iterative process. Finally, the superiority of the proposed algorithm is evidenced through intensive experiments and comparison with baseline algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20151v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinlei Ge, Yang Li, Xing Zhang, Yukun Sun, Yunji Zhao</dc:creator>
    </item>
    <item>
      <title>Defending Against Network Attacks for Secure AI Agent Migration in Vehicular Metaverses</title>
      <link>https://arxiv.org/abs/2412.20154</link>
      <description>arXiv:2412.20154v1 Announce Type: new 
Abstract: Vehicular metaverses, blending traditional vehicular networks with metaverse technology, are expected to revolutionize fields such as autonomous driving. As virtual intelligent assistants in vehicular metaverses, Artificial Intelligence (AI) agents powered by large language models can create immersive 3D virtual spaces for passengers to enjoy on-broad vehicular applications and services. To provide users with seamless and engaging virtual interactions, resource-limited vehicles offload AI agents to RoadSide Units (RSUs) with adequate communication and computational capabilities. Due to the mobility of vehicles and the limited coverage of RSUs, AI agents need to migrate from one RSU to another RSU. However, potential network attacks pose significant challenges to ensuring reliable and efficient AI agent migration. In this paper, we first explore specific network attacks including traffic-based attacks (i.e., DDoS attacks) and infrastructure-based attacks (i.e., malicious RSU attacks). Then, we model the AI agent migration process as a Partially Observable Markov Decision Process (POMDP) and apply multi-agent proximal policy optimization algorithms to mitigate DDoS attacks. In addition, we propose a trust assessment mechanism to counter malicious RSU attacks. Numerical results validate that the proposed solutions effectively defend against these network attacks and reduce the total latency of AI agent migration by approximately 43.3%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20154v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinru Wen, Jinbo Wen, Ming Xiao, Jiawen Kang, Tao Zhang, Xiaohuan Li, Chuanxi Chen, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>An Optimization Driven Link SINR Assurance in RIS-assisted Indoor Networks</title>
      <link>https://arxiv.org/abs/2412.20254</link>
      <description>arXiv:2412.20254v1 Announce Type: new 
Abstract: Future smart factories are expected to deploy applications over high-performance indoor wireless channels in the millimeter-wave (mmWave) bands, which on the other hand are susceptible to high path losses and Line-of Sight (LoS) blockages. Low-cost Reconfigurable Intelligent Surfaces (RISs) can provide great opportunities in such scenarios, due to its ability to alleviate LoS link blockages. In this paper, we formulate a combinatorial optimization problem, solved with Integer Linear Programming (ILP) to optimally maintain connectivity by solving the problem of allocating RIS to robots in a wireless indoor network. Our model exploits the characteristic of nulling interference from RISs by tuning RIS reflection coefficients. We further consider Quality-of-Service (QoS) at receivers in terms of Signal-to-Interference-plus-Noise Ratio (SINR) and connection outages due to insufficient transmission quality service. Numerical results for optimal solutions and heuristics show the benefits of optimally deploying RISs by providing continuous connectivity through SINR, which significantly reduces outages due to link quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20254v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Cao Vien Phung, Max Franke, Ehsan Tohidi, June Heinemann, Andre Drummond, Stefan Schmid, Slawomir Stanczak, Admela Jukan</dc:creator>
    </item>
    <item>
      <title>SatFlow: Scalable Network Planning for LEO Mega-Constellations</title>
      <link>https://arxiv.org/abs/2412.20475</link>
      <description>arXiv:2412.20475v1 Announce Type: new 
Abstract: Low-earth-orbit (LEO) satellite communication networks have evolved into mega-constellations with hundreds to thousands of satellites inter-connecting with inter-satellite links (ISLs). Network planning, which plans for network resources and architecture to improve the network performance and save operational costs, is crucial for satellite network management. However, due to the large scale of mega-constellations, high dynamics of satellites, and complex distribution of real-world traffic, it is extremely challenging to conduct scalable network planning on mega-constellations with high performance. In this paper, we propose SatFlow, a distributed and hierarchical network planning framework to plan for the network topology, traffic allocation, and fine-grained ISL terminal power allocation for mega-constellations. To tackle the hardness of the original problem, we decompose the grand problem into two hierarchical sub-problems, tackled by two-tier modules. A multi-agent reinforcement learning approach is proposed for the upper-level module so that the overall laser energy consumption and ISL operational costs can be minimized; A distributed alternating step algorithm is proposed for the lower-level module so that the laser energy consumption could be minimized with low time complexity for a given topology. Extensive simulations on various mega-constellations validate SatFlow's scalability on the constellation size, reducing the flow violation ratio by up to 21.0% and reducing the total costs by up to 89.4%, compared with various state-of-the-art benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20475v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheng Cen, Qiying Pan, Yifei Zhu, Bo Li</dc:creator>
    </item>
    <item>
      <title>Exploiting NOMA Transmissions in Multi-UAV-assisted Wireless Networks: From Aerial-RIS to Mode-switching UAVs</title>
      <link>https://arxiv.org/abs/2412.20484</link>
      <description>arXiv:2412.20484v1 Announce Type: new 
Abstract: In this paper, we consider an aerial reconfigurable intelligent surface (ARIS)-assisted wireless network, where multiple unmanned aerial vehicles (UAVs) collect data from ground users (GUs) by using the non-orthogonal multiple access (NOMA) method. The ARIS provides enhanced channel controllability to improve the NOMA transmissions and reduce the co-channel interference among UAVs. We also propose a novel dual-mode switching scheme, where each UAV equipped with both an ARIS and a radio frequency (RF) transceiver can adaptively perform passive reflection or active transmission. We aim to maximize the overall network throughput by jointly optimizing the UAVs' trajectory planning and operating modes, the ARIS's passive beamforming, and the GUs' transmission control strategies. We propose an optimization-driven hierarchical deep reinforcement learning (O-HDRL) method to decompose it into a series of subproblems. Specifically, the multi-agent deep deterministic policy gradient (MADDPG) adjusts the UAVs' trajectory planning and mode switching strategies, while the passive beamforming and transmission control strategies are tackled by the optimization methods. Numerical results reveal that the O-HDRL efficiently improves the learning stability and reward performance compared to the benchmark methods. Meanwhile, the dual-mode switching scheme is verified to achieve a higher throughput performance compared to the fixed ARIS scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20484v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Songhan Zhao, Shimin Gong, Bo Gu, Lanhua Li, Bin Lyu, Dinh Thai Hoang, Changyan Yi</dc:creator>
    </item>
    <item>
      <title>Ns3 meets Sionna: Using Realistic Channels in Network Simulation</title>
      <link>https://arxiv.org/abs/2412.20524</link>
      <description>arXiv:2412.20524v1 Announce Type: new 
Abstract: Network simulators are indispensable tools for the advancement of wireless network technologies, offering a cost-effective and controlled environment to simulate real-world network behavior. However, traditional simulators, such as the widely used ns-3, exhibit limitations in accurately modeling indoor and outdoor scenarios due to their reliance on simplified statistical and stochastic channel propagation models, which often fail to accurately capture physical phenomena like multipath signal propagation and shadowing by obstacles in the line-of-sight path. We present Ns3Sionna, which integrates a ray tracing-based channel model, implemented using the Sionna RT framework, within the ns-3 network simulator. It allows to simulate environment-specific and physically accurate channel realizations for a given 3D scene and wireless device positions. Additionally, a mobility model based on ray tracing was developed to accurately represent device movements within the simulated 3D space. Ns3Sionna provides more realistic path and delay loss estimates for both indoor and outdoor environments than existing ns-3 propagation models, particularly in terms of spatial and temporal correlation. Moreover, fine-grained channel state information is provided, which could be used for the development of sensing applications. Due to the significant computational demands of ray tracing, Ns3Sionna takes advantage of the parallel execution capabilities of modern GPUs and multi-core CPUs by incorporating intelligent pre-caching mechanisms that leverage the channel's coherence time to optimize runtime performance. This enables the efficient simulation of scenarios with a small to medium number of mobile nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20524v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anatolij Zubow, Yannik Pilz, Sascha R\"osler, Falko Dressler</dc:creator>
    </item>
    <item>
      <title>3GPP Evolution from 5G to 6G: A 10-Year Retrospective</title>
      <link>https://arxiv.org/abs/2412.21077</link>
      <description>arXiv:2412.21077v1 Announce Type: new 
Abstract: The 3rd Generation Partnership Project (3GPP) evolution of mobile communication technologies from 5G to 6G has been a transformative journey spanning a decade, shaped by six releases from Release 15 to Release 20. This article provides a retrospective of this evolution, highlighting the technical advancements, challenges, and milestones that have defined the transition from the foundational 5G era to the emergence of 6G. Starting with Release 15, which marked the birth of 5G and its New Radio (NR) air interface, the journey progressed through Release 16, where 5G was qualified as an International Mobile Telecommunications-2020 (IMT-2020) technology, and Release 17, which expanded 5G into new domains such as non-terrestrial networks. Release 18 ushered in the 5G-Advanced era, incorporating novel technologies like artificial intelligence. Releases 19 and 20 continue this momentum, focusing on commercially driven enhancements while laying the groundwork for the 6G era. This article explores how 3GPP technology evolution has shaped the telecommunications landscape over the past decade, bridging two mobile generations. It concludes with insights into learned lessons, future challenges, and opportunities, offering guidelines on 6G evolution for 2030 and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21077v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingqin Lin</dc:creator>
    </item>
    <item>
      <title>Open RAN-Enabled Deep Learning-Assisted Mobility Management for Connected Vehicles</title>
      <link>https://arxiv.org/abs/2412.21161</link>
      <description>arXiv:2412.21161v1 Announce Type: new 
Abstract: Connected Vehicles (CVs) can leverage the unique features of 5G and future 6G/NextG networks to enhance Intelligent Transportation System (ITS) services. However, even with advancements in cellular network generations, CV applications may experience communication interruptions in high-mobility scenarios due to frequent changes of serving base station, also known as handovers (HOs). This paper proposes the adoption of Open Radio Access Network (Open RAN/O-RAN) and deep learning models for decision-making to prevent Quality of Service (QoS) degradation due to HOs and to ensure the timely connectivity needed for CV services. The solution utilizes the O-RAN Software Community (OSC), an open-source O-RAN platform developed by the collaboration between the O-RAN Alliance and Linux Foundation, to develop xApps that are executed in the near-Real-Time RIC of OSC. To demonstrate the proposal's effectiveness, an integrated framework combining the OMNeT++ simulator and OSC was created. Evaluations used real-world datasets in urban application scenarios, such as video streaming transmission and over-the-air (OTA) updates. Results indicate that the proposal achieved superior performance and reduced latency compared to the standard 3GPP HO procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21161v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Barbosa, Kelvin Dias</dc:creator>
    </item>
    <item>
      <title>Open-Source 5G Core Platforms: A Low-Cost Solution and Performance Evaluation</title>
      <link>https://arxiv.org/abs/2412.21162</link>
      <description>arXiv:2412.21162v1 Announce Type: new 
Abstract: An essential component for the Fifth Generation of Mobile Networks deployments is the 5G Core (5GC), which bridges the 5G Radio Access Network (RAN) to the rest of the Internet. Some open-source platforms for the 5GC have emerged and been deployed in Common Off-the-Shelf (COTS)-based setups. Despite these open-source 5GC initiatives following the 3GPP specifications, they differ in implementing some features and their stages in the timeline of 3GPP releases. Besides that, they may yield different performance to metrics related to the data and control planes. This article reviews the major open-source 5GC platforms and evaluates their performance in a 5G Standalone (SA) COTS-based testbed. The results indicate that Open5GS provides the best latencies for control plane procedures, OpenAirInterface offers the highest data rates, and Free5GC has the lowest resource consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21162v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Barbosa, Marcelo Silva, Ednelson Cavalcanti, Kelvin Dias</dc:creator>
    </item>
    <item>
      <title>Adversarial Attack and Defense for LoRa Device Identification and Authentication via Deep Learning</title>
      <link>https://arxiv.org/abs/2412.21164</link>
      <description>arXiv:2412.21164v1 Announce Type: new 
Abstract: LoRa provides long-range, energy-efficient communications in Internet of Things (IoT) applications that rely on Low-Power Wide-Area Network (LPWAN) capabilities. Despite these merits, concerns persist regarding the security of LoRa networks, especially in situations where device identification and authentication are imperative to secure the reliable access to the LoRa networks. This paper explores a deep learning (DL) approach to tackle these concerns, focusing on two critical tasks, namely (i) identifying LoRa devices and (ii) classifying them to legitimate and rogue devices. Deep neural networks (DNNs), encompassing both convolutional and feedforward neural networks, are trained for these tasks using actual LoRa signal data. In this setting, the adversaries may spoof rogue LoRa signals through the kernel density estimation (KDE) method based on legitimate device signals that are received by the adversaries. Two cases are considered, (i) training two separate classifiers, one for each of the two tasks, and (ii) training a multi-task classifier for both tasks. The vulnerabilities of the resulting DNNs to manipulations in input samples are studied in form of untargeted and targeted adversarial attacks using the Fast Gradient Sign Method (FGSM). Individual and common perturbations are considered against single-task and multi-task classifiers for the LoRa signal analysis. To provide resilience against such attacks, a defense approach is presented by increasing the robustness of classifiers with adversarial training. Results quantify how vulnerable LoRa signal classification tasks are to adversarial attacks and emphasize the need to fortify IoT applications against these subtle yet effective threats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21164v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yalin E. Sagduyu, Tugba Erpek</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Q-Learning for Real-Time Load Balancing User Association and Handover in Mobile Networks</title>
      <link>https://arxiv.org/abs/2412.19835</link>
      <description>arXiv:2412.19835v1 Announce Type: cross 
Abstract: As next generation cellular networks become denser, associating users with the optimal base stations at each time while ensuring no base station is overloaded becomes critical for achieving stable and high network performance. We propose multi-agent online Q-learning (QL) algorithms for performing real-time load balancing user association and handover in dense cellular networks. The load balancing constraints at all base stations couple the actions of user agents, and we propose two multi-agent action selection policies, one centralized and one distributed, to satisfy load balancing at every learning step. In the centralized policy, the actions of UEs are determined by a central load balancer (CLB) running an algorithm based on swapping the worst connection to maximize the total learning reward. In the distributed policy, each UE takes an action based on its local information by participating in a distributed matching game with the BSs to maximize the local reward. We then integrate these action selection policies into an online QL algorithm that adapts in real-time to network dynamics including channel variations and user mobility, using a reward function that considers a handover cost to reduce handover frequency. The proposed multi-agent QL algorithm features low-complexity and fast convergence, outperforming 3GPP max-SINR association. Both policies adapt well to network dynamics at various UE speed profiles from walking, running, to biking and suburban driving, illustrating their robustness and real-time adaptability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19835v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alireza Alizadeh, Byungju Lim, Mai Vu</dc:creator>
    </item>
    <item>
      <title>Adaptive Parameter-Efficient Federated Fine-Tuning on Heterogeneous Devices</title>
      <link>https://arxiv.org/abs/2412.20004</link>
      <description>arXiv:2412.20004v1 Announce Type: cross 
Abstract: Federated fine-tuning (FedFT) has been proposed to fine-tune the pre-trained language models in a distributed manner. However, there are two critical challenges for efficient FedFT in practical applications, i.e., resource constraints and system heterogeneity. Existing works rely on parameter-efficient fine-tuning methods, e.g., low-rank adaptation (LoRA), but with major limitations. Herein, based on the inherent characteristics of FedFT, we observe that LoRA layers with higher ranks added close to the output help to save resource consumption while achieving comparable fine-tuning performance. Then we propose a novel LoRA-based FedFT framework, termed LEGEND, which faces the difficulty of determining the number of LoRA layers (called, LoRA depth) and the rank of each LoRA layer (called, rank distribution). We analyze the coupled relationship between LoRA depth and rank distribution, and design an efficient LoRA configuration algorithm for heterogeneous devices, thereby promoting fine-tuning efficiency. Extensive experiments are conducted on a physical platform with 80 commercial devices. The results show that LEGEND can achieve a speedup of 1.5-2.8$\times$ and save communication costs by about 42.3% when achieving the target accuracy, compared to the advanced solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20004v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun Liu, Yunming Liao, Hongli Xu, Yang Xu, Jianchun Liu, Chen Qian</dc:creator>
    </item>
    <item>
      <title>ASE: Practical Acoustic Speed Estimation Beyond Doppler via Sound Diffusion Field</title>
      <link>https://arxiv.org/abs/2412.20142</link>
      <description>arXiv:2412.20142v1 Announce Type: cross 
Abstract: Passive human speed estimation plays a critical role in acoustic sensing. Despite extensive study, existing systems, however, suffer from various limitations: First, previous acoustic speed estimation exploits Doppler Frequency Shifts (DFS) created by moving targets and relies on microphone arrays, making them only capable of sensing the radial speed within a constrained distance. Second, the channel measurement rate proves inadequate to estimate high moving speeds. To overcome these issues, we present ASE, an accurate and robust Acoustic Speed Estimation system on a single commodity microphone. We model the sound propagation from a unique perspective of the acoustic diffusion field, and infer the speed from the acoustic spatial distribution, a completely different way of thinking about speed estimation beyond prior DFS-based approaches. We then propose a novel Orthogonal Time-Delayed Multiplexing (OTDM) scheme for acoustic channel estimation at a high rate that was previously infeasible, making it possible to estimate high speeds. We further develop novel techniques for motion detection and signal enhancement to deliver a robust and practical system. We implement and evaluate ASE through extensive real-world experiments. Our results show that ASE reliably tracks walking speed, independently of target location and direction, with a mean error of 0.13 m/s, a reduction of 2.5x from DFS, and a detection rate of 97.4% for large coverage, e.g., free walking in a 4m $\times$ 4m room. We believe ASE pushes acoustic speed estimation beyond the conventional DFS-based paradigm and will inspire exciting research in acoustic sensing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20142v1</guid>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>eess.SP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheng Lyu, Chenshu Wu</dc:creator>
    </item>
    <item>
      <title>Revisiting Cache Freshness for Emerging Real-Time Applications</title>
      <link>https://arxiv.org/abs/2412.20221</link>
      <description>arXiv:2412.20221v1 Announce Type: cross 
Abstract: Caching is widely used in industry to improve application performance by reducing data-access latency and taking the load off the backend infrastructure. TTLs have become the de-facto mechanism used to keep cached data reasonably fresh (i.e., not too out of date with the backend). However, the emergence of real-time applications requires tighter data freshness, which is impractical to achieve with TTLs. We discuss why this is the case, and propose a simple yet effective adaptive policy to achieve the desired freshness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20221v1</guid>
      <category>cs.OS</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3696348.3696858</arxiv:DOI>
      <dc:creator>Ziming Mao, Rishabh Iyer, Scott Shenker, Ion Stoica</dc:creator>
    </item>
    <item>
      <title>Design of an improved microstrip antenna operating at a frequency band of 28GHz</title>
      <link>https://arxiv.org/abs/2412.20400</link>
      <description>arXiv:2412.20400v1 Announce Type: cross 
Abstract: The design of an improved microstrip antenna operating in the 28 GHz frequency spectrum is the main goal of this work. The design used a Roger RT 5880 LZ substrate with a thickness and permittivity of 0.762mm and 1.96, respectively. The antenna was simulated in CST Microwave Studio. As the antenna feed, a quarter-wave transformer was used to provide an impedance match of 50 ohms. To improve the antenna's performance, a Ushaped element was added to the ground plane. The antenna resonated at 28 GHz frequency, according to simulation data, with a return loss of -21.4 dB, VSWR of 1.18, bandwidth of 2.026 GHz, and gain of 8.19 dB. The proposed antenna exhibits a performance improvement in terms of gain and bandwidth due to the addition of U-shaped element when benchmarked with existing designs in the literature work</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20400v1</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:journal_reference>Faculty of Engineering and Technology Conference (FETiCON 2024), Jun. 2 - 6, 2024, University of Ilorin, Nigeria</arxiv:journal_reference>
      <dc:creator>S. O. Zakariyya, B. O. Sadiq, R. A. Alao, J. A. Adesina, E. Obi</dc:creator>
    </item>
    <item>
      <title>NetFlowGen: Leveraging Generative Pre-training for Network Traffic Dynamics</title>
      <link>https://arxiv.org/abs/2412.20635</link>
      <description>arXiv:2412.20635v1 Announce Type: cross 
Abstract: Understanding the traffic dynamics in networks is a core capability for automated systems to monitor and analyze networking behaviors, reducing expensive human efforts and economic risks through tasks such as traffic classification, congestion prediction, and attack detection. However, it is still challenging to accurately model network traffic with machine learning approaches in an efficient and broadly applicable manner. Task-specific models trained from scratch are used for different networking applications, which limits the efficiency of model development and generalization of model deployment. Furthermore, while networking data is abundant, high-quality task-specific labels are often insufficient for training individual models. Large-scale self-supervised learning on unlabeled data provides a natural pathway for tackling these challenges. We propose to pre-train a general-purpose machine learning model to capture traffic dynamics with only traffic data from NetFlow records, with the goal of fine-tuning for different downstream tasks with small amount of labels. Our presented NetFlowGen framework goes beyond a proof-of-concept for network traffic pre-training and addresses specific challenges such as unifying network feature representations, learning from large unlabeled traffic data volume, and testing on real downstream tasks in DDoS attack detection. Experiments demonstrate promising results of our pre-training framework on capturing traffic dynamics and adapting to different networking tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20635v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Zhou, Woojeong Kim, Zhiying Xu, Alexander M. Rush, Minlan Yu</dc:creator>
    </item>
    <item>
      <title>Environmental and Economic Impact of I/O Device Obsolescence</title>
      <link>https://arxiv.org/abs/2412.20655</link>
      <description>arXiv:2412.20655v1 Announce Type: cross 
Abstract: This paper analyzes the proportion of Input/output devices made obsolete by changes in technology generations. This obsolescence may be by new software/hardware generations rendering otherwise functional devices unusable. Concluding with brief analysis on the economic and environmental impacts of the e-waste produced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20655v1</guid>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Gould, Guanqun Song, Ting Zhu</dc:creator>
    </item>
    <item>
      <title>Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense</title>
      <link>https://arxiv.org/abs/2412.21051</link>
      <description>arXiv:2412.21051v1 Announce Type: cross 
Abstract: The rapid evolution of cloud computing technologies and the increasing number of cloud applications have provided a large number of benefits in daily lives. However, the diversity and complexity of different components pose a significant challenge to cloud security, especially when dealing with sophisticated and advanced cyberattacks. Recent advancements in generative foundation models (GFMs), particularly in the large language models (LLMs), offer promising solutions for security intelligence. By exploiting the powerful abilities in language understanding, data analysis, task inference, action planning, and code generation, we present LLM-PD, a novel proactive defense architecture that defeats various threats in a proactive manner. LLM-PD can efficiently make a decision through comprehensive data analysis and sequential reasoning, as well as dynamically creating and deploying actionable defense mechanisms on the target cloud. Furthermore, it can flexibly self-evolve based on experience learned from previous interactions and adapt to new attack scenarios without additional training. The experimental results demonstrate its remarkable ability in terms of defense effectiveness and efficiency, particularly highlighting an outstanding success rate when compared with other existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21051v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuyang Zhou, Guang Cheng, Kang Du, Zihan Chen</dc:creator>
    </item>
    <item>
      <title>Privacy-Aware Multi-Device Cooperative Edge Inference with Distributed Resource Bidding</title>
      <link>https://arxiv.org/abs/2412.21069</link>
      <description>arXiv:2412.21069v1 Announce Type: cross 
Abstract: Mobile edge computing (MEC) has empowered mobile devices (MDs) in supporting artificial intelligence (AI) applications through collaborative efforts with proximal MEC servers. Unfortunately, despite the great promise of device-edge cooperative AI inference, data privacy becomes an increasing concern. In this paper, we develop a privacy-aware multi-device cooperative edge inference system for classification tasks, which integrates a distributed bidding mechanism for the MEC server's computational resources. Intermediate feature compression is adopted as a principled approach to minimize data privacy leakage. To determine the bidding values and feature compression ratios in a distributed fashion, we formulate a decentralized partially observable Markov decision process (DEC-POMDP) model, for which, a multi-agent deep deterministic policy gradient (MADDPG)-based algorithm is developed. Simulation results demonstrate the effectiveness of the proposed algorithm in privacy-preserving cooperative edge inference. Specifically, given a sufficient level of data privacy protection, the proposed algorithm achieves 0.31-0.95% improvements in classification accuracy compared to the approach being agnostic to the wireless channel conditions. The performance is further enhanced by 1.54-1.67% by considering the difficulties of inference data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21069v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenhao Zhuang, Yuyi Mao</dc:creator>
    </item>
    <item>
      <title>Distributed Mixture-of-Agents for Edge Inference with Large Language Models</title>
      <link>https://arxiv.org/abs/2412.21200</link>
      <description>arXiv:2412.21200v1 Announce Type: cross 
Abstract: Mixture-of-Agents (MoA) has recently been proposed as a method to enhance performance of large language models (LLMs), enabling multiple individual LLMs to work together for collaborative inference. This collaborative approach results in improved responses to user prompts compared to relying on a single LLM. In this paper, we consider such an MoA architecture in a distributed setting, where LLMs operate on individual edge devices, each uniquely associated with a user and equipped with its own distributed computing power. These devices exchange information using decentralized gossip algorithms, allowing different device nodes to talk without the supervision of a centralized server. In the considered setup, different users have their own LLM models to address user prompts. Additionally, the devices gossip either their own user-specific prompts or augmented prompts to generate more refined answers to certain queries. User prompts are temporarily stored in the device queues when their corresponding LLMs are busy. Given the memory limitations of edge devices, it is crucial to ensure that the average queue sizes in the system remain bounded. In this paper, we address this by theoretically calculating the queuing stability conditions for the device queues under reasonable assumptions, which we validate experimentally as well. Further, we demonstrate through experiments, leveraging open-source LLMs for the implementation of distributed MoA, that certain MoA configurations produce higher-quality responses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The implementation is available at: https://github.com/purbeshmitra/distributed_moa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21200v1</guid>
      <category>cs.IT</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Purbesh Mitra, Priyanka Kaswan, Sennur Ulukus</dc:creator>
    </item>
    <item>
      <title>Delay and Power consumption Analysis for Queue State Dependent Service Rate Control in WirelessHart System</title>
      <link>https://arxiv.org/abs/2103.13306</link>
      <description>arXiv:2103.13306v2 Announce Type: replace 
Abstract: To solve the problem of power supply limitation of machines working in wireless industry automation, we evaluated the workload aware service rate control design implanted in the medium access control component of these small devices and proposed a bio-intelligence based algorithm to optimise the design regarding the delay constraint while minimizing power consumption. To achieve this, we provide an accurate analysis of the delay cost of this design and for the first time pinpoint an exact departure process model in order to evaluate the overall delay cost in consideration of the medium access time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.13306v2</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dibyajyoti Guha, Jie Chen, Abhijit Dutta Banik, Biplab Sikdar</dc:creator>
    </item>
    <item>
      <title>Adaptive Resource Allocation for Virtualized Base Stations in O-RAN with Online Learning</title>
      <link>https://arxiv.org/abs/2309.01730</link>
      <description>arXiv:2309.01730v2 Announce Type: replace 
Abstract: Open Radio Access Network systems, with their virtualized base stations (vBSs), offer operators the benefits of increased flexibility, reduced costs, vendor diversity, and interoperability. Optimizing the allocation of resources in a vBS is challenging since it requires knowledge of the environment, (i.e., "external'' information), such as traffic demands and channel quality, which is difficult to acquire precisely over short intervals of a few seconds. To tackle this problem, we propose an online learning algorithm that balances the effective throughput and vBS energy consumption, even under unforeseeable and "challenging'' environments; for instance, non-stationary or adversarial traffic demands. We also develop a meta-learning scheme, which leverages the power of other algorithmic approaches, tailored for more "easy'' environments, and dynamically chooses the best performing one, thus enhancing the overall system's versatility and effectiveness. We prove the proposed solutions achieve sub-linear regret, providing zero average optimality gap even in challenging environments. The performance of the algorithms is evaluated with real-world data and various trace-driven evaluations, indicating savings of up to 64.5% in the power consumption of a vBS compared with state-of-the-art benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.01730v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TCOMM.2024.3461569</arxiv:DOI>
      <dc:creator>Michail Kalntis, George Iosifidis, Fernando A. Kuipers</dc:creator>
    </item>
    <item>
      <title>Routing and Spectrum Allocation in Broadband Quantum Entanglement Distribution</title>
      <link>https://arxiv.org/abs/2404.08744</link>
      <description>arXiv:2404.08744v3 Announce Type: replace 
Abstract: We investigate resource allocation for quantum entanglement distribution over an optical network. We characterize and model a network architecture that employs a single broadband quasi-deterministic time-frequency heralded Einstein-Podolsky-Rosen (EPR) pair source, and develop a routing and spectrum allocation scheme for distributing entangled photon pairs over such a network. As our setting allows separately solving the routing and spectrum allocation problems, we first find an optimal polynomial-time routing algorithm. We then employ max-min fairness criterion for spectrum allocation, which presents an NP-hard problem. Thus, we focus on approximately-optimal schemes. We compare their performance by evaluating the max-min and median number of EPR-pair rates assigned by them, and the associated Jain index. We identify two polynomial-time approximation algorithms that perform well, or better than others under these metrics. We also investigate scalability by analyzing how the network size and connectivity affect performance using Watts-Strogatz random graphs. We find that a spectrum allocation approach that achieves higher minimum EPR-pair rate can perform significantly worse when the median EPR-pair rate, Jain index, and computational resources are considered. Additionally, we evaluate the effect of the source node placement on the performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08744v3</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <category>quant-ph</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohan Bali, Ashley N. Tittelbaugh, Shelbi L. Jenkins, Anuj Agrawal, Jerry Horgan, Marco Ruffini, Daniel C. Kilper, Boulat A. Bash</dc:creator>
    </item>
    <item>
      <title>Schedulability Analysis in Time-Sensitive Networking: A Systematic Literature Review</title>
      <link>https://arxiv.org/abs/2407.15031</link>
      <description>arXiv:2407.15031v2 Announce Type: replace 
Abstract: Time-Sensitive Networking (TSN) is a set of standards that provide low-latency, high-reliability guarantees for the transmission of traffic in networks, and it is becoming an accepted solution for complex time-critical systems such as those in industrial automation and the automotive. In time-critical systems, it is essential to verify the timing predictability of the system, and the application of scheduling mechanisms in TSN can also bring about changes in system timing. Therefore, schedulability analysis techniques can be used to verify that the system is scheduled according to the scheduling mechanism and meets the timing requirements. In this paper, we provide a clear overview of the state-of-the-art works on the topic of schedulability analysis in TSN in an attempt to clarify the purpose of schedulability analysis, categorize the methods of schedulability analysis and compare their respective strengths and weaknesses, point out the scheduling mechanisms under analyzing and the corresponding traffic classes, clarify the network scenarios constructed during the evaluation and list the challenges and directions still needing to be worked on in schedulability analysis in TSN. To this end, we conducted a systematic literature review and finally identified 123 relevant research papers published in major conferences and journals in the past 15 years. Based on a comprehensive review of the relevant literature, we have identified several key findings and emphasized the future challenges in schedulability analysis for TSN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15031v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zitong Wang, Feng Luo, Yunpeng Li, Haotian Gan, Lei Zhu</dc:creator>
    </item>
    <item>
      <title>Federated Deep Reinforcement Learning-Based Intelligent Channel Access in Dense Wi-Fi Deployments</title>
      <link>https://arxiv.org/abs/2409.01004</link>
      <description>arXiv:2409.01004v2 Announce Type: replace 
Abstract: The IEEE 802.11 MAC layer utilizes the Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) mechanism for channel contention, but dense Wi-Fi deployments often cause high collision rates. To address this, this paper proposes an intelligent channel contention access mechanism that combines Federated Learning (FL) and Deep Deterministic Policy Gradient (DDPG) algorithms. We introduce a training pruning strategy and a weight aggregation algorithm to enhance model efficiency and reduce MAC delay. Using the NS3-AI framework, simulations show our method reduces average MAC delay by 25.24\% in static scenarios and outperforms A-FRL and DRL by 25.72\% and 45.9\% in dynamic environments, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01004v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyang Du, Xuming Fang, Rong He, Li Yan, Liuming Lu, Chaoming Luo</dc:creator>
    </item>
    <item>
      <title>Generative AI-Enhanced Multi-Modal Semantic Communication in Internet of Vehicles: System Design and Methodologies</title>
      <link>https://arxiv.org/abs/2409.15642</link>
      <description>arXiv:2409.15642v2 Announce Type: replace 
Abstract: Vehicle-to-everything (V2X) communication supports numerous tasks, from driving safety to entertainment services. To achieve a holistic view, vehicles are typically equipped with multiple sensors to compensate for undetectable blind spots. However, processing large volumes of multi-modal data increases transmission load, while the dynamic nature of vehicular networks adds to transmission instability. To address these challenges, we propose a novel framework, Generative Artificial intelligence (GAI)-enhanced multi-modal semantic communication (SemCom), referred to as G-MSC, designed to handle various vehicular network tasks by employing suitable analog or digital transmission. GAI presents a promising opportunity to transform the SemCom framework by significantly enhancing semantic encoding to facilitate the optimized integration of multi-modal information, enhancing channel robustness, and fortifying semantic decoding against noise interference. To validate the effectiveness of the G-MSC framework, we conduct a case study showcasing its performance in vehicular communication networks for predictive tasks. The experimental results show that the design achieves reliable and efficient communication in V2X networks. In the end, we present future research directions on G-MSC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15642v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayi Lu, Wanting Yang, Zehui Xiong, Chengwen Xing, Rahim Tafazolli, Tony Q. S. Quek, Merouane Debbah</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Wireless Networks: An Overview from the Prompt Engineering Perspective</title>
      <link>https://arxiv.org/abs/2411.04136</link>
      <description>arXiv:2411.04136v2 Announce Type: replace 
Abstract: Recently, large language models (LLMs) have been successfully applied to many fields, showing outstanding comprehension and reasoning capabilities. Despite their great potential, LLMs usually require dedicated pre-training and fine-tuning for domain-specific applications such as wireless networks. These adaptations can be extremely demanding for computational resources and datasets, while most network devices have limited computation power, and there are a limited number of high-quality networking datasets. To this end, this work explores LLM-enabled wireless networks from the prompt engineering perspective, i.e., designing prompts to guide LLMs to generate desired output without updating LLM parameters. Compared with other LLM-driven methods, prompt engineering can better align with the demands of wireless network devices, e.g., higher deployment flexibility, rapid response time, and lower requirements on computation power. In particular, this work first introduces LLM fundamentals and compares different prompting techniques such as in-context learning, chain-of-thought, and self-refinement. Then we propose two novel prompting schemes for network applications: iterative prompting for network optimization, and self-refined prompting for network prediction. The case studies show that the proposed schemes can achieve comparable performance as conventional machine learning techniques, and our proposed prompting-based methods avoid the complexity of dedicated model training and fine-tuning, which is one of the key bottlenecks of existing machine learning techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04136v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Hao Zhou, Chengming Hu, Dun Yuan, Ye Yuan, Di Wu, Xi Chen, Hina Tabassum, Xue Liu</dc:creator>
    </item>
    <item>
      <title>UAV Communications: Impact of Obstacles on Channel Characteristics</title>
      <link>https://arxiv.org/abs/2412.17934</link>
      <description>arXiv:2412.17934v2 Announce Type: replace 
Abstract: In recent years, Unmanned Aerial Vehicles (UAVs) have been utilized as effective platforms for carrying Wi-Fi Access Points (APs) and cellular Base Stations (BSs), enabling low-cost, agile, and flexible wireless networks with high Quality of Service (QoS). The next generation of wireless communications will rely on increasingly higher frequencies, which are easily obstructed by obstacles. One of the most critical concepts yet to be fully addressed is positioning the UAV at optimal coordinates while accounting for obstacles. To ensure a line of sight (LoS) between UAVs and user equipment (UE), improve QoS, and establish reliable wireless links with maximum coverage, obstacles must be integrated into the proposed placement algorithms. This paper introduces a simulation-based measurement approach for characterizing an air-to-ground (AG) channel in a simple scenario. By considering obstacles, we present a novel perspective on channel characterization. The results, in terms of throughput, packet delivery, packet loss, and delay, are compared using the proposed positioning approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17934v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kamal Shayegan</dc:creator>
    </item>
    <item>
      <title>Hybrid LLM-DDQN based Joint Optimization of V2I Communication and Autonomous Driving</title>
      <link>https://arxiv.org/abs/2410.08854</link>
      <description>arXiv:2410.08854v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have received considerable interest recently due to their outstanding reasoning and comprehension capabilities. This work explores applying LLMs to vehicular networks, aiming to jointly optimize vehicle-to-infrastructure (V2I) communications and autonomous driving (AD) policies. We deploy LLMs for AD decision-making to maximize traffic flow and avoid collisions for road safety, and a double deep Q-learning algorithm (DDQN) is used for V2I optimization to maximize the received data rate and reduce frequent handovers. In particular, for LLM-enabled AD, we employ the Euclidean distance to identify previously explored AD experiences, and then LLMs can learn from past good and bad decisions for further improvement. Then, LLM-based AD decisions will become part of states in V2I problems, and DDQN will optimize the V2I decisions accordingly. After that, the AD and V2I decisions are iteratively optimized until convergence. Such an iterative optimization approach can better explore the interactions between LLMs and conventional reinforcement learning techniques, revealing the potential of using LLMs for network optimization and management. Finally, the simulations demonstrate that our proposed hybrid LLM-DDQN approach outperforms the conventional DDQN algorithm, showing faster convergence and higher average rewards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08854v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zijiang Yan, Hao Zhou, Hina Tabassum, Xue Liu</dc:creator>
    </item>
  </channel>
</rss>
