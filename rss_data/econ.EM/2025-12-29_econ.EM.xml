<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Dec 2025 05:00:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>US labor market conditions and migration: a reassessment of Bahar (2025)</title>
      <link>https://arxiv.org/abs/2512.21429</link>
      <description>arXiv:2512.21429v1 Announce Type: new 
Abstract: Bahar (2025) argues that there is a long-term cointegrating relationship between US job vacancies and southwest border crossings. We show that this conclusion is based on a misspecified Engle-Granger test applied to first differences. Once the Engle-Granger test is correctly applied to levels, evidence for a cointegrating relationship vanishes, invalidating the paper's approach to estimating short- and long-run elasticities. Bahar's approach is therefore uninformative about the relationship between US labor market conditions and migration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21429v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco Rodriguez, Giancarlo Bravo</dc:creator>
    </item>
    <item>
      <title>Nonparametric methods for comparing distribution functionals for dependent samples with application to inequality measures</title>
      <link>https://arxiv.org/abs/2512.21862</link>
      <description>arXiv:2512.21862v1 Announce Type: new 
Abstract: This paper proposes asymptotically distribution-free inference methods for comparing a broad range of welfare indices across dependent samples, including those employed in inequality, poverty, and risk analysis. Two distinct situations are considered. \emph{First}, we propose asymptotic and bootstrap intersection methods which are completely robust to arbitrary dependence between two samples. \emph{Second}, we focus on the common case of overlapping samples -- a special form of dependent samples where sample dependence arises solely from matched pairs -- and provide asymptotic and bootstrap methods for comparing indices. We derive consistent estimates for asymptotic variances using the influence function approach. The performance of the proposed methods is studied in a simulation experiment: we find that confidence intervals with overlapping samples exhibit satisfactory coverage rates with reasonable precision, whereas conventional methods based on an assumption of independent samples have an inferior performance in terms of coverage rates and interval widths. Asymptotic inference can be less reliable when dealing with heavy-tailed distributions, while the bootstrap method provides a viable remedy, unless the variance is substantial or nonexistent. The intersection method yields reliable results with arbitrary dependent samples, including instances where overlapping samples are not feasible. We demonstrate the practical applicability of our proposed methods in analyzing dynamic changes in household financial inequality in Italy over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21862v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-Marie Dufour, Tianyu He</dc:creator>
    </item>
    <item>
      <title>Semiparametric Preference Optimization: Your Language Model is Secretly a Single-Index Model</title>
      <link>https://arxiv.org/abs/2512.21917</link>
      <description>arXiv:2512.21917v1 Announce Type: cross 
Abstract: Aligning large language models to preference data is commonly implemented by assuming a known link function between the distribution of observed preferences and the unobserved rewards (e.g., a logistic link as in Bradley-Terry). If the link is wrong, however, inferred rewards can be biased and policies be misaligned. We study policy alignment to preferences under an unknown and unrestricted link. We consider an $f$-divergence-constrained reward maximization problem and show that realizability of the solution in a policy class implies a semiparametric single-index binary choice model, where a scalar-valued index determined by a policy captures the dependence on demonstrations and the rest of the preference distribution is an unrestricted function thereof. Rather than focus on estimation of identifiable finite-dimensional structural parameters in the index as in econometrics, we focus on policy learning, focusing on error to the optimal policy and allowing unidentifiable and nonparametric indices. We develop a variety of policy learners based on profiling the link function, orthogonalizing the link function, and using link-agnostic bipartite ranking objectives. We analyze these and provide finite-sample policy error bounds that depend on generic functional complexity measures of the index class. We further consider practical implementations using first-order optimization suited to neural networks and batched data. The resulting methods are robust to unknown preference noise distribution and scale, while preserving the direct optimization of policies without explicitly fitting rewards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21917v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Robustness to missing data: breakdown point analysis</title>
      <link>https://arxiv.org/abs/2406.06804</link>
      <description>arXiv:2406.06804v2 Announce Type: replace 
Abstract: Missing data is pervasive in econometric applications, and rarely is it plausible that the data are missing (completely) at random. This paper proposes a methodology for studying the robustness of results drawn from incomplete datasets. Selection is measured as the divergence from the distribution of complete observations to the distribution of incomplete observations. The breakdown point is defined as the minimal amount of selection needed to overturn a given result. Reporting point estimates and lower confidence intervals of the breakdown point is a simple, concise way to communicate the robustness of a result. An estimator of the breakdown point is proposed and shown root-n consistent and asymptotically normal. This estimator can be applied directly to conclusions drawn from any model identified with the generalized method of moments (GMM) that satisfies mild assumptions. Simulations demonstrate the finite sample performance of the breakdown point estimator on averages, linear regression, and logistic regression. The methodology is illustrated by estimating the breakdown point of conclusions drawn from several randomized controlled trails suffering from missing data due to attrition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06804v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jeconom.2025.106151</arxiv:DOI>
      <arxiv:journal_reference>Journal of Econometrics, Volume 253, January 2026, 106151</arxiv:journal_reference>
      <dc:creator>Daniel Ober-Reynolds</dc:creator>
    </item>
    <item>
      <title>Optimal estimation for regression discontinuity design with binary outcomes</title>
      <link>https://arxiv.org/abs/2509.18857</link>
      <description>arXiv:2509.18857v2 Announce Type: replace 
Abstract: We develop a finite-sample optimal estimator for regression discontinuity design when the outcomes are bounded, including binary outcomes as the leading case. Our estimator achieves minimax mean squared error among linear shrinkage estimators with nonnegative weights when the regression function lies in a Lipschitz class. Although the original minimax problem involves an iterative noncovex optimization problem, we show that our estimator is obtained by solving a convex optimization problem. A key advantage of the proposed estimator is that the Lipschitz constant is its only tuning parameter. We also propose a uniformly valid inference procedure without a large-sample approximation. In a simulation exercise for small samples, our estimator exhibits smaller mean squared errors and shorter confidence intervals than those of conventional large-sample techniques. In an empirical multi-cutoff design in which the sample size for each cutoff is small, our method yields informative confidence intervals, in contrast to the leading large-sample approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18857v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Ishihara, Masayuki Sawada, Kohei Yata</dc:creator>
    </item>
    <item>
      <title>Branching Fixed Effects: A Proposal for Communicating Uncertainty</title>
      <link>https://arxiv.org/abs/2512.08101</link>
      <description>arXiv:2512.08101v2 Announce Type: replace 
Abstract: Economists often rely on estimates of linear fixed effects models produced by other teams of researchers. Assessing the uncertainty in these estimates can be challenging. I propose a form of sample splitting for networks that partitions the data into statistically independent branches, each of which can be used to compute an unbiased estimate of the parameters of interest in two-way fixed effects models. These branches facilitate uncertainty quantification, moment estimation, and shrinkage. Drawing on results from the graph theory literature on tree packing, I develop algorithms to efficiently extract branches from large networks. I illustrate these techniques using a benchmark dataset from Veneto, Italy that has been widely used to study firm wage effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08101v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Kline</dc:creator>
    </item>
    <item>
      <title>Treatment Effects with Correlated Spillovers: Bridging Discrete and Continuous Methods</title>
      <link>https://arxiv.org/abs/2512.12653</link>
      <description>arXiv:2512.12653v4 Announce Type: replace 
Abstract: This paper develops a continuous functional framework for treatment effects propagating through geographic space and economic networks. We derive a master equation from three independent economic foundations -- heterogeneous agent aggregation, market equilibrium, and cost minimization -- establishing that the framework rests on fundamental principles rather than ad hoc specifications. The framework nests conventional econometric models -- autoregressive specifications, spatial autoregressive models, and network treatment effect models -- as special cases, providing a bridge between discrete and continuous methods. A key theoretical result shows that the spatial-network interaction coefficient equals the mutual information between geographic and network coordinates, providing a parameter-free measure of channel complementarity. The Feynman-Kac representation characterizes treatment effects as accumulated policy exposure along stochastic paths representing economic linkages, connecting the continuous framework to event study methodology. The no-spillover case emerges as a testable restriction, creating a one-sided risk profile where correct inference is maintained regardless of whether spillovers exist. Monte Carlo simulations confirm that conventional estimators exhibit 25-38% bias when spillovers are present, while our estimator maintains correct inference across all configurations including the no-spillover case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12653v4</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.ME</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Inference in partially identified moment models via regularized optimal transport</title>
      <link>https://arxiv.org/abs/2512.18084</link>
      <description>arXiv:2512.18084v2 Announce Type: replace 
Abstract: Partial identification often arises when the joint distribution of the data is known only up to its marginals. We consider the corresponding partially identified GMM model and develop a methodology for identification, estimation, and inference in this model. We characterize the sharp identified set for the parameter of interest via a support-function/optimal-transport (OT) representation. For estimation, we employ entropic regularization, which provides a smooth approximation to classical OT and can be computed efficiently by the Sinkhorn algorithm. We also propose a statistic for testing hypotheses and constructing confidence regions for the identified set. To derive the asymptotic distribution of this statistic, we establish a novel central limit theorem for the entropic OT value under general smooth costs. We then obtain valid critical values using the bootstrap for directionally differentiable functionals of Fang and Santos (2019). The resulting testing procedure controls size locally uniformly, including at parameter values on the boundary of the identified set. We illustrate its performance in a Monte Carlo simulation. Our methodology is applicable to a wide range of empirical settings, such as panels with attrition and refreshment samples, nonlinear treatment effects, nonparametric instrumental variables without large-support conditions, and Euler equations with repeated cross-sections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18084v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigory Franguridi, Laura Liu</dc:creator>
    </item>
    <item>
      <title>Efficient estimation of average treatment effects with unmeasured confounding and proxies</title>
      <link>https://arxiv.org/abs/2501.02214</link>
      <description>arXiv:2501.02214v2 Announce Type: replace-cross 
Abstract: Proximal causal inference provides a framework for estimating the average treatment effect (ATE) in the presence of unmeasured confounding by leveraging outcome and treatment proxies. Identification in this framework relies on the existence of a so-called bridge function. Standard approaches typically postulate a parametric specification for the bridge function, which is estimated in a first step and then plugged into an ATE estimator. However, this sequential procedure suffers from two potential sources of efficiency loss: (i) the difficulty of efficiently estimating a bridge function defined by an integral equation, and (ii) the failure to account for the correlation between the estimation steps. To overcome these limitations, we propose a novel approach that approximates the integral equation with increasing moment restrictions and jointly estimates the bridge function and the ATE. We show that, under suitable conditions, our estimator is efficient. Additionally, we provide a data-driven procedure for selecting the tuning parameter (i.e., the number of moment restrictions). Simulation studies reveal that the proposed method performs well in finite samples, and an application to the right heart catheterization dataset from the SUPPORT study demonstrates its practical value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02214v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5705/ss.202025.0104</arxiv:DOI>
      <arxiv:journal_reference>Statistica Sinica (2025)</arxiv:journal_reference>
      <dc:creator>Chunrong Ai, Jiawei Shan</dc:creator>
    </item>
  </channel>
</rss>
