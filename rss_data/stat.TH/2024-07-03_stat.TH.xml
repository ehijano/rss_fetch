<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Jul 2024 01:50:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Asymptotic tests for monotonicity and convexity of a probability mass function</title>
      <link>https://arxiv.org/abs/2407.01751</link>
      <description>arXiv:2407.01751v1 Announce Type: new 
Abstract: In shape-constrained nonparametric inference, it is often necessary to perform preliminary tests to verify whether a probability mass function (p.m.f.) satisfies qualitative constraints such as monotonicity, convexity or in general $k$-monotonicity. In this paper, we are interested in testing $k$-monotonicity of a compactly supported p.m.f. and we put our main focus on monotonicity and convexity; i.e., $k \in \{1,2\}$. We consider new testing procedures that are directly derived from the definition of $k$-monotonicity and rely exclusively on the empirical measure, as well as tests that are based on the projection of the empirical measure on the class of $k$-monotone p.m.f.s. The asymptotic behaviour of the introduced test statistics is derived and a simulation study is performed to assess the finite sample performance of all the proposed tests. Applications to real datasets are presented to illustrate the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01751v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fadoua Balabdaoui, Antonio Di Noia</dc:creator>
    </item>
    <item>
      <title>Simultaneous semiparametric inference for single-index models</title>
      <link>https://arxiv.org/abs/2407.01874</link>
      <description>arXiv:2407.01874v1 Announce Type: new 
Abstract: In the common partially linear single-index model we establish a Bahadur representation for a smoothing spline estimator of all model parameters and use this result to prove the joint weak convergence of the estimator of the index link function at a given point, together with the estimators of the parametric regression coefficients. We obtain the surprising result that, despite of the nature of single-index models where the link function is evaluated at a linear combination of the index-coefficients, the estimator of the link function and the estimator of the index-coefficients are asymptotically independent. Our approach leverages a delicate analysis based on reproducing kernel Hilbert space and empirical process theory.
  We show that the smoothing spline estimator achieves the minimax optimal rate with respect to the $L^2$-risk and consider several statistical applications where joint inference on all model parameters is of interest. In particular, we develop a simultaneous confidence band for the link function and propose inference tools to investigate if the maximum absolute deviation between the (unknown) link function and a given function exceeds a given threshold. We also construct tests for joint hypotheses regarding model parameters which involve both the nonparametric and parametric components and propose novel multiplier bootstrap procedures to avoid the estimation of unknown asymptotic quantities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01874v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiajun Tang, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Asymptotics of estimators for structured covariance matrices</title>
      <link>https://arxiv.org/abs/2407.01974</link>
      <description>arXiv:2407.01974v1 Announce Type: new 
Abstract: We show that the limiting variance of a sequence of estimators for a structured covariance matrix has a general form that appears as the variance of a scaled projection of a random matrix that is of radial type and a similar result is obtained for the corresponding sequence of estimators for the vector of variance components. These results are illustrated by the limiting behavior of estimators for a linear covariance structure in a variety of multivariate statistical models. We also derive a characterization for the influence function of corresponding functionals. Furthermore, we derive the limiting distribution and influence function of scale invariant mappings of such estimators and their corresponding functionals. As a consequence, the asymptotic relative efficiency of different estimators for the shape component of a structured covariance matrix can be compared by means of a single scalar and the gross error sensitivity of the corresponding influence functions can be compared by means of a single index. Similar results are obtained for estimators of the normalized vector of variance components. We apply our results to investigate how the efficiency, gross error sensitivity, and breakdown point of S-estimators for the normalized variance components are affected simultaneously by varying their cutoff value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01974v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hendrik Paul Lopuha\"a</dc:creator>
    </item>
    <item>
      <title>Stein's Method of Moments on the Sphere</title>
      <link>https://arxiv.org/abs/2407.02299</link>
      <description>arXiv:2407.02299v1 Announce Type: new 
Abstract: We use Stein characterizations to obtain new moment-type estimators for the parameters of three classical spherical distributions (namely the Fisher-Bingham, the von Mises-Fisher, and the Watson distributions) in the i.i.d. case. This leads to explicit estimators which have good asymptotic properties (close to efficiency) and therefore lead to interesting alternatives to classical maximum likelihood methods or more recent score matching estimators. We perform competitive simulation studies to assess the quality of the new estimators. Finally, the practical relevance of our estimators is illustrated on a real data application in spherical latent representations of handwritten numbers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02299v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrian Fischer, Robert E. Gaunt, Yvik Swan</dc:creator>
    </item>
    <item>
      <title>Contrastive independent component analysis</title>
      <link>https://arxiv.org/abs/2407.02357</link>
      <description>arXiv:2407.02357v1 Announce Type: new 
Abstract: Visualizing data and finding patterns in data are ubiquitous problems in the sciences. Increasingly, applications seek signal and structure in a contrastive setting: a foreground dataset relative to a background dataset. For this purpose, we propose contrastive independent component analysis (cICA). This generalizes independent component analysis to independent latent variables across a foreground and background. We propose a hierarchical tensor decomposition algorithm for cICA. We study the identifiability of cICA and demonstrate its performance visualizing data and finding patterns in data, using synthetic and real-world datasets, comparing the approach to existing contrastive methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02357v1</guid>
      <category>math.ST</category>
      <category>math.AG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kexin Wang, Aida Maraj, Anna Seigal</dc:creator>
    </item>
    <item>
      <title>Statistical Advantages of Oblique Randomized Decision Trees and Forests</title>
      <link>https://arxiv.org/abs/2407.02458</link>
      <description>arXiv:2407.02458v1 Announce Type: new 
Abstract: This work studies the statistical advantages of using features comprised of general linear combinations of covariates to partition the data in randomized decision tree and forest regression algorithms. Using random tessellation theory in stochastic geometry, we provide a theoretical analysis of a class of efficiently generated random tree and forest estimators that allow for oblique splits along such features. We call these estimators oblique Mondrian trees and forests, as the trees are generated by first selecting a set of features from linear combinations of the covariates and then running a Mondrian process that hierarchically partitions the data along these features. Generalization error bounds and convergence rates are obtained for the flexible dimension reduction model class of ridge functions (also known as multi-index models), where the output is assumed to depend on a low dimensional relevant feature subspace of the input domain. The results highlight how the risk of these estimators depends on the choice of features and quantify how robust the risk is with respect to error in the estimation of relevant features. The asymptotic analysis also provides conditions on the selected features along which the data is split for these estimators to obtain minimax optimal rates of convergence with respect to the dimension of the relevant feature subspace. Additionally, a lower bound on the risk of axis-aligned Mondrian trees (where features are restricted to the set of covariates) is obtained proving that these estimators are suboptimal for these linear dimension reduction models in general, no matter how the distribution over the covariates used to divide the data at each tree node is weighted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02458v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eliza O'Reilly</dc:creator>
    </item>
    <item>
      <title>Model Identifiability for Bivariate Failure Time Data with Competing Risks: Parametric Cause-specific Hazards and Non-parametric Frailty</title>
      <link>https://arxiv.org/abs/2407.01631</link>
      <description>arXiv:2407.01631v1 Announce Type: cross 
Abstract: One of the commonly used approaches to capture dependence in multivariate survival data is through the frailty variables. The identifiability issues should be carefully investigated while modeling multivariate survival with or without competing risks. The use of non-parametric frailty distribution(s) is sometimes preferred for its robustness and flexibility properties. In this paper, we consider modeling of bivariate survival data with competing risks through four different kinds of non-parametric frailty and parametric baseline cause-specific hazard functions to investigate the corresponding model identifiability. We make the common assumption of the frailty mean being equal to unity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01631v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Biswadeep Ghosh, Anup Dewanji, Sudipta Das</dc:creator>
    </item>
    <item>
      <title>Entropic Optimal Transport Eigenmaps for Nonlinear Alignment and Joint Embedding of High-Dimensional Datasets</title>
      <link>https://arxiv.org/abs/2407.01718</link>
      <description>arXiv:2407.01718v1 Announce Type: cross 
Abstract: Embedding high-dimensional data into a low-dimensional space is an indispensable component of data analysis. In numerous applications, it is necessary to align and jointly embed multiple datasets from different studies or experimental conditions. Such datasets may share underlying structures of interest but exhibit individual distortions, resulting in misaligned embeddings using traditional techniques. In this work, we propose \textit{Entropic Optimal Transport (EOT) eigenmaps}, a principled approach for aligning and jointly embedding a pair of datasets with theoretical guarantees. Our approach leverages the leading singular vectors of the EOT plan matrix between two datasets to extract their shared underlying structure and align the datasets accordingly in a common embedding space. We interpret our approach as an inter-data variant of the classical Laplacian eigenmaps and diffusion maps embeddings, showing that it enjoys many favorable analogous properties. We then analyze a data-generative model where two observed high-dimensional datasets share latent variables on a common low-dimensional manifold, but each dataset is subject to data-specific translation, scaling, nuisance structures, and noise. We show that in a high-dimensional asymptotic regime, the EOT plan recovers the shared manifold structure by approximating a kernel function evaluated at the locations of the latent variables. Subsequently, we provide a geometric interpretation of our embedding by relating it to the eigenfunctions of population-level operators encoding the density and geometry of the shared manifold. Finally, we showcase the performance of our approach for data integration and embedding through simulations and analyses of real-world biological data, demonstrating its advantages over alternative methods in challenging scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01718v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boris Landa, Yuval Kluger, Rong Ma</dc:creator>
    </item>
    <item>
      <title>Conditionally valid Probabilistic Conformal Prediction</title>
      <link>https://arxiv.org/abs/2407.01794</link>
      <description>arXiv:2407.01794v1 Announce Type: cross 
Abstract: We develop a new method for creating prediction sets that combines the flexibility of conformal methods with an estimate of the conditional distribution $P_{Y \mid X}$. Most existing methods, such as conformalized quantile regression and probabilistic conformal prediction, only offer marginal coverage guarantees. Our approach extends these methods to achieve conditional coverage, which is essential for many practical applications. While exact conditional guarantees are impossible without assumptions on the data distribution, we provide non-asymptotic bounds that explicitly depend on the quality of the available estimate of the conditional distribution. Our confidence sets are highly adaptive to the local structure of the data, making them particularly useful in high heteroskedasticity situations. We demonstrate the effectiveness of our approach through extensive simulations, showing that it outperforms existing methods in terms of conditional coverage and improves the reliability of statistical inference in a wide range of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01794v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Plassier, Alexander Fishkov, Maxim Panov, Eric Moulines</dc:creator>
    </item>
    <item>
      <title>Attack-Aware Noise Calibration for Differential Privacy</title>
      <link>https://arxiv.org/abs/2407.02191</link>
      <description>arXiv:2407.02191v1 Announce Type: cross 
Abstract: Differential privacy (DP) is a widely used approach for mitigating privacy risks when training machine learning models on sensitive data. DP mechanisms add noise during training to limit the risk of information leakage. The scale of the added noise is critical, as it determines the trade-off between privacy and utility. The standard practice is to select the noise scale in terms of a privacy budget parameter $\epsilon$. This parameter is in turn interpreted in terms of operational attack risk, such as accuracy, or sensitivity and specificity of inference attacks against the privacy of the data. We demonstrate that this two-step procedure of first calibrating the noise scale to a privacy budget $\epsilon$, and then translating $\epsilon$ to attack risk leads to overly conservative risk assessments and unnecessarily low utility. We propose methods to directly calibrate the noise scale to a desired attack risk level, bypassing the intermediate step of choosing $\epsilon$. For a target attack risk, our approach significantly decreases noise scale, leading to increased utility at the same level of privacy. We empirically demonstrate that calibrating noise to attack sensitivity/specificity, rather than $\epsilon$, when training privacy-preserving ML models substantially improves model accuracy for the same risk level. Our work provides a principled and practical way to improve the utility of privacy-preserving ML without compromising on privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02191v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bogdan Kulynych, Juan Felipe Gomez, Georgios Kaissis, Flavio du Pin Calmon, Carmela Troncoso</dc:creator>
    </item>
    <item>
      <title>Asymptotic expansion of a Hurst index estimator for a stochastic differential equation driven by fBm</title>
      <link>https://arxiv.org/abs/2407.02254</link>
      <description>arXiv:2407.02254v1 Announce Type: cross 
Abstract: We study the asymptotic properties of an estimator of Hurst parameter of a stochastic differential equation driven by a fractional Brownian motion with $H &gt; 1/2$. Utilizing the theory of asymptotic expansion of Skorohod integrals introduced by Nualart and Yoshida [NY19], we derive an asymptotic expansion formula of the distribution of the estimator. As an corollary, we also obtain a mixed central limit theorem for the statistic, indicating that the rate of convergence is $n^{-\frac12}$, which improves the results in the previous literature. To handle second-order quadratic variations appearing in the estimator, a theory of exponent has been developed based on weighted graphs to estimate asymptotic orders of norms of functionals involved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02254v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hayate Yamagishi</dc:creator>
    </item>
    <item>
      <title>Gordon Growth Model with Vector Autoregressive Process</title>
      <link>https://arxiv.org/abs/2406.19424</link>
      <description>arXiv:2406.19424v2 Announce Type: replace 
Abstract: In this study, we introduce a Gordon's dividend discount model, based on Vector Autoregressive Process (VAR). We provide two Propositions, which are related to generic Gordon growth model and Gordon growth model, which is based on the VAR process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19424v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Battulga Gankhuu</dc:creator>
    </item>
    <item>
      <title>On minimum contrast method for multivariate spatial point processes</title>
      <link>https://arxiv.org/abs/2208.07044</link>
      <description>arXiv:2208.07044v3 Announce Type: replace-cross 
Abstract: Compared to widely used likelihood-based approaches, the minimum contrast (MC) method offers a computationally efficient method for estimation and inference of spatial point processes. These relative gains in computing time become more pronounced when analyzing complicated multivariate point process models. Despite this, there has been little exploration of the MC method for multivariate spatial point processes. Therefore, this article introduces a new MC method for parametric multivariate spatial point processes. A contrast function is computed based on the trace of the power of the difference between the conjectured $K$-function matrix and its nonparametric unbiased edge-corrected estimator. Under standard assumptions, we derive the asymptotic normality of our MC estimator. The performance of the proposed method is demonstrated through simulation studies of bivariate log-Gaussian Cox processes and five-variate product-shot-noise Cox processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.07044v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Zhu, Junho Yang, Mikyoung Jun, Scott Cook</dc:creator>
    </item>
    <item>
      <title>Quantitative limit theorems and bootstrap approximations for empirical spectral projectors</title>
      <link>https://arxiv.org/abs/2208.12871</link>
      <description>arXiv:2208.12871v2 Announce Type: replace-cross 
Abstract: Given finite i.i.d.~samples in a Hilbert space with zero mean and trace-class covariance operator $\Sigma$, the problem of recovering the spectral projectors of $\Sigma$ naturally arises in many applications. In this paper, we consider the problem of finding distributional approximations of the spectral projectors of the empirical covariance operator $\hat \Sigma$, and offer a dimension-free framework where the complexity is characterized by the so-called relative rank of $\Sigma$. In this setting, novel quantitative limit theorems and bootstrap approximations are presented subject only to mild conditions in terms of moments and spectral decay. In many cases, these even improve upon existing results in a Gaussian setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.12871v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Jirak, Martin Wahl</dc:creator>
    </item>
    <item>
      <title>On the asymptotics of extremal lp-blocks cluster inference</title>
      <link>https://arxiv.org/abs/2212.13521</link>
      <description>arXiv:2212.13521v3 Announce Type: replace-cross 
Abstract: Extremes occur in stationary regularly varying time series as short periods with several large observations, known as extremal blocks. We study cluster statistics summarizing the behavior of functions acting on these extremal blocks. Examples of cluster statistics are the extremal index, cluster size probabilities, and other cluster indices. The purpose of our work is twofold. First, we state the asymptotic normality of block estimators for cluster inference based on consecutive observations with large $\ell^p$-norms, for $p&gt; 0$.  The case $p=\alpha$, where $\alpha&gt; 0$ is the tail index of the time series, has specific nice properties thus we analyze the asymptotic of blocks estimators when approximating $\alpha$ using the Hill estimator. Second, we verify the conditions we require on classical models such as linear models and solutions of  stochastic recurrence equations. Regarding linear models, we prove that the asymptotic variance of classical index cluster-based estimators is null as first conjectured in Hsing T. [26]. We illustrate our findings on simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.13521v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gloria Buritic\'a (UNIGE), Olivier Wintenberger (LPSM)</dc:creator>
    </item>
    <item>
      <title>Doeblin Coefficients and Related Measures</title>
      <link>https://arxiv.org/abs/2309.08475</link>
      <description>arXiv:2309.08475v2 Announce Type: replace-cross 
Abstract: Doeblin coefficients are a classical tool for analyzing the ergodicity and exponential convergence rates of Markov chains. Propelled by recent works on contraction coefficients of strong data processing inequalities, we investigate whether Doeblin coefficients also exhibit some of the notable properties of canonical contraction coefficients. In this paper, we present several new structural and geometric properties of Doeblin coefficients. Specifically, we show that Doeblin coefficients form a multi-way divergence, exhibit tensorization, and possess an extremal trace characterization. We then show that they also have extremal coupling and simultaneously maximal coupling characterizations. By leveraging these characterizations, we demonstrate that Doeblin coefficients act as a nice generalization of the well-known total variation (TV) distance to a multi-way divergence, enabling us to measure the "distance" between multiple distributions rather than just two. We then prove that Doeblin coefficients exhibit contraction properties over Bayesian networks similar to other canonical contraction coefficients. We additionally derive some other results and discuss an application of Doeblin coefficients to distribution fusion. Finally, in a complementary vein, we introduce and discuss three new quantities: max-Doeblin coefficient, max-DeGroot distance, and min-DeGroot distance. The max-Doeblin coefficient shares a connection with the concept of maximal leakage in information security; we explore its properties and provide a coupling characterization. On the other hand, the max-DeGroot and min-DeGroot measures extend the concept of DeGroot distance to multiple distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08475v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2024.3367856</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Information Theory, vol. 70, no. 7, July 2024</arxiv:journal_reference>
      <dc:creator>Anuran Makur, Japneet Singh</dc:creator>
    </item>
    <item>
      <title>Inference for Rank-Rank Regressions</title>
      <link>https://arxiv.org/abs/2310.15512</link>
      <description>arXiv:2310.15512v3 Announce Type: replace-cross 
Abstract: The slope coefficient in a rank-rank regression is a popular measure of intergenerational mobility. In this article, we first show that commonly used inference methods for this slope parameter are invalid. Second, when the underlying distribution is not continuous, the OLS estimator and its asymptotic distribution may be highly sensitive to how ties in the ranks are handled. Motivated by these findings we develop a new asymptotic theory for the OLS estimator in a general class of rank-rank regression specifications without imposing any assumptions about the continuity of the underlying distribution. We then extend the asymptotic theory to other regressions involving ranks that have been used in empirical work. Finally, we apply our new inference methods to two empirical studies on intergenerational mobility, highlighting the practical implications of our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15512v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denis Chetverikov, Daniel Wilhelm</dc:creator>
    </item>
    <item>
      <title>Characteristic Learning for Provable One Step Generation</title>
      <link>https://arxiv.org/abs/2405.05512</link>
      <description>arXiv:2405.05512v3 Announce Type: replace-cross 
Abstract: We propose the characteristic generator, a novel one-step generative model that combines the efficiency of sampling in Generative Adversarial Networks (GANs) with the stable performance of flow-based models. Our model is driven by characteristics, along which the probability density transport can be described by ordinary differential equations (ODEs). Specifically, We estimate the velocity field through nonparametric regression and utilize Euler method to solve the probability flow ODE, generating a series of discrete approximations to the characteristics. We then use a deep neural network to fit these characteristics, ensuring a one-step mapping that effectively pushes the prior distribution towards the target distribution. In the theoretical aspect, we analyze the errors in velocity matching, Euler discretization, and characteristic fitting to establish a non-asymptotic convergence rate for the characteristic generator in 2-Wasserstein distance. To the best of our knowledge, this is the first thorough analysis for simulation-free one step generative models. Additionally, our analysis refines the error analysis of flow-based generative models in prior works. We apply our method on both synthetic and real datasets, and the results demonstrate that the characteristic generator achieves high generation quality with just a single evaluation of neural network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05512v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhao Ding, Chenguang Duan, Yuling Jiao, Ruoxuan Li, Jerry Zhijian Yang, Pingwen Zhang</dc:creator>
    </item>
  </channel>
</rss>
