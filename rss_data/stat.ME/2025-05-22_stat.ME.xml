<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 May 2025 04:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimal Treatment Allocations Accounting for Population Differences</title>
      <link>https://arxiv.org/abs/2505.15944</link>
      <description>arXiv:2505.15944v1 Announce Type: new 
Abstract: The treatment allocation mechanism in a randomized clinical trial can be optimized by maximizing the nonparametric efficiency bound for a specific measure of treatment effect. Optimal treatment allocations which may or may not depend on baseline covariates have been derived for a variety of effect measures focusing on the trial population, the patient population represented by the trial participants. Frequently, clinical trial data are used to estimate treatment effects in a target population that is related to but different from the trial population. This article provides optimal treatment allocations that account for the impact of such population differences. We consider three cases with different data configurations: transportation, generalization, and post-stratification. Our results indicate that, for general effect measures, optimal treatment allocations may depend on the covariate distribution in the target population but not on the configuration of data or information that describes the target covariate distribution. For estimating average treatment effects, there is a unique covariate-dependent allocation that achieves maximal efficiency regardless of the target covariate distribution and the associated data configuration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15944v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Zhang, Zhiwei Zhang, Aiyi Liu</dc:creator>
    </item>
    <item>
      <title>Quantile Predictions for Equity Premium using Penalized Quantile Regression with Consistent Variable Selection across Multiple Quantiles</title>
      <link>https://arxiv.org/abs/2505.16019</link>
      <description>arXiv:2505.16019v1 Announce Type: new 
Abstract: This paper considers equity premium prediction, for which mean regression can be problematic due to heteroscedasticity and heavy-tails of the error. We show advantages of quantile predictions using a novel penalized quantile regression that offers a model for a full spectrum analysis on the equity premium distribution. To enhance model interpretability and address the well-known issue of crossing quantile predictions in quantile regression, we propose a model that enforces the selection of a common set of variables across all quantiles. Such a selection consistency is achieved by simultaneously estimating all quantiles with a group penalty that ensures sparsity pattern is the same for all quantiles. Consistency results are provided that allow the number of predictors to increase with the sample size. A Huberized quantile loss function and an augmented data approach are implemented for computational efficiency. Simulation studies show the effectiveness of the proposed approach. Empirical results show that the proposed method outperforms several benchmark methods. Moreover, we find some important predictors reverse their relationship to the excess return from lower to upper quantiles, potentially offering interesting insights to the domain experts. Our proposed method can be applied to other fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16019v1</guid>
      <category>stat.ME</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaobo Li, Ben Sherwood</dc:creator>
    </item>
    <item>
      <title>Bayesian adaptive randomization in the I-SPY2.2 sequential multiple assignment randomized trial</title>
      <link>https://arxiv.org/abs/2505.16047</link>
      <description>arXiv:2505.16047v1 Announce Type: new 
Abstract: The I-SPY2 phase 2 clinical trial is a
  long-running platform trial evaluating neoadjuvant treatments for
  locally advanced breast cancer, assigning subjects to tumor
  subtype-specific experimental agents via a response-adaptive
  randomization algorithm that updates randomization probabilities
  based on accruing evidence of efficacy. Recently, I-SPY2 has been
  reconfigured as a sequential multiple assignment randomized trial
  (SMART), known as I-SPY2.2, in which subjects who are predicted to
  not achieve a satisfactory response to an initial assigned therapy
  are re-randomized to a second subtype-specific treatment followed by
  standard rescue therapy if a satisfactory response is not predicted.
  The I-SPY2.2 SMART thus supports evaluation of entire treatment
  regimes that dictate the choice of treatments at each stage on the
  basis of the outcome pathological complete response (pCR). The
  transition of I-SPY2 to a SMART required development of a
  trial-specific response-adaptive randomization scheme in which
  randomization probabilities at each stage are updated based on
  evolving evidence on the efficacy of full regimes, so as to skew
  probabilities toward treatments involved in regimes that the current
  evidence suggests are optimal in the sense of maximizing the
  probability of pCR. The approach uses Thompson sampling, which
  updates randomization probabilities based on the posterior
  probability that treatments are implicated in optimal regimes. We
  present the proposed algorithm and empirical studies that
  demonstrate it improves within-trial regime-specific pCR rates and
  recommends optimal regimes at similar rates relative to uniform,
  nonadaptive randomization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16047v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Norwood, Christina Yau, Denise Wolf, Anastasios Tsiatis, Marie Davidian</dc:creator>
    </item>
    <item>
      <title>Controlling the false discovery rate in high-dimensional linear models using model-X knockoffs and $p$-values</title>
      <link>https://arxiv.org/abs/2505.16124</link>
      <description>arXiv:2505.16124v1 Announce Type: new 
Abstract: In this paper, we propose novel multiple testing methods for controlling the false discovery rate (FDR) in the context of high-dimensional linear models. Our development innovatively integrates model-X knockoff techniques with debiased penalized regression estimators. The proposed approach addresses two fundamental challenges in high-dimensional statistical inference: (i) constructing valid test statistics and corresponding $p$-values in solving problems with a diverging number of model parameters, and (ii) ensuring FDR control under complex and unknown dependence structures among test statistics. A central contribution of our methodology lies in the rigorous construction and theoretical analysis of two paired sets of test statistics. Based on these test statistics, our methodology adopts two $p$-value-based multiple testing algorithms. The first applies the conventional Benjamini-Hochberg procedure, justified by the asymptotic mutual independence and normality of one set of the test statistics. The second leverages the paired structure of both sets of test statistics to improve detection power while maintaining rigorous FDR control. We provide comprehensive theoretical analysis, establishing the validity of the debiasing framework and ensuring that the proposed methods achieve proper FDR control. Extensive simulation studies demonstrate that our procedures outperform existing approaches - particularly those relying on empirical evaluations of false discovery proportions - in terms of both power and empirical control of the FDR. Notably, our methodology yields substantial improvements in settings characterized by weaker signals, smaller sample sizes, and lower pre-specified FDR levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16124v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Chenlong Li, Cheng Yong Tang, Zhengtian Zhu</dc:creator>
    </item>
    <item>
      <title>Statistical Analysis of Large-scale Item Response Data under Measurement Non-invariance: A Statistically Consistent Method and Its Application to PISA 2022</title>
      <link>https://arxiv.org/abs/2505.16608</link>
      <description>arXiv:2505.16608v1 Announce Type: new 
Abstract: With the process of globalization, International Large-scale Assessments in education (ILSAs), such as the Programme for International Student Assessment (PISA), have become increasingly important in educational research and policy-making. They collect valuable data on education quality and performance development across many education systems worldwide, allowing countries to share techniques and policies that have proven efficient and successful. A key to analyzing ILSA data is an Item Response Theory (IRT) model, which is used to estimate the performance distributions of different groups (e.g., countries) and then produce a ranking. A major challenge in calibrating the IRT model is that some items suffer from Differential Item Functioning (DIF), i.e., different groups have different probabilities of correctly answering the items after controlling for individual proficiency levels. DIF is particularly common in ILSA due to the differences in test languages, cultural contexts, and curriculum designs across different groups. Ignoring or improperly accounting for DIF when calibrating the IRT model can lead to severe biases in the estimated performance distributions, which may further distort the ranking of the groups. Unfortunately, existing methods cannot guarantee the statistically consistent recovery of the group ranking without unrealistic assumptions for ILSA, such as the existence and knowledge of reference groups and anchor items. To fill this gap, this paper proposes a new approach to DIF analysis across multiple groups. This approach is computationally efficient and statistically consistent, without making strong assumptions about reference groups and anchor items. The proposed method is applied to PISA 2022 data from the mathematics, science, and reading domains, providing insights into their DIF structures and performance rankings of countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16608v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Ouyang, Yunxiao Chen, Chengcheng Li, Gongjun Xu</dc:creator>
    </item>
    <item>
      <title>A simulation and case study to evaluate the extrapolation performance of flexible Bayesian survival models when incorporating real-world data</title>
      <link>https://arxiv.org/abs/2505.16835</link>
      <description>arXiv:2505.16835v1 Announce Type: new 
Abstract: Background: Assessment of long-term survival for health technology assessment often necessitates extrapolation beyond the duration of a clinical trial. Without robust methods and external data, extrapolations are unreliable. Flexible Bayesian survival models that incorporate longer-term data sources, including registry data and population mortality, have been proposed as an alternative to using standard parametric models with trial data alone.
  Methods: The accuracy and uncertainty of extrapolations from the survextrap Bayesian survival model and R package were evaluated. In case studies and simulations, we assessed the accuracy of estimates with and without long-term data, under different assumptions about the long-term hazard rate and how it differs between datasets, and about treatment effects.
  Results: The survextrap model gives accurate extrapolations of long-term survival when long-term data on the patients of interest are included. Even using moderately biased external data gives improvements over using the short-term trial data alone. Furthermore, the model gives accurate extrapolations of differences in survival between treatment groups, provided that a reasonably accurate assumption is made about how the treatment effect will change over time. If no long-term data are available, then the model can quantify structural uncertainty about potential future changes in hazard rates.
  Conclusions: This analysis shows that Bayesian modelling can give accurate and reliable survival extrapolations by making the most of all available trial and real-world data. This work improves confidence in the use of a powerful tool for evidence-based healthcare decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16835v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iain R. Timmins, Fatemeh Torabi, Christopher H. Jackson, Paul C. Lambert, Michael J. Sweeting</dc:creator>
    </item>
    <item>
      <title>A monotonic MM-type algorithm for estimation of nonparametric finite mixture models with dependent marginals</title>
      <link>https://arxiv.org/abs/2505.16878</link>
      <description>arXiv:2505.16878v1 Announce Type: new 
Abstract: In this manuscript, we consider a finite nonparametric mixture model with non-independent marginal density functions. Dependence between the marginal densities is modeled using a copula device. Until recently, no deterministic algorithms capable of estimating components of such a model have been available. A deterministic algorithm that is capable of this has been proposed in \citet*{levine2024smoothed}. That algorithm seeks to maximize a smoothed nonparametric penalized log-likelihood; it seems to perform well in practice but does not possess the monotonicity property. In this manuscript, we introduce a deterministic MM (Minorization-Maximization) algorithm for estimation of components of this model that is also maximizing a smoothed penalized nonparametric log-likelihood but that is monotonic with respect to this objective functional. Besides the convergence of the objective functional, the convergence of a subsequence of arguments of this functional, generated by this algorithm, is also established. The behavior of this algorithm is illustrated using both simulated datasets as well as a real dataset. The results illustrate performance that is at least comparable to the earlier algorithm of \citet*{levine2024smoothed}. A discussion of the results and possible future research directions make up the last part of the manuscript.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16878v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Levine</dc:creator>
    </item>
    <item>
      <title>Exposure measurement error correction in longitudinal studies with discrete outcomes</title>
      <link>https://arxiv.org/abs/2505.16914</link>
      <description>arXiv:2505.16914v1 Announce Type: new 
Abstract: Environmental epidemiologists are often interested in estimating the effect of time-varying functions of the exposure history on health outcomes. However, the individual exposure measurements that constitute the history upon which an exposure history function is constructed are usually subject to measurement errors. To obtain unbiased estimates of the effects of such mismeasured functions in longitudinal studies with discrete outcomes, a method applicable to the main study/validation study design is developed. Various estimation procedures are explored. Simulation studies were conducted to assess its performance compared to standard analysis, and we found that the proposed method had good performance in terms of finite sample bias reduction and nominal coverage probability improvement. As an illustrative example, we applied the new method to a study of long-term exposure to PM2.5, in relation to the occurrence of anxiety disorders in the Nurses Health Study II. Failing to correct the error-prone exposure can lead to an underestimation of the chronic exposure effect of PM2.5.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16914v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ce Yang, Ning Zhang, Jiaxuan Li, Unnati V. Mehta, Jaime E. Hart, Donna Spiegelman, Molin Wang</dc:creator>
    </item>
    <item>
      <title>Hilbert space methods for approximating multi-output latent variable Gaussian processes</title>
      <link>https://arxiv.org/abs/2505.16919</link>
      <description>arXiv:2505.16919v1 Announce Type: new 
Abstract: Gaussian processes are a powerful class of non-linear models, but have limited applicability for larger datasets due to their high computational complexity. In such cases, approximate methods are required, for example, the recently developed class of Hilbert space Gaussian processes. They have been shown to drastically reduce computation time while retaining most of the favourable properties of exact Gaussian processes. However, Hilbert space approximations have so far only been developed for uni-dimensional outputs and manifest (known) inputs. To this end, we generalise Hilbert space methods to multi-output and latent input settings. Through extensive simulations, we show that the developed approximate Gaussian processes are indeed not only faster, but also provides similar or even better uncertainty calibration and accuracy of latent variable estimates compared to exact Gaussian processes. While not necessarily faster than alternative Gaussian process approximations, our new models provide better calibration and estimation accuracy, thus striking an excellent balance between trustworthiness and speed. We additionally validate our findings in a real world case study from single cell biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16919v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Soham Mukherjee, Manfred Claassen, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Oh SnapMMD! Forecasting Stochastic Dynamics Beyond the Schr\"odinger Bridge's End</title>
      <link>https://arxiv.org/abs/2505.16082</link>
      <description>arXiv:2505.16082v1 Announce Type: cross 
Abstract: Scientists often want to make predictions beyond the observed time horizon of "snapshot" data following latent stochastic dynamics. For example, in time course single-cell mRNA profiling, scientists have access to cellular transcriptional state measurements (snapshots) from different biological replicates at different time points, but they cannot access the trajectory of any one cell because measurement destroys the cell. Researchers want to forecast (e.g.) differentiation outcomes from early state measurements of stem cells. Recent Schr\"odinger-bridge (SB) methods are natural for interpolating between snapshots. But past SB papers have not addressed forecasting -- likely since existing methods either (1) reduce to following pre-set reference dynamics (chosen before seeing data) or (2) require the user to choose a fixed, state-independent volatility since they minimize a Kullback-Leibler divergence. Either case can lead to poor forecasting quality. In the present work, we propose a new framework, SnapMMD, that learns dynamics by directly fitting the joint distribution of both state measurements and observation time with a maximum mean discrepancy (MMD) loss. Unlike past work, our method allows us to infer unknown and state-dependent volatilities from the observed data. We show in a variety of real and synthetic experiments that our method delivers accurate forecasts. Moreover, our approach allows us to learn in the presence of incomplete state measurements and yields an $R^2$-style statistic that diagnoses fit. We also find that our method's performance at interpolation (and general velocity-field reconstruction) is at least as good as (and often better than) state-of-the-art in almost all of our experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16082v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renato Berlinghieri, Yunyi Shen, Jialong Jiang, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions</title>
      <link>https://arxiv.org/abs/2505.16311</link>
      <description>arXiv:2505.16311v1 Announce Type: cross 
Abstract: Recent advances in generative artificial intelligence (GenAI) models have enabled the generation of personalized content that adapts to up-to-date user context. While personalized decision systems are often modeled using bandit formulations, the integration of GenAI introduces new structure into otherwise classical sequential learning problems. In GenAI-powered interventions, the agent selects a query, but the environment experiences a stochastic response drawn from the generative model. Standard bandit methods do not explicitly account for this structure, where actions influence rewards only through stochastic, observed treatments. We introduce generator-mediated bandit-Thompson sampling (GAMBITTS), a bandit approach designed for this action/treatment split, using mobile health interventions with large language model-generated text as a motivating case study. GAMBITTS explicitly models both the treatment and reward generation processes, using information in the delivered treatment to accelerate policy learning relative to standard methods. We establish regret bounds for GAMBITTS by decomposing sources of uncertainty in treatment and reward, identifying conditions where it achieves stronger guarantees than standard bandit approaches. In simulation studies, GAMBITTS consistently outperforms conventional algorithms by leveraging observed treatments to more accurately estimate expected rewards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16311v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Brooks, Gabriel Durham, Kihyuk Hong, Ambuj Tewari</dc:creator>
    </item>
    <item>
      <title>Optimal design of dynamic experiments for scalar-on-function linear models with application to a biopharmaceutical study</title>
      <link>https://arxiv.org/abs/2110.09115</link>
      <description>arXiv:2110.09115v3 Announce Type: replace 
Abstract: A Bayesian optimal experimental design framework is developed for experiments where settings of one or more variables, referred to as profile variables, can be functions. For this type of experiment, a design consists of combinations of functions for each run of the experiment. Within a scalar-on-function linear model, profile variables are represented through basis expansions. This allows finite-dimensional representation of the profile variables and optimal designs to be found. The approach enables control over the complexity of the profile variables and model. The method is illustrated on a real application involving dynamic feeding strategies in an Ambr250 modular bioreactor system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.09115v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damianos Michaelides, Maria Adamou, David C. Woods, Antony M. Overstall</dc:creator>
    </item>
    <item>
      <title>On the Wasserstein median of probability measures</title>
      <link>https://arxiv.org/abs/2209.03318</link>
      <description>arXiv:2209.03318v4 Announce Type: replace 
Abstract: The primary choice to summarize a finite collection of random objects is by using measures of central tendency, such as mean and median. In the field of optimal transport, the Wasserstein barycenter corresponds to the Fr\'{e}chet or geometric mean of a set of probability measures, which is defined as a minimizer of the sum of squared distances to each element in a given set with respect to the Wasserstein distance of order 2. We introduce the Wasserstein median as a robust alternative to the Wasserstein barycenter. The Wasserstein median corresponds to the Fr\'{e}chet median under the 2-Wasserstein metric. The existence and consistency of the Wasserstein median are first established, along with its robustness property. In addition, we present a general computational pipeline that employs any recognized algorithms for the Wasserstein barycenter in an iterative fashion and demonstrate its convergence. The utility of the Wasserstein median as a robust measure of central tendency is demonstrated using real and simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.03318v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2024.2374580</arxiv:DOI>
      <dc:creator>Kisung You, Dennis Shung, Mauro Giuffr\`e</dc:creator>
    </item>
    <item>
      <title>Variable Selection and Minimax Prediction in High-dimensional Functional Linear Model</title>
      <link>https://arxiv.org/abs/2310.14419</link>
      <description>arXiv:2310.14419v4 Announce Type: replace 
Abstract: High-dimensional functional data have become increasingly prevalent in modern applications such as high-frequency financial data and neuroimaging data analysis. We investigate a class of high-dimensional linear regression models, where each predictor is a random element in an infinite-dimensional function space, and the number of functional predictors $p$ can potentially be ultra-high. Assuming that each of the unknown coefficient functions belongs to some reproducing kernel Hilbert space (RKHS), we regularize the fitting of the model by imposing a group elastic-net type of penalty on the RKHS norms of the coefficient functions. We show that our loss function is Gateaux sub-differentiable, and our functional elastic-net estimator exists uniquely in the product RKHS. Under suitable sparsity assumptions and a functional version of the irrepresentable condition, we derive a non-asymptotic tail bound for variable selection consistency of our method. Allowing the number of true functional predictors $q$ to diverge with the sample size, we also show a post-selection refined estimator can achieve the oracle minimax optimal prediction rate. The proposed methods are illustrated through simulation studies and a real-data application from the Human Connectome Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14419v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingche Guo, Yehua Li, Tailen Hsing</dc:creator>
    </item>
    <item>
      <title>Sparse Bayesian multidimensional scaling(s)</title>
      <link>https://arxiv.org/abs/2406.15573</link>
      <description>arXiv:2406.15573v3 Announce Type: replace 
Abstract: Bayesian multidimensional scaling (BMDS) is a probabilistic dimension reduction tool that allows one to model and visualize data consisting of dissimilarities between pairs of objects. Although BMDS has proven useful within, e.g., Bayesian phylogenetic inference, its likelihood and gradient calculations require a burdensome order of $N^2$ floating-point operations, where $N$ is the number of data points. Thus, BMDS becomes impractical as $N$ grows large. We propose and compare two sparse versions of BMDS (sBMDS) that apply log-likelihood and gradient computations to subsets of the observed dissimilarity matrix data. Landmark sBMDS (L-sBMDS) extracts columns, while banded sBMDS (B-sBMDS) extracts diagonals of the data. These sparse variants let one specify a time complexity between $N^2$ and $N$. Under simplified settings, we prove posterior consistency for subsampled distance matrices. Through simulations, we examine the accuracy and computational efficiency across all models using both the Metropolis-Hastings and Hamiltonian Monte Carlo algorithms. We observe approximately 3-fold, 10-fold and 40-fold speedups with negligible loss of accuracy, when applying the sBMDS likelihoods and gradients to 500, 1,000 and 5,000 data points with 50 bands (landmarks); these speedups only increase with the size of data considered. Finally, we apply the sBMDS variants to: 1) the phylogeographic modeling of multiple influenza subtypes to better understand how these strains spread through global air transportation networks and 2) the clustering of ArXiv manuscripts based on low-dimensional representations of article abstracts. In the first application, sBMDS contributes to holistic uncertainty quantification within a larger Bayesian hierarchical model. In the second, sBMDS provides uncertainty quantification for a downstream modeling task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15573v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ami Sheth, Aaron Smith, Andrew J. Holbrook</dc:creator>
    </item>
    <item>
      <title>Tail calibration of probabilistic forecasts</title>
      <link>https://arxiv.org/abs/2407.03167</link>
      <description>arXiv:2407.03167v3 Announce Type: replace 
Abstract: Probabilistic forecasts comprehensively describe the uncertainty in the unknown future outcome, making them essential for decision making and risk management. While several methods have been introduced to evaluate probabilistic forecasts, existing evaluation techniques are ill-suited to the evaluation of tail properties of such forecasts. However, these tail properties are often of particular interest to forecast users due to the severe impacts caused by extreme outcomes. In this work, we introduce a general notion of tail calibration for probabilistic forecasts, which allows forecasters to assess the reliability of their predictions for extreme outcomes. We study the relationships between tail calibration and standard notions of forecast calibration, and discuss connections to peaks-over-threshold models in extreme value theory. Diagnostic tools are introduced and applied in a case study on European precipitation forecasts</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03167v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam Allen, Jonathan Koh, Johan Segers, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>Infer-and-widen, or not?</title>
      <link>https://arxiv.org/abs/2408.06323</link>
      <description>arXiv:2408.06323v2 Announce Type: replace 
Abstract: In recent years, there has been substantial interest in the task of selective inference: inference on a parameter that is selected from the data. Many of the existing proposals fall into what we refer to as the \emph{infer-and-widen} framework: they produce symmetric confidence intervals whose midpoints do not account for selection and therefore are biased; thus, the intervals must be wide enough to account for this bias. In this paper, we investigate infer-and-widen approaches in three vignettes: the winner's curse, maximal contrasts, and inference after the lasso. In each of these examples, we show that a state-of-the-art infer-and-widen proposal leads to confidence intervals that are much wider than simple alternatives when all methods are tuned to yield \emph{identical} randomized selection events. Furthermore, even an ``oracle'' infer-and-widen confidence interval -- the narrowest possible interval that could be theoretically attained via infer-and-widen -- is often wider than these alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06323v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronan Perry, Zichun Xu, Olivia McGough, Daniela Witten</dc:creator>
    </item>
    <item>
      <title>A new statistical approach for joint modeling of longitudinal outcomes measured in electronic health records with clinically informative presence and observation processes</title>
      <link>https://arxiv.org/abs/2410.13113</link>
      <description>arXiv:2410.13113v2 Announce Type: replace 
Abstract: Biobanks with genetics-linked electronic health records (EHR) have opened up opportunities to study associations between genetic, social, or environmental factors and longitudinal lab biomarkers. However, in EHRs, the timing of patient visits and the recording of lab tests often depend on patient health status, referred to as informative presence (IP) and informative observation (IO), which can bias exposure-biomarker associations. Two gaps remain in EHR-based research: (1) the performance of existing IP-aware methods is unclear in real-world EHR settings, and (2) no existing methods handle IP and IO simultaneously. To address these challenges, we first conduct extensive simulation studies tailored to EHR-specific IP patterns to assess existing methods. We then propose a joint modeling framework, EHRJoint, that simultaneously models the visiting, observation, and longitudinal biomarker processes to address both IP and IO. We develop a computationally efficient estimation procedure based on estimating equations and provide asymptotically valid inference. Simulations show that EHRJoint yields unbiased exposure effect estimates under both IP and IO, while existing methods fail. We apply EHRJoint to the Michigan Genomics Initiative data to examine associations between repeated glucose measurements and two exposures: genetic variants and educational disadvantage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13113v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiacong Du, Xu Shi, Bhramar Mukherjee</dc:creator>
    </item>
    <item>
      <title>From Estimands to Robust Inference of Treatment Effects in Platform Trials</title>
      <link>https://arxiv.org/abs/2411.12944</link>
      <description>arXiv:2411.12944v4 Announce Type: replace 
Abstract: A platform trial is an innovative clinical trial design that uses a master protocol to evaluate multiple treatments, where patients are often assigned to different subsets of treatment arms based on individual characteristics, enrollment timing, and treatment availability. While offering increased flexibility, this constrained and non-uniform treatment assignment poses inferential challenges, with two fundamental ones being the precise definition of treatment effects and robust, efficient inference on these effects. Such challenges arise primarily because some commonly used analysis approaches may target estimands defined on populations inadvertently depending on randomization ratios or trial operation format, thereby undermining interpretability. This article, for the first time, presents a formal framework for constructing a clinically meaningful estimand with precise specification of the population of interest. Specifically, the proposed entire concurrently eligible (ECE) population not only preserves the integrity of randomized comparisons but also remains invariant to both the randomization ratio and trial operation format. Then, we develop weighting and post-stratification methods to estimate treatment effects under the same minimal assumptions used in traditional randomized trials. We also consider model-assisted covariate adjustment to fully unlock the efficiency potential of platform trials while maintaining robustness against model misspecification. For all proposed estimators, we derive asymptotic distributions and propose robust variance estimators and compare them in theory and through simulations. The SIMPLIFY trial, a master protocol assessing continuation versus discontinuation of two common therapies in cystic fibrosis, is utilized to further highlight the practical significance of this research. All analyses are conducted using the R package RobinCID.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12944v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhan Qian, Yifan Yi, Jun Shao, Yanyao Yi, Gregory Levin, Nicole Mayer-Hamblett, Patrick J. Heagerty, Ting Ye</dc:creator>
    </item>
    <item>
      <title>EW D-optimal Designs for Experiments with Mixed Factors</title>
      <link>https://arxiv.org/abs/2505.00629</link>
      <description>arXiv:2505.00629v2 Announce Type: replace 
Abstract: We characterize EW D-optimal designs as robust designs against unknown parameter values for experiments under a general parametric model with discrete and continuous factors. When a pilot study is available, we recommend sample-based EW D-optimal designs for subsequent experiments. Otherwise, we recommend EW D-optimal designs under a prior distribution for model parameters. We propose an EW ForLion algorithm for finding EW D-optimal designs with mixed factors, and justify that the designs found by our algorithm are EW D-optimal. To facilitate potential users in practice, we also develop a rounding algorithm that converts an approximate design with mixed factors to exact designs with prespecified grid points and the number of experimental units. By applying our algorithms for real experiments under multinomial logistic models or generalized linear models, we show that our designs are highly efficient with respect to locally D-optimal designs and more robust against parameter value misspecifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00629v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siting Lin, Yifei Huang, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Pre-validation Revisited</title>
      <link>https://arxiv.org/abs/2505.14985</link>
      <description>arXiv:2505.14985v2 Announce Type: replace 
Abstract: Pre-validation is a way to build prediction model with two datasets of significantly different feature dimensions. Previous work showed that the asymptotic distribution of the resulting test statistic for the pre-validated predictor deviates from a standard Normal, hence leads to issues in hypothesis testing. In this paper, we revisit the pre-validation procedure and extend the problem formulation without any independence assumption on the two feature sets. We propose not only an analytical distribution of the test statistic for the pre-validated predictor under certain models, but also a generic bootstrap procedure to conduct inference. We show properties and benefits of pre-validation in prediction, inference and error estimation by simulations and applications, including analysis of a breast cancer study and a synthetic GWAS example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14985v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Shang, Sourav Chatterjee, Trevor Hastie, Robert Tibshirani</dc:creator>
    </item>
    <item>
      <title>Estimate-Then-Optimize versus Integrated-Estimation-Optimization versus Sample Average Approximation: A Stochastic Dominance Perspective</title>
      <link>https://arxiv.org/abs/2304.06833</link>
      <description>arXiv:2304.06833v4 Announce Type: replace-cross 
Abstract: In data-driven stochastic optimization, model parameters of the underlying distribution need to be estimated from data in addition to the optimization task. Recent literature considers integrating the estimation and optimization processes by selecting model parameters that lead to the best empirical objective performance. This integrated approach, which we call integrated-estimation-optimization (IEO), can be readily shown to outperform simple estimate-then-optimize (ETO) when the model is misspecified. In this paper, we show that a reverse behavior appears when the model class is well-specified and there is sufficient data. Specifically, for a general class of nonlinear stochastic optimization problems, we show that simple ETO outperforms IEO asymptotically when the model class covers the ground truth, in the strong sense of stochastic dominance of the regret. Namely, the entire distribution of the regret, not only its mean or other moments, is always better for ETO compared to IEO. Our results also apply to constrained, contextual optimization problems where the decision depends on observed features. Whenever applicable, we also demonstrate how standard sample average approximation (SAA) performs the worst when the model class is well-specified in terms of regret, and best when it is misspecified. Finally, we provide experimental results to support our theoretical comparisons and illustrate when our insights hold in finite-sample regimes and under various degrees of misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.06833v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam N. Elmachtoub, Henry Lam, Haofeng Zhang, Yunfan Zhao</dc:creator>
    </item>
    <item>
      <title>Causality Pursuit from Heterogeneous Environments via Neural Adversarial Invariance Learning</title>
      <link>https://arxiv.org/abs/2405.04715</link>
      <description>arXiv:2405.04715v4 Announce Type: replace-cross 
Abstract: Pursuing causality from data is a fundamental problem in scientific discovery, treatment intervention, and transfer learning. This paper introduces a novel algorithmic method for addressing nonparametric invariance and causality learning in regression models across multiple environments, where the joint distribution of response variables and covariates varies, but the conditional expectations of outcome given an unknown set of quasi-causal variables are invariant. The challenge of finding such an unknown set of quasi-causal or invariant variables is compounded by the presence of endogenous variables that have heterogeneous effects across different environments. The proposed Focused Adversarial Invariant Regularization (FAIR) framework utilizes an innovative minimax optimization approach that drives regression models toward prediction-invariant solutions through adversarial testing. Leveraging the representation power of neural networks, FAIR neural networks (FAIR-NN) are introduced for causality pursuit. It is shown that FAIR-NN can find the invariant variables and quasi-causal variables under a minimal identification condition and that the resulting procedure is adaptive to low-dimensional composition structures in a non-asymptotic analysis. Under a structural causal model, variables identified by FAIR-NN represent pragmatic causality and provably align with exact causal mechanisms under conditions of sufficient heterogeneity. Computationally, FAIR-NN employs a novel Gumbel approximation with decreased temperature and a stochastic gradient descent ascent algorithm. The procedures are demonstrated using simulated and real-data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04715v4</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Gu, Cong Fang, Peter B\"uhlmann, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>fdesigns: Bayesian Optimal Designs of Experiments for Functional Models in R</title>
      <link>https://arxiv.org/abs/2411.09225</link>
      <description>arXiv:2411.09225v2 Announce Type: replace-cross 
Abstract: This paper describes the R package fdesigns that implements a methodology for identifying Bayesian optimal experimental designs for models whose factor settings are functions, known as profile factors. This type of experiments which involve factors that vary dynamically over time, presenting unique challenges in both estimation and design due to the infinite-dimensional nature of functions. The package fdesigns implements a dimension reduction method leveraging basis functions of the B-spline basis system. The package fdesigns contains functions that effectively reduce the design problem to the optimisation of basis coefficients for functional linear functional generalised linear models, and it accommodates various options. Applications of the fdesigns package are demonstrated through a series of examples that showcase its capabilities in identifying optimal designs for functional linear and generalised linear models. The examples highlight how the package's functions can be used to efficiently design experiments involving both profile and scalar factors, including interactions and polynomial effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09225v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damianos Michaelides, Antony Overstall, Dave Woods</dc:creator>
    </item>
    <item>
      <title>LiDDA: Data Driven Attribution at LinkedIn</title>
      <link>https://arxiv.org/abs/2505.09861</link>
      <description>arXiv:2505.09861v2 Announce Type: replace-cross 
Abstract: Data Driven Attribution, which assigns conversion credits to marketing interactions based on causal patterns learned from data, is the foundation of modern marketing intelligence and vital to any marketing businesses and advertising platform. In this paper, we introduce a unified transformer-based attribution approach that can handle member-level data, aggregate-level data, and integration of external macro factors. We detail the large scale implementation of the approach at LinkedIn, showcasing significant impact. We also share learning and insights that are broadly applicable to the marketing and ad tech fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09861v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>stat.ME</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Bencina, Erkut Aykutlug, Yue Chen, Zerui Zhang, Stephanie Sorenson, Shao Tang, Changshuai Wei</dc:creator>
    </item>
  </channel>
</rss>
