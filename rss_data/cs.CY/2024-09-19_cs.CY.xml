<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Sep 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Computing-specific pedagogies and theoretical models: common uses and relationships</title>
      <link>https://arxiv.org/abs/2409.12245</link>
      <description>arXiv:2409.12245v1 Announce Type: new 
Abstract: Computing education widely applies general learning theories and pedagogical practices. However, computing also includes specific disciplinary knowledge and skills, e.g., programming and software development methods, for which there has been a long history of development and application of specific pedagogical practices. In recent years, there has also been substantial interest in developing computing-specific theoretical models, which seek to describe and explain the complex interactions within teaching and learning computing in various contexts. In this paper, we explore connections between computing-specific pedagogies and theoretical models as reported in the literature. Our goal is to enrich computing education research and practice by illustrating how explicit use of field-specific theories and pedagogies can further the whole field. We have collected a list of computing-specific pedagogical practices and theoretical models from a literature search, identifying source papers where they have been first introduced or well described. We then searched for papers in the ACM digital library that cite source papers from each list, and analyzed the type of interaction between the model and pedagogy in each paper. We developed a categorization of how theoretical models and pedagogies have supported or discounted each other, have been used together in empirical studies or used to build new artefacts. Our results showed that pair programming and parsons problems have had the most interactions with theoretical models in the explored papers, and we present findings of the analysis of these interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12245v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lauri Malmi, Judy Sheard, Claudia Szabo, P\"aivi Kinnunen</dc:creator>
    </item>
    <item>
      <title>An Outline for a Jupyter-Materials-Based Repository Website Focused on the Computational Sciences</title>
      <link>https://arxiv.org/abs/2409.12246</link>
      <description>arXiv:2409.12246v1 Announce Type: new 
Abstract: As access to the internet has become increasingly ubiquitous, along with the reliability and speed of internet providers, so too has the implementation of internet-based learning tools. These tools provide students opportunities to do meaningful work away from university, however, often at a financial cost to universities and students. Moreover, limited and high-cost internet access in less-developed countries and remote areas acts as a barrier to implementing these tools in a meaningful way, leading to inequalities in both the quality of education and the opportunities provided. This paper outlines the development process, and benefits, of a low-cost and light-weight repository website centered around disseminating open-source textbooks and other supplemental learning materials for computational sciences using Jupyter Notebooks. The website focuses on allowing students to download their textbooks and other materials from a centralized location, to be used offline or with limited internet access. Internet access is not the only constraining factor; access to reasonably priced personal computers also limits the effectiveness of internet-based learning tools. As such, this paper will also explore the feasibility of integrating low-cost Raspberry Pi kits into this development process as a way of increasing the reach of an online repository of open-source Jupyter Notebook textbooks. While this paper focuses on Canadian universities and remote communities, many of the website's proposed applications are relevant worldwide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12246v1</guid>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Berg, Zachary Kelly</dc:creator>
    </item>
    <item>
      <title>Perceptions of the Fairness Impacts of Multiplicity in Machine Learning</title>
      <link>https://arxiv.org/abs/2409.12332</link>
      <description>arXiv:2409.12332v1 Announce Type: new 
Abstract: Machine learning (ML) is increasingly used in high-stakes settings, yet multiplicity -- the existence of multiple good models -- means that some predictions are essentially arbitrary. ML researchers and philosophers posit that multiplicity poses a fairness risk, but no studies have investigated whether stakeholders agree. In this work, we conduct a survey to see how the presence of multiplicity impacts lay stakeholders' -- i.e., decision subjects' -- perceptions of ML fairness, and which approaches to address multiplicity they prefer. We investigate how these perceptions are modulated by task characteristics (e.g., stakes and uncertainty). Survey respondents think that multiplicity lowers distributional, but not procedural, fairness, even though existing work suggests the opposite. Participants are strongly against resolving multiplicity by using a single good model (effectively ignoring multiplicity) or by randomizing over possible outcomes. Our results indicate that model developers should be intentional about dealing with multiplicity in order to maintain fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12332v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna P. Meyer, Yea-Seul Kim, Aws Albarghouthi, Loris D'Antoni</dc:creator>
    </item>
    <item>
      <title>ARTAI: An Evaluation Platform to Assess Societal Risk of Recommender Algorithms</title>
      <link>https://arxiv.org/abs/2409.12396</link>
      <description>arXiv:2409.12396v1 Announce Type: new 
Abstract: Societal risk emanating from how recommender algorithms disseminate content online is now well documented. Emergent regulation aims to mitigate this risk through ethical audits and enabling new research on the social impact of algorithms. However, there is currently a need for tools and methods that enable such evaluation. This paper presents ARTAI, an evaluation environment that enables large-scale assessments of recommender algorithms to identify harmful patterns in how content is distributed online and enables the implementation of new regulatory requirements for increased transparency in recommender systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12396v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qin Ruan, Jin Xu, Ruihai Dong, Arjumand Younus, Tai Tan Mai, Barry O'Sullivan, Susan Leavy</dc:creator>
    </item>
    <item>
      <title>Towards adaptive trajectories for mixed autonomous and human-operated ships</title>
      <link>https://arxiv.org/abs/2409.12714</link>
      <description>arXiv:2409.12714v1 Announce Type: new 
Abstract: We are witnessing the rise of autonomous cars, which will likely revolutionize the way we travel. Arguably, the maritime domain lags behind, as ships operate on many more degrees of freedom (thus, a much larger search space): there is less physical infrastructure, and rules are less consistent and constraining than what is found on roads. The problem is further complicated by the inevitable co-existence of autonomous and human-operated ships: the latter may take unpredictable decisions, which require adjustments on the autonomous ones. Finally, the problem is inherently decentralised, there is no central authority, and communication means can be very diverse in terms of communication distance and performance, mandating special care on which information is shared and how. In this work, we elaborate on the challenges of trajectory prediction and adaptation for mixed autonomous and human-operated ships, and we propose initial ideas on potential approaches to address them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12714v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danilo Pianini, Sven Tomforde</dc:creator>
    </item>
    <item>
      <title>AI Thinking: A framework for rethinking artificial intelligence in practice</title>
      <link>https://arxiv.org/abs/2409.12922</link>
      <description>arXiv:2409.12922v1 Announce Type: new 
Abstract: Artificial intelligence is transforming the way we work with information across disciplines and practical contexts. A growing range of disciplines are now involved in studying, developing, and assessing the use of AI in practice, but these disciplines often employ conflicting understandings of what AI is and what is involved in its use. New, interdisciplinary approaches are needed to bridge competing conceptualisations of AI in practice and help shape the future of AI use. I propose a novel conceptual framework called AI Thinking, which models key decisions and considerations involved in AI use across disciplinary perspectives. The AI Thinking model addresses five practice-based competencies involved in applying AI in context: motivating AI use in information processes, formulating AI methods, assessing available tools and technologies, selecting appropriate data, and situating AI in the sociotechnical contexts it is used in. A hypothetical case study is provided to illustrate the application of AI Thinking in practice. This article situates AI Thinking in broader cross-disciplinary discourses of AI, including its connections to ongoing discussions around AI literacy and AI-driven innovation. AI Thinking can help to bridge divides between academic disciplines and diverse contexts of AI use, and to reshape the future of AI in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12922v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Denis Newman-Griffis</dc:creator>
    </item>
    <item>
      <title>Nteasee: A mixed methods study of expert and general population perspectives on deploying AI for health in African countries</title>
      <link>https://arxiv.org/abs/2409.12197</link>
      <description>arXiv:2409.12197v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) for health has the potential to significantly change and improve healthcare. However in most African countries, identifying culturally and contextually attuned approaches for deploying these solutions is not well understood. To bridge this gap, we conduct a qualitative study to investigate the best practices, fairness indicators, and potential biases to mitigate when deploying AI for health in African countries, as well as explore opportunities where artificial intelligence could make a positive impact in health. We used a mixed methods approach combining in-depth interviews (IDIs) and surveys. We conduct 1.5-2 hour long IDIs with 50 experts in health, policy, and AI across 17 countries, and through an inductive approach we conduct a qualitative thematic analysis on expert IDI responses. We administer a blinded 30-minute survey with case studies to 672 general population participants across 5 countries in Africa and analyze responses on quantitative scales, statistically comparing responses by country, age, gender, and level of familiarity with AI. We thematically summarize open-ended responses from surveys. Our results find generally positive attitudes, high levels of trust, accompanied by moderate levels of concern among general population participants for AI usage for health in Africa. This contrasts with expert responses, where major themes revolved around trust/mistrust, ethical concerns, and systemic barriers to integration, among others. This work presents the first-of-its-kind qualitative research study of the potential of AI for health in Africa from an algorithmic fairness angle, with perspectives from both experts and the general population. We hope that this work guides policymakers and drives home the need for further research and the inclusion of general population perspectives in decision-making around AI usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12197v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mercy Nyamewaa Asiedu, Iskandar Haykel, Awa Dieng, Kerrie Kauer, Tousif Ahmed, Florence Ofori, Charisma Chan, Stephen Pfohl, Negar Rostamzadeh, Katherine Heller</dc:creator>
    </item>
    <item>
      <title>Sustainable Visions: Unsupervised Machine Learning Insights on Global Development Goals</title>
      <link>https://arxiv.org/abs/2409.12427</link>
      <description>arXiv:2409.12427v1 Announce Type: cross 
Abstract: The United Nations 2030 Agenda for Sustainable Development outlines 17 goals to address global challenges. However, progress has been slower than expected and, consequently, there is a need to investigate the reasons behind this fact. In this study, we used a novel data-driven methodology to analyze data from 107 countries (2000$-$2022) using unsupervised machine learning techniques. Our analysis reveals strong positive and negative correlations between certain SDGs. The findings show that progress toward the SDGs is heavily influenced by geographical, cultural and socioeconomic factors, with no country on track to achieve all goals by 2030. This highlights the need for a region specific, systemic approach to sustainable development that acknowledges the complex interdependencies of the goals and the diverse capacities of nations. Our approach provides a robust framework for developing efficient and data-informed strategies, to promote cooperative and targeted initiatives for sustainable progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12427v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Garc\'ia-Rodr\'iguez, Matias N\'u\~nez, Miguel Robles P\'erez, Tzipe Govezensky, Rafael A. Barrio, Carlos Gershenson, Kimmo K. Kaski, Julia Tag\"ue\~na</dc:creator>
    </item>
    <item>
      <title>Is it Still Fair? A Comparative Evaluation of Fairness Algorithms through the Lens of Covariate Drift</title>
      <link>https://arxiv.org/abs/2409.12428</link>
      <description>arXiv:2409.12428v1 Announce Type: cross 
Abstract: Over the last few decades, machine learning (ML) applications have grown exponentially, yielding several benefits to society. However, these benefits are tempered with concerns of discriminatory behaviours exhibited by ML models. In this regard, fairness in machine learning has emerged as a priority research area. Consequently, several fairness metrics and algorithms have been developed to mitigate against discriminatory behaviours that ML models may possess. Yet still, very little attention has been paid to the problem of naturally occurring changes in data patterns (\textit{aka} data distributional drift), and its impact on fairness algorithms and metrics. In this work, we study this problem comprehensively by analyzing 4 fairness-unaware baseline algorithms and 7 fairness-aware algorithms, carefully curated to cover the breadth of its typology, across 5 datasets including public and proprietary data, and evaluated them using 3 predictive performance and 10 fairness metrics. In doing so, we show that (1) data distributional drift is not a trivial occurrence, and in several cases can lead to serious deterioration of fairness in so-called fair models; (2) contrary to some existing literature, the size and direction of data distributional drift is not correlated to the resulting size and direction of unfairness; and (3) choice of, and training of fairness algorithms is impacted by the effect of data distributional drift which is largely ignored in the literature. Emanating from our findings, we synthesize several policy implications of data distributional drift on fairness algorithms that can be very relevant to stakeholders and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12428v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Oscar Blessed Deho, Michael Bewong, Selasi Kwashie, Jiuyong Li, Jixue Liu, Lin Liu, Srecko Joksimovic</dc:creator>
    </item>
    <item>
      <title>Exploring Engagement and Perceived Learning Outcomes in an Immersive Flipped Learning Context</title>
      <link>https://arxiv.org/abs/2409.12674</link>
      <description>arXiv:2409.12674v1 Announce Type: cross 
Abstract: The flipped classroom model has been widely acknowledged as a practical pedagogical approach to enhancing student engagement and learning. However, it faces challenges such as improving student interaction with learning content and peers, particularly in Japanese universities where digital technologies are not always fully utilized. To address these challenges and identify potential solutions, a case study was conducted in which an online flipped course on academic skills was developed and implemented in an immersive virtual environment. The primary objective during this initial phase was not to establish a causal relationship between the use of immersive flipped learning and students' engagement and perceived learning outcomes. Instead, this initiative aimed to explore the benefits and challenges of the immersive flipped learning approach in relation to students' online engagement and their perceived learning outcomes. Following a mixed-methods research approach, quantitative and qualitative data were collected through a survey (N=50) and students' reflective reports (N=80). The study revealed high levels of student engagement and perceived learning outcomes, although it also identified areas needing improvement, particularly in supporting student interactions in the target language. Despite the exploratory nature of this study, the findings suggest that a well-designed flipped learning approach, set in an engaging immersive environment, can significantly enhance student engagement, thereby supporting the learning process. When creating an immersive flipped learning course, educators should incorporate best practices from the literature on both flipped learning and immersive learning design to ensure optimal learning outcomes. The findings of this study can serve as a valuable resource for educators seeking to design engaging and effective remote learning experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12674v1</guid>
      <category>physics.ed-ph</category>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.32664/ijitgeb.v6i2.155</arxiv:DOI>
      <arxiv:journal_reference>The International Journal in Information Technology in Governance, Education, and Business 2024</arxiv:journal_reference>
      <dc:creator>Mehrasa Alizadeh</dc:creator>
    </item>
    <item>
      <title>Ethical Analysis on the Application of Neurotechnology for Human Augmentation in Physicians and Surgeons</title>
      <link>https://arxiv.org/abs/2006.16925</link>
      <description>arXiv:2006.16925v3 Announce Type: replace 
Abstract: With the shortage of physicians and surgeons and increase in demand worldwide due to situations such as the COVID-19 pandemic, there is a growing interest in finding solutions to help address the problem. A solution to this problem would be to use neurotechnology to provide them augmented cognition, senses and action for optimal diagnosis and treatment. Consequently, doing so can negatively impact them and others. We argue that applying neurotechnology for human enhancement in physicians and surgeons can cause injustices, and harm to them and patients. In this paper, we will first describe the augmentations and neurotechnologies that can be used to achieve the relevant augmentations for physicians and surgeons. We will then review selected ethical concerns discussed within literature, discuss the neuroengineering behind using neurotechnology for augmentation purposes, then conclude with an analysis on outcomes and ethical issues of implementing human augmentation via neurotechnology in medical and surgical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.16925v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>physics.med-ph</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-030-63092-8_6</arxiv:DOI>
      <dc:creator>Soaad Hossain, Syed Ishtiaque Ahmed</dc:creator>
    </item>
    <item>
      <title>The complementary contributions of academia and industry to AI research</title>
      <link>https://arxiv.org/abs/2401.10268</link>
      <description>arXiv:2401.10268v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) has seen fast paced development in industry and academia. However, striking recent advances by industry have stunned the field, inviting a fresh perspective on the role of academic research on this progress. Here, we characterize the impact and type of AI produced by both environments over the last 25 years and establish several patterns. We find that articles published by teams consisting exclusively of industry researchers tend to get greater attention, with a higher chance of being highly cited and citation-disruptive, and several times more likely to produce state-of-the-art models. In contrast, we find that exclusively academic teams publish the bulk of AI research and tend to produce higher novelty work, with single papers having several times higher likelihood of being unconventional and atypical. The respective impact-novelty advantages of industry and academia are robust to controls for subfield, team size, seniority, and prestige. We find that academic-industry collaborations produce the most impactful work overall but do not have the novelty level of academic teams. Together, our findings identify the unique and nearly irreplaceable contributions that both academia and industry make toward the progress of AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10268v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lizhen Liang (Syracuse University), Han Zhuang (Northeastern University), James Zou (Stanford University), Daniel E. Acuna (University of Colorado at Boulder)</dc:creator>
    </item>
    <item>
      <title>Ethical Artificial Intelligence Principles and Guidelines for the Governance and Utilization of Highly Advanced Large Language Models</title>
      <link>https://arxiv.org/abs/2401.10745</link>
      <description>arXiv:2401.10745v2 Announce Type: replace 
Abstract: Given the success of ChatGPT, LaMDA and other large language models (LLMs), there has been an increase in development and usage of LLMs within the technology sector and other sectors. While the level in which LLMs has not reached a level where it has surpassed human intelligence, there will be a time when it will. Such LLMs can be referred to as advanced LLMs. Currently, there are limited usage of ethical artificial intelligence (AI) principles and guidelines addressing advanced LLMs due to the fact that we have not reached that point yet. However, this is a problem as once we do reach that point, we will not be adequately prepared to deal with the aftermath of it in an ethical and optimal way, which will lead to undesired and unexpected consequences. This paper addresses this issue by discussing what ethical AI principles and guidelines can be used to address highly advanced LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10745v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soaad Hossain, Syed Ishtiaque Ahmed</dc:creator>
    </item>
    <item>
      <title>Nicer Than Humans: How do Large Language Models Behave in the Prisoner's Dilemma?</title>
      <link>https://arxiv.org/abs/2406.13605</link>
      <description>arXiv:2406.13605v2 Announce Type: replace 
Abstract: The behavior of Large Language Models (LLMs) as artificial social agents is largely unexplored, and we still lack extensive evidence of how these agents react to simple social stimuli. Testing the behavior of AI agents in classic Game Theory experiments provides a promising theoretical framework for evaluating the norms and values of these agents in archetypal social situations. In this work, we investigate the cooperative behavior of three LLMs (Llama2, Llama3, and GPT3.5) when playing the Iterated Prisoner's Dilemma against random adversaries displaying various levels of hostility. We introduce a systematic methodology to evaluate an LLM's comprehension of the game rules and its capability to parse historical gameplay logs for decision-making. We conducted simulations of games lasting for 100 rounds and analyzed the LLMs' decisions in terms of dimensions defined in the behavioral economics literature. We find that all models tend not to initiate defection but act cautiously, favoring cooperation over defection only when the opponent's defection rate is low. Overall, LLMs behave at least as cooperatively as the typical human player, although our results indicate some substantial differences among models. In particular, Llama2 and GPT3.5 are more cooperative than humans, and especially forgiving and non-retaliatory for opponent defection rates below 30%. More similar to humans, Llama3 exhibits consistently uncooperative and exploitative behavior unless the opponent always cooperates. Our systematic approach to the study of LLMs in game theoretical scenarios is a step towards using these simulations to inform practices of LLM auditing and alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13605v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>physics.soc-ph</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicol\'o Fontana, Francesco Pierri, Luca Maria Aiello</dc:creator>
    </item>
    <item>
      <title>Taking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education</title>
      <link>https://arxiv.org/abs/2401.00832</link>
      <description>arXiv:2401.00832v3 Announce Type: replace-cross 
Abstract: The integration of Artificial Intelligence (AI), particularly Large Language Model (LLM)-based systems, in education has shown promise in enhancing teaching and learning experiences. However, the advent of Multimodal Large Language Models (MLLMs) like GPT-4 with vision (GPT-4V), capable of processing multimodal data including text, sound, and visual inputs, opens a new era of enriched, personalized, and interactive learning landscapes in education. Grounded in theory of multimedia learning, this paper explores the transformative role of MLLMs in central aspects of science education by presenting exemplary innovative learning scenarios. Possible applications for MLLMs could range from content creation to tailored support for learning, fostering competencies in scientific practices, and providing assessment and feedback. These scenarios are not limited to text-based and uni-modal formats but can be multimodal, increasing thus personalization, accessibility, and potential learning effectiveness. Besides many opportunities, challenges such as data protection and ethical considerations become more salient, calling for robust frameworks to ensure responsible integration. This paper underscores the necessity for a balanced approach in implementing MLLMs, where the technology complements rather than supplants the educator's role, ensuring thus an effective and ethical use of AI in science education. It calls for further research to explore the nuanced implications of MLLMs on the evolving role of educators and to extend the discourse beyond science education to other disciplines. Through the exploration of potentials, challenges, and future implications, we aim to contribute to a preliminary understanding of the transformative trajectory of MLLMs in science education and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00832v3</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arne Bewersdorff, Christian Hartmann, Marie Hornberger, Kathrin Se{\ss}ler, Maria Bannert, Enkelejda Kasneci, Gjergji Kasneci, Xiaoming Zhai, Claudia Nerdel</dc:creator>
    </item>
    <item>
      <title>ELIZA Reinterpreted: The world's first chatbot was not intended as a chatbot at all</title>
      <link>https://arxiv.org/abs/2406.17650</link>
      <description>arXiv:2406.17650v2 Announce Type: replace-cross 
Abstract: ELIZA, often considered the world's first chatbot, was written by Joseph Weizenbaum in the early 1960s. Weizenbaum did not intend to invent the chatbot, but rather to build a platform for research into human-machine conversation and the important cognitive processes of interpretation and misinterpretation. His purpose was obscured by ELIZA's fame, resulting in large part from the fortuitous timing of it's creation, and it's escape into the wild. In this paper I provide a rich historical context for ELIZA's creation, demonstrating that ELIZA arose from the intersection of some of the central threads in the technical history of AI. I also briefly discuss how ELIZA escaped into the world, and how its accidental escape, along with several coincidental turns of the programming language screws, led both to the misapprehension that ELIZA was intended as a chatbot, and to the loss of the original ELIZA to history for over 50 years.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17650v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeff Shrager</dc:creator>
    </item>
    <item>
      <title>Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis</title>
      <link>https://arxiv.org/abs/2409.05292</link>
      <description>arXiv:2409.05292v3 Announce Type: replace-cross 
Abstract: The world is currently experiencing an outbreak of mpox, which has been declared a Public Health Emergency of International Concern by WHO. No prior work related to social media mining has focused on the development of a dataset of Instagram posts about the mpox outbreak. The work presented in this paper aims to address this research gap and makes two scientific contributions to this field. First, it presents a multilingual dataset of 60,127 Instagram posts about mpox, published between July 23, 2022, and September 5, 2024. The dataset, available at https://dx.doi.org/10.21227/7fvc-y093, contains Instagram posts about mpox in 52 languages. For each of these posts, the Post ID, Post Description, Date of publication, language, and translated version of the post (translation to English was performed using the Google Translate API) are presented as separate attributes in the dataset. After developing this dataset, sentiment analysis, hate speech detection, and anxiety or stress detection were performed. This process included classifying each post into (i) one of the sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no anxiety/stress detected. These results are presented as separate attributes in the dataset. Second, this paper presents the results of performing sentiment analysis, hate speech analysis, and anxiety or stress analysis. The variation of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and 50.64%, respectively. In terms of hate speech detection, 95.75% of the posts did not contain hate and the remaining 4.25% of the posts contained hate. Finally, 72.05% of the posts did not indicate any anxiety/stress, and the remaining 27.95% of the posts represented some form of anxiety/stress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05292v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nirmalya Thakur</dc:creator>
    </item>
    <item>
      <title>Instigating Cooperation among LLM Agents Using Adaptive Information Modulation</title>
      <link>https://arxiv.org/abs/2409.10372</link>
      <description>arXiv:2409.10372v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel framework combining LLM agents as proxies for human strategic behavior with reinforcement learning (RL) to engage these agents in evolving strategic interactions within team environments. Our approach extends traditional agent-based simulations by using strategic LLM agents (SLA) and introducing dynamic and adaptive governance through a pro-social promoting RL agent (PPA) that modulates information access across agents in a network, optimizing social welfare and promoting pro-social behavior. Through validation in iterative games, including the prisoner dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations. The PPA agent effectively learns to adjust information transparency, resulting in enhanced cooperation rates. This framework offers significant insights into AI-mediated social dynamics, contributing to the deployment of AI in real-world team settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10372v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiliang Chen, Sepehr Ilami, Nunzio Lore, Babak Heydari</dc:creator>
    </item>
    <item>
      <title>Gender Representation and Bias in Indian Civil Service Mock Interviews</title>
      <link>https://arxiv.org/abs/2409.12194</link>
      <description>arXiv:2409.12194v2 Announce Type: replace-cross 
Abstract: This paper makes three key contributions. First, via a substantial corpus of 51,278 interview questions sourced from 888 YouTube videos of mock interviews of Indian civil service candidates, we demonstrate stark gender bias in the broad nature of questions asked to male and female candidates. Second, our experiments with large language models show a strong presence of gender bias in explanations provided by the LLMs on the gender inference task. Finally, we present a novel dataset of 51,278 interview questions that can inform future social science studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12194v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Somonnoy Banerjee, Sujan Dutta, Soumyajit Datta, Ashiqur R. KhudaBukhsh</dc:creator>
    </item>
  </channel>
</rss>
