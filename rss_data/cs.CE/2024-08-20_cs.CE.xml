<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Aug 2024 01:47:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Single Atom Convolutional Matching Pursuit: Theoretical Framework and Application to Lamb Waves based Structural Health Monitoring</title>
      <link>https://arxiv.org/abs/2408.08929</link>
      <description>arXiv:2408.08929v1 Announce Type: new 
Abstract: Structural Health Monitoring (SHM) aims to monitor in real time the health state of engineering structures. For thin structures, Lamb Waves (LW) are very efficient for SHM purposes. A bonded piezoelectric transducer (PZT) emits LW in the structure in the form of a short tone burst. This initial wave packet (IWP) propagates in the structure and interacts with its boundaries and discontinuities and with eventual damages generating additional wave packets. The main issues with LW based SHM are that at least two LW modes are simultaneously excited and that those modes are dispersive. Matching Pursuit Method (MPM), which consists of approximating a signal as a sum of different delayed and scaled atoms taken from an a priori known learning dictionary, seems very appealing in such a context, however is limited to nondispersive signals and relies on a priori known dictionary. An improved version of MPM called the Single Atom Convolutional Matching Pursuit method (SACMPM), which addresses the dispersion phenomena by decomposing a measured signal as delayed and dispersed atoms and limits the learning dictionary to only one atom, is proposed here. Its performances are illustrated when dealing with numerical and experimental signals as well as its usage for damage detection. Although the signal approximation method proposed in this paper finds an original application in the context of SHM, this method remains completely general and can be easily applied to any signal processing problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08929v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Rodriguez, Marc R\'ebillat, Shweta Paunikar, Pierre Margerit, Eric Monteiro, Francisco Chinesta, Nazih Mechbal</dc:creator>
    </item>
    <item>
      <title>Provide Proactive Reproducible Analysis Transparency with Every Publication</title>
      <link>https://arxiv.org/abs/2408.09103</link>
      <description>arXiv:2408.09103v1 Announce Type: new 
Abstract: The high incidence of irreproducible research has led to urgent appeals for transparency and equitable practices in open science. For the scientific disciplines that rely on computationally intensive analyses of large data sets, a granular understanding of the analysis methodology is an essential component of reproducibility. This paper discusses the guiding principles of a computational reproducibility framework that enables a scientist to proactively generate a complete reproducible trace as analysis unfolds, and share data, methods and executable tools as part of a scientific publication, allowing other researchers to verify results and easily re-execute the steps of the scientific investigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09103v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul Meijer, Nicole Howard, Jessica Liang, Autumn Kelsey, Sathya Subramanian, Ed Johnson, Paul Mariz, James Harvey, Madeline Ambrose, Vitalii Tereshchenko, Aldan Beaubien, Neelima Inala, Yousef Aggoune, Stark Pister, Anne Vetto, Melissa Kinsey, Tom Bumol, Ananda Goldrath, Xiaojun Li, Troy Torgerson, Peter Skene, Lauren Okada, Christian La France, Zach Thomson, Lucas Graybuck</dc:creator>
    </item>
    <item>
      <title>Effects of the Plan V\'elo I and II on vehicular flow in Paris -- An Empirical Analysis</title>
      <link>https://arxiv.org/abs/2408.09836</link>
      <description>arXiv:2408.09836v1 Announce Type: new 
Abstract: In recent years, Paris, France, transformed its transportation infrastructure, marked by a notable reallocation of space away from cars to active modes of transportation. Key initiatives driving this transformation included Plan V\'elo I and II, during which the city created over 1,000 kilometres of new bike paths to encourage cycling. For this, substantial road capacity has been removed from the system. This transformation provides a unique opportunity to investigate the impact of the large-scale network re-configuration on the network-wide traffic flow. Using the Network Fundamental Diagram (NFD) and a re-sampling methodology for its estimation, we investigate with empirical loop detector data from 2010 and 2023 the impact on the network's capacity, critical density, and free-flow speed resulting from these policy interventions. We find that in the urban core with the most policy interventions, per lane capacity decreased by over 50%, accompanied by a 60% drop in free-flow speed. Similarly, in the zone with fewer interventions, capacity declined by 34%, with a 40% reduction in free-flow speed. While these changes seem substantial, the NFDs show that overall congestion did not increase, indicating a modal shift to other modes of transport and hence presumably more sustainable urban mobility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09836v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elena Natterer, Allister Loder, Klaus Bogenberger</dc:creator>
    </item>
    <item>
      <title>Multi-layer diffusion model of photovoltaic installations</title>
      <link>https://arxiv.org/abs/2408.09904</link>
      <description>arXiv:2408.09904v1 Announce Type: new 
Abstract: Nowadays, harmful effects of climate change are becoming increasingly apparent. A vital issue that must be addressed is the generation of energy from non-renewable and often polluted sources. For this reason, the development of renewable energy sources is of great importance. Unfortunately, too rapid spread of renewables can disrupt stability of the power system and lead to energy blackouts. One should not simply support it, without ensuring sustainability and understanding of the diffusion process. In this research, we propose a new agent-based model of diffusion of photovoltaic panels. It is an extension of the $q$-voter model that utilizes multi-layer network structure. The model is analyzed by Monte Carlo simulations and mean-field approximation. The impact of parameters and specifications on the basic properties of the model is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09904v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomasz Weron, Janusz Szwabinski</dc:creator>
    </item>
    <item>
      <title>Liquid Fourier Latent Dynamics Networks for fast GPU-based numerical simulations in computational cardiology</title>
      <link>https://arxiv.org/abs/2408.09818</link>
      <description>arXiv:2408.09818v1 Announce Type: cross 
Abstract: Scientific Machine Learning (ML) is gaining momentum as a cost-effective alternative to physics-based numerical solvers in many engineering applications. In fact, scientific ML is currently being used to build accurate and efficient surrogate models starting from high-fidelity numerical simulations, effectively encoding the parameterized temporal dynamics underlying Ordinary Differential Equations (ODEs), or even the spatio-temporal behavior underlying Partial Differential Equations (PDEs), in appropriately designed neural networks. We propose an extension of Latent Dynamics Networks (LDNets), namely Liquid Fourier LDNets (LFLDNets), to create parameterized space-time surrogate models for multiscale and multiphysics sets of highly nonlinear differential equations on complex geometries. LFLDNets employ a neurologically-inspired, sparse, liquid neural network for temporal dynamics, relaxing the requirement of a numerical solver for time advancement and leading to superior performance in terms of tunable parameters, accuracy, efficiency and learned trajectories with respect to neural ODEs based on feedforward fully-connected neural networks. Furthermore, in our implementation of LFLDNets, we use a Fourier embedding with a tunable kernel in the reconstruction network to learn high-frequency functions better and faster than using space coordinates directly as input. We challenge LFLDNets in the framework of computational cardiology and evaluate their capabilities on two 3-dimensional test cases arising from multiscale cardiac electrophysiology and cardiovascular hemodynamics. This paper illustrates the capability to run Artificial Intelligence-based numerical simulations on single or multiple GPUs in a matter of minutes and represents a significant step forward in the development of physics-informed digital twins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09818v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Salvador, Alison L. Marsden</dc:creator>
    </item>
    <item>
      <title>Symplectic Neural Networks Based on Dynamical Systems</title>
      <link>https://arxiv.org/abs/2408.09821</link>
      <description>arXiv:2408.09821v1 Announce Type: cross 
Abstract: We present and analyze a framework for designing symplectic neural networks (SympNets) based on geometric integrators for Hamiltonian differential equations. The SympNets are universal approximators in the space of Hamiltonian diffeomorphisms, interpretable and have a non-vanishing gradient property. We also give a representation theory for linear systems, meaning the proposed P-SympNets can exactly parameterize any symplectic map corresponding to quadratic Hamiltonians. Extensive numerical tests demonstrate increased expressiveness and accuracy -- often several orders of magnitude better -- for lower training cost over existing architectures. Lastly, we show how to perform symbolic Hamiltonian regression with SympNets for polynomial systems using backward error analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09821v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin K Tapley</dc:creator>
    </item>
    <item>
      <title>Quantum computing with error mitigation for data-driven computational homogenization</title>
      <link>https://arxiv.org/abs/2312.14460</link>
      <description>arXiv:2312.14460v2 Announce Type: replace 
Abstract: As a crossover frontier of physics and mechanics, quantum computing is showing its great potential in computational mechanics. However, quantum hardware noise remains a critical barrier to achieving accurate simulation results due to the limitation of the current hardware. In this paper, we integrate error-mitigated quantum computing in data-driven computational homogenization, where the zero-noise extrapolation (ZNE) technique is employed to improve the reliability of quantum computing. Specifically, ZNE is utilized to mitigate the quantum hardware noise in two quantum algorithms for distance calculation, namely a Swap-based algorithm and an H-based algorithm, thereby improving the overall accuracy of data-driven computational homogenization. Numerical examples including a multiscale simulation of a composite L-shaped beam are conducted with the quantum computer simulator Qiskit, and the results validate the effectiveness of the proposed method. We believe this work presents a promising step towards using quantum computing in computational mechanics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14460v2</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zengtao Kuang, Yongchun Xu, Qun Huang, Jie Yang, Chafik El Kihal, Heng Hu</dc:creator>
    </item>
    <item>
      <title>Accurate, scalable, and efficient Bayesian optimal experimental design with derivative-informed neural operators</title>
      <link>https://arxiv.org/abs/2312.14810</link>
      <description>arXiv:2312.14810v3 Announce Type: replace 
Abstract: We consider optimal experimental design (OED) problems in selecting the most informative observation sensors to estimate model parameters in a Bayesian framework. Such problems are computationally prohibitive when the parameter-to-observable (PtO) map is expensive to evaluate, the parameters are high-dimensional, and the optimization for sensor selection is combinatorial and high-dimensional. To address these challenges, we develop an accurate, scalable, and efficient computational framework based on derivative-informed neural operators (DINO). We propose to use derivative-informed dimension reduction to reduce the parameter dimensions, based on which we train DINO with derivative information as an accurate and efficient surrogate for the PtO map and its derivative. Moreover, we derive DINO-enabled efficient formulations in computing the maximum a posteriori (MAP) point, the eigenvalues of approximate posterior covariance, and three commonly used optimality criteria for the OED problems. Furthermore, we provide detailed error analysis for the approximations of the MAP point, the eigenvalues, and the optimality criteria. We also propose a modified swapping greedy algorithm for the sensor selection optimization and demonstrate that the proposed computational framework is scalable to preserve the accuracy for increasing parameter dimensions and achieves high computational efficiency, with an over 1000$\times$ speedup accounting for both offline construction and online evaluation costs, compared to high-fidelity Bayesian OED solutions for a three-dimensional nonlinear convection-diffusion-reaction example with tens of thousands of parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14810v3</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinwoo Go, Peng Chen</dc:creator>
    </item>
    <item>
      <title>Black-Box Optimization with Implicit Constraints for Public Policy</title>
      <link>https://arxiv.org/abs/2310.18449</link>
      <description>arXiv:2310.18449v4 Announce Type: replace-cross 
Abstract: Black-box optimization (BBO) has become increasingly relevant for tackling complex decision-making problems, especially in public policy domains such as police redistricting. However, its broader application in public policymaking is hindered by the complexity of defining feasible regions and the high-dimensionality of decisions. This paper introduces a novel BBO framework, termed as the Conditional And Generative Black-box Optimization (CageBO). This approach leverages a conditional variational autoencoder to learn the distribution of feasible decisions, enabling a two-way mapping between the original decision space and a simplified, constraint-free latent space. The CageBO efficiently handles the implicit constraints often found in public policy applications, allowing for optimization in the latent space while evaluating objectives in the original space. We validate our method through a case study on large-scale police redistricting problems in Atlanta, Georgia. Our results reveal that our CageBO offers notable improvements in performance and efficiency compared to the baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18449v4</guid>
      <category>stat.ML</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqian Xing, Jungho Lee, Chong Liu, Shixiang Zhu</dc:creator>
    </item>
    <item>
      <title>Design Editing for Offline Model-based Optimization</title>
      <link>https://arxiv.org/abs/2405.13964</link>
      <description>arXiv:2405.13964v3 Announce Type: replace-cross 
Abstract: Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores. These tasks span various domains, such as robotics, material design, and protein and molecular engineering. A common approach involves training a surrogate model using existing designs and their corresponding scores, and then generating new designs through gradient-based updates with respect to the surrogate model. This method suffers from the out-of-distribution issue, where the surrogate model may erroneously predict high scores for unseen designs. To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization} (DEMO), which leverages a diffusion prior to calibrate overly optimized designs. DEMO first generates pseudo design candidates by performing gradient ascent with respect to a surrogate model. Then, an editing process refines these pseudo design candidates by introducing noise and subsequently denoising them with a diffusion prior trained on the offline dataset, ensuring they align with the distribution of valid designs. We provide a theoretical proof that the difference between the final optimized designs generated by DEMO and the prior distribution of the offline dataset is controlled by the noise injected during the editing process. Empirical evaluations on seven offline MBO tasks show that DEMO outperforms various baseline methods, achieving the highest mean rank of 2.1 and a median rank of 1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13964v3</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ye Yuan, Youyuan Zhang, Can Chen, Haolun Wu, Zixuan Li, Jianmo Li, James J. Clark, Xue Liu</dc:creator>
    </item>
    <item>
      <title>A Theoretical Framework for an Efficient Normalizing Flow-Based Solution to the Electronic Schrodinger Equation</title>
      <link>https://arxiv.org/abs/2406.00047</link>
      <description>arXiv:2406.00047v2 Announce Type: replace-cross 
Abstract: A central problem in quantum mechanics involves solving the Electronic Schrodinger Equation for a molecule or material. The Variational Monte Carlo approach to this problem approximates a particular variational objective via sampling, and then optimizes this approximated objective over a chosen parameterized family of wavefunctions, known as the ansatz. Recently neural networks have been used as the ansatz, with accompanying success. However, sampling from such wavefunctions has required the use of a Markov Chain Monte Carlo approach, which is inherently inefficient. In this work, we propose a solution to this problem via an ansatz which is cheap to sample from, yet satisfies the requisite quantum mechanical properties. We prove that a normalizing flow using the following two essential ingredients satisfies our requirements: (a) a base distribution which is constructed from Determinantal Point Processes; (b) flow layers which are equivariant to a particular subgroup of the permutation group. We then show how to construct both continuous and discrete normalizing flows which satisfy the requisite equivariance. We further demonstrate the manner in which the non-smooth nature ("cusps") of the wavefunction may be captured, and how the framework may be generalized to provide induction across multiple molecules. The resulting theoretical framework entails an efficient approach to solving the Electronic Schrodinger Equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00047v2</guid>
      <category>physics.chem-ph</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Freedman, Eyal Rozenberg, Alex Bronstein</dc:creator>
    </item>
  </channel>
</rss>
