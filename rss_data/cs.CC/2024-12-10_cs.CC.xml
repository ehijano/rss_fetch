<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Dec 2024 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the Expressive Power of Modern Hopfield Networks</title>
      <link>https://arxiv.org/abs/2412.05562</link>
      <description>arXiv:2412.05562v1 Announce Type: new 
Abstract: Modern Hopfield networks (MHNs) have emerged as powerful tools in deep learning, capable of replacing components such as pooling layers, LSTMs, and attention mechanisms. Recent advancements have enhanced their storage capacity, retrieval speed, and error rates. However, the fundamental limits of their computational expressiveness remain unexplored. Understanding the expressive power of MHNs is crucial for optimizing their integration into deep learning architectures. In this work, we establish rigorous theoretical bounds on the computational capabilities of MHNs using circuit complexity theory. Our key contribution is that we show that MHNs are $\mathsf{DLOGTIME}$-uniform $\mathsf{TC}^0$. Hence, unless $\mathsf{TC}^0 = \mathsf{NC}^1$, a $\mathrm{poly}(n)$-precision modern Hopfield networks with a constant number of layers and $O(n)$ hidden dimension cannot solve $\mathsf{NC}^1$-hard problems such as the undirected graph connectivity problem and the tree isomorphism problem. We also extended our results to Kernelized Hopfield Networks. These results demonstrate the limitation in the expressive power of the modern Hopfield networks. Moreover, Our theoretical analysis provides insights to guide the development of new Hopfield-based architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05562v1</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Li, Yuanpeng Li, Yingyu Liang, Zhenmei Shi, Zhao Song</dc:creator>
    </item>
    <item>
      <title>The Computational Limits of State-Space Models and Mamba via the Lens of Circuit Complexity</title>
      <link>https://arxiv.org/abs/2412.06148</link>
      <description>arXiv:2412.06148v1 Announce Type: new 
Abstract: In this paper, we analyze the computational limitations of Mamba and State-space Models (SSMs) by using the circuit complexity framework. Despite Mamba's stateful design and recent attention as a strong candidate to outperform Transformers, we have demonstrated that both Mamba and SSMs with $\mathrm{poly}(n)$-precision and constant-depth layers reside within the $\mathsf{DLOGTIME}$-uniform $\mathsf{TC}^0$ complexity class. This result indicates Mamba has the same computational capabilities as Transformer theoretically, and it cannot solve problems like arithmetic formula problems, boolean formula value problems, and permutation composition problems if $\mathsf{TC}^0 \neq \mathsf{NC}^1$. Therefore, it challenges the assumption Mamba is more computationally expressive than Transformers. Our contributions include rigorous proofs showing that Selective SSM and Mamba architectures can be simulated by $\mathsf{DLOGTIME}$-uniform $\mathsf{TC}^0$ circuits, and they cannot solve problems outside $\mathsf{TC}^0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06148v1</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song</dc:creator>
    </item>
    <item>
      <title>Query Complexity with Unknowns</title>
      <link>https://arxiv.org/abs/2412.06395</link>
      <description>arXiv:2412.06395v1 Announce Type: new 
Abstract: We initiate the study of a new model of query complexity of Boolean functions where, in addition to 0 and 1, the oracle can answer queries with ``unknown''. The query algorithm is expected to output the function value if it can be conclusively determined by the partial information gathered, and it must output ``unknown'' if not. We formalize this model by using Kleene's strong logic of indeterminacy on three variables to capture unknowns. We call this model the `u-query model'.
  We relate the query complexity of functions in the new u-query model with their analogs in the standard query model. We show an explicit function that is exponentially harder in the u-query model than in the usual query model. We give sufficient conditions for a function to have u-query complexity asymptotically the same as its query complexity.
  Using u-query analogs of the combinatorial measures of sensitivity, block sensitivity, and certificate complexity, we show that deterministic, randomized, and quantum u-query complexities of all total Boolean functions are polynomially related to each other, just as in the usual query models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06395v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikhil S. Mande, Karteek Sreenivasaiah</dc:creator>
    </item>
    <item>
      <title>Direct Sums for Parity Decision Trees</title>
      <link>https://arxiv.org/abs/2412.06552</link>
      <description>arXiv:2412.06552v1 Announce Type: new 
Abstract: Direct sum theorems state that the cost of solving $k$ instances of a problem is at least $\Omega(k)$ times the cost of solving a single instance. We prove the first such results in the randomised parity decision tree model. We show that a direct sum theorem holds whenever (1) the lower bound for parity decision trees is proved using the discrepancy method; or (2) the lower bound is proved relative to a product distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06552v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tyler Besselman, Mika G\"o\"os, Siyao Guo, Gilbert Maystre, Weiqiang Yuan</dc:creator>
    </item>
    <item>
      <title>PAC codes with Bounded-Complexity Sequential Decoding: Pareto Distribution and Code Design</title>
      <link>https://arxiv.org/abs/2412.06072</link>
      <description>arXiv:2412.06072v1 Announce Type: cross 
Abstract: Recently, a novel variation of polar codes known as polarization-adjusted convolutional (PAC) codes has been introduced by Ar{\i}kan. These codes significantly outperform conventional polar and convolutional codes, particularly for short codeword lengths, and are shown to operate very close to the optimal bounds. It has also been shown that if the rate profile of PAC codes does not adhere to certain polarized cutoff rate constraints, the computation complexity for their sequential decoding grows exponentially. In this paper, we address the converse problem, demonstrating that if the rate profile of a PAC code follows the polarized cutoff rate constraints, the required computations for its sequential decoding can be bounded with a distribution that follows a Pareto distribution. This serves as a guideline for the rate-profile design of PAC codes. For a high-rate PAC\,$(1024,899)$ code, simulation results show that the PAC code with Fano decoder, when constructed based on the polarized cutoff rate constraints, achieves a coding gain of more than $0.75$ dB at a frame error rate (FER) of $10^{-5}$ compared to the state-of-the-art 5G polar and LDPC codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06072v1</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>math.IT</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohsen Moradi, Hessam Mahdavifar</dc:creator>
    </item>
    <item>
      <title>Fast Matrix Multiplication meets the Subdmodular Width</title>
      <link>https://arxiv.org/abs/2412.06189</link>
      <description>arXiv:2412.06189v1 Announce Type: cross 
Abstract: One fundamental question in database theory is the following: Given a Boolean conjunctive query Q, what is the best complexity for computing the answer to Q in terms of the input database size N? When restricted to the class of combinatorial algorithms, it is known that the best known complexity for any query Q is captured by the submodular width of Q. However, beyond combinatorial algorithms, certain queries are known to admit faster algorithms that often involve a clever combination of fast matrix multiplication and data partitioning. Nevertheless, there is no systematic way to derive and analyze the complexity of such algorithms for arbitrary queries Q.
  In this work, we introduce a general framework that captures the best complexity for answering any Boolean conjunctive query Q using matrix multiplication. Our framework unifies both combinatorial and non-combinatorial techniques under the umbrella of information theory. It generalizes the notion of submodular width to a new stronger notion called the omega-submodular width that naturally incorporates the power of fast matrix multiplication. We describe a matching algorithm that computes the answer to any query Q in time corresponding to the omega-submodular width of Q. We show that our framework recovers the best known complexities for Boolean queries that have been studied in the literature, to the best of our knowledge, and also discovers new algorithms for some classes of queries that improve upon the best known complexities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06189v1</guid>
      <category>cs.DB</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahmoud Abo-Khamis, Xiao Hu, Dan Suciu</dc:creator>
    </item>
    <item>
      <title>Plagiarism Detection Using Machine Learning</title>
      <link>https://arxiv.org/abs/2412.06241</link>
      <description>arXiv:2412.06241v1 Announce Type: cross 
Abstract: Plagiarism is an act of using someone else's work without proper acknowledgment, and this sin is seen to cut across various arenas including the academy, publishing, and other similar arenas. The traditional methods of plagiarism detection through keyword matching and review by humans usually fail to cope with increasingly sophisticated techniques used to mask copy pasted content. This paper aims to introduce a plagiarism detection approach based on machine learning that utilizes natural language processing and complex classification algorithms toward efficient detection of similarities between the documents. The developed model has the capability to detect both exact and paraphrased plagiarism accurately using advanced feature extraction techniques with supervised learning algorithms. We adapted and tested our model on an extensive text sample dataset. And we demonstrated some promising results about precision, recall, and detection accuracy. These outcomes showed that applying machine learning techniques can significantly enhance the functionalities of plagiarism detection systems and improve traditional ones with robust scalability. Future work would include enlargement of the dataset and fine-tuning of the model toward more complicated cases of disguised plagiarism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06241v1</guid>
      <category>cs.ET</category>
      <category>cs.CC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Omraj Kamat, Tridib Ghosh, Kalaivani J, Angayarkanni V, Rama P</dc:creator>
    </item>
    <item>
      <title>How many continuous measurements are needed to learn a vector?</title>
      <link>https://arxiv.org/abs/2412.06468</link>
      <description>arXiv:2412.06468v1 Announce Type: cross 
Abstract: One can recover vectors from $\mathbb{R}^m$ with arbitrary precision, using only $\lceil \log_2(m+1)\rceil +1$ continuous measurements that are chosen adaptively. This surprising result is explained and discussed, and we present applications to infinite-dimensional approximation problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06468v1</guid>
      <category>math.NA</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Krieg, Erich Novak, Mario Ullrich</dc:creator>
    </item>
    <item>
      <title>Broadcast Channel Coding: Algorithmic Aspects and Non-Signaling Assistance</title>
      <link>https://arxiv.org/abs/2310.05515</link>
      <description>arXiv:2310.05515v2 Announce Type: replace-cross 
Abstract: We address the problem of coding for classical broadcast channels, which entails maximizing the success probability that can be achieved by sending a fixed number of messages over a broadcast channel. For point-to-point channels, Barman and Fawzi found in~\cite{BF18} a $(1-e^{-1})$-approximation algorithm running in polynomial time, and showed that it is \textrm{NP}-hard to achieve a strictly better approximation ratio. Furthermore, these algorithmic results were at the core of the limitations they established on the power of non-signaling assistance for point-to-point channels. It is natural to ask if similar results hold for broadcast channels, exploiting links between approximation algorithms of the channel coding problem and the non-signaling assisted capacity region.
  In this work, we make several contributions on algorithmic aspects and non-signaling assisted capacity regions of broadcast channels. For the class of deterministic broadcast channels, we describe a $(1-e^{-1})^2$-approximation algorithm running in polynomial time, and we show that the capacity region for that class is the same with or without non-signaling assistance. Finally, we show that in the value query model, we cannot achieve a better approximation ratio than $\Omega\left(\frac{1}{\sqrt{m}}\right)$ in polynomial time for the general broadcast channel coding problem, with $m$ the size of one of the outputs of the channel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05515v2</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>math.IT</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2024.3410047</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Information Theory (Volume: 70, Issue: 11, November 2024)</arxiv:journal_reference>
      <dc:creator>Omar Fawzi, Paul Ferm\'e</dc:creator>
    </item>
    <item>
      <title>Rethinking Model-based, Policy-based, and Value-based Reinforcement Learning via the Lens of Representation Complexity</title>
      <link>https://arxiv.org/abs/2312.17248</link>
      <description>arXiv:2312.17248v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning (RL) encompasses diverse paradigms, including model-based RL, policy-based RL, and value-based RL, each tailored to approximate the model, optimal policy, and optimal value function, respectively. This work investigates the potential hierarchy of representation complexity -- the complexity of functions to be represented -- among these RL paradigms. We first demonstrate that, for a broad class of Markov decision processes (MDPs), the model can be represented by constant-depth circuits with polynomial size or Multi-Layer Perceptrons (MLPs) with constant layers and polynomial hidden dimension. However, the representation of the optimal policy and optimal value proves to be $\mathsf{NP}$-complete and unattainable by constant-layer MLPs with polynomial size. This demonstrates a significant representation complexity gap between model-based RL and model-free RL, which includes policy-based RL and value-based RL. To further explore the representation complexity hierarchy between policy-based RL and value-based RL, we introduce another general class of MDPs where both the model and optimal policy can be represented by constant-depth circuits with polynomial size or constant-layer MLPs with polynomial size. In contrast, representing the optimal value is $\mathsf{P}$-complete and intractable via a constant-layer MLP with polynomial hidden dimension. This accentuates the intricate representation complexity associated with value-based RL compared to policy-based RL. In summary, we unveil a potential representation complexity hierarchy within RL -- representing the model emerges as the easiest task, followed by the optimal policy, while representing the optimal value function presents the most intricate challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17248v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guhao Feng, Han Zhong</dc:creator>
    </item>
    <item>
      <title>On the MST-ratio: Theoretical Bounds and Complexity of Finding the Maximum</title>
      <link>https://arxiv.org/abs/2409.11079</link>
      <description>arXiv:2409.11079v2 Announce Type: replace-cross 
Abstract: Given a finite set of red and blue points in $\Rspace^d$, the MST-ratio is defined as the total length of the Euclidean minimum spanning trees of the red points and the blue points, divided by the length of the Euclidean minimum spanning tree of their union. The maximum MST-ratio of a point set is the maximum MST-ratio over all non-trivial colorings of its points by red and blue. We prove that finding the maximum MST-ratio of a given point set is NP-hard when the dimension is part of the input. Moreover, we present a quadratic-time $3$-approximation algorithm for this problem. As part of the proof, we show that, in any metric space, the maximum MST-ratio is smaller than $3$. Additionally, we study the average MST-ratio over all colorings of a set of $n$ points. We show that this average is always at least $\frac{n-2}{n-1}$, and for $n$ random points uniformly distributed in a $d$-dimensional unit cube, the average tends to $\sqrt[d]{2}$ in expectation as $n$ approaches infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11079v2</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Afrouz Jabal Ameli, Faezeh Motiei, Morteza Saghafian</dc:creator>
    </item>
    <item>
      <title>Paired-domination Problem on Circle and $k$-polygon Graphs</title>
      <link>https://arxiv.org/abs/2411.19473</link>
      <description>arXiv:2411.19473v3 Announce Type: replace-cross 
Abstract: A vertex set $D \subseteq V$ is considered a dominating set of $G$ if every vertex in $V - D$ is adjacent to at least one vertex in $D$. We called a dominating set $D$ as a paired-dominating set if the subgraph of $G$ induced by $D$ contains a perfect matching. In this paper, we show that determining the minimum paired-dominating set on circle graphs is NP-complete. We further propose an $O(n(\frac{n}{k^2-k})^{2k^2-2k})$-time algorithm for $k$-polygon graphs, a subclass of circle graphs, for finding the minimum paired-dominating set. Moreover, we extend our method to improve the algorithm for finding the minimum dominating set on $k$-polygon graphs in~[\emph{E.S.~Elmallah and L.K.~Stewart, Independence and domination in polygon graphs, Discrete Appl. Math., 1993}] and reduce their time-complexity from $O(n^{4k^2+3})$ to $O(n(\frac{n}{k^2-k})^{2k^2-4k})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19473v3</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ta-Yu Mu, Ching-Chi Lin</dc:creator>
    </item>
  </channel>
</rss>
