<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Aug 2024 01:39:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 31 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Warped multifidelity Gaussian processes for data fusion of skewed environmental data</title>
      <link>https://arxiv.org/abs/2407.20295</link>
      <description>arXiv:2407.20295v1 Announce Type: new 
Abstract: Understanding the dynamics of climate variables is paramount for numerous sectors, like energy and environmental monitoring. This study focuses on the critical need for a precise mapping of environmental variables for national or regional monitoring networks, a task notably challenging when dealing with skewed data. To address this issue, we propose a novel data fusion approach, the \textit{warped multifidelity Gaussian process} (WMFGP). The method performs prediction using multiple time-series, accommodating varying reliability and resolutions and effectively handling skewness. In an extended simulation experiment the benefits and the limitations of the methods are explored, while as a case study, we focused on the wind speed monitored by the network of ARPA Lombardia, one of the regional environmental agencies operting in Italy. ARPA grapples with data gaps, and due to the connection between wind speed and air quality, it struggles with an effective air quality management. We illustrate the efficacy of our approach in filling the wind speed data gaps through two extensive simulation experiments. The case study provides more informative wind speed predictions crucial for predicting air pollutant concentrations, enhancing network maintenance, and advancing understanding of relevant meteorological and climatic phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20295v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pietro Colombo, Claire Miller, Xiaochen Yang, Ruth O'Donnell, Paolo Maranzano</dc:creator>
    </item>
    <item>
      <title>Parameters Evolution in Source-Sink Space Population Evolutionary Models</title>
      <link>https://arxiv.org/abs/2407.21000</link>
      <description>arXiv:2407.21000v1 Announce Type: new 
Abstract: MOCAT-SSEM is a Source-Sink model that predicts the Low Earth Orbit (LEO) space population divided into families using a predefined set of interaction parameters. Thanks to data from the Monte Carlo version of the model (MOCAT-MC), which propagates singularly every object, it is possible to estimate such parameters, assumed as additional stochastic variables. Thus, this paper proposed a new set of parameters so that the new Source-Sink model prediction better fits the computationally expensive and accurate MOCAT-MC simulation. Estimation is performed by extracting stochastic quantities from the space population, which has been analyzed to fit common probability density functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21000v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Erin Ashley, Carla Simon Sanz, Simone Servadio, Giovanni Lavezzi</dc:creator>
    </item>
    <item>
      <title>Solve the Refugee Crisis with Data</title>
      <link>https://arxiv.org/abs/2407.20235</link>
      <description>arXiv:2407.20235v1 Announce Type: cross 
Abstract: In this study, we addressed the refugee crisis through two main models. For predicting the ultimate number of refugees, we first established a Logistic Regression Model, but due to the limited data points, its prediction accuracy was suboptimal. Consequently, we incorporated Gray Theory to develop the Gary Verhulst Model, which provided scientifically sound and reasonable predictions. Statistical tests comparing both models highlighted the superiority of the Gary Verhulst Model. For formulating refugee allocation schemes, we initially used the Factor Analysis Method but found it too subjective and lacking in rigorous validation measures. We then developed a Refugee Allocation Model based on the Analytic Hierarchy Process (AHP), which absorbed the advantages of the former method. This model underwent extensive validation and passed consistency checks, resulting in an effective and scientific refugee allocation scheme. We also compared our model with the current allocation schemes and proposed improvements. Finally, we discussed the advantages and disadvantages of our models, their applicability, and scalability. Sensitivity analysis was conducted, and directions for future model improvements were identified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20235v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunfei Liu</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification under Noisy Constraints, with Applications to Raking</title>
      <link>https://arxiv.org/abs/2407.20520</link>
      <description>arXiv:2407.20520v1 Announce Type: cross 
Abstract: We consider statistical inference problems under uncertain equality constraints, and provide asymptotically valid uncertainty estimates for inferred parameters. The proposed approach leverages the implicit function theorem and primal-dual optimality conditions for a particular problem class. The motivating application is multi-dimensional raking, where observations are adjusted to match marginals; for example, adjusting estimated deaths across race, county, and cause in order to match state all-race all-cause totals. We review raking from a convex optimization perspective, providing explicit primal-dual formulations, algorithms, and optimality conditions for a wide array of raking applications, which are then leveraged to obtain the uncertainty estimates. Empirical results show that the approach obtains, at the cost of a single solve, nearly the same uncertainty estimates as computationally intensive Monte Carlo techniques that pass thousands of observed and of marginal draws through the entire raking process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20520v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ariane Ducellier (Institute for Health Metrics and Evaluation, Seattle, WA), Alexander Hsu (Institute for Health Metrics and Evaluation, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA), Parkes Kendrick (Institute for Health Metrics and Evaluation, Seattle, WA), Bill Gustafson (Institute for Health Metrics and Evaluation, Seattle, WA), Laura Dwyer-Lindgren (Institute for Health Metrics and Evaluation, Seattle, WA), Christopher Murray (Institute for Health Metrics and Evaluation, Seattle, WA), Peng Zheng (Institute for Health Metrics and Evaluation, Seattle, WA), Aleksandr Aravkin (Institute for Health Metrics and Evaluation, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA)</dc:creator>
    </item>
    <item>
      <title>Powerful A/B-Testing Metrics and Where to Find Them</title>
      <link>https://arxiv.org/abs/2407.20665</link>
      <description>arXiv:2407.20665v1 Announce Type: cross 
Abstract: Online controlled experiments, colloquially known as A/B-tests, are the bread and butter of real-world recommender system evaluation. Typically, end-users are randomly assigned some system variant, and a plethora of metrics are then tracked, collected, and aggregated throughout the experiment. A North Star metric (e.g. long-term growth or revenue) is used to assess which system variant should be deemed superior. As a result, most collected metrics are supporting in nature, and serve to either (i) provide an understanding of how the experiment impacts user experience, or (ii) allow for confident decision-making when the North Star metric moves insignificantly (i.e. a false negative or type-II error). The latter is not straightforward: suppose a treatment variant leads to fewer but longer sessions, with more views but fewer engagements; should this be considered a positive or negative outcome?
  The question then becomes: how do we assess a supporting metric's utility when it comes to decision-making using A/B-testing? Online platforms typically run dozens of experiments at any given time. This provides a wealth of information about interventions and treatment effects that can be used to evaluate metrics' utility for online evaluation. We propose to collect this information and leverage it to quantify type-I, type-II, and type-III errors for the metrics of interest, alongside a distribution of measurements of their statistical power (e.g. $z$-scores and $p$-values). We present results and insights from building this pipeline at scale for two large-scale short-video platforms: ShareChat and Moj; leveraging hundreds of past experiments to find online metrics with high statistical power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20665v1</guid>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier Jeunen, Shubham Baweja, Neeti Pokharna, Aleksei Ustimenko</dc:creator>
    </item>
    <item>
      <title>ROC curve analysis for functional markers</title>
      <link>https://arxiv.org/abs/2407.20929</link>
      <description>arXiv:2407.20929v1 Announce Type: cross 
Abstract: Functional markers become a more frequent tool in medical diagnosis. In this paper, we aim to define an index allowing to discriminate between populations when the observations are functional data belonging to a Hilbert space. We discuss some of the problems arising when estimating optimal directions defined to maximize the area under the curve of a projection index and we construct the corresponding ROC curve. We also go one step forward and consider the case of possibly different covariance operators, for which we recommend a quadratic discrimination rule. Consistency results are derived for both linear and quadratic indexes, under mild conditions. The results of our numerical experiments allow to see the advantages of the quadratic rule when the populations have different covariance operators. We also illustrate the considered methods on a real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20929v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana M. Bianco, Graciela Boente, Juan Carlos Pardo-Fern\'andez</dc:creator>
    </item>
    <item>
      <title>A changepoint approach to modelling non-stationary soil moisture dynamics</title>
      <link>https://arxiv.org/abs/2310.17546</link>
      <description>arXiv:2310.17546v2 Announce Type: replace 
Abstract: Soil moisture dynamics provide an indicator of soil health that scientists model via drydown curves. The typical modelling process requires the soil moisture time series to be manually separated into drydown segments and then exponential decay models are fitted to them independently. Sensor development over recent years means that experiments that were previously conducted over a few field campaigns can now be scaled to months or years at a higher sampling rate. To better meet the challenge of increasing data size, this paper proposes a novel changepoint-based approach to automatically identify structural changes in the soil drying process and simultaneously estimate the drydown parameters that are of interest to soil scientists. A simulation study is carried out to demonstrate the performance of the method in detecting changes and retrieving model parameters. Practical aspects of the method such as adding covariates and penalty learning are discussed. The method is applied to hourly soil moisture time series from the NEON data portal to investigate the temporal dynamics of soil moisture drydown. We recover known relationships previously identified manually, alongside delivering new insights into the temporal variability across soil types and locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17546v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengyi Gong, Rebecca Killick, Christopher Nemeth, John Quinton</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices</title>
      <link>https://arxiv.org/abs/2403.05441</link>
      <description>arXiv:2403.05441v2 Announce Type: replace 
Abstract: We present a first study of Bayesian forecasting of electricity prices traded on the German continuous intraday market which fully incorporates parameter uncertainty. A particularly large set of endogenous and exogenous covariables is used, handled through feature selection with Orthogonal Matching Pursuit (OMP) and regularising priors. Our target variable is the IDFull price index, forecasts are given in terms of posterior predictive distributions. For validation we use the exceedingly volatile electricity prices of 2022, which have hardly been the subject of forecasting studies before. As a benchmark model, we use all available intraday transactions at the time of forecast creation to compute a current value for the IDFull. According to the weak-form efficiency hypothesis, it would not be possible to significantly improve this benchmark built from last price information. We do, however, observe statistically significant improvement in terms of both point measures and probability scores. Finally, we challenge the declared gold standard of using LASSO for feature selection in electricity price forecasting by presenting strong statistical evidence that OMP leads to better forecasting performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05441v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniel Nickelsen, Gernot M\"uller</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal areal models to support small area estimation: An application to national-scale forest carbon monitoring</title>
      <link>https://arxiv.org/abs/2407.09909</link>
      <description>arXiv:2407.09909v2 Announce Type: replace 
Abstract: National Forest Inventory (NFI) programs can provide vital information on the status, trend, and change in forest parameters. These programs are being increasingly asked to provide forest parameter estimates for spatial and temporal extents smaller than their current design and accompanying design-based methods can deliver with desired levels of uncertainty. Many NFI designs and estimation methods focus on status and are not well equipped to provide acceptable estimates for trend and change parameters, especially over small spatial domains and/or short time periods. Fine-scale space-time indexed estimates are critical to a variety of environmental, ecological, and economic monitoring efforts. Estimates for forest carbon status, trend, and change are of particular importance to international initiatives to track carbon dynamics. Model-based small area estimation (SAE) methods for NFI and similar ecological monitoring data typically pursue inference on status within small spatial domains, with few demonstrated methods that account for spatio-temporal dependence needed for trend and change estimation. We propose a spatio-temporal Bayesian model framework that delivers statistically valid estimates with full uncertainty quantification for status, trend, and change. The framework accommodates a variety of space and time dependency structures, and we detail model configurations for different settings. Through analysis of simulated datasets, we compare the relative performance of candidate models and a traditional direct estimator. We then apply candidate models to a large-scale NFI dataset to demonstrate the utility of the proposed framework for providing unique quantification of forest carbon dynamics in the contiguous United States. We also provide computationally efficient algorithms, software, and data to reproduce our results and for benchmarking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09909v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elliot S. Shannon, Andrew O. Finley, Paul B. May, Grant M. Domke, Hans-Erik Andersen, George C. Gaines III, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Inferring a population composition from survey data with nonignorable nonresponse: Borrowing information from external sources</title>
      <link>https://arxiv.org/abs/2210.08346</link>
      <description>arXiv:2210.08346v2 Announce Type: replace-cross 
Abstract: We introduce a method to make inference on the composition of a heterogeneous population using survey data, accounting for the possibility that capture heterogeneity is related to key survey variables. To deal with nonignorable nonresponse, we combine different data sources and propose the use of Fisher's noncentral hypergeometric model in a Bayesian framework. To illustrate the potentialities of our methodology, we focus on a case study aimed at estimating the composition of the population of Italian graduates by their occupational status one year after graduating, stratifying by gender and degree program. We account for the possibility that surveys inquiring about the occupational status of new graduates may have response rates that depend on individuals' employment status, implying the nonignorability of the nonresponse. Our findings show that employed people are generally more inclined to answer the questionnaire. Neglecting the nonresponse bias in such contexts might lead to overestimating the employment rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08346v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Veronica Ballerini, Brunero Liseo</dc:creator>
    </item>
    <item>
      <title>Thompson sampling for zero-inflated count outcomes with an application to the Drink Less mobile health study</title>
      <link>https://arxiv.org/abs/2311.14359</link>
      <description>arXiv:2311.14359v2 Announce Type: replace-cross 
Abstract: Mobile health (mHealth) interventions often aim to improve distal outcomes, such as clinical conditions, by optimizing proximal outcomes through just-in-time adaptive interventions. Contextual bandits provide a suitable framework for customizing such interventions according to individual time-varying contexts. However, unique challenges, such as modeling count outcomes within bandit frameworks, have hindered the widespread application of contextual bandits to mHealth studies. The current work addresses this challenge by leveraging count data models into online decision-making approaches. Specifically, we combine four common offline count data models (Poisson, negative binomial, zero-inflated Poisson, and zero-inflated negative binomial regressions) with Thompson sampling, a popular contextual bandit algorithm. The proposed algorithms are motivated by and evaluated on a real dataset from the Drink Less trial, where they are shown to improve user engagement with the mHealth platform. The proposed methods are further evaluated on simulated data, achieving improvement in maximizing cumulative proximal outcomes over existing algorithms. Theoretical results on regret bounds are also derived. The countts R package provides an implementation of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14359v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xueqing Liu, Nina Deliu, Tanujit Chakraborty, Lauren Bell, Bibhas Chakraborty</dc:creator>
    </item>
    <item>
      <title>Model-based Clustering of Zero-Inflated Single-Cell RNA Sequencing Data via the EM Algorithm</title>
      <link>https://arxiv.org/abs/2406.00245</link>
      <description>arXiv:2406.00245v2 Announce Type: replace-cross 
Abstract: Biological cells can be distinguished by their phenotype or at the molecular level, based on their genome, epigenome, and transcriptome. This paper focuses on the transcriptome, which encompasses all the RNA transcripts in a given cell population, indicating the genes being expressed at a given time. We consider single-cell RNA sequencing data and develop a novel model-based clustering method to group cells based on their transcriptome profiles. Our clustering approach takes into account the presence of zero inflation in the data, which can occur due to genuine biological zeros or technological noise. The proposed model for clustering involves a mixture of zero-inflated Poisson or zero-inflated negative binomial distributions, and parameter estimation is carried out using the EM algorithm. We evaluate the performance of our proposed methodology through simulation studies and analyses of publicly available datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00245v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zahra AghahosseinaliShirazi, Pedro A. Rangel, Camila P. E. de Souza</dc:creator>
    </item>
    <item>
      <title>A Bayesian modelling framework for health care resource use and costs in trial-based economic evaluations</title>
      <link>https://arxiv.org/abs/2407.17036</link>
      <description>arXiv:2407.17036v2 Announce Type: replace-cross 
Abstract: Individual-level effectiveness and healthcare resource use (HRU) data are routinely collected in trial-based economic evaluations. While effectiveness is often expressed in terms of utility scores derived from some health-related quality of life instruments (e.g.~EQ-5D questionnaires), different types of HRU may be included. Costs are usually generated by applying unit prices to HRU data and statistical methods have been traditionally implemented to analyse costs and utilities or after combining them into aggregated variables (e.g. Quality-Adjusted Life Years). When outcome data are not fully observed, e.g. some patients drop out or only provided partial information, the validity of the results may be hindered both in terms of efficiency and bias. Often, partially-complete HRU data are handled using "ad-hoc" methods, implicitly relying on some assumptions (e.g. fill-in a zero) which are hard to justify beside the practical convenience of increasing the completion rate. We present a general Bayesian framework for the modelling of partially-observed HRUs which allows a flexible model specification to accommodate the typical complexities of the data and to quantify the impact of different types of uncertainty on the results. We show the benefits of using our approach using a motivating example and compare the results to those from traditional analyses focussed on the modelling of cost variables after adopting some ad-hoc imputation strategy for HRU data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17036v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Gabrio</dc:creator>
    </item>
  </channel>
</rss>
