<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Oct 2024 04:00:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Constructing Interpretable Prediction Models with 1D DNNs: An Example in Irregular ECG Classification</title>
      <link>https://arxiv.org/abs/2410.12059</link>
      <description>arXiv:2410.12059v1 Announce Type: new 
Abstract: This manuscript proposes a novel methodology for developing an interpretable prediction model for irregular Electrocardiogram (ECG) classification, using features extracted by a 1-D Deconvolutional Neural Network (1-D DNN). Given the increasing prevalence of cardiovascular disease, there is a growing demand for models that provide transparent and clinically relevant predictions, which are essential for advancing the development of automated diagnostic tools.
  The features extracted by the 1-D DNN are included in a simple Logistic Regression (LR) model to predict abnormal ECG patterns. Our analysis demonstrates that the features are consistent with clinical knowledge and provide an interpretable and reliable classification of conditions such as Atrial Fibrillation (AF), Myocardial Infarction (MI), and Sinus Bradycardia Rhythm (SBR).
  Moreover, our findings show that the simple LR model has similar predictive accuracy to more complex models, such as a 1-D Convolutional Neural Network (1-D CNN), providing a concrete example of how to efficiently integrate Explainable Artificial Intelligence (XAI) methodologies with traditional regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12059v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giacomo Lancia, Cristian Spitoni</dc:creator>
    </item>
    <item>
      <title>K-Contact Distance for Noisy Nonhomogeneous Spatial Point Data with application to Repeating Fast Radio Burst sources</title>
      <link>https://arxiv.org/abs/2410.12146</link>
      <description>arXiv:2410.12146v1 Announce Type: new 
Abstract: This paper introduces an approach to analyze nonhomogeneous Poisson processes (NHPP) observed with noise, focusing on previously unstudied second-order characteristics of the noisy process. Utilizing a hierarchical Bayesian model with noisy data, we estimate hyperparameters governing a physically motivated NHPP intensity. Simulation studies demonstrate the reliability of this methodology in accurately estimating hyperparameters. Leveraging the posterior distribution, we then infer the probability of detecting a certain number of events within a given radius, the $k$-contact distance. We demonstrate our methodology with an application to observations of fast radio bursts (FRBs) detected by the Canadian Hydrogen Intensity Mapping Experiment's FRB Project (CHIME/FRB). This approach allows us to identify repeating FRB sources by bounding or directly simulating the probability of observing $k$ physically independent sources within some radius in the detection domain, or the $\textit{probability of coincidence}$ ($P_{\text{C}}$). The new methodology improves the repeater detection $P_{\text{C}}$ in 86% of cases when applied to the largest sample of previously classified observations, with a median improvement factor (existing metric over $P_{\text{C}}$ from our methodology) of $\sim$ 3000.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12146v1</guid>
      <category>stat.AP</category>
      <category>astro-ph.IM</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. M. Cook (David A. Dunlap Department of Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON, Dunlap Institute for Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON), Dayi Li (Department of Statistical Science, University of Toronto, Toronto, ON), Gwendolyn M. Eadie (Department of Statistical Science, University of Toronto, Toronto, ON, David A. Dunlap Department of Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON), David C. Stenning (Department of Statistics and Actuarial Science, Simon Fraser University, Burnaby, BC), Paul Scholz (Department of Physics and Astronomy, York University, Toronto, ON, Dunlap Institute for Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON), Derek Bingham (Department of Statistics and Actuarial Science, Simon Fraser University, Burnaby, BC), Radu Craiu (Department of Statistical Science, University of Toronto, Toronto, ON), B. M. Gaensler (Department of Astronomy and Astrophysics, University of California, Santa Cruz, Santa Cruz, CA, David A. Dunlap Department of Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON), Kiyoshi W. Masui (MIT Kavli Institute for Astrophysics and Space Research, Massachusetts Institute of Technology, Cambridge, MA, Department of Physics, Massachusetts Institute of Technology, Cambridge, MA), Ziggy Pleunis (Anton Pannekoek Institute for Astronomy, University of Amsterdam, Amsterdam, ASTRON, Netherlands Institute for Radio Astronomy, Dwingeloo), Antonio Herrera-Martin (David A. Dunlap Department of Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON, Department of Statistical Science, University of Toronto, Toronto, ON), Ronniy C. Joseph (Trottier Space Institute, McGill University, Montr\'eal, QC, Department of Physics, McGill University, Montr\'eal, QC), Ayush Pandhi (David A. Dunlap Department of Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON, Dunlap Institute for Astronomy &amp; Astrophysics, University of Toronto, Toronto, ON), Aaron B. Pearlman (Department of Physics, McGill University, Montr\'eal, QC, Department of Physics, McGill University, Montr\'eal, QC), J. Xavier Prochaska (Department of Astronomy and Astrophysics, University of California, Santa Cruz, Santa Cruz, CA)</dc:creator>
    </item>
    <item>
      <title>A geometrical perspective on parametric psychometric models</title>
      <link>https://arxiv.org/abs/2410.12450</link>
      <description>arXiv:2410.12450v1 Announce Type: new 
Abstract: Psychometrics and quantitative psychology rely strongly on statistical models to measure psychological processes. As a branch of mathematics, geometry is inherently connected to measurement and focuses on properties such as distance and volume. However, despite the common root of measurement, geometry is currently not used a lot in psychological measurement. In this paper, my aim is to illustrate how ideas from non-Euclidean geometry may be relevant for psychometrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12450v1</guid>
      <category>stat.AP</category>
      <category>math.DG</category>
      <category>stat.OT</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francis Tuerlinckx</dc:creator>
    </item>
    <item>
      <title>Zombie Epidemic -- on Modeling the Effect of Interventions</title>
      <link>https://arxiv.org/abs/2410.12502</link>
      <description>arXiv:2410.12502v1 Announce Type: new 
Abstract: The recent COVID-19 pandemic has highlighted the need of studying extreme, life-threatening phenomena in advance. In this article, a zombie epidemic in Uusimaa region in Finland is modeled. A stochastic agent based simulation model is proposed and extensive simulations are conducted for this purpose. The model utilizes knowledge on defensive human behavior during crises. Studying the effects of a hypothetical zombie attack resembles examining the spread of deadly diseases and of rumors. A zombie attack is simulated in the most densely populated region in Finland. The region's exact population densities over its rasterized geographical map are utilized. Furthermore, the simulations are used to study the effect of implementing a (strict or partial) quarantine area in the epicenter. Computationally efficient Scala codes and video animations of the simulated epidemics are provided. The main findings emphasize the importance of implementing very strict measures, without delay, to stop the outbreak.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12502v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kaisa Ek, Aleksi Avela, Willehard Haaki, Sami Helander, Ville Lumme, Terho Mutikainen, Jaakko Pere, Natalia Vesselinova, Lauri Viitasaari, Pauliina Ilmonen</dc:creator>
    </item>
    <item>
      <title>Generating Synthetic Functional Data for Privacy-Preserving GPS Trajectories</title>
      <link>https://arxiv.org/abs/2410.12514</link>
      <description>arXiv:2410.12514v1 Announce Type: new 
Abstract: This research presents FDASynthesis, a novel algorithm designed to generate synthetic GPS trajectory data while preserving privacy. After pre-processing the input GPS data, human mobility traces are modeled as multidimensional curves using Functional Data Analysis (FDA). Then, the synthesis process identifies the K-nearest trajectories and averages their Square-Root Velocity Functions (SRVFs) to generate synthetic data. This results in synthetic trajectories that maintain the utility of the original data while ensuring privacy. Although applied for human mobility research, FDASynthesis is highly adaptable to different types of functional data, offering a scalable solution in various application domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12514v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arianna Burzacchi, Lise Bellanger, Klervi Le Gall, Aymeric Stamm, Simone Vantini</dc:creator>
    </item>
    <item>
      <title>Spatio-Temporal Analysis of Public Transportation Undercrowding: Leveraging APC Data for a Comprehensive Evaluation of Usage Rates</title>
      <link>https://arxiv.org/abs/2410.12618</link>
      <description>arXiv:2410.12618v1 Announce Type: new 
Abstract: The analysis of the transportation usage rate provides opportunities for evaluating the efficacy of the transportation service offered by proposing an indicator that integrates actual demand and capacity. This study aims to develop a methodology for analyzing the occupancy rate from large-scale datasets to identify gaps between supply and demand in public transportation. Leveraging the spatio-temporal granularity of data from Automatic People Counting (APC) and relying on the Generalized Linear Mixed Effects Model and the Generalized Mixed-Effect Random Forest, in this study we propose a methodology for analyzing factors determining undercrowding. The results of the model are examined at both the segment and ride levels. Initially, the analysis focuses on identifying segments more likely associated with undercrowding, understanding factors influencing the probability of undercrowding, and exploring their relationships. Subsequently, the analysis extends to the temporal distribution of undercrowding, encompassing its impact on the entire journey. The proposed methodology is applied to analyze APC data, provided by the company responsible for public transport management in Milan, on a radial route of the surface transportation network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12618v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arianna Burzacchi, Valeria Maria Urbano, Marika Arena, Giovanni Azzone, Piercesare Secchi, Simone Vantini</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis of Ruin of an Insurance Company in Ghana</title>
      <link>https://arxiv.org/abs/2410.11846</link>
      <description>arXiv:2410.11846v1 Announce Type: cross 
Abstract: An insurance company, as a risk bearer, is exposed to the likelihood of running into ruin. This is the situation where the initial surplus falls below zero. There is the need to find the required start-up capital to hedge against insolvency. Most researchers, irrespective of whether the test for claim dependency holds or not, assume claim independence in their computing of ruin probabilities to start up their initial capital. The objective of this study is to carry out comparative sensitivity analysis of ruin probability under both assumptions of dependence and independence, irrespective of whether the data exhibits independence or not, based on data from an insurance company in Ghana. Secondary data from an insurance company was obtained from the National Insurance Commission (NIC) for the period of 2013 to 2017. The study employed copulas to determine the claim dependence among the various insurance products and the company in general. The study concluded that when there is dependence in the claim data, computing the ruin probability based on the assumption of independence results in underestimation. Among the various insurance products, the most profitable insurance product was motor insurance, and Fire and Allied insurance exhibited the highest dependency. At a higher start-up capital, when claims are dependent, assuming independence in calculating the ruin probability results in a significant difference. Hence, it was recommended that insurance companies should adopt the assumption of dependence between the claims data as the initial reserves become larger, particularly for larger insurance companies, to avoid misleading results. Also, awareness of the perils and consequences of fire outbreaks and disasters should be raised to the general public to reduce the risk of frequent occurrence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11846v1</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Tawiah Pabifio</dc:creator>
    </item>
    <item>
      <title>Copula based joint regression models for correlated data: an analysis in the bivariate case</title>
      <link>https://arxiv.org/abs/2410.11892</link>
      <description>arXiv:2410.11892v1 Announce Type: cross 
Abstract: Regression analysis of non-normal correlated data is commonly performed using generalized linear mixed models (GLMM) and generalized estimating equations (GEE). The recent development of generalized joint regression models (GJRM) presents an alternative to these approaches by using copulas to flexibly model response variables and their dependence structures. This paper provides a simulation study that compares the GJRM with alternative methods. We focus on the case of the marginal distributions having the same form, for example, in models for longitudinal data.
  We find that for the normal model with identity link, all models provide accurate estimates of the parameters of interest. However, for non-normal models and when a non-identity link function is used, GLMMs in general provide biased estimates of marginal model parameters with inaccurately low standard errors. GLMM bias is more pronounced when the marginal distributions are more skewed or highly correlated. However, in the case that a GLMM parameter is estimated independently of the random effect term, we show it is possible to extract accurate parameter estimates, shown for a longitudinal time parameter with a logarithmic link model. In contrast, we find that GJRM and GEE provide unbiased estimates for all parameters with accurate standard errors when using a logarithmic link. In addition, we show that GJRM provides a model fit comparable to GLMM. In a real-world study of doctor visits, we further demonstrate that the GJRM provides better model fits than a comparable GEE or GLM, due to its greater flexibility in choice of marginal distribution and copula fit to dependence structures. We conclude that the GJRM provides a superior approach to current popular models for analysis of non-normal correlated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11892v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aydin Sareff-Hibbert, Gillian Z. Heller</dc:creator>
    </item>
    <item>
      <title>Stochastic 3D reconstruction of cracked polycrystalline NMC particles using 2D SEM data</title>
      <link>https://arxiv.org/abs/2410.12020</link>
      <description>arXiv:2410.12020v1 Announce Type: cross 
Abstract: Li-ion battery performance is strongly influenced by their cathodes' properties and consequently by the 3D microstructure of the particles the cathodes are comprised of. During calendaring and cycling, cracks develop within cathode particles, which may affect performance in multiple ways. On the one hand, cracks reduce internal connectivity such that electron transport within cathode particles is hindered. On the other hand, intra-particle cracks can increase the cathode reactive surface. Due to these contradictory effects, it is necessary to quantitatively investigate how battery cycling effects cracking and how cracking in-turn influences battery performance. Thus, it is necessary to characterize the 3D particle morphology with structural descriptors and quantitatively correlate them with effective battery properties. Typically, 3D structural characterization is performed using image data. However, informative 3D imaging techniques are time-consuming, costly and rarely available, such that analyses often have to rely on 2D image data. This paper presents a novel stereological approach for generating virtual 3D cathode particles that exhibit crack networks that are statistically equivalent to those observed in 2D sections of experimentally measured particles. Consequently, more easily available 2D image data suffices for deriving a full 3D characterization of cracked cathodes particles. In future research, the virtually generated 3D particles will be used as geometry input for spatially resolved electro-chemo-mechanical simulations, to enhance our understanding of structure-property relationships of cathodes in Li-ion batteries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12020v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Rieder, Orkun Furat, Francois L. E. Usseglio-Viretta, Jeffery Allen, Peter J. Weddle, Donal P. Finegan, Kandler Smith, Volker Schmidt</dc:creator>
    </item>
    <item>
      <title>Deep Optimal Sensor Placement for Black Box Stochastic Simulations</title>
      <link>https://arxiv.org/abs/2410.12036</link>
      <description>arXiv:2410.12036v1 Announce Type: cross 
Abstract: Selecting cost-effective optimal sensor configurations for subsequent inference of parameters in black-box stochastic systems faces significant computational barriers. We propose a novel and robust approach, modelling the joint distribution over input parameters and solution with a joint energy-based model, trained on simulation data. Unlike existing simulation-based inference approaches, which must be tied to a specific set of point evaluations, we learn a functional representation of parameters and solution. This is used as a resolution-independent plug-and-play surrogate for the joint distribution, which can be conditioned over any set of points, permitting an efficient approach to sensor placement. We demonstrate the validity of our framework on a variety of stochastic problems, showing that our method provides highly informative sensor locations at a lower computational cost compared to conventional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12036v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paula Cordero-Encinar, Tobias Schr\"oder, Peter Yatsyshin, Andrew Duncan</dc:creator>
    </item>
    <item>
      <title>Sparse Causal Effect Estimation using Two-Sample Summary Statistics in the Presence of Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2410.12300</link>
      <description>arXiv:2410.12300v1 Announce Type: cross 
Abstract: Observational genome-wide association studies are now widely used for causal inference in genetic epidemiology. To maintain privacy, such data is often only publicly available as summary statistics, and often studies for the endogenous covariates and the outcome are available separately. This has necessitated methods tailored to two-sample summary statistics. Current state-of-the-art methods modify linear instrumental variable (IV) regression -- with genetic variants as instruments -- to account for unmeasured confounding. However, since the endogenous covariates can be high dimensional, standard IV assumptions are generally insufficient to identify all causal effects simultaneously. We ensure identifiability by assuming the causal effects are sparse and propose a sparse causal effect two-sample IV estimator, spaceTSIV, adapting the spaceIV estimator by Pfister and Peters (2022) for two-sample summary statistics. We provide two methods, based on L0- and L1-penalization, respectively. We prove identifiability of the sparse causal effects in the two-sample setting and consistency of spaceTSIV. The performance of spaceTSIV is compared with existing two-sample IV methods in simulations. Finally, we showcase our methods using real proteomic and gene-expression data for drug-target discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12300v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shimeng Huang, Niklas Pfister, Jack Bowden</dc:creator>
    </item>
    <item>
      <title>Characterizing Behavioral Differences and Adaptations of Automated Vehicles and Human Drivers at Unsignalized Intersections: Insights from Waymo and Lyft Open Datasets</title>
      <link>https://arxiv.org/abs/2410.12538</link>
      <description>arXiv:2410.12538v1 Announce Type: cross 
Abstract: The integration of autonomous vehicles (AVs) into transportation systems presents an unprecedented opportunity to enhance road safety and efficiency. However, understanding the interactions between AVs and human-driven vehicles (HVs) at intersections remains an open research question. This study aims to bridge this gap by examining behavioral differences and adaptations of AVs and HVs at unsignalized intersections by utilizing two comprehensive AV datasets from Waymo and Lyft. Using a systematic methodology, the research identifies and analyzes merging and crossing conflicts by calculating key safety and efficiency metrics, including time to collision (TTC), post-encroachment time (PET), maximum required deceleration (MRD), time advantage (TA), and speed and acceleration profiles. The findings reveal a paradox in mixed traffic flow: while AVs maintain larger safety margins, their conservative behavior can lead to unexpected situations for human drivers, potentially causing unsafe conditions. From a performance point of view, human drivers exhibit more consistent behavior when interacting with AVs versus other HVs, suggesting AVs may contribute to harmonizing traffic flow patterns. Moreover, notable differences were observed between Waymo and Lyft vehicles, which highlights the importance of considering manufacturer-specific AV behaviors in traffic modeling and management strategies for the safe integration of AVs. The processed dataset utilized in this study is openly published to foster the research on AV-HV interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12538v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saeed Rahmani (Gavin),  Zhenlin (Gavin),  Xu, Simeon C. Calvert, Bart van Arem</dc:creator>
    </item>
    <item>
      <title>Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators</title>
      <link>https://arxiv.org/abs/2410.12690</link>
      <description>arXiv:2410.12690v1 Announce Type: cross 
Abstract: A critical bottleneck for scientific progress is the costly nature of computer simulations for complex systems. Surrogate models provide an appealing solution: such models are trained on simulator evaluations, then used to emulate and quantify uncertainty on the expensive simulator at unexplored inputs. In many applications, one often has available data on related systems. For example, in designing a new jet turbine, there may be existing studies on turbines with similar configurations. A key question is how information from such "source" systems can be transferred for effective surrogate training on the "target" system of interest. We thus propose a new LOcal transfer Learning Gaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian process to transfer such information for surrogate modeling. The key novelty of the LOL-GP is a latent regularization model, which identifies regions where transfer should be performed and regions where it should be avoided. This "local transfer" property is desirable in scientific systems: at certain parameters, such systems may behave similarly and thus transfer is beneficial; at other parameters, they may behave differently and thus transfer is detrimental. By accounting for local transfer, the LOL-GP can rectify a critical limitation of "negative transfer" in existing transfer learning models, where the transfer of information worsens predictive performance. We derive a Gibbs sampling algorithm for efficient posterior predictive sampling on the LOL-GP, for both the multi-source and multi-fidelity transfer settings. We then show, via a suite of numerical experiments and an application for jet turbine design, the improved surrogate performance of the LOL-GP over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12690v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinming Wang, Simon Mak, John Miller, Jianguo Wu</dc:creator>
    </item>
    <item>
      <title>Phase vs coin vs position disorder as a probe for the resilience and revival of single particle entanglement in cyclic quantum walks</title>
      <link>https://arxiv.org/abs/2410.12710</link>
      <description>arXiv:2410.12710v1 Announce Type: cross 
Abstract: Quantum states exhibiting single-particle entanglement (SPE) can encode and process quantum information more robustly than their multi-particle analogs. Understanding the vulnerability and resilience of SPE to disorder is therefore crucial. This letter investigates phase, coin, and position disorder via discrete-time quantum walks on odd and even cyclic graphs to study their effect on SPE. The reduction in SPE is insignificant for low levels of phase or coin disorder, showing the resilience of SPE to minor perturbations. However, SPE is seen to be more vulnerable to position disorder. We analytically prove that maximally entangled single-particle states (MESPS) at time step $t=1$ are impervious to phase disorder regardless of the choice of the initial state. Further, MESPS at timestep $t=1$ is also wholly immune to coin disorder for phase-symmetric initial states. Position disorder breaks odd-even parity and distorts the physical time cone of the quantum walker, unlike phase or coin disorder. SPE saturates towards a fixed value for position disorder, irrespective of the disorder strength at large timestep $t$. Furthermore, SPE can be enhanced with moderate to significant phase or coin disorder strengths at specific time steps. Interestingly, disorder can revive single-particle entanglement from absolute zero in some instances, too. These results are crucial in understanding single-particle entanglement evolution and dynamics in a lab setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12710v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.comp-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dinesh Kumar Panda, Colin Benjamin</dc:creator>
    </item>
    <item>
      <title>Score-Driven Exponential Random Graphs: A New Class of Time-Varying Parameter Models for Dynamical Networks</title>
      <link>https://arxiv.org/abs/1905.10806</link>
      <description>arXiv:1905.10806v3 Announce Type: replace 
Abstract: Motivated by the increasing abundance of data describing real-world networks that exhibit dynamical features, we propose an extension of the Exponential Random Graph Models (ERGMs) that accommodates the time variation of its parameters. Inspired by the fast-growing literature on Dynamic Conditional Score models, each parameter evolves according to an updating rule driven by the score of the ERGM distribution. We demonstrate the flexibility of score-driven ERGMs (SD-ERGMs) as data-generating processes and filters and show the advantages of the dynamic version over the static one. We discuss two applications to temporal networks from financial and political systems. First, we consider the prediction of future links in the Italian interbank credit network. Second, we show that the SD-ERGM allows discriminating between static or time-varying parameters when used to model the U.S. Congress co-voting network dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:1905.10806v3</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Domenico Di Gangi, Giacomo Bormetti, Fabrizio Lillo</dc:creator>
    </item>
    <item>
      <title>Is the age pension in Australia sustainable and fair? Evidence from forecasting the old-age dependency ratio using the Hamilton-Perry model</title>
      <link>https://arxiv.org/abs/2401.13943</link>
      <description>arXiv:2401.13943v2 Announce Type: replace 
Abstract: The age pension aims to assist eligible elderly Australians meet specific age and residency criteria in maintaining basic living standards. In designing efficient pension systems, government policymakers seek to satisfy the expectations of the overall aging population in Australia. However, the population's unique demographic characteristics at the state and territory level are often overlooked due to the lack of available data. We use the Hamilton-Perry model, which requires minimum input, to model and forecast the evolution of age-specific populations at the state level. We also integrate the obtained sub-national demographic information to determine sustainable pension ages up to 2051. We also investigate pension welfare distribution in all states and territories to identify disadvantaged residents under the current pension system. Using the sub-national mortality data for Australia from 1971 to 2021 obtained from AHMD (2023), we implement the Hamilton-Perry model with the help of functional time series forecasting techniques. With forecasts of age-specific population sizes for each state and territory, we compute the old age dependency ratio to determine the nationwide sustainable pension age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13943v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sizhe Chen, Han Lin Shang, Yang Yang</dc:creator>
    </item>
    <item>
      <title>Properties and maximum likelihood estimation of the gamma-normal and related probability distributions</title>
      <link>https://arxiv.org/abs/2402.11088</link>
      <description>arXiv:2402.11088v2 Announce Type: replace 
Abstract: This paper presents likelihood-based inference methods for the family of univariate gamma-normal distributions GN({\alpha}, r, {\mu}, {\sigma}^2 ) that result from summing independent gamma({\alpha}, r) and N({\mu}, {\sigma}^2 ) random variables. First, the probability density function of a gamma-normal variable is provided in compact form with the use of parabolic cylinder functions, along with key properties. We then provide analytic expressions for the maximum-likelihood score equations and the Fisher information matrix, and discuss inferential methods for the gamma-normal distribution. Given the widespread use of the two constituting distributions, the gamma-normal distribution is a general purpose tool for a variety of applications. In particular, we discuss two distributions that are obtained as special cases and that are featured in a variety of statistical applications: the exponential-normal distribution and the chi-squared-normal (or overdispersed chi-squared) distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11088v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Massimiliano Bonamente, Dale Zimmerman</dc:creator>
    </item>
    <item>
      <title>Modeling Extreme Events: Univariate and Multivariate Data-Driven Approaches</title>
      <link>https://arxiv.org/abs/2401.14910</link>
      <description>arXiv:2401.14910v2 Announce Type: replace-cross 
Abstract: This article summarizes the contribution of team genEVA to the EVA (2023) Conference Data Challenge. The challenge comprises four individual tasks, with two focused on univariate extremes and two related to multivariate extremes. In the first univariate assignment, we estimate a conditional extremal quantile using a quantile regression approach with neural networks. For the second, we develop a fine-tuning procedure for improved extremal quantile estimation with a given conservative loss function. In the first multivariate sub-challenge, we approximate the data-generating process with a copula model. In the remaining task, we use clustering to separate a high-dimensional problem into approximately independent components. Overall, competitive results were achieved for all challenges, and our approaches for the univariate tasks yielded the most accurate quantile estimates in the competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14910v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gloria Buritic\'a, Manuel Hentschel, Olivier C. Pasche, Frank R\"ottger, Zhongwei Zhang</dc:creator>
    </item>
    <item>
      <title>Overfitting Reduction in Convex Regression</title>
      <link>https://arxiv.org/abs/2404.09528</link>
      <description>arXiv:2404.09528v2 Announce Type: replace-cross 
Abstract: Convex regression is a method for estimating the convex function from a data set. This method has played an important role in operations research, economics, machine learning, and many other areas. However, it has been empirically observed that convex regression produces inconsistent estimates of convex functions and extremely large subgradients near the boundary as the sample size increases. In this paper, we provide theoretical evidence of this overfitting behavior. To eliminate this behavior, we propose two new estimators by placing a bound on the subgradients of the convex function. We further show that our proposed estimators can reduce overfitting by proving that they converge to the underlying true convex function and that their subgradients converge to the gradient of the underlying function, both uniformly over the domain with probability one as the sample size is increasing to infinity. An application to Finnish electricity distribution firms confirms the superior performance of the proposed methods in predictive power over the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09528v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqiang Liao, Sheng Dai, Eunji Lim, Timo Kuosmanen</dc:creator>
    </item>
    <item>
      <title>This is not normal! (Re-) Evaluating the lower $n$ guidelines for regression analysis</title>
      <link>https://arxiv.org/abs/2409.06413</link>
      <description>arXiv:2409.06413v2 Announce Type: replace-cross 
Abstract: The commonly cited rule of thumb for regression analysis, which suggests that a sample size of $n \geq 30$ is sufficient to ensure valid inferences, is frequently referenced but rarely scrutinized. This research note evaluates the lower bound for the number of observations required for regression analysis by exploring how different distributional characteristics, such as skewness and kurtosis, influence the convergence of t-values to the t-distribution in linear regression models. Through an extensive simulation study involving over 22 billion regression models, this paper examines a range of symmetric, platykurtic, and skewed distributions, testing sample sizes from 4 to 10,000. The results show that it is sufficient that either the dependent or independent variable follow a symmetric distribution for the t-values to converge at much smaller sample sizes than $n=30$, unless the other variable is extremely skewed. This is contrary to previous guidance which suggests that the error term needs to be normally distributed for this convergence to happen at low $n$. However, when both variables are highly skewed, much larger sample sizes are required. These findings suggest the $n \geq 30$ rule is overly conservative in some cases and insufficient in others, offering revised guidelines for determining minimum sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06413v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Randahl</dc:creator>
    </item>
  </channel>
</rss>
