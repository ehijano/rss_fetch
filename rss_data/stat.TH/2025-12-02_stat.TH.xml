<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Dec 2025 02:39:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Statistical Inference under Adaptive Sampling with LinUCB</title>
      <link>https://arxiv.org/abs/2512.00222</link>
      <description>arXiv:2512.00222v1 Announce Type: new 
Abstract: Adaptively collected data has become ubiquitous within modern practice. However, even seemingly benign adaptive sampling schemes can introduce severe biases, rendering traditional statistical inference tools inapplicable. This can be mitigated by a property called stability, which states that if the rate at which an algorithm takes actions converges to a deterministic limit, one can expect that certain parameters are asymptotically normal. Building on a recent line of work for the multi-armed bandit setting, we show that the linear upper confidence bound (LinUCB) algorithm for linear bandits satisfies this property. In doing so, we painstakingly characterize the behavior of the eigenvalues and eigenvectors of the random design feature covariance matrix in the setting where the action set is the unit ball, showing that it decomposes into a rank-one direction that locks onto the true parameter and an almost-isotropic bulk that grows at a predictable $\sqrt{T}$ rate. This allows us to establish a central limit theorem for the LinUCB algorithm, establishing asymptotic normality for the limiting distribution of the estimation error where the convergence occurs at a $T^{-1/4}$ rate. The resulting Wald-type confidence sets and hypothesis tests do not depend on the feature covariance matrix and are asymptotically tighter than existing nonasymptotic confidence sets. Numerical simulations corroborate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00222v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Fan, Kevin Tan, Yuting Wei</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of function-on-function Polynomial regression model</title>
      <link>https://arxiv.org/abs/2512.00549</link>
      <description>arXiv:2512.00549v1 Announce Type: new 
Abstract: In this article, we study the convergence behavior of the regularization-based algorithm for solving the polynomial regression model when both input data and responses are from infinite-dimensional Hilbert spaces. We derive convergence rates for estimation and prediction error by employing general (spectral) regularization under a general smoothness condition without imposing any additional conditions on the index function. We also establish lower bounds for any learning algorithm to explain the optimality of our convergence rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00549v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naveen Gupta, Sivananthan Sampath</dc:creator>
    </item>
    <item>
      <title>Infinitely divisible privacy and beyond I: resolution of the $s^2=2k$ conjecture</title>
      <link>https://arxiv.org/abs/2512.00734</link>
      <description>arXiv:2512.00734v1 Announce Type: new 
Abstract: Differential privacy is increasingly formalized through the lens of hypothesis testing via the robust and interpretable $f$-DP framework, where privacy guarantees are encoded by a baseline Blackwell trade-off function $f_{\infty} = T(P_{\infty}, Q_{\infty})$ involving a pair of distributions $(P_{\infty}, Q_{\infty})$. The problem of choosing the right privacy metric in practice leads to a central question: what is a statistically appropriate baseline $f_{\infty}$ given some prior modeling assumptions? The special case of Gaussian differential privacy (GDP) showed that, under compositions of nearly perfect mechanisms, these trade-off functions exhibit a central limit behavior with a Gaussian limit experiment. Inspired by Le Cam's theory of limits of statistical experiments, we answer this question in full generality in an infinitely divisible setting.
  We show that suitable composition experiments $(P_n^{\otimes n}, Q_n^{\otimes n})$ converge to a binary limit experiment $(P_{\infty}, Q_{\infty})$ whose log-likelihood ratio $L = \log(dQ_{\infty} / dP_{\infty})$ is infinitely divisible under $P_{\infty}$. Thus any limiting trade-off function $f_{\infty}$ is determined by an infinitely divisible law $P_{\infty}$, characterized by its Levy--Khintchine triplet, and its Esscher tilt defined by $dQ_{\infty}(x) = e^{x} dP_{\infty}(x)$. This characterizes all limiting baseline trade-off functions $f_{\infty}$ arising from compositions of nearly perfect differentially private mechanisms. Our framework recovers GDP as the purely Gaussian case and yields explicit non-Gaussian limits, including Poisson examples. It also positively resolves the empirical $s^2 = 2k$ phenomenon observed in the GDP paper and provides an optimal mechanism for count statistics achieving asymmetric Poisson differential privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00734v1</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaradhya Pandey, Arian Maleki, Sanjeev Kulkarni</dc:creator>
    </item>
    <item>
      <title>Asymptotic inference in a stationary quantum time series</title>
      <link>https://arxiv.org/abs/2512.01026</link>
      <description>arXiv:2512.01026v1 Announce Type: new 
Abstract: We consider a statistical model of a n-mode quantum Gaussian state which is shift invariant and also gauge invariant. Such models can be considered analogs of classical Gaussian stationary time series, parametrized by their spectral density. Defining an appropriate quantum spectral density as the parameter, we establish that the quantum Gaussian time series model is asymptotically equivalent to a classical nonlinear regression model given as a collection of independent geometric random variables. The asymptotic equivalence is established in the sense of the quantum Le Cam distance between statistical models (experiments). The geometric regression model has a further classical approximation as a certain Gaussian white noise model with a transformed quantum spectral density as signal. In this sense, the result is a quantum analog of the asymptotic equivalence of classical spectral density estimation and Gaussian white noise, which is known for Gaussian stationary time series. In a forthcoming version of this preprint, we will also identify a quantum analog of the periodogram and provide optimal parametric and nonparametric estimates of the quantum spectral density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01026v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Nussbaum, Arleta Szko{\l}a</dc:creator>
    </item>
    <item>
      <title>Volatility change point detection for linear parabolic SPDEs</title>
      <link>https://arxiv.org/abs/2512.01277</link>
      <description>arXiv:2512.01277v1 Announce Type: new 
Abstract: We consider change point detection for the volatility in second order linear parabolic stochastic partial differential equations based on high frequency spatio-temporal data. We give a test statistic to detect changes in the volatility based on change point analysis for diffusion processes and derive the asymptotic null distribution of the test statistic. We also show that the test is consistent. Moreover, we provide some examples and then perform numerical simulations of the proposed test statistic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01277v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yozo Tonaki, Yusuke Kaino, Masayuki Uchida</dc:creator>
    </item>
    <item>
      <title>Hawkes process with a diffusion-driven baseline: long-run behavior, inference, statistical tests</title>
      <link>https://arxiv.org/abs/2512.01447</link>
      <description>arXiv:2512.01447v1 Announce Type: new 
Abstract: Event-driven systems in fields such as neuroscience, social networks, and finance often exhibit dynamics influenced by continuously evolving external covariates. Motivated by these applications, we introduce a new class of multivariate Hawkes processes, in which the spontaneous rate of events is modulated by a diffusion process. This framework allows the point process to adapt dynamically to continuously evolving covariates, capturing both intrinsic self-excitation and external influences. In this article, we establish the probabilistic properties of the coupled process, proving stability and ergodicity under moderate assumptions. Classical functional results, including law of large numbers and mixing properties, are extended to this diffusion-driven setting. Building on these results, we study parametric inference for the Hawkes component: we derive consistency and asymptotic normality of the maximum likelihood estimator in the long-time regime, and derive stronger convergence results under additional assumptions on the covariate process. We further propose hypothesis testing procedures to assess the statistical relevance of the covariate. Simulation studies illustrate the validity of the asymptotic results and the effectiveness of the proposed inference methods. Overall, this work provides theoretical and practical foundations for diffusion-driven Hawkes models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01447v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maya Sadeler Perrin, Anna Bonnet, Charlotte Dion-Blanc, Adeline Samson</dc:creator>
    </item>
    <item>
      <title>A novel sequential method for building upper and lower bounds of moments of distributions</title>
      <link>https://arxiv.org/abs/2512.01761</link>
      <description>arXiv:2512.01761v1 Announce Type: new 
Abstract: Approximating integrals is a fundamental task in probability theory and statistical inference, and their applied fields of signal processing, and Bayesian learning, as soon as expectations over probability distributions must be computed efficiently and accurately. When these integrals lack closed-form expressions, numerical methods must be used, from the Newton-Cotes formulas and Gaussian quadrature, to Monte Carlo and variational approximation techniques. Despite these numerous tools, few are guaranteed to preserve majoration/minoration inequalities, while this feature is fundamental in certain applications in statistics. In this paper, we focus on the integration problem arising in the estimation of moments of scalar, unnormalized, distributions. We introduce a sequential method for constructing upper and lower bounds on the sought integral. Our approach leverages the majorization-minimization framework to iteratively refine these bounds, in an enveloped principle. The method has proven convergence, and controlled accuracy, under mild conditions. We demonstrate its effectiveness through a detailed numerical example of the estimation of a Monte-Carlo sampler variance in a Bayesian inference problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01761v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Solal Martin, Emilie Chouzenoux, Victor Elvira</dc:creator>
    </item>
    <item>
      <title>Sharp Self-Normalized Concentration Inequalities of Marginal Mean with Sample Variance Only</title>
      <link>https://arxiv.org/abs/2512.01817</link>
      <description>arXiv:2512.01817v2 Announce Type: new 
Abstract: (This is the first version of a working paper. A more detailed follow-up with applications is in preparation.) We develop a family of self-normalized concentration inequalities for marginal mean under martingale-difference structure and $\phi/\tilde{\phi}$-mixing conditions, where the latter includes many processes that are not strongly mixing. The variance term is fully data-observable: naive sample variance in the martingale case and an empirical block long-run variance under mixing conditions. Thus, no predictable variance proxy is required. No specific assumption on the decay of the mixing coefficients (e.g. summability) is needed for the validity. The constants are explicit and the bounds are ready to use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01817v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Yuan</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit testing from observations with multiplicative measurement error</title>
      <link>https://arxiv.org/abs/2512.01838</link>
      <description>arXiv:2512.01838v1 Announce Type: new 
Abstract: Given observations from a positive random variable contaminated by multiplicative measurement error, we consider a nonparametric goodness-of-fit testing task for its unknown density in a non-asymptotic framework. We propose a testing procedure based on estimating a quadratic functional of the Mellin transform of the unknown density and the null. We derive non-asymptotic testing radii and testing rates over Mellin-Sobolev spaces, which naturally characterize regularity and ill-posedness in this model. By employing a multiple testing procedure with Bonferroni correction, we obtain data-driven procedures and analyze their performance. Compared with the non-adaptive tests, their testing radii deteriorate by at most a logarithmic factor. We illustrate the testing procedures with a simulation study using various choices of densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01838v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Johannes, Bianca Neubert</dc:creator>
    </item>
    <item>
      <title>Iterated sampling importance resampling with adaptive number of proposals</title>
      <link>https://arxiv.org/abs/2512.00220</link>
      <description>arXiv:2512.00220v1 Announce Type: cross 
Abstract: Iterated sampling importance resampling (i-SIR) is a Markov chain Monte Carlo (MCMC) algorithm which is based on $N$ independent proposals. As $N$ grows, its samples become nearly independent, but with an increased computational cost. We discuss a method which finds an approximately optimal number of proposals $N$ in terms of the asymptotic efficiency. The optimal $N$ depends on both the mixing properties of the i-SIR chain and the (parallel) computing costs. Our method for finding an appropriate $N$ is based on an approximate asymptotic variance of the i-SIR, which has similar properties as the i-SIR asymptotic variance, and a generalised i-SIR transition having fractional `number of proposals.' These lead to an adaptive i-SIR algorithm, which tunes the number of proposals automatically during sampling. Our experiments demonstrate that our approximate efficiency and the adaptive i-SIR algorithm have promising empirical behaviour. We also present new theoretical results regarding the i-SIR, such as the convexity of asymptotic variance in the number of proposals, which can be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00220v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pietari Laitinen, Matti Vihola</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Valid Rank Confidence Sets for a Broad Class of Statistical and Machine Learning Models</title>
      <link>https://arxiv.org/abs/2512.00316</link>
      <description>arXiv:2512.00316v1 Announce Type: cross 
Abstract: Ranking populations such as institutions based on certain characteristics is often of interest, and these ranks are typically estimated using samples drawn from the populations. Due to sample randomness, it is important to quantify the uncertainty associated with the estimated ranks. This becomes crucial when latent characteristics are poorly separated and where many rank estimates may be incorrectly ordered. Understanding uncertainty can help quantify and mitigate these issues and provide a fuller picture. However, this task is especially challenging because the rank parameters are discrete and the central limit theorem does not apply to the rank estimates. In this article, we propose a Repro Samples Method to address this nontrivial inference problem by developing a confidence set for the true, unobserved population ranks. This method provides finite-sample coverage guarantees and is broadly applicable to ranking problems. The effectiveness of the method is illustrated and compared with several published large sample ranking approaches using simulation studies and real data examples involving samples both from traditional statistical models and modern data science algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00316v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onrina Chandra, Min-ge Xie</dc:creator>
    </item>
    <item>
      <title>On Statistical Inference for High-Dimensional Binary Time Series</title>
      <link>https://arxiv.org/abs/2512.00338</link>
      <description>arXiv:2512.00338v2 Announce Type: cross 
Abstract: The analysis of non-real-valued data, such as binary time series, has attracted great interest in recent years. This manuscript proposes a post-selection estimator for estimating the coefficient matrices of a high-dimensional generalized binary vector autoregressive process and establishes a Gaussian approximation theorem for the proposed estimator. Furthermore, it introduces a second-order wild bootstrap algorithm to enable statistical inference on the coefficient matrices. Numerical studies and empirical applications demonstrate the good finite-sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00338v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dehao Dai, Yunyi Zhang</dc:creator>
    </item>
    <item>
      <title>Convergence of Reflected Langevin Diffusion for Constrained Sampling</title>
      <link>https://arxiv.org/abs/2512.00386</link>
      <description>arXiv:2512.00386v1 Announce Type: cross 
Abstract: We examine the Langevin diffusion confined to a closed, convex domain $D\subset\mathbb{R}^d$, represented as a reflected stochastic differential equation. We introduce a sequence of penalized stochastic differential equations and prove that their invariant measures converge, in Wasserstein-2 distance and with explicit polynomial rate, to the invariant measure of the reflected Langevin diffusion. We also analyze a time-discretization of the penalized process obtained via the Euler-Maruyama scheme and demonstrate the convergence to the original constrained measure. These results provide a rigorous approximation framework for reflected Langevin dynamics in both continuous and discrete time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00386v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarika Mane</dc:creator>
    </item>
    <item>
      <title>Statistical-computational gap in multiple Gaussian graph alignment</title>
      <link>https://arxiv.org/abs/2512.00610</link>
      <description>arXiv:2512.00610v1 Announce Type: cross 
Abstract: We investigate the existence of a statistical-computational gap in multiple Gaussian graph alignment. We first generalize a previously established informational threshold from Vassaux and Massouli\'e (2025) to regimes where the number of observed graphs $p$ may also grow with the number of nodes $n$: when $p \leq O(n/\log(n))$, we recover the results from Vassaux and Massouli\'e (2025), and $p \geq \Omega(n/\log(n))$ corresponds to a regime where the problem is as difficult as aligning one single graph with some unknown "signal" graph. Moreover, when $\log p = \omega(\log n)$, the informational thresholds for partial and exact recovery no longer coincide, in contrast to the all-or-nothing phenomenon observed when $\log p=O(\log n)$. Then, we provide the first computational barrier in the low-degree framework for (multiple) Gaussian graph alignment. We prove that when the correlation $\rho$ is less than $1$, up to logarithmic terms, low degree non-trivial estimation fails. Our results suggest that the task of aligning $p$ graphs in polynomial time is as hard as the problem of aligning two graphs in polynomial time, up to logarithmic factors. These results characterize the existence of a statistical-computational gap and provide another example in which polynomial-time algorithms cannot handle complex combinatorial bi-dimensional structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00610v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bertrand Even, Luca Ganassali</dc:creator>
    </item>
    <item>
      <title>Grouped Competition Test with Unified False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2512.00901</link>
      <description>arXiv:2512.00901v1 Announce Type: cross 
Abstract: This paper discusses several p-value-free multiple hypothesis testing methods proposed in recent years and organizes them by introducing a unified framework termed competition test. Although existing competition tests are effective in controlling the False Discovery Rate (FDR), they struggle with handling data with strong heterogeneity or dependency structures. Based on this framework, the paper proposes a novel approach that applies a corrected competition procedure to group data with certain structure, and then integrates the results from each group. Using the favorable properties of competition test, the paper proposes a theorem demonstrating that this approach controls the global FDR. We further show that although the correction parameters may lead to a slight loss in power, such loss is typically minimal. Through simulation experiments and mass spectrometry data analysis, we illustrate the flexibility and efficacy of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00901v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingzhou Deng, Yan Fu</dc:creator>
    </item>
    <item>
      <title>Bayesian Distributionally Robust Merton Problem with Nonlinear Wasserstein Projections</title>
      <link>https://arxiv.org/abs/2512.01408</link>
      <description>arXiv:2512.01408v1 Announce Type: cross 
Abstract: We revisit Merton's continuous-time portfolio selection through a data-driven, distributionally robust lens. Our aim is to tap the benefits of frequent trading over short horizons while acknowledging that drift is hard to pin down, whereas volatility can be screened using realized or implied measures for appropriately selected assets. Rather than time-rectangular distributional robust control -- which replenishes adversarial power at every instant and induces over-pessimism -- we place a single ambiguity set on the drift prior within a Bayesian Merton model. This prior-level ambiguity preserves learning and tractability: a minimax swap reduces the robust control to optimizing a nonlinear functional of the prior, enabling Karatzas and Zhao \cite{KZ98}-type's closed-form evaluation for each candidate prior. We then characterize small-radius worst-case priors under Wasserstein uncertainty via an explicit asymptotically optimal pushforward of the nominal prior, and we calibrate the ambiguity radius through a nonlinear Wasserstein projection tailored to the Merton functional. Synthetic and real-data studies demonstrate reduced pessimism relative to DRC and improved performance over myopic DRO-Markowitz under frequent rebalancing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01408v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Blanchet, Jiayi Cheng, Hao Liu, Yang Liu</dc:creator>
    </item>
    <item>
      <title>The Klebanov theorem for the group $\mathbb{R}\times \mathbb{Z}(2)$</title>
      <link>https://arxiv.org/abs/2512.01689</link>
      <description>arXiv:2512.01689v1 Announce Type: cross 
Abstract: L. Klebanov proved the following theorem. Let $\xi_1, \dots, \xi_n$ be independent random variables. Consider linear forms $L_1=a_1\xi_1+\cdots+a_n\xi_n,$ $L_2=b_1\xi_1+\cdots+b_n\xi_n,$ $L_3=c_1\xi_1+\cdots+c_n\xi_n,$ $L_4=d_1\xi_1+\cdots+d_n\xi_n,$ where the coefficients $a_j, b_j, c_j, d_j$ are real numbers. If the random vectors $(L_1,L_2)$ and $(L_3,L_4)$ are identically distributed, then all $\xi_i$ for which $a_id_j-b_ic_j\neq 0$ for all $j=\overline{1,n}$ are Gaussian random variables. The present article is devoted to an analogue of the Klebanov theorem in the case when random variables take values in the group $\mathbb{R}\times \mathbb{Z}(2)$ and the coefficients of the linear forms are topological endomorphisms of this group.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01689v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Margaryta Myronyuk</dc:creator>
    </item>
    <item>
      <title>An hybrid stochastic Newton algorithm for logistic regression</title>
      <link>https://arxiv.org/abs/2512.01790</link>
      <description>arXiv:2512.01790v1 Announce Type: cross 
Abstract: In this paper, we investigate a second-order stochastic algorithm for solving large-scale binary classification problems. We propose to make use of a new hybrid stochastic Newton algorithm that includes two weighted components in the Hessian matrix estimation: the first one coming from the natural Hessian estimate and the second associated with the stochastic gradient information. Our motivation comes from the fact that both parts evaluated at the true parameter of logistic regression, are equal to the Hessian matrix. This new formulation has several advantages and it enables us to prove the almost sure convergence of our stochastic algorithm to the true parameter. Moreover, we significantly improve the almost sure rate of convergence to the Hessian matrix. Furthermore, we establish the central limit theorem for our hybrid stochastic Newton algorithm. Finally, we show a surprising result on the almost sure convergence of the cumulative excess risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01790v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernard Bercu, Luis Fredes, Em\'eric Gbaguidi</dc:creator>
    </item>
    <item>
      <title>Dimension-free error estimate for diffusion model and optimal scheduling</title>
      <link>https://arxiv.org/abs/2512.01820</link>
      <description>arXiv:2512.01820v1 Announce Type: cross 
Abstract: Diffusion generative models have emerged as powerful tools for producing synthetic data from an empirically observed distribution. A common approach involves simulating the time-reversal of an Ornstein-Uhlenbeck (OU) process initialized at the true data distribution. Since the score function associated with the OU process is typically unknown, it is approximated using a trained neural network. This approximation, along with finite time simulation, time discretization and statistical approximation, introduce several sources of error whose impact on the generated samples must be carefully understood. Previous analyses have quantified the error between the generated and the true data distributions in terms of Wasserstein distance or Kullback-Leibler (KL) divergence. However, both metrics present limitations: KL divergence requires absolute continuity between distributions, while Wasserstein distance, though more general, leads to error bounds that scale poorly with dimension, rendering them impractical in high-dimensional settings. In this work, we derive an explicit, dimension-free bound on the discrepancy between the generated and the true data distributions. The bound is expressed in terms of a smooth test functional with bounded first and second derivatives. The key novelty lies in the use of this weaker, functional metric to obtain dimension-independent guarantees, at the cost of higher regularity on the test functions. As an application, we formulate and solve a variational problem to minimize the time-discretization error, leading to the derivation of an optimal time-scheduling strategy for the reverse-time diffusion. Interestingly, this scheduler has appeared previously in the literature in a different context; our analysis provides a new justification for its optimality, now grounded in minimizing the discretization bias in generative sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01820v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentin de Bortoli, Romuald Elie, Anna Kazeykina, Zhenjie Ren, Jiacheng Zhang</dc:creator>
    </item>
    <item>
      <title>Basis Choices for Frequency Domain Statistical Independence Tests and Algorithms for Algebraic Relation Extraction</title>
      <link>https://arxiv.org/abs/2512.01963</link>
      <description>arXiv:2512.01963v1 Announce Type: cross 
Abstract: In this paper, we explore how different selections of basis functions impact the efficacy of frequency domain techniques in statistical independence tests, and study different algorithms for extracting low-dimensional algebraic relations from dependent data. We examine a range of complete orthonormal bases functions including the Legendre polynomials, Fourier series, Walsh functions, and standard and nonstandard Haar wavelet bases. We utilize fast transformation algorithms to efficiently transform physical domain data to frequency domain coefficients. The main focuses of this paper are the effectiveness of different basis selections in detecting data dependency using frequency domain data, e.g., whether varying basis choices significantly influence statistical power loss for small data with large noise; and on the stability of different optimization formulations for finding proper algebraic relations when data are dependent. We present numerical results to demonstrate the effectiveness of frequency domain-based statistical analysis methods and provide guidance for selecting the proper basis and algorithm to detect a particular type of relations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01963v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan Shi, Wenbo Wang, Wan Zhang, Han Bao, Sergio Chavez, Jingfang Huang, Yichao Wu, Kai Zhang</dc:creator>
    </item>
    <item>
      <title>Estimating the Mixing Coefficients of Geometrically Ergodic Markov Processes</title>
      <link>https://arxiv.org/abs/2402.07296</link>
      <description>arXiv:2402.07296v2 Announce Type: replace 
Abstract: We propose methods to estimate the individual $\beta$-mixing coefficients of a real-valued geometrically ergodic Markov process from a single sample-path $X_0,X_1, \dots,X_n$. Under standard smoothness conditions on the densities, namely, that the joint density of the pair $(X_0,X_m)$ for each $m$ lies in a Besov space $B^s_{1,\infty}(\mathbb R^2)$ for some known $s&gt;0$, we obtain a rate of convergence of order $\mathcal{O}(\log(n) n^{-[s]/(2[s]+2)})$ for the expected error of our estimator in this case\footnote{We use $[s]$ to denote the integer part of the decomposition $s=[s]+\{s\}$ of $s \in (0,\infty)$ into an integer term and a {\em strictly positive} remainder term $\{s\} \in (0,1]$.}. We complement this result with a high-probability bound on the estimation error, and further obtain analogues of these bounds in the case where the state-space is finite. Naturally no density assumptions are required in this setting; the expected error rate is shown to be of order $\mathcal O(\log(n) n^{-1/2})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07296v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steffen Gr\"unew\"alder, Azadeh Khaleghi</dc:creator>
    </item>
    <item>
      <title>Optimistic Estimation of Convergence in Markov Chains with the Average-Mixing Time</title>
      <link>https://arxiv.org/abs/2402.10506</link>
      <description>arXiv:2402.10506v3 Announce Type: replace 
Abstract: The convergence rate of a Markov chain to its stationary distribution is typically assessed using the concept of total variation mixing time. However, this worst-case measure often yields pessimistic estimates and is challenging to infer from observations. In this paper, we advocate for the use of the average-mixing time as a more optimistic and demonstrably easier-to-estimate alternative. We further illustrate its applicability across a range of settings, from two-point to countable spaces, and discuss some practical implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10506v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geoffrey Wolfer, Pierre Alquier</dc:creator>
    </item>
    <item>
      <title>From Poisson Observations to Fitted Negative Binomial Distribution</title>
      <link>https://arxiv.org/abs/2404.07457</link>
      <description>arXiv:2404.07457v3 Announce Type: replace 
Abstract: The negative binomial distribution has been widely used as a more flexible model than the Poisson distribution for count data. However, when the true data-generating process is Poisson, it is often challenging to distinguish it from a negative binomial distribution with extreme parameter values, and existing maximum likelihood estimation procedures for the negative binomial distribution may fail or produce unstable estimates. To address this issue, we develop a new algorithm for computing the maximum likelihood estimate of negative binomial parameters, which is more efficient and more accurate than existing methods. We further extend negative binomial distributions with a new parameterization to cover Poisson distributions as a special class. We provide theoretical justifications showing that, when applied to a Poisson data, the estimated parameters of the extended negative binomial distribution can consistently recover the true Poisson distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07457v3</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Yang, Niloufar Dousti Mousavi, Zhou Yu, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Autoregressive networks with dependent edges</title>
      <link>https://arxiv.org/abs/2404.15654</link>
      <description>arXiv:2404.15654v3 Announce Type: replace 
Abstract: We propose an autoregressive framework for modelling dynamic networks with dependent edges. It encompasses models that accommodate, for example, transitivity, degree heterogenenity, and other stylized features often observed in real network data. By assuming the edges of networks at each time are independent conditionally on their lagged values, the models, which exhibit a close connection with temporal ERGMs, facilitate both simulation and the maximum likelihood estimation in a straightforward manner. Due to the possibly large number of parameters in the models, the natural MLEs may suffer from slow convergence rates. An improved estimator for each component parameter is proposed based on an iteration employing projection, which mitigates the impact of the other parameters (Chang et al., 2021; Chang et al., 2023). Leveraging a martingale difference structure, the asymptotic distribution of the improved estimator is derived without the assumption of stationarity. The limiting distribution is not normal in general, although it reduces to normal when the underlying process satisfies some mixing conditions. Illustration with a transitivity model was carried out in both simulation and a real network data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15654v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Qin Fang, Eric D. Kolaczyk, Peter W. MacDonald, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>Exact recovery in the double sparse model: sufficient and necessary signal conditions</title>
      <link>https://arxiv.org/abs/2501.04551</link>
      <description>arXiv:2501.04551v2 Announce Type: replace 
Abstract: The double sparse linear model, which has both group-wise and element-wise sparsity in regression coefficients, has attracted lots of attention recently. This paper establishes the sufficient and necessary relationship between the exact support recovery and the optimal minimum signal conditions in the double sparse model. Specifically, sharply under the proposed signal conditions, a two-stage double sparse iterative hard thresholding procedure achieves exact support recovery with a suitably chosen threshold parameter. Also, this procedure maintains asymptotic normality aligning with an OLS estimator given true support, hence holding the oracle properties. Conversely, we prove that no method can achieve exact support recovery if these signal conditions are violated. This fills a critical gap in the minimax optimality theory on support recovery of the double sparse model. Finally, numerical experiments are provided to support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04551v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shixiang Liu, Zhifan Li, Yanhang Zhang, Jianxin Yin</dc:creator>
    </item>
    <item>
      <title>A Generalized Back-Door Criterion for Linear Regression</title>
      <link>https://arxiv.org/abs/2511.04060</link>
      <description>arXiv:2511.04060v5 Announce Type: replace 
Abstract: What assumptions about the data-generating process are required to permit a causal interpretation of partial regression coefficients? To answer this question, this paper generalizes Pearl's single-door and back-door criteria and proposes a new criterion that enables the identification of total or partial causal effects. In addition, this paper elucidates the mechanism of post-treatment bias, showing that a repeated sequence of nodes can be a potential source of this bias. The results apply to linear data-generating processes represented by directed acyclic graphs with distribution-free error terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04060v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masato Shimokawa</dc:creator>
    </item>
    <item>
      <title>The Algorithmic Phase Transition in Correlated Spiked Models</title>
      <link>https://arxiv.org/abs/2511.06040</link>
      <description>arXiv:2511.06040v4 Announce Type: replace 
Abstract: We study the computational task of detecting and estimating correlated signals in a pair of spiked matrices $$ X=\tfrac{\lambda}{\sqrt{n}} xu^{\top}+W, \quad Y=\tfrac{\mu}{\sqrt{n}} yv^{\top}+Z $$ where the spikes $x,y$ have correlation $\rho$. Specifically, we consider two fundamental models: (1) Correlated spiked Wigner model with signal-to-noise ratio $\lambda,\mu$; (2) Correlated spiked $n*N$ Wishart (covariance) model with signal-to-noise ratio $\sqrt\lambda,\sqrt\mu$.
  We propose an efficient detection and estimation algorithm based on counting a specific family of edge-decorated cycles. The algorithm's performance is governed by the function $$ F(\lambda,\mu,\rho,\gamma)=\max\Big\{ \frac{ \lambda^2 }{ \gamma }, \frac{ \mu^2 }{ \gamma }, \frac{ \lambda^2 \rho^2 }{ \gamma-\lambda^2+\lambda^2 \rho^2 } + \frac{ \mu^2 \rho^2 }{ \gamma-\mu^2+\mu^2 \rho^2 } \Big\} \,. $$ We prove our algorithm succeeds for the correlated spiked Wigner model whenever $F(\lambda,\mu,\rho,1)&gt;1$, and succeeds for the correlated spiked Wishart model whenever $F(\lambda,\mu,\rho,\tfrac{n}{N})&gt;1$. Our result shows that an algorithm can leverage the correlation between the spikes to detect and estimate the signals even in regimes where efficiently recovering either $x$ from ${X}$ alone or $y$ from ${Y}$ alone is believed to be computationally infeasible.
  We complement our algorithmic results with evidence for a matching computational lower bound. In particular, we prove that when $F(\lambda,\mu,\rho,1)&lt;1$ for the correlated spiked Wigner model and when $F(\lambda,\mu,\rho,\tfrac{n}{N})&lt;1$ for the spiked Wishart model, all algorithms based on low-degree polynomials fails to distinguish $({X},{Y})$ with two independent noise matrices. This strongly suggests that $F=1$ is the precise computation threshold for our models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06040v4</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhangsong Li</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation of Joint Entropy via Partitioned Sample-Spacing</title>
      <link>https://arxiv.org/abs/2511.13602</link>
      <description>arXiv:2511.13602v2 Announce Type: replace 
Abstract: We propose a nonparametric estimator of multivariate joint entropy based on partitioned sample spacing (PSS). The method extends univariate spacing ideas to $\mathbb{R}^{d}$ by partitioning into localized cells and aggregating within-cell statistics, with strong consistency guarantees under mild conditions. In benchmarks across diverse distributions, PSS consistently outperforms $k$-nearest neighbor estimators and achieves accuracy competitive with recent normalizing flow-based methods, while requiring no training or auxiliary density modeling. The estimator scales favorably in moderately high dimensions ($d = 10$--$40$) and shows particular robustness to correlated or skewed distributions. These properties position PSS as a practical and reliable alternative to both $k$NN and NF-based entropy estimators, with broad utility in information-theoretic machine learning tasks such as total-correlation estimation, representation learning, and feature selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13602v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jungwoo Ho, Sangun Park, Soyeong Oh</dc:creator>
    </item>
    <item>
      <title>Multiple Randomization Designs: Estimation and Inference with Interference</title>
      <link>https://arxiv.org/abs/2112.13495</link>
      <description>arXiv:2112.13495v4 Announce Type: replace-cross 
Abstract: Completely randomized experiments, originally developed by Fisher and Neyman in the 1930s, are still widely used in practice, even in online experimentation. However, such designs are of limited value for answering standard questions in marketplaces, where multiple populations of agents interact strategically, leading to complex patterns of spillover effects. In this paper, we derive the finite-sample properties of tractable estimators for "Simple Multiple Randomization Designs" (SMRDs), a new class of experimental designs which account for complex spillover effects in randomized experiments. Our derivations are obtained under a natural and general form of cross-unit interference, which we call "local interference". We discuss the estimation of main effects, direct effects, and spillovers, and present associated central limit theorems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.13495v4</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssb/qkaf073</arxiv:DOI>
      <dc:creator>Lorenzo Masoero, Suhas Vijaykumar, Thomas Richardson, James McQueen, Ido Rosen, Brian Burdick, Pat Bajari, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>Sparse PCA With Multiple Components</title>
      <link>https://arxiv.org/abs/2209.14790</link>
      <description>arXiv:2209.14790v4 Announce Type: replace-cross 
Abstract: Sparse Principal Component Analysis (sPCA) is a cardinal technique for obtaining combinations of features, or principal components (PCs), that explain the variance of high-dimensional datasets in an interpretable manner. This involves solving a sparsity and orthogonality constrained convex maximization problem, which is extremely computationally challenging. Most existing works address sparse PCA via methods-such as iteratively computing one sparse PC and deflating the covariance matrix-that do not guarantee the orthogonality, let alone the optimality, of the resulting solution when we seek multiple mutually orthogonal PCs. We challenge this status by reformulating the orthogonality conditions as rank constraints and optimizing over the sparsity and rank constraints simultaneously. We design tight semidefinite relaxations to supply high-quality upper bounds, which we strengthen via additional second-order cone inequalities when each PC's individual sparsity is specified. Further, we derive a combinatorial upper bound on the maximum amount of variance explained as a function of the support. We exploit these relaxations and bounds to propose exact methods and rounding mechanisms that, together, obtain solutions with a bound gap on the order of 0%-15% for real-world datasets with p = 100s or 1000s of features and r \in {2, 3} components. Numerically, our algorithms match (and sometimes surpass) the best performing methods in terms of fraction of variance explained and systematically return PCs that are sparse and orthogonal. In contrast, we find that existing methods like deflation return solutions that violate the orthogonality constraints, even when the data is generated according to sparse orthogonal PCs. Altogether, our approach solves sparse PCA problems with multiple components to certifiable (near) optimality in a practically tractable fashion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.14790v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Cory-Wright, Jean Pauphilet</dc:creator>
    </item>
    <item>
      <title>Robust Estimation of Polychoric Correlation</title>
      <link>https://arxiv.org/abs/2407.18835</link>
      <description>arXiv:2407.18835v5 Announce Type: replace-cross 
Abstract: Polychoric correlation is often an important building block in the analysis of rating data, particularly for structural equation models. However, the commonly employed maximum likelihood (ML) estimator is highly susceptible to misspecification of the polychoric correlation model, for instance through violations of latent normality assumptions. We propose a novel estimator that is designed to be robust against partial misspecification of the polychoric model, that is, when the model is misspecified for an unknown fraction of observations, such as careless respondents. To this end, the estimator minimizes a robust loss function based on the divergence between observed frequencies and theoretical frequencies implied by the polychoric model. In contrast to existing literature, our estimator makes no assumption on the type or degree of model misspecification. It furthermore generalizes ML estimation, is consistent as well as asymptotically normally distributed, and comes at no additional computational cost. We demonstrate the robustness and practical usefulness of our estimator in simulation studies and an empirical application on a Big Five administration. In the latter, the polychoric correlation estimates of our estimator and ML differ substantially, which, after further inspection, is likely due to the presence of careless respondents that the estimator helps identify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18835v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1017/psy.2025.10066</arxiv:DOI>
      <arxiv:journal_reference>Psychometrika (2025+), forthcoming</arxiv:journal_reference>
      <dc:creator>Max Welz, Patrick Mair, Andreas Alfons</dc:creator>
    </item>
    <item>
      <title>Convergence of Shallow ReLU Networks on Weakly Interacting Data</title>
      <link>https://arxiv.org/abs/2502.16977</link>
      <description>arXiv:2502.16977v2 Announce Type: replace-cross 
Abstract: We analyse the convergence of one-hidden-layer ReLU networks trained by gradient flow on $n$ data points. Our main contribution leverages the high dimensionality of the ambient space, which implies low correlation of the input samples, to demonstrate that a network with width of order $\log(n)$ neurons suffices for global convergence with high probability. Our analysis uses a Polyak-{\L}ojasiewicz viewpoint along the gradient-flow trajectory, which provides an exponential rate of convergence of $\frac{1}{n}$. When the data are exactly orthogonal, we give further refined characterizations of the convergence speed, proving its asymptotic behavior lies between the orders $\frac{1}{n}$ and $\frac{1}{\sqrt{n}}$, and exhibiting a phase-transition phenomenon in the convergence rate, during which it evolves from the lower bound to the upper, and in a relative time of order $\frac{1}{\log(n)}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16977v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'eo Dana (SIERRA), Francis Bach (SIERRA), Loucas Pillaud-Vivien (ENPC, CERMICS)</dc:creator>
    </item>
    <item>
      <title>Convergence rate of Euler-Maruyama scheme to the invariant probability measure under total variation distance</title>
      <link>https://arxiv.org/abs/2505.04218</link>
      <description>arXiv:2505.04218v2 Announce Type: replace-cross 
Abstract: This article shows the geometric decay rate of Euler-Maruyama scheme for one-dimensional stochastic differential equation towards its invariant probability measure under total variation distance. Firstly, the existence and uniqueness of invariant probability measure and the uniform geometric ergodicity of the chain are studied through introduction of non-atomic Markov chains. Secondly, the equivalent conditions for uniform geometric ergodicity of the chain are discovered, by constructing a split Markov chain based on the original Euler-Maruyama scheme. It turns out that this convergence rate is independent with the step size under total variation distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04218v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuke Wang, Yinna Ye</dc:creator>
    </item>
    <item>
      <title>How many measurements are enough? Bayesian recovery in inverse problems with general distributions</title>
      <link>https://arxiv.org/abs/2505.10630</link>
      <description>arXiv:2505.10630v2 Announce Type: replace-cross 
Abstract: We study the sample complexity of Bayesian recovery for solving inverse problems with general prior, forward operator and noise distributions. We consider posterior sampling according to an approximate prior $\mathcal{P}$, and establish sufficient conditions for stable and accurate recovery with high probability. Our main result is a non-asymptotic bound that shows that the sample complexity depends on (i) the intrinsic complexity of $\mathcal{P}$, quantified by its so-called approximate covering number, and (ii) concentration bounds for the forward operator and noise distributions. As a key application, we specialize to generative priors, where $\mathcal{P}$ is the pushforward of a latent distribution via a Deep Neural Network (DNN). We show that the sample complexity scales log-linearly with the latent dimension $k$, thus establishing the efficacy of DNN-based priors. Generalizing existing results on deterministic (i.e., non-Bayesian) recovery for the important problem of random sampling with an orthogonal matrix $U$, we show how the sample complexity is determined by the coherence of $U$ with respect to the support of $\mathcal{P}$. Hence, we establish that coherence plays a fundamental role in Bayesian recovery as well. Overall, our framework unifies and extends prior work, providing rigorous guarantees for the sample complexity of solving Bayesian inverse problems with arbitrary distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10630v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Adcock, Nick Huang</dc:creator>
    </item>
    <item>
      <title>Overfitting has a limitation: a model-independent generalization gap bound based on R\'enyi entropy</title>
      <link>https://arxiv.org/abs/2506.00182</link>
      <description>arXiv:2506.00182v2 Announce Type: replace-cross 
Abstract: Will further scaling up of machine learning models continue to bring success? A significant challenge in answering this question lies in understanding generalization gap, which is the impact of overfitting. Understanding generalization gap behavior of increasingly large-scale machine learning models remains a significant area of investigation, as conventional analyses often link error bounds to model complexity, failing to fully explain the success of extremely large architectures. This research introduces a novel perspective by establishing a model-independent upper bound for generalization gap applicable to algorithms whose outputs are determined solely by the data's histogram, such as empirical risk minimization or gradient-based methods. Crucially, this bound is shown to depend only on the R\'enyi entropy of the data-generating distribution, suggesting that a small generalization gap can be maintained even with arbitrarily large models, provided the data quantity is sufficient relative to this entropy. This framework offers a direct explanation for the phenomenon where generalization performance degrades significantly upon injecting random noise into data, where the performance degrade is attributed to the consequent increase in the data distribution's R\'enyi entropy. Furthermore, we adapt the no-free-lunch theorem to be data-distribution-dependent, demonstrating that an amount of data corresponding to the R\'enyi entropy is indeed essential for successful learning, thereby highlighting the tightness of our proposed generalization bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00182v2</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atsushi Suzuki, Jing Wang</dc:creator>
    </item>
    <item>
      <title>A Weighted Regression Approach to Break-Point Detection in Panel Data</title>
      <link>https://arxiv.org/abs/2510.00598</link>
      <description>arXiv:2510.00598v2 Announce Type: replace-cross 
Abstract: New procedures for detecting a change in the cross-sectional mean of panel data are proposed. The procedures rely on estimating nuisance parameters using certain cross-sectional means across panels using a weighted least squares regression. In the case of weak cross-sectional dependence between panels, we show how test statistics can be constructed to have a limit null distribution not depending on any choice of bandwidths typically needed to estimate the long-run variances of the panel errors. The theoretical assertions are derived for general choices of the regression weights, and it is shown that consistent test procedures can be obtained from the proposed process. The theoretical results are extended to the case where strong cross-sectional dependence exist between panels. The paper concludes with a numerical study illustrating the behavior of several special cases of the test procedure in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00598v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Charl Pretorius, Heinrich Roodt</dc:creator>
    </item>
    <item>
      <title>Robust Estimation for Dependent Binary Network Data</title>
      <link>https://arxiv.org/abs/2510.22177</link>
      <description>arXiv:2510.22177v2 Announce Type: replace-cross 
Abstract: We consider the problem of learning the interaction strength between the nodes of a network based on dependent binary observations residing on these nodes, generated from a Markov Random Field (MRF). Since these observations can possibly be corrupted/noisy in larger networks in practice, it is important to robustly estimate the parameters of the underlying true MRF to account for such inherent contamination in observed data. However, it is well-known that classical likelihood and pseudolikelihood based approaches are highly sensitive to even a small amount of data contamination. So, in this paper, we propose a density power divergence (DPD) based robust generalization of the computationally efficient maximum pseudolikelihood (MPL) estimator of the interaction strength parameter, and derive its rate of consistency under the pure model. Along the way, we establish consistency and asymptotics for a class of general $Z$-estimators, covering our proposed DPD based estimators, under flexible assumptions that hold for a substantial class of standard models. To the best of our knowledge, these are the first central limit theorems for the class of general $Z$-estimators in such settings. Moreover, we show that the gross error sensitivities of the proposed DPD based estimators are significantly smaller than that of the MPL estimator, thereby theoretically justifying the greater (local) robustness of the former under contaminated settings. Finally, we demonstrate the superior (finite sample) performance of the DPD based variants over the traditional MPL estimator in a number of synthetically generated contaminated network datasets, and apply them to learn the network interaction strength in several real datasets from diverse domains of social science, neurobiology and genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22177v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Liu, Somabha Mukherjee, Abhik Ghosh</dc:creator>
    </item>
    <item>
      <title>Geometric Calibration and Neutral Zones for Uncertainty-Aware Multi-Class Classification</title>
      <link>https://arxiv.org/abs/2511.20960</link>
      <description>arXiv:2511.20960v2 Announce Type: replace-cross 
Abstract: Modern artificial intelligence systems make critical decisions yet often fail silently when uncertain -- even well-calibrated models provide no mechanism to identify \textit{which specific predictions} are unreliable. We develop a geometric framework addressing both calibration and instance-level uncertainty quantification for neural network probability outputs. Treating probability vectors as points on the $(c-1)$-dimensional probability simplex equipped with the Fisher--Rao metric, we construct: (i) Additive Log-Ratio (ALR) calibration maps that reduce exactly to Platt scaling for binary problems while extending naturally to multi-class settings, and (ii) geometric reliability scores that translate calibrated probabilities into actionable uncertainty measures, enabling principled deferral of ambiguous predictions to human review.
  Theoretical contributions include: consistency of the calibration estimator at rate $O_p(n^{-1/2})$ via M-estimation theory (Theorem~1), and tight concentration bounds for reliability scores with explicit sub-Gaussian parameters enabling sample size calculations for validation set design (Theorem~2). We conjecture Neyman--Pearson optimality of our neutral zone construction based on connections to Bhattacharyya coefficients. Empirical validation on Adeno-Associated Virus classification demonstrates that the two-stage framework captures 72.5\% of errors while deferring 34.5\% of samples, reducing automated decision error rates from 16.8\% to 6.9\%. Notably, calibration alone yields marginal accuracy gains; the operational benefit arises primarily from the reliability scoring mechanism, which applies to any well-calibrated probability output. This work bridges information geometry and statistical learning, offering formal guarantees for uncertainty-aware classification in applications requiring rigorous validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20960v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soumojit Das, Nairanjana Dasgupta, Prashanta Dutta</dc:creator>
    </item>
    <item>
      <title>Convergence of a Sequential Monte Carlo algorithm towards multimodal distributions on Rd</title>
      <link>https://arxiv.org/abs/2511.22564</link>
      <description>arXiv:2511.22564v2 Announce Type: replace-cross 
Abstract: In an earlier joint work, we studied a sequential Monte Carlo algorithm to sample from the Gibbs measure supported on torus with a non-convex energy function at a low temperature, where we proved that the time complexity of the algorithm is polynomial in the inverse temperature. However, the analysis in that torus setting relied crucially on compactness and does not directly extend to unbounded domains. This work introduces a new approach that resolves this issue and establishes a similar result for sampling from Gibbs measures supported on Rd.
  In particular, our main result shows that when the energy function is double-well with equal depth, the time complexity scales as seventh power of the inverse temperature, and quadratically in both the inverse allowed absolute error and probability error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22564v2</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiyu Han</dc:creator>
    </item>
  </channel>
</rss>
