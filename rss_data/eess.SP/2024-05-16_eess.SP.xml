<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.SP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.SP</link>
    <description>eess.SP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.SP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 May 2024 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Explainable Self-Organizing Artificial Intelligence Captures Landscape Changes Correlated with Human Impact Data</title>
      <link>https://arxiv.org/abs/2405.09547</link>
      <description>arXiv:2405.09547v1 Announce Type: new 
Abstract: Novel methods of analysis are needed to help advance our understanding of the intricate interplay between landscape changes, population dynamics, and sustainable development. Self organized machine learning has been highly successful in the analysis of visual data the human expert eye may not be able to see. Thus, subtle but significant changes in fine visual detail in images relating to trending alterations in natural or urban landscapes may remain undetected. In the course of time, such changes may be the cause or the consequence of measurable human impact. Capturing such change in imaging data as early as possible can make critical information readily available to citizens, professionals and policymakers. This promotes change awareness, and facilitates early decision making for action. Here, we use unsupervised Artificial Intelligence (AI) that exploits principles of self-organized biological visual learning for the analysis of imaging time series. The quantization error in the output of a Self Organizing Map prototype is exploited as a computational metric of variability and change. Given the proven sensitivity of this neural network metric to the intensity and polarity of image pixel colour, it is shown to capture critical changes in urban landscapes. This is achieved here on imaging data for two regions of geographic interest in Las Vegas County, Nevada, USA. The SOM analysis is combined with the statistical analysis of demographic data revealing human impacts. These latter are significantly correlated with the structural change trends in the numerical data for the specific regions of interest. By correlating data relative to the impact of human activities with numerical data indicating structural evolution, human footprint related environmental changes can be predictably scaled.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09547v1</guid>
      <category>eess.SP</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Computer Science and Software Development, 2024, 3(1), 103</arxiv:journal_reference>
      <dc:creator>John M. Wandeto, Birgitta Dresp-Langley</dc:creator>
    </item>
    <item>
      <title>Efficient Bilevel Source Mask Optimization</title>
      <link>https://arxiv.org/abs/2405.09548</link>
      <description>arXiv:2405.09548v1 Announce Type: new 
Abstract: Resolution Enhancement Techniques (RETs) are critical to meet the demands of advanced technology nodes. Among RETs, Source Mask Optimization (SMO) is pivotal, concurrently optimizing both the source and the mask to expand the process window. Traditional SMO methods, however, are limited by sequential and alternating optimizations, leading to extended runtimes without performance guarantees. This paper introduces a unified SMO framework utilizing the accelerated Abbe forward imaging to enhance precision and efficiency. Further, we propose the innovative \texttt{BiSMO} framework, which reformulates SMO through a bilevel optimization approach, and present three gradient-based methods to tackle the challenges of bilevel SMO. Our experimental results demonstrate that \texttt{BiSMO} achieves a remarkable 40\% reduction in error metrics and 8$\times$ increase in runtime efficiency, signifying a major leap forward in SMO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09548v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guojin Chen, Hongquan He, Peng Xu, Hao Geng, Bei Yu</dc:creator>
    </item>
    <item>
      <title>Towards Bi-Hemispheric Emotion Mapping through EEG: A Dual-Stream Neural Network Approach</title>
      <link>https://arxiv.org/abs/2405.09551</link>
      <description>arXiv:2405.09551v1 Announce Type: new 
Abstract: Emotion classification through EEG signals plays a significant role in psychology, neuroscience, and human-computer interaction. This paper addresses the challenge of mapping human emotions using EEG data in the Mapping Human Emotions through EEG Signals FG24 competition. Subjects mimic the facial expressions of an avatar, displaying fear, joy, anger, sadness, disgust, and surprise in a VR setting. EEG data is captured using a multi-channel sensor system to discern brain activity patterns. We propose a novel two-stream neural network employing a Bi-Hemispheric approach for emotion inference, surpassing baseline methods and enhancing emotion recognition accuracy. Additionally, we conduct a temporal analysis revealing that specific signal intervals at the beginning and end of the emotion stimulus sequence contribute significantly to improve accuracy. Leveraging insights gained from this temporal analysis, our approach offers enhanced performance in capturing subtle variations in the states of emotions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09551v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Freire-Obreg\'on, Daniel Hern\'andez-Sosa, Oliverio J. Santana, Javier Lorenzo-Navarro, Modesto Castrill\'on-Santana</dc:creator>
    </item>
    <item>
      <title>Underdetermined DOA Estimation of Off-Grid Sources Based on the Generalized Double Pareto Prior</title>
      <link>https://arxiv.org/abs/2405.09554</link>
      <description>arXiv:2405.09554v1 Announce Type: new 
Abstract: In this letter, we investigate a new generalized double Pareto based on off-grid sparse Bayesian learning (GDPOGSBL) approach to improve the performance of direction of arrival (DOA) estimation in underdetermined scenarios. The method aims to enhance the sparsity of source signal by utilizing the generalized double Pareto (GDP) prior. Firstly, we employ a first-order linear Taylor expansion to model the real array manifold matrix, and Bayesian inference is utilized to calculate the off-grid error, which mitigates the grid dictionary mismatch problem in underdetermined scenarios. Secondly, an innovative grid refinement method is introduced, treating grid points as iterative parameters to minimize the modeling error between the source and grid points. The numerical simulation results verify the superiority of the proposed strategy, especially when dealing with a coarse grid and few snapshots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09554v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yongfeng Huang, Zhendong Chen, Kun Ye, Lang Zhou, Haixin Sun</dc:creator>
    </item>
    <item>
      <title>Analysis of Near-Field Effects, Spatial Non-Stationary Characteristics Based on 11-15 GHz Channel Measurement in Indoor Scenario</title>
      <link>https://arxiv.org/abs/2405.09555</link>
      <description>arXiv:2405.09555v1 Announce Type: new 
Abstract: In the sixth-generation (6G), with the further expansion of array element number and frequency bands, the wireless communications are expected to operate in the near-field region. The near-field radio communications (NFRC) will become crucial in 6G communication systems. The new mid-band (6-24 GHz) is the 6G potential candidate spectrum. In this paper, we will investigate the channel measurements and characteristics for the emerging NFRC. First, the near-field spherical-wave signal model is derived in detail, and the stationary interval (SI) division method is discussed based on the channel statistical properties. Then, the influence of line-of-sight (LOS) and obstructed-LOS (OLOS) environments on the near-field effects and spatial non-stationary (SnS) characteristic are explored based on the near-field channel measurements at 11-15 GHz band. We hope that this work will give some reference to the NFRC research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09555v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiyang Miao, Pan Tang, Weirang Zuo, Qi Wei, Lei Tian, Jianhua Zhang</dc:creator>
    </item>
    <item>
      <title>Co-learning-aided Multi-modal-deep-learning Framework of Passive DOA Estimators for a Heterogeneous Hybrid Massive MIMO Receiver</title>
      <link>https://arxiv.org/abs/2405.09556</link>
      <description>arXiv:2405.09556v1 Announce Type: new 
Abstract: Due to its excellent performance in rate and resolution, fully-digital (FD) massive multiple-input multiple-output (MIMO) antenna arrays has been widely applied in data transmission and direction of arrival (DOA) measurements, etc. But it confronts with two main challenges: high computational complexity and circuit cost. The two problems may be addressed well by hybrid analog-digital (HAD) structure. But there exists the problem of phase ambiguity for HAD, which leads to its low-efficiency or high-latency. Does exist there such a MIMO structure of owning low-cost, low-complexity and high time efficiency at the same time. To satisfy the three properties, a novel heterogeneous hybrid MIMO receiver structure of integrating FD and heterogeneous HAD ($\rm{H}^2$AD-FD) is proposed and corresponding multi-modal (MD)-learning framework is developed. The framework includes three major stages: 1) generate the candidate sets via root multiple signal classification (Root-MUSIC) or deep learning (DL); 2) infer the class of true solutions from candidate sets using machine learning (ML) methods; 3) fuse the two-part true solutions to achieve a better DOA estimation. The above process form two methods named MD-Root-MUSIC and MDDL. To improve DOA estimation accuracy and reduce the clustering complexity, a co-learning-aided MD framework is proposed to form two enhanced methods named CoMDDL and CoMD-RootMUSIC. Moreover, the Cramer-Rao lower bound (CRLB) for the proposed $\rm{H}^2$AD-FD structure is also derived. Experimental results demonstrate that our proposed four methods could approach the CRLB for signal-to-noise ratio (SNR) &gt; 0 dB and the proposed CoMDDL and MDDL perform better than CoMD-RootMUSIC and MD-RootMUSIC, particularly in the extremely low SNR region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09556v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiatong Bai, Feng Shu, Qinghe Zheng, Bo Xu, Baihua Shi, Yiwen Chen, Weibin Zhang, Xianpeng Wang</dc:creator>
    </item>
    <item>
      <title>Machine Learning in Short-Reach Optical Systems: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2405.09557</link>
      <description>arXiv:2405.09557v1 Announce Type: new 
Abstract: In recent years, extensive research has been conducted to explore the utilization of machine learning algorithms in various direct-detected and self-coherent short-reach communication applications. These applications encompass a wide range of tasks, including bandwidth request prediction, signal quality monitoring, fault detection, traffic prediction, and digital signal processing (DSP)-based equalization. As a versatile approach, machine learning demonstrates the ability to address stochastic phenomena in optical systems networks where deterministic methods may fall short. However, when it comes to DSP equalization algorithms, their performance improvements are often marginal, and their complexity is prohibitively high, especially in cost-sensitive short-reach communications scenarios such as passive optical networks (PONs). They excel in capturing temporal dependencies, handling irregular or nonlinear patterns effectively, and accommodating variable time intervals. Within this extensive survey, we outline the application of machine learning techniques in short-reach communications, specifically emphasizing their utilization in high-bandwidth demanding PONs. Notably, we introduce a novel taxonomy for time-series methods employed in machine learning signal processing, providing a structured classification framework. Our taxonomy categorizes current time series methods into four distinct groups: traditional methods, Fourier convolution-based methods, transformer-based models, and time-series convolutional networks. Finally, we highlight prospective research directions within this rapidly evolving field and outline specific solutions to mitigate the complexity associated with hardware implementations. We aim to pave the way for more practical and efficient deployment of machine learning approaches in short-reach optical communication systems by addressing complexity concerns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09557v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Shao, Syed Moktacim Billah, Elias Giacoumidis, Shi Li, Jialei Li, Prashasti Sahu, Andre Richter, Tobias Kaefer, Michael Faerber</dc:creator>
    </item>
    <item>
      <title>An EM Body Model for Device-Free Localization with Multiple Antenna Receivers: A First Study</title>
      <link>https://arxiv.org/abs/2405.09558</link>
      <description>arXiv:2405.09558v1 Announce Type: new 
Abstract: Device-Free Localization (DFL) employs passive radio techniques capable to detect and locate people without imposing them to wear any electronic device. By exploiting the Integrated Sensing and Communication paradigm, DFL networks employ Radio Frequency (RF) nodes to measure the excess attenuation introduced by the subjects (i.e., human bodies) moving inside the monitored area, and to estimate their positions and movements. Physical, statistical, and ElectroMagnetic (EM) models have been proposed in the literature to estimate the body positions according to the RF signals collected by the nodes. These body models usually employ a single-antenna processing for localization purposes. However, the availability of low-cost multi-antenna devices such as those used for WLAN (Wireless Local Area Network) applications and the timely development of array-based body models, allow us to employ array-based processing techniques in DFL networks. By exploiting a suitable array-capable EM body model, this paper proposes an array-based framework to improve people sensing and localization. In particular, some simulations are proposed and discussed to compare the model results in both single- and multi-antenna scenarios. The proposed framework paves the way for a wider use of multi-antenna devices (e.g., those employed in current IEEE 802.11ac/ax/be and forthcoming IEEE 802.11be networks) and novel beamforming algorithms for DFL scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09558v1</guid>
      <category>eess.SP</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/APWC57320.2023.10297446</arxiv:DOI>
      <arxiv:journal_reference>2023 IEEE-APS Topical Conference on Antennas and Propagation in Wireless Communications (APWC)</arxiv:journal_reference>
      <dc:creator>Vittorio Rampa, Federica Fieramosca, Stefano Savazzi, Michele D'Amico</dc:creator>
    </item>
    <item>
      <title>KID-PPG: Knowledge Informed Deep Learning for Extracting Heart Rate from a Smartwatch</title>
      <link>https://arxiv.org/abs/2405.09559</link>
      <description>arXiv:2405.09559v1 Announce Type: new 
Abstract: Accurate extraction of heart rate from photoplethysmography (PPG) signals remains challenging due to motion artifacts and signal degradation. Although deep learning methods trained as a data-driven inference problem offer promising solutions, they often underutilize existing knowledge from the medical and signal processing community. In this paper, we address three shortcomings of deep learning models: motion artifact removal, degradation assessment, and physiologically plausible analysis of the PPG signal. We propose KID-PPG, a knowledge-informed deep learning model that integrates expert knowledge through adaptive linear filtering, deep probabilistic inference, and data augmentation. We evaluate KID-PPG on the PPGDalia dataset, achieving an average mean absolute error of 2.85 beats per minute, surpassing existing reproducible methods. Our results demonstrate a significant performance improvement in heart rate tracking through the incorporation of prior knowledge into deep learning models. This approach shows promise in enhancing various biomedical applications by incorporating existing expert knowledge in deep learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09559v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christodoulos Kechris, Jonathan Dan, Jose Miranda, David Atienza</dc:creator>
    </item>
    <item>
      <title>Using In-Service Train Vibration for Detecting Railway Maintenance Needs</title>
      <link>https://arxiv.org/abs/2405.09560</link>
      <description>arXiv:2405.09560v1 Announce Type: new 
Abstract: The need for the maintenance of railway track systems have been increasing. Traditional methods that are currently being used are either inaccurate, labor and time intensive, or does not enable continuous monitoring of the system. As a result, in-service train vibrations have been shown to be a cheaper alternative for monitoring of railway track systems. In this paper, a method is proposed to detect different maintenance needs of railway track systems using a single pass of train direction. The DR-Train dataset that is publicly available was used. Results show that by using a simple classifier such as the k-nearest neighbor (k-NN) algorithm, the signal energy features of the acceleration data can achieve 76\% accuracy on two types of maintenance needs, tamping and surfacing. The results show that the transverse direction is able to more accurately detect maintenance needs, and triaxial accelerometer can give further information on the maintenance needs. Furthermore, this paper demonstrates the use of multi-label classification to detect multiple types of maintenance needs simultaneously. The results show multi-label classification performs only slightly worse than the simple binary classification (72\% accuracy) and that this can be a simple method that can easily be deployed in areas that have a history of many maintenance issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09560v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Irene Alisjahbana</dc:creator>
    </item>
    <item>
      <title>GAD: A Real-time Gait Anomaly Detection System with Online Adaptive Learning</title>
      <link>https://arxiv.org/abs/2405.09561</link>
      <description>arXiv:2405.09561v1 Announce Type: new 
Abstract: Gait anomaly detection is a task that involves detecting deviations from a person's normal gait pattern. These deviations can indicate health issues and medical conditions in the healthcare domain, or fraudulent impersonation and unauthorized identity access in the security domain. A number of gait anomaly detection approaches have been introduced, but many of them require offline data preprocessing, offline model learning, setting parameters, and so on, which might restrict their effectiveness and applicability in real-world scenarios. To address these issues, this paper introduces GAD, a real-time gait anomaly detection system. GAD focuses on detecting anomalies within an individual's three-dimensional accelerometer readings based on dimensionality reduction and Long Short-Term Memory (LSTM). Upon being launched, GAD begins collecting a gait segment from the user and training an anomaly detector to learn the user's walking pattern on the fly. If the subsequent model verification is successful, which involves validating the trained detector using the user's subsequent steps, the detector is employed to identify abnormalities in the user's subsequent gait readings at the user's request. The anomaly detector will be retained online to adapt to minor pattern changes and will undergo retraining as long as it cannot provide adequate prediction. We explored two methods for capturing users' gait segments: a personalized method tailored to each individual's step length, and a uniform method utilizing a fixed step length. Experimental results using an open-source gait dataset show that GAD achieves a higher detection accuracy ratio when combined with the personalized method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09561v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming-Chang Lee, Jia-Chun Lin, Sokratis Katsikas</dc:creator>
    </item>
    <item>
      <title>MEET: Mixture of Experts Extra Tree-Based sEMG Hand Gesture Identification</title>
      <link>https://arxiv.org/abs/2405.09562</link>
      <description>arXiv:2405.09562v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has made significant advances in recent years and opened up new possibilities in exploring applications in various fields such as biomedical, robotics, education, industry, etc. Among these fields, human hand gesture recognition is a subject of study that has recently emerged as a research interest in robotic hand control using electromyography (EMG). Surface electromyography (sEMG) is a primary technique used in EMG, which is popular due to its non-invasive nature and is used to capture gesture movements using signal acquisition devices placed on the surface of the forearm. Moreover, these signals are pre-processed to extract significant handcrafted features through time and frequency domain analysis. These are helpful and act as input to machine learning (ML) models to identify hand gestures. However, handling multiple classes and biases are major limitations that can affect the performance of an ML model. Therefore, to address this issue, a new mixture of experts extra tree (MEET) model is proposed to identify more accurate and effective hand gesture movements. This model combines individual ML models referred to as experts, each focusing on a minimal class of two. Moreover, a fully trained model known as the gate is employed to weigh the output of individual expert models. This amalgamation of the expert models with the gate model is known as a mixture of experts extra tree (MEET) model. In this study, four subjects with six hand gesture movements have been considered and their identification is evaluated among eleven models, including the MEET classifier. Results elucidate that the MEET classifier performed best among other algorithms and identified hand gesture movement accurately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09562v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Naveen Gehlot, Ashutosh Jena, Rajesh Kumar, Mahipal Bukya</dc:creator>
    </item>
    <item>
      <title>Stressor Type Matters! -- Exploring Factors Influencing Cross-Dataset Generalizability of Physiological Stress Detection</title>
      <link>https://arxiv.org/abs/2405.09563</link>
      <description>arXiv:2405.09563v1 Announce Type: new 
Abstract: Automatic stress detection using heart rate variability (HRV) features has gained significant traction as it utilizes unobtrusive wearable sensors measuring signals like electrocardiogram (ECG) or blood volume pulse (BVP). However, detecting stress through such physiological signals presents a considerable challenge owing to the variations in recorded signals influenced by factors, such as perceived stress intensity and measurement devices. Consequently, stress detection models developed on one dataset may perform poorly on unseen data collected under different conditions. To address this challenge, this study explores the generalizability of machine learning models trained on HRV features for binary stress detection. Our goal extends beyond evaluating generalization performance; we aim to identify the characteristics of datasets that have the most significant influence on generalizability. We leverage four publicly available stress datasets (WESAD, SWELL-KW, ForDigitStress, VerBIO) that vary in at least one of the characteristics such as stress elicitation techniques, stress intensity, and sensor devices. Employing a cross-dataset evaluation approach, we explore which of these characteristics strongly influence model generalizability. Our findings reveal a crucial factor affecting model generalizability: stressor type. Models achieved good performance across datasets when the type of stressor (e.g., social stress in our case) remains consistent. Factors like stress intensity or brand of the measurement device had minimal impact on cross-dataset performance. Based on our findings, we recommend matching the stressor type when deploying HRV-based stress models in new environments. To the best of our knowledge, this is the first study to systematically investigate factors influencing the cross-dataset applicability of HRV-based stress models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09563v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pooja Prajod, Bhargavi Mahesh, Elisabeth Andr\'e</dc:creator>
    </item>
    <item>
      <title>Detecting 5G Narrowband Jammers with CNN, k-nearest Neighbors, and Support Vector Machines</title>
      <link>https://arxiv.org/abs/2405.09564</link>
      <description>arXiv:2405.09564v1 Announce Type: new 
Abstract: 5G cellular networks are particularly vulnerable against narrowband jammers that target specific control sub-channels in the radio signal. One mitigation approach is to detect such jamming attacks with an online observation system, based on machine learning. We propose to detect jamming at the physical layer with a pre-trained machine learning model that performs binary classification. Based on data from an experimental 5G network, we study the performance of different classification models. A convolutional neural network will be compared to support vector machines and k-nearest neighbors, where the last two methods are combined with principal component analysis. The obtained results show substantial differences in terms of classification accuracy and computation time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09564v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Varotto, Florian Heinrichs, Timo Schuerg, Stefano Tomasin, Stefan Valentin</dc:creator>
    </item>
    <item>
      <title>One-Class Classification as GLRT for Jamming Detection in Private 5G Networks</title>
      <link>https://arxiv.org/abs/2405.09565</link>
      <description>arXiv:2405.09565v1 Announce Type: new 
Abstract: 5G mobile networks are vulnerable to jamming attacks that may jeopardize valuable applications such as industry automation. In this paper, we propose to analyze radio signals with a dedicated device to detect jamming attacks. We pursue a learning approach, with the detector being a CNN implementing a GLRT. To this end, the CNN is trained as a two-class classifier using two datasets: one of real legitimate signals and another generated artificially so that the resulting classifier implements the GLRT. The artificial dataset is generated mimicking different types of jamming signals. We evaluate the performance of this detector using experimental data obtained from a private 5G network and several jamming signals, showing the technique's effectiveness in detecting the attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09565v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Varotto, Stefan Valentin, Francesco Ardizzon, Samuele Marzotto, Stefano Tomasin</dc:creator>
    </item>
    <item>
      <title>Detection of Sleep Oxygen Desaturations from Electroencephalogram Signals</title>
      <link>https://arxiv.org/abs/2405.09566</link>
      <description>arXiv:2405.09566v1 Announce Type: new 
Abstract: In this work, we leverage machine learning techniques to identify potential biomarkers of oxygen desaturation during sleep exclusively from electroencephalogram (EEG) signals in pediatric patients with sleep apnea. Development of a machine learning technique which can successfully identify EEG signals from patients with sleep apnea as well as identify latent EEG signals which come from subjects who experience oxygen desaturations but do not themselves occur during oxygen desaturation events would provide a strong step towards developing a brain-based biomarker for sleep apnea in order to aid with easier diagnosis of this disease. We leverage a large corpus of data, and show that machine learning enables us to classify EEG signals as occurring during oxygen desaturations or not occurring during oxygen desaturations with an average 66.8% balanced accuracy. We furthermore investigate the ability of machine learning models to identify subjects who experience oxygen desaturations from EEG data that does not occur during oxygen desaturations. We conclude that there is a potential biomarker for oxygen desaturation in EEG data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09566v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shashank Manjunath, Aarti Sathyanarayana</dc:creator>
    </item>
    <item>
      <title>ECG-SMART-NET: A Deep Learning Architecture for Precise ECG Diagnosis of Occlusion Myocardial Infarction</title>
      <link>https://arxiv.org/abs/2405.09567</link>
      <description>arXiv:2405.09567v1 Announce Type: new 
Abstract: In this paper we describe ECG-SMART-NET for identification of occlusion myocardial infarction (OMI). OMI is a severe form of heart attack characterized by complete blockage of one or more coronary arteries requiring immediate referral for cardiac catheterization to restore blood flow to the heart. Two thirds of OMI cases are difficult to visually identify from a 12-lead electrocardiogram (ECG) and can be potentially fatal if not identified in a timely fashion. Previous works on this topic are scarce, and current state-of-the-art evidence suggests that both random forests with engineered features and convolutional neural networks (CNNs) are promising approaches to improve the ECG detection of OMI. While the ResNet architecture has been successfully adapted for use with ECG recordings, it is not ideally suited to capture informative temporal features within each lead and the spatial concordance or discordance across leads. We propose a clinically informed modification of the ResNet-18 architecture. The model first learns temporal features through temporal convolutional layers with 1xk kernels followed by a spatial convolutional layer, after the residual blocks, with 12x1 kernels to learn spatial features. The new ECG-SMART-NET was benchmarked against the original ResNet-18 and other state-of-the-art models on a multisite real-word clinical dataset that consists of 10,893 ECGs from 7,297 unique patients (rate of OMI = 6.5%). ECG-SMART-NET outperformed other models in the classification of OMI with a test AUC score of 0.889 +/- 0.027 and a test average precision score of 0.587 +/- 0.087.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09567v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan T. Riek, Murat Akcakaya, Zeineb Bouzid, Tanmay Gokhale, Stephanie Helman, Karina Kraevsky-Philips, Rui Qi Ji, Ervin Sejdic, Jessica K. Z\`egre-Hemsey, Christian Martin-Gill, Clifton W. Callaway, Samir Saba, Salah Al-Zaiti</dc:creator>
    </item>
    <item>
      <title>Dynamic GNNs for Precise Seizure Detection and Classification from EEG Data</title>
      <link>https://arxiv.org/abs/2405.09568</link>
      <description>arXiv:2405.09568v1 Announce Type: new 
Abstract: Diagnosing epilepsy requires accurate seizure detection and classification, but traditional manual EEG signal analysis is resource-intensive. Meanwhile, automated algorithms often overlook EEG's geometric and semantic properties critical for interpreting brain activity. This paper introduces NeuroGNN, a dynamic Graph Neural Network (GNN) framework that captures the dynamic interplay between the EEG electrode locations and the semantics of their corresponding brain regions. The specific brain region where an electrode is placed critically shapes the nature of captured EEG signals. Each brain region governs distinct cognitive functions, emotions, and sensory processing, influencing both the semantic and spatial relationships within the EEG data. Understanding and modeling these intricate brain relationships are essential for accurate and meaningful insights into brain activity. This is precisely where the proposed NeuroGNN framework excels by dynamically constructing a graph that encapsulates these evolving spatial, temporal, semantic, and taxonomic correlations to improve precision in seizure detection and classification. Our extensive experiments with real-world data demonstrate that NeuroGNN significantly outperforms existing state-of-the-art models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09568v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-981-97-2238-9_16</arxiv:DOI>
      <arxiv:journal_reference>Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD'24). Singapore: Springer Nature Singapore, 2024</arxiv:journal_reference>
      <dc:creator>Arash Hajisafi, Haowen Lin, Yao-Yi Chiang, Cyrus Shahabi</dc:creator>
    </item>
    <item>
      <title>GaitMotion: A Multitask Dataset for Pathological Gait Forecasting</title>
      <link>https://arxiv.org/abs/2405.09569</link>
      <description>arXiv:2405.09569v1 Announce Type: new 
Abstract: Gait benchmark empowers uncounted encouraging research fields such as gait recognition, humanoid locomotion, etc. Despite the growing focus on gait analysis, the research community is hindered by the limitations of the currently available databases, which mostly consist of videos or images with limited labeling. In this paper, we introduce GaitMotion, a multitask dataset leveraging wearable sensors to capture the patients' real-time movement with pathological gait. This dataset offers extensive ground-truth labeling for multiple tasks, including step/stride segmentation and step/stride length prediction, empowers researchers with a more holistic understanding of gait disturbances linked to neurological impairments. The wearable gait analysis suit captures the gait cycle, pattern, and parameters for both normal and pathological subjects. This data may prove beneficial for healthcare products focused on patient progress monitoring and post-disease recovery, as well as for forensics technologies aimed at person reidentification, and biomechanics research to aid in the development of humanoid robotics. Moreover, the analysis has considered the drift in data distribution across individual subjects. This drift can be attributed to each participant's unique behavioral habits or potential displacement of the sensor. Stride length variance for normal, Parkinson's, and stroke patients are compared to recognize the pathological walking pattern. As the baseline and benchmark, we provide an error of 14.1, 13.3, and 12.2 centimeters of stride length prediction for normal, Parkinson's, and Stroke gaits separately. We also analyzed the gait characteristics for normal and pathological gaits in terms of the gait cycle and gait parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09569v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenwen Zhang, Hao Zhang, Zenan Jiang, Jing Wang, Amir Servati, Peyman Servati</dc:creator>
    </item>
    <item>
      <title>FunnelNet: An End-to-End Deep Learning Framework to Monitor Digital Heart Murmur in Real-Time</title>
      <link>https://arxiv.org/abs/2405.09570</link>
      <description>arXiv:2405.09570v1 Announce Type: new 
Abstract: Objective: Heart murmurs are abnormal sounds caused by turbulent blood flow within the heart. Several diagnostic methods are available to detect heart murmurs and their severity, such as cardiac auscultation, echocardiography, phonocardiogram (PCG), etc. However, these methods have limitations, including extensive training and experience among healthcare providers, cost and accessibility of echocardiography, as well as noise interference and PCG data processing. This study aims to develop a novel end-to-end real-time heart murmur detection approach using traditional and depthwise separable convolutional networks. Methods: Continuous wavelet transform (CWT) was applied to extract meaningful features from the PCG data. The proposed network has three parts: the Squeeze net, the Bottleneck, and the Expansion net. The Squeeze net generates a compressed data representation, whereas the Bottleneck layer reduces computational complexity using a depthwise-separable convolutional network. The Expansion net is responsible for up-sampling the compressed data to a higher dimension, capturing tiny details of the representative data. Results: For evaluation, we used four publicly available datasets and achieved state-of-the-art performance in all datasets. Furthermore, we tested our proposed network on two resource-constrained devices: a Raspberry PI and an Android device, stripping it down into a tiny machine learning model (TinyML), achieving a maximum of 99.70%. Conclusion: The proposed model offers a deep learning framework for real-time accurate heart murmur detection within limited resources. Significance: It will significantly result in more accessible and practical medical services and reduced diagnosis time to assist medical professionals. The code is publicly available at TBA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09570v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Jobayer, Md. Mehedi Hasan Shawon, Md Rakibul Hasan, Shreya Ghosh, Tom Gedeon, Md Zakir Hossain</dc:creator>
    </item>
    <item>
      <title>The Best Radar Ranging Pulse to Resolve Two Reflectors</title>
      <link>https://arxiv.org/abs/2405.09571</link>
      <description>arXiv:2405.09571v1 Announce Type: new 
Abstract: Previous work established fundamental bounds on subwavelength resolution for the radar range resolution problem, called superradar [Phys. Rev. Appl. 20, 064046 (2023)]. In this work, we identify the optimal waveforms for distinguishing the range resolution between two reflectors of identical strength. We discuss both the unnormalized optimal waveform as well as the best square-integrable pulse, and their variants. Using orthogonal function theory, we give an explicit algorithm to optimize the wave pulse in finite time to have the best performance. We also explore range resolution estimation with unnormalized waveforms with multi-parameter methods to also independently estimate loss and time of arrival. These results are consistent with the earlier single parameter approach of range resolution only and give deeper insight into the ranging estimation problem. Experimental results are presented using radio pulse reflections inside coaxial cables, showing robust range resolution smaller than a tenth of the inverse bandedge, with uncertainties close to the derived Cram\'er-Rao bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09571v1</guid>
      <category>eess.SP</category>
      <category>physics.data-an</category>
      <category>physics.optics</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew N. Jordan, John C. Howell, Achim Kempf, Shunxing Zhang, Derek White</dc:creator>
    </item>
    <item>
      <title>Deep Neural Operator Enabled Digital Twin Modeling for Additive Manufacturing</title>
      <link>https://arxiv.org/abs/2405.09572</link>
      <description>arXiv:2405.09572v1 Announce Type: new 
Abstract: A digital twin (DT), with the components of a physics-based model, a data-driven model, and a machine learning (ML) enabled efficient surrogate, behaves as a virtual twin of the real-world physical process. In terms of Laser Powder Bed Fusion (L-PBF) based additive manufacturing (AM), a DT can predict the current and future states of the melt pool and the resulting defects corresponding to the input laser parameters, evolve itself by assimilating in-situ sensor data, and optimize the laser parameters to mitigate defect formation. In this paper, we present a deep neural operator enabled computational framework of the DT for closed-loop feedback control of the L-PBF process. This is accomplished by building a high-fidelity computational model to accurately represent the melt pool states, an efficient surrogate model to approximate the melt pool solution field, followed by an physics-based procedure to extract information from the computed melt pool simulation that can further be correlated to the defect quantities of interest (e.g., surface roughness). In particular, we leverage the data generated from the high-fidelity physics-based model and train a series of Fourier neural operator (FNO) based ML models to effectively learn the relation between the input laser parameters and the corresponding full temperature field of the melt pool. Subsequently, a set of physics-informed variables such as the melt pool dimensions and the peak temperature can be extracted to compute the resulting defects. An optimization algorithm is then exercised to control laser input and minimize defects. On the other hand, the constructed DT can also evolve with the physical twin via offline finetuning and online material calibration. Finally, a probabilistic framework is adopted for uncertainty quantification. The developed DT is envisioned to guide the AM process and facilitate high-quality manufacturing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09572v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ning Liu, Xuxiao Li, Manoj R. Rajanna, Edward W. Reutzel, Brady Sawyer, Prahalada Rao, Jim Lua, Nam Phan, Yue Yu</dc:creator>
    </item>
    <item>
      <title>JNEEG shield for Jetson Nano for real-time EEG signal processing with deep learning</title>
      <link>https://arxiv.org/abs/2405.09575</link>
      <description>arXiv:2405.09575v1 Announce Type: new 
Abstract: The article presents an accessible route into the field of neuroscience through the JNEEG device. This device allows converting the Jetson Nano board into a brain-computer interface, making it easy to measure EEG, EMG, and ECG signals with 8 channels. With Jetson Nano is possible use deep learning for real-time signal processing and feature extraction from EEG in real-time without any data transmission. Over the past decade, the proliferation of artificial intelligence has significantly impacted various industries, including neurobiology. The integration of machine learning techniques has opened avenues for practical applications of EEG signals across technology sectors. This surge in interest has led to the widespread popularity of low-cost brain-computer interface devices capable of recording EEG signals using non-invasive electrodes. JNEEG device demonstrates satisfactory noise levels and accuracy for use in applied tasks with machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09575v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ildar Rakhmatulin</dc:creator>
    </item>
    <item>
      <title>Design and Implementation of mmWave Surface Wave Enabled Fluid Antennas and Experimental Results for Fluid Antenna Multiple Access</title>
      <link>https://arxiv.org/abs/2405.09663</link>
      <description>arXiv:2405.09663v1 Announce Type: new 
Abstract: While multiple-input multiple-output (MIMO) technologies continue to advance, concerns arise as to how MIMO can remain scalable if more users are to be accommodated with an increasing number of antennas at the base station (BS) in the upcoming sixth generation (6G). Recently, the concept of fluid antenna system (FAS) has emerged, which promotes position flexibility to enable transmitter channel state information (CSI) free spatial multiple access on one radio frequency (RF) chain. On the theoretical side, the fluid antenna multiple access (FAMA) approach offers a scalable alternative to massive MIMO spatial multiplexing. However, FAMA lacks experimental validation and the hardware implementation of FAS remains a mysterious approach. The aim of this paper is to provide a novel hardware design for FAS and evaluate the performance of FAMA using experimental data. Our FAS design is based on a dynamically reconfigurable "fluid" radiator which is capable of adjusting its position within a predefined space. One single-channel fluid antenna (SCFA) and one double-channel fluid antenna (DCFA) are designed, electromagnetically simulated, fabricated, and measured. The measured radiation patterns of prototypes are imported into channel and network models for evaluating their performance in FAMA. The experimental results demonstrate that in the 5G millimeter-wave (mmWave) bands (24-30 GHz), the FAS prototypes can vary their gain up to an averaged value of 11 dBi. In the case of 4-user FAMA, the double-channel FAS can significantly reduce outage probability by 57% and increases the multiplexing gain to 2.27 when compared to a static omnidirectional antenna.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09663v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanjun Shen, Boyi Tang, Shuai Gao, Kin-Fai Tong, Hang Wong, Kai-Kit Wong, Yangyang Zhang</dc:creator>
    </item>
    <item>
      <title>Distributed PMCW Radar Network in Presence of Phase Noise</title>
      <link>https://arxiv.org/abs/2405.09680</link>
      <description>arXiv:2405.09680v1 Announce Type: new 
Abstract: In Frequency Modulated Continuous Waveform (FMCW) radar systems, the phase noise from the Phase-Locked Loop (PLL) can increase the noise floor in the Range-Doppler map. The adverse effects of phase noise on close targets can be mitigated if the transmitter (Tx) and receiver (Rx) employ the same chirp, a phenomenon known as the range correlation effect.
  In the context of a multi-static radar network, sharing the chirp between distant radars becomes challenging. Each radar generates its own chirp, leading to uncorrelated phase noise. Consequently, the system performance cannot benefit from the range correlation effect.
  Previous studies show that selecting a suitable code sequence for a Phase Modulated Continuous Waveform (PMCW) radar can reduce the impact of uncorrelated phase noise in the range dimension. In this paper, we demonstrate how to leverage this property to exploit both the mono- and multi-static signals of each radar in the network without having to share any signal at the carrier frequency. The paper introduces a detailed signal model for PMCW radar networks, analyzing both correlated and uncorrelated phase noise effects in the Doppler dimension. Additionally, a solution for compensating uncorrelated phase noise in Doppler is presented and supported by numerical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09680v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jialun Kou, Marc Bauduin, Andr\'e Bourdoux, Sofie Pollin</dc:creator>
    </item>
    <item>
      <title>A Deep Joint Source-Channel Coding Scheme for Hybrid Mobile Multi-hop Networks</title>
      <link>https://arxiv.org/abs/2405.09698</link>
      <description>arXiv:2405.09698v1 Announce Type: new 
Abstract: Efficient data transmission across mobile multi-hop networks that connect edge devices to core servers presents significant challenges, particularly due to the variability in link qualities between wireless and wired segments. This variability necessitates a robust transmission scheme that transcends the limitations of existing deep joint source-channel coding (DeepJSCC) strategies, which often struggle at the intersection of analog and digital methods. Addressing this need, this paper introduces a novel hybrid DeepJSCC framework, h-DJSCC, tailored for effective image transmission from edge devices through a network architecture that includes initial wireless transmission followed by multiple wired hops. Our approach harnesses the strengths of DeepJSCC for the initial, variable-quality wireless link to avoid the cliff effect inherent in purely digital schemes. For the subsequent wired hops, which feature more stable and high-capacity connections, we implement digital compression and forwarding techniques to prevent noise accumulation. This dual-mode strategy is adaptable even in scenarios with limited knowledge of the image distribution, enhancing the framework's robustness and utility. Extensive numerical simulations demonstrate that our hybrid solution outperforms traditional fully digital approaches by effectively managing transitions between different network segments and optimizing for variable signal-to-noise ratios (SNRs). We also introduce a fully adaptive h-DJSCC architecture capable of adjusting to different network conditions and achieving diverse rate-distortion objectives, thereby reducing the memory requirements on network nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09698v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Chenghong Bian, Yulin Shao, Deniz G\"und\"uz</dc:creator>
    </item>
    <item>
      <title>Deep Learning-Enabled One-Bit DoA Estimation</title>
      <link>https://arxiv.org/abs/2405.09712</link>
      <description>arXiv:2405.09712v1 Announce Type: new 
Abstract: Unrolled deep neural networks have attracted significant attention for their success in various practical applications. In this paper, we explore an application of deep unrolling in the direction of arrival (DoA) estimation problem when coarse quantization is applied to the measurements. We present a compressed sensing formulation for DoA estimation from one-bit data in which estimating target DoAs requires recovering a sparse signal from a limited number of severely quantized linear measurements. In particular, we exploit covariance recovery from one-bit dither samples. To recover the covariance of transmitted signal, the learned iterative shrinkage and thresholding algorithm (LISTA) is employed fed by one-bit data. We demonstrate that the upper bound of estimation performance is governed by the recovery error of the transmitted signal covariance matrix. Through numerical experiments, we demonstrate the proposed LISTA-based algorithm's capability in estimating target locations. The code employed in this study is available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09712v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Farhang Yeganegi, Arian Eamaz, Tara Esmaeilbeig, Mojtaba Soltanalian</dc:creator>
    </item>
    <item>
      <title>Attention-aided Outdoor Localization in Commercial 5G NR Systems</title>
      <link>https://arxiv.org/abs/2405.09715</link>
      <description>arXiv:2405.09715v1 Announce Type: new 
Abstract: The integration of high-precision cellular localization and machine learning (ML) is considered a cornerstone technique in future cellular navigation systems, offering unparalleled accuracy and functionality. This study focuses on localization based on uplink channel measurements in a fifth-generation (5G) new radio (NR) system. An attention-aided ML-based single-snapshot localization pipeline is presented, which consists of several cascaded blocks, namely a signal processing block, an attention-aided block, and an uncertainty estimation block. Specifically, the signal processing block generates an impulse response beam matrix for all beams. The attention-aided block trains on the channel impulse responses using an attention-aided network, which captures the correlation between impulse responses for different beams. The uncertainty estimation block predicts the probability density function of the UE position, thereby also indicating the confidence level of the localization result. Two representative uncertainty estimation techniques, the negative log-likelihood and the regression-by-classification techniques, are applied and compared. Furthermore, for dynamic measurements with multiple snapshots available, we combine the proposed pipeline with a Kalman filter to enhance localization accuracy. To evaluate our approach, we extract channel impulse responses for different beams from a commercial base station. The outdoor measurement campaign covers Line-of-Sight (LoS), Non-Line-of-Sight (NLoS), and a mix of LoS and NLoS scenarios. The results show that sub-meter localization accuracy can be achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09715v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guoda Tian, Dino Pjani\'c, Xuesong Cai, Bo Bernhardsson, Fredrik Tufvesson</dc:creator>
    </item>
    <item>
      <title>Optimizing Curved EM Skins for Opportunistic Relaying in Vehicular Networks</title>
      <link>https://arxiv.org/abs/2405.09730</link>
      <description>arXiv:2405.09730v1 Announce Type: new 
Abstract: Electromagnetic skins (EMSs) are recognized for enhancing communication performance, spanning from coverage to capacity. While much of the scientific literature focuses on reconfigurable intelligent surfaces that dynamically adjust phase configurations over time, this study takes a different approach by considering low-cost static passive curved EMS (CEMS)s. These are pre-configured during manufacturing to conform to the shape of irregular surfaces, e.g., car doors, effectively transforming them into anomalous mirrors. This design allows vehicles to serve as opportunistic passive relays, mitigating blockage issues in vehicular networks. This paper delves into a novel design method for the phase profile of CEMS based on coarse a-priori distributions of incident and reflection angles onto the surface, influenced by vehicular traffic patterns. A penalty-based method is employed to optimize both the average spectral efficiency (SE) and average coverage probability, and it is compared against a lower-complexity and physically intuitive modular architecture, utilizing a codebook-based discrete optimization technique. Numerical results demonstrate that properly designed CEMS lead to a remarkable improvements in average SE and coverage probability, namely when the direct path is blocked.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09730v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Reza Aghazadeh Ayoubi, Silvia Mura, Dario Tagliaferri, Marouan Mizmizi, Umberto Spagnolini</dc:creator>
    </item>
    <item>
      <title>Time-Varying Graph Signal Recovery Using High-Order Smoothness and Adaptive Low-rankness</title>
      <link>https://arxiv.org/abs/2405.09752</link>
      <description>arXiv:2405.09752v1 Announce Type: new 
Abstract: Time-varying graph signal recovery has been widely used in many applications, including climate change, environmental hazard monitoring, and epidemic studies. It is crucial to choose appropriate regularizations to describe the characteristics of the underlying signals, such as the smoothness of the signal over the graph domain and the low-rank structure of the spatial-temporal signal modeled in a matrix form. As one of the most popular options, the graph Laplacian is commonly adopted in designing graph regularizations for reconstructing signals defined on a graph from partially observed data. In this work, we propose a time-varying graph signal recovery method based on the high-order Sobolev smoothness and an error-function weighted nuclear norm regularization to enforce the low-rankness. Two efficient algorithms based on the alternating direction method of multipliers and iterative reweighting are proposed, and convergence of one algorithm is shown in detail. We conduct various numerical experiments on synthetic and real-world data sets to demonstrate the proposed method's effectiveness compared to the state-of-the-art in graph signal recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09752v1</guid>
      <category>eess.SP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weihong Guo, Yifei Lou, Jing Qin, Ming Yan</dc:creator>
    </item>
    <item>
      <title>Beam Pattern Modulation Embedded Hybrid Transceiver Optimization for Integrated Sensing and Communication</title>
      <link>https://arxiv.org/abs/2405.09778</link>
      <description>arXiv:2405.09778v1 Announce Type: new 
Abstract: Integrated sensing and communication (ISAC) emerges as a promising technology for B5G/6G, particularly in the millimeter-wave (mmWave) band. However, the widely utilized hybrid architecture in mmWave systems compromises multiplexing gain due to the constraints of limited radio frequency chains. Moreover, additional sensing functionalities exacerbate the impairment of spectrum efficiency (SE). In this paper, we present an optimized beam pattern modulation-embedded ISAC (BPM-ISAC) transceiver design, which spares one RF chain for sensing and the others for communication. To compensate for the reduced SE, index modulation across communication beams is applied. We formulate an optimization problem aimed at minimizing the mean squared error (MSE) of the sensing beampattern, subject to a symbol MSE constraint. This problem is then solved by sequentially optimizing the analog and digital parts. Both the multi-aperture structure (MAS) and the multi-beam structure (MBS) are considered for the design of the analog part. We conduct theoretical analysis on the asymptotic pairwise error probability (APEP) and the Cram\'er-Rao bound (CRB) of direction of arrival (DoA) estimation. Numerical simulations validate the overall enhanced ISAC performance over existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09778v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boxun Liu, Shijian Gao, Zonghui Yang, Xiang Cheng, Liuqing Yang</dc:creator>
    </item>
    <item>
      <title>Rethinking Multi-User Semantic Communications with Deep Generative Models</title>
      <link>https://arxiv.org/abs/2405.09866</link>
      <description>arXiv:2405.09866v1 Announce Type: new 
Abstract: In recent years, novel communication strategies have emerged to face the challenges that the increased number of connected devices and the higher quality of transmitted information are posing. Among them, semantic communication obtained promising results especially when combined with state-of-the-art deep generative models, such as large language or diffusion models, able to regenerate content from extremely compressed semantic information. However, most of these approaches focus on single-user scenarios processing the received content at the receiver on top of conventional communication systems. In this paper, we propose to go beyond these methods by developing a novel generative semantic communication framework tailored for multi-user scenarios. This system assigns the channel to users knowing that the lost information can be filled in with a diffusion model at the receivers. Under this innovative perspective, OFDMA systems should not aim to transmit the largest part of information, but solely the bits necessary to the generative model to semantically regenerate the missing ones. The thorough experimental evaluation shows the capabilities of the novel diffusion model and the effectiveness of the proposed framework, leading towards a GenAI-based next generation of communications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09866v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleonora Grassucci, Jinho Choi, Jihong Park, Riccardo F. Gramaccioni, Giordano Cicchetti, Danilo Comminiello</dc:creator>
    </item>
    <item>
      <title>End-to-end Optimization of Optical Communication Systems based on Directly Modulated Lasers</title>
      <link>https://arxiv.org/abs/2405.09907</link>
      <description>arXiv:2405.09907v1 Announce Type: new 
Abstract: The use of directly modulated lasers (DMLs) is attractive in low-power, cost-constrained short-reach optical links. However, their limited modulation bandwidth can induce waveform distortion, undermining their data throughput. Traditional distortion mitigation techniques have relied mainly on the separate training of transmitter-side pre-distortion and receiver-side equalization. This approach overlooks the potential gains obtained by simultaneous optimization of transmitter (constellation and pulse shaping) and receiver (equalization and symbol demapping). Moreover, in the context of DML operation, the choice of laser-driving configuration parameters such as the bias current and peak-to-peak modulation current has a significant impact on system performance. We propose a novel end-to-end optimization approach for DML systems, incorporating the learning of bias and peak-to-peak modulation current to the optimization of constellation points, pulse shaping and equalization. The simulation of the DML dynamics is based on the use of the laser rate equations at symbol rates between 15 and 25 Gbaud. The resulting output sequences from the rate equations are used to build a differentiable data-driven model, simplifying the calculation of gradients needed for end-to-end optimization. The proposed end-to-end approach is compared to 3 additional benchmark approaches: the uncompensated system without equalization, a receiver-side finite impulse response equalization approach and an end-to-end approach with learnable pulse shape and nonlinear Volterra equalization but fixed bias and peak-to-peak modulation current. The numerical simulations on the four approaches show that the joint optimization of bias, peak-to-peak current, constellation points, pulse shaping and equalization outperforms all other approaches throughout the tested symbol rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09907v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergio Hernandez F., Christophe Peucheret, Francesco Da Ros, Darko Zibar</dc:creator>
    </item>
    <item>
      <title>Sparse Regression Codes for Non-Coherent SIMO channels</title>
      <link>https://arxiv.org/abs/2405.09915</link>
      <description>arXiv:2405.09915v1 Announce Type: new 
Abstract: We study the sparse regression codes over flat-fading channels with multiple receive antennas. We consider a practical scenario where the channel state information is not available at the transmitter and the receiver. In this setting, we study the maximum likelihood (ML) detector for SPARC, which has a prohibitively high search complexity. We propose a novel practical decoder, named maximum likelihood matching pursuit (MLMP), which incorporates a greedy search mechanism along with the ML metric. We also introduce a parallel search mechanism for MLMP. Comparing with the existing block-orthogonal matching pursuit based decoders, we show that MLMP has significant gains in the block error rate (BLER) performance. We also show that the proposed approach has significant gains over polar codes employing pilot-aided channel estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09915v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sai Dinesh Kancharana, Madhusudan Kumar Sinha, Arun Pachai Kannu</dc:creator>
    </item>
    <item>
      <title>A Unified Deep Transfer Learning Model for Accurate IoT Localization in Diverse Environments</title>
      <link>https://arxiv.org/abs/2405.09960</link>
      <description>arXiv:2405.09960v1 Announce Type: new 
Abstract: Internet of Things (IoT) is an ever-evolving technological paradigm that is reshaping industries and societies globally. Real-time data collection, analysis, and decision-making facilitated by localization solutions form the foundation for location-based services, enabling them to support critical functions within diverse IoT ecosystems. However, most existing works on localization focus on single environment, resulting in the development of multiple models to support multiple environments. In the context of smart cities, these raise costs and complexity due to the dynamicity of such environments. To address these challenges, this paper presents a unified indoor-outdoor localization solution that leverages transfer learning (TL) schemes to build a single deep learning model. The model accurately predicts the localization of IoT devices in diverse environments. The performance evaluation shows that by adopting an encoder-based TL scheme, we can improve the baseline model by about 17.18% in indoor environments and 9.79% in outdoor environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09960v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdullahi Isa Ahmed, Yaya Etiabi, Ali Waqar Azim, El Mehdi Amhoud</dc:creator>
    </item>
    <item>
      <title>Harmonic and Interharmonic Detection in Power Systems Based on Fractal-Optimized Variational Mode Decomposition</title>
      <link>https://arxiv.org/abs/2405.09979</link>
      <description>arXiv:2405.09979v1 Announce Type: new 
Abstract: The proposed method introduces a parameter determination approach based on the minimum Fractal box dimension (FBD) of Variational Mode Decomposition (VMD) components, aiming to address the issue of manual determination of VMD decomposition layers in advance. Initially, VMD is applied to the original power signal, and the layer number for VMD decomposition is determined by selecting the K value associated with the smallest fractal box dimension among its components. Subsequently, several Intrinsic Mode Functions (IMFs) are obtained as fundamental, harmonic, and interharmonic signals representing different aspects of the power system. Furthermore, Hilbert transform(HT) is employed to extract instantaneous amplitude and frequency information from these harmonic signals. Experimental evaluation using simulation data and real-world power system data demonstrates that compared to Empirical Mode Decomposition (EMD) and Ensemble Empirical Mode Decomposition (EEMD), our proposed method achieves more accurate identification and effective extraction of harmonic signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09979v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pei Yuhang, Yu Min, Yu Yan</dc:creator>
    </item>
    <item>
      <title>Semantic Communication via Rate Distortion Perception Bottleneck</title>
      <link>https://arxiv.org/abs/2405.09995</link>
      <description>arXiv:2405.09995v1 Announce Type: new 
Abstract: With the advancement of Artificial Intelligence (AI) technology, next-generation wireless communication network is facing unprecedented challenge. Semantic communication has become a novel solution to address such challenges, with enhancing the efficiency of bandwidth utilization by transmitting meaningful information and filtering out superfluous data. Unfortunately, recent studies have shown that classical Shannon information theory primarily focuses on the bit-level distortion, which cannot adequately address the perceptual quality issues of data reconstruction at the receiver end. In this work, we consider the impact of semantic-level distortion on semantic communication. We develop an image inference network based on the Information Bottleneck (IB) framework and concurrently establish an image reconstruction network. This network is designed to achieve joint optimization of perception and bit-level distortion, as well as image inference, associated with compressing information. To maintain consistency with the principles of IB for handling high-dimensional data, we employ variational approximation methods to simplify the optimization problem. Finally, we confirm the existence of the rate distortion perception tradeoff within IB framework through experimental analysis conducted on the MNIST dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09995v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihe Zhao, Chunyue Wang</dc:creator>
    </item>
    <item>
      <title>On Detecting Low-pass Graph Signals under Partial Observations</title>
      <link>https://arxiv.org/abs/2405.10001</link>
      <description>arXiv:2405.10001v1 Announce Type: new 
Abstract: The application of graph signal processing (GSP) on partially observed graph signals with missing nodes has gained attention recently. This is because processing data from large graphs are difficult, if not impossible due to the lack of availability of full observations. Many prior works have been developed using the assumption that the generated graph signals are smooth or low pass filtered. This paper treats a blind graph filter detection problem under this context. We propose a detector that certifies whether the partially observed graph signals are low pass filtered, without requiring the graph topology knowledge. As an example application, our detector leads to a pre-screening method to filter out non low pass signals and thus robustify the prior GSP algorithms. We also bound the sample complexity of our detector in terms of the class of filters, number of observed nodes, etc. Numerical experiments verify the efficacy of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10001v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hoang-Son Nguyen, Hoi-To Wai</dc:creator>
    </item>
    <item>
      <title>Continuous Transfer Learning for UAV Communication-aware Trajectory Design</title>
      <link>https://arxiv.org/abs/2405.10087</link>
      <description>arXiv:2405.10087v1 Announce Type: new 
Abstract: Deep Reinforcement Learning (DRL) emerges as a prime solution for Unmanned Aerial Vehicle (UAV) trajectory planning, offering proficiency in navigating high-dimensional spaces, adaptability to dynamic environments, and making sequential decisions based on real-time feedback. Despite these advantages, the use of DRL for UAV trajectory planning requires significant retraining when the UAV is confronted with a new environment, resulting in wasted resources and time. Therefore, it is essential to develop techniques that can reduce the overhead of retraining DRL models, enabling them to adapt to constantly changing environments. This paper presents a novel method to reduce the need for extensive retraining using a double deep Q network (DDQN) model as a pretrained base, which is subsequently adapted to different urban environments through Continuous Transfer Learning (CTL). Our method involves transferring the learned model weights and adapting the learning parameters, including the learning and exploration rates, to suit each new environment specific characteristics. The effectiveness of our approach is validated in three scenarios, each with different levels of similarity. CTL significantly improves learning speed and success rates compared to DDQN models initiated from scratch. For similar environments, Transfer Learning (TL) improved stability, accelerated convergence by 65%, and facilitated 35% faster adaptation in dissimilar settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10087v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenrui Sun, Gianluca Fontanesi, Swarna Bindu Chetty, Xuanyu Liang, Berk Canberk, Hamed Ahmadi</dc:creator>
    </item>
    <item>
      <title>End-to-End Optimization of Directly Modulated Laser Links using Chirp-Aware Modeling</title>
      <link>https://arxiv.org/abs/2405.10257</link>
      <description>arXiv:2405.10257v1 Announce Type: new 
Abstract: The rate and reach of directly-modulated laser links is often limited by the interplay between chirp and fiber chromatic dispersion. We address this by optimizing the transmitter, receiver, bias and peak-to-peak current to the laser jointly. Our approach outperforms Volterra post-equalization at various symbol rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10257v1</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergio Hernandez F., Christophe Peucheret, Francesco Da Ros, Darko Zibar</dc:creator>
    </item>
    <item>
      <title>Training Deep Learning Models with Hybrid Datasets for Robust Automatic Target Detection on real SAR images</title>
      <link>https://arxiv.org/abs/2405.09588</link>
      <description>arXiv:2405.09588v1 Announce Type: cross 
Abstract: In this work, we propose to tackle several challenges hindering the development of Automatic Target Detection (ATD) algorithms for ground targets in SAR images. To address the lack of representative training data, we propose a Deep Learning approach to train ATD models with synthetic target signatures produced with the MOCEM simulator. We define an incrustation pipeline to incorporate synthetic targets into real backgrounds. Using this hybrid dataset, we train ATD models specifically tailored to bridge the domain gap between synthetic and real data. Our approach notably relies on massive physics-based data augmentation techniques and Adversarial Training of two deep-learning detection architectures. We then test these models on several datasets, including (1) patchworks of real SAR images, (2) images with the incrustation of real targets in real backgrounds, and (3) images with the incrustation of synthetic background objects in real backgrounds. Results show that the produced hybrid datasets are exempt from image overlay bias. Our approach can reach up to 90% of Average Precision on real data while exclusively using synthetic targets for training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09588v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Camus (DGA.MI), Th\'eo Voillemin (DGA.MI), Corentin Le Barbu (DGA.MI), Jean-Christophe Louvign\'e (DGA.MI), Carole Belloni (DGA.MI), Emmanuel Vall\'ee (DGA.MI)</dc:creator>
    </item>
    <item>
      <title>Energy Consumption of Plant Factory with Artificial Light: Challenges and Opportunities</title>
      <link>https://arxiv.org/abs/2405.09643</link>
      <description>arXiv:2405.09643v1 Announce Type: cross 
Abstract: Plant factory with artificial light (PFAL) is a promising technology for relieving the food crisis, especially in urban areas or arid regions endowed with abundant resources. However, lighting and HVAC (heating, ventilation, and air conditioning) systems of PFAL have led to much greater energy consumption than open-field and greenhouse farming, limiting the application of PFAL to a wider extent. Recent researches pay much more attention to the optimization of energy consumption in order to develop and promote the PFAL technology with reduced energy usage. This work comprehensively summarizes the current energy-saving methods on lighting, HVAC systems, as well as their coupling methods for a more energy-efficient PFAL. Besides, we offer our perspectives on further energy-saving strategies and exploit the renewable energy resources for PFAL to respond to the urgent need for energy-efficient production.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09643v1</guid>
      <category>physics.soc-ph</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenyi Cai, Kunlang Bu, Lingyan Zha, Jingjin Zhang, Dayi Lai, Hua Bao</dc:creator>
    </item>
    <item>
      <title>Stacked Intelligent Metasurfaces for Holographic MIMO Aided Cell-Free Networks</title>
      <link>https://arxiv.org/abs/2405.09753</link>
      <description>arXiv:2405.09753v1 Announce Type: cross 
Abstract: Large-scale multiple-input and multiple-output (MIMO) systems are capable of achieving high date rate. However, given the high hardware cost and excessive power consumption of massive MIMO systems, as a remedy, intelligent metasurfaces have been designed for efficient holographic MIMO (HMIMO) systems. In this paper, we propose a HMIMO architecture based on stacked intelligent metasurfaces (SIM) for the uplink of cell-free systems, where the SIM is employed at the access points (APs) for improving the spectral- and energy-efficiency. Specifically, we conceive distributed beamforming for SIM-assisted cell-free networks, where both the SIM coefficients and the local receiver combiner vectors of each AP are optimized based on the local channel state information (CSI) for the local detection of each user equipment (UE) information. Afterward, the central processing unit (CPU) fuses the local detections gleaned from all APs to detect the aggregate multi-user signal. Specifically, to design the SIM coefficients and the combining vectors of the APs, a low-complexity layer-by-layer iterative optimization algorithm is proposed for maximizing the equivalent gain of the channel spanning from the UEs to the APs. At the CPU, the weight vector used for combining the local detections from all APs is designed based on the minimum mean square error (MMSE) criterion, where the hardware impairments (HWIs) are also taken into consideration based on their statistics. The simulation results show that the SIM-based HMIMO outperforms the conventional single-layer HMIMO in terms of the achievable rate. We demonstrate that both the HWI of the radio frequency (RF) chains at the APs and the UEs limit the achievable rate in the high signal-to-noise-ratio (SNR) region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09753v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingchao Li, Mohammed El-Hajjar, Chao Xu, Jiancheng An, Chau Yuen, Lajos Hanzo</dc:creator>
    </item>
    <item>
      <title>Cell-Free Terahertz Massive MIMO: A Novel Paradigm Beyond Ultra-Massive MIMO</title>
      <link>https://arxiv.org/abs/2405.09905</link>
      <description>arXiv:2405.09905v1 Announce Type: cross 
Abstract: Terahertz (THz) frequencies have recently garnered considerable attention due to their potential to offer abundant spectral resources for communication, as well as distinct advantages in sensing, positioning, and imaging. Nevertheless, practical implementation encounters challenges stemming from the limited distances of signal transmission, primarily due to notable propagation, absorption, and blockage losses. To address this issue, the current strategy involves employing ultra-massive multi-input multi-output (UMMIMO) to generate high beamforming gains, thereby extending the transmission range. This paper introduces an alternative solution through the utilization of cell-free massive MIMO (CFmMIMO) architecture, wherein the closest access point is actively chosen to reduce the distance, rather than relying solely on a substantial number of antennas. We compare these two techniques through simulations and the numerical results justify that CFmMIMO is superior to UMMIMO in both spectral and energy efficiency at THz frequencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09905v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wei Jiang, Hans D. Schotten</dc:creator>
    </item>
    <item>
      <title>Scaling convolutional neural networks achieves expert-level seizure detection in neonatal EEG</title>
      <link>https://arxiv.org/abs/2405.09911</link>
      <description>arXiv:2405.09911v1 Announce Type: cross 
Abstract: Background: Neonatal seizures are a neurological emergency that require urgent treatment. They are hard to diagnose clinically and can go undetected if EEG monitoring is unavailable. EEG interpretation requires specialised expertise which is not widely available. Algorithms to detect EEG seizures can address this limitation but have yet to reach widespread clinical adoption.
  Methods: Retrospective EEG data from 332 neonates was used to develop and validate a seizure-detection model. The model was trained and tested with a development dataset ($n=202$) that was annotated with over 12k seizure events on a per-channel basis. This dataset was used to develop a convolutional neural network (CNN) using a modern architecture and training methods. The final model was then validated on two independent multi-reviewer datasets ($n=51$ and $n=79$).
  Results: Increasing dataset and model size improved model performance: Matthews correlation coefficient (MCC) and Pearson's correlation ($r$) increased by up to 50% with data scaling and up to 15% with model scaling. Over 50k hours of annotated single-channel EEG was used for training a model with 21 million parameters. State-of-the-art was achieved on an open-access dataset (MCC=0.764, $r=0.824$, and AUC=0.982). The CNN attains expert-level performance on both held-out validation sets, with no significant difference in inter-rater agreement among the experts and among experts and algorithm ($\Delta \kappa &lt; -0.095$, $p&gt;0.05$).
  Conclusion: With orders of magnitude increases in data and model scale we have produced a new state-of-the-art model for neonatal seizure detection. Expert-level equivalence on completely unseen data, a first in this field, provides a strong indication that the model is ready for further clinical validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09911v1</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>physics.med-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Robert Hogan, Sean R. Mathieson, Aurel Luca, Soraia Ventura, Sean Griffin, Geraldine B. Boylan, John M. O'Toole</dc:creator>
    </item>
    <item>
      <title>Distributed Joint User Activity Detection, Channel Estimation, and Data Detection via Expectation Propagation in Cell-Free Massive MIMO</title>
      <link>https://arxiv.org/abs/2405.09914</link>
      <description>arXiv:2405.09914v1 Announce Type: cross 
Abstract: We consider the uplink of a grant-free cell-free massive multiple-input multiple-output (GF-CF-MaMIMO) system. We propose an algorithm for distributed joint activity detection, channel estimation, and data detection (JACD) based on expectation propagation (EP) called JACD-EP. We develop the algorithm by factorizing the a posteriori probability (APP) of activities, channels, and transmitted data, then, mapping functions and variables onto a factor graph, and finally, performing a message passing on the resulting factor graph. If users with the same pilot sequence are sufficiently distant from each other, the JACD-EP algorithm is able to mitigate the effects of pilot contamination which naturally occurs in grant-free systems due to the large number of potential users and limited signaling resources. Furthermore, it outperforms state-of-the-art algorithms for JACD in GF-CF-MaMIMO systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09914v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Forsch, Alexander Karataev, Laura Cottatellucci</dc:creator>
    </item>
    <item>
      <title>Unified Modeling and Performance Comparison for Cellular and Cell-Free Massive MIMO</title>
      <link>https://arxiv.org/abs/2405.09928</link>
      <description>arXiv:2405.09928v1 Announce Type: cross 
Abstract: Cell-free massive multi-input multi-output (MIMO) has recently gained a lot of attention due to its high potential in sixth-generation (6G) wireless systems. The goal of this paper is to first present a unified modeling for massive MIMO, encompassing both cellular and cell-free architectures with a variable number of antennas per access point. We derive signal transmission models and achievable spectral efficiency in both the downlink and uplink using zero-forcing and maximal-ratio schemes. We also provide performance comparisons in terms of per-user and sum spectral efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09928v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wei Jiang, Hans D. Schotten</dc:creator>
    </item>
    <item>
      <title>A Review of Multiple Access Techniques for Intelligent Reflecting Surface-Assisted Systems</title>
      <link>https://arxiv.org/abs/2405.09951</link>
      <description>arXiv:2405.09951v1 Announce Type: cross 
Abstract: Intelligent Reflecting Surface (IRS) is envisioned to be a technical enabler for the sixth-generation (6G) wireless system. Its potential lies in delivering high performance while maintaining both power efficiency and cost-effectiveness. Previous studies have primarily focused on point-to-point IRS communications involving a single user. Nevertheless, a practical system must serve multiple users simultaneously. The unique characteristics of IRS, such as non-frequency-selective reflection and the necessity for joint active/passive beamforming, create obstacles to the use of conventional multiple access (MA) techniques. This motivates us to review various MA techniques to make clear their functionalities in the presence of IRS. Through this paper, our aim is to provide researchers with a comprehensive understanding of challenges and available solutions, offering insights to foster their design of efficient multiple access for IRS-aided systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09951v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wei Jiang, Hans Schotten</dc:creator>
    </item>
    <item>
      <title>Language-Oriented Semantic Latent Representation for Image Transmission</title>
      <link>https://arxiv.org/abs/2405.09976</link>
      <description>arXiv:2405.09976v1 Announce Type: cross 
Abstract: In the new paradigm of semantic communication (SC), the focus is on delivering meanings behind bits by extracting semantic information from raw data. Recent advances in data-to-text models facilitate language-oriented SC, particularly for text-transformed image communication via image-to-text (I2T) encoding and text-to-image (T2I) decoding. However, although semantically aligned, the text is too coarse to precisely capture sophisticated visual features such as spatial locations, color, and texture, incurring a significant perceptual difference between intended and reconstructed images. To address this limitation, in this paper, we propose a novel language-oriented SC framework that communicates both text and a compressed image embedding and combines them using a latent diffusion model to reconstruct the intended image. Experimental results validate the potential of our approach, which transmits only 2.09\% of the original image size while achieving higher perceptual similarities in noisy communication channels compared to a baseline SC method that communicates only through text.The code is available at https://github.com/ispamm/Img2Img-SC/ .</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09976v1</guid>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giordano Cicchetti, Eleonora Grassucci, Jihong Park, Jinho Choi, Sergio Barbarossa, Danilo Comminiello</dc:creator>
    </item>
    <item>
      <title>Enhancing Energy Efficiency in O-RAN Through Intelligent xApps Deployment</title>
      <link>https://arxiv.org/abs/2405.10116</link>
      <description>arXiv:2405.10116v1 Announce Type: cross 
Abstract: The proliferation of 5G technology presents an unprecedented challenge in managing the energy consumption of densely deployed network infrastructures, particularly Base Stations (BSs), which account for the majority of power usage in mobile networks. The O-RAN architecture, with its emphasis on open and intelligent design, offers a promising framework to address the Energy Efficiency (EE) demands of modern telecommunication systems. This paper introduces two xApps designed for the O-RAN architecture to optimize power savings without compromising the Quality of Service (QoS). Utilizing a commercial RAN Intelligent Controller (RIC) simulator, we demonstrate the effectiveness of our proposed xApps through extensive simulations that reflect real-world operational conditions. Our results show a significant reduction in power consumption, achieving up to 50% power savings with a minimal number of User Equipments (UEs), by intelligently managing the operational state of Radio Cards (RCs), particularly through switching between active and sleep modes based on network resource block usage conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10116v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuanyu Liang, Ahmed Al-Tahmeesschi, Qiao Wang, Swarna Chetty, Chenrui Sun, Hamed Ahmadi</dc:creator>
    </item>
    <item>
      <title>Low-Rank Adaptation of Time Series Foundational Models for Out-of-Domain Modality Forecasting</title>
      <link>https://arxiv.org/abs/2405.10216</link>
      <description>arXiv:2405.10216v1 Announce Type: cross 
Abstract: Low-Rank Adaptation (LoRA) is a widely used technique for fine-tuning large pre-trained or foundational models across different modalities and tasks. However, its application to time series data, particularly within foundational models, remains underexplored. This paper examines the impact of LoRA on contemporary time series foundational models: Lag-Llama, MOIRAI, and Chronos. We demonstrate LoRA's fine-tuning potential for forecasting the vital signs of sepsis patients in intensive care units (ICUs), emphasizing the models' adaptability to previously unseen, out-of-domain modalities. Integrating LoRA aims to enhance forecasting performance while reducing inefficiencies associated with fine-tuning large models on limited domain-specific data. Our experiments show that LoRA fine-tuning of time series foundational models significantly improves forecasting, achieving results comparable to state-of-the-art models trained from scratch on similar modalities. We conduct comprehensive ablation studies to demonstrate the trade-offs between the number of tunable parameters and forecasting performance and assess the impact of varying LoRA matrix ranks on model performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10216v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Divij Gupta, Anubhav Bhatti, Suraj Parmar, Chen Dan, Yuwei Liu, Bingjie Shen, San Lee</dc:creator>
    </item>
    <item>
      <title>Towards Task-Compatible Compressible Representations</title>
      <link>https://arxiv.org/abs/2405.10244</link>
      <description>arXiv:2405.10244v1 Announce Type: cross 
Abstract: We identify an issue in multi-task learnable compression, in which a representation learned for one task does not positively contribute to the rate-distortion performance of a different task as much as expected, given the estimated amount of information available in it. We interpret this issue using the predictive $\mathcal{V}$-information framework. In learnable scalable coding, previous work increased the utilization of side-information for input reconstruction by also rewarding input reconstruction when learning this shared representation. We evaluate the impact of this idea in the context of input reconstruction more rigorously and extended it to other computer vision tasks. We perform experiments using representations trained for object detection on COCO 2017 and depth estimation on the Cityscapes dataset, and use them to assist in image reconstruction and semantic segmentation tasks. The results show considerable improvements in the rate-distortion performance of the assisted tasks. Moreover, using the proposed representations, the performance of the base tasks are also improved. Results suggest that the proposed method induces simpler representations that are more compatible with downstream processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10244v1</guid>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anderson de Andrade, Ivan Baji\'c</dc:creator>
    </item>
    <item>
      <title>UAV-assisted IoT Monitoring Network: Adaptive Multiuser Access for Low-Latency and High-Reliability Under Bursty Traffic</title>
      <link>https://arxiv.org/abs/2304.12684</link>
      <description>arXiv:2304.12684v2 Announce Type: replace 
Abstract: In this work, we propose an adaptive system design for an Internet of Things (IoT) monitoring network with latency and reliability requirements, where IoT devices generate time-critical and event-triggered bursty traffic, and an unmanned aerial vehicle (UAV) aggregates and relays sensed data to the base station. Existing transmission schemes based on the overall average traffic rates over-utilize network resources when traffic is smooth, and suffer from packet collisions when traffic is bursty which occurs in an event of interest. We address such problems by designing an adaptive transmission scheme employing multiuser shared access (MUSA) based grant-free non-orthogonal multiple access and use short packet communication for low latency of the IoT-to-UAV communication. Specifically, to accommodate bursty traffic, we design an analytical framework and formulate an optimization problem to maximize the performance by determining the optimal number of transmission time slots, subject to the stringent reliability and latency constraints. We compare the performance of the proposed scheme with a non-adaptive power-diversity based scheme with a fixed number of time slots. Our results show that the proposed scheme has superior reliability and stability in comparison to the state-of-the-art scheme at moderate to high average traffic rates, while satisfying the stringent latency requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.12684v2</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nilupuli Senadhira, Salman Durrani, Sheeraz A. Alvi, Nan Yang, Xiangyun Zhou</dc:creator>
    </item>
    <item>
      <title>Employing High-Dimensional RIS Information for RIS-aided Localization Systems</title>
      <link>https://arxiv.org/abs/2403.16521</link>
      <description>arXiv:2403.16521v2 Announce Type: replace 
Abstract: Reconfigurable intelligent surface (RIS)-aided localization systems have attracted extensive research attention due to their accuracy enhancement capabilities. However, most studies primarily utilized the base stations (BS) received signal, i.e., BS information, for localization algorithm design, neglecting the potential of RIS received signal, i.e., RIS information. Compared with BS information, RIS information offers higher dimension and richer feature set, thereby significantly improving the ability to extract positions of the mobile users (MUs). Addressing this oversight, this paper explores the algorithm design based on the high-dimensional RIS information. Specifically, we first propose a RIS information reconstruction (RIS-IR) algorithm to reconstruct the high-dimensional RIS information from the low-dimensional BS information. The proposed RIS-IR algorithm comprises a data processing module for preprocessing BS information, a convolution neural network (CNN) module for feature extraction, and an output module for outputting the reconstructed RIS information. Then, we propose a transfer learning based fingerprint (TFBF) algorithm that employs the reconstructed high-dimensional RIS information for MU localization. This involves adapting a pre-trained DenseNet-121 model to map the reconstructed RIS signal to the MU's three-dimensional (3D) position. Empirical results affirm that the localization performance is significantly influenced by the high-dimensional RIS information and maintains robustness against unoptimized phase shifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16521v2</guid>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuo Wu, Cunhua Pan, Kangda Zhi, Hong Ren, Maged Elkashlan, Jiangzhou Wang, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy</title>
      <link>https://arxiv.org/abs/2401.01268</link>
      <description>arXiv:2401.01268v2 Announce Type: replace-cross 
Abstract: In deep learning, classification tasks are formalized as optimization problems often solved via the minimization of the cross-entropy. However, recent advancements in the design of objective functions allow the usage of the $f$-divergence to generalize the formulation of the optimization problem for classification. We adopt a Bayesian perspective and formulate the classification task as a maximum a posteriori probability problem. We propose a class of objective functions based on the variational representation of the $f$-divergence. Furthermore, driven by the challenge of improving the state-of-the-art approach, we propose a bottom-up method that leads us to the formulation of an objective function corresponding to a novel $f$-divergence referred to as shifted log (SL). We theoretically analyze the objective functions proposed and numerically test them in three application scenarios: toy examples, image datasets, and signal detection/decoding problems. The analyzed scenarios demonstrate the effectiveness of the proposed approach and that the SL divergence achieves the highest classification accuracy in almost all the considered cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01268v2</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicola Novello, Andrea M. Tonello</dc:creator>
    </item>
    <item>
      <title>Wireless Information and Energy Transfer in the Era of 6G Communications</title>
      <link>https://arxiv.org/abs/2404.18705</link>
      <description>arXiv:2404.18705v2 Announce Type: replace-cross 
Abstract: Wireless information and energy transfer (WIET) represents an emerging paradigm which employs controllable transmission of radio-frequency signals for the dual purpose of data communication and wireless charging. As such, WIET is widely regarded as an enabler of envisioned 6G use cases that rely on energy-sustainable Internet-of-Things (IoT) networks, such as smart cities and smart grids. Meeting the quality-of-service demands of WIET, in terms of both data transfer and power delivery, requires effective co-design of the information and energy signals. In this article, we present the main principles and design aspects of WIET, focusing on its integration in 6G networks. First, we discuss how conventional communication notions such as resource allocation and waveform design need to be revisited in the context of WIET. Next, we consider various candidate 6G technologies that can boost WIET efficiency, namely, holographic multiple-input multiple-output, near-field beamforming, terahertz communication, intelligent reflecting surfaces (IRSs), and reconfigurable (fluid) antenna arrays. We introduce respective WIET design methods, analyze the promising performance gains of these WIET systems, and discuss challenges, open issues, and future research directions. Finally, a near-field energy beamforming scheme and a power-based IRS beamforming algorithm are experimentally validated using a wireless energy transfer testbed. The vision of WIET in communication systems has been gaining momentum in recent years, with constant progress with respect to theoretical but also practical aspects. The comprehensive overview of the state of the art of WIET presented in this paper highlights the potentials of WIET systems as well as their overall benefits in 6G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18705v2</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Constantinos Psomas, Konstantinos Ntougias, Nikita Shanin, Dongfang Xu, Kenneth MacSporran Mayer, Nguyen Minh Tran, Laura Cottatellucci, Kae Won Choi, Dong In Kim, Robert Schober, Ioannis Krikidis</dc:creator>
    </item>
    <item>
      <title>Beamforming Inferring by Conditional WGAN-GP for Holographic Antenna Arrays</title>
      <link>https://arxiv.org/abs/2405.00391</link>
      <description>arXiv:2405.00391v2 Announce Type: replace-cross 
Abstract: The beamforming technology with large holographic antenna arrays is one of the key enablers for the next generation of wireless systems, which can significantly improve the spectral efficiency. However, the deployment of large antenna arrays implies high algorithm complexity and resource overhead at both receiver and transmitter ends. To address this issue, advanced technologies such as artificial intelligence have been developed to reduce beamforming overhead. Intuitively, if we can implement the near-optimal beamforming only using a tiny subset of the all channel information, the overhead for channel estimation and beamforming would be reduced significantly compared with the traditional beamforming methods that usually need full channel information and the inversion of large dimensional matrix. In light of this idea, we propose a novel scheme that utilizes Wasserstein generative adversarial network with gradient penalty to infer the full beamforming matrices based on very little of channel information. Simulation results confirm that it can accomplish comparable performance with the weighted minimum mean-square error algorithm, while reducing the overhead by over 50%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00391v2</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fenghao Zhu, Xinquan Wang, Chongwen Huang, Ahmed Alhammadi, Hui Chen, Zhaoyang Zhang, Chau Yuen, M\'erouane Debbah</dc:creator>
    </item>
  </channel>
</rss>
