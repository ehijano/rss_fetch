<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Jul 2025 04:00:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Interpretable Gradient Descent for Kalman Gain</title>
      <link>https://arxiv.org/abs/2507.14354</link>
      <description>arXiv:2507.14354v1 Announce Type: new 
Abstract: We derive a decomposition for the gradient of the innovation loss with respect to the filter gain in a linear time-invariant system, decomposing as a product of an observability Gramian and a term quantifying the ``non-orthogonality" between the estimation error and the innovation. We leverage this decomposition to give a convergence proof of gradient descent to the optimal Kalman gain, specifically identifying how recovery of the Kalman gain depends on a non-standard observability condition, and obtaining an interpretable geometric convergence rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14354v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. A. Belabbas, A. Olshevsky</dc:creator>
    </item>
    <item>
      <title>On the Convergence and Complexity of Proximal Gradient and Accelerated Proximal Gradient Methods under Adaptive Gradient Estimation</title>
      <link>https://arxiv.org/abs/2507.14479</link>
      <description>arXiv:2507.14479v1 Announce Type: new 
Abstract: In this paper, we propose a proximal gradient method and an accelerated proximal gradient method for solving composite optimization problems, where the objective function is the sum of a smooth and a convex, possibly nonsmooth, function. We consider settings where the smooth component is either a finite-sum function or an expectation of a stochastic function, making it computationally expensive or impractical to evaluate its gradient. To address this, we utilize gradient estimates within the proximal gradient framework. Our methods dynamically adjust the accuracy of these estimates, increasing it as the iterates approach a solution, thereby enabling high-precision solutions with minimal computational cost. We analyze the methods when the smooth component is nonconvex, convex, or strongly convex, using a biased gradient estimate. In all cases, the methods achieve the optimal iteration complexity for first-order methods. When the gradient estimate is unbiased, we further refine the analysis to show that the methods simultaneously achieve optimal iteration complexity and optimal complexity in terms of the number of stochastic gradient evaluations. Finally, we validate our theoretical results through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14479v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raghu Bollapragada, Shagun Gupta</dc:creator>
    </item>
    <item>
      <title>Neural Event-Triggered Control with Optimal Scheduling</title>
      <link>https://arxiv.org/abs/2507.14653</link>
      <description>arXiv:2507.14653v1 Announce Type: new 
Abstract: Learning-enabled controllers with stability certificate functions have demonstrated impressive empirical performance in addressing control problems in recent years. Nevertheless, directly deploying the neural controllers onto actual digital platforms requires impractically excessive communication resources due to a continuously updating demand from the closed-loop feedback controller. We introduce a framework aimed at learning the event-triggered controller (ETC) with optimal scheduling, i.e., minimal triggering times, to address this challenge in resource-constrained scenarios. Our proposed framework, denoted by Neural ETC, includes two practical algorithms: the path integral algorithm based on directly simulating the event-triggered dynamics, and the Monte Carlo algorithm derived from new theoretical results regarding lower bound of inter-event time. Furthermore, we propose a projection operation with an analytical expression that ensures theoretical stability and schedule optimality for Neural ETC. Compared to the conventional neural controllers, our empirical results show that the Neural ETC significantly reduces the required communication resources while enhancing the control performance in constrained communication resources scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14653v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luan Yang, Jingdong Zhang, Qunxi Zhu, Wei Lin</dc:creator>
    </item>
    <item>
      <title>Electro-thermal topology optimization of an electric machine by the topological derivative considering drive cycles</title>
      <link>https://arxiv.org/abs/2507.14759</link>
      <description>arXiv:2507.14759v1 Announce Type: new 
Abstract: We consider a 2d permanent magnet synchronous machine operating in a sequence of static operating points coming from a drive cycle. We aim to find a rotor design which maximizes the efficiency defined as the quotient of input and output energy considering Joule losses in the stator and eddy current losses in the permanent magnets. A coupled electromagnetic-thermal analysis of the rotor considers the eddy current losses as heat source and adds a temperature constraint to avoid damage of the permanent magnets. Additionally we impose Von-Mises stress constraints to maintain the mechanical integrity of the design. To solve the resulting free form topology optimization problem we use a level set description of the design and the topological derivative as sensitivity information. We show the effect of these constraints at very high speeds which is a trend in recent machine development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14759v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nepomuk Krenn, Th\'eodore Cherri\`ere, Sebastian Sch\"ops, Peter Gangl</dc:creator>
    </item>
    <item>
      <title>Indefinite Linear-Quadratic Optimal Control Problems of Backward Stochastic Differential Equations with Partial Information</title>
      <link>https://arxiv.org/abs/2507.14992</link>
      <description>arXiv:2507.14992v1 Announce Type: new 
Abstract: This paper is concerned with a kind of linear-quadratic (LQ) optimal control problem of backward stochastic differential equation (BSDE) with partial information. The cost functional includes cross terms between the state and control, and the weighting matrices are allowed to be indefinite. Through variational methods and stochastic filtering techniques, we derive the necessary and sufficient conditions for the optimal control, where a Hamiltonian system plays a crucial role. Moreover, to construct the optimal control, we introduce a matrix-valued differential equation and a BSDE with filtering, and establish their solvability under the assumption that the cost functional is uniformly convex. Finally, we present explicit forms of the optimal control and value function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14992v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialong Li, Zhiyong Yu, Wanying Yue</dc:creator>
    </item>
    <item>
      <title>Initialization-driven neural generation and training for high-dimensional optimal control and first-order mean field games</title>
      <link>https://arxiv.org/abs/2507.15126</link>
      <description>arXiv:2507.15126v1 Announce Type: new 
Abstract: This paper first introduces a method to approximate the value function of high-dimensional optimal control by neural networks. Based on the established relationship between Pontryagin's maximum principle (PMP) and the value function of the optimal control problem, which is characterized as being the unique solution to an associated Hamilton-Jacobi-Bellman (HJB) equation, we propose an approach that begins by using neural networks to provide a first rough estimate of the value function, which serves as initialization for solving the two point boundary value problem in the PMP and, as a result, generates reliable data. To train the neural network we define a loss function that takes into account this dataset and also penalizes deviations from the HJB equation.
  In the second part, we address the computation of equilibria in first-order Mean Field Game (MFG) problems by integrating our method with the fictitious play algorithm. These equilibria are characterized by a coupled system of a first-order HJB equation and a continuity equation. To approximate the solution to the continuity equation, we introduce a second neural network that learns the flow map transporting the initial distribution of agents. This network is trained on data generated by solving the underlying ODEs for a batch of initial conditions sampled from the initial distribution of agents. By combining this flow approximation, the previously described method for approximating the value function, and the fictitious play algorithm, we obtain an effective method to tackle high-dimensional deterministic MFGs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15126v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mouhcine Assouli, Justina Gianatti, Badr Missaoui, Francisco J. Silva</dc:creator>
    </item>
    <item>
      <title>Sequential feedback optimization with application to wind farm control</title>
      <link>https://arxiv.org/abs/2507.15127</link>
      <description>arXiv:2507.15127v1 Announce Type: new 
Abstract: This paper develops a sequential-linearization feedback optimization framework for driving nonlinear dynamical systems to an
  optimal steady state. A fundamental challenge in feedback optimization is the requirement of accurate first-order information
  of the steady-state input-output mapping, which is computationally prohibitive for high-dimensional nonlinear systems and
  often leads to poor performance when approximated around a fixed operating point. To address this limitation, we propose a
  sequential algorithm that adaptively updates the linearization point during optimization, maintaining local accuracy throughout
  the trajectory. We prove convergence to a neighborhood of the optimal steady state with explicit error bounds. To reduce the
  computational burden of repeated linearization operations, we further develop a multi-timescale variant where linearization
  updates occur at a slower timescale than optimization iterations, achieving significant computational savings while preserving
  convergence guarantees. The effectiveness of the proposed framework is demonstrated via numerical simulations of a realistic
  wind farm control problem. The results validate both the theoretical convergence predictions and the expected computational
  advantages of our multi-timescale formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15127v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Huang, Sergio Grammatico</dc:creator>
    </item>
    <item>
      <title>Adaptive Parameter Optimization in Gaussian Processes: A Comprehensive Study of Uncertainty Quantification and Dimensional Scaling</title>
      <link>https://arxiv.org/abs/2507.15138</link>
      <description>arXiv:2507.15138v1 Announce Type: new 
Abstract: Gaussian Process (GP) models have also become extremely useful for optimization under uncertainty algorithms, especially where the objective functions are costly to compute. Yet, the more classical methods usually adopt strategies that, in certain circumstances, might be effective but not flexible to be applied to a wide range of problem terrains. This study aims to adapt parameter optimization in GP models and especially how uncertainty quantification can assist in the learning process. We investigate the effect of adaptive kappa parameters that govern the exploration-exploitation trade-off and the interplay between dimensionality, penalty on uncertainty, and noise levels to influence optimization results. Uncertainty quantification is built directly into our comprehensive theoretical framework and gives us new algorithms to dynamically tune exploration-exploitation trade-offs according to the uncertainty trend observed in nature. We rigorously empirically test various strategies, parametrizing our tests by dimensionality, noise, penalty terms, and evaluate the performance of any given strategery in a wide variety of test settings, and show conclusively that adaptive strategies always outperform fixed ones, but in difficult settings, where the dimensions are large and the noise is severe, the advantage is enormous. We build theoretical assurances of convergence under different settings as well as furnish a sensible direction on the application of adaptive GP-based optimization even in very complicated conditions. The results of our work will help in the development of more efficient and robust methods of optimization of realistic problems in which there are only a few functions available for evaluation, and when quantifying the uncertainty, there is a need to know more about the uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15138v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nishant Gadde</dc:creator>
    </item>
    <item>
      <title>Stabilization of jump-diffusion stochastic differential equations by hysteresis switching</title>
      <link>https://arxiv.org/abs/2507.15191</link>
      <description>arXiv:2507.15191v1 Announce Type: new 
Abstract: We address the stabilization of both classical and quantum systems modeled by jump-diffusion stochastic differential equations using a novel hysteresis switching strategy. Unlike traditional methods that depend on global Lyapunov functions or require each subsystem to stabilize the target state individually, our approach employs local Lyapunov-like conditions and state-dependent switching to achieve global asymptotic or exponential stability with finitely many switches almost surely. We rigorously establish the well-posedness of the resulting switched systems and derive sufficient conditions for stability. The framework is further extended to quantum feedback control systems governed by stochastic master equations with both diffusive and jump dynamics. Notably, our method relaxes restrictive invariance assumptions often necessary in prior work, enhancing practical applicability in experimental quantum settings. Additionally, the proposed strategy offers promising avenues for robust control under model uncertainties and perturbations, paving the way for future developments in both classical and quantum control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15191v1</guid>
      <category>math.OC</category>
      <category>quant-ph</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weichao Liang, Gaoyue Guo</dc:creator>
    </item>
    <item>
      <title>Asymptotic Optimality in Data-driven Decision Making</title>
      <link>https://arxiv.org/abs/2507.15215</link>
      <description>arXiv:2507.15215v1 Announce Type: new 
Abstract: Given data generated by an abstract stochastic process, we study how to construct statistically optimal decisions for general stochastic optimization problems. Our setting encompasses non-standard data structures, including data originating from heterogeneous sources or from processes with randomly evolving data-generating mechanisms. We propose a decision-making approach that identifies optimal decisions for which a specific notion of shifted regret risk, evaluated with respect to the underlying stochastic process, decays to zero at a prescribed exponential rate under a minimal model perturbation. This optimal decision arises as the solution to a multi-objective optimization problem, where the trade-offs are governed by the statistical properties of the data-generating process. Central to our framework is a rate function, generalizing the classical concept from large deviation theory. Our general formulation recovers classical results in decision-making under uncertainty, such as those from distributionally robust optimization, as special cases. However, it goes beyond them: it enables decision-makers to systematically balance a desired rate of asymptotic risk decay against a potential loss in statistical consistency of the resulting data-driven decision. We demonstrate the effectiveness of the proposed approach through two illustrative examples from operations research: the classical newsvendor problem and a portfolio optimization problem under aleatoric uncertainty induced by heterogeneous data sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15215v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radek Sala\v{c}, Michael Kupper, Tobias Sutter</dc:creator>
    </item>
    <item>
      <title>An Optimization-Based Framework for Solving Forward-Backward Stochastic Differential Equations: Convergence Analysis and Error Bounds</title>
      <link>https://arxiv.org/abs/2507.15234</link>
      <description>arXiv:2507.15234v1 Announce Type: new 
Abstract: In this paper, we develop an optimization-based framework for solving coupled forward-backward stochastic differential equations. We introduce an integral-form objective function and prove its equivalence to the error between consecutive Picard iterates. Our convergence analysis establishes that minimizing this objective generates sequences that converge to the true solution. We provide explicit upper and lower bounds that relate the objective value to the error between trial and exact solutions. We validate our approach using two analytical test cases and demonstrate its effectiveness by achieving numerical convergence in a nonlinear stochastic optimal control problem with up to 1000 dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15234v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutian Wang, Yuan-Hua Ni, Xun Li</dc:creator>
    </item>
    <item>
      <title>On exploration of an interior mirror descent flow for stochastic nonconvex constrained problem</title>
      <link>https://arxiv.org/abs/2507.15264</link>
      <description>arXiv:2507.15264v1 Announce Type: new 
Abstract: We study a nonsmooth nonconvex optimization problem defined over nonconvex constraints, where the feasible set is given by the intersection of the closure of an open set and a smooth manifold. By endowing the open set with a Riemannian metric induced by a barrier function, we obtain a Riemannian subgradient flow formulated as a differential inclusion, which remains strictly within the interior of the feasible set. This continuous dynamical system unifies two classes of iterative optimization methods, namely the Hessian barrier method and mirror descent scheme, by revealing that these methods can be interpreted as discrete approximations of the continuous flow. We explore the long-term behavior of the trajectories generated by this dynamical system and show that the existing deficient convergence properties of the Hessian barrier and mirror descent scheme can be unifily and more insightfully interpreted through these of the continuous trajectory. For instance, the notorious spurious stationary points \cite{chen2024spurious} observed in Hessian barrier method and mirror descent scheme are interpreted as stable equilibria of the dynamical system that do not correspond to real stationary points of the original optimization problem. We provide two sufficient condition such that these spurious stationary points can be avoided if the strict complementarity conditions holds. In the absence of these regularity condition, we propose a random perturbation strategy that ensures the trajectory converges (subsequentially) to an approximate stationary point. Building on these insights, we introduce two iterative Riemannian subgradient methods, form of interior point methods, that generalizes the existing Hessian barrier method and mirror descent scheme for solving nonsmooth nonconvex optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15264v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kuangyu Ding, Kim-Chuan Toh</dc:creator>
    </item>
    <item>
      <title>Event-Triggered Resilient Consensus of Networked Euler-Lagrange Systems Under Byzantine Attacks</title>
      <link>https://arxiv.org/abs/2507.15283</link>
      <description>arXiv:2507.15283v1 Announce Type: new 
Abstract: The resilient consensus problem is investigated in this paper for a class of networked Euler-Lagrange systems with event-triggered communication in the presence of Byzantine attacks. One challenge that we face in addressing the considered problem is the inapplicability of existing resilient decision algorithms designed for one-dimensional multi-agent systems. This is because the networked Euler-Lagrange systems fall into the category of multi-dimensional multi-agent systems with coupling among state vector components. To address this problem, we propose a new resilient decision algorithm. This algorithm constructs auxiliary variables related to the coordinative objectives for each normal agent, and transforms the considered resilient consensus problem into the consensus problem of the designed auxiliary variables. Furthermore, to relax the constraints imposed on Byzantine agent behavior patterns within continuous-time scenarios, the event-triggered communication scheme is adopted. Finally, the effectiveness of the proposed algorithm is demonstrated through case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15283v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TAC.2025.3582531</arxiv:DOI>
      <dc:creator>Yuliang Fu, Guanghui Wen, Dan Zhao, Wei Xing Zheng, Xiaolei Li</dc:creator>
    </item>
    <item>
      <title>Learning in Memristive Neural Networks</title>
      <link>https://arxiv.org/abs/2507.15324</link>
      <description>arXiv:2507.15324v1 Announce Type: new 
Abstract: Memristors are nonlinear two-terminal circuit elements whose resistance at a given time depends on past electrical stimuli. Recently, networks of memristors have received attention in neuromorphic computing since they can be used to implement Artificial Neural Networks (ANNs) in hardware. For this, one can use a class of memristive circuits called crossbar arrays. In this paper, we describe a circuit implementation of an ANN and resolve three questions concerning such an implementation. In particular, we show (1) how to evaluate the implementation at an input, (2) how the resistance values of the memristors at a given time can be determined from external (current) measurements, and (3) how the resistances can be steered to desired values by applying suitable external voltages to the network. The results will be applied to two examples: an academic example to show proof of concept and an ANN that was trained using the MNIST dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15324v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H. M. Heidema, H. J. van Waarde, B. Besselink</dc:creator>
    </item>
    <item>
      <title>Multi-beam Beamforming in RIS-aided MIMO Subject to Reradiation Mask Constraints -- Optimization and Machine Learning Design</title>
      <link>https://arxiv.org/abs/2507.15367</link>
      <description>arXiv:2507.15367v1 Announce Type: new 
Abstract: Reconfigurable intelligent surfaces (RISs) are an emerging technology for improving spectral efficiency and reducing power consumption in future wireless systems. This paper investigates the joint design of the transmit precoding matrices and the RIS phase shift vector in a multi-user RIS-aided multiple-input multiple-output (MIMO) communication system. We formulate a max-min optimization problem to maximize the minimum achievable rate while considering transmit power and reradiation mask constraints. The achievable rate is simplified using the Arimoto-Blahut algorithm, and the problem is broken into quadratic programs with quadratic constraints (QPQC) sub-problems using an alternating optimization approach. To improve efficiency, we develop a model-based neural network optimization that utilizes the one-hot encoding for the angles of incidence and reflection. We address practical RIS limitations by using a greedy search algorithm to solve the optimization problem for discrete phase shifts. Simulation results demonstrate that the proposed methods effectively shape the multi-beam radiation pattern towards desired directions while satisfying reradiation mask constraints. The neural network design reduces the execution time, and the discrete phase shift scheme performs well with a small reduction of the beamforming gain by using only four phase shift levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15367v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shumin Wang, Hajar El Hassani, Marco Di Renzo, Marios Poulakis</dc:creator>
    </item>
    <item>
      <title>Information Preserving Line Search via Bayesian Optimization</title>
      <link>https://arxiv.org/abs/2507.15485</link>
      <description>arXiv:2507.15485v1 Announce Type: new 
Abstract: Line search is a fundamental part of iterative optimization methods for unconstrained and bound-constrained optimization problems to determine suitable step lengths that provide sufficient improvement in each iteration. Traditional line search methods are based on iterative interval refinement, where valuable information about function value and gradient is discarded in each iteration. We propose a line search method via Bayesian optimization, preserving and utilizing otherwise discarded information to improve step-length choices. Our approach is guaranteed to converge and shows superior performance compared to state-of-the-art methods based on empirical tests on the challenging unconstrained and bound-constrained optimization problems from the CUTEst test set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15485v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robin Labryga, Tomislav Prusina, S\"oren Laue</dc:creator>
    </item>
    <item>
      <title>Constrained Control Allocation With Continuous-Time Rate Constraints: Three-Dimensional Case</title>
      <link>https://arxiv.org/abs/2507.15489</link>
      <description>arXiv:2507.15489v1 Announce Type: new 
Abstract: This paper presents a novel quadratic programming (QP) approach for constrained control allocation that directly incorporates continuous-time actuator rate constraints without requiring slack variables. Over-actuated aircraft configurations, particularly prevalent in eVTOL and military applications, require control allocation algorithms to distribute commanded control moments among available actuators while respecting position and rate constraints. Existing methods such as direct allocation, pseudo-inverse, cascaded generalized inverse, and exact redistributed pseudo-inverse either cannot handle rate constraints in continuous time or require discretization approaches that compromise performance. Current QP methods that incorporate rate constraints rely on slack variables to ensure feasibility, which prevents full utilization of the attainable moment set and degrades allocation performance. The proposed methodology addresses this limitation by calculating the attainable moment set from both position and rate constraints through convex hull operations, then ensuring feasibility by scaling unattainable commanded moments to the boundary of the attainable moment set while preserving their direction. This approach guarantees the feasibility of the optimization problem without slack variables. The method is validated through simulation on an F-18 fighter aircraft control allocation problem, demonstrating equivalent performance to the established exact redistributed pseudo-inverse method while providing smoother actuator behavior and enhanced constraint satisfaction. Results show that incorporating continuous-time rate constraints leads to improved actuator tracking, reduced overshoot, and more precise adherence to position limits, which is essential for aircraft safety, ride comfort, and actuator longevity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15489v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>S\"uleyman \"Ozkurt, Adrian Grimm, Walter Fichter</dc:creator>
    </item>
    <item>
      <title>Any-Dimensional Polynomial Optimization via de Finetti Theorems</title>
      <link>https://arxiv.org/abs/2507.15632</link>
      <description>arXiv:2507.15632v1 Announce Type: new 
Abstract: Polynomial optimization problems often arise in sequences indexed by dimension, and it is of interest to produce bounds on the optimal values of all problems in the sequence. Examples include certifying inequalities between symmetric functions or graph homomorphism densities that hold over vectors and graphs of all sizes, and computing the value of mean-field games viewed as limits of games with a growing number of players. In this paper, we study such any-dimensional polynomial problems using the theory of representation stability, and we develop a systematic framework to produce hierarchies of bounds for their limiting optimal values in terms of finite-dimensional polynomial optimization problems. Our bounds converge at explicit rates, and they follow as a consequence of new de Finetti-type theorems pertaining to sequences of random arrays projecting onto each other in different ways. The proofs of these theorems are based on applying results from probability to representations of certain categories. We apply our framework to produce new bounds on problems arising in a number of application domains such as mean-field games, extremal graph theory, and symmetric function theory, and we illustrate our methods via numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15632v1</guid>
      <category>math.OC</category>
      <category>math.RT</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eitan Levin, Venkat Chandrasekaran</dc:creator>
    </item>
    <item>
      <title>Geometric design of the tangent term in landing algorithms for orthogonality constraints</title>
      <link>https://arxiv.org/abs/2507.15638</link>
      <description>arXiv:2507.15638v1 Announce Type: new 
Abstract: We propose a family a metrics over the set of full-rank $n\times p$ real matrices, and apply them to the landing framework for optimization under orthogonality constraints. The family of metrics we propose is a natural extension of the $\beta$-metric, defined on the Stiefel manifold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15638v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florentin Goyens, P. -A. Absil, Florian Feppon</dc:creator>
    </item>
    <item>
      <title>Sensor network localization has a benign landscape after low-dimensional relaxation</title>
      <link>https://arxiv.org/abs/2507.15662</link>
      <description>arXiv:2507.15662v1 Announce Type: new 
Abstract: We consider the sensor network localization problem, also known as multidimensional scaling or Euclidean distance matrix completion. Given a ground truth configuration of $n$ points in $\mathbb{R}^\ell$, we observe a subset of the pairwise distances and aim to recover the underlying configuration (up to rigid transformations). We show with a simple counterexample that the associated optimization problem is nonconvex and may admit spurious local minimizers, even when all distances are known. Yet, inspired by numerical experiments, we argue that all second-order critical points become global minimizers when the problem is relaxed by optimizing over configurations in dimension $k &gt; \ell$. Specifically, we show this for two settings, both when all pairwise distances are known: (1) for arbitrary ground truth points, and $k= O(\sqrt{\ell n})$, and: (2) for isotropic random ground truth points, and $k = O(\ell + \log n)$. To prove these results, we identify and exploit key properties of the linear map which sends inner products to squared distances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15662v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Criscitiello, Andrew D. McRae, Quentin Rebjock, Nicolas Boumal</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of Reshaped Wirtinger Flow with Random Initialization for Phase Retrieval</title>
      <link>https://arxiv.org/abs/2507.15684</link>
      <description>arXiv:2507.15684v1 Announce Type: new 
Abstract: This paper investigates phase retrieval using the Reshaped Wirtinger Flow (RWF) algorithm, focusing on recovering target vector $\vx \in \R^n$ from magnitude measurements \(y_i = \left| \langle \va_i, \vx \rangle \right|, \; i = 1, \ldots, m,\) under random initialization, where $\va_i \in \R^n$ are measurement vectors. For Gaussian measurement designs, we prove that when $m\ge O(n \log^2 n\log^3 m)$, the RWF algorithm with random initialization achieves $\epsilon$-accuracy within \(O\big(\log n + \log(1/\epsilon)\big)\) iterations, thereby attaining nearly optimal sample and computational complexities comparable to those previously established for spectrally initialized methods. Numerical experiments demonstrate that the convergence rate is robust to initialization randomness and remains stable even with larger step sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15684v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linbin Li, Haiyang Peng, Yong Xia, Meng Huang</dc:creator>
    </item>
    <item>
      <title>Extremum Seeking (ES) is Practically Stable Whenever Model-Based ES is Stable</title>
      <link>https://arxiv.org/abs/2507.15713</link>
      <description>arXiv:2507.15713v1 Announce Type: new 
Abstract: Extremum seeking control (ESC) are optimization algorithms in continuous time, with model-based ESCs using true derivative information of the cost function and model-free ESCs utilizing perturbation-based estimates instead. Stability analysis of model-free ESCs often employs the associated average system, whose stability is dependent on the selection of the dither signal. We demonstrate first the challenge of this analysis approach by showing selections of relative dither amplitudes and rates at different ESC inputs which result in the average system always having an unstable equilibrium. Then we go on to show that, if the model-based ESC is globally asymptotically stable (GAS), then the average system is semiglobally practically asymptotically stable (sGPAS), and the model-free ESC is semiglobally practically uniformly asymptotically stable (sGPUAS). Thus, we free the system analyst from analyzing the stability of the average system with various dither signals, as it is sufficient to analyze the stability of the model-based ESC. The result for the original model-free ESC also provides a guideline for the user for how to select the dither amplitudes to ensure sGPUAS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15713v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick McNamee, Zahra Nili Ahmadabadi, Mirslav Krsti\'c</dc:creator>
    </item>
    <item>
      <title>Approximating Rockafellians Mitigate Distributional Perturbations: Discontinuous Integrands and Chance-Constrained Applications</title>
      <link>https://arxiv.org/abs/2507.15801</link>
      <description>arXiv:2507.15801v1 Announce Type: new 
Abstract: In this paper, we show how approximating Rockafellians serve as a principled and effective alternative for improving the stability of stochastic programs under distributional changes. Unlike previous efforts that focus on special distributions and continuous integrands, our results accommodate general probability distributions and discontinuous integrands. Thus, our results apply to chance-constrained programs, for which we obtain improved qualitative and quantitative stability results under weaker assumptions pertaining to metric subregularity and upper outer-Minkowski content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15801v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lai Tian, Johannes O. Royset</dc:creator>
    </item>
    <item>
      <title>Identifying Solution Constraints for ODE Systems</title>
      <link>https://arxiv.org/abs/2507.15805</link>
      <description>arXiv:2507.15805v1 Announce Type: new 
Abstract: This work develops a framework to discover relations between the components of the solution to a given initial-value problem for a first-order system of ordinary differential equations. This is done by using sparse identification techniques on the data represented by the numerical solution of the initial-value problem at hand. The only assumption is that there are only a few terms that connects the components, so that the mathematical relations to be discovered are sparse in the set of possible functions. We illustrate the method through examples of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15805v1</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolae Tarfulea</dc:creator>
    </item>
    <item>
      <title>Power-Constrained Policy Gradient Methods for LQR</title>
      <link>https://arxiv.org/abs/2507.15806</link>
      <description>arXiv:2507.15806v1 Announce Type: new 
Abstract: Consider a discrete-time Linear Quadratic Regulator (LQR) problem solved using policy gradient descent when the system matrices are unknown. The gradient is transmitted across a noisy channel over a finite time horizon using analog communication by a transmitter with an average power constraint. This is a simple setup at the intersection of reinforcement learning and networked control systems. We first consider a communication-constrained optimization framework, where gradient descent is applied to optimize a non-convex function under noisy gradient transmission. We provide an optimal power allocation algorithm that minimizes an upper bound on the expected optimality error at the final iteration and show that adaptive power allocation can lead to better convergence rate as compared to standard gradient descent with uniform power distribution. We then apply our results to the LQR setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15806v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashwin Verma, Aritra Mitra, Lintao Ye, Vijay Gupta</dc:creator>
    </item>
    <item>
      <title>Rigorous dense graph limit of a model for biological transportation networks</title>
      <link>https://arxiv.org/abs/2507.15829</link>
      <description>arXiv:2507.15829v1 Announce Type: new 
Abstract: We rigorously derive the dense graph limit of a discrete model describing the formation of biological transportation networks. The discrete model, defined on undirected graphs with pressure-driven flows, incorporates a convex energy functional combining pumping and metabolic costs. It is constrained by a Kirchhoff law reflecting the local mass conservation. We first rescale and reformulate the discrete energy functional as an integral `semi-discrete' functional, where the Kirchhoff law transforms into a nonlocal elliptic integral equation. Assuming that the sequence of graphs is uniformly connected and that the limiting graphon is 0-1 valued, we prove two results: (1) rigorous Gamma-convergence of the sequence of the semi-discrete functionals to a continuum limit as the number of graph nodes and edges tends to infinity; (2) convergence of global minimizers of the discrete functionals to a global minimizer of the limiting continuum functional. Our results provide a rigorous mathematical foundation for the continuum description of biological transport structures emerging from discrete networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15829v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nuno J. Alves, Jan Haskovec</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data</title>
      <link>https://arxiv.org/abs/2507.14268</link>
      <description>arXiv:2507.14268v1 Announce Type: cross 
Abstract: This paper presents a comparative analysis of algorithmic strategies for fitting tessellation models to 3D image data of materials such as polycrystals and foams. In this steadily advancing field, we review and assess optimization-based methods -- including linear and nonlinear programming, stochastic optimization via the cross-entropy method, and gradient descent -- for generating Voronoi, Laguerre, and generalized balanced power diagrams (GBPDs) that approximate voxelbased grain structures. The quality of fit is evaluated on real-world datasets using discrepancy measures that quantify differences in grain volume, surface area, and topology. Our results highlight trade-offs between model complexity, the complexity of the optimization routines involved, and the quality of approximation, providing guidance for selecting appropriate methods based on data characteristics and application needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14268v1</guid>
      <category>cs.CV</category>
      <category>cond-mat.mtrl-sci</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Alpers, Orkun Furat, Christian Jung, Matthias Neumann, Claudia Redenbach, Aigerim Saken, Volker Schmidt</dc:creator>
    </item>
    <item>
      <title>Stable matchings with switching costs</title>
      <link>https://arxiv.org/abs/2507.14362</link>
      <description>arXiv:2507.14362v1 Announce Type: cross 
Abstract: In a stable matching problem there are two groups of agents, with agents on one side having their individual preferences for agents on another side as a potential match. It is assumed silently that agents can freely and costlessly ``switch" partners. A matching is called stable if no two unmatched agents prefer each other to their matches. Half a century ago, for equinumerous sides, Knuth demonstrated existence of preferences for which there are exponentially many stable matchings, and he posed a problem of evaluating an expected number of stable matchings when the preferences are uniformly random and independent. It was shown later by Pittel that this expectation is quite moderate, asymptotic to $e^{-1}n\log n$, $n$ being the number of agents on each side. The proof used Knuth's integral formula for the expectation based on a classic inclusion-exclusion counting. In later papers by Pittel this and other integral formulas were obtained by generating preference lists via a pair of random matrices, with independently random, $[0,1]$-uniform entries, rather than from a progressively problematic counting approach. The novelty of this paper is that we view the matrix entries as a basis for cardinal utilities that each agent ascribes to the agents on the other side. Relaxing the notion of stability, we declare matching stable if no unmatched pair of agents {\it strongly\/} prefer each other to their partners, with ``strength'' measured by a parameter $\eps&gt;0$, $\eps=0$ corresponding to classic stability. We show that for $\eps$ of order $n^{-1}\log n$ the expected number of $\eps$-stable matchings is polynomially large, but for $\eps\gg n^{-1}\log n$ {\it however slightly\/} the expectation is suddenly super-polynomially large. This ``explosion'' phenomenon for a very small strength parameter $\eps$ continues to hold for imbalanced matchings, regardless of the imbalance size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14362v1</guid>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boris Pittel, Kirill Rudov</dc:creator>
    </item>
    <item>
      <title>Statistical and Algorithmic Foundations of Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2507.14444</link>
      <description>arXiv:2507.14444v1 Announce Type: cross 
Abstract: As a paradigm for sequential decision making in unknown environments, reinforcement learning (RL) has received a flurry of attention in recent years. However, the explosion of model complexity in emerging applications and the presence of nonconvexity exacerbate the challenge of achieving efficient RL in sample-starved situations, where data collection is expensive, time-consuming, or even high-stakes (e.g., in clinical trials, autonomous systems, and online advertising). How to understand and enhance the sample and computational efficacies of RL algorithms is thus of great interest. In this tutorial, we aim to introduce several important algorithmic and theoretical developments in RL, highlighting the connections between new ideas and classical topics. Employing Markov Decision Processes as the central mathematical model, we cover several distinctive RL scenarios (i.e., RL with a simulator, online RL, offline RL, robust RL, and RL with human feedback), and present several mainstream RL approaches (i.e., model-based approach, value-based approach, and policy optimization). Our discussions gravitate around the issues of sample complexity, computational efficiency, as well as algorithm-dependent and information-theoretic lower bounds from a non-asymptotic viewpoint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14444v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuejie Chi, Yuxin Chen, Yuting Wei</dc:creator>
    </item>
    <item>
      <title>Coherent Ising Machines: The Good, The Bad, The Ugly</title>
      <link>https://arxiv.org/abs/2507.14489</link>
      <description>arXiv:2507.14489v1 Announce Type: cross 
Abstract: Analog computing using bosonic computational states is a leading approach to surpassing the computational speed and energy limitations of von Neumann architectures. But the challenges of manufacturing large-scale photonic integrated circuits (PIC) has led to hybrid solutions that integrate optical analog and electronic digital components. A notable example is the coherent Ising machine (CIM), that was primarily invented for solving quadratic binary optimization problems. In this paper, we focus on a mean-field interpretation of the dynamics of optical pulses in the CIM as solutions to Langevin dynamics, a stochastic differential equation (SDE) that plays a key role in non-convex optimization and generative AI. This interpretation establishes a computational framework for understanding the system's operation, the computational role of each component, and its performance, strengths, and limitations. We then infer that the CIM is inherently a continuous state machine, capable of integrating a broad range of SDEs, in particular for solving a continuous global (or mildly constrained) optimization problems. Nevertheless, we observe that the iterative digital-to-analog and analog-to-digital conversions within the protocol create a bottleneck for the low power and high speed of optics to shine. This observation underscores the need for major advances in PIC technologies as we envision that fully analog opto-electronic realizations of such experiments can open doors for broader applications, and orders of magnitude improvements in speed and energy consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14489v1</guid>
      <category>physics.optics</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farhad Khosravi, Martin Perreault, Artur Scherer, Pooya Ronagh</dc:creator>
    </item>
    <item>
      <title>Neural Brownian Motion</title>
      <link>https://arxiv.org/abs/2507.14499</link>
      <description>arXiv:2507.14499v1 Announce Type: cross 
Abstract: This paper introduces the Neural-Brownian Motion (NBM), a new class of stochastic processes for modeling dynamics under learned uncertainty. The NBM is defined axiomatically by replacing the classical martingale property with respect to linear expectation with one relative to a non-linear Neural Expectation Operator, $\varepsilon^\theta$, generated by a Backward Stochastic Differential Equation (BSDE) whose driver $f_\theta$ is parameterized by a neural network. Our main result is a representation theorem for a canonical NBM, which we define as a continuous $\varepsilon^\theta$-martingale with zero drift under the physical measure. We prove that, under a key structural assumption on the driver, such a canonical NBM exists and is the unique strong solution to a stochastic differential equation of the form ${\rm d} M_t = \nu_\theta(t, M_t) {\rm d} W_t$. Crucially, the volatility function $\nu_\theta$ is not postulated a priori but is implicitly defined by the algebraic constraint $g_\theta(t, M_t, \nu_\theta(t, M_t)) = 0$, where $g_\theta$ is a specialization of the BSDE driver. We develop the stochastic calculus for this process and prove a Girsanov-type theorem for the quadratic case, showing that an NBM acquires a drift under a new, learned measure. The character of this measure, whether pessimistic or optimistic, is endogenously determined by the learned parameters $\theta$, providing a rigorous foundation for models where the attitude towards uncertainty is a discoverable feature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14499v1</guid>
      <category>math.PR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qian Qi</dc:creator>
    </item>
    <item>
      <title>Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games</title>
      <link>https://arxiv.org/abs/2507.14529</link>
      <description>arXiv:2507.14529v1 Announce Type: cross 
Abstract: We consider the maximum causal entropy inverse reinforcement learning problem for infinite-horizon stationary mean-field games, in which we model the unknown reward function within a reproducing kernel Hilbert space. This allows the inference of rich and potentially nonlinear reward structures directly from expert demonstrations, in contrast to most existing inverse reinforcement learning approaches for mean-field games that typically restrict the reward function to a linear combination of a fixed finite set of basis functions. We also focus on the infinite-horizon cost structure, whereas prior studies primarily rely on finite-horizon formulations. We introduce a Lagrangian relaxation to this maximum causal entropy inverse reinforcement learning problem that enables us to reformulate it as an unconstrained log-likelihood maximization problem, and obtain a solution \lk{via} a gradient ascent algorithm. To illustrate the theoretical consistency of the algorithm, we establish the smoothness of the log-likelihood objective by proving the Fr\'echet differentiability of the related soft Bellman operators with respect to the parameters in the reproducing kernel Hilbert space. We demonstrate the effectiveness of our method on a mean-field traffic routing game, where it accurately recovers expert behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14529v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Berkay Anahtarci, Can Deha Kariksiz, Naci Saldi</dc:creator>
    </item>
    <item>
      <title>Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization</title>
      <link>https://arxiv.org/abs/2507.14746</link>
      <description>arXiv:2507.14746v1 Announce Type: cross 
Abstract: High-fidelity simulations and physical experiments are essential for engineering analysis and design. However, their high cost often limits their applications in two critical tasks: global sensitivity analysis (GSA) and optimization. This limitation motivates the common use of Gaussian processes (GPs) as proxy regression models to provide uncertainty-aware predictions based on a limited number of high-quality observations. GPs naturally enable efficient sampling strategies that support informed decision-making under uncertainty by extracting information from a subset of possible functions for the model of interest. Despite their popularity in machine learning and statistics communities, sampling from GPs has received little attention in the community of engineering optimization. In this paper, we present the formulation and detailed implementation of two notable sampling methods -- random Fourier features and pathwise conditioning -- for generating posterior samples from GPs. Alternative approaches are briefly described. Importantly, we detail how the generated samples can be applied in GSA, single-objective optimization, and multi-objective optimization. We show successful applications of these sampling methods through a series of numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14746v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bach Do, Nafeezat A. Ajenifuja, Taiwo A. Adebiyi, Ruda Zhang</dc:creator>
    </item>
    <item>
      <title>On an Abstraction of Lyapunov and Lagrange Stability</title>
      <link>https://arxiv.org/abs/2507.15047</link>
      <description>arXiv:2507.15047v1 Announce Type: cross 
Abstract: This paper studies a set-theoretic generalization of Lyapunov and Lagrange stability for abstract systems described by set-valued maps. Lyapunov stability is characterized as the property of inversely mapping filters to filters, Lagrange stability as that of mapping ideals to ideals. These abstract definitions unveil a deep duality between the two stability notions, enable a definition of global stability for abstract systems, and yield an agile generalization of the stability theorems for basic series, parallel, and feedback interconnections, including a small-gain theorem. Moreover, it is shown that Lagrange stability is abstractly identical to other properties of interest in control theory, such as safety and positivity, whose preservation under interconnections can be thus studied owing to the developed stability results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15047v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michelangelo Bin, David Angeli</dc:creator>
    </item>
    <item>
      <title>On Strongly Convex Sets and Farthest Distance Functions</title>
      <link>https://arxiv.org/abs/2507.15053</link>
      <description>arXiv:2507.15053v1 Announce Type: cross 
Abstract: A polarity notion for sets in a Banach space is introduced in such a way that the second polar of a set coincides with the smallest strongly convex set with respect to R that contains it. Strongly convex sets are characterized in terms of their associated farthest distance functions, and farthest distance functions associated with strongly convex sets are characterized, too.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15053v1</guid>
      <category>math.FA</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan Enrique Mart\'inez-Legaz</dc:creator>
    </item>
    <item>
      <title>Robust Control with Gradient Uncertainty</title>
      <link>https://arxiv.org/abs/2507.15082</link>
      <description>arXiv:2507.15082v1 Announce Type: cross 
Abstract: We introduce a novel extension to robust control theory that explicitly addresses uncertainty in the value function's gradient, a form of uncertainty endemic to applications like reinforcement learning where value functions are approximated. We formulate a zero-sum dynamic game where an adversary perturbs both system dynamics and the value function gradient, leading to a new, highly nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness by proving a comparison principle for its viscosity solutions under a uniform ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a key insight: we prove that the classical quadratic value function assumption fails for any non-zero gradient uncertainty, fundamentally altering the problem structure. A formal perturbation analysis characterizes the non-polynomial correction to the value function and the resulting nonlinearity of the optimal control law, which we validate with numerical studies. Finally, we bridge theory to practice by proposing a novel Gradient-Uncertainty-Robust Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating its effectiveness in stabilizing training. This work provides a new direction for robust control, holding significant implications for fields where function approximation is common, including reinforcement learning and computational finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15082v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qian Qi</dc:creator>
    </item>
    <item>
      <title>Does the draw matter in an incomplete round-robin tournament? The case of the UEFA Champions League</title>
      <link>https://arxiv.org/abs/2507.15320</link>
      <description>arXiv:2507.15320v1 Announce Type: cross 
Abstract: A fundamental reform has been introduced in the 2024/25 season of club competitions organised by the Union of European Football Associations (UEFA): the well-established group stage has been replaced by an incomplete round-robin format. In this format, the 36 teams are ranked in a single league table, but play against only a subset of the competitors. While this innovative change has highlighted that the incomplete round-robin tournament is a reasonable alternative to the standard design of allocating the teams into round-robin groups, the characteristics of the new format remain unexplored. Our paper contributes to this topic by using simulations to compare the uncertainty generated by the draw in the old format with that in the new format of the UEFA Champions League. We develop a method to break down the impact of the 2024/25 reform into various components for each team. The new format is found to decrease the overall effect of the draw. However, this reduction can mainly be attributed to the inaccurate seeding system used by UEFA. When teams are seeded based on their actual strength, the impact of the draw is about the same in a tournament with an incomplete round-robin league or a group stage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15320v1</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, Andr\'as Gyimesi, Dries Goossens, Karel Devriesere, Roel Lambers, Frits Spieksma</dc:creator>
    </item>
    <item>
      <title>Strategically Robust Game Theory via Optimal Transport</title>
      <link>https://arxiv.org/abs/2507.15325</link>
      <description>arXiv:2507.15325v1 Announce Type: cross 
Abstract: In many game-theoretic settings, agents are challenged with taking decisions against the uncertain behavior exhibited by others. Often, this uncertainty arises from multiple sources, e.g., incomplete information, limited computation, bounded rationality. While it may be possible to guide the agents' decisions by modeling each source, their joint presence makes this task particularly daunting. Toward this goal, it is natural for agents to seek protection against deviations around the emergent behavior itself, which is ultimately impacted by all the above sources of uncertainty. To do so, we propose that each agent takes decisions in face of the worst-case behavior contained in an ambiguity set of tunable size, centered at the emergent behavior so implicitly defined. This gives rise to a novel equilibrium notion, which we call strategically robust equilibrium. Building on its definition, we show that, when judiciously operationalized via optimal transport, strategically robust equilibria (i) are guaranteed to exist under the same assumptions required for Nash equilibria; (ii) interpolate between Nash and security strategies; (iii) come at no additional computational cost compared to Nash equilibria. Through a variety of experiments, including bi-matrix games, congestion games, and Cournot competition, we show that strategic robustness protects against uncertainty in the opponents' behavior and, surprisingly, often results in higher equilibrium payoffs - an effect we refer to as coordination via robustification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15325v1</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Lanzetti, Sylvain Fricker, Saverio Bolognani, Florian D\"orfler, Dario Paccagnan</dc:creator>
    </item>
    <item>
      <title>MAP Estimation with Denoisers: Convergence Rates and Guarantees</title>
      <link>https://arxiv.org/abs/2507.15397</link>
      <description>arXiv:2507.15397v1 Announce Type: cross 
Abstract: Denoiser models have become powerful tools for inverse problems, enabling the use of pretrained networks to approximate the score of a smoothed prior distribution. These models are often used in heuristic iterative schemes aimed at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal operator of the negative log-prior plays a central role. In practice, this operator is intractable, and practitioners plug in a pretrained denoiser as a surrogate-despite the lack of general theoretical justification for this substitution. In this work, we show that a simple algorithm, closely related to several used in practice, provably converges to the proximal operator under a log-concavity assumption on the prior $p$. We show that this algorithm can be interpreted as a gradient descent on smoothed proximal objectives. Our analysis thus provides a theoretical foundation for a class of empirically successful but previously heuristic methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15397v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Scott Pesme, Giacomo Meanti, Michael Arbel, Julien Mairal</dc:creator>
    </item>
    <item>
      <title>Scaled Relative Graph Analysis of General Interconnections of SISO Nonlinear Systems</title>
      <link>https://arxiv.org/abs/2507.15564</link>
      <description>arXiv:2507.15564v1 Announce Type: cross 
Abstract: Scaled Relative Graphs (SRGs) provide a novel graphical frequency-domain method for the analysis of nonlinear systems. However, we show that the current SRG analysis suffers from a pitfall that limits its applicability in analyzing practical nonlinear systems. We overcome this pitfall by introducing a novel reformulation of the SRG of a linear time-invariant operator and combining the SRG with the Nyquist criterion. The result is a theorem that can be used to assess stability and $L_2$-gain performance for general interconnections of nonlinear dynamic systems. We provide practical calculation results for canonical interconnections and apply our result to Lur'e systems to obtain a generalization of the celebrated circle criterion, which deals with broader class of nonlinearities, and we derive (incremental) $L_2$-gain performance bounds. We illustrate the power of the new approach on the analysis of several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15564v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julius P. J. Krebbekx, Roland T\'oth, Amritam Das</dc:creator>
    </item>
    <item>
      <title>Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy</title>
      <link>https://arxiv.org/abs/2507.15566</link>
      <description>arXiv:2507.15566v1 Announce Type: cross 
Abstract: The availability of downstream resources plays a critical role in planning the admission of patients undergoing elective surgery, with inpatient beds being one of the most crucial resources. When planning patient admissions, predictions on their length-of-stay (LOS) made by machine learning (ML) models are used to ensure bed availability. However, the actual LOS for each patient may differ considerably from the predicted value, potentially making the schedule infeasible. To address such infeasibilities, rescheduling strategies that take advantage of operational flexibility can be implemented. For example, adjustments may include postponing admission dates, relocating patients to different wards, or even transferring patients who are already admitted. The common assumption is that more accurate LOS predictions reduce the impact of rescheduling. However, training ML models that can make such accurate predictions can be costly. Building on previous work that proposed simulated \ac{ml} for evaluating data-driven approaches, this paper explores the relationship between LOS prediction accuracy and rescheduling flexibility across various corrective policies. Specifically, we examine the most effective patient rescheduling strategies under LOS prediction errors to prevent bed overflows while optimizing resource utilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15566v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pieter Smet, Martina Doneda, Ettore Lanzarone, Giuliana Carello</dc:creator>
    </item>
    <item>
      <title>Data-driven optimal approximation on Hardy spaces in simply connected domains</title>
      <link>https://arxiv.org/abs/2507.15837</link>
      <description>arXiv:2507.15837v1 Announce Type: cross 
Abstract: We consider optimal interpolation of functions analytic in simply connected domains in the complex plane. By choosing a specific structure for the approximant, we show that the resulting first order optimality conditions can be interpreted as optimal $\mathcal{H}_2$ interpolation conditions for discrete-time dynamical systems. Connections to the implicit Euler method, the midpoint method, and backward differentiation methods are also established. A data-driven algorithm is developed to compute a (locally) optimal approximant. Our method is tested on three numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15837v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Borghi, Tobias Breiten</dc:creator>
    </item>
    <item>
      <title>AILS-II: An Adaptive Iterated Local Search Heuristic for the Large-scale Capacitated Vehicle Routing Problem</title>
      <link>https://arxiv.org/abs/2205.12082</link>
      <description>arXiv:2205.12082v2 Announce Type: replace 
Abstract: A recent study on the classical Capacitated Vehicle Routing Problem (CVRP) introduced an adaptive version of the widely used Iterated Local Search (ILS) paradigm, hybridized with a path-relinking strategy (PR). The solution method, called AILS-PR, outperformed existing meta-heuristics for the CVRP on benchmark instances. However, tests on large-scale instances of the CVRP suggested that PR was too slow, making AILS-PR less advantageous in this case. To overcome this challenge, this paper presents an Adaptive Iterated Local Search (AILS) with two phases in its search process. Both phases include the perturbation and local search steps of ILS. The main difference between them is that the reference solution in the first phase is found by the acceptance criterion, while in the second phase it is selected from a pool of the best solutions found in the search process, the so-called elite set. This algorithm, called AILS-II, is very competitive on smaller instances, outperforming the other methods from the literature with respect to the average gap to the best known solutions. Moreover, AILS-II consistently outperformed the state of the art on larger instances with up to 30,000 vertices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.12082v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1287/ijoc.2023.0106</arxiv:DOI>
      <arxiv:journal_reference>INFORMS Journal on Computing, Vol. 36, No. 4, 974-986, 2024</arxiv:journal_reference>
      <dc:creator>Vin\'icius R. M\'aximo, Jean-Fran\c{c}ois Cordeau, Mari\'a C. V. Nascimento</dc:creator>
    </item>
    <item>
      <title>Shape of extremal functions for weighted Sobolev-type inequalities</title>
      <link>https://arxiv.org/abs/2311.01083</link>
      <description>arXiv:2311.01083v2 Announce Type: replace 
Abstract: We study the shape of solutions to some variational problems in Sobolev spaces with weights that are powers of |x|. In particular, we detect situations when the extremal functions lack symmetry properties such as radial symmetry and antisymmetry. We also prove an isoperimetric inequality for the first non-zero eigenvalue of a weighted Neumann problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01083v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Friedemann Brock (MLU), Francesco Chiacchio (SAMM), Gisella Croce (SAMM), Anna Mercaldo</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of Non-Strongly-Monotone Stochastic Quasi-Variational Inequalities</title>
      <link>https://arxiv.org/abs/2401.03076</link>
      <description>arXiv:2401.03076v3 Announce Type: replace 
Abstract: While Variational Inequality (VI) is a well-established mathematical framework that subsumes Nash equilibrium and saddle-point problems, less is known about its extension, Quasi-Variational Inequalities (QVI). QVI allows for cases where the constraint set changes as the decision variable varies allowing for a more versatile setting. In this paper, we propose extra-gradient and gradient-based methods for solving a class of monotone Stochastic Quasi-Variational Inequalities (SQVI) and establish a rigorous convergence rate analysis for these methods. Our approach not only advances the theoretical understanding of SQVI but also demonstrates its practical applicability. Specifically, we highlight its effectiveness in reformulating and solving problems such as generalized Nash Equilibrium, bilevel optimization, and saddle-point problems with coupling constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03076v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeinab Alizadeh, Afrooz Jalilzadeh</dc:creator>
    </item>
    <item>
      <title>Hovering Flight in Flapping Insects and Hummingbirds: A Natural Real-Time and Stable Extremum Seeking Feedback System</title>
      <link>https://arxiv.org/abs/2402.04985</link>
      <description>arXiv:2402.04985v4 Announce Type: replace 
Abstract: In this paper, we take an initial and novel step toward characterizing the hovering phenomenon in flapping insects and hummingbirds as a new class of extremum seeking (ES) feedback systems. By characterizing hovering flight in insects and hummingbirds as a natural hovering ES system, we achieve: (1) very simple, (2) stable, (3) model-free, and (4) real-time hovering. More importantly, our hovering ES characterization only needs the natural oscillations of the wing as the ES input. That is, unlike other control techniques in the literature, the natural hovering ES system only needs the natural flapping action built in the system, and feedback of local sensations (measurements) related to the altitude where the insect seeks to stabilize itself. Said ES characterization, can become an important initial step in starting a new line of research that may succeed in resolving the longstanding gap between model-based control theory and the biologically observed mechanisms that stabilize hovering flight. We provide simulation trials, including comparisons with some approaches from literature, to demonstrate the effectiveness and robustness of our results. We used literature data for hawkmoth, cranefly, bumblebee, dragonfly, hoverfly, and a hummingbird.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04985v4</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed A. Elgohary, Sameh A. Eisa</dc:creator>
    </item>
    <item>
      <title>Algebraic Riccati Tensor Equations with Applications in Multilinear Control Systems</title>
      <link>https://arxiv.org/abs/2402.13491</link>
      <description>arXiv:2402.13491v5 Announce Type: replace 
Abstract: In a recent paper by Chen et al. [8], the authors initiated the control-theoretic study of a class of discrete-time multilinear time-invariant (MLTI) control systems, where system states, inputs, and outputs are all tensors endowed with the Einstein product. They established criteria for fundamental system-theoretic notions such as stability, reachability, and observability through tensor decomposition. Building on this new research direction, the purpose of our paper is to extend the study to continuous-time MLTI control systems. Specifically, we define Hamiltonian tensors and symplectic tensors, and we establish the Schur-Hamiltonian tensor decomposition and the symplectic tensor singular value decomposition (SVD). Based on these concepts, we propose the algebraic Riccati tensor equation (ARTE) and demonstrate that it has a unique positive semidefinite solution if the system is stabilizable and detectable. To find numerical solutions to the ARTE, we introduce a tensor-based Newton method. Additionally, we establish the tensor versions of the bounded real lemma and the small gain theorem. A first-order robustness analysis of the ARTE is also conducted. Finally, we provide a numerical example to illustrate the proposed theory and algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13491v5</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchao Wang, Yimin Wei, Guofeng Zhang, Shih Yu Chang</dc:creator>
    </item>
    <item>
      <title>Variance-reduction for Variational Inequality Problems with Bregman Distance Function</title>
      <link>https://arxiv.org/abs/2405.10735</link>
      <description>arXiv:2405.10735v3 Announce Type: replace 
Abstract: In this paper, we address variational inequalities (VI) with a finite-sum structure. We introduce a novel single-loop stochastic variance-reduced algorithm, incorporating the Bregman distance function, and establish an optimal convergence guarantee under a monotone setting. Additionally, we explore a structured class of non-monotone problems that exhibit weak Minty solutions, and analyze the complexity of our proposed method, highlighting a significant improvement over existing approaches. Numerical experiments are presented to demonstrate the performance of our algorithm compared to state-of-the-art methods</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10735v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeinab Alizadeh, Erfan Yazdandoost Hamedani, Afrooz Jalilzadeh</dc:creator>
    </item>
    <item>
      <title>Lipschitz continuity of solution multifunctions of extended $\ell_1$ regularization problems</title>
      <link>https://arxiv.org/abs/2406.16053</link>
      <description>arXiv:2406.16053v2 Announce Type: replace 
Abstract: The Lasso and the basis pursuit in compressed sensing and machine learning are convex optimization problems with three parameters: the regularization scalar, the observation vector and the data matrix. Relative to the first two parameters, we obtain the Lipschitz continuity of the solution multifunction on its convex domain. When the data matrix of the Lasso also perturbs, where non-polyhedral structure may display, we obtain full characterizations for the Lipschitz continuity of the solution multifunction on the product of a compact and convex set in the space of first two parameters and a neighborhood of the fixed data matrix. Moreover for the solution multifunction of the Lasso, we show that the Lipschitz continuity implies its single-valuedness. Our analysis is based on polyhedron theory, a sufficient condition that ensures the Lipschitz continuity of a polyhedral multifunction with a convex domain, and an explicit representation of the solution multifunction, where the latter is a consequence of the Lipschitz continuity of the solution multifunction relative to the first two parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16053v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kaiwen Meng, Pengcheng Wu, Xiaoqi Yang</dc:creator>
    </item>
    <item>
      <title>Inverse of the Gomory Corner Relaxation of Integer Programs</title>
      <link>https://arxiv.org/abs/2410.22653</link>
      <description>arXiv:2410.22653v2 Announce Type: replace 
Abstract: We explore the inverse of integer programs (IPs) by studying the inverse of their Gomory corner relaxations (GCRs). We show that solving a set of inverse GCR problems always yields an upper bound on the optimal value of the inverse IP that is at least as tight as the optimal value of the inverse of the linear program (LP) relaxation. We provide conditions under which solving a set of inverse GCR problems exactly solves the inverse IP. We propose an LP formulation for solving the inverse GCR under the $L_1$ and $L_\infty$ norms by reformulating the inverse GCR as the inverse of a shortest path problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22653v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>George Lyu, Fatemeh Nosrat, Andrew J. Schaefer</dc:creator>
    </item>
    <item>
      <title>A Parameterized Barzilai-Borwein Method via Interpolated Least Squares</title>
      <link>https://arxiv.org/abs/2411.03899</link>
      <description>arXiv:2411.03899v2 Announce Type: replace 
Abstract: The Barzilai-Borwein (BB) method is an effective gradient descent algorithm for solving unconstrained optimization problems. Based on the observation of two classical BB step sizes, by constructing an interpolated least squares model, we propose a novel class of BB step sizes, each of which still retains the quasi-Newton property, with the original two BB step sizes being their two extreme cases. We present the mathematical principle underlying the adaptive alternating BB (ABB) method. Based on this principle, we develop a class of effective adaptive interpolation parameters. For strictly convex quadratic optimization problems, we establish the R-linear convergence of this new gradient descent method by investigating the evolution pattern of the ratio of the absolute values of the gradient components. Numerical experiments are conducted to illustrate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03899v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Xu</dc:creator>
    </item>
    <item>
      <title>Differential estimates for fast first-order multilevel nonconvex optimisation</title>
      <link>https://arxiv.org/abs/2412.01481</link>
      <description>arXiv:2412.01481v2 Announce Type: replace 
Abstract: With a view on bilevel and PDE-constrained optimisation, we develop iterative estimates $\widetilde{F'}(x^k)$ of $F'(x^k)$ for composite functions $F :=J \circ S$, where $S$ is the solution mapping of the inner optimisation problem or PDE. The idea is to form a single-loop method by interweaving updates of the iterate $x^k$ by an outer optimisation method, with updates of the estimate by single steps of standard optimisation methods and linear system solvers. When the inner methods satisfy simple tracking inequalities, the differential estimates can almost directly be employed in standard convergence proofs for general forward-backward type methods. We adapt those proofs to a general inexact setting in normed spaces, that, besides our differential estimates, also covers mismatched adjoints and unreachable optimality conditions in measure spaces. As a side product of these efforts, we provide improved convergence results for nonconvex Primal-Dual Proximal Splitting (PDPS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01481v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil Dizon, Tuomo Valkonen</dc:creator>
    </item>
    <item>
      <title>Algebraic Control: Complete Stable Inversion with Necessary and Sufficient Conditions</title>
      <link>https://arxiv.org/abs/2501.00172</link>
      <description>arXiv:2501.00172v3 Announce Type: replace 
Abstract: In this paper, we establish necessary and sufficient conditions for stable inversion, addressing challenges in non-minimum phase, non-square, and singular systems. An H-Infinity based algebraic approximation is introduced for near-perfect tracking without preview. Additionally, we propose a novel robust control strategy combining the nominal model with dual feedforward control to form a feedback structure. Numerical comparison demonstrates the approach's effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00172v3</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Burak K\"urk\c{c}\"u, Masayoshi Tomizuka</dc:creator>
    </item>
    <item>
      <title>Efficient Primal Heuristics for Mixed Binary Quadratic Programs Using Suboptimal Rounding Guidance</title>
      <link>https://arxiv.org/abs/2501.05052</link>
      <description>arXiv:2501.05052v2 Announce Type: replace 
Abstract: Mixed Binary Quadratic Programs (MBQPs) are a class of NP-hard problems that arise in a wide range of applications, including finance, machine learning, and chemical and energy systems. Large-scale MBQPs are challenging to solve with exact algorithms due to the combinatorial search space and nonlinearity. Primal heuristics have been developed to quickly identify high-quality solutions to challenging combinatorial optimization problems. In this paper, we propose an extension for two well-established rounding-based primal heuristics, RENS and Undercover. Instead of using the optimal solution to a relaxation for variable rounding and search as in RENS, we use a suboptimal relaxation solution of the MBQP as the basis for rounding and guidance for searching over a restricted subproblem where a certain percentage of binary variables are free. We apply a similar idea to the Undercover heuristic that fixes a variable cover to the rounded relaxation values. Instead, we relax a subset of the cover variables based on the suboptimal relaxation and search over a larger restricted subproblem. We evaluate our proposed methods on synthetic MBQP benchmarks and real-world wind farm layout optimization problem instances. The results show that our proposed heuristics identify high-quality solutions within a small time limit and significantly reduce the primal gap and primal integral compared to RENS, Undercover, and solvers with additional primal heuristics integrated inside Branch-and-Bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05052v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weimin Huang, Natalie M. Isenberg, Jan Drgona, Draguna L Vrabie, Bistra Dilkina</dc:creator>
    </item>
    <item>
      <title>Efficient Sparse Flow Decomposition Methods for RNA Multi-Assembly</title>
      <link>https://arxiv.org/abs/2501.14662</link>
      <description>arXiv:2501.14662v5 Announce Type: replace 
Abstract: Decomposing a flow on a Directed Acyclic Graph (DAG) into a weighted sum of a small number of paths is an essential task in operations research and bioinformatics. This problem, referred to as Sparse Flow Decomposition (SFD), has gained significant interest, in particular for its application in RNA transcript multi-assembly, the identification of the multiple transcripts corresponding to a given gene and their relative abundance. Several recent approaches cast SFD variants as integer optimization problems, motivated by the NP-hardness of the formulations they consider. We propose an alternative formulation of SFD as a data fitting problem on the conic hull of the flow polytope. By reformulating the problem on the flow polytope for compactness and solving it using specific variants of the Frank-Wolfe algorithm, we obtain a method converging rapidly to the minimizer of the chosen loss function while producing a parsimonious decomposition. Our approach subsumes previous formulations of SFD with exact and inexact flows and can model different priors on the error distributions. Computational experiments show that our method outperforms recent integer optimization approaches in runtime, but is also highly competitive in terms of reconstruction of the underlying transcripts, despite not explicitly minimizing the solution cardinality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14662v5</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathieu Besan\c{c}on</dc:creator>
    </item>
    <item>
      <title>Sequential Extremal Principle and Necessary Conditions for Minimizing Sequences</title>
      <link>https://arxiv.org/abs/2502.19884</link>
      <description>arXiv:2502.19884v2 Announce Type: replace 
Abstract: The conventional definition of extremality of a finite collection of sets is extended by replacing a fixed point (extremal point) in the intersection of the sets by a collection of sequences of points in the individual sets with the distances between the corresponding points tending to zero. This allows one to consider collections of unbounded sets with empty intersection. Exploiting the ideas behind the conventional extremal principle, we derive an extended sequential version of the latter result in terms of Fr\'echet and Clarke normals. Sequential versions of the related concepts of stationarity, approximate stationarity and transversality of collections of sets are also studied. As an application, we establish sequential necessary conditions for minimizing (and more general firmly stationary, stationary and approximately stationary) sequences in a constrained optimization problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19884v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/02331934.2025.2534122</arxiv:DOI>
      <arxiv:journal_reference>Optimization (2025)</arxiv:journal_reference>
      <dc:creator>Nguyen Duy Cuong, Alexander Y. Kruger</dc:creator>
    </item>
    <item>
      <title>Hardware-Compatible Single-Shot Feasible-Space Heuristics for Solving the Quadratic Assignment Problem</title>
      <link>https://arxiv.org/abs/2503.09676</link>
      <description>arXiv:2503.09676v2 Announce Type: replace 
Abstract: Research into the development of special-purpose computing architectures designed to solve quadratic unconstrained binary optimization (QUBO) problems has flourished in recent years. It has been demonstrated in the literature that such special-purpose solvers can outperform traditional CMOS architectures by orders of magnitude with respect to timing metrics on synthetic problems. However, they face challenges with constrained problems such as the quadratic assignment problem (QAP), where mapping to binary formulations such as QUBO introduces overhead and limits parallelism. In-memory computing (IMC) devices, such as memristor-based analog Ising machines, offer significant speedups and efficiency gains over traditional CPU-based solvers, particularly for solving combinatorial optimization problems. In this work, we present a novel local search heuristic designed for IMC hardware to tackle the QAP. Our approach enables massive parallelism that allows for computing of full neighbourhoods simultaneously to make update decisions. We ensure binary solutions remain feasible by selecting local moves that lead to neighbouring feasible solutions, leveraging feasible-space search heuristics and the underlying structure of a given problem. Our approach is compatible with both digital computers and analog hardware. We demonstrate its effectiveness in CPU implementations by comparing it with state-of-the-art heuristics for solving the QAP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09676v2</guid>
      <category>math.OC</category>
      <category>cs.AR</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haesol Im, Chan-Woo Yang, Moslem Noori, Dmitrii Dobrynin, Elisabetta Valiante, Giacomo Pedretti, Arne Heittmann, Thomas Van Vaerenbergh, Masoud Mohseni, John Paul Strachan, Dmitri Strukov, Ray Beausoleil, Ignacio Rozada</dc:creator>
    </item>
    <item>
      <title>Two Innovations in Inexact Augmented Lagrangian Methods for Convex Optimization</title>
      <link>https://arxiv.org/abs/2503.11809</link>
      <description>arXiv:2503.11809v2 Announce Type: replace 
Abstract: This paper presents two new techniques relating to inexact solution of subproblems in augmented Lagrangian methods for convex programming. The first involves combining a relative error criterion for solution of the subproblems with over- or under-relaxation of the multiplier update step. In one interpretation of our proposed iterative scheme, a predetermined amount of relaxation effects the criterion for an acceptably accurate solution value. Alternatively, the amount of multiplier step relaxation can be adapted to the accuracy of the subproblem subject to a viability test employing the discriminant of a certain quadratic function. The second innovation involves solution of augmented Lagrangian subproblems for problems posed in standard Fenchel-Rockafellar form. We show that applying alternating minimization to this subproblem, as in the first two steps of the ADMM, is equivalent to executing the classical proximal gradient method on a dual formulation of the subproblem. By substituting more sophisticated variants of the proximal gradient method for the classical one, it is possible to construct new ADMM-like methods with better empirical performance than using ordinary alternating minimization within an inexact augmented Lagrangian framework. The paper concludes by describing some computational experiments exploring using these two innovations, both separately and jointly, to solve LASSO problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11809v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Eckstein, Chang Yu</dc:creator>
    </item>
    <item>
      <title>Feedback Optimization with State Constraints through Control Barrier Functions</title>
      <link>https://arxiv.org/abs/2504.00813</link>
      <description>arXiv:2504.00813v2 Announce Type: replace 
Abstract: Recently, there has been a surge of research on a class of methods called feedback optimization. These are methods to steer the state of a control system to an equilibrium that arises as the solution of an optimization problem. Despite the growing literature on the topic, the important problem of enforcing state constraints at all times remains unaddressed. In this work, we present the first feedback-optimization method that enforces state constraints. The method combines a class of dynamics called safe gradient flows with high-order control barrier functions. We provide a number of results on our proposed controller, including well-posedness guarantees, anytime constraint-satisfaction guarantees, equivalence between the closed-loop's equilibria and the optimization problem's critical points, and local asymptotic stability of optima.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00813v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giannis Delimpaltadakis, Pol Mestres, Jorge Cort\'es, W. P. M. H. Heemels</dc:creator>
    </item>
    <item>
      <title>Geometry of efficient weight vectors</title>
      <link>https://arxiv.org/abs/2504.19400</link>
      <description>arXiv:2504.19400v3 Announce Type: replace 
Abstract: Pairwise comparison matrices and the weight vectors obtained from them are important concepts in multi-criteria decision making. A weight vector calculated from a pairwise comparison matrix is called Pareto efficient if the approximation of the matrix elements by the weight ratios cannot be improved for any element of the matrix without worsening it for another element. The aim of this paper is to show the geometrical properties of the set of Pareto efficient weight vectors for $4\times 4$ pairwise comparison matrices. We prove that the set of efficient weight vectors is a union of three tetrahedra, each determined by four weight vectors calculated from an incomplete submatrix of the pairwise comparison matrix that can be represented by a path spanning tree graph. It is shown that with suitable rearrangements the orientations of the $4$-cycles in the Blanquero-Carrizosa-Conde graphs for the efficient weight vectors are determined as well. The special cases (double perturbed, simple perturbed, consistent matrices) are discussed in the online appendices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19400v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Krist\'of \'Abele-Nagy, S\'andor Boz\'oki, Zsombor Sz\'adoczki</dc:creator>
    </item>
    <item>
      <title>Platoon Coordination and Leader Selection in Mixed Transportation Systems via Dynamic Programming</title>
      <link>https://arxiv.org/abs/2505.00847</link>
      <description>arXiv:2505.00847v2 Announce Type: replace 
Abstract: With the growing penetration of electric trucks, freight transportation is transitioning toward a mixed system comprising both fuel-powered and electric trucks. Enhancing truck platoon formation in such a heterogeneous environment presents new challenges. This paper investigates the hub-based platoon coordination problem in a mixed truck fleet, where the focus is to optimize the trucks' waiting times, charging amounts for electric trucks, and platoon leader assignments. The objective is to maximize the overall platoon revenue of the fleet while accounting for the associated waiting and charging costs. We formulate the problem as a mixed-integer linear program and present a dynamic programming approach to compute its sub-optimal solution efficiently. The proposed method operates in polynomial time, ensuring scalable computational efficiency. Simulation studies involving 1,000 trucks traveling between two hubs in Sweden demonstrate the effectiveness and scalability of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00847v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Wang, Ting Bai, Andreas A. Malikopoulos</dc:creator>
    </item>
    <item>
      <title>Relative Explanations for Contextual Problems with Endogenous Uncertainty: An Application to Competitive Facility Location</title>
      <link>https://arxiv.org/abs/2506.19155</link>
      <description>arXiv:2506.19155v2 Announce Type: replace 
Abstract: In this paper, we consider contextual stochastic optimization problems subject to endogenous uncertainty, where the decisions affect the underlying distributions. To implement such decisions in practice, it is crucial to ensure that their outcomes are interpretable and trustworthy. To this end, we compute relative counterfactual explanations, providing practitioners with concrete changes in the contextual covariates required for a solution to satisfy specific constraints. Whereas relative explanations have been introduced in prior literature, to the best of our knowledge, this is the first work focused on problems with binary decision variables and subject to endogenous uncertainty. We propose a methodology that uses Wasserstein distance as regularization and to compute a lower bound. It leads to a drastic reduction in computation times, compared to the unregularized counterpart. We illustrate the method using a choice-based competitive facility location problem, and present numerical experiments that demonstrate its ability to efficiently compute sparse and interpretable explanations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19155v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jasone Ram\'irez-Ayerbe, Emma Frejinger</dc:creator>
    </item>
    <item>
      <title>Escaping Saddle Points for Nonsmooth Weakly Convex Functions via Perturbed Proximal Algorithms</title>
      <link>https://arxiv.org/abs/2102.02837</link>
      <description>arXiv:2102.02837v3 Announce Type: replace-cross 
Abstract: We propose perturbed proximal algorithms that can provably escape strict saddles for nonsmooth weakly convex functions. The main results are based on a novel characterization of $\epsilon$-approximate local minimum for nonsmooth functions, and recent developments on perturbed gradient methods for escaping saddle points for smooth problems. Specifically, we show that under standard assumptions, the perturbed proximal point, perturbed proximal gradient and perturbed proximal linear algorithms find $\epsilon$-approximate local minimum for nonsmooth weakly convex functions in $O(\epsilon^{-2}\log(d)^4)$ iterations, where $d$ is the dimension of the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.02837v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minhui Huang, Weiming Zhu</dc:creator>
    </item>
    <item>
      <title>Polynomial argmin for recovery and approximation of multivariate discontinuous functions</title>
      <link>https://arxiv.org/abs/2302.06945</link>
      <description>arXiv:2302.06945v3 Announce Type: replace-cross 
Abstract: We propose to approximate a (possibly discontinuous) multivariate function f (x) on a compact set by the partial minimizer arg miny p(x, y) of an appropriate polynomial p whose construction can be cast in a univariate sum of squares (SOS) framework, resulting in a highly structured convex semidefinite program. In a number of non-trivial cases (e.g. when f is a piecewise polynomial) we prove that the approximation is exact with a low-degree polynomial p. Our approach has three distinguishing features: (i) It is mesh-free and does not require the knowledge of the discontinuity locations. (ii) It is model-free in the sense that we only assume that the function to be approximated is available through samples (point evaluations). (iii) The size of the semidefinite program is independent of the ambient dimension and depends linearly on the number of samples. We also analyze the sample complexity of the approach, proving a generalization error bound in a probabilistic setting. This allows for a comparison with machine learning approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06945v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Didier Henrion (LAAS-POP), Milan Korda (LAAS-POP), Jean-Bernard Lasserre (LAAS-POP)</dc:creator>
    </item>
    <item>
      <title>Optimizer's Information Criterion: Dissecting and Correcting Bias in Data-Driven Optimization</title>
      <link>https://arxiv.org/abs/2306.10081</link>
      <description>arXiv:2306.10081v4 Announce Type: replace-cross 
Abstract: In data-driven optimization, the sample performance of the obtained decision typically incurs an optimistic bias against the true performance, a phenomenon commonly known as the Optimizer's Curse and intimately related to overfitting in machine learning. Common techniques to correct this bias, such as cross-validation, require repeatedly solving additional optimization problems and are therefore computationally expensive. We develop a general bias correction approach, building on what we call Optimizer's Information Criterion (OIC), that directly approximates the first-order bias and does not require solving any additional optimization problems. Our OIC generalizes the celebrated Akaike Information Criterion to evaluate the objective performance in data-driven optimization, which crucially involves not only model fitting but also its interplay with the downstream optimization. As such it can be used for decision selection instead of only model selection. We apply our approach to a range of data-driven optimization formulations comprising empirical and parametric models, their regularized counterparts, and furthermore contextual optimization. Finally, we provide numerical validation on the superior performance of our approach under synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10081v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Garud Iyengar, Henry Lam, Tianyu Wang</dc:creator>
    </item>
    <item>
      <title>Maximum Causal Entropy IRL in Mean-Field Games and GNEP Framework for Forward RL</title>
      <link>https://arxiv.org/abs/2401.06566</link>
      <description>arXiv:2401.06566v2 Announce Type: replace-cross 
Abstract: This paper explores the use of Maximum Causal Entropy Inverse Reinforcement Learning (IRL) within the context of discrete-time stationary Mean-Field Games (MFGs) characterized by finite state spaces and an infinite-horizon, discounted-reward setting. Although the resulting optimization problem is non-convex with respect to policies, we reformulate it as a convex optimization problem in terms of state-action occupation measures by leveraging the linear programming framework of Markov Decision Processes. Based on this convex reformulation, we introduce a gradient descent algorithm with a guaranteed convergence rate to efficiently compute the optimal solution. Moreover, we develop a new method that conceptualizes the MFG problem as a Generalized Nash Equilibrium Problem (GNEP), enabling effective computation of the mean-field equilibrium for forward reinforcement learning (RL) problems and marking an advancement in MFG solution techniques. We further illustrate the practical applicability of our GNEP approach by employing this algorithm to generate data for numerical MFG examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06566v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Berkay Anahtarci, Can Deha Kariksiz, Naci Saldi</dc:creator>
    </item>
    <item>
      <title>Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport</title>
      <link>https://arxiv.org/abs/2403.12887</link>
      <description>arXiv:2403.12887v2 Announce Type: replace-cross 
Abstract: We study the convergence of gradient flow for the training of deep neural networks. If Residual Neural Networks are a popular example of very deep architectures, their training constitutes a challenging optimization problem due notably to the non-convexity and the non-coercivity of the objective. Yet, in applications, those tasks are successfully solved by simple optimization algorithms such as gradient descent. To better understand this phenomenon, we focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide ResNet, parameterized by probability measures over the product set of layers and parameters and with constant marginal on the set of layers. Indeed, in the case of shallow neural networks, mean field models have proven to benefit from simplified loss-landscapes and good theoretical guarantees when trained with gradient flow for the Wasserstein metric on the set of probability measures. Motivated by this approach, we propose to train our model with gradient flow w.r.t. the conditional Optimal Transport distance: a restriction of the classical Wasserstein distance which enforces our marginal condition. Relying on the theory of gradient flows in metric spaces we first show the well-posedness of the gradient flow equation and its consistency with the training of ResNets at finite width. Performing a local Polyak-\L{}ojasiewicz analysis, we then show convergence of the gradient flow for well-chosen initializations: if the number of features is finite but sufficiently large and the risk is sufficiently small at initialization, the gradient flow converges towards a global minimizer. This is the first result of this type for infinitely deep and arbitrarily wide ResNets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12887v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rapha\"el Barboni (ENS-PSL), Gabriel Peyr\'e (CNRS,ENS-PSL), Fran\c{c}ois-Xavier Vialard (LIGM)</dc:creator>
    </item>
    <item>
      <title>Non-commutative optimization problems with differential constraints</title>
      <link>https://arxiv.org/abs/2408.02572</link>
      <description>arXiv:2408.02572v3 Announce Type: replace-cross 
Abstract: Non-commutative polynomial optimization (NPO) problems seek to minimize the state average of a polynomial of some operator variables, subject to polynomial constraints, over all states and operators, as well as the Hilbert spaces where those might be defined. Many of these problems are known to admit a complete hierarchy of semidefinite programming (SDP) relaxations. In this work, we consider a variant of NPO problems where a subset of the operator variables satisfies a system of ordinary differential equations. We find that, under mild conditions of operator boundedness, for every such problem one can construct a standard NPO problem with the same solution. This allows us to define a complete hierarchy of SDPs to tackle the original differential problem. We apply this method to bound averages of local observables in quantum spin systems subject to a Hamiltonian evolution (i.e., a quench). We find that, even in the thermodynamic limit of infinitely many sites, low levels of the hierarchy provide very good approximations for reasonably long evolution times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02572v3</guid>
      <category>quant-ph</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mateus Ara\'ujo, Andrew J. P. Garner, Miguel Navascues</dc:creator>
    </item>
    <item>
      <title>Linear-Quadratic Dynamic Games as Receding-Horizon Variational Inequalities</title>
      <link>https://arxiv.org/abs/2408.15703</link>
      <description>arXiv:2408.15703v2 Announce Type: replace-cross 
Abstract: We consider dynamic games with linear dynamics and quadratic objective functions. We observe that the unconstrained open-loop Nash equilibrium coincides with a linear quadratic regulator in an augmented space, thus deriving an explicit expression of the cost-to-go. With such cost-to-go as a terminal cost, we show asymptotic stability for the receding-horizon solution of the finite-horizon, constrained game. Furthermore, we show that the problem is equivalent to a non-symmetric variational inequality, which does not correspond to any Nash equilibrium problem. For unconstrained closed-loop Nash equilibria, we derive a receding-horizon controller that is equivalent to the infinite-horizon one and ensures asymptotic stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15703v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Emilio Benenati, Sergio Grammatico</dc:creator>
    </item>
    <item>
      <title>In-depth Analysis of Low-rank Matrix Factorisation in a Federated Setting</title>
      <link>https://arxiv.org/abs/2409.08771</link>
      <description>arXiv:2409.08771v2 Announce Type: replace-cross 
Abstract: We analyze a distributed algorithm to compute a low-rank matrix factorization on $N$ clients, each holding a local dataset $\mathbf{S}^i \in \mathbb{R}^{n_i \times d}$, mathematically, we seek to solve $min_{\mathbf{U}^i \in \mathbb{R}^{n_i\times r}, \mathbf{V}\in \mathbb{R}^{d \times r} } \frac{1}{2} \sum_{i=1}^N \|\mathbf{S}^i - \mathbf{U}^i \mathbf{V}^\top\|^2_{\text{F}}$. Considering a power initialization of $\mathbf{V}$, we rewrite the previous smooth non-convex problem into a smooth strongly-convex problem that we solve using a parallel Nesterov gradient descent potentially requiring a single step of communication at the initialization step. For any client $i$ in $\{1, \dots, N\}$, we obtain a global $\mathbf{V}$ in $\mathbb{R}^{d \times r}$ common to all clients and a local variable $\mathbf{U}^i$ in $\mathbb{R}^{n_i \times r}$. We provide a linear rate of convergence of the excess loss which depends on $\sigma_{\max} / \sigma_{r}$, where $\sigma_{r}$ is the $r^{\mathrm{th}}$ singular value of the concatenation $\mathbf{S}$ of the matrices $(\mathbf{S}^i)_{i=1}^N$. This result improves the rates of convergence given in the literature, which depend on $\sigma_{\max}^2 / \sigma_{\min}^2$. We provide an upper bound on the Frobenius-norm error of reconstruction under the power initialization strategy. We complete our analysis with experiments on both synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08771v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Constantin Philippenko, Kevin Scaman, Laurent Massouli\'e</dc:creator>
    </item>
    <item>
      <title>Model-Free and Real-Time Bioinspired Unicycle-Based Source Seeking: Differential Wheeled Robotic Experiments</title>
      <link>https://arxiv.org/abs/2501.02184</link>
      <description>arXiv:2501.02184v3 Announce Type: replace-cross 
Abstract: Many autonomous robots aimed at source-seeking are studied, and their controls designed, using unicycle modeling and formulation. This is true not only for model-based controllers, but also for model-free, real-time control methods such as extremum seeking control (ESC). In this paper, we propose a unicycle-based ESC design applicable to differential wheeled robots that: (1) is very simple design, based on one simple control-affine law, and without state integrators; (2) attenuates oscillations known to persist in ESC designs (i.e., fully stop at the source); and (3) operates in a model-free, real-time setting, tolerating environmental/sensor noise. We provide simulation and real-world robotic experimental results for fixed and moving light source seeking by a differential wheeled robot using our proposed design. Results indicate clear advantages of our proposed design when compared to the literature, including attenuation of undesired oscillations, improved convergence speed, and better handling of noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02184v3</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed A. Elgohary, Sameh A. Eisa, Shivam Bajpai</dc:creator>
    </item>
    <item>
      <title>Global Regularity Estimates for Optimal Transport via Entropic Regularisation</title>
      <link>https://arxiv.org/abs/2501.11382</link>
      <description>arXiv:2501.11382v4 Announce Type: replace-cross 
Abstract: We develop a general approach to prove global regularity estimates for quadratic optimal transport using the entropic regularisation of the problem and the Prekopa-Leindler inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11382v4</guid>
      <category>math.FA</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathael Gozlan (MAP5 - UMR 8145), Maxime Sylvestre (CEREMADE)</dc:creator>
    </item>
    <item>
      <title>An Efficient Frequency-Based Approach for Maximal Square Detection in Binary Matrices</title>
      <link>https://arxiv.org/abs/2503.18974</link>
      <description>arXiv:2503.18974v3 Announce Type: replace-cross 
Abstract: Detecting maximal square submatrices of ones in binary matrices is a fundamental problem with applications in computer vision and pattern recognition. While the standard dynamic programming (DP) solution achieves optimal asymptotic complexity, its practical performance suffers from repeated minimum operations and inefficient memory access patterns that degrade cache utilization. To address these limitations, we introduce a novel frequency-based algorithm that employs a greedy approach to track the columnar continuity of ones through an adaptive frequency array and a dynamic thresholding mechanism. Extensive benchmarking demonstrates that the frequency-based algorithm achieves faster performance than the standard DP in 100% of test cases with an average speedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x across matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's average speedup exceeds 2.5x for all densities and rises to over 3.5x for densities of 0.7 and higher across all matrix sizes. These results demonstrate that the frequency-based approach is a superior alternative to standard DP and opens new possibilities for efficient matrix analysis in performance-critical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18974v3</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swastik Bhandari</dc:creator>
    </item>
    <item>
      <title>Ultra-fast feature learning for the training of two-layer neural networks in the two-timescale regime</title>
      <link>https://arxiv.org/abs/2504.18208</link>
      <description>arXiv:2504.18208v2 Announce Type: replace-cross 
Abstract: We study the convergence of gradient methods for the training of mean-field single-hidden-layer neural networks with square loss. For this high-dimensional and non-convex optimization problem, most known convergence results are either qualitative or rely on a neural tangent kernel analysis where nonlinear representations of the data are fixed. Using that this problem belongs to the class of separable nonlinear least squares problems, we consider here a Variable Projection (VarPro) or two-timescale learning algorithm, thereby eliminating the linear variables and reducing the learning problem to the training of nonlinear features. In a teacher-student scenario, we show such a strategy enables provable convergence rates for the sampling of a teacher feature distribution. Precisely, in the limit where the regularization strength vanishes, we show that the dynamic of the feature distribution corresponds to a weighted ultra-fast diffusion equation. Recent results on the asymptotic behavior of such PDEs then give quantitative guarantees for the convergence of the learned feature distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18208v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rapha\"el Barboni (\'ENS-PSL), Gabriel Peyr\'e (CNRS,\'ENS-PSL), Fran\c{c}ois-Xavier Vialard (LIGM)</dc:creator>
    </item>
    <item>
      <title>An Explicit Formula for Vertex Enumeration in the CUT(n) Polytope via Probabilistic Methods</title>
      <link>https://arxiv.org/abs/2506.21787</link>
      <description>arXiv:2506.21787v2 Announce Type: replace-cross 
Abstract: We present an explicit closed-form formula for the vertices of the classical cut polytope $\operatorname{CUT}(n)$, defined as the convex hull of cut vectors of the complete graph $K_n$. Our derivation proceeds via a related polytope, denoted $\mathbf{1}$-$\operatorname{CUT}(n)$, whose vertices are obtained by flipping all bits of the $\operatorname{CUT}(n)$ vertices. This polytope arises naturally in a probabilistic context involving agreement probabilities among symmetric Bernoulli random variables which serves as the starting point of this work.
  Our approach constructs the vertex set recursively via a binary encoding that stems from this probabilistic perspective. We prove that the resulting sequence of encoded integers, when appropriately scaled, exhibits an almost-linear behavior closely approximating the line $y = x - \frac{1}{2}$. This structure motivates the introduction of the alternating cycle function, an integer-valued map whose key property is power-of-two composition invariance. The function serves as the foundation for our closed-form enumeration formula.
  The result provides a rare instance of explicit vertex characterization for a $0$/$1$-polytope and offers a transparent combinatorial construction independent of enumeration algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21787v2</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nevena Mari\'c</dc:creator>
    </item>
    <item>
      <title>Greedy Low-Rank Gradient Compression for Distributed Learning with Convergence Guarantees</title>
      <link>https://arxiv.org/abs/2507.08784</link>
      <description>arXiv:2507.08784v2 Announce Type: replace-cross 
Abstract: Distributed optimization is pivotal for large-scale signal processing and machine learning, yet communication overhead remains a major bottleneck. Low-rank gradient compression, in which the transmitted gradients are approximated by low-rank matrices to reduce communication, offers a promising remedy. Existing methods typically adopt either randomized or greedy compression strategies: randomized approaches project gradients onto randomly chosen subspaces, introducing high variance and degrading empirical performance; greedy methods select the most informative subspaces, achieving strong empirical results but lacking convergence guarantees. To address this gap, we propose GreedyLore--the first Greedy Low-Rank gradient compression algorithm for distributed learning with rigorous convergence guarantees. GreedyLore incorporates error feedback to correct the bias introduced by greedy compression and introduces a semi-lazy subspace update that ensures the compression operator remains contractive throughout all iterations. With these techniques, we prove that GreedyLore achieves a convergence rate of $\mathcal{O}(\sigma/\sqrt{NT} + 1/T)$ under standard optimizers such as MSGD and Adam--marking the first linear speedup convergence rate for low-rank gradient compression. Extensive experiments are conducted to validate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08784v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuyan Chen, Yutong He, Pengrui Li, Weichen Jia, Kun Yuan</dc:creator>
    </item>
    <item>
      <title>Probabilistic Robustness in the Gap Metric</title>
      <link>https://arxiv.org/abs/2507.10010</link>
      <description>arXiv:2507.10010v2 Announce Type: replace-cross 
Abstract: Uncertainties influencing the dynamical systems pose a significant challenge in estimating the achievable performance of a controller aiming to control such uncertain systems. When the uncertainties are of stochastic nature, obtaining hard guarantees for the robustness of a controller aiming to hedge against the uncertainty is not possible. This issue set the platform for the development of probabilistic robust control approaches. In this work, we utilise the gap metric between the known nominal model and the unknown perturbed model of the uncertain system as a tool to gauge the robustness of a controller and formulate the gap as a random variable in the setting with stochastic uncertainties. The main results of this paper include giving a probabilistic bound on the gap exceeding a known threshold, followed by bounds on the expected gap value and probabilistic robust stability and performance guarantees in terms of the gap metric. We also provide a probabilistic controller performance certification under gap uncertainty and probabilistic guarantee on the achievable $\mathcal{H}_{\infty}$ robustness. Numerical simulations are provided to demonstrate the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10010v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Venkatraman Renganathan</dc:creator>
    </item>
  </channel>
</rss>
