<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 Jan 2025 13:58:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Reinforcement Learning for Respondent-Driven Sampling</title>
      <link>https://arxiv.org/abs/2501.01505</link>
      <description>arXiv:2501.01505v1 Announce Type: new 
Abstract: Respondent-driven sampling (RDS) is widely used to study hidden or hard-to-reach populations by incentivizing study participants to recruit their social connections. The success and efficiency of RDS can depend critically on the nature of the incentives, including their number, value, call to action, etc. Standard RDS uses an incentive structure that is set a priori and held fixed throughout the study. Thus, it does not make use of accumulating information on which incentives are effective and for whom. We propose a reinforcement learning (RL) based adaptive RDS study design in which the incentives are tailored over time to maximize cumulative utility during the study. We show that these designs are more efficient, cost-effective, and can generate new insights into the social structure of hidden populations. In addition, we develop methods for valid post-study inference which are non-trivial due to the adaptive sampling induced by RL as well as the complex dependencies among subjects due to latent (unobserved) social network structure. We provide asymptotic regret bounds and illustrate its finite sample behavior through a suite of simulation experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01505v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Weltz, Angela Yoon, Yichi Zhang, Alexander Volfovsky, Eric Laber</dc:creator>
    </item>
    <item>
      <title>Denoising Diffused Embeddings: a Generative Approach for Hypergraphs</title>
      <link>https://arxiv.org/abs/2501.01541</link>
      <description>arXiv:2501.01541v1 Announce Type: new 
Abstract: Hypergraph data, which capture multi-way interactions among entities, are becoming increasingly prevalent in the big data eta. Generating new hyperlinks from an observed, usually high-dimensional hypergraph is an important yet challenging task with diverse applications, such as electronic health record analysis and biological research. This task is fraught with several challenges. The discrete nature of hyperlinks renders many existing generative models inapplicable. Additionally, powerful machine learning-based generative models often operate as black boxes, providing limited interpretability. Key structural characteristics of hypergraphs, including node degree heterogeneity and hyperlink sparsity, further complicate the modeling process and must be carefully addressed. To tackle these challenges, we propose Denoising Diffused Embeddings (DDE), a general generative model architecture for hypergraphs. DDE exploits potential low-rank structures in high-dimensional hypergraphs and adopts the state-of-the-art diffusion model framework. Theoretically, we show that when true embeddings are accessible, DDE exactly reduces the task of generating new high-dimensional hyperlinks to generating new low-dimensional embeddings. Moreover, we analyze the implications of using estimated embeddings in DDE, revealing how hypergraph properties--such as dimensionality, node degree heterogeneity, and hyperlink sparsity--impact its generative performance. Simulation studies demonstrate the superiority of DDE over existing methods, in terms of both computational efficiency and generative accuracy. Furthermore, an application to a symptom co-occurrence hypergraph derived from electronic medical records uncovers interesting findings and highlights the advantages of DDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01541v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shihao Wu, Junyi Yang, Gongjun Xu, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Bootstrap Nonparametric Inference under Data Integration</title>
      <link>https://arxiv.org/abs/2501.01610</link>
      <description>arXiv:2501.01610v1 Announce Type: new 
Abstract: We propose multiplier bootstrap procedures for nonparametric inference and uncertainty quantification of the target mean function, based on a novel framework of integrating target and source data. We begin with the relatively easier covariate shift scenario with equal target and source mean functions and propose estimation and inferential procedures through a straightforward combination of all target and source datasets. We next consider the more general and flexible distribution shift scenario with arbitrary target and source mean functions, and propose a two-step inferential procedure. First, we estimate the target-to-source differences based on separate portions of the target and source data. Second, the remaining source data are adjusted by these differences and combined with the remaining target data to perform the multiplier bootstrap procedure. Our method enables local and global inference on the target mean function without using asymptotic distributions. To justify our approach, we derive an optimal convergence rate for the nonparametric estimator and establish bootstrap consistency to estimate the asymptotic distribution of the nonparametric estimator. The proof of global bootstrap consistency involves a central limit theorem for quadratic forms with dependent variables under a conditional probability measure. Our method applies to arbitrary source and target datasets, provided that the data sizes meet a specific quantitative relationship. Simulation studies and real data analysis are provided to examine the performance of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01610v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zuofeng Shang, Peijun Sang, Chong Jin</dc:creator>
    </item>
    <item>
      <title>Change Point Detection for Random Objects with Possibly Periodic Behavior</title>
      <link>https://arxiv.org/abs/2501.01657</link>
      <description>arXiv:2501.01657v1 Announce Type: new 
Abstract: Time-varying random objects have been increasingly encountered in modern data analysis. Moreover, in a substantial number of these applications, periodic behavior of the random objects has been observed. We introduce a new, powerful scan statistic and corresponding test for the precise identification and localization of abrupt changes in the distribution of non-Euclidean random objects with possibly periodic behavior. Our approach is nonparametric and effectively captures the entire distribution of these random objects. Remarkably, it operates with minimal tuning parameters, requiring only the specification of cut-off intervals near endpoints, where change points are assumed not to occur. Our theoretical contributions include deriving the asymptotic distribution of the test statistic under the null hypothesis of no change points, establishing the consistency of the test in the presence of change points under contiguous alternatives and providing rigorous guarantees on the near-optimal consistency in estimating the number and locations of change points, whether dealing with a single change point or multiple ones. We demonstrate that the most competitive method currently in the literature for change point detection in random objects is degraded by periodic behavior, as periodicity leads to blurring of the changes that this procedure aims to discover. Through comprehensive simulation studies, we demonstrate the superior power and accuracy of our approach in both detecting change points and pinpointing their locations, across scenarios involving both periodic and nonperiodic random objects. Our main application is to weighted networks, represented through graph Laplacians. The proposed method delivers highly interpretable results, as evidenced by the identification of meaningful change points in the New York City Citi Bike sharing system that align with significant historical events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01657v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiazhen Xu, Andrew T. A. Wood, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Martingale Posteriors from Score Functions</title>
      <link>https://arxiv.org/abs/2501.01890</link>
      <description>arXiv:2501.01890v1 Announce Type: new 
Abstract: Uncertainty associated with statistical problems arises due to what has not been seen as opposed to what has been seen. Using probability to quantify the uncertainty the task is to construct a probability model for what has not been seen conditional on what has been seen. The traditional Bayesian approach is to use prior distributions for constructing the predictive distributions, though recently a novel approach has used density estimators and the use of martingales to establish convergence of parameter values. In this paper we reply on martingales constructed using score functions. Hence, the method only requires the computing of gradients arising from parametric families of density functions. A key point is that we do not rely on Markov Chain Monte Carlo (MCMC) algorithms, and that the method can be implemented in parallel. We present the theoretical properties of the score driven martingale posterior. Further, we present illustrations under different models and settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01890v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fuheng Cui, Stephen G. Walker</dc:creator>
    </item>
    <item>
      <title>Inference in matrix-valued time series with common stochastic trends and multifactor error structure</title>
      <link>https://arxiv.org/abs/2501.01925</link>
      <description>arXiv:2501.01925v1 Announce Type: new 
Abstract: We develop an estimation methodology for a factor model for high-dimensional matrix-valued time series, where common stochastic trends and common stationary factors can be present. We study, in particular, the estimation of (row and column) loading spaces, of the common stochastic trends and of the common stationary factors, and the row and column ranks thereof. In a set of (negative) preliminary results, we show that a projection-based technique fails to improve the rates of convergence compared to a "flattened" estimation technique which does not take into account the matrix nature of the data. Hence, we develop a three-step algorithm where: (i) we first project the data onto the orthogonal complement to the (row and column) loadings of the common stochastic trends; (ii) we subsequently use such "trend free" data to estimate the stationary common component; (iii) we remove the estimated common stationary component from the data, and re-estimate, using a projection-based estimator, the row and column common stochastic trends and their loadings. We show that this estimator succeeds in refining the rates of convergence of the initial, "flattened" estimator. As a by-product, we develop consistent eigenvalue-ratio based estimators for the number of stationary and nonstationary common factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01925v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rong Chen, Simone Giannerini, Greta Goracci, Lorenzo Trapani</dc:creator>
    </item>
    <item>
      <title>Sparsified Simultaneous Confidence Intervals for High-Dimensional Linear Models</title>
      <link>https://arxiv.org/abs/2307.07574</link>
      <description>arXiv:2307.07574v2 Announce Type: replace 
Abstract: Statistical inference of the high-dimensional regression coefficients is challenging because the uncertainty introduced by the model selection procedure is hard to account for. A critical question remains unsettled; that is, is it possible and how to embed the inference of the model into the simultaneous inference of the coefficients? To this end, we propose a notion of simultaneous confidence intervals called the sparsified simultaneous confidence intervals. Our intervals are sparse in the sense that some of the intervals' upper and lower bounds are shrunken to zero (i.e., $[0,0]$), indicating the unimportance of the corresponding covariates. These covariates should be excluded from the final model. The rest of the intervals, either containing zero (e.g., $[-1,1]$ or $[0,1]$) or not containing zero (e.g., $[2,3]$), indicate the plausible and significant covariates, respectively. The proposed method can be coupled with various selection procedures, making it ideal for comparing their uncertainty. For the proposed method, we establish desirable asymptotic properties, develop intuitive graphical tools for visualization, and justify its superior performance through simulation and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07574v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s00184-024-00975-z</arxiv:DOI>
      <arxiv:journal_reference>Metrika, 2024</arxiv:journal_reference>
      <dc:creator>Xiaorui Zhu, Yichen Qin, Peng Wang</dc:creator>
    </item>
    <item>
      <title>Data fusion using weakly aligned sources</title>
      <link>https://arxiv.org/abs/2308.14836</link>
      <description>arXiv:2308.14836v3 Announce Type: replace 
Abstract: We introduce a new data fusion method that utilizes multiple data sources to estimate a smooth, finite-dimensional parameter. Most existing methods only make use of fully aligned data sources that share common conditional distributions of one or more variables of interest. However, in many settings, the scarcity of fully aligned sources can make existing methods require unduly large sample sizes to be useful. Our approach enables the incorporation of weakly aligned data sources that are not perfectly aligned, provided their degree of misalignment is known up to finite-dimensional parameters. {We quantify the additional efficiency gains achieved through the integration of these weakly aligned sources. We characterize the semiparametric efficiency bound and provide a general means to construct estimators achieving these efficiency gains.} We illustrate our results by fusing data from two harmonized HIV monoclonal antibody prevention efficacy trials to study how a neutralizing antibody biomarker associates with HIV genotype.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14836v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sijia Li, Peter B. Gilbert, Rui Duan, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Treatment Effect Heterogeneity and Importance Measures for Multivariate Continuous Treatments</title>
      <link>https://arxiv.org/abs/2404.09126</link>
      <description>arXiv:2404.09126v2 Announce Type: replace 
Abstract: Estimating the joint effect of a multivariate, continuous exposure is crucial, particularly in environmental health where interest lies in simultaneously evaluating the impact of multiple environmental pollutants on health. We develop novel methodology that addresses two key issues for estimation of treatment effects of multivariate, continuous exposures. We use nonparametric Bayesian methodology that is flexible to ensure our approach can capture a wide range of data generating processes. Additionally, we allow the effect of the exposures to be heterogeneous with respect to covariates. Treatment effect heterogeneity has not been well explored in the causal inference literature for multivariate, continuous exposures, and therefore we introduce novel estimands that summarize the nature and extent of the heterogeneity, and propose estimation procedures for new estimands related to treatment effect heterogeneity. We provide theoretical support for the proposed models in the form of posterior contraction rates and show that it works well in simulated examples both with and without heterogeneity. Our approach is motivated by a study of the health effects of simultaneous exposure to the components of PM$_{2.5}$, where we find that the negative health effects of exposure to environmental pollutants are exacerbated by low socioeconomic status, race and age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09126v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejun Shin, Antonio Linero, Michelle Audirac, Kezia Irene, Danielle Braun, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal Transport Metrics for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2411.14674</link>
      <description>arXiv:2411.14674v4 Announce Type: replace 
Abstract: Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing density estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures. Estimating the mixing measure implies inference on the mixture density and the random partition. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14674v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Peter Mueller</dc:creator>
    </item>
    <item>
      <title>Identifying average causal effect in regression discontinuity design with auxiliary data</title>
      <link>https://arxiv.org/abs/2412.20840</link>
      <description>arXiv:2412.20840v3 Announce Type: replace 
Abstract: Regression discontinuity designs are widely used when treatment assignment is determined by whether a running variable exceeds a predefined threshold. However, most research focuses on estimating local causal effects at the threshold, leaving the challenge of identifying treatment effects away from the cutoff largely unaddressed. The primary difficulty in this context is that the treatment assignment is deterministically defined by the running variable, violating the commonly assumed positivity assumption. In this paper, we introduce a novel framework for identifying the average causal effect in regression discontinuity designs. Our approach assumes the existence of an auxiliary variable for which the running variable can be seen as a surrogate, and an additional dataset that consists of the running variable and the auxiliary variable alongside the traditional regression discontinuity design setup. Under this framework, we propose three estimation methods for the ATE, which resembles the outcome regression, inverse propensity weighted and doubly robust estimators in classical causal inference literature. Asymptotically valid inference procedures are also provided. To demonstrate the practical application of our method, simulations are conducted to show the good performance of our methods; besides, we use the proposed methods to assess the causal effects of vitamin A supplementation on the severity of autism spectrum disorders in children, where a positive effect is found but with no statistical significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20840v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinqin Feng, Wenjie Hu, Pu Yang, Tingyu Li, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Marginal Effects for Probit and Tobit with Endogeneity</title>
      <link>https://arxiv.org/abs/2306.14862</link>
      <description>arXiv:2306.14862v4 Announce Type: replace-cross 
Abstract: When evaluating partial effects, it is important to distinguish between structural endogeneity and measurement errors. In contrast to linear models, these two sources of endogeneity affect partial effects differently in nonlinear models. We study this issue focusing on the Instrumental Variable (IV) Probit and Tobit models. We show that even when a valid IV is available, failing to differentiate between the two types of endogeneity can lead to either under- or over-estimation of the partial effects. We develop simple estimators of the bounds on the partial effects and provide easy to implement confidence intervals that correctly account for both types of endogeneity. We illustrate the methods in a Monte Carlo simulation and an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14862v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kirill S. Evdokimov, Ilze Kalnina, Andrei Zeleneev</dc:creator>
    </item>
  </channel>
</rss>
