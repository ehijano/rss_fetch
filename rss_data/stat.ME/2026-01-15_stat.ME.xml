<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 Jan 2026 05:01:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>MLCBART: Multilabel Classification with Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2601.08964</link>
      <description>arXiv:2601.08964v1 Announce Type: new 
Abstract: Multilabel Classification (MLC) deals with the simultaneous classification of multiple binary labels. The task is challenging because, not only may there be arbitrarily different and complex relationships between predictor variables and each label, but associations among labels may exist even after accounting for effects of predictor variables. In this paper, we present a Bayesian additive regression tree (BART) framework to model the problem. BART is a nonparametric and flexible model structure capable of uncovering complex relationships within the data. Our adaptation, MLCBART, assumes that labels arise from thresholding an underlying numeric scale, where a multivariate normal model allows explicit estimation of the correlation structure among labels. This enables the discovery of complicated relationships in various forms and improves MLC predictive performance. Our Bayesian framework not only enables uncertainty quantification for each predicted label, but our MCMC draws produce an estimated conditional probability distribution of label combinations for any predictor values. Simulation experiments demonstrate the effectiveness of the proposed model by comparing its performance with a set of models, including the oracle model with the correct functional form. Results show that our model predicts vectors of labels more accurately than other contenders and its performance is close to the oracle model. An example highlights how the method's ability to produce measures of uncertainty on predictions provides nuanced understanding of classification results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08964v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiahao Tian, Hugh Chipman, Thomas Loughin</dc:creator>
    </item>
    <item>
      <title>Approximate Shapley value estimation using sampling without replacement and variance estimation via the new Symmetric bootstrap and the Doubled half bootstrap</title>
      <link>https://arxiv.org/abs/2601.08981</link>
      <description>arXiv:2601.08981v1 Announce Type: new 
Abstract: In this paper I consider improving the KernelSHAP algorithm. I suggest to use the Wallenius' noncentral hypergeometric distribution for sampling the number of coalitions and perform sampling without replacement, so that the KernelSHAP estimation framework is improved further. I also introduce the Symmetric bootstrap to calculate the standard deviations and also use the Doubled half bootstrap method to compare the performance. The new bootstrap algorithm performs better or equally well in the two simulation studies performed in this paper. The new KernelSHAP algorithm performs similarly as the improved KernelSHAP method in the state-of-the-art R-package shapr, which samples coalitions with replacement in one of the options</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08981v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fredrik Lohne Aanes</dc:creator>
    </item>
    <item>
      <title>Semiparametric estimation of GLMs with interval-censored covariates via an augmented Turnbull estimator</title>
      <link>https://arxiv.org/abs/2601.08996</link>
      <description>arXiv:2601.08996v1 Announce Type: new 
Abstract: Interval-censored covariates are frequently encountered in biomedical studies, particularly in time-to-event data or when measurements are subject to detection or quantification limits. Yet, the estimation of regression models with interval-censored covariates remains methodologically underdeveloped. In this article, we address the estimation of generalized linear models when one covariate is subject to interval censoring. We propose a likelihood-based approach, GELc, that builds upon an augmented version of Turnbull's nonparametric estimator for interval-censored data. We prove that the GELc estimator is consistent and asymptotically normal under mild regularity conditions, with available standard errors. Simulation studies demonstrate favorable finite-sample performance of the estimator and satisfactory coverage of the confidence intervals. Finally, we illustrate the method using two real-world applications: the AIDS Clinical Trials Group Study 359 and an observational nutrition study on circulating carotenoids. The proposed methodology is available as an R package at github.com/atoloba/ICenCov.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08996v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Toloba, Klaus Langohr, Guadalupe G\'omez Melis</dc:creator>
    </item>
    <item>
      <title>Fisher's fundamental theorem and regression in causal analysis</title>
      <link>https://arxiv.org/abs/2601.09011</link>
      <description>arXiv:2601.09011v1 Announce Type: new 
Abstract: Fisher's fundamental theorem describes the change caused by natural selection as the change in gene frequencies multiplied by the partial regression coefficients for the average effects of genes on fitness. Fisher's result has generated extensive controversy in biology. I show that the theorem is a simple example of a general partition for change in regression predictions across altered contexts. By that rule, the total change in a mean response is the sum of two terms. The first ascribes change to the difference in predictor variables, holding constant the regression coefficients. The second ascribes change to altered context, captured by shifts in the regression coefficients. This general result follows immediately from the product rule for finite differences applied to a regression equation. Economics widely applies this same partition, the Oaxaca-Blinder decomposition, as a fundamental tool that can in proper situations be used for causal analysis. Recognizing the underlying mathematical generality clarifies Fisher's theorem, provides a useful tool for causal analysis, and reveals connections across disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09011v1</guid>
      <category>stat.ME</category>
      <category>q-bio.PE</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven A. Frank</dc:creator>
    </item>
    <item>
      <title>Graph Canonical Coherence Analysis</title>
      <link>https://arxiv.org/abs/2601.09038</link>
      <description>arXiv:2601.09038v1 Announce Type: new 
Abstract: We propose graph canonical coherence analysis (gCChA), a novel framework that extends canonical correlation analysis to multivariate graph signals in the graph frequency domain. The proposed method addresses challenges posed by the inherent features of graphs: discreteness, finiteness, and irregularity. It identifies pairs of canonical graph signals that maximize their coherence, enabling the exploration of relationships between two sets of graph signals from a spectral perspective. This framework shows how these relationships change across different structural scales of the graph. We demonstrate the usefulness of this method through applications to economic and energy datasets of G20 countries and the USPS handwritten digit dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09038v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyusoon Kim, Hee-Seok Oh</dc:creator>
    </item>
    <item>
      <title>Scalar-on-distribution regression via generalized odds with applications to accelerometry-assessed disability in multiple sclerosis</title>
      <link>https://arxiv.org/abs/2601.09126</link>
      <description>arXiv:2601.09126v1 Announce Type: new 
Abstract: Distributional representations of data collected using digital health technologies have been shown to outperform scalar summaries for clinical prediction, with carefully quantified tail-behavior often driving the gains. Motivated by these findings, we propose a unified generalized odds (GO) framework that represents subject-specific distributions through ratios of probabilities over arbitrary regions of the sample space, subsuming hazard, survival, and residual life representations as special cases. We develop a scale-on-odds regression model using spline-based functional representations with penalization for efficient estimation. Applied to wrist-worn accelerometry data from the HEAL-MS study, generalized odds models yield improved prediction of Expanded Disability Status Scale (EDSS) scores compared to classical scalar and survival-based approaches, demonstrating the value of odds-based distributional covariates for modeling DHT data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09126v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratim Guha Niyogi, Muraleetharan Sanjayan, Kathryn C. Fitzgerald, Ellen M. Mowry, Vadim Zipunnikov</dc:creator>
    </item>
    <item>
      <title>A Multilayer Probit Network Model for Community Detection with Dependent Edges and Layers</title>
      <link>https://arxiv.org/abs/2601.09161</link>
      <description>arXiv:2601.09161v1 Announce Type: new 
Abstract: Community detection in multilayer networks, which aims to identify groups of nodes exhibiting similar connectivity patterns across multiple network layers, has attracted considerable attention in recent years. Most existing methods are based on the assumption that different layers are either independent or follow specific dependence structures, and edges within the same layer are independent. In this article, we propose a novel method for community detection in multilayer networks that accounts for a broad range of inter-layer and intra-layer dependence structures. The proposed method integrates the multilayer stochastic block model for community detection with a multivariate probit model to capture the structures of inter-layer dependence, which also allows intra-layer dependence. To facilitate parameter estimation, we develop a constrained pairwise likelihood method coupled with an efficient alternating updating algorithm. The asymptotic properties of the proposed method are also established, with a focus on examining the influence of inter-layer and intra-layer dependences on the accuracy of both parameter estimation and community detection. The theoretical results are supported by extensive numerical experiments on both simulated networks and a real-world multilayer trade network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09161v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dapeng Shi, Haoran Zhang, Tiandong Wang, Junhui Wang</dc:creator>
    </item>
    <item>
      <title>White noise testing for functional time series via functional quantile autocorrelation</title>
      <link>https://arxiv.org/abs/2601.09371</link>
      <description>arXiv:2601.09371v1 Announce Type: new 
Abstract: We introduce a novel class of nonlinear tests for serial dependence in functional time series, grounded in the functional quantile autocorrelation framework. Unlike traditional approaches based on the classical autocovariance kernel, the functional quantile autocorrelation framework leverages quantile-based excursion sets to robustly capture temporal dependence within infinite-dimensional functional data, accommodating potential outliers and complex nonlinear dependencies. We propose omnibus test statistics and study their asymptotic properties under both known and estimated quantile curves, establishing their asymptotic distribution and consistency under mild assumptions. In particular, no moment conditions are required for the validity of the tests. Extensive simulations and an application to high-frequency financial functional time series demonstrate the methodology's effectiveness, reliably detecting complex serial dependence with superior power relative to several existing tests. This work expands the toolkit for functional time series, providing a robust framework for inference in settings where traditional methods may fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09371v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>\'Angel L\'opez-Oriona, Ying Sun, Hanlin Shang</dc:creator>
    </item>
    <item>
      <title>Tools to help patients and other stakeholders' input into choice of estimand and intercurrent event strategy in randomised trials</title>
      <link>https://arxiv.org/abs/2601.09442</link>
      <description>arXiv:2601.09442v1 Announce Type: new 
Abstract: Estimands can help to clarify the research questions being addressed in randomised trials. Because the choice of estimand can affect how relevant trial results are to patients and other stakeholders, such as clinicians or policymakers, it is important for them to be involved in these decisions. However, there are barriers to having these conversations. For instance, discussions around how intercurrent events should be addressed in the estimand definition typically involve complex concepts as well as technical language. We three tools to facilitate conversations between researchers and patients and other stakeholders about the choice of estimand and intercurrent event strategy: (i) a video explaining the concept of an estimand and the five different ways that intercurrent events can be incorporated into the estimand definition; (ii) an infographic outlining these five strategies; and (iii) an editable PowerPoint slide which can be completed with trial-specific details to facilitate conversations around choice of estimand for a particular trial. These resources can help to start conversations between the trial team and patients and other stakeholders about the best choice of estimand and intercurrent event strategies for a randomised trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09442v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joanna Hindley, Charlotte Hartley, Jennifer Hellier, Kate Sturgeon, Sophie Greenwood, Ian Newsome, Katherine Barrett, Debs Smith, Tra My Pham, Dongquan Bi, Beatriz Goulao, Suzie Cro, Brennan C Kahan</dc:creator>
    </item>
    <item>
      <title>Sparse covariate-driven factorization of high-dimensional brain connectivity with application to site effect correction</title>
      <link>https://arxiv.org/abs/2601.09525</link>
      <description>arXiv:2601.09525v1 Announce Type: new 
Abstract: Large-scale neuroimaging studies often collect data from multiple scanners across different sites, where variations in scanners, scanning procedures, and other conditions across sites can introduce artificial site effects. These effects may bias brain connectivity measures, such as functional connectivity (FC), which quantify functional network organization derived from functional magnetic resonance imaging (fMRI). How to leverage high-dimensional network structures to effectively mitigate site effects has yet to be addressed. In this paper, we propose SLACC (Sparse LAtent Covariate-driven Connectome) factorization, a multivariate method that explicitly parameterizes covariate effects in latent subject scores corresponding to sparse rank-1 latent patterns derived from brain connectivity. The proposed method identifies localized site-driven variability within and across brain networks, enabling targeted correction. We develop a penalized Expectation-Maximization (EM) algorithm for parameter estimation, incorporating the Bayesian Information Criterion (BIC) to guide optimization. Extensive simulations validate SLACC's robustness in recovering the true parameters and underlying connectivity patterns. Applied to the Autism Brain Imaging Data Exchange (ABIDE) dataset, SLACC demonstrates its ability to reduce site effects. The R package to implement our method is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09525v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rongqian Zhang, Elena Tuzhilina, Jun Young Park</dc:creator>
    </item>
    <item>
      <title>How to interpret hazard ratios</title>
      <link>https://arxiv.org/abs/2601.09571</link>
      <description>arXiv:2601.09571v1 Announce Type: new 
Abstract: The hazard ratio, typically estimated using Cox's famous proportional hazards model, is the most common effect measure used to describe the association or effect of a covariate on a time-to-event outcome. In recent years the hazard ratio has been argued by some to lack a causal interpretation, even in randomised trials, and even if the proportional hazards assumption holds. This is concerning, not least due to the ubiquity of hazard ratios in analyses of time-to-event data. We review these criticisms, describe how we think hazard ratios should be interpreted, and argue that they retain a valid causal interpretation. Nevertheless, alternative measures may be preferable to describe effects of exposures or treatments on time-to-event outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09571v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan W. Bartlett, Dominic Magirr, Tim P. Morris</dc:creator>
    </item>
    <item>
      <title>Smoothing spline density estimation from doubly truncated data</title>
      <link>https://arxiv.org/abs/2601.09576</link>
      <description>arXiv:2601.09576v1 Announce Type: new 
Abstract: In Astronomy, Survival Analysis and Epidemiology, among many other fields, doubly truncated data often appear. Double truncation generally induces a sampling bias, so ordinary estimators may be inconsistent. In this paper, smoothing spline density estimation from doubly truncated data is investigated. For this purpose, an appropriate correction of the penalized likelihood that accounts for the sampling bias is considered. The theoretical properties of the estimator are discussed, and its practical performance is evaluated through simulations. Two real datasets are analyzed using the proposed method for illustrative purposes. Comparison to kernel density smoothing is included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09576v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Bamio, Jacobo de U\~na-\'Alvarez</dc:creator>
    </item>
    <item>
      <title>LARGE: A Locally Adaptive Regularization Approach for Estimating Gaussian Graphical Models</title>
      <link>https://arxiv.org/abs/2601.09686</link>
      <description>arXiv:2601.09686v1 Announce Type: new 
Abstract: The graphical Lasso (GLASSO) is a widely used algorithm for learning high-dimensional undirected Gaussian graphical models (GGM). Given i.i.d. observations from a multivariate normal distribution, GLASSO estimates the precision matrix by maximizing the log-likelihood with an \ell_1-penalty on the off-diagonal entries. However, selecting an optimal regularization parameter \lambda in this unsupervised setting remains a significant challenge. A well-known issue is that existing methods, such as out-of-sample likelihood maximization, select a single global \lambda and do not account for heterogeneity in variable scaling or partial variances. Standardizing the data to unit variances, although a common workaround, has been shown to negatively affect graph recovery. Addressing the problem of nodewise adaptive tuning in graph estimation is crucial for applications like computational neuroscience, where brain networks are constructed from highly heterogeneous, region-specific fMRI data.
  In this work, we develop Locally Adaptive Regularization for Graph Estimation (LARGE), an approach to adaptively learn nodewise tuning parameters to improve graph estimation and selection. In each block coordinate descent step of GLASSO, we augment the nodewise Lasso regression to jointly estimate the regression coefficients and error variance, which in turn guides the adaptive learning of nodewise penalties. In simulations, LARGE consistently outperforms benchmark methods in graph recovery, demonstrates greater stability across replications, and achieves the best estimation accuracy in the most difficult simulation settings. We demonstrate the practical utility of our method by estimating brain functional connectivity from a real fMRI data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09686v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ha Nguyen, Sumanta Basu</dc:creator>
    </item>
    <item>
      <title>Human-AI Co-design for Clinical Prediction Models</title>
      <link>https://arxiv.org/abs/2601.09072</link>
      <description>arXiv:2601.09072v1 Announce Type: cross 
Abstract: Developing safe, effective, and practically useful clinical prediction models (CPMs) traditionally requires iterative collaboration between clinical experts, data scientists, and informaticists. This process refines the often small but critical details of the model building process, such as which features/patients to include and how clinical categories should be defined. However, this traditional collaboration process is extremely time- and resource-intensive, resulting in only a small fraction of CPMs reaching clinical practice. This challenge intensifies when teams attempt to incorporate unstructured clinical notes, which can contain an enormous number of concepts. To address this challenge, we introduce HACHI, an iterative human-in-the-loop framework that uses AI agents to accelerate the development of fully interpretable CPMs by enabling the exploration of concepts in clinical notes. HACHI alternates between (i) an AI agent rapidly exploring and evaluating candidate concepts in clinical notes and (ii) clinical and domain experts providing feedback to improve the CPM learning process. HACHI defines concepts as simple yes-no questions that are used in linear models, allowing the clinical AI team to transparently review, refine, and validate the CPM learned in each round. In two real-world prediction tasks (acute kidney injury and traumatic brain injury), HACHI outperforms existing approaches, surfaces new clinically relevant concepts not included in commonly-used CPMs, and improves model generalizability across clinical sites and time periods. Furthermore, HACHI reveals the critical role of the clinical AI team, such as directing the AI agent to explore concepts that it had not previously considered, adjusting the granularity of concepts it considers, changing the objective function to better align with the clinical objectives, and identifying issues of data bias and leakage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09072v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean Feng, Avni Kothari, Patrick Vossler, Andrew Bishara, Lucas Zier, Newton Addo, Aaron Kornblith, Yan Shuo Tan, Chandan Singh</dc:creator>
    </item>
    <item>
      <title>Adaptive Neyman Allocation</title>
      <link>https://arxiv.org/abs/2309.08808</link>
      <description>arXiv:2309.08808v5 Announce Type: replace 
Abstract: In the experimental design literature, Neyman allocation refers to the practice of allocating units into treated and control groups, potentially in unequal numbers proportional to their respective standard deviations, with the objective of minimizing the variance of the treatment effect estimator. This widely recognized approach increases statistical power in scenarios where the treated and control groups have different standard deviations, as is often the case in social experiments, clinical trials, marketing research, and online A/B testing. However, Neyman allocation cannot be implemented unless the standard deviations are known in advance. Fortunately, the multi-stage nature of the aforementioned applications allows the use of earlier stage observations to estimate the standard deviations, which further guide allocation decisions in later stages. In this paper, we introduce a competitive analysis framework to study this multi-stage experimental design problem. We propose a simple adaptive Neyman allocation algorithm, which almost matches the information-theoretic limit of conducting experiments. We provide theory for estimation and inference using data collected from our adaptive Neyman allocation algorithm. We demonstrate the effectiveness of our adaptive Neyman allocation algorithm using both online A/B testing data from a social media site and synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08808v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>Choosing Covariate Balancing Methods for Causal Inference: Practical Insights from a Simulation Study</title>
      <link>https://arxiv.org/abs/2412.00280</link>
      <description>arXiv:2412.00280v2 Announce Type: replace 
Abstract: Background: Inverse probability of treatment weighting (IPTW) is used for confounding adjustment in observational studies. Newer weighting methods include energy balancing (EB), kernel optimal matching (KOM), and tailored-loss covariate balancing propensity scores (TLF), but practical guidance remains limited. We evaluate their performance when implemented according to published recommendations.
  Methods: We conducted Monte Carlo simulations across 36 scenarios varying sample size, treatment prevalence, and a complexity factor increasing confounding and reducing overlap. Data generation used predominantly categorical covariates with some correlation. Average treatment effect and average treatment effect on the treated were estimated using IPTW, EB, KOM, and TLF combined with weighted least squares and, when supported, a doubly robust (DR) estimators. Inference followed published recommendations for each method when feasible, using standard alternatives otherwise. \textsc{PROBITsim} dataset used for illustration.
  Results: DR reduced sensitivity to the weighting scheme with an outcome regression adjusted for all confounders, despite functional-form misspecification. EB and KOM were most reliable; EB was tuning-free but scale dependent, whereas KOM required kernel and penalty choices. IPTW was variance sensitive when treatment prevalence was far from 50\%. TLF traded lower variance for higher bias, producing an RMSE plateau and sub-nominal confidence interval coverage. \textsc{PROBITsim} results mirrored these patterns.
  Conclusions: Rather than identifying a best method, our findings highlight failure modes and tuning choices to monitor. When the outcome regression adjusts for all confounders, DR estimation can be dependable across weighting schemes. Incorporating weight-estimation uncertainty into confidence intervals remains a key challenge for newer approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00280v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Etienne Peyrot, Rapha\"el Porcher, Francois Petit</dc:creator>
    </item>
    <item>
      <title>Estimation of time-varying treatment effects using marginal structural models dependent on partial treatment history</title>
      <link>https://arxiv.org/abs/2412.08042</link>
      <description>arXiv:2412.08042v3 Announce Type: replace 
Abstract: Inverse probability (IP) weighting of marginal structural models (MSMs) can provide consistent estimators of time-varying treatment effects under correct model specifications and identifiability assumptions, even in the presence of time-varying confounding. However, this method has two problems: (i) inefficiency due to IP-weights cumulating all time points and (ii) bias and inefficiency due to the MSM misspecification. To address these problems, we propose (i) new IP-weights for estimating parameters of the MSM that depends on partial treatment history and (ii) closed testing procedures for selecting partial treatment history (how far back in time the MSM depends on past treatments). We derive the theoretical properties of our proposed methods under known IP-weights and discuss their extension to estimated IP-weights. Although some of our theoretical results are derived under additional assumptions beyond standard identifiability assumptions, some of which can be checked empirically from the data. In simulation studies, our proposed methods outperformed existing methods both in terms of performance in estimating time-varying treatment effects and in selecting partial treatment history. Our proposed methods have also been applied to real data of hemodialysis patients with reasonable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08042v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nodoka Seya, Masataka Taguri, Takeo Ishii</dc:creator>
    </item>
    <item>
      <title>REML implementations of kernel-based genomic prediction models for genotype x environment x management interactions</title>
      <link>https://arxiv.org/abs/2501.06844</link>
      <description>arXiv:2501.06844v2 Announce Type: replace 
Abstract: High-throughput pheno-, geno-, and envirotyping allows characterization of plant genotypes and the trials they are evaluated in, producing different types of -omics data. These different data modalities can be integrated into statistical or machine learning models for genomic prediction in several ways. One commonly used approach within the analysis of multi-environment trial data in plant breeding is to create linear or nonlinear kernels which are subsequently used in linear mixed models (LMMs) to model genotype by environment (GxE) interactions. Current implementations of these kernel-based LMMs present a number of opportunities in terms of methodological extensions. Here we show how these models can be implemented in standard software, allowing direct restricted maximum likelihood (REML) estimation of all parameters. We also extend the models by combining the kernels with unstructured covariance matrices for three-way interactions in genotype by environment by management (GxExM) datasets, while simultaneously allowing for environment-specific genetic variances. We show how the models incorporating nonlinear kernels and heterogeneous variances maximize the amount of genetic variance captured by environmental covariables and perform best in prediction settings. We discuss the opportunities regarding models with multiple kernels or kernels obtained after environmental feature selection, as well as the similarities to models regressing phenotypes on latent and observed environmental covariables. Finally, we discuss the flexibility provided by our implementation in terms of modeling complex plant breeding datasets, allowing for straightforward integration of phenomics, enviromics, and genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06844v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Killian A. C. Melsen, Salvador Gezan, Daniel J. Tolhurst, Fred A. van Eeuwijk, Carel F. W. Peeters</dc:creator>
    </item>
    <item>
      <title>Residual lifetime prediction for heterogeneous degradation data by Bayesian semi-parametric method</title>
      <link>https://arxiv.org/abs/2504.15794</link>
      <description>arXiv:2504.15794v2 Announce Type: replace 
Abstract: Degradation data are considered for assessing reliability in highly reliable systems. The usual assumption is that degradation units come from a homogeneous population. But in presence of high variability in the manufacturing process, this assumption is not true in general; that is different sub-populations are involved in the study. Predicting residual lifetime of a functioning unit is a major challenge in the degradation modeling especially in heterogeneous environment. To account for heterogeneous degradation data, we have proposed a Bayesian semi-parametric approach to relax the conventional modeling assumptions. We model the degradation path using Dirichlet process mixture of normal distributions. Based on the samples obtained from posterior distribution of model parameters we obtain residual lifetime distribution for individual unit. Transformation based MCMC technique is used for simulating values from the derived residual lifetime distribution for prediction of residual lifetime. A simulation study is undertaken to check performance of the proposed semi-parametric model compared with parametric model. Fatigue Crack Size data is analyzed to illustrate the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15794v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Barin Karmakar, Biswabrata Pradhan</dc:creator>
    </item>
    <item>
      <title>Proper Correlation Coefficients for Nominal Random Variables</title>
      <link>https://arxiv.org/abs/2505.00785</link>
      <description>arXiv:2505.00785v2 Announce Type: replace 
Abstract: This paper develops an intuitive concept of perfect dependence between two variables of which at least one has a nominal scale. Perfect dependence is attainable for all marginal distributions. It furthermore proposes a set of dependence measures that are 1 if and only if this perfect dependence is satisfied. The advantages of these dependence measures relative to classical dependence measures like contingency coefficients, Goodman-Kruskal's lambda and tau and the so-called uncertainty coefficient are twofold. Firstly, they are defined if one of the variables exhibits continuities. Secondly, they satisfy the property of attainability. That is, they can take all values in the interval [0,1] irrespective of the marginals involved. Both properties are not shared by classical dependence measures which need two discrete marginal distributions and can in some situations yield values close to 0 even though the dependence is strong or even perfect. Additionally, the paper provides a consistent estimator for one of the new dependence measures together with its asymptotic distribution under independence as well as in the general case. This allows to construct confidence intervals and an independence test with good finite sample properties, as a subsequent simulation study shows. Finally, two applications on the dependence between the variables country and income, and country and religion, respectively, illustrate the use of the new measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00785v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Lukas Wermuth</dc:creator>
    </item>
    <item>
      <title>Coarse-to-fine spatial modeling: A scalable, machine-learning-compatible spatial model</title>
      <link>https://arxiv.org/abs/2510.00968</link>
      <description>arXiv:2510.00968v2 Announce Type: replace 
Abstract: This study proposes coarse-to-fine spatial modeling (CFSM) as a scalable and machine learning-compatible alternative to conventional spatial process models. Unlike conventional covariance-based spatial models, CFSM represents spatial processes using a multiscale ensemble of local models. To ensure stable model training, larger-scale patterns that are easier to learn are modeled first, followed by smaller-scale patterns, with training terminated once the validation score stops improving. The training procedure, which is based on holdout validation, can be easily integrated with other machine learning algorithms, including random forests and neural networks. CFSM training is computationally efficient because it avoids explicit matrix inversion, which is a major computational bottleneck in conventional spatial Gaussian processes. Comparative Monte Carlo experiments demonstrated that the CFSM, as well as its integration with random forests, achieved superior predictive performance compared to existing models. Finally, we applied the proposed methods to an analysis of residential land prices in the Tokyo metropolitan area, Japan. The CFSM is implemented in an R package spCF (https://cran.r-project.org/web/packages/spCF/).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00968v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daisuke Murakami, Alexis Comber, Takahiro Yoshida, Narumasa Tsutsumida, Chris Brunsdon, Tomoki Nakaya</dc:creator>
    </item>
    <item>
      <title>A General Framework for Joint Multi-State Models</title>
      <link>https://arxiv.org/abs/2510.07128</link>
      <description>arXiv:2510.07128v2 Announce Type: replace 
Abstract: Classical joint modeling approaches often rely on competing risks or recurrent event formulations to describe complex processes involving evolving longitudinal biomarkers and discrete event occurrences, but these frameworks typically capture only limited aspects of the underlying event dynamics. We propose a general multi-state joint modeling framework that unifies longitudinal biomarker dynamics with multi-state time-to-event processes defined on arbitrary directed graphs. The proposed framework accommodates arbitrary directed transition graphs, nonlinear longitudinal submodels, and scalable inference via stochastic gradient descent. This formulation encompasses both Markovian and semi-Markovian transition structures, allowing recurrent cycles and terminal absorptions to be naturally represented. The longitudinal and event processes are linked through shared latent structures within nonlinear mixed-effects models, extending classical joint modeling formulations. We derive the complete likelihood, establish conditions for identifiability, and develop scalable inference procedures based on stochastic gradient descent to enable high-dimensional and large-scale applications. In addition, we formulate a dynamic prediction framework that provides individualized state-transition probabilities and personalized risk assessments along complex event trajectories. Through simulation and application to the PAQUID cohort, we demonstrate accurate parameter recovery and individualized prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07128v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>F\'elix Laplante, Christophe Ambroise</dc:creator>
    </item>
    <item>
      <title>Multivariable Bidirectional Mendelian Randomization via Bayesian Directed Cyclic Graphical Models with Correlated Errors</title>
      <link>https://arxiv.org/abs/2510.09991</link>
      <description>arXiv:2510.09991v2 Announce Type: replace 
Abstract: Mendelian randomization (MR) is a pivotal tool in genetics, genomics, and epidemiology, leveraging genetic variants as instrumental variables to infer causal relationships between exposures and outcomes. Traditional MR methods, while powerful, often rely on stringent assumptions such as the absence of feedback loops, which are frequently violated in complex biological networks. In addition, many popular MR approaches focus on only two variables (i.e., one exposure and one outcome), whereas our motivating applications of gene regulatory networks have many variables. In this article, we introduce a novel Bayesian framework for multivariable MR that concurrently addresses unmeasured confounding and feedback loops. Central to our approach is a sparse conditional cyclic graphical model with a sparse error variance-covariance matrix. Two structural priors are employed to enable the modeling and inference of causal relationships as well as latent confounding structures. Our method is designed to operate effectively with summary-level data, facilitating its application in contexts where individual-level data are inaccessible, e.g., due to privacy concerns. It can also account for horizontal pleiotropy, under which we establish the sufficient identifiability conditions. Through extensive simulations and applications to the GTEx and OneK1K data, we demonstrate the superior performance of our approach in recovering biologically plausible causal relationships in the presence of possible feedback loops and unmeasured confounding. Using posterior samples, we further quantify uncertainty in inferred network motifs by computing their posterior probabilities. The R package MR.RGM that implements the proposed method is available on CRAN (https://cran.r-project.org/package=MR.RGM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09991v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bitan Sarkar, Yuchao Jiang, Tian Ge, Yang Ni</dc:creator>
    </item>
    <item>
      <title>Change-in-velocity detection for multidimensional data</title>
      <link>https://arxiv.org/abs/2510.27150</link>
      <description>arXiv:2510.27150v3 Announce Type: replace 
Abstract: In this work, we introduce CPLASS (Continuous Piecewise-Linear Approximation via Stochastic Search), an algorithm for detecting changes in velocity within multidimensional data. The one-dimensional version of this problem is known as the change-in-slope problem (see Fearnhead &amp; Grose, 2022; Baranowski et al., 2019). Unlike traditional changepoint detection methods that focus on changes in mean, detecting changes in velocity requires a specialized approach due to continuity constraints and parameter dependencies, which frustrate popular algorithms like binary segmentation and dynamic programming. To overcome these difficulties, we introduce a specialized penalty function to balance improvements in likelihood due to model complexity, and a Markov Chain Monte Carlo (MCMC)-based approach with tailored proposal mechanisms for efficient parameter exploration. Our method is particularly suited for analyzing intracellular transport data, where the multidimensional trajectories of microscale cargo are driven by teams of molecular motors that undergo complex biophysical transitions. To ensure biophysical realism in the results, we introduce a speed penalty that discourages overfitted of short noisy segments while maintaining consistency in the large-sample limit. Additionally, we introduce a summary statistic called the Cumulative Speed Allocation, which is robust with respect to idiosyncracies of changepoint detection while maintaining the ability to discriminate between biophysically distinct populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27150v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linh Do, Dat Do, Keisha J. Cook, Scott A. McKinley</dc:creator>
    </item>
    <item>
      <title>Decoder-only Clustering in Graphs with Dynamic Attributes</title>
      <link>https://arxiv.org/abs/2511.04859</link>
      <description>arXiv:2511.04859v2 Announce Type: replace 
Abstract: This manuscript studies nodal clustering in graphs having a time series at each node. The proposed framework includes priors for low-dimensional representations and a decoder that bridges latent representations with time series. Addressing the limitation that the evolution of nodal attributes is often overlooked, temporal and structural patterns are fused into low-dimensional representations to facilitate clustering. Parameters are learned via maximum approximate likelihood, with a graph-fused LASSO regularization imposed on prior parameters. The optimization problem is solved via alternating direction method of multipliers; Langevin dynamics are employed for posterior inference. Simulation studies on block and grid graphs with autoregressive dynamics, and applications to California county temperatures and a word co-occurrence network demonstrate the effectiveness of the proposed clustering method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04859v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yik Lun Kei, Oscar Hernan Madrid Padilla, Rebecca Killick, James Wilson, Xi Chen, Robert Lund</dc:creator>
    </item>
    <item>
      <title>Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease</title>
      <link>https://arxiv.org/abs/2512.17340</link>
      <description>arXiv:2512.17340v2 Announce Type: replace 
Abstract: Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17340v2</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carter H. Nakamoto, Lucia Lushi Chen, Agata Foryciarz, Sherri Rose</dc:creator>
    </item>
    <item>
      <title>Trustworthy scientific inference with generative models</title>
      <link>https://arxiv.org/abs/2508.02602</link>
      <description>arXiv:2508.02602v3 Announce Type: replace-cross 
Abstract: Generative artificial intelligence (AI) excels at producing complex data structures (text, images, videos) by learning patterns from training examples. Across scientific disciplines, researchers are now applying generative models to "inverse problems" to directly predict hidden parameters from observed data along with measures of uncertainty. While these predictive or posterior-based methods can handle intractable likelihoods and large-scale studies, they can also produce biased or overconfident conclusions even without model misspecifications. We present a solution with Frequentist-Bayes (FreB), a mathematically rigorous protocol that reshapes AI-generated posterior probability distributions into (locally valid) confidence regions that consistently include true parameters with the expected probability, while achieving minimum size when training and target data align. We demonstrate FreB's effectiveness by tackling diverse case studies in the physical sciences: identifying unknown sources under dataset shift, reconciling competing theoretical models, and mitigating selection bias and systematics in observational studies. By providing validity guarantees with interpretable diagnostics, FreB enables trustworthy scientific inference across fields where direct likelihood evaluation remains impossible or prohibitively expensive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02602v3</guid>
      <category>stat.ML</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Carzon, Luca Masserano, Joshua D. Ingram, Alex Shen, Antonio Carlos Herling Ribeiro Junior, Tommaso Dorigo, Michele Doro, Joshua S. Speagle, Rafael Izbicki, Ann B. Lee</dc:creator>
    </item>
    <item>
      <title>Recidivism and Peer Influence with LLM Text Embeddings in Low Security Correctional Facilities</title>
      <link>https://arxiv.org/abs/2509.20634</link>
      <description>arXiv:2509.20634v2 Announce Type: replace-cross 
Abstract: Studying peer effects in language is critical because they often reflect behavioral and personality traits that are important determinants of economic outcomes. However, language is unstructured, non-numeric, and high-dimensional. We combine Large Language Model (LLM) embeddings with structural econometric identification to provide a unified framework for identifying peer effects in language. This unified framework is applied to 80,000-120,000 written exchanges among residents of low security correctional facilities. The LLM language profiles predict three-year recidivism 30\% more accurately than pre-entry covariates alone, showing that text representations capture meaningful signals. We analyze peer effects on multidimensional language embeddings while addressing network endogeneity. We develop novel instrumental variable estimators for peer effects that accommodate multivariate outcomes, sparse networks, and multidimensional latent variables. Our methods achieve root-N consistency and asymptotic normality under realistic sparsity conditions, relaxing the dense-network assumption. Results reveal significant peer effects in residents' language profiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20634v2</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shanjukta Nath, Jiwon Hong, Jae Ho Chang, Keith Warren, Subhadeep Paul</dc:creator>
    </item>
    <item>
      <title>Coupling Generative Modeling and an Autoencoder with the Causal Bridge</title>
      <link>https://arxiv.org/abs/2509.25599</link>
      <description>arXiv:2509.25599v2 Announce Type: replace-cross 
Abstract: We consider inferring the causal effect of a treatment (intervention) on an outcome of interest in situations where there is potentially an unobserved confounder influencing both the treatment and the outcome. This is achievable by assuming access to two separate sets of control (proxy) measurements associated with treatment and outcomes, which are used to estimate treatment effects through a function termed the em causal bridge (CB). We present a new theoretical perspective, associated assumptions for when estimating treatment effects with the CB is feasible, and a bound on the average error of the treatment effect when the CB assumptions are violated. From this new perspective, we then demonstrate how coupling the CB with an autoencoder architecture allows for the sharing of statistical strength between observed quantities (proxies, treatment, and outcomes), thus improving the quality of the CB estimates. Experiments on synthetic and real-world data demonstrate the effectiveness of the proposed approach in relation to the state-of-the-art methodology for proxy measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25599v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruolin Meng, Ming-Yu Chung, Dhanajit Brahma, Ricardo Henao, Lawrence Carin</dc:creator>
    </item>
    <item>
      <title>Why are there many equally good models? An Anatomy of the Rashomon Effect</title>
      <link>https://arxiv.org/abs/2601.06730</link>
      <description>arXiv:2601.06730v2 Announce Type: replace-cross 
Abstract: The Rashomon effect -- the existence of multiple, distinct models that achieve nearly equivalent predictive performance -- has emerged as a fundamental phenomenon in modern machine learning and statistics. In this paper, we explore the causes underlying the Rashomon effect, organizing them into three categories: statistical sources arising from finite samples and noise in the data-generating process; structural sources arising from non-convexity of optimization objectives and unobserved variables that create fundamental non-identifiability; and procedural sources arising from limitations of optimization algorithms and deliberate restrictions to suboptimal model classes. We synthesize insights from machine learning, statistics, and optimization literature to provide a unified framework for understanding why the multiplicity of good models arises. A key distinction emerges: statistical multiplicity diminishes with more data, structural multiplicity persists asymptotically and cannot be resolved without different data or additional assumptions, and procedural multiplicity reflects choices made by practitioners. Beyond characterizing causes, we discuss both the challenges and opportunities presented by the Rashomon effect, including implications for inference, interpretability, fairness, and decision-making under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06730v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Harsh Parikh</dc:creator>
    </item>
  </channel>
</rss>
