<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Nov 2024 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Modelling Alzheimer's Protein Dynamics: A Data-Driven Integration of Stochastic Methods, Machine Learning and Connectome Insights</title>
      <link>https://arxiv.org/abs/2411.02644</link>
      <description>arXiv:2411.02644v1 Announce Type: new 
Abstract: Alzheimer's disease (AD) is a complex neurodegenerative disorder characterized by the progressive accumulation of misfolded proteins, leading to cognitive decline. This study presents a novel stochastic modelling approach to simulate the propagation of these proteins within the brain. We employ a network diffusion model utilizing the Laplacian matrix derived from MRI data provided by the Human Connectome Project (https://braingraph.org/cms/). The deterministic model is extended by incorporating stochastic differential equations (SDEs) to account for inherent uncertainties in disease progression. Introducing stochastic components into the model allows for a more realistic simulation of the disease due to the multi-factorial nature of AD. By simulation, the model captures the variability in misfolded protein concentration across brain regions over time. Bayesian inference is a statistical method that uses prior beliefs and given data to model a posterior distribution for relevant parameter values. This allows us to better understand the impact of noise and external factors on AD progression. Deterministic results suggest that AD progresses at different speeds within each lobe of the brain, moreover, the frontal takes the longest to reach a perfect disease state. We find that in the presence of noise, the model never reaches a perfect disease state and the later years of AD are more unpredictable than earlier on in the disease. These results highlight the importance of integrating stochastic elements into deterministic models to achieve more realistic simulations, providing valuable insights for future studies on the dynamics of neurodegenerative diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02644v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec MacIver, Hina Shaheen</dc:creator>
    </item>
    <item>
      <title>Geometry of naturalistic object representations in recurrent neural network models of working memory</title>
      <link>https://arxiv.org/abs/2411.02685</link>
      <description>arXiv:2411.02685v1 Announce Type: cross 
Abstract: Working memory is a central cognitive ability crucial for intelligent decision-making. Recent experimental and computational work studying working memory has primarily used categorical (i.e., one-hot) inputs, rather than ecologically relevant, multidimensional naturalistic ones. Moreover, studies have primarily investigated working memory during single or few cognitive tasks. As a result, an understanding of how naturalistic object information is maintained in working memory in neural networks is still lacking. To bridge this gap, we developed sensory-cognitive models, comprising a convolutional neural network (CNN) coupled with a recurrent neural network (RNN), and trained them on nine distinct N-back tasks using naturalistic stimuli. By examining the RNN's latent space, we found that: (1) Multi-task RNNs represent both task-relevant and irrelevant information simultaneously while performing tasks; (2) The latent subspaces used to maintain specific object properties in vanilla RNNs are largely shared across tasks, but highly task-specific in gated RNNs such as GRU and LSTM; (3) Surprisingly, RNNs embed objects in new representational spaces in which individual object features are less orthogonalized relative to the perceptual space; (4) The transformation of working memory encodings (i.e., embedding of visual inputs in the RNN latent space) into memory was shared across stimuli, yet the transformations governing the retention of a memory in the face of incoming distractor stimuli were distinct across time. Our findings indicate that goal-driven RNNs employ chronological memory subspaces to track information over short time spans, enabling testable predictions with neural data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02685v1</guid>
      <category>cs.AI</category>
      <category>cs.CG</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoxuan Lei, Takuya Ito, Pouya Bashivan</dc:creator>
    </item>
    <item>
      <title>Back to the Continuous Attractor</title>
      <link>https://arxiv.org/abs/2408.00109</link>
      <description>arXiv:2408.00109v2 Announce Type: replace 
Abstract: Continuous attractors offer a unique class of solutions for storing continuous-valued variables in recurrent system states for indefinitely long time intervals. Unfortunately, continuous attractors suffer from severe structural instability in general--they are destroyed by most infinitesimal changes of the dynamical law that defines them. This fragility limits their utility especially in biological systems as their recurrent dynamics are subject to constant perturbations. We observe that the bifurcations from continuous attractors in theoretical neuroscience models display various structurally stable forms. Although their asymptotic behaviors to maintain memory are categorically distinct, their finite-time behaviors are similar. We build on the persistent manifold theory to explain the commonalities between bifurcations from and approximations of continuous attractors. Fast-slow decomposition analysis uncovers the persistent manifold that survives the seemingly destructive bifurcation. Moreover, recurrent neural networks trained on analog memory tasks display approximate continuous attractors with predicted slow manifold structures. Therefore, continuous attractors are functionally robust and remain useful as a universal analogy for understanding analog memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00109v2</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <category>nlin.AO</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>In Proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2024)</arxiv:journal_reference>
      <dc:creator>\'Abel S\'agodi, Guillermo Mart\'in-S\'anchez, Piotr Sok\'o\l, Il Memming Park</dc:creator>
    </item>
    <item>
      <title>Generative AI Enables EEG Super-Resolution via Spatio-Temporal Adaptive Diffusion Learning</title>
      <link>https://arxiv.org/abs/2407.03089</link>
      <description>arXiv:2407.03089v4 Announce Type: replace-cross 
Abstract: Electroencephalogram (EEG) technology, particularly high-density EEG (HD EEG) devices, are widely used in fields such as neuroscience. HD EEG devices improve the spatial resolution of EEG by placing more electrodes on the scalp, which meet the requirements of clinical diagnostic applications such as epilepsy focus localization. However, this technique faces challenges, such as high acquisition costs and limited usage scenarios. In this paper, spatio-temporal adaptive diffusion models (STAD) are proposed to pioneer the use of diffusion models for achieving spatial SR reconstruction from low-resolution (LR, 64 channels or fewer) EEG to high-resolution (HR, 256 channels) EEG. Specifically, a spatio-temporal condition module is designed to extract the spatio-temporal features of LR EEG, which then used as conditional inputs to direct the reverse denoising process. Additionally, a multi-scale Transformer denoising module is constructed to leverage multi-scale convolution blocks and cross-attention-based diffusion Transformer blocks for conditional guidance to generate subject-adaptive SR EEG. Experimental results demonstrate that the STAD significantly enhances the spatial resolution of LR EEG and quantitatively outperforms existing methods. Furthermore, STAD demonstrate their value by applying synthetic SR EEG to classification and source localization tasks, indicating their potential to Substantially boost the spatial resolution of EEG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03089v4</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 06 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tong Zhou, Shuqiang Wang</dc:creator>
    </item>
  </channel>
</rss>
