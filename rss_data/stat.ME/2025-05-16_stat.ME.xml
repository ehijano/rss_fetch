<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 May 2025 04:00:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Forests for Differences: Robust Causal Inference Beyond Parametric DiD</title>
      <link>https://arxiv.org/abs/2505.09706</link>
      <description>arXiv:2505.09706v1 Announce Type: new 
Abstract: This paper introduces the Difference-in-Differences Bayesian Causal Forest (DiD-BCF), a novel non-parametric model addressing key challenges in DiD estimation, such as staggered adoption and heterogeneous treatment effects. DiD-BCF provides a unified framework for estimating Average (ATE), Group-Average (GATE), and Conditional Average Treatment Effects (CATE). A core innovation, its Parallel Trends Assumption (PTA)-based reparameterization, enhances estimation accuracy and stability in complex panel data settings. Extensive simulations demonstrate DiD-BCF's superior performance over established benchmarks, particularly under non-linearity, selection biases, and effect heterogeneity. Applied to U.S. minimum wage policy, the model uncovers significant conditional treatment effect heterogeneity related to county population, insights obscured by traditional methods. DiD-BCF offers a robust and versatile tool for more nuanced causal inference in modern DiD applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09706v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hugo Gobato Souto, Francisco Louzada Neto</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for Neyman-Scott point processes with anisotropic clusters</title>
      <link>https://arxiv.org/abs/2505.09786</link>
      <description>arXiv:2505.09786v1 Announce Type: new 
Abstract: There are few inference methods available to accommodate covariate-dependent anisotropy in point process models. To address this, we propose an extended Bayesian MCMC approach for Neyman-Scott cluster processes. We focus on anisotropy and inhomogeneity in the offspring distribution. Our approach provides parameter estimates as well as significance tests for the covariates and anisotropy through credible intervals, which are determined by the posterior distributions. Additionally, it is possible to test the hypothesis of constant orientation of clusters or constant elongation of clusters. We demonstrate the applicability of this approach through a simulation study for a Thomas-type cluster process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09786v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ji\v{r}\'i Dvo\v{r}\'ak, Emily Ewers, Tom\'a\v{s} Mrkvi\v{c}ka, Claudia Redenbach</dc:creator>
    </item>
    <item>
      <title>Inference for Dispersion and Curvature of Random Objects</title>
      <link>https://arxiv.org/abs/2505.09844</link>
      <description>arXiv:2505.09844v1 Announce Type: new 
Abstract: There are many open questions pertaining to the statistical analysis of random objects, which are increasingly encountered. A major challenge is the absence of linear operations in such spaces. A basic statistical task is to quantify statistical dispersion or spread. For two measures of dispersion for data objects in geodesic metric spaces, Fr\'echet variance and metric variance, we derive a central limit theorem (CLT) for their joint distribution. This analysis reveals that the Alexandrov curvature of the geodesic space determines the relationship between these two dispersion measures. This suggests a novel test for inferring the curvature of a space based on the asymptotic distribution of the dispersion measures. We demonstrate how this test can be employed to detect the intrinsic curvature of an unknown underlying space, which emerges as a joint property of the space and the underlying probability measure that generates the random objects. We investigate the asymptotic properties of the test and its finite-sample behavior for various data types, including distributional data and point cloud data. We illustrate the proposed inference for intrinsic curvature of random objects using gait synchronization data represented as symmetric positive definite matrices and energy compositional data on the sphere.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09844v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wookyeong Song, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Robust and Computationally Efficient Trimmed L-Moments Estimation for Parametric Distributions</title>
      <link>https://arxiv.org/abs/2505.09860</link>
      <description>arXiv:2505.09860v1 Announce Type: new 
Abstract: This paper proposes a robust and computationally efficient estimation framework for fitting parametric distributions based on trimmed L-moments. Trimmed L-moments extend classical L-moment theory by downweighting or excluding extreme order statistics, resulting in estimators that are less sensitive to outliers and heavy tails. We construct estimators for both location-scale and shape parameters using asymmetric trimming schemes tailored to different moments, and establish their asymptotic properties for inferential justification using the general structural theory of L-statistics, deriving simplified single-integration expressions to ensure numerical stability. State-of-the-art algorithms are developed to resolve the sign ambiguity in estimating the scale parameter for location-scale models and the tail index for the Frechet model. The proposed estimators offer improved efficiency over traditional robust alternatives for selected asymmetric trimming configurations, while retaining closed-form expressions for a wide range of common distributions, facilitating fast and stable computation. Simulation studies demonstrate strong finite-sample performance. An application to financial claim severity modeling highlights the practical relevance and flexibility of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09860v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chudamani Poudyal, Qian Zhao, Hari Sitaula</dc:creator>
    </item>
    <item>
      <title>Closed-form solutions for parameter estimation in exponential families based on maximum a posteriori equations</title>
      <link>https://arxiv.org/abs/2505.09871</link>
      <description>arXiv:2505.09871v1 Announce Type: new 
Abstract: In this paper, we derive closed-form estimators for the parameters of certain exponential family distributions through the maximum a posteriori (MAP) equations. A Monte Carlo simulation is conducted to assess the performance of the proposed estimators. The results show that, as expected, their accuracy improves with increasing sample size, with both bias and mean squared error approaching zero. Moreover, the proposed estimators exhibit performance comparable to that of traditional MAP and maximum likelihood (ML) estimators. A notable advantage of the proposed method lies in its computational simplicity, as it eliminates the need for numerical optimization required by MAP and ML estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09871v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo, Eduardo Nakano</dc:creator>
    </item>
    <item>
      <title>A mover-stayer model with time-dependent stayer fraction</title>
      <link>https://arxiv.org/abs/2505.10065</link>
      <description>arXiv:2505.10065v1 Announce Type: new 
Abstract: Mover-stayer models are used in social sciences and economics to model heterogeneous population dynamics in which some individuals never experience the event of interest ("stayers"), while others transition between states over time ("movers"). Conventionally, the mover-stayer status is determined at baseline and time-dependent covariates are only incorporated in the movers' transition probabilities. In this paper, we present a novel dynamic version of the mover-stayer model, allowing potential movers to become stayers over time based on time-varying circumstances. Using a multinomial logistic framework, our model incorporates both time-fixed and exogenous time-varying covariates to estimate transition probabilities among the states of potential movers, movers, and stayers. Both the initial state and transitions to the stayer state are treated as latent. The introduction of this new model is motivated by the study of student mobility. Specifically focusing on panel data on the inter-university mobility of Italian students, factors such as the students' change of course and university size are considered as time-varying covariates in modelling their probability of moving or becoming stayers; sex and age at enrolment as time-fixed covariates. We propose a maximum likelihood estimation approach and investigate its finite-sample performance through simulations, comparing it to established models in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10065v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eni Musta, Martina Vittorietti</dc:creator>
    </item>
    <item>
      <title>Estimating the number of household TV profiles based in customer behaviour using Gaussian mixture model averaging</title>
      <link>https://arxiv.org/abs/2505.10279</link>
      <description>arXiv:2505.10279v1 Announce Type: new 
Abstract: TV customers today face many choices from many live channels and on-demand services. Providing a personalised experience that saves customers time when discovering content is essential for TV providers. However, a reliable understanding of their behaviour and preferences is key. When creating personalised recommendations for TV, the biggest challenge is understanding viewing behaviour within households when multiple people are watching. The objective is to detect and combine individual profiles to make better-personalised recommendations for group viewing. Our challenge is that we have little explicit information about who is watching the devices at any time (individuals or groups). Also, we do not have a way to combine more than one individual profile to make better recommendations for group viewing. We propose a novel framework using a Gaussian mixture model averaging to obtain point estimates for the number of household TV profiles and a Bayesian random walk model to introduce uncertainty. We applied our approach using data from real customers whose TV-watching data totalled approximately half a million observations. Our results indicate that combining our framework with the selected features provides a means to estimate the number of household TV profiles and their characteristics, including shifts over time and quantification of uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10279v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel R. Palma, Sally McClean, Brahim Allan, Zeeshan Tariq, Rafael A. Moral</dc:creator>
    </item>
    <item>
      <title>Causal discovery on vector-valued variables and consistency-guided aggregation</title>
      <link>https://arxiv.org/abs/2505.10476</link>
      <description>arXiv:2505.10476v1 Announce Type: new 
Abstract: Causal discovery (CD) aims to discover the causal graph underlying the data generation mechanism of observed variables. In many real-world applications, the observed variables are vector-valued, such as in climate science where variables are defined over a spatial grid and the task is called spatio-temporal causal discovery. We motivate CD in vector-valued variable setting while considering different possibilities for the underlying model, and highlight the pitfalls of commonly-used approaches when compared to a fully vectorized approach. Furthermore, often the vector-valued variables are high-dimensional, and aggregations of the variables, such as averages, are considered in interest of efficiency and robustness. In the absence of interventional data, testing for the soundness of aggregate variables as consistent abstractions that map a low-level to a high-level structural causal model (SCM) is hard, and recent works have illustrated the stringency of conditions required for testing consistency. In this work, we take a careful look at the task of vector-valued CD via constraint-based methods, focusing on the problem of consistency of aggregation for this task. We derive three aggregation consistency scores, based on compatibility of independence models and (partial) aggregation, that quantify different aspects of the soundness of an aggregation map for the CD problem. We present the argument that the consistency of causal abstractions must be separated from the task-dependent consistency of aggregation maps. As an actionable conclusion of our findings, we propose a wrapper Adag to optimize a chosen aggregation consistency score for aggregate-CD, to make the output of CD over aggregate variables more reliable. We supplement all our findings with experimental evaluations on synthetic non-time series and spatio-temporal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10476v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Urmi Ninad, Jonas Wahl, Andreas Gerhardus, Jakob Runge</dc:creator>
    </item>
    <item>
      <title>Efficient Uncertainty Propagation in Bayesian Two-Step Procedures</title>
      <link>https://arxiv.org/abs/2505.10510</link>
      <description>arXiv:2505.10510v1 Announce Type: new 
Abstract: Bayesian inference provides a principled framework for probabilistic reasoning. If inference is performed in two steps, uncertainty propagation plays a crucial role in accounting for all sources of uncertainty and variability. This becomes particularly important when both aleatoric uncertainty, caused by data variability, and epistemic uncertainty, arising from incomplete knowledge or missing data, are present. Examples include surrogate models and missing data problems. In surrogate modeling, the surrogate is used as a simplified approximation of a resource-heavy and costly simulation. The uncertainty from the surrogate-fitting process can be propagated using a two-step procedure. For modeling with missing data, methods like Multivariate Imputation by Chained Equations (MICE) generate multiple datasets to account for imputation uncertainty. These approaches, however, are computationally expensive, as multiple models must be fitted separately to surrogate parameters respectively imputed datasets.
  To address these challenges, we propose an efficient two-step approach that reduces computational overhead while maintaining accuracy. By selecting a representative subset of draws or imputations, we construct a mixture distribution to approximate the desired posteriors using Pareto smoothed importance sampling. For more complex scenarios, this is further refined with importance weighted moment matching and an iterative procedure that broadens the mixture distribution to better capture diverse posterior distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10510v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Svenja Jedhoff, Hadi Kutabi, Anne Meyer, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>On the Foundations of the Design-Based Approach</title>
      <link>https://arxiv.org/abs/2505.10519</link>
      <description>arXiv:2505.10519v1 Announce Type: new 
Abstract: The design-based paradigm may be adopted in causal queries and survey sampling when we assume Rubin's stable unit treatment value assumption (SUTVA) or impose similar frameworks. While often taken for granted, such assumptions entail strong claims about the data generating process. We develop an alternative design-based approach: we first invoke a generalized, non-parametric model that allows for unrestricted forms of interference, such as spillover. We define a new set of inferential targets and discuss their interpretation under SUTVA and a weaker assumption that we call the No Unmodeled Revealable Variation Assumption (NURVA). We then reconstruct the standard paradigm, reconsidering SUTVA at the end rather than assuming it at the beginning. Despite its similarity to SUTVA, we demonstrate the practical insufficiency of NURVA in identifying substantively interesting quantities. In so doing, we provide clarity on the nature and importance of SUTVA for applied research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10519v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. M. Aronow, Austin Jang, Molly Offer-Westort</dc:creator>
    </item>
    <item>
      <title>LiDDA: Data Driven Attribution at LinkedIn</title>
      <link>https://arxiv.org/abs/2505.09861</link>
      <description>arXiv:2505.09861v1 Announce Type: cross 
Abstract: Data Driven Attribution, which assigns conversion credits to marketing interactions based on causal patterns learned from data, is the foundation of modern marketing intelligence and vital to any marketing businesses and advertising platform. In this paper, we introduce a unified transformer-based attribution approach that can handle member-level data, aggregate-level data, and integration of external macro factors. We detail the large scale implementation of the approach at LinkedIn, showcasing significant impact. We also share learning and insights that are broadly applicable to the marketing and ad tech fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09861v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Bencina, Erkut Aykutlug, Yue Chen, Zerui Zhang, Stephanie Sorenson, Shao Tang, Changshuai Wei</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian hierarchical models for basket trials enabling joint evaluation of toxicity and efficacy</title>
      <link>https://arxiv.org/abs/2505.10317</link>
      <description>arXiv:2505.10317v1 Announce Type: cross 
Abstract: Basket trials have gained increasing attention for their efficiency, as multiple patient subgroups are evaluated simultaneously. Conducted basket trials focus primarily on establishing the early efficacy of a treatment, yet continued monitoring of toxicity is essential. In this paper, we propose two Bayesian hierarchical models that enable bivariate analyses of toxicity and efficacy, while accounting for heterogeneity present in the treatment effects across patient subgroups. Specifically, one assumes the subgroup-specific toxicity and efficacy treatment effects, as a parameter vector, can be exchangeable or non-exchangeable; the other allows either the toxicity or efficacy parameters specific to the subgroups, to be exchangeable or non-exchangeable. The bivariate exchangeability and non-exchangeability distributions introduce a correlation parameter between treatment effects, while we stipulate a zero correlation when only toxicity or efficacy parameters are exchangeable. Simulation results show that our models perform robustly under different scenarios compared to the standard Bayesian hierarchical model and the stand-alone analyses, especially in producing higher power when the subgroup-specific effects are exchangeable in toxicity or efficacy only. When considerable correlation between the toxicity and efficacy effects exists, our methodology gives small error rates and greater power than alternatives that analyse toxicity and efficacy by parts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10317v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Cao, Pavel Mozgunov, Haiyan Zheng</dc:creator>
    </item>
    <item>
      <title>Optimal Post-Hoc Theorizing</title>
      <link>https://arxiv.org/abs/2505.10370</link>
      <description>arXiv:2505.10370v1 Announce Type: cross 
Abstract: For many economic questions, the empirical results are not interesting unless they are strong. For these questions, theorizing before the results are known is not always optimal. Instead, the optimal sequencing of theory and empirics trades off a ``Darwinian Learning'' effect from theorizing first with a ``Statistical Learning'' effect from examining the data first. This short paper formalizes the tradeoff in a Bayesian model. In the modern era of mature economic theory and enormous datasets, I argue that post hoc theorizing is typically optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10370v1</guid>
      <category>econ.EM</category>
      <category>q-fin.GN</category>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Y. Chen</dc:creator>
    </item>
    <item>
      <title>Batched Nonparametric Bandits via k-Nearest Neighbor UCB</title>
      <link>https://arxiv.org/abs/2505.10498</link>
      <description>arXiv:2505.10498v1 Announce Type: cross 
Abstract: We study sequential decision-making in batched nonparametric contextual bandits, where actions are selected over a finite horizon divided into a small number of batches. Motivated by constraints in domains such as medicine and marketing -- where online feedback is limited -- we propose a nonparametric algorithm that combines adaptive k-nearest neighbor (k-NN) regression with the upper confidence bound (UCB) principle. Our method, BaNk-UCB, is fully nonparametric, adapts to the context dimension, and is simple to implement. Unlike prior work relying on parametric or binning-based estimators, BaNk-UCB uses local geometry to estimate rewards and adaptively balances exploration and exploitation. We provide near-optimal regret guarantees under standard Lipschitz smoothness and margin assumptions, using a theoretically motivated batch schedule that balances regret across batches and achieves minimax-optimal rates. Empirical evaluations on synthetic and real-world datasets demonstrate that BaNk-UCB consistently outperforms binning-based baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10498v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Arya</dc:creator>
    </item>
    <item>
      <title>Expectile Periodograms</title>
      <link>https://arxiv.org/abs/2403.02060</link>
      <description>arXiv:2403.02060v3 Announce Type: replace 
Abstract: This paper introduces a novel periodogram-like function, called the expectile periodogram, for modeling spectral features of time series and detecting hidden periodicities. The expectile periodogram is constructed from trigonometric expectile regression, in which a specially designed check function is used to substitute the squared $l_2$ norm that leads to the ordinary periodogram. The expectile periodogram retains the key properties of the ordinary periodogram as a frequency-domain representation of serial dependence in time series, while offering a more comprehensive understanding by examining the data across the entire range of expectile levels. We establish the asymptotic theory and investigate the relationship between the expectile periodogram and the so called expectile spectrum. Simulations demonstrate the efficiency of the expectile periodogram in the presence of hidden periodicities. Finally, by leveraging the inherent two-dimensional nature of the expectile periodogram, we train a deep learning (DL) model to classify earthquake waveform data. Remarkably, our approach outperforms alternative periodogram-based methods in terms of classification accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02060v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianbo Chen</dc:creator>
    </item>
    <item>
      <title>Efficient estimation of the target population average treatment effect from multi-source data</title>
      <link>https://arxiv.org/abs/2405.10769</link>
      <description>arXiv:2405.10769v2 Announce Type: replace 
Abstract: We consider estimation of the target population average treatment effect (TATE) when outcome information is unavailable. Instead, we observe the outcome in multiple source populations and wish to combine the treatment effects therein to make inference on the TATE. In contrast to existing works that assume transportability on the conditional distribution of potential outcomes or conditional treatment-specific means, we work under a weaker form of effect transportability. Following the framework for causally interpretable meta-analysis, we assume transportability of conditional average treatment effects across multiple populations, which may hold with fewer standardization variables. Under this assumption, we derive the semiparametric efficiency bound of the TATE and characterize a class of doubly robust and asymptotically linear estimators. Within this class, an efficient estimator assigns optimal weights to observations from different data sources. Additionally, we suggest estimators of a low-dimensional summary of effect heterogeneity in the target population. We illustrate the use of the proposed estimators on a multicentre weight management clinical trial for semaglutide, a glucagon-like peptide-1 receptor agonist, on overweight or obese patients. Using outcome information from other regions, we estimate the weight loss effect of semaglutide in the United States subgroup.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10769v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zehao Su, Helene Charlotte Rytgaard, Henrik Ravn, Frank Eriksson</dc:creator>
    </item>
    <item>
      <title>Assessing uncertainty in Gaussian mixtures-based entropy estimation</title>
      <link>https://arxiv.org/abs/2405.17265</link>
      <description>arXiv:2405.17265v2 Announce Type: replace 
Abstract: Entropy estimation plays a crucial role in various fields, such as information theory, statistical data science, and machine learning. However, traditional entropy estimation methods often struggle with complex data distributions. Mixture-based estimation of entropy has been recently proposed and gained attention due to its ease of use and accuracy. This paper presents a novel approach to quantify the uncertainty associated with this mixture-based entropy estimation method using weighted likelihood bootstrap. Unlike standard methods, our approach leverages the underlying mixture structure by assigning random weights to observations in a weighted likelihood bootstrap procedure, leading to more accurate uncertainty estimation. The generation of weights is also investigated, leading to the proposal of using weights obtained from a specific Dirichlet distribution which, in conjunction with centered percentile intervals, yields the optimal setting to ensure empirical coverage closer to the nominal level. Extensive simulation studies comparing different resampling strategies are presented and results discussed. The proposed approach is illustrated by analyzing the log-returns of daily Gold prices at COMEX for the years 2014--2022, and the Net Rating scores, an advanced statistic used in basketball analytics, for NBA teams with reference to the 2022/23 regular season.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17265v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Scrucca</dc:creator>
    </item>
    <item>
      <title>On the statistical analysis of grouped data: when Pearson $\chi^2$ and other divisible statistics are not goodness-of-fit tests</title>
      <link>https://arxiv.org/abs/2406.09195</link>
      <description>arXiv:2406.09195v4 Announce Type: replace 
Abstract: Thousands of experiments are analyzed and papers are published each year involving the statistical analysis of grouped data. While this area of statistics is often perceived -- somewhat naively -- as saturated, several misconceptions still affect everyday practice, and new frontiers have so far remained unexplored. Researchers must be aware of the limitations affecting their analyses and what are the new possibilities in their hands.
  Motivated by this need, the article introduces a unifying approach to the analysis of grouped data, which allows us to study the class of divisible statistics -- that includes Pearson's $\chi^2$, the likelihood ratio as special cases -- with a fresh perspective. The contributions collected in this manuscript span from modeling and estimation to distribution-free goodness-of-fit tests.
  Perhaps the most surprising result presented here is that, in a sparse regime, all tests proposed in the literature are dominated by members of the class of weighted linear statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09195v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Algeri, Estate V. Khmaladze</dc:creator>
    </item>
    <item>
      <title>Partially exchangeable stochastic block models for (node-colored) multilayer networks</title>
      <link>https://arxiv.org/abs/2410.10619</link>
      <description>arXiv:2410.10619v2 Announce Type: replace 
Abstract: Multilayer networks generalize single-layered connectivity data in several directions. These generalizations include, among others, settings where multiple types of edges are observed among the same set of nodes (edge-colored networks) or where a single notion of connectivity is measured between nodes belonging to different pre-specified layers (node-colored networks). While progress has been made in statistical modeling of edge-colored networks, principled approaches that flexibly account for both within and across layer block-connectivity structures while incorporating layer information through a rigorous probabilistic construction are still lacking for node-colored multilayer networks. We fill this gap by introducing a novel class of partially exchangeable stochastic block models specified in terms of a hierarchical random partition prior for the allocation of nodes to groups, whose number is learned by the model. This goal is achieved without jeopardizing probabilistic coherence, uncertainty quantification and derivation of closed-form predictive within- and across-layer co-clustering probabilities. Our approach facilitates prior elicitation, the understanding of theoretical properties and the development of yet-unexplored predictive strategies for both the connections and the allocations of future incoming nodes. Posterior inference proceeds via a tractable collapsed Gibbs sampler, while performance is illustrated in simulations and in a real-world criminal network application. The notable gains achieved over competitors clarify the importance of developing general stochastic block models based on suitable node-exchangeability structures coherent with the type of multilayer network being analyzed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10619v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniele Durante, Francesco Gaffi, Antonio Lijoi, Igor Pr\"unster</dc:creator>
    </item>
    <item>
      <title>Doubly protected estimation for survival outcomes utilizing external controls for randomized clinical trials</title>
      <link>https://arxiv.org/abs/2410.18409</link>
      <description>arXiv:2410.18409v2 Announce Type: replace 
Abstract: Censored survival data are common in clinical trials, but small control groups can pose challenges, particularly in rare diseases or where balanced randomization is impractical. Recent approaches leverage external controls from historical studies or real-world data to strengthen treatment evaluation for survival outcomes. However, using external controls directly may introduce biases due to data heterogeneity. We propose a doubly protected estimator for the treatment-specific restricted mean survival time difference that is more efficient than trial-only estimators and mitigates biases from external data. Our method adjusts for covariate shifts via doubly robust estimation and addresses outcome drift using the DR-Learner for selective borrowing. The approach can incorporate machine learning to approximate survival curves and detect outcome drifts without strict parametric assumptions, borrowing only comparable external controls. Extensive simulation studies and a real-data application evaluating the efficacy of Galcanezumab in mitigating migraine headaches have been conducted to illustrate the effectiveness of our proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18409v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyin Gao, Shu Yang, Mingyang Shan, Wenyu Wendy Ye, Ilya Lipkovich, Douglas Faries</dc:creator>
    </item>
    <item>
      <title>Bayesian High-dimensional Grouped-regression using Sparse Projection-posterior</title>
      <link>https://arxiv.org/abs/2411.15713</link>
      <description>arXiv:2411.15713v2 Announce Type: replace 
Abstract: We present a novel Bayesian approach for high-dimensional grouped regression under sparsity. We leverage a sparse projection method that uses a sparsity-inducing map to derive an induced posterior on a lower-dimensional parameter space. Our method introduces three distinct projection maps based on popular penalty functions: the Group LASSO Projection Posterior, Group SCAD Projection Posterior, and Adaptive Group LASSO Projection Posterior. Each projection map is constructed to immerse dense posterior samples into a structured, sparse space, allowing for effective group selection and estimation in high-dimensional settings. We derive optimal posterior contraction rates for estimation and prediction, proving that the methods are model selection consistent. Additionally, we propose a Debiased Group LASSO Projection Map, which ensures exact coverage of credible sets. Our methodology is particularly suited for applications in nonparametric additive models, where we apply it with B-spline expansions to capture complex relationships between covariates and response. Extensive simulations validate our theoretical findings, demonstrating the robustness of our approach across different settings. Finally, we illustrate the practical utility of our method with an application to brain MRI volume data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), where our model identifies key brain regions associated with Alzheimer's progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15713v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samhita Pal, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>A Model-Based Clustering Approach for Bounded Data Using Transformation-Based Gaussian Mixture Models</title>
      <link>https://arxiv.org/abs/2412.13572</link>
      <description>arXiv:2412.13572v2 Announce Type: replace 
Abstract: The clustering of bounded data presents unique challenges in statistical analysis due to the constraints imposed on the data values. This paper introduces a novel method for model-based clustering specifically designed for bounded data. Building on the transformation-based approach to Gaussian mixture density estimation introduced by Scrucca (2019), we extend this framework to develop a probabilistic clustering algorithm for data with bounded support that allows for accurate clustering while respecting the natural bounds of the variables. In our proposal, a flexible range-power transformation is employed to map the data from its bounded domain to the unrestricted real space, hence enabling the estimation of Gaussian mixture models in the transformed space. Despite the close connection to density estimation, the behavior of this approach has not been previously investigated in the literature. Furthermore, we introduce a novel measure of clustering uncertainty, the Normalized Classification Entropy (NCE), which provides a general and interpretable measure of classification uncertainty. The performance of the proposed method is evaluated through real-world data applications involving both fully and partially bounded data, in both univariate and multivariate settings, showing improved cluster recovery and interpretability. Overall, the empirical results demonstrate the effectiveness and advantages of our approach over traditional and advanced model-based clustering techniques that rely on distributions with bounded support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13572v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Scrucca</dc:creator>
    </item>
    <item>
      <title>Targeted Data Fusion for Causal Survival Analysis Under Distribution Shift</title>
      <link>https://arxiv.org/abs/2501.18798</link>
      <description>arXiv:2501.18798v2 Announce Type: replace 
Abstract: Causal inference across multiple data sources offers a promising avenue to enhance the generalizability and replicability of scientific findings. However, data integration methods for time-to-event outcomes, common in biomedical research, are underdeveloped. Existing approaches focus on binary or continuous outcomes but fail to address the unique challenges of survival analysis, such as censoring and the integration of discrete and continuous time. To bridge this gap, we propose two novel methods for estimating target site-specific causal effects in multi-source settings. First, we develop a semiparametric efficient estimator for settings where individual-level data can be shared across sites. Second, we introduce a federated learning framework designed for privacy-constrained environments, which dynamically reweights source-specific contributions to account for discrepancies with the target population. Both methods leverage flexible, nonparametric machine learning models to improve robustness and efficiency. We illustrate the utility of our approaches through simulation studies and an application to multi-site randomized trials of monoclonal neutralizing antibodies for HIV-1 prevention, conducted among cisgender men and transgender persons in the United States, Brazil, Peru, and Switzerland, as well as among women in sub-Saharan Africa. Our findings underscore the potential of these methods to enable efficient, privacy-preserving causal inference for time-to-event outcomes under distribution shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18798v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Liu, Alexander W. Levis, Ke Zhu, Shu Yang, Peter B. Gilbert, Larry Han</dc:creator>
    </item>
    <item>
      <title>Bayesian local clustering of functional data via semi-Markovian random partitions</title>
      <link>https://arxiv.org/abs/2503.08881</link>
      <description>arXiv:2503.08881v2 Announce Type: replace 
Abstract: We introduce a Bayesian framework for indirect local clustering of functional data, leveraging B-spline basis expansions and a novel dependent random partition model. By exploiting the local support properties of B-splines, our approach allows partially coincident functional behaviors, achieved when shared basis coefficients span sufficiently contiguous regions. This is accomplished through a cutting-edge dependent random partition model that enforces semi-Markovian dependence across a sequence of partitions. By matching the order of the B-spline basis with the semi-Markovian dependence structure, the proposed model serves as a highly flexible prior, enabling efficient modeling of localized features in functional data. Furthermore, we extend the utility of the dependent random partition model beyond functional data, demonstrating its applicability to a broad class of problems where sequences of dependent partitions are central, and standard Markovian assumptions prove overly restrictive. Empirical illustrations, including analyses of simulated data and tide level measurements from the Venice Lagoon, showcase the effectiveness and versatility of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08881v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Toto, Antonio Canale</dc:creator>
    </item>
    <item>
      <title>An Improved Satterthwaite Effective Degrees of Freedom Correction for Weighted Syntheses of Variance</title>
      <link>https://arxiv.org/abs/2503.22080</link>
      <description>arXiv:2503.22080v4 Announce Type: replace 
Abstract: This article presents an improved approximation for the effective degrees of freedom in the Satterthwaite (1941, 1946) method which estimates the distribution of a weighted combination of variance components The standard Satterthwaite approximation assumes a scaled chisquare distribution for the composite variance estimator but is known to be biased downward when component degrees of freedom are small. Building on recent work by von Davier (2025), we propose an adjusted estimator that corrects this bias by modifying both the numerator and denominator of the traditional formula. The new approximation incorporates a weighted average of component degrees of freedom and a scaling factor that ensures consistency as the number of components or their degrees of freedom increases. We demonstrate the utility of this adjustment in practical settings, including Rubin's (1987) total variance estimation in multiple imputations, where weighted variance combinations are common. The proposed estimator generalizes and further improves von Davier's (2025) unweighted case and more accurately approximates synthetic variance estimators with arbitrary weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22080v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matthias von Davier</dc:creator>
    </item>
    <item>
      <title>Time-Uniform Confidence Spheres for Means of Random Vectors</title>
      <link>https://arxiv.org/abs/2311.08168</link>
      <description>arXiv:2311.08168v5 Announce Type: replace-cross 
Abstract: We study sequential mean estimation in $\mathbb{R}^d$. In particular, we derive time-uniform confidence spheres -- confidence sphere sequences (CSSs) -- which contain the mean of random vectors with high probability simultaneously across all sample sizes. Our results include a dimension-free CSS for log-concave random vectors, a dimension-free CSS for sub-Gaussian random vectors, and CSSs for sub-$\psi$ random vectors (which includes sub-gamma, sub-Poisson, and sub-exponential distributions). Many of our results are optimal. For sub-Gaussian distributions we also provide a CSS which tracks a time-varying mean, generalizing Robbins' mixture approach to the multivariate setting. Finally, we provide several CSSs for heavy-tailed random vectors (two moments only). Our bounds hold under a martingale assumption on the mean and do not require that the observations be iid. Our work is based on PAC-Bayesian theory and inspired by an approach of Catoni and Giulini.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08168v5</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Chugg, Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Towards more realistic climate model outputs: A multivariate bias correction based on zero-inflated vine copulas</title>
      <link>https://arxiv.org/abs/2410.15931</link>
      <description>arXiv:2410.15931v2 Announce Type: replace-cross 
Abstract: Climate model large ensembles are an essential research tool for analysing and quantifying natural climate variability and providing robust information for rare extreme events. The models simulated representations of reality are susceptible to bias due to incomplete understanding of physical processes. This paper aims to correct the bias of five climate variables from the CRCM5 Large Ensemble over Central Europe at a 3-hourly temporal resolution. At this high temporal resolution, two variables, precipitation and radiation, exhibit a high share of zero inflation. We propose a novel bias-correction method, VBC (Vine copula bias correction), that models and transfers multivariate dependence structures for zero-inflated margins in the data from its error-prone model domain to a reference domain. VBC estimates the model and reference distribution using vine copulas and corrects the model distribution via (inverse) Rosenblatt transformation. To deal with the variables' zero-inflated nature, we develop a new vine density decomposition that accommodates such variables and employs an adequately randomized version of the Rosenblatt transform. This novel approach allows for more accurate modelling of multivariate zero-inflated climate data. Compared with state-of-the-art correction methods, VBC is generally the best-performing correction and the most accurate method for correcting zero-inflated events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15931v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henri Funk, Ralf Ludwig, Helmut Kuechenhoff, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>Lightspeed Geometric Dataset Distance via Sliced Optimal Transport</title>
      <link>https://arxiv.org/abs/2501.18901</link>
      <description>arXiv:2501.18901v2 Announce Type: replace-cross 
Abstract: We introduce sliced optimal transport dataset distance (s-OTDD), a model-agnostic, embedding-agnostic approach for dataset comparison that requires no training, is robust to variations in the number of classes, and can handle disjoint label sets. The core innovation is Moment Transform Projection (MTP), which maps a label, represented as a distribution over features, to a real number. Using MTP, we derive a data point projection that transforms datasets into one-dimensional distributions. The s-OTDD is defined as the expected Wasserstein distance between the projected distributions, with respect to random projection parameters. Leveraging the closed form solution of one-dimensional optimal transport, s-OTDD achieves (near-)linear computational complexity in the number of data points and feature dimensions and is independent of the number of classes. With its geometrically meaningful projection, s-OTDD strongly correlates with the optimal transport dataset distance while being more efficient than existing dataset discrepancy measures. Moreover, it correlates well with the performance gap in transfer learning and classification accuracy in data augmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18901v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Hai Nguyen, Tuan Pham, Nhat Ho</dc:creator>
    </item>
  </channel>
</rss>
