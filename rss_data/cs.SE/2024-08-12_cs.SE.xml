<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Aug 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Early-Stage Requirements Transformation Approaches: A Systematic Review</title>
      <link>https://arxiv.org/abs/2408.05221</link>
      <description>arXiv:2408.05221v1 Announce Type: new 
Abstract: Transformation approaches for automatically constructing analysis models from textual requirements are critical to software development, as they can bring forward the use of precise formal languages from the coding phase to the requirement analysis phase in the software development life-cycle. Over the decades, numerous transformation approaches have been developed in an attempt to fully or partially automate this initial phase. This systematic review examines transformation approaches in the early stages of software development, examining 25 studies on early-stage requirements transformation documented between 2000 and 2014. The review highlights the widespread use of natural language processing techniques, with tools like the Stanford parser and WordNet being essential. Intermediate models are often used in the transformation process to bridge the gap between textual requirements and analysis models. Significant advancements have been made in early-stage requirements transformation approaches; however, several areas require attention to enhance their effectiveness and reliability. A challenge identified is the lack of robust evaluation methods, with most approaches using simple case studies and running examples for evaluation. This makes it difficult to compare and evaluate the performance these approaches. Although most approaches can generate structural models from textual requirements, many generate incomplete models with missing elements. Furthermore, requirements traceability is largely neglected, with only two approaches addressing it and lacking explicit detail on how traceability links are maintained during the transformation process. This review emphasize the need for formalized evaluation techniques and greater transparency and accessibility of approaches used in the early-stage requirements transformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05221v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keletso J. Letsholo</dc:creator>
    </item>
    <item>
      <title>Beyond Diversity:Computing for Inclusive Software</title>
      <link>https://arxiv.org/abs/2408.05304</link>
      <description>arXiv:2408.05304v1 Announce Type: new 
Abstract: This chapter presents, from our research on inclusive software within the context of a diversity and inclusion based STEM program at the University of Victoria, INSPIRE: STEM for Social Impact (hereafter Inspire). In a society with an ever increasing reliance on technology, we often neglect the fact that software development processes and practices unintentionally marginalize certain groups of end users. While the Inspire program and its first iteration in 2022 are described in detail in CHAPTER 26, here we describe our insights from an analysis of the development processes and practices used by the teams. We found that empathy-based requirements gathering techniques and certain influences on the software development teams' motivation levels impact the teams' ability to build inclusive software. This chapter begins with an explanation of the Inspire program and a discussion on what the term ``inclusive software'' actually means in our context before highlighting useful practices for designing inclusive software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05304v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-1-4842-9651-6_10</arxiv:DOI>
      <dc:creator>Kezia Devathasan, Nowshin Nawar Arony, Daniela Damian</dc:creator>
    </item>
    <item>
      <title>Automated flakiness detection in quantum software bug reports</title>
      <link>https://arxiv.org/abs/2408.05331</link>
      <description>arXiv:2408.05331v1 Announce Type: new 
Abstract: A flaky test yields inconsistent results upon repetition, posing a significant challenge to software developers. An extensive study of their presence and characteristics has been done in classical computer software but not quantum computer software. In this paper, we outline challenges and potential solutions for the automated detection of flaky tests in bug reports of quantum software. We aim to raise awareness of flakiness in quantum software and encourage the software engineering community to work collaboratively to solve this emerging challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05331v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei Zhang, Andriy Miranskyy</dc:creator>
    </item>
    <item>
      <title>Temporal Analysis and Repair of Flaky Dockerfiles</title>
      <link>https://arxiv.org/abs/2408.05379</link>
      <description>arXiv:2408.05379v1 Announce Type: new 
Abstract: Dockerfile flakiness, characterized by inconsistent build behavior without Dockerfile or project source code changes, poses significant challenges in Continuous Integration and Delivery (CI/CD) pipelines. This issue can lead to unreliable deployments and increased debugging efforts, yet it remains underexplored in current research. We conduct a systematic analysis of Dockerfile flakiness, presenting a comprehensive taxonomy of common flakiness categories, including dependency-related errors and server connectivity issues. Furthermore, we introduce FlakiDock, a tool leveraging large language models and retrieval-augmented generation techniques with dynamic analysis and an iterative feedback loop to automatically repair flaky Dockerfiles. Our evaluation shows that FlakiDock achieves a 73.55% repair accuracy, outperforming existing tools such as PARFUM by 12,581% and GPT-4-based prompting by 94.63%. These results underscore the effectiveness of FlakiDock in addressing Dockerfile flakiness and improving build reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05379v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taha Shabani, Noor Nashid, Parsa Alian, Ali Mesbah</dc:creator>
    </item>
    <item>
      <title>Can LLMs Replace Manual Annotation of Software Engineering Artifacts?</title>
      <link>https://arxiv.org/abs/2408.05534</link>
      <description>arXiv:2408.05534v1 Announce Type: new 
Abstract: Experimental evaluations of software engineering innovations, e.g., tools and processes, often include human-subject studies as a component of a multi-pronged strategy to obtain greater generalizability of the findings. However, human-subject studies in our field are challenging, due to the cost and difficulty of finding and employing suitable subjects, ideally, professional programmers with varying degrees of experience. Meanwhile, large language models (LLMs) have recently started to demonstrate human-level performance in several areas. This paper explores the possibility of substituting costly human subjects with much cheaper LLM queries in evaluations of code and code-related artifacts. We study this idea by applying six state-of-the-art LLMs to ten annotation tasks from five datasets created by prior work, such as judging the accuracy of a natural language summary of a method or deciding whether a code change fixes a static analysis warning. Our results show that replacing some human annotation effort with LLMs can produce inter-rater agreements equal or close to human-rater agreement. To help decide when and how to use LLMs in human-subject studies, we propose model-model agreement as a predictor of whether a given task is suitable for LLMs at all, and model confidence as a means to select specific samples where LLMs can safely replace human annotators. Overall, our work is the first step toward mixed human-LLM evaluations in software engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05534v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toufique Ahmed, Premkumar Devanbu, Christoph Treude, Michael Pradel</dc:creator>
    </item>
    <item>
      <title>You Augment Me: Exploring ChatGPT-based Data Augmentation for Semantic Code Search</title>
      <link>https://arxiv.org/abs/2408.05542</link>
      <description>arXiv:2408.05542v1 Announce Type: new 
Abstract: Code search plays a crucial role in software development, enabling developers to retrieve and reuse code using natural language queries. While the performance of code search models improves with an increase in high-quality data, obtaining such data can be challenging and expensive. Recently, large language models (LLMs) such as ChatGPT have made remarkable progress in both natural and programming language understanding and generation, offering user-friendly interaction via simple prompts. Inspired by these advancements, we propose a novel approach ChatDANCE, which utilizes high-quality and diverse augmented data generated by a large language model and leverages a filtering mechanism to eliminate low-quality augmentations. Specifically, we first propose a set of ChatGPT prompting rules that are specifically designed for source code and queries. Then, we leverage ChatGPT to rewrite code and queries based on the according prompts and then propose a filtering mechanism which trains a cross-encoder from the backbone model UniXcoder to filter out code and query pairs with low matching scores. Finally, we re-train the backbone model using the obtained high-quality augmented data. Experimental results show that ChatDANCE achieves state-of-the-art performance, improving the best baseline by 13.2% (R@1) and 7% (MRR). Surprisingly, we find that this augment-filter-retrain strategy enables the backbone model (UniXcoder) to self-grow. Moreover, extensive experiments show the effectiveness of each component and ChatDANCE has stable performance under different hyperparameter settings. In addition, we conduct qualitative and quantitative analyses to investigate why ChatDANCE works well and find that it learns a more uniform distribution of representations and effectively aligns the code and query spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05542v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanlin Wang, Lianghong Guo, Ensheng Shic, Wenqing Chen, Jiachi Chen, Wanjun Zhong, Menghan Wang, Hui Li, Hongyu Zhang, Ziyu Lyu, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>SHREC: a SRE Behaviour Knowledge Graph Model for Shell Command Recommendations</title>
      <link>https://arxiv.org/abs/2408.05592</link>
      <description>arXiv:2408.05592v1 Announce Type: new 
Abstract: In IT system operations, shell commands are common command line tools used by site reliability engineers (SREs) for daily tasks, such as system configuration, package deployment, and performance optimization. The efficiency in their execution has a crucial business impact since shell commands very often aim to execute critical operations, such as the resolution of system faults. However, many shell commands involve long parameters that make them hard to remember and type. Additionally, the experience and knowledge of SREs using these commands are almost always not preserved. In this work, we propose SHREC, a SRE behaviour knowledge graph model for shell command recommendations. We model the SRE shell behaviour knowledge as a knowledge graph and propose a strategy to directly extract such a knowledge from SRE historical shell operations. The knowledge graph is then used to provide shell command recommendations in real-time to improve the SRE operation efficiency. Our empirical study based on real shell commands executed in our company demonstrates that SHREC can improve the SRE operation efficiency, allowing to share and re-utilize the SRE knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05592v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/SANER60148.2024.00048</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2024 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE, 2024. p. 406-416</arxiv:journal_reference>
      <dc:creator>Andrea Tonon, Bora Caglayan, MingXue Wang, Peng Hu, Fei Shen, Puchao Zhang</dc:creator>
    </item>
    <item>
      <title>Implementing and Executing Static Analysis Using LLVM and CodeChecker</title>
      <link>https://arxiv.org/abs/2408.05657</link>
      <description>arXiv:2408.05657v1 Announce Type: new 
Abstract: Static analysis is a method of analyzing source code without executing it. It is widely used to find bugs and code smells in industrial software. Besides other methods, the most important techniques are those based on the abstract syntax tree and those performing symbolic execution. Both of these methods found their role in modern software development as they have different advantages and limitations. In this tutorial, we present two problems from the C++ programming language: the elimination of redundant pointers, and the reporting of dangling pointers originating from incorrect use of the std::string class. These two issues have different theoretical backgrounds and finding them requires different implementation techniques. We will provide a step-by-step guide to implement the checkers (software to identify the aforementioned problems) - one based on the abstract syntax analysis method, the other exploring the possibilities of symbolic execution. The methods are explained in great detail and supported by code examples. The intended audience for this tutorial are both architects of static analysis tools and developers who want to understand the advantages and constraints of the different methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05657v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabor Horvath, Reka Kovacs, Richard Szalay, Zoltan Porkolab</dc:creator>
    </item>
    <item>
      <title>The Good, the Bad, and the Ugly: Predicting Highly Change-Prone Source Code Methods at Their Inception</title>
      <link>https://arxiv.org/abs/2408.05704</link>
      <description>arXiv:2408.05704v1 Announce Type: new 
Abstract: The cost of software maintenance often surpasses the initial development expenses, making it a significant concern for the software industry. A key strategy for alleviating future maintenance burdens is the early prediction and identification of change-prone code components, which allows for timely optimizations. While prior research has largely concentrated on predicting change-prone files and classes, an approach less favored by practitioners, this paper shifts focus to predicting highly change-prone methods, aligning with the preferences of both practitioners and researchers. We analyzed 774,051 source code methods from 49 prominent open-source Java projects. Our findings reveal that approximately 80% of changes are concentrated in just 20% of the methods, demonstrating the Pareto 80/20 principle. Moreover, this subset of methods is responsible for the majority of the identified bugs in these projects. After establishing their critical role in mitigating software maintenance costs, our study shows that machine learning models can effectively identify these highly change-prone methods from their inception. Additionally, we conducted a thorough manual analysis to uncover common patterns (or concepts) among the more difficult-to-predict methods. These insights can help future research develop new features and enhance prediction accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05704v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaiful Chowdhury</dc:creator>
    </item>
    <item>
      <title>Hotfixing Large Language Models for Cod</title>
      <link>https://arxiv.org/abs/2408.05727</link>
      <description>arXiv:2408.05727v1 Announce Type: new 
Abstract: Large Language Models for Code (LLM4Code) have become an integral part of developers' workflows, assisting with tasks such as code completion and generation. However, these models are found to exhibit undesired behaviors after their release, like generating buggy code, due to their extensive training on vast amounts of source code that contain such buggy code. The training data (usually coming from open-source software) keeps evolving, e.g., developers fix the buggy code. However, adapting such evolution to mitigate LLM4Code's undesired behaviors is non-trivial, as retraining models on the updated dataset usually takes much time and resources. This motivates us to propose the concept of hotfixing LLM4Code, mitigating LLM4Code's undesired behaviors effectively and efficiently with minimal negative effects.
  This paper mainly focuses on hotfixing LLM4Code to make them generate less buggy code and more fixed code. We begin by demonstrating that models from the popular CodeGen family frequently generate buggy code. Then, we define three learning objectives in hotfixing and design multiple loss functions for each objective: (1) learn the desired behaviors, (2) unlearn the undesired behaviors, and (3) retain knowledge of other code. We evaluate four different fine-tuning techniques for hotfixing the models and gain the following insights. Optimizing these three learning goals together, using LoRA (low-rank adaptation), effectively influences the model's behavior. Specifically, it increases the generation of fixed code by up to 108.42% and decreases the generation of buggy code by up to 50.47%. Statistical tests confirm that hotfixing does not significantly affect the models' functional correctness on the HumanEval benchmark. We also show that hotfixing demonstrates strong time efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05727v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhou Yang, David Lo</dc:creator>
    </item>
    <item>
      <title>Unlocking the Power of Numbers: Log Compression via Numeric Token Parsing</title>
      <link>https://arxiv.org/abs/2408.05760</link>
      <description>arXiv:2408.05760v1 Announce Type: new 
Abstract: Parser-based log compressors have been widely explored in recent years because the explosive growth of log volumes makes the compression performance of general-purpose compressors unsatisfactory. These parser-based compressors preprocess logs by grouping the logs based on the parsing result and then feed the preprocessed files into a general-purpose compressor. However, parser-based compressors have their limitations. First, the goals of parsing and compression are misaligned, so the inherent characteristics of logs were not fully utilized. In addition, the performance of parser-based compressors depends on the sample logs and thus it is very unstable. Moreover, parser-based compressors often incur a long processing time. To address these limitations, we propose Denum, a simple, general log compressor with high compression ratio and speed. The core insight is that a majority of the tokens in logs are numeric tokens (i.e. pure numbers, tokens with only numbers and special characters, and numeric variables) and effective compression of them is critical for log compression. Specifically, Denum contains a Numeric Token Parsing module, which extracts all numeric tokens and applies tailored processing methods (e.g. store the differences of incremental numbers like timestamps), and a String Processing module, which processes the remaining log content without numbers. The processed files of the two modules are then fed as input to a general-purpose compressor and it outputs the final compression results. Denum has been evaluated on 16 log datasets and it achieves an 8.7%-434.7% higher average compression ratio and 2.6x-37.7x faster average compression speed (i.e. 26.2MB/S) compared to the baselines. Moreover, integrating Denum's Numeric Token Parsing into existing log compressors can provide an 11.8% improvement in their average compression ratio and achieve 37% faster average compression speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05760v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyu Yu, Yifan Wu, Ying Li, Pinjia He</dc:creator>
    </item>
    <item>
      <title>Supporting Software Maintenance with Dynamically Generated Document Hierarchies</title>
      <link>https://arxiv.org/abs/2408.05829</link>
      <description>arXiv:2408.05829v1 Announce Type: new 
Abstract: Software documentation supports a broad set of software maintenance tasks; however, creating and maintaining high-quality, multi-level software documentation can be incredibly time-consuming and therefore many code bases suffer from a lack of adequate documentation. We address this problem through presenting HGEN, a fully automated pipeline that leverages LLMs to transform source code through a series of six stages into a well-organized hierarchy of formatted documents. We evaluate HGEN both quantitatively and qualitatively. First, we use it to generate documentation for three diverse projects, and engage key developers in comparing the quality of the generated documentation against their own previously produced manually-crafted documentation. We then pilot HGEN in nine different industrial projects using diverse datasets provided by each project. We collect feedback from project stakeholders, and analyze it using an inductive approach to identify recurring themes. Results show that HGEN produces artifact hierarchies similar in quality to manually constructed documentation, with much higher coverage of the core concepts than the baseline approach. Stakeholder feedback highlights HGEN's commercial impact potential as a tool for accelerating code comprehension and maintenance tasks. Results and associated supplemental materials can be found at https://zenodo.org/records/11403244</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05829v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katherine R. Dearstyne, Alberto D. Rodriguez, Jane Cleland-Huang</dc:creator>
    </item>
    <item>
      <title>A Metascience Study of the Impact of Low-Code Techniques in Modeling Publications</title>
      <link>https://arxiv.org/abs/2408.05975</link>
      <description>arXiv:2408.05975v1 Announce Type: new 
Abstract: In the last years, model-related publications have been exploring the application of modeling techniques in different domains. Initially focused on UML and the Model-Driven Architecture approach, the literature has been evolving towards the usage of more general concepts such as Model-Driven Development or Model-Driven Engineering. With the emergence of Low-Code software development platforms, the modeling community has been studying how these two fields may combine and benefit from each other, thus leading to the publication of a number of works in recent years. In this paper, we present a metascience study of Low-Code. Our study has a two-fold approach: (1) to examine the composition (size and diversity) of the emerging Low-Code community; and (2) to investigate how this community differs from the "classical" model-driven community in terms of people, venues, and types of publications. Through this study, we aim to benefit the low-code community by helping them better understand its relationship with the broader modeling community. Ultimately, we hope to trigger a discussion about the current and possible future evolution of the low-code community as part of its consolidation as a new research field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05975v1</guid>
      <category>cs.SE</category>
      <category>cs.DL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mauro Dalle Lucca Tosi, Javier Luis C\'anovas Izquierdo, Jordi Cabot</dc:creator>
    </item>
    <item>
      <title>BPMN Analyzer 2.0: Instantaneous, Comprehensible, and Fixable Control Flow Analysis for Realistic BPMN Models</title>
      <link>https://arxiv.org/abs/2408.06028</link>
      <description>arXiv:2408.06028v1 Announce Type: new 
Abstract: Many business process models contain control flow errors, such as deadlocks or livelocks, which hinder proper execution. In this paper, we introduce a new tool that can instantaneously identify control flow errors in BPMN models, make them understandable for modelers, and suggest corrections to resolve them. We demonstrate that detection is instantaneous by benchmarking our tool against synthetic BPMN models with increasing size and state space complexity, as well as realistic models. Moreover, the tool directly displays detected errors in the model, including an interactive visualization, and suggests fixes to resolve them. The tool is open source, extensible, and integrated into a popular BPMN modeling tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06028v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Kr\"auter, Patrick St\"unkel, Adrian Rutle, Yngve Lamo, Harald K\"onig</dc:creator>
    </item>
    <item>
      <title>Hyperion: Unveiling DApp Inconsistencies using LLM and Dataflow-Guided Symbolic Execution</title>
      <link>https://arxiv.org/abs/2408.06037</link>
      <description>arXiv:2408.06037v1 Announce Type: new 
Abstract: The rapid advancement of blockchain platforms has significantly accelerated the growth of decentralized applications (DApps). Similar to traditional applications, DApps integrate front-end descriptions that showcase their features to attract users, and back-end smart contracts for executing their business logic. However, inconsistencies between the features promoted in front-end descriptions and those actually implemented in the contract can confuse users and undermine DApps's trustworthiness. In this paper, we first conducted an empirical study to identify seven types of inconsistencies, each exemplified by a real-world DApp. Furthermore, we introduce HYPERION, an approach designed to automatically identify inconsistencies between front-end descriptions and back-end code implementation in DApps. This method leverages a fine-tuned large language model LLaMA2 to analyze DApp descriptions and employs dataflow-guided symbolic execution for contract bytecode analysis. Finally, HYPERION reports the inconsistency based on predefined detection patterns. The experiment on our ground truth dataset consisting of 54 DApps shows that HYPERION reaches 84.06% overall recall and 92.06% overall precision in reporting DApp inconsistencies. We also implement HYPERION to analyze 835 real-world DApps. The experimental results show that HYPERION discovers 459 real-world DApps containing at least one inconsistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06037v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuo Yang, Xingwei Lin, Jiachi Chen, Qingyuan Zhong, Lei Xiao, Renke Huang, Yanlin Wang, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>AutoCheck: Automatically Identifying Variables for Checkpointing by Data Dependency Analysis</title>
      <link>https://arxiv.org/abs/2408.06082</link>
      <description>arXiv:2408.06082v1 Announce Type: new 
Abstract: Checkpoint/Restart (C/R) has been widely deployed in numerous HPC systems, Clouds, and industrial data centers, which are typically operated by system engineers. Nevertheless, there is no existing approach that helps system engineers without domain expertise, and domain scientists without system fault tolerance knowledge identify those critical variables accounted for correct application execution restoration in a failure for C/R. To address this problem, we propose an analytical model and a tool (AutoCheck) that can automatically identify critical variables to checkpoint for C/R. AutoCheck relies on first, analytically tracking and optimizing data dependency between variables and other application execution state, and second, a set of heuristics that identify critical variables for checkpointing from the refined data dependency graph (DDG). AutoCheck allows programmers to pinpoint critical variables to checkpoint quickly within a few minutes. We evaluate AutoCheck on 14 representative HPC benchmarks, demonstrating that AutoCheck can efficiently identify correct critical variables to checkpoint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06082v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Fu (Nanchang Hangkong University), Weiping Zhang (Nanchang Hangkong University), Xin Huang (Nanchang Hangkong University), Shiman Meng (Nanchang Hangkong University), Wubiao Xu (Nanchang Hangkong University), Luanzheng Guo (Pacific Northwest National Laboratory), Kento Sato (R-CCS, RIKEN)</dc:creator>
    </item>
    <item>
      <title>A Practical System Architecture for Contract Automation: Design and Uses</title>
      <link>https://arxiv.org/abs/2408.06084</link>
      <description>arXiv:2408.06084v1 Announce Type: new 
Abstract: While the blockchain-based smart contract has become a hot topic of research over the last decade, not the least in the context of Industry 4.0, it now has well-known legal and technical shortcomings that currently prohibit its real-world application. These shortcomings come from (1) that a smart contract is a computer program, not a document describing legal obligations, and (2) that blockchain-based systems are complicated to use and operate. In this paper, we present a refined and extended summary of our work taking key technologies from the blockchain sphere and applying them to the ricardian contract, which is a traditional contract in digital form with machine-readable parameters. By putting the ricardian contract in the context of our contract network architecture, we facilitate the infrastructure required for contracts to be offered, negotiated, performed, renegotiated and terminated in a completely digital and automatable fashion. Our architecture circumvents the legal issues of blockchains by facilitating an artifact very much alike a traditional contract, as well as its operational complexity by requiring consensus only between nodes representing directly involved parties. To demonstrate its utility, we also present how it could be used for (1) private data purchasing, (2) treasury management, (3) order-driven manufacturing and (4) automated device on-boarding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06084v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuel Palm, Ulf Bodin, Olov Schel\'en</dc:creator>
    </item>
    <item>
      <title>Contexts Matter: An Empirical Study on Contextual Influence in Fairness Testing for Deep Learning Systems</title>
      <link>https://arxiv.org/abs/2408.06102</link>
      <description>arXiv:2408.06102v1 Announce Type: new 
Abstract: Background: Fairness testing for deep learning systems has been becoming increasingly important. However, much work assumes perfect context and conditions from the other parts: well-tuned hyperparameters for accuracy; rectified bias in data, and mitigated bias in the labeling. Yet, these are often difficult to achieve in practice due to their resource-/labour-intensive nature. Aims: In this paper, we aim to understand how varying contexts affect fairness testing outcomes. Method:We conduct an extensive empirical study, which covers $10,800$ cases, to investigate how contexts can change the fairness testing result at the model level against the existing assumptions. We also study why the outcomes were observed from the lens of correlation/fitness landscape analysis. Results: Our results show that different context types and settings generally lead to a significant impact on the testing, which is mainly caused by the shifts of the fitness landscape under varying contexts. Conclusions: Our findings provide key insights for practitioners to evaluate the test generators and hint at future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06102v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengwen Du, Tao Chen</dc:creator>
    </item>
    <item>
      <title>A pragmatic look at education and training of software test engineers: Further cooperation of academia and industry is needed</title>
      <link>https://arxiv.org/abs/2408.06144</link>
      <description>arXiv:2408.06144v1 Announce Type: new 
Abstract: Alongside software testing education in universities, a great extent of effort and resources are spent on software-testing training activities in industry. For example, there are several international certification schemes in testing, such as those provided by the International Software Testing Qualifications Board (ISTQB), which have been issued to more than 914K testers so far. To train the highly qualified test engineers of tomorrow, it is important for both university educators and trainers in industry to be aware of the status of software testing education in academia versus its training in industry, to analyze the relationships of these two approaches, and to assess ways on how to improve the education / training landscape. For that purpose, this paper provides a pragmatic overview of the issue, presents several recommendations, and hopes to trigger further discussions in the community, between industry and academia, on how to further improve the status-quo, and to find further best practices for more effective education and training of software testers. The paper is based on combined ~40 years of the two authors' technical experience in test engineering, and their ~30 years of experience in providing testing education and training in more than six countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06144v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vahid Garousi, Alper Bu\u{g}ra Kele\c{s}</dc:creator>
    </item>
    <item>
      <title>Coverage measurement in model-based testing of web applications: Tool support and an industrial experience report</title>
      <link>https://arxiv.org/abs/2408.06148</link>
      <description>arXiv:2408.06148v1 Announce Type: new 
Abstract: There are many widely used tools for measuring test-coverage and code-coverage. Test coverage is the ratio of requirements or other non-code artifacts covered by a test suite, while code-coverage is the ratio of source code covered by tests. Almost all coverage tools show a few certain subset of coverage values, and almost always either test-coverage or code-coverage measures. In a large-scale industrial web-application-testing setting, we were faced with the need to "integrate" several types of coverage data (including front-end and back-end code coverage with requirements coverage), and to see all of them "live" as large model-based test suites were running. By being unable to find any off-the-shelf toolset to address the above need, we have developed an open-source test coverage tool, specific for MBT, named MBTCover. In addition to code coverage, the tool measures and reports requirements and model coverage, "live" as a given MBT test suite is executing. In this paper, we present the features of the MBTCover tool and our experience from using it in multiple large test-automation projects in practice. Other software test engineers, who conduct web application testing and MBT, may find the tool useful in their projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06148v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vahid Garousi, Alper Bu\u{g}ra Kele\c{s}, Yunus Balaman, Alper Mermer, Zeynep \"Ozdemir G\"uler</dc:creator>
    </item>
    <item>
      <title>A Multi-Year Grey Literature Review on AI-assisted Test Automation</title>
      <link>https://arxiv.org/abs/2408.06224</link>
      <description>arXiv:2408.06224v1 Announce Type: new 
Abstract: Context: Test Automation (TA) techniques are crucial for quality assurance in software engineering but face limitations such as high test suite maintenance costs and the need for extensive programming skills. Artificial Intelligence (AI) offers new opportunities to address these issues through automation and improved practices. Objectives: This study surveys grey literature to explore how AI is adopted in TA, focusing on the problems it solves, its solutions, and the available tools. Additionally, the study gathers expert insights to understand AI's current and future role in TA. Methods: We reviewed over 3,600 grey literature sources over five years, including blogs, white papers, and user manuals, and finally filtered 342 documents to develop taxonomies of TA problems and AI solutions. We also cataloged 100 AI-driven TA tools and interviewed five expert software testers to gain insights into AI's current and future role in TA. Results: The study found that manual test code development and maintenance are the main challenges in TA. In contrast, automated test generation and self-healing test scripts are the most common AI solutions. We identified 100 AI-based TA tools, with Applitools, Testim, Functionize, AccelQ, and Mabl being the most adopted in practice. Conclusion: This paper offers a detailed overview of AI's impact on TA through grey literature analysis and expert interviews. It presents new taxonomies of TA problems and AI solutions, provides a catalog of AI-driven tools, and relates solutions to problems and tools to solutions. Interview insights further revealed the state and future potential of AI in TA. Our findings support practitioners in selecting TA tools and guide future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06224v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filippo Ricca, Alessandro Marchetto, Andrea Stocco</dc:creator>
    </item>
    <item>
      <title>A Large-Scale Study of Model Integration in ML-Enabled Software Systems</title>
      <link>https://arxiv.org/abs/2408.06226</link>
      <description>arXiv:2408.06226v1 Announce Type: new 
Abstract: The rise of machine learning (ML) and its embedding in systems has drastically changed the engineering of software-intensive systems. Traditionally, software engineering focuses on manually created artifacts such as source code and the process of creating them, as well as best practices for integrating them, i.e., software architectures. In contrast, the development of ML artifacts, i.e. ML models, comes from data science and focuses on the ML models and their training data. However, to deliver value to end users, these ML models must be embedded in traditional software, often forming complex topologies. In fact, ML-enabled software can easily incorporate many different ML models. While the challenges and practices of building ML-enabled systems have been studied to some extent, beyond isolated examples, little is known about the characteristics of real-world ML-enabled systems. Properly embedding ML models in systems so that they can be easily maintained or reused is far from trivial. We need to improve our empirical understanding of such systems, which we address by presenting the first large-scale study of real ML-enabled software systems, covering over 2,928 open source systems on GitHub. We classified and analyzed them to determine their characteristics, as well as their practices for reusing ML models and related code, and the architecture of these systems. Our findings provide practitioners and researchers with insight into practices for embedding and integrating ML models, bringing data science and software engineering closer together.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06226v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yorick Sens, Henriette Knopp, Sven Peldszus, Thorsten Berger</dc:creator>
    </item>
    <item>
      <title>Deep Learning System Boundary Testing through Latent Space Style Mixing</title>
      <link>https://arxiv.org/abs/2408.06258</link>
      <description>arXiv:2408.06258v1 Announce Type: new 
Abstract: Evaluating the behavioral frontier of deep learning (DL) systems is crucial for understanding their generalizability and robustness. However, boundary testing is challenging due to their high-dimensional input space. Generative artificial intelligence offers a promising solution by modeling data distribution within compact latent space representations, thereby facilitating finer-grained explorations. In this work, we introduce MIMICRY, a novel black-box system-agnostic test generator that leverages these latent representations to generate frontier inputs for the DL systems under test. Specifically, MIMICRY uses style-based generative adversarial networks trained to learn the representation of inputs with disentangled features. This representation enables embedding style-mixing operations between a source and a target input, combining their features to explore the boundary between them. We evaluated the effectiveness of different MIMICRY configurations in generating boundary inputs for four popular DL image classification systems. Our results show that manipulating the latent space allows for effective and efficient exploration of behavioral frontiers. As opposed to a model-based baseline, MIMICRY generates a higher quality frontier of behaviors which includes more and closer inputs. Additionally, we assessed the validity of these inputs, revealing a high validity rate according to human assessors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06258v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amr Abdellatif, Xingcheng Chen, Vincenzo Riccio, Andrea Stocco</dc:creator>
    </item>
    <item>
      <title>AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations</title>
      <link>https://arxiv.org/abs/2408.05344</link>
      <description>arXiv:2408.05344v1 Announce Type: cross 
Abstract: In this work, we discuss a recently popular type of recommender system: an LLM-based coding assistant. Connecting the task of providing code recommendations in multiple formats to traditional RecSys challenges, we outline several similarities and differences due to domain specifics. We emphasize the importance of providing relevant context to an LLM for this use case and discuss lessons learned from context enhancements &amp; offline and online evaluation of such AI-assisted coding systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05344v1</guid>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3640457.3688060</arxiv:DOI>
      <dc:creator>Jan Hartman, Rishabh Mehrotra, Hitesh Sagtani, Dominic Cooney, Rafal Gajdulewicz, Beyang Liu, Julie Tibshirani, Quinn Slack</dc:creator>
    </item>
    <item>
      <title>A Web-based Software Development Kit for Quantum Network Simulation</title>
      <link>https://arxiv.org/abs/2408.05588</link>
      <description>arXiv:2408.05588v1 Announce Type: cross 
Abstract: Quantum network simulation is an essential step towards developing applications for quantum networks and determining minimal requirements for the network hardware. As it is with classical networking, a simulation ecosystem allows for application development, standardization, and overall community building. Currently, there is limited traction towards building a quantum networking community-there are limited open-source platforms, challenging frameworks with steep learning curves, and strong requirements of software engineering skills. Our Quantum Network Development Kit (QNDK) project aims to solve these issues. It includes a graphical user interface to easily develop and run quantum network simulations with very little code. It integrates various quantum network simulation engines and provides a single interface to them, allowing users to use the features from any of them. Further, it deploys simulation execution in a cloud environment, offloading strong computing requirements to a high-performance computing system. In this paper, we detail the core features of the QNDK and outline the development roadmap to enabling virtual quantum testbeds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05588v1</guid>
      <category>quant-ph</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen DiAdamo, Francesco Vista</dc:creator>
    </item>
    <item>
      <title>Top Pass: Improve Code Generation by Pass@k-Maximized Code Ranking</title>
      <link>https://arxiv.org/abs/2408.05715</link>
      <description>arXiv:2408.05715v1 Announce Type: cross 
Abstract: Code generation has been greatly enhanced by the profound advancements in Large Language Models (LLMs) recently. Nevertheless, such LLM-based code generation approaches still struggle to generate error-free code in a few tries when faced with complex problems. To address this, the prevailing strategy is to sample a huge number of candidate programs, with the hope of any one in them could work. However, users of code generation systems usually expect to find a correct program by reviewing or testing only a small number of code candidates. Otherwise, the system would be unhelpful. In this paper, we propose Top Pass, a code ranking approach that identifies potential correct solutions from a large number of candidates. Top Pass directly optimizes the pass@k loss function, enhancing the quality at the top of the candidate list. This enables the user to find the correct solution within as few tries as possible. Experimental results on four benchmarks indicate that our Top Pass method enhances the usability of code generation models by producing better ranking results, particularly achieving a 32.9\% relative improvement in pass@1 on CodeContests when compared to the state-of-the-art ranking method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05715v1</guid>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi-Cun Lyu, Xin-Ye Li, Zheng Xie, Ming Li</dc:creator>
    </item>
    <item>
      <title>120 Domain-Specific Languages for Security</title>
      <link>https://arxiv.org/abs/2408.06219</link>
      <description>arXiv:2408.06219v1 Announce Type: cross 
Abstract: Security engineering, from security requirements engineering to the implementation of cryptographic protocols, is often supported by domain-specific languages (DSLs). Unfortunately, a lack of knowledge about these DSLs, such as which security aspects are addressed and when, hinders their effective use and further research. This systematic literature review examines 120 security-oriented DSLs based on six research questions concerning security aspects and goals, language-specific characteristics, integration into the software development lifecycle (SDLC), and effectiveness of the DSLs. We observe a high degree of fragmentation, which leads to opportunities for integration. We also need to improve the usability and evaluation of security DSLs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06219v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Markus Krausz, Sven Peldszus, Francesco Regazzoni, Thorsten Berger, Tim Tim G\"uneysu</dc:creator>
    </item>
    <item>
      <title>A Process To Support Cloud Release Preparation</title>
      <link>https://arxiv.org/abs/2205.01372</link>
      <description>arXiv:2205.01372v2 Announce Type: replace 
Abstract: This paper presents concepts and methods to support preparing software and system releases to production.
  Keywords: Operational Readiness Review, ORR, IT Services, IT Operations, ITIL, Process Engineering, Reliability, Availability, Software Architecture, Cloud Computing, Networking, Site Reliability Engineering, DevOps, Agile Methods, Quality, Defect Prevention, Release Management, Risk Management, Data Visualization, Organizational Change Management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.01372v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>James J. Cusick</dc:creator>
    </item>
    <item>
      <title>Insights from the Usage of the Ansible Lightspeed Code Completion Service</title>
      <link>https://arxiv.org/abs/2402.17442</link>
      <description>arXiv:2402.17442v2 Announce Type: replace 
Abstract: The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for Information Technology (IT) automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Ansible Lightspeed is an LLM-based service designed explicitly to generate Ansible YAML given natural language prompt.
  This paper first presents the design and implementation of the Ansible Lightspeed service. We then evaluate its utility to developers using diverse indicators, including extended utilization, analysis of user rejected suggestions, as well as analysis of user sentiments. The analysis is based on data collected for 10,696 real users including 3,910 returning users. The code for Ansible Lightspeed service and the analysis framework is made available for others to use.
  To our knowledge, our study is the first to involve thousands of users in evaluating code assistants for domain-specific languages. We propose an improved version of user acceptance rate and we are the first code completion tool to present N-Day user retention figures. With our findings we provide insights into the effectiveness of small, dedicated models in a domain-specific context. We hope this work serves as a reference for software engineering and machine learning researchers exploring code completion services for domain-specific languages in particular and programming languages in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17442v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Priyam Sahoo, Saurabh Pujar, Ganesh Nalawade, Richard Gebhardt, Louis Mandel, Luca Buratti</dc:creator>
    </item>
    <item>
      <title>A formal definition of loop unrolling with applications to test coverage</title>
      <link>https://arxiv.org/abs/2403.08923</link>
      <description>arXiv:2403.08923v2 Announce Type: replace 
Abstract: Techniques to achieve various forms of test coverage, such as branch coverage, typically do not iterate loops; in other words, they treat a loop as a conditional, executed zero or one time. Existing work by the author and collaborators produces test suites guaranteeing full branch coverage. More recent work has shown that by "unrolling" loops the approach can find significantly more bugs. The present discussion provides the theoretical basis and precise definition for this concept of unrolling.
  While initially motivated by the need to improve standard test coverage practices (which execute loop bodies only once), to better testing coverage, the framework presented here is applicable to any form of reasoning about loops.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08923v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bertrand Meyer</dc:creator>
    </item>
    <item>
      <title>An Empirical Study of ChatGPT-Related Projects and Their Issues on GitHub</title>
      <link>https://arxiv.org/abs/2403.17437</link>
      <description>arXiv:2403.17437v2 Announce Type: replace 
Abstract: Since the launch of ChatGPT in 2022, an increasing number of ChatGPT-related projects are being published on GitHub, sparking widespread discussions. However, GitHub does not provide a detailed classification of these projects to help users effectively explore interested projects. Additionally, the issues raised by users for these projects cover various aspects, e.g., installation, usage, and updates. It would be valuable to help developers prioritize more urgent issues and improve development efficiency. We retrieved 71,244 projects from GitHub using the keyword `ChatGPT' and selected the top 200 representative projects with the highest numbers of stars as our dataset. By analyzing the project descriptions, we identified three primary categories of ChatGPT-related projects, namely ChatGPT Implementation &amp; Training, ChatGPT Application, ChatGPT Improvement &amp; Extension. Next, we applied a topic modeling technique to 23,609 issues of those projects and identified ten issue topics, e.g., model reply and interaction interface. We further analyzed the popularity, difficulty, and evolution of each issue topic within the three project categories. Our main findings are: 1) The increase in the number of projects within the three categories is closely related to the development of ChatGPT; and 2) There are significant differences in the popularity, difficulty, and evolutionary trends of the issue topics across the three project categories. Based on these findings, we finally provided implications for project developers and platform managers on how to better develop and manage ChatGPT-related projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17437v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zheng Lin, Neng Zhang, Chao Liu, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>Quantum Computing for All: Online Courses Built Around Interactive Visual Quantum Circuit Simulator</title>
      <link>https://arxiv.org/abs/2404.10328</link>
      <description>arXiv:2404.10328v2 Announce Type: replace 
Abstract: Quantum computing is a highly abstract scientific discipline, which, however, is expected to have great practical relevance in future information technology. This forces educators to seek new methods to teach quantum computing for students with diverse backgrounds and with no prior knowledge of quantum physics. We have developed an online course built around an interactive quantum circuit simulator designed to enable easy creation and maintenance of course material with ranging difficulty. The immediate feedback and automatically evaluated tasks lowers the entry barrier to quantum computing for all students, regardless of their background.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10328v2</guid>
      <category>cs.SE</category>
      <category>physics.ed-ph</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Juha Reinikainen, Vlad Stirbu, Teiko Heinosaari, Vesa Lappalainen, Tommi Mikkonen</dc:creator>
    </item>
    <item>
      <title>TestART: Improving LLM-based Unit Test via Co-evolution of Automated Generation and Repair Iteration</title>
      <link>https://arxiv.org/abs/2408.03095</link>
      <description>arXiv:2408.03095v3 Announce Type: replace 
Abstract: Unit test is crucial for detecting bugs in individual program units but consumes time and effort. The existing automated unit test generation methods are mainly based on search-based software testing (SBST) and language models to liberate developers. Recently, large language models (LLMs) have demonstrated remarkable reasoning and generation capabilities. However, several problems limit their ability to generate high-quality test cases: (1) LLMs may generate invalid test cases under insufficient context, resulting in compilation errors; (2) Lack of test and coverage feedback information may cause runtime errors and low coverage rates. (3) The repetitive suppression problem causes LLMs to get stuck into the repetition loop of self-repair or re-generation attempts. In this paper, we propose TestART, a novel unit test generation method that leverages the strengths of LLMs while overcoming the limitations mentioned. TestART improves LLM-based unit test via co-evolution of automated generation and repair iteration. TestART leverages the template-based repair technique to fix bugs in LLM-generated test cases, using prompt injection to guide the next-step automated generation and avoid repetition suppression. Furthermore, TestART extracts coverage information from the passed test cases and utilizes it as testing feedback to enhance the sufficiency of the final test case. This synergy between generation and repair elevates the quality, effectiveness, and readability of the produced test cases significantly beyond previous methods. In comparative experiments, the pass rate of TestART-generated test cases is 78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and the same ChatGPT-3.5-based method ChatUniTest. It also achieves an impressive line coverage rate of 90.96% on the focal methods that passed the test, exceeding EvoSuite by 3.4%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03095v3</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siqi Gu, Chunrong Fang, Quanjun Zhang, Fangyuan Tian, Jianyi Zhou, Zhenyu Chen</dc:creator>
    </item>
    <item>
      <title>CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases</title>
      <link>https://arxiv.org/abs/2408.03910</link>
      <description>arXiv:2408.03910v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval and MBPP, but struggle with handling entire code repositories. This challenge has prompted research on enhancing LLM-codebase interaction at a repository scale. Current solutions rely on similarity-based retrieval or manual tools and APIs, each with notable drawbacks. Similarity-based retrieval often has low recall in complex tasks, while manual tools and APIs are typically task-specific and require expert knowledge, reducing their generalizability across diverse code tasks and real-world applications. To mitigate these limitations, we introduce CodexGraph, a system that integrates LLM agents with graph database interfaces extracted from code repositories. By leveraging the structural properties of graph databases and the flexibility of the graph query language, CodexGraph enables the LLM agent to construct and execute queries, allowing for precise, code structure-aware context retrieval and code navigation. We assess CodexGraph using three benchmarks: CrossCodeEval, SWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding applications. With a unified graph database schema, CodexGraph demonstrates competitive performance and potential in both academic and real-world environments, showcasing its versatility and efficacy in software engineering. Our application demo: https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03910v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Shieh, Wenmeng Zhou</dc:creator>
    </item>
    <item>
      <title>An Empirical Study on Challenges for LLM Developers</title>
      <link>https://arxiv.org/abs/2408.05002</link>
      <description>arXiv:2408.05002v2 Announce Type: replace 
Abstract: In recent years, large language models (LLMs) have seen rapid advancements, significantly impacting various fields such as natural language processing, and software engineering. These LLMs, exemplified by OpenAI's ChatGPT, have revolutionized the way we approach language understanding and generation tasks. However, in contrast to traditional software development practices, LLM development introduces new challenges for AI developers in design, implementation, and deployment. These challenges span different areas (such as prompts, APIs, and plugins), requiring developers to navigate unique methodologies and considerations specific to LLM development.
  Despite the profound influence of LLMs, to the best of our knowledge, these challenges have not been thoroughly investigated in previous empirical studies. To fill this gap, we present the first comprehensive study on understanding the challenges faced by LLM developers. Specifically, we crawl and analyze 29,057 relevant questions from a popular OpenAI developer forum. We first examine their popularity and difficulty. After manually analyzing 2,364 sampled questions, we construct a taxonomy of challenges faced by LLM developers. Based on this taxonomy, we summarize a set of findings and actionable implications for LLM-related stakeholders, including developers and providers (especially the OpenAI organization).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05002v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Chen, Chaoyang Gao, Chunyang Chen, Guangbei Zhang, Yong Liu</dc:creator>
    </item>
    <item>
      <title>Software Engineering Through Community-Engaged Learning and an Inclusive Network</title>
      <link>https://arxiv.org/abs/2302.07100</link>
      <description>arXiv:2302.07100v2 Announce Type: replace-cross 
Abstract: Retaining diverse, underrepresented students in computer science and software engineering programs is a significant concern for universities. In this chapter, we describe the INSPIRE: STEM for Social Impact program at the University of Victoria, Canada, which leverages the three principles of self-determination theory competence, relatedness, and autonomy in the design of strategies to empower women and other underrepresented groups in using software and other engineering solutions to approach sustainability, community-driven problems. We also describe lessons learned from a first successful year that involved over 30 students, 6 community partners (sustainability problem owners), and over 20 industry and academic mentors and reached out to more than 200 solution end users in our communities. Finally, we provide recommendations for universities and organizations who may want to adopt our approach. In the program 24 diverse students (in terms of gender, sexual orientation, ethnicity, academic standing, and background) divided into six teams paired with six community partners worked on solving society impactful problems and developed solutions for a number of respective community partners. Each team was supported by an experienced upper year student and mentors from industry and community throughout the program. The experiential learning approach of the program allowed the students to learn a variety of soft and technical skills while developing a solution that has a social and/or environmental impact. Having a diverse team and creating a solution for real end users motivated the students to actively collaborate with their peers, community partners, and mentors resulting in the development of an inclusive network. A network of like minded people is crucial in empowering underrepresented individuals and inspiring them to remain in the computer science and software engineering fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07100v2</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-1-4842-9651-6_26</arxiv:DOI>
      <dc:creator>Nowshin Nawar Arony, Kezia Devathasan, Ze Shi Li, Daniela Damian</dc:creator>
    </item>
    <item>
      <title>Ownership in low-level intermediate representation</title>
      <link>https://arxiv.org/abs/2408.04043</link>
      <description>arXiv:2408.04043v2 Announce Type: replace-cross 
Abstract: The concept of ownership in high level languages can aid both the programmer and the compiler to reason about the validity of memory operations. Previously, ownership semantics has been used successfully in high level automatic program verification to model a reference to data by a first order logic (FOL) representation of data instead of maintaining an address map. However, ownership semantics is not used in low level program verification. We have identified two challenges. First, ownership information is lost when a program is compiled to a low level intermediate representation (e.g., in LLVM IR). Second, pointers in low level programs point to bytes using an address map (e.g., in unsafe Rust) and thus the verification condition (VC) cannot always replace a pointer by its FOL abstraction. To remedy the situation, we develop ownership semantics for an LLVM like low level intermediate representation. Using these semantics, the VC can opportunistically model some memory accesses by a direct access of a pointer cache that stores byte representation of data. This scheme reduces instances where an address map must be maintained, especially for mostly safe programs that follow ownership semantics. For unsafe functionality, memory accesses are modelled by operations on an address map and we provide mechanisms to keep the address map and pointer cache in sync. We implement these semantics in SEABMC, a bit precise bounded model checker for LLVM. For evaluation, the source programs are assumed to be written in C. Since C does not have ownership built in, suitable macros are added that introduce and preserve ownership during translation to LLVM like IR for verification. This approach is evaluated on mature open source C code. For both handcrafted benchmarks and practical programs, we observe a speedup of $1.3x--5x$ during SMT solving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04043v2</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddharth Priya, Arie Gurfinkel</dc:creator>
    </item>
  </channel>
</rss>
