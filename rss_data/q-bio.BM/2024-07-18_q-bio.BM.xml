<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.BM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.BM</link>
    <description>q-bio.BM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.BM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Jul 2024 04:05:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Hitchhiker's Guide to Deep Chemical Language Processing for Bioactivity Prediction</title>
      <link>https://arxiv.org/abs/2407.12152</link>
      <description>arXiv:2407.12152v1 Announce Type: new 
Abstract: Deep learning has significantly accelerated drug discovery, with 'chemical language' processing (CLP) emerging as a prominent approach. CLP learns from molecular string representations (e.g., Simplified Molecular Input Line Entry Systems [SMILES] and Self-Referencing Embedded Strings [SELFIES]) with methods akin to natural language processing. Despite their growing importance, training predictive CLP models is far from trivial, as it involves many 'bells and whistles'. Here, we analyze the key elements of CLP training, to provide guidelines for newcomers and experts alike. Our study spans three neural network architectures, two string representations, three embedding strategies, across ten bioactivity datasets, for both classification and regression purposes. This 'hitchhiker's guide' not only underscores the importance of certain methodological choices, but it also equips researchers with practical recommendations on ideal choices, e.g., in terms of neural network architectures, molecular representations, and hyperparameter optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12152v1</guid>
      <category>q-bio.BM</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R{\i}za \"Oz\c{c}elik, Francesca Grisoni</dc:creator>
    </item>
    <item>
      <title>Directly Optimizing for Synthesizability in Generative Molecular Design using Retrosynthesis Models</title>
      <link>https://arxiv.org/abs/2407.12186</link>
      <description>arXiv:2407.12186v1 Announce Type: new 
Abstract: Synthesizability in generative molecular design remains a pressing challenge. Existing methods to assess synthesizability span heuristics-based methods, retrosynthesis models, and synthesizability-constrained molecular generation. The latter has become increasingly prevalent and proceeds by defining a set of permitted actions a model can take when generating molecules, such that all generations are anchored in "synthetically-feasible" chemical transformations. To date, retrosynthesis models have been mostly used as a post-hoc filtering tool as their inference cost remains prohibitive to use directly in an optimization loop. In this work, we show that with a sufficiently sample-efficient generative model, it is straightforward to directly optimize for synthesizability using retrosynthesis models in goal-directed generation. Under a heavily-constrained computational budget, our model can generate molecules satisfying a multi-parameter drug discovery optimization task while being synthesizable, as deemed by the retrosynthesis model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12186v1</guid>
      <category>q-bio.BM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeff Guo, Philippe Schwaller</dc:creator>
    </item>
    <item>
      <title>A foundation model approach to guide antimicrobial peptide design in the era of artificial intelligence driven scientific discovery</title>
      <link>https://arxiv.org/abs/2407.12296</link>
      <description>arXiv:2407.12296v1 Announce Type: new 
Abstract: We propose AMP-Designer, an LLM-based foundation model approach for the rapid design of novel antimicrobial peptides (AMPs) with multiple desired properties. Within 11 days, AMP-Designer enables de novo design of 18 novel candidates with broad-spectrum potency against Gram-negative bacteria. Subsequent in vitro validation experiments demonstrate that almost all in silico recommended candidates exhibit notable antibacterial activity, yielding a 94.4% positive rate. Two of these candidates exhibit exceptional activity, minimal hemotoxicity, substantial stability in human plasma, and a low propensity of inducing antibiotic resistance as observed in murine lung infection experiments, showcasing their significant efficacy in reducing bacterial load by approximately one hundredfold. The entire process, from in silico design to in vitro and in vivo validation, is completed within a timeframe of 48 days. Moreover, AMP-Designer demonstrates its remarkable capability in designing specific AMPs to target strains with extremely limited labeled datasets. The most outstanding candidate against Propionibacterium acnes suggested by AMP-Designer exhibits an in vitro minimum inhibitory concentration value of 2.0 $\mu$g/ml. Through the integration of advanced machine learning methodologies such as contrastive prompt tuning, knowledge distillation, and reinforcement learning within the AMP-Designer framework, the process of designing AMPs demonstrates exceptional efficiency. This efficiency remains conspicuous even in the face of challenges posed by constraints arising from a scarcity of labeled data. These findings highlight the tremendous potential of AMP-Designer as a promising approach in combating the global health threat of antibiotic resistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12296v1</guid>
      <category>q-bio.BM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jike Wang, Jianwen Feng, Yu Kang, Peichen Pan, Jingxuan Ge, Yan Wang, Mingyang Wang, Zhenxing Wu, Xingcai Zhang, Jiameng Yu, Xujun Zhang, Tianyue Wang, Lirong Wen, Guangning Yan, Yafeng Deng, Hui Shi, Chang-Yu Hsieh, Zhihui Jiang, Tingjun Hou</dc:creator>
    </item>
    <item>
      <title>Importance Weighted Expectation-Maximization for Protein Sequence Design</title>
      <link>https://arxiv.org/abs/2305.00386</link>
      <description>arXiv:2305.00386v3 Announce Type: replace 
Abstract: Designing protein sequences with desired biological function is crucial in biology and chemistry. Recent machine learning methods use a surrogate sequence-function model to replace the expensive wet-lab validation. How can we efficiently generate diverse and novel protein sequences with high fitness? In this paper, we propose IsEM-Pro, an approach to generate protein sequences towards a given fitness criterion. At its core, IsEM-Pro is a latent generative model, augmented by combinatorial structure features from a separately learned Markov random fields (MRFs). We develop an Monte Carlo Expectation-Maximization method (MCEM) to learn the model. During inference, sampling from its latent space enhances diversity while its MRFs features guide the exploration in high fitness regions. Experiments on eight protein sequence design tasks show that our IsEM-Pro outperforms the previous best methods by at least 55% on average fitness score and generates more diverse and novel protein sequences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00386v3</guid>
      <category>q-bio.BM</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenqiao Song, Lei Li</dc:creator>
    </item>
    <item>
      <title>DisorderUnetLM: Validating ProteinUnet for efficient protein intrinsic disorder prediction</title>
      <link>https://arxiv.org/abs/2404.08108</link>
      <description>arXiv:2404.08108v3 Announce Type: replace-cross 
Abstract: The prediction of intrinsic disorder regions has significant implications for understanding protein functions and dynamics. It can help to discover novel protein-protein interactions essential for designing new drugs and enzymes. Recently, a new generation of predictors based on protein language models (pLMs) is emerging. These algorithms reach state-of-the-art accuracy with-out calculating time-consuming multiple sequence alignments (MSAs). The article introduces the new DisorderUnetLM disorder predictor, which builds upon the idea of ProteinUnet. It uses the Attention U-Net convolutional neural network and incorporates features from the ProtTrans pLM. DisorderUnetLM achieves top results in the direct comparison with recent predictors exploiting MSAs and pLMs. Moreover, among 43 predictors from the latest CAID-2 benchmark, it ranks 1st for the Disorder-NOX subset (ROC-AUC of 0.844) and 10th for the Disorder-PDB subset (ROC-AUC of 0.924). The code and model are publicly available and fully reproducible at doi.org/10.24433/CO.7350682.v1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08108v3</guid>
      <category>cs.LG</category>
      <category>q-bio.BM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Krzysztof Kotowski, Irena Roterman, Katarzyna Stapor</dc:creator>
    </item>
  </channel>
</rss>
