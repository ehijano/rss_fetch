<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Oct 2024 20:49:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fast Approximate Counting of Cycles</title>
      <link>https://arxiv.org/abs/2409.19292</link>
      <description>arXiv:2409.19292v1 Announce Type: new 
Abstract: We consider the problem of approximate counting of triangles and longer fixed length cycles in directed graphs. For triangles, T\v{e}tek [ICALP'22] gave an algorithm that returns a $(1 \pm \eps)$-approximation in $\tilde{O}(n^\omega/t^{\omega-2})$ time, where $t$ is the unknown number of triangles in the given $n$ node graph and $\omega&lt;2.372$ is the matrix multiplication exponent. We obtain an improved algorithm whose running time is, within polylogarithmic factors the same as that for multiplying an $n\times n/t$ matrix by an $n/t \times n$ matrix. We then extend our framework to obtain the first nontrivial $(1 \pm \eps)$-approximation algorithms for the number of $h$-cycles in a graph, for any constant $h\geq 3$. Our running time is
  \[\tilde{O}(\mathsf{MM}(n,n/t^{1/(h-2)},n)), \textrm{the time to multiply } n\times \frac{n}{t^{1/(h-2)}} \textrm{ by } \frac{n}{t^{1/(h-2)}}\times n \textrm{ matrices}.\]
  Finally, we show that under popular fine-grained hypotheses, this running time is optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19292v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keren Censor-Hillel, Tomer Even, Virginia Vassilevska Williams</dc:creator>
    </item>
    <item>
      <title>Exact Algorithms for Clustered Planarity with Linear Saturators</title>
      <link>https://arxiv.org/abs/2409.19410</link>
      <description>arXiv:2409.19410v1 Announce Type: new 
Abstract: We study Clustered Planarity with Linear Saturators, which is the problem of augmenting an $n$-vertex planar graph whose vertices are partitioned into independent sets (called clusters) with paths - one for each cluster - that connect all the vertices in each cluster while maintaining planarity. We show that the problem can be solved in time $2^{O(n)}$ for both the variable and fixed embedding case. Moreover, we show that it can be solved in subexponential time $2^{O(\sqrt{n}\log n)}$ in the fixed embedding case if additionally the input graph is connected. The latter time complexity is tight under the Exponential-Time Hypothesis. We also show that $n$ can be replaced with the vertex cover number of the input graph by providing a linear (resp. polynomial) kernel for the variable-embedding (resp. fixed-embedding) case; these results contrast the NP-hardness of the problem on graphs of bounded treewidth (and even on trees). Finally, we complement known lower bounds for the problem by showing that Clustered Planarity with Linear Saturators is NP-hard even when the number of clusters is at most $3$, thus excluding the algorithmic use of the number of clusters as a parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19410v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giordano Da Lozzo, Robert Ganian, Siddharth Gupta, Bojan Mohar, Sebastian Ordyniak, Meirav Zehavi</dc:creator>
    </item>
    <item>
      <title>Online and Offline Algorithms for Counting Distinct Closed Factors via Sliding Suffix Trees</title>
      <link>https://arxiv.org/abs/2409.19576</link>
      <description>arXiv:2409.19576v1 Announce Type: new 
Abstract: A string is said to be closed if its length is one, or if it has a non-empty factor that occurs both as a prefix and as a suffix of the string, but does not occur elsewhere. The notion of closed words was introduced by [Fici, WORDS 2011]. Recently, the maximum number of distinct closed factors occurring in a string was investigated by [Parshina and Puzynina, Theor. Comput. Sci. 2024], and an asymptotic tight bound was proved. In this paper, we propose two algorithms to count the distinct closed factors in a string T of length n over an alphabet of size \sigma. The first algorithm runs in O(n log \sigma) time using O(n) space for string T given in an online manner. The second algorithm runs in O(n) time using O(n) space for string T given in an offline manner. Both algorithms utilize suffix trees for sliding windows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19576v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Mieno, Shun Takahashi, Kazuhisa Seto, Takashi Horiyama</dc:creator>
    </item>
    <item>
      <title>The problem of computing a $2$-T-connected spanning subgraph with minimum number of edges in directed graphs</title>
      <link>https://arxiv.org/abs/2409.19773</link>
      <description>arXiv:2409.19773v1 Announce Type: new 
Abstract: Let $G=(V,E)$ be a strongly connected graph with $|V|\geq 3$. For $T\subseteq V$, the strongly connected graph $G$ is $2$-T-connected if $G$ is $2$-edge-connected and for each vertex $w$ in $T$, $w$ is not a strong articulation point. This concept generalizes the concept of $2$-vertex connectivity when $T$ contains all the vertices in $G$. This concept also generalizes the concept of $2$-edge connectivity when $|T|=0$. The concept of $2$-T-connectivity was introduced by Durand de Gevigney and Szigeti in $2018$. In this paper, we prove that there is a polynomial-time 4-approximation algorithm for the following problem: given a $2$-T-connected graph $G=(V,E)$, identify a subset $E^ {2T} \subseteq E$ of minimum cardinality such that $(V,E^{2T})$ is $2$-T-connected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19773v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raed Jaberi, Reham Mansour</dc:creator>
    </item>
    <item>
      <title>Acceleration Meets Inverse Maintenance: Faster $\ell_{\infty}$-Regression</title>
      <link>https://arxiv.org/abs/2409.20030</link>
      <description>arXiv:2409.20030v1 Announce Type: new 
Abstract: We propose a randomized multiplicative weight update (MWU) algorithm for $\ell_{\infty}$ regression that runs in $\widetilde{O}\left(n^{2+1/22.5} \text{poly}(1/\epsilon)\right)$ time when $\omega = 2+o(1)$, improving upon the previous best $\widetilde{O}\left(n^{2+1/18} \text{poly} \log(1/\epsilon)\right)$ runtime in the low-accuracy regime. Our algorithm combines state-of-the-art inverse maintenance data structures with acceleration. In order to do so, we propose a novel acceleration scheme for MWU that exhibits {\it stabiliy} and {\it robustness}, which are required for the efficient implementations of the inverse maintenance data structures.
  We also design a faster {\it deterministic} MWU algorithm that runs in $\widetilde{O}\left(n^{2+1/12}\text{poly}(1/\epsilon)\right))$ time when $\omega = 2+o(1)$, improving upon the previous best $\widetilde{O}\left(n^{2+1/6} \text{poly} \log(1/\epsilon)\right)$ runtime in the low-accuracy regime. We achieve this by showing a novel stability result that goes beyond the previous known works based on interior point methods (IPMs).
  Our work is the first to use acceleration and inverse maintenance together efficiently, finally making the two most important building blocks of modern structured convex optimization compatible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20030v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deeksha Adil, Shunhua Jiang, Rasmus Kyng</dc:creator>
    </item>
    <item>
      <title>Simple Realizability of Abstract Topological Graphs</title>
      <link>https://arxiv.org/abs/2409.20108</link>
      <description>arXiv:2409.20108v1 Announce Type: new 
Abstract: An abstract topological graph (AT-graph) is a pair $A=(G,\mathcal{X})$, where $G=(V,E)$ is a graph and $\mathcal{X} \subseteq {E \choose 2}$ is a set of pairs of edges of $G$. A realization of $A$ is a drawing $\Gamma_A$ of $G$ in the plane such that any two edges $e_1,e_2$ of $G$ cross in $\Gamma_A$ if and only if $(e_1,e_2) \in \mathcal{X}$; $\Gamma_A$ is simple if any two edges intersect at most once (either at a common endpoint or at a proper crossing). The AT-graph Realizability (ATR) problem asks whether an input AT-graph admits a realization. The version of this problem that requires a simple realization is called Simple AT-graph Realizability (SATR). It is a classical result that both ATR and SATR are NP-complete.
  In this paper, we study the SATR problem from a new structural perspective. More precisely, we consider the size $\mathrm{\lambda}(A)$ of the largest connected component of the crossing graph of any realization of $A$, i.e., the graph ${\cal C}(A) = (E, \mathcal{X})$. This parameter represents a natural way to measure the level of interplay among edge crossings. First, we prove that SATR is NP-complete when $\mathrm{\lambda}(A) \geq 6$. On the positive side, we give an optimal linear-time algorithm that solves SATR when $\mathrm{\lambda}(A) \leq 3$ and returns a simple realization if one exists. Our algorithm is based on several ingredients, in particular the reduction to a new embedding problem subject to constraints that require certain pairs of edges to alternate (in the rotation system), and a sequence of transformations that exploit the interplay between alternation constraints and the SPQR-tree and PQ-tree data structures to eventually arrive at a simpler embedding problem that can be solved with standard techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20108v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giordano Da Lozzo, Walter Didimo, Fabrizio Montecchiani, Miriam M\"unch, Maurizio Patrignani, Ignaz Rutter</dc:creator>
    </item>
    <item>
      <title>Sliding Block (Slick) Hashing: An Implementation &amp; Benchmarks</title>
      <link>https://arxiv.org/abs/2409.20125</link>
      <description>arXiv:2409.20125v1 Announce Type: new 
Abstract: With hash tables being one of the most used data structures, Lehmann, Sanders and Walzer propose a novel, light-weight hash table, referred to as Slick Hash. Their idea is to hit a sweet spot between space consumption and speed. Building on the theoretical ideas by the authors, an implementation and experiments are required to evaluate the practical performance of Slick Hash. This work contributes to fulfilling this requirement by providing a basic implementation of Slick Hash, an analysis of its performance, and an evaluation of the entry deletion, focusing on the impact of backyard cleaning. The findings are discussed, and a conclusion is drawn.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20125v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Oberst</dc:creator>
    </item>
    <item>
      <title>RSVP: Beyond Weisfeiler Lehman Graph Isomorphism Test</title>
      <link>https://arxiv.org/abs/2409.20157</link>
      <description>arXiv:2409.20157v1 Announce Type: new 
Abstract: Graph isomorphism, a classical algorithmic problem, determines whether two input graphs are structurally identical or not. Interestingly, it is one of the few problems that is not yet known to belong to either the P or NP-complete complexity classes. As such, intelligent search-space pruning based strategies were proposed for developing isomorphism testing solvers like nauty and bliss, which are still, unfortunately, exponential in the worst-case scenario. Thus, the polynomial-time Weisfeiler-Lehman (WL) isomorphism testing heuristic, based on colour refinement, has been widely adopted in the literature. However, WL fails for multiple classes of non-isomorphic graph instances such as strongly regular graphs, block structures, and switched edges, among others. In this paper, we propose a novel polynomial-time graph isomorphism testing heuristic, RSVP, and depict its enhanced discriminative power compared to the Weisfeiler-Lehman approach for several challenging classes of graphs. Bounded by a run-time complexity of O(m^2+mn^2+n^3) (where n and m are the number of vertices and edges respectively), we show that RSVP can identify non-isomorphism in several 'hard' graph instance classes including Miyazaki, Paulus, cubic hypohamiltonian, strongly regular, Latin series and Steiner triple system graphs, where the 3-WL test fails. Similar to the WL test, our proposed algorithm is prone to only one-sided errors, where isomorphic graphs will never be determined to be non-isomorphic, although the reverse can happen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20157v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Dutta, Arnab Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Efficient Approximation of Fractional Hypertree Width</title>
      <link>https://arxiv.org/abs/2409.20172</link>
      <description>arXiv:2409.20172v1 Announce Type: new 
Abstract: We give two new approximation algorithms to compute the fractional hypertree width of an input hypergraph. The first algorithm takes as input $n$-vertex $m$-edge hypergraph $H$ of fractional hypertree width at most $\omega$, runs in polynomial time and produces a tree decomposition of $H$ of fractional hypertree width $O(\omega \log n \log \omega)$. As an immediate corollary this yields polynomial time $O(\log^2 n \log \omega)$-approximation algorithms for (generalized) hypertree width as well. To the best of our knowledge our algorithm is the first non-trivial polynomial-time approximation algorithm for fractional hypertree width and (generalized) hypertree width, as opposed to algorithms that run in polynomial time only when $\omega$ is considered a constant. For hypergraphs with the bounded intersection property we get better bounds, comparable with that recent algorithm of Lanzinger and Razgon [STACS 2024].
  The second algorithm runs in time $n^{\omega}m^{O(1)}$ and produces a tree decomposition of $H$ of fractional hypertree width $O(\omega \log^2 \omega)$. This significantly improves over the $(n+m)^{O(\omega^3)}$ time algorithm of Marx [ACM TALG 2010], which produces a tree decomposition of fractional hypertree width $O(\omega^3)$, both in terms of running time and the approximation ratio.
  Our main technical contribution, and the key insight behind both algorithms, is a variant of the classic Menger's Theorem for clique separators in graphs: For every graph $G$, vertex sets $A$ and $B$, family ${\cal F}$ of cliques in $G$, and positive rational $f$, either there exists a sub-family of $O(f \cdot \log^2 n)$ cliques in ${\cal F}$ whose union separates $A$ from $B$, or there exist $f \cdot \log |{\cal F}|$ paths from $A$ to $B$ such that no clique in ${\cal F}$ intersects more than $\log |{\cal F}|$ paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20172v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.DM</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Viktoriia Korchemna, Daniel Lokshtanov, Saket Saurabh, Vaishali Surianarayanan, Jie Xue</dc:creator>
    </item>
    <item>
      <title>Graph matching based on similarities in structure and attributes</title>
      <link>https://arxiv.org/abs/2409.20212</link>
      <description>arXiv:2409.20212v1 Announce Type: new 
Abstract: Finding vertex-to-vertex correspondences in real-world graphs is a challenging task with applications in a wide variety of domains. Structural matching based on graphs connectivities has attracted considerable attention, while the integration of all the other information stemming from vertices and edges attributes has been mostly left aside. Here we present the Graph Attributes and Structure Matching (GASM) algorithm, which provides high-quality solutions by integrating all the available information in a unified framework. Parameters quantifying the reliability of the attributes can tune how much the solutions should rely on the structure or on the attributes. We further show that even without attributes GASM consistently finds as-good-as or better solutions than state-of-the-art algorithms, with similar processing times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20212v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rapha\"el Candelier</dc:creator>
    </item>
    <item>
      <title>Parameterised Approximation and Complexity of Minimum Flow Decompositions</title>
      <link>https://arxiv.org/abs/2409.20278</link>
      <description>arXiv:2409.20278v1 Announce Type: new 
Abstract: Minimum flow decomposition (MFD) is the strongly NP-hard problem of finding a smallest set of integer weighted paths in a graph $G$ whose weighted sum is equal to a given flow $f$ on $G$. Despite its many practical applications, we lack an understanding of graph structures that make MFD easy or hard. In particular, it is not known whether a good approximation algorithm exists when the weights are positive.
  On the positive side, the main result of this paper is that MFD can be approximated within a factor $O(\log\Vert f\Vert)$ (where $\Vert f\Vert$ is the largest flow weight of all edges) times the ratio between the parallel-width of $G$ (introduced by Deligkas and Meir, MFCS 2018) and the width of $G$ (minimum number of paths to cover all edges). In particular, when the MFD size is at least the parallel-width of $G$, this becomes the first parameterised $O(\log\Vert f\Vert)$-factor approximation algorithm for MFD over positive integers. We also show that there exist instances where the ratio between the parallel-width of $G$ and the MFD size is arbitrarily large, thus narrowing down the class of graphs whose approximation is still open. We achieve these results by introducing a new notion of flow-width of $(G,f)$, which unifies both the width and the parallel-width and may be of independent interest.
  On the negative side, we show that small-width graphs do not make MFD easy. This question was previously open, because width-1 graphs (i.e. paths) are trivially solvable, and the existing NP-hardness proofs use graphs of unbounded width. We close this problem by showing the tight results that MFD remains strongly NP-hard on graphs of width 3, and NP-hard on graphs of width 2 (and thus also parallel-width 2). Moreover, on width-2 graphs (and more generally, on constant parallel-width graphs), MFD is solvable in quasi-polynomial time on unary-coded flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20278v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Grigorjew, Wanchote Jiamjitrak, Brendan Mumey, Alexandru I. Tomescu</dc:creator>
    </item>
    <item>
      <title>A faster algorithm for the $k$-forest problem: breaking the $O_k(n^{3/2})$ complexity barrier</title>
      <link>https://arxiv.org/abs/2409.20314</link>
      <description>arXiv:2409.20314v1 Announce Type: new 
Abstract: The $k$-forest problem asks to find $k$ forests in a graph $G$ maximizing the number of edges in their union. We show how to solve this problem in $O(k^3 \min\{kn, m\} \log^2 n + k \cdot{\rm MAXFLOW}(m, m) \log n)$ time, breaking the $O_k(n^{3/2})$ complexity barrier of previously known approaches. Our algorithm relies on three subroutines: the directed $k$-forest problem with bounded indegree condition, the $k$-pseudoforest problem, and the top clump computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20314v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavel Arkhipov, Vladimir Kolmogorov</dc:creator>
    </item>
    <item>
      <title>TwinArray Sort: An Ultrarapid Conditional Non-Comparison Based Sorting Algorithm</title>
      <link>https://arxiv.org/abs/2409.20362</link>
      <description>arXiv:2409.20362v1 Announce Type: new 
Abstract: In computer science, sorting algorithms are crucial for data processing and machine learning. Large datasets and high efficiency requirements provide challenges for comparison-based algorithms like Quicksort and Merge sort, which achieve O(n log n) time complexity. Non-comparison-based algorithms like Spreadsort and Counting Sort have memory consumption issues and a relatively high computational demand, even if they can attain linear time complexity under certain circumstances. We present TwinArray Sort, a novel conditional non-comparison-based sorting algorithm that effectively uses array indices. When it comes to worst-case time and space complexities, TwinArray Sort achieves O(n+k). The approach remains efficient under all settings and works well with datasets with randomly sorted, reverse-sorted, or nearly sorted distributions. TwinArray Sort can handle duplicates and optimize memory efficiently since thanks to its two auxiliary arrays for value storage and frequency counting, as well as a conditional distinct array verifier. TwinArray Sort constantly performs better than conventional algorithms, according to experimental assessments and particularly when sorting unique arrays under all data distribution scenarios. The approach is suitable for massive data processing and machine learning dataset management due to its creative use of dual auxiliary arrays and a conditional distinct array verification, which improves memory use and duplication handling. TwinArray Sort overcomes conventional sorting algorithmic constraints by combining cutting-edge methods with non-comparison-based sorting advantages. Its reliable performance in a range of data distributions makes it an adaptable and effective answer for contemporary computing requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20362v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amin Amini</dc:creator>
    </item>
    <item>
      <title>The Secretary Problem with Predicted Additive Gap</title>
      <link>https://arxiv.org/abs/2409.20460</link>
      <description>arXiv:2409.20460v1 Announce Type: new 
Abstract: The secretary problem is one of the fundamental problems in online decision making; a tight competitive ratio for this problem of $1/\mathrm{e} \approx 0.368$ has been known since the 1960s. Much more recently, the study of algorithms with predictions was introduced: The algorithm is equipped with a (possibly erroneous) additional piece of information upfront which can be used to improve the algorithm's performance. Complementing previous work on secretary problems with prior knowledge, we tackle the following question:
  What is the weakest piece of information that allows us to break the $1/\mathrm{e}$ barrier?
  To this end, we introduce the secretary problem with predicted additive gap. As in the classical problem, weights are fixed by an adversary and elements appear in random order. In contrast to previous variants of predictions, our algorithm only has access to a much weaker piece of information: an \emph{additive gap} $c$. This gap is the difference between the highest and $k$-th highest weight in the sequence. Unlike previous pieces of advice, knowing an exact additive gap does not make the problem trivial. Our contribution is twofold. First, we show that for any index $k$ and any gap $c$, we can obtain a competitive ratio of $0.4$ when knowing the exact gap (even if we do not know $k$), hence beating the prevalent bound for the classical problem by a constant. Second, a slightly modified version of our algorithm allows to prove standard robustness-consistency properties as well as improved guarantees when knowing a range for the error of the prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20460v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Braun, Sherry Sarkar</dc:creator>
    </item>
    <item>
      <title>A Characterization of List Regression</title>
      <link>https://arxiv.org/abs/2409.19218</link>
      <description>arXiv:2409.19218v1 Announce Type: cross 
Abstract: There has been a recent interest in understanding and characterizing the sample complexity of list learning tasks, where the learning algorithm is allowed to make a short list of $k$ predictions, and we simply require one of the predictions to be correct. This includes recent works characterizing the PAC sample complexity of standard list classification and online list classification.
  Adding to this theme, in this work, we provide a complete characterization of list PAC regression. We propose two combinatorial dimensions, namely the $k$-OIG dimension and the $k$-fat-shattering dimension, and show that they optimally characterize realizable and agnostic $k$-list regression respectively. These quantities generalize known dimensions for standard regression. Our work thus extends existing list learning characterizations from classification to regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19218v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chirag Pabbaraju, Sahasrajit Sarmasarkar</dc:creator>
    </item>
    <item>
      <title>Symmetry Preservation in Swarms of Oblivious Robots with Limited Visibility</title>
      <link>https://arxiv.org/abs/2409.19277</link>
      <description>arXiv:2409.19277v1 Announce Type: cross 
Abstract: In the general pattern formation (GPF) problem, a swarm of simple autonomous, disoriented robots must form a given pattern. The robots' simplicity imply a strong limitation: When the initial configuration is rotationally symmetric, only patterns with a similar symmetry can be formed [Yamashita, Suzyuki; TCS 2010]. The only known algorithm to form large patterns with limited visibility and without memory requires the robots to start in a near-gathering (a swarm of constant diameter) [Hahn et al.; SAND 2024]. However, not only do we not know any near-gathering algorithm guaranteed to preserve symmetry but most natural gathering strategies trivially increase symmetries [Castenow et al.; OPODIS 2022].
  Thus, we study near-gathering without changing the swarm's rotational symmetry for disoriented, oblivious robots with limited visibility (the OBLOT-model, see [Flocchini et al.; 2019]). We introduce a technique based on the theory of dynamical systems to analyze how a given algorithm affects symmetry and provide sufficient conditions for symmetry preservation. Until now, it was unknown whether the considered OBLOT-model allows for any non-trivial algorithm that always preserves symmetry. Our first result shows that a variant of Go-to-the-Average always preserves symmetry but may sometimes lead to multiple, unconnected near-gathering clusters. Our second result is a symmetry-preserving near-gathering algorithm that works on swarms with a convex boundary (the outer boundary of the unit disc graph) and without holes (circles of diameter 1 inside the boundary without any robots).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19277v1</guid>
      <category>cs.RO</category>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Gerlach, S\"oren von der Gracht, Christopher Hahn, Jonas Harbig, Peter Kling</dc:creator>
    </item>
    <item>
      <title>Strongly-Polynomial Time and Validation Analysis of Policy Gradient Methods</title>
      <link>https://arxiv.org/abs/2409.19437</link>
      <description>arXiv:2409.19437v1 Announce Type: cross 
Abstract: Reinforcement learning lacks a principled measure of optimality, causing research to rely on algorithm-to-algorithm or baselines comparisons with no certificate of optimality. Focusing on finite state and action Markov decision processes (MDP), we develop a simple, computable gap function that provides both upper and lower bounds on the optimality gap. Therefore, convergence of the gap function is a stronger mode of convergence than convergence of the optimality gap, and it is equivalent to a new notion we call distribution-free convergence, where convergence is independent of any problem-dependent distribution. We show the basic policy mirror descent exhibits fast distribution-free convergence for both the deterministic and stochastic setting. We leverage the distribution-free convergence to a uncover a couple new results. First, the deterministic policy mirror descent can solve unregularized MDPs in strongly-polynomial time. Second, accuracy estimates can be obtained with no additional samples while running stochastic policy mirror descent and can be used as a termination criteria, which can be verified in the validation step.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19437v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caleb Ju, Guanghui Lan</dc:creator>
    </item>
    <item>
      <title>Making Quickhull More Like Quicksort: A Simple Randomized Output-Sensitive Convex Hull Algorithm</title>
      <link>https://arxiv.org/abs/2409.19784</link>
      <description>arXiv:2409.19784v1 Announce Type: cross 
Abstract: In this paper, we present Ray-shooting Quickhull, which is a simple, randomized, outputsensitive version of the Quickhull algorithm for constructing the convex hull of a set of n points in the plane. We show that the randomized Ray-shooting Quickhull algorithm runs in O(n log h) expected time, where h is the number of points on the boundary of the convex hull. Keeping with the spirit of the original Quickhull algorithm, our algorithm is quite simple and is, in fact, closer in spirit to the well-known randomized Quicksort algorithm. Unlike the original Quickhull algorithm, however, which can run in ${\Theta}(n^2) time$ for some input distributions, the expected performance bounds for the randomized Ray-shooting Quickhull algorithm match or improve the performance bounds of more complicated algorithms. Importantly, the expectation in our output-sensitive performance bound does not depend on assumptions about the distribution of input points. Still, we show that, like the deterministic Quickhull algorithm, our randomized Ray-shooting Quickhull algorithm runs in O(n) expected time for n points chosen uniformly at random from a bounded convex region. We also provide experimental evidence that the randomized Ray-shooting Quickhull algorithm is on par or faster than deterministic Quickhull in practice, depending on the input distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19784v1</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael T. Goodrich, Ryuto Kitagawa</dc:creator>
    </item>
    <item>
      <title>Efficient Constant-Factor Approximate Enumeration of Minimal Subsets for Monotone Properties with Weight Constraints</title>
      <link>https://arxiv.org/abs/2009.08830</link>
      <description>arXiv:2009.08830v4 Announce Type: replace 
Abstract: A property $\Pi$ on a finite set $U$ is \emph{monotone} if for every $X \subseteq U$ satisfying $\Pi$, every superset $Y \subseteq U$ of $X$ also satisfies $\Pi$. Many combinatorial properties can be seen as monotone properties. The problem of finding a minimum subset of $U$ satisfying $\Pi$ is a central problem in combinatorial optimization. Although many approximate/exact algorithms have been developed to solve this kind of problem on numerous properties, a solution obtained by these algorithms is often unsuitable for real-world applications due to the difficulty of building accurate mathematical models on real-world problems. A promising approach to overcome this difficulty is to \emph{enumerate} multiple small solutions rather than to \emph{find} a single small solution. To this end, given a weight function $w: U \to \mathbb N$ and an integer $k$, we devise algorithms that \emph{approximately} enumerate all minimal subsets of $U$ with weight at most $k$ satisfying $\Pi$ for various monotone properties $\Pi$, where "approximate enumeration" means that algorithms output all minimal subsets satisfying $\Pi$ whose weight at most $k$ and may output some minimal subsets satisfying $\Pi$ whose weight exceeds $k$ but is at most $ck$ for some constant $c \ge 1$. These algorithms allow us to efficiently enumerate minimal vertex covers, minimal dominating sets in bounded degree graphs, minimal feedback vertex sets, minimal hitting sets in bounded rank hypergraphs, etc., of weight at most $k$ with constant approximation factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2009.08830v4</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuaki Kobayashi, Kazuhiro Kurita, Kunihiro Wasa</dc:creator>
    </item>
    <item>
      <title>Simple and efficient four-cycle counting on sparse graphs</title>
      <link>https://arxiv.org/abs/2303.06090</link>
      <description>arXiv:2303.06090v5 Announce Type: replace 
Abstract: We consider the problem of counting 4-cycles ($C_4$) in an undirected graph $G$ of $n$ vertices and $m$ edges (in bipartite graphs, 4-cycles are also often referred to as $\textit{butterflies}$). Most recently, Wang et al. (2019, 2022) developed algorithms for this problem based on hash tables and sorting the graph by degree. Their algorithm takes $O(m\bar\delta)$ expected time and $O(m)$ space, where $\bar \delta \leq O(\sqrt{m})$ is the $\textit{average degeneracy}$ parameter introduced by Burkhardt, Faber \&amp; Harris (2020). We develop a streamlined version of this algorithm requiring $O(m\bar\delta)$ time and precisely $n$ words of space. It has several practical improvements and optimizations; for example, it is fully deterministic, does not require any auxiliary storage or sorting of the input graph, and uses only addition and array access in its inner loops.
  Our algorithm is very simple and easily adapted to count 4-cycles incident to each vertex and edge. Empirical tests demonstrate that our array-based approach is $4\times$ -- $7\times$ faster on average compared to popular hash table implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.06090v5</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Burkhardt, David G. Harris</dc:creator>
    </item>
    <item>
      <title>Total Domination, Separated Clusters, CD-Coloring: Algorithms and Hardness</title>
      <link>https://arxiv.org/abs/2307.12073</link>
      <description>arXiv:2307.12073v2 Announce Type: replace 
Abstract: Domination and coloring are two classic problems in graph theory. The major focus of this paper is the CD-COLORING problem which combines the flavours of domination and colouring. Let $G$ be an undirected graph. A proper vertex coloring of $G$ is a $cd-coloring$ if each color class has a dominating vertex in $G$. The minimum integer $k$ for which there exists a $cd-coloring$ of $G$ using $k$ colors is called the cd-chromatic number, $\chi_{cd}(G)$. A set $S\subseteq V(G)$ is a total dominating set if any vertex in $G$ has a neighbor in $S$. The total domination number, $\gamma_t(G)$ of $G$ is the minimum integer $k$ such that $G$ has a total dominating set of size $k$. A set $S\subseteq V(G)$ is a $separated-cluster$ if no two vertices in $S$ lie at a distance 2 in $G$. The separated-cluster number, $\omega_s(G)$, of $G$ is the maximum integer $k$ such that $G$ has a separated-cluster of size $k$.
  In this paper, first we explore the connection between CD-COLORING and TOTAL DOMINATION. We prove that CD-COLORING and TOTAL DOMINATION are NP-Complete on triangle-free $d$-regular graphs for each fixed integer $d\geq 3$. We also study the relationship between the parameters $\chi_{cd}(G)$ and $\omega_s(G)$. Analogous to the well-known notion of `perfectness', here we introduce the notion of `cd-perfectness'. We prove a sufficient condition for a graph $G$ to be cd-perfect (i.e. $\chi_{cd}(H)= \omega_s(H)$, for any induced subgraph $H$ of $G$) which is also necessary for certain graph classes (like triangle-free graphs). Here, we propose a generalized framework via which we obtain several exciting consequences in the algorithmic complexities of special graph classes. In addition, we settle an open problem by showing that the SEPARATED-CLUSTER is polynomially solvable for interval graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12073v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dhanyamol Antony, L. Sunil Chandran, Ankit Gayen, Shirish Gosavi, Dalu Jacob</dc:creator>
    </item>
    <item>
      <title>Approximating Fair $k$-Min-Sum-Radii in Euclidean Space</title>
      <link>https://arxiv.org/abs/2309.00834</link>
      <description>arXiv:2309.00834v2 Announce Type: replace 
Abstract: The $k$-center problem is a classical clustering problem in which one is asked to find a partitioning of a point set $P$ into $k$ clusters such that the maximum radius of any cluster is minimized. It is well-studied. But what if we add up the radii of the clusters instead of only considering the cluster with maximum radius? This natural variant is called the $k$-min-sum-radii problem. It has become the subject of more and more interest in recent years, inspiring the development of approximation algorithms for the $k$-min-sum-radii problem in its plain version as well as in constrained settings.
  We study the problem for Euclidean spaces $\mathbb{R}^d$ of arbitrary dimension but assume the number $k$ of clusters to be constant. In this case, a PTAS for the problem is known (see Bandyapadhyay, Lochet and Saurabh, SoCG, 2023). Our aim is to extend the knowledge base for $k$-min-sum-radii to the domain of fair clustering. We study several group fairness constraints, such as the one introduced by Chierichetti et al. (NeurIPS, 2017). In this model, input points have an additional attribute (e.g., colors such as red and blue), and clusters have to preserve the ratio between different attribute values (e.g., have the same fraction of red and blue points as the ground set). Different variants of this general idea have been studied in the literature. To the best of our knowledge, no approximative results for the fair $k$-min-sum-radii problem are known, despite the immense amount of work on the related fair $k$-center problem.
  We propose a PTAS for the fair $k$-min-sum-radii problem in Euclidean spaces of arbitrary dimension for the case of constant $k$. To the best of our knowledge, this is the first PTAS for the problem. It works for different notions of group fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.00834v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-49815-2_9</arxiv:DOI>
      <arxiv:journal_reference>Approximation and Online Algorithms (WAOA 2023). Lecture Notes in Computer Science, vol 14297, pp. 119-133</arxiv:journal_reference>
      <dc:creator>Lukas Drexler, Annika Hennes, Abhiruk Lahiri, Melanie Schmidt, Julian Wargalla</dc:creator>
    </item>
    <item>
      <title>Single Family Algebra Operation on BDDs and ZDDs Leads To Exponential Blow-Up</title>
      <link>https://arxiv.org/abs/2403.05074</link>
      <description>arXiv:2403.05074v2 Announce Type: replace 
Abstract: Binary decision diagram (BDD) and zero-suppressed binary decision diagram (ZDD) are data structures to represent a family of (sub)sets compactly, and it can be used as succinct indexes for a family of sets. To build BDD/ZDD representing a desired family of sets, there are many transformation operations that take BDDs/ZDDs as inputs and output BDD/ZDD representing the resultant family after performing operations such as set union and intersection. However, except for some basic operations, the worst-time complexity of taking such transformation on BDDs/ZDDs has not been extensively studied, and some contradictory statements about it have arisen in the literature. In this paper, we show that many transformation operations on BDDs/ZDDs, including all operations for families of sets that appear in Knuth's book, cannot be performed in worst-case polynomial time in the size of input BDDs/ZDDs. This refutes some of the folklore circulated in past literature and resolves an open problem raised by Knuth. Our results are stronger in that such blow-up of computational time occurs even when the ordering, which has a significant impact on the efficiency of treating BDDs/ZDDs, is chosen arbitrarily.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05074v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kengo Nakamura, Masaaki Nishino, Shuhei Denzumi</dc:creator>
    </item>
    <item>
      <title>Finding Decision Tree Splits in Streaming and Massively Parallel Models</title>
      <link>https://arxiv.org/abs/2403.19867</link>
      <description>arXiv:2403.19867v3 Announce Type: replace 
Abstract: In this work, we provide data stream algorithms that compute optimal splits in decision tree learning. In particular, given a data stream of observations $x_i$ and their labels $y_i$, the goal is to find the optimal split $j$ that divides the data into two sets such that the mean squared error (for regression) or misclassification rate and Gini impurity (for classification) is minimized. We provide several fast streaming algorithms that use sublinear space and a small number of passes for these problems. These algorithms can also be extended to the massively parallel computation model. Our work, while not directly comparable, complements the seminal work of Domingos-Hulten (KDD 2000) and Hulten-Spencer-Domingos (KDD 2001).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19867v3</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huy Pham, Hoang Ta, Hoa T. Vu</dc:creator>
    </item>
    <item>
      <title>Online Multi-level Aggregation with Delays and Stochastic Arrivals</title>
      <link>https://arxiv.org/abs/2404.09711</link>
      <description>arXiv:2404.09711v2 Announce Type: replace 
Abstract: This paper presents a new research direction for online Multi-Level Aggregation (MLA) with delays. In this problem, we are given an edge-weighted rooted tree $T$, and we have to serve a sequence of requests arriving at its vertices in an online manner. Each request $r$ is characterized by two parameters: its arrival time $t(r)$ and location $l(r)$ (a vertex). Once a request $r$ arrives, we can either serve it immediately or postpone this action until any time $t &gt; t(r)$. We can serve several pending requests at the same time, and the service cost of a service corresponds to the weight of the subtree that contains all the requests served and the root of $T$. Postponing the service of a request $r$ to time $t &gt; t(r)$ generates an additional delay cost of $t - t(r)$. The goal is to serve all requests in an online manner such that the total cost (i.e., the total sum of service and delay costs) is minimized. The current best algorithm for this problem achieves a competitive ratio of $O(d^2)$ (Azar and Touitou, FOCS'19), where $d$ denotes the depth of the tree.
  Here, we consider a stochastic version of MLA where the requests follow a Poisson arrival process. We present a deterministic online algorithm which achieves a constant ratio of expectations, meaning that the ratio between the expected costs of the solution generated by our algorithm and the optimal offline solution is bounded by a constant. Our algorithm is obtained by carefully combining two strategies. In the first one, we plan periodic oblivious visits to the subset of frequent vertices, whereas in the second one, we greedily serve the pending requests in the remaining vertices. This problem is complex enough to demonstrate a very rare phenomenon that ``single-minded" or ``sample-average" strategies are not enough in stochastic optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09711v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathieu Mari, Micha{\l} Paw{\l}owski, Runtian Ren, Piotr Sankowski</dc:creator>
    </item>
    <item>
      <title>Online Disjoint Set Covers: Randomization is not Necessary</title>
      <link>https://arxiv.org/abs/2404.15554</link>
      <description>arXiv:2404.15554v2 Announce Type: replace 
Abstract: In the online disjoint set covers problem, the edges of a hypergraph are revealed online, and the goal is to partition them into a maximum number of disjoint set covers. That is, n nodes of a hypergraph are given at the beginning, and then a sequence of hyperedges (subsets of [n]) is presented to an algorithm. For each hyperedge, an online algorithm must assign a color (an integer). Once an input terminates, the gain of the algorithm is the number of colors that correspond to valid set covers (i.e., the union of hyperedges that have that color contains all n nodes).
  We present a deterministic online algorithm that is O(log^2 n)-competitive, exponentially improving on the previous bound of O(n) and matching the performance of the best randomized algorithm by Emek et al. [ESA 2019].
  For color selection, our algorithm uses a novel potential function, which can be seen as an online counterpart of the derandomization method of conditional probabilities and pessimistic estimators. There are only a few cases where derandomization has been successfully used in the field of online algorithms. In contrast to previous approaches, our result extends to the following new challenges: (i) the potential function derandomizes not only the Chernoff bound, but also the coupon collector's problem, (ii) the value of OPT of the maximization problem is not bounded a priori, and (iii) we do not produce a fractional solution first, but work directly on the input.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15554v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcin Bienkowski, Jaros{\l}aw Byrka, {\L}ukasz Je\.z</dc:creator>
    </item>
    <item>
      <title>ReeSPOT: Reeb Graph Models Semantic Patterns of Normalcy in Human Trajectories</title>
      <link>https://arxiv.org/abs/2405.00808</link>
      <description>arXiv:2405.00808v3 Announce Type: replace 
Abstract: This paper introduces ReeSPOT, a novel Reeb graph-based method to model patterns of life in human trajectories (akin to a fingerprint). Human behavior typically follows a pattern of normalcy in day-to-day activities. This is marked by recurring activities within specific time periods. In this paper, we model this behavior using Reeb graphs where any deviation from usual day-to-day activities is encoded as nodes in the Reeb graph. The complexity of the proposed algorithm is linear with respect to the number of time points in a given trajectory. We demonstrate the usage of ReeSPOT and how it captures the critically significant spatial and temporal deviations using the nodes of the Reeb graph. Our case study presented in this paper includes realistic human movement scenarios: visiting uncommon locations, taking odd routes at infrequent times, uncommon time visits, and uncommon stay durations. We analyze the Reeb graph to interpret the topological structure of the GPS trajectories. Potential applications of ReeSPOT include urban planning, security surveillance, and behavioral research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00808v3</guid>
      <category>cs.DS</category>
      <category>cs.CE</category>
      <category>cs.SI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bowen Zhang, S. Shailja, Chandrakanth Gudavalli, Connor Levenson, Amil Khan, B. S. Manjunath</dc:creator>
    </item>
    <item>
      <title>Knapsack with Vertex Cover, Set Cover, and Hitting Set</title>
      <link>https://arxiv.org/abs/2406.01057</link>
      <description>arXiv:2406.01057v3 Announce Type: replace 
Abstract: Given an undirected graph $\mathcal{G}=(\mathcal{V},\mathcal{E})$, with vertex weights $(w(u))_{u\in\mathcal{V}}$, vertex values $(\alpha(u))_{u\in\mathcal{V}}$, a knapsack size $s$, and a target value $d$, the \vcknapsack problem is to determine if there exists a subset $\mathcal{U}\subseteq\mathcal{V}$ of vertices such that $\mathcal{U}$ forms a vertex cover, $w(\mathcal{U})=\sum_{u\in\mathcal{U}} w(u) \le s$, and $\alpha(\mathcal{U})=\sum_{u\in\mathcal{U}} \alpha(u) \ge d$. In this paper, we closely study the \vcknapsack problem and its variations, such as \vcknapsackbudget, \minimalvcknapsack, and \minimumvcknapsack, for both general graphs and trees. We first prove that the \vcknapsack problem belongs to the complexity class \NPC and then study the complexity of the other variations. We generalize the problem to \setc and \hs versions and design polynomial time $H_g$-factor approximation algorithm for the \setckp problem and d-factor approximation algorithm for \hstp using primal dual method. We further show that \setcks and \hsmb are hard to approximate in polynomial time. Additionally, we develop a fixed parameter tractable algorithm running in time $8^{\mathcal{O}({\rm tw})}\cdot n\cdot {\sf min}\{s,d\}$ where ${\rm tw},s,d,n$ are respectively treewidth of the graph, the size of the knapsack, the target value of the knapsack, and the number of items for the \minimalvcknapsack problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01057v3</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Palash Dey, Ashlesha Hota, Sudeshna Kolay, Sipra Singh</dc:creator>
    </item>
    <item>
      <title>Optimizing Information Access in Networks via Edge Augmentation</title>
      <link>https://arxiv.org/abs/2407.02624</link>
      <description>arXiv:2407.02624v2 Announce Type: replace 
Abstract: Given a graph $G = (V, E)$ and a model of information flow on that network, a fundamental question is to understand whether all nodes have sufficient access to information generated at other nodes in the graph. If not, we can ask if a small set of interventions in the form of edge additions improve information access. Formally, the broadcast value of a network is defined to be the minimum over pairs $u,v \in V$ of the probability that an information cascade starting at $u$ reaches $v$. Having a high broadcast value ensures that every node has sufficient access to information spreading in a network, thus quantifying fairness of access. In this paper, we formally study the Broadcast Improvement problem: given $G$ and a parameter $k$, the goal is to find the best set of $k$ edges to add to $G$ in order to maximize the broadcast value of the resulting graph.
  We develop efficient approximation algorithms for this problem. If the optimal solution adds $k$ edges and achieves a broadcast of $\beta^*$, we develop algorithms that can (a) add $k$ edges and achieve a broadcast value roughly $(\beta^*)^4/16^k$, or (b) add $O(k\log n)$ edges and achieve a broadcast roughly $\beta^*$. We also provide other trade-offs that can be better depending on the parameter values. Our algorithms rely on novel probabilistic tools to reason about the existence of paths in edge-sampled graphs, and extend to a single-source variant of the problem, where we obtain analogous algorithmic results. We complement our results by proving that unless P = NP, any algorithm that adds $O(k)$ edges must lose significantly in the approximation of $\beta^*$, resolving an open question from prior work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02624v2</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Bhaskara, Alex Crane, Shweta Jain, Md Mumtahin Habib Ullah Mazumder, Blair D. Sullivan, Prasanth Yalamanchili</dc:creator>
    </item>
    <item>
      <title>Infrequent Resolving Algorithm for Online Linear Programming</title>
      <link>https://arxiv.org/abs/2408.00465</link>
      <description>arXiv:2408.00465v3 Announce Type: replace 
Abstract: Online linear programming (OLP) has gained significant attention from both researchers and practitioners due to its extensive applications, such as online auction, network revenue management and advertising. Existing OLP algorithms fall into two categories: LP-based algorithms and LP-free algorithms. The former one typically guarantees better performance, even offering a constant regret, but requires solving a large number of LPs, which could be computationally expensive. In contrast, LP-free algorithm only requires first-order computations but induces a worse performance, lacking a constant regret bound. In this work, we study the case where the inputs are drawn from an unknown finite-support distribution, and bridge the gap between these two extremes by proposing an algorithm that achieves a constant regret while solving LPs only $O(\log\log T)$ times over the time horizon $T$. Moreover, when we are allowed to solve LPs only $M$ times, we propose an algorithm that can guarantee an $O\left(T^{(1/2+\epsilon)^{M-1}}\right)$ regret. Furthermore, when the arrival probabilities are known at the beginning, our algorithm can guarantee a constant regret by solving LPs $O(\log\log T)$ times, and an $O\left(T^{(1/2+\epsilon)^{M}}\right)$ regret by solving LPs only $M$ times. Numerical experiments are conducted to demonstrate the efficiency of the proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00465v3</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guokai Li, Zizhuo Wang, Jingwei Zhang</dc:creator>
    </item>
    <item>
      <title>Deterministic Algorithm and Faster Algorithm for Submodular Maximization subject to a Matroid Constraint</title>
      <link>https://arxiv.org/abs/2408.03583</link>
      <description>arXiv:2408.03583v2 Announce Type: replace 
Abstract: We study the problem of maximizing a monotone submodular function subject to a matroid constraint, and present for it a deterministic non-oblivious local search algorithm that has an approximation guarantee of $1 - 1/e - \varepsilon$ (for any $\varepsilon &gt; 0$) and query complexity of $\tilde{O}_\varepsilon(nr)$, where $n$ is the size of the ground set and $r$ is the rank of the matroid. Our algorithm vastly improves over the previous state-of-the-art $0.5008$-approximation deterministic algorithm, and in fact, shows that there is no separation between the approximation guarantees that can be obtained by deterministic and randomized algorithms for the problem considered. The query complexity of our algorithm can be improved to $\tilde{O}_\varepsilon(n + r\sqrt{n})$ using randomization, which is nearly-linear for $r = O(\sqrt{n})$, and is always at least as good as the previous state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03583v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niv Buchbinder, Moran Feldman</dc:creator>
    </item>
    <item>
      <title>Algebraic Vertex Ordering of a Sparse Graph for Adjacency Access Locality and Graph Compression</title>
      <link>https://arxiv.org/abs/2408.08439</link>
      <description>arXiv:2408.08439v2 Announce Type: replace 
Abstract: In this work, we establish theoretical and practical connections between vertex indexing for sparse graph/network compression and matrix ordering for sparse matrix-vector multiplication and variable elimination. We present a fundamental analysis of adjacency access locality in vertex ordering from the perspective of graph composition of, or decomposition into, elementary compact graphs. We introduce an algebraic indexing approach that maintains the advantageous features of existing methods, mitigates their shortcomings, and adapts to the degree distribution. The new method demonstrates superior and versatile performance in graph compression across diverse types of graphs. It also renders proportional improvement in the efficiency of matrix-vector multiplications for subspace iterations in response to random walk queries on a large network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08439v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimitris Floros, Nikos Pitsianis, Xiaobai Sun</dc:creator>
    </item>
    <item>
      <title>Parameterised distance to local irregularity</title>
      <link>https://arxiv.org/abs/2307.04583</link>
      <description>arXiv:2307.04583v2 Announce Type: replace-cross 
Abstract: A graph $G$ is \emph{locally irregular} if no two of its adjacent vertices have the same degree. In [Fioravantes et al. Complexity of finding maximum locally irregular induced subgraph. {\it SWAT}, 2022], the authors introduced and studied the problem of finding a locally irregular induced subgraph of a given a graph $G$ of maximum order, or, equivalently, computing a subset $S$ of $V(G)$ of minimum order, whose deletion from $G$ results in a locally irregular graph; $S$ is denoted as an \emph{optimal vertex-irregulator of $G$}. In this work we provide an in-depth analysis of the parameterised complexity of computing an optimal vertex-irregulator of a given graph $G$. Moreover, we introduce and study a variation of this problem, where $S$ is a substet of the edges of $G$; in this case, $S$ is denoted as an \emph{optimal edge-irregulator of $G$}. In particular, we prove that computing an optimal vertex-irregulator of a graph $G$ is in FPT when parameterised by the vertex integrity, neighborhood diversity or cluster deletion number of $G$, while it is $W[1]$-hard when parameterised by the feedback vertex set number or the treedepth of $G$. In the case of computing an optimal edge-irregulator of a graph $G$, we prove that this problem is in FPT when parameterised by the vertex integrity of $G$, while it is NP-hard even if $G$ is a planar bipartite graph of maximum degree $4$, and $W[1]$-hard when parameterised by the size of the solution, the feedback vertex set or the treedepth of $G$. Our results paint a comprehensive picture of the tractability of both problems studied here, considering most of the standard graph-structural parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04583v2</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Foivos Fioravantes, Nikolaos Melissinos, Theofilos Triommatis</dc:creator>
    </item>
    <item>
      <title>Scaling up the Banded Matrix Factorization Mechanism for Differentially Private ML</title>
      <link>https://arxiv.org/abs/2405.15913</link>
      <description>arXiv:2405.15913v2 Announce Type: replace-cross 
Abstract: Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have proven to be effective alternatives to DP-SGD in large-epsilon few-epoch training regimes. Significant work has been done to find the best correlated noise strategies, and the current state-of-the-art approach is DP-BandMF, which optimally balances the benefits of privacy amplification and noise correlation. Despite it's utility advantages, severe scalability limitations prevent this mechanism from handling large-scale training scenarios where the number of training iterations may exceed $10^4$ and the number of model parameters may exceed $10^7$. In this work, we present techniques to scale up DP-BandMF along these two dimensions, significantly extending it's reach and enabling it to effectively handle settings with over $10^6$ training iterations and $10^9$ model parameters, with negligible utility degradation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15913v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan McKenna</dc:creator>
    </item>
  </channel>
</rss>
