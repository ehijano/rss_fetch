<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Aug 2024 01:39:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>WIP: Identifying Tutorial Affordances for Interdisciplinary Learning Environments</title>
      <link>https://arxiv.org/abs/2408.14576</link>
      <description>arXiv:2408.14576v1 Announce Type: new 
Abstract: This work-in-progress research paper explores the effectiveness of tutorials in interdisciplinary learning environments, specifically focusing on bioinformatics. Tutorials are typically designed for a single audience, but our study aims to uncover how they function in contexts where learners have diverse backgrounds. With the rise of interdisciplinary learning, the importance of learning materials that accommodate diverse learner needs has become evident. We chose bioinformatics as our context because it involves at least two distinct user groups: those with computational backgrounds and those with biological backgrounds. The goal of our research is to better understand current bioinformatics software tutorial designs and assess them in the conceptual framework of interdisciplinarity. We conducted a content analysis of 22 representative bioinformatics software tutorials to identify design patterns and understand their strengths and limitations. We found common codes in the representative tutorials and synthesized them into ten themes. Our assessment shows degrees to which current bioinformatics software tutorials fulfill interdisciplinarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14576v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Kim, Sergei L. Kosakovsky Pond, Stephen MacNeil</dc:creator>
    </item>
    <item>
      <title>The Impact of Group Discussion and Formation on Student Performance: An Experience Report in a Large CS1 Course</title>
      <link>https://arxiv.org/abs/2408.14610</link>
      <description>arXiv:2408.14610v1 Announce Type: new 
Abstract: Programming instructors often conduct collaborative learning activities, such as Peer Instruction (PI), to enhance student motivation, engagement, and learning gains. However, the impact of group discussion and formation mechanisms on student performance remains unclear. To investigate this, we conducted an 11-session experiment in a large, in-person CS1 course. We employed both random and expertise-balanced grouping methods to examine the efficacy of different group mechanisms and the impact of expert students' presence on collaborative learning. Our observations revealed complex dynamics within the collaborative learning environment. Among 255 groups, 146 actively engaged in discussions, with 96 of these groups demonstrating improvement for poor-performing students. Interestingly, our analysis revealed that different grouping methods (expertise-balanced or random) did not significantly influence discussion engagement or poor-performing students' improvement. In our deeper qualitative analysis, we found that struggling students often derived benefits from interactions with expert peers, but this positive effect was not consistent across all groups. We identified challenges that expert students face in peer instruction interactions, highlighting the complexity of leveraging expertise within group discussions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14610v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tong Wu, Xiaohang Tang, Sam Wong, Xi Chen, Clifford A. Shaffer, Yan Chen</dc:creator>
    </item>
    <item>
      <title>Properties of Effective Information Anonymity Regulations</title>
      <link>https://arxiv.org/abs/2408.14740</link>
      <description>arXiv:2408.14740v1 Announce Type: new 
Abstract: A firm seeks to analyze a dataset and to release the results. The dataset contains information about individual people, and the firm is subject to some regulation that forbids the release of the dataset itself. The regulation also imposes conditions on the release of the results. What properties should the regulation satisfy? We restrict our attention to regulations tailored to controlling the downstream effects of the release specifically on the individuals to whom the data relate. A particular example of interest is an anonymization rule, where a data protection regulation limiting the disclosure of personally identifiable information does not restrict the distribution of data that has been sufficiently anonymized.
  In this paper, we develop a set of technical requirements for anonymization rules and related regulations. The requirements are derived by situating within a simple abstract model of data processing a set of guiding general principles put forth in prior work. We describe an approach to evaluating such regulations using these requirements -- thus enabling the application of the general principles for the design of mechanisms. As an exemplar, we evaluate competing interpretations of regulatory requirements from the EU's General Data Protection Regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14740v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aloni Cohen, Micah Altman, Francesca Falzon, Evangelina Anna Markatou, Kobbi Nissim</dc:creator>
    </item>
    <item>
      <title>Measuring Human Contribution in AI-Assisted Content Generation</title>
      <link>https://arxiv.org/abs/2408.14792</link>
      <description>arXiv:2408.14792v1 Announce Type: new 
Abstract: With the growing prevalence of generative artificial intelligence (AI), an increasing amount of content is no longer exclusively generated by humans but by generative AI models with human guidance. This shift presents notable challenges for the delineation of originality due to the varying degrees of human contribution in AI-assisted works. This study raises the research question of measuring human contribution in AI-assisted content generation and introduces a framework to address this question that is grounded in information theory. By calculating mutual information between human input and AI-assisted output relative to self-information of AI-assisted output, we quantify the proportional information contribution of humans in content generation. Our experimental results demonstrate that the proposed measure effectively discriminates between varying degrees of human contribution across multiple creative domains. We hope that this work lays a foundation for measuring human contributions in AI-assisted content generation in the era of generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14792v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueqi Xie, Tao Qi, Jingwei Yi, Ryan Whalen, Junming Huang, Qian Ding, Yu Xie, Xing Xie, Fangzhao Wu</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification in Alzheimer's Disease Progression Modeling</title>
      <link>https://arxiv.org/abs/2408.14478</link>
      <description>arXiv:2408.14478v1 Announce Type: cross 
Abstract: With the increasing number of patients diagnosed with Alzheimer's Disease, prognosis models have the potential to aid in early disease detection. However, current approaches raise dependability concerns as they do not account for uncertainty. In this work, we compare the performance of Monte Carlo Dropout, Variational Inference, Markov Chain Monte Carlo, and Ensemble Learning trained on 512 patients to predict 4-year cognitive score trajectories with confidence bounds. We show that MC Dropout and MCMC are able to produce well-calibrated, and accurate predictions under noisy training data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14478v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wael Mobeirek, Shirley Mao</dc:creator>
    </item>
    <item>
      <title>Aiding Humans in Financial Fraud Decision Making: Toward an XAI-Visualization Framework</title>
      <link>https://arxiv.org/abs/2408.14552</link>
      <description>arXiv:2408.14552v1 Announce Type: cross 
Abstract: AI prevails in financial fraud detection and decision making. Yet, due to concerns about biased automated decision making or profiling, regulations mandate that final decisions are made by humans. Financial fraud investigators face the challenge of manually synthesizing vast amounts of unstructured information, including AI alerts, transaction histories, social media insights, and governmental laws. Current Visual Analytics (VA) systems primarily support isolated aspects of this process, such as explaining binary AI alerts and visualizing transaction patterns, thus adding yet another layer of information to the overall complexity. In this work, we propose a framework where the VA system supports decision makers throughout all stages of financial fraud investigation, including data collection, information synthesis, and human criteria iteration. We illustrate how VA can claim a central role in AI-aided decision making, ensuring that human judgment remains in control while minimizing potential biases and labor-intensive tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14552v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angelos Chatzimparmpas, Evanthia Dimara</dc:creator>
    </item>
    <item>
      <title>Sustainable Data Democratization: A Multifaceted Investment for an Equitable Future</title>
      <link>https://arxiv.org/abs/2408.14627</link>
      <description>arXiv:2408.14627v1 Announce Type: cross 
Abstract: The urgent need for data democratization in scientific research was the focal point of a panel discussion at SC23 in Denver, Colorado, from November 12 to 17, 2023. This article summarizes the outcomes of that discussion and subsequent conversations. We advocate for strategic investments in financial, human, and technological resources for sustainable data democratization. Emphasizing that data is central to scientific discovery and AI deployment, we highlight barriers such as limited access, inadequate financial incentives for cross-domain collaboration, and a shortage of workforce development initiatives. Our recommendations aim to guide decision-makers in fostering an inclusive research community, breaking down research silos, and developing a skilled workforce to advance scientific discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14627v1</guid>
      <category>cs.DB</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michela Taufer, Valerio Pascucci, Christine R. Kirkpatric, Ian T. Foster</dc:creator>
    </item>
    <item>
      <title>Constraining Participation: Affordances of Feedback Features in Interfaces to Large Language Models</title>
      <link>https://arxiv.org/abs/2408.15066</link>
      <description>arXiv:2408.15066v1 Announce Type: cross 
Abstract: Large language models (LLMs) are now accessible to anyone with a computer, a web browser, and an internet connection via browser-based interfaces, shifting the dynamics of participation in AI development. This paper examines the affordances of interactive feedback features in ChatGPT's interface, analysing how they shape user input and participation in LLM iteration. Drawing on a survey of ChatGPT users and applying the mechanisms and conditions framework of affordances, we demonstrate that these features encourage simple, frequent, and performance-focused feedback while discouraging collective input and discussions among users. We argue that this feedback format significantly constrains user participation, reinforcing power imbalances between users, the public, and companies developing LLMs. Our analysis contributes to the growing body of literature on participatory AI by critically examining the limitations of existing feedback processes and proposing directions for their redesign. To enable more meaningful public participation in AI development, we advocate for a shift away from processes focused on aligning model outputs with specific user preferences. Instead, we emphasise the need for processes that facilitate dialogue between companies and diverse 'publics' about the purpose and applications of LLMs. This approach requires attention to the ongoing work of infrastructuring - creating and sustaining the social, technical, and institutional structures necessary to address matters of concern to groups impacted by AI development and deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15066v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ned Cooper, Alexandra Zafiroglu</dc:creator>
    </item>
    <item>
      <title>Aligning XAI with EU Regulations for Smart Biomedical Devices: A Methodology for Compliance Analysis</title>
      <link>https://arxiv.org/abs/2408.15121</link>
      <description>arXiv:2408.15121v1 Announce Type: cross 
Abstract: Significant investment and development have gone into integrating Artificial Intelligence (AI) in medical and healthcare applications, leading to advanced control systems in medical technology. However, the opacity of AI systems raises concerns about essential characteristics needed in such sensitive applications, like transparency and trustworthiness. Our study addresses these concerns by investigating a process for selecting the most adequate Explainable AI (XAI) methods to comply with the explanation requirements of key EU regulations in the context of smart bioelectronics for medical devices. The adopted methodology starts with categorising smart devices by their control mechanisms (open-loop, closed-loop, and semi-closed-loop systems) and delving into their technology. Then, we analyse these regulations to define their explainability requirements for the various devices and related goals. Simultaneously, we classify XAI methods by their explanatory objectives. This allows for matching legal explainability requirements with XAI explanatory goals and determining the suitable XAI algorithms for achieving them. Our findings provide a nuanced understanding of which XAI algorithms align better with EU regulations for different types of medical devices. We demonstrate this through practical case studies on different neural implants, from chronic disease management to advanced prosthetics. This study fills a crucial gap in aligning XAI applications in bioelectronics with stringent provisions of EU regulations. It provides a practical framework for developers and researchers, ensuring their AI innovations advance healthcare technology and adhere to legal and ethical standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15121v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Sovrano, Michael Lognoul, Giulia Vilone</dc:creator>
    </item>
    <item>
      <title>Evaluating the Energy Consumption of Machine Learning: Systematic Literature Review and Experiments</title>
      <link>https://arxiv.org/abs/2408.15128</link>
      <description>arXiv:2408.15128v1 Announce Type: cross 
Abstract: Monitoring, understanding, and optimizing the energy consumption of Machine Learning (ML) are various reasons why it is necessary to evaluate the energy usage of ML. However, there exists no universal tool that can answer this question for all use cases, and there may even be disagreement on how to evaluate energy consumption for a specific use case. Tools and methods are based on different approaches, each with their own advantages and drawbacks, and they need to be mapped out and explained in order to select the most suitable one for a given situation. We address this challenge through two approaches. First, we conduct a systematic literature review of all tools and methods that permit to evaluate the energy consumption of ML (both at training and at inference), irrespective of whether they were originally designed for machine learning or general software. Second, we develop and use an experimental protocol to compare a selection of these tools and methods. The comparison is both qualitative and quantitative on a range of ML tasks of different nature (vision, language) and computational complexity. The systematic literature review serves as a comprehensive guide for understanding the array of tools and methods used in evaluating energy consumption of ML, for various use cases going from basic energy monitoring to consumption optimization. Two open-source repositories are provided for further exploration. The first one contains tools that can be used to replicate this work or extend the current review. The second repository houses the experimental protocol, allowing users to augment the protocol with new ML computing tasks and additional energy evaluation tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15128v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charlotte Rodriguez, Laura Degioanni, Laetitia Kameni, Richard Vidal, Giovanni Neglia</dc:creator>
    </item>
    <item>
      <title>Easy-access online social media metrics can effectively identify misinformation sharing users</title>
      <link>https://arxiv.org/abs/2408.15186</link>
      <description>arXiv:2408.15186v1 Announce Type: cross 
Abstract: Misinformation poses a significant challenge studied extensively by researchers, yet acquiring data to identify primary sharers is costly and challenging. To address this, we propose a low-barrier approach to differentiate social media users who are more likely to share misinformation from those who are less likely. Leveraging insights from previous studies, we demonstrate that easy-access online social network metrics -- average daily tweet count, and account age -- can be leveraged to help identify potential low factuality content spreaders on X (previously known as Twitter). We find that higher tweet frequency is positively associated with low factuality in shared content, while account age is negatively associated with it. We also find that some of the effects, namely the effect of the number of accounts followed and the number of tweets produced, differ depending on the number of followers a user has. Our findings show that relying on these easy-access social network metrics could serve as a low-barrier approach for initial identification of users who are more likely to spread misinformation, and therefore contribute to combating misinformation effectively on social media platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15186v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J\'ulia Sz\'amely, Alessandro Galeazzi, J\'ulia Koltai, Elisa Omodei</dc:creator>
    </item>
    <item>
      <title>LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet</title>
      <link>https://arxiv.org/abs/2408.15221</link>
      <description>arXiv:2408.15221v1 Announce Type: cross 
Abstract: Recent large language model (LLM) defenses have greatly improved models' ability to refuse harmful queries, even when adversarially attacked. However, LLM defenses are primarily evaluated against automated adversarial attacks in a single turn of conversation, an insufficient threat model for real-world malicious use. We demonstrate that multi-turn human jailbreaks uncover significant vulnerabilities, exceeding 70% attack success rate (ASR) on HarmBench against defenses that report single-digit ASRs with automated single-turn attacks. Human jailbreaks also reveal vulnerabilities in machine unlearning defenses, successfully recovering dual-use biosecurity knowledge from unlearned models. We compile these results into Multi-Turn Human Jailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks. We publicly release MHJ alongside a compendium of jailbreak tactics developed across dozens of commercial red teaming engagements, supporting research towards stronger LLM defenses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15221v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Li, Ziwen Han, Ian Steneker, Willow Primack, Riley Goodside, Hugh Zhang, Zifan Wang, Cristina Menghini, Summer Yue</dc:creator>
    </item>
    <item>
      <title>Attention is All They Need: Exploring the Media Archaeology of the Computer Vision Research Paper</title>
      <link>https://arxiv.org/abs/2209.11200</link>
      <description>arXiv:2209.11200v3 Announce Type: replace 
Abstract: Research papers, in addition to textual documents, are a designed interface through which researchers communicate. Recently, rapid growth has transformed that interface in many fields of computing. In this work, we examine the effects of this growth from a media archaeology perspective, through the changes to figures and tables in research papers. Specifically, we study these changes in computer vision over the past decade, as the deep learning revolution has driven unprecedented growth in the discipline. We ground our investigation through interviews with veteran researchers spanning computer vision, graphics, and visualization. Our analysis focuses on the research attention economy: how research paper elements contribute towards advertising, measuring, and disseminating an increasingly commodified "contribution." Through this work, we seek to motivate future discussion surrounding the design of both the research paper itself as well as the larger sociotechnical research publishing system, including tools for finding, reading, and writing research papers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.11200v3</guid>
      <category>cs.CY</category>
      <category>cs.CV</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3686955</arxiv:DOI>
      <dc:creator>Samuel Goree, Gabriel Appleby, David Crandall, Norman Su</dc:creator>
    </item>
    <item>
      <title>Unlikely Organizers: The Rise of Labor Activism Among Professionals in the U.S. Technology Industry</title>
      <link>https://arxiv.org/abs/2307.15790</link>
      <description>arXiv:2307.15790v2 Announce Type: replace 
Abstract: Tech workers -- professional workers in the technology industry including software engineers, product managers, UX designers, etc. -- are not normally associated with labor activism. Yet, since 2017, we have seen a significant rise in labor actions among this group. Using an original dataset, we demonstrate how, in the case of tech workers, periods of intense workplace social activism preceded later periods of heightened labor activism. Regression analysis confirms that participation in social activism increases the likelihood of labor activism six months to one year later at the same company. This finding extends Fantasia's cultures of solidarity argument to professional workers. We find that organizing emerges out of collective action and ensuing conflict with management: first, tech workers, guided by their professional interest in socially beneficial work, engage in workplace social activism. This generates solidarity among employee-participants but also creates conflict with management and leads to the emergence of labor activism among professionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15790v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>JS Tan, Nataliya Nedzhvetskaya, Emily Mazo</dc:creator>
    </item>
    <item>
      <title>PressProtect: Helping Journalists Navigate Social Media in the Face of Online Harassment</title>
      <link>https://arxiv.org/abs/2401.11032</link>
      <description>arXiv:2401.11032v2 Announce Type: replace 
Abstract: Social media has become a critical tool for journalists to disseminate their work, engage with their audience, and connect with sources. Unfortunately, journalists also regularly endure significant online harassment on social media platforms, ranging from personal attacks to doxxing to threats of physical harm. In this paper, we seek to understand how to make social media usable for journalists who face constant digital harassment. To begin, we conduct a set of need-finding interviews with Asian American and Pacific Islander journalists to understand where existing platform tools and newsroom resources fall short in adequately protecting journalists, especially those of marginalized identities. We map journalists' unmet needs to concrete design goals, which we use to build PressProtect, an interface that provides journalists greater agency when engaging with readers on Twitter/X. Through user testing with eight journalists, we evaluate PressProtect and find that participants felt it effectively protected them against harassment and could also generalize to serve other visible and vulnerable groups. We conclude with a discussion of our findings and recommendations for social platforms hoping to build defensive defaults for journalists facing online harassment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11032v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3687048</arxiv:DOI>
      <dc:creator>Catherine Han, Anne Li, Deepak Kumar, Zakir Durumeric</dc:creator>
    </item>
    <item>
      <title>Rank, Pack, or Approve: Voting Methods in Participatory Budgeting</title>
      <link>https://arxiv.org/abs/2401.12423</link>
      <description>arXiv:2401.12423v4 Announce Type: replace 
Abstract: Participatory budgeting is a popular method to engage residents in budgeting decisions by local governments. The Stanford Participatory Budgeting platform is an online platform that has been used to engage residents in more than 150 budgeting processes. We present a data set with anonymized budget opinions from these processes with K-approval, K-ranking or knapsack primary ballots. For a subset of the voters, it includes paired votes with a different elicitation method in the same process. This presents a unique data set, as the voters, projects and setting are all related to real-world decisions that the voters have an actual interest in. With data from primary ballots we find that while ballot complexity (number of projects to choose from, number of projects to select and ballot length) is correlated with a higher median time spent by voters, it is not correlated with a higher abandonment rate.
  We use vote pairs with different voting methods to analyze the effect of voting methods on the cost of selected projects, more comprehensively than was previously possible. In most elections, voters selected significantly more expensive projects using K-approval than using knapsack, although we also find a small number of examples with a significant effect in the opposite direction. This effect happens at the aggregate level as well as for individual voters, and is influenced both by the implicit constraints of the voting method and the explicit constraints of the voting interface. Finally, we validate the use of K-ranking elicitation to offer a paper alternative for knapsack voting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12423v4</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1609/icwsm.v18i1.31326</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the International AAAI Conference on Web and Social Media, 18 (2024) 448-461</arxiv:journal_reference>
      <dc:creator>Lodewijk Gelauff, Ashish Goel</dc:creator>
    </item>
    <item>
      <title>Trust and ethical considerations in a multi-modal, explainable AI-driven chatbot tutoring system: The case of collaboratively solving Rubik's Cube</title>
      <link>https://arxiv.org/abs/2402.01760</link>
      <description>arXiv:2402.01760v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) has the potential to transform education with its power of uncovering insights from massive data about student learning patterns. However, ethical and trustworthy concerns of AI have been raised but are unsolved. Prominent ethical issues in high school AI education include data privacy, information leakage, abusive language, and fairness. This paper describes technological components that were built to address ethical and trustworthy concerns in a multi-modal collaborative platform (called ALLURE chatbot) for high school students to collaborate with AI to solve the Rubik's cube. In data privacy, we want to ensure that the informed consent of children, parents, and teachers, is at the center of any data that is managed. Since children are involved, language, whether textual, audio, or visual, is acceptable both from users and AI and the system can steer interaction away from dangerous situations. In information management, we also want to ensure that the system, while learning to improve over time, does not leak information about users from one group to another.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01760v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kausik Lakkaraju, Vedant Khandelwal, Biplav Srivastava, Forest Agostinelli, Hengtao Tang, Prathamjeet Singh, Dezhi Wu, Matt Irvin, Ashish Kundu</dc:creator>
    </item>
    <item>
      <title>Packaging Up Media Mix Modeling: An Introduction to Robyn's Open-Source Approach</title>
      <link>https://arxiv.org/abs/2403.14674</link>
      <description>arXiv:2403.14674v2 Announce Type: replace 
Abstract: As privacy-centric changes reshape the digital advertising landscape, deterministic attribution and measurement of advertising-related user behavior is increasingly constrained. In response, there has been a resurgence in the use of traditional probabilistic measurement techniques, such as media and marketing mix modeling (m/MMM), particularly among digital-first advertisers. However, small and midsize businesses often lack the resources to implement advanced proprietary modeling systems, which require specialized expertise and significant team investments. To address this gap, marketing data scientists at Meta have developed the open-source computational package Robyn, designed to facilitate the adoption of m/MMM for digital advertising measurement. This article explores the computational components and design choices that underpin Robyn, emphasizing how it "packages up" m/MMM to promote organizational acceptance and mitigate common biases. As a widely adopted and actively maintained open-source tool, Robyn is continually evolving. Consequently, the solutions described here should not be seen as definitive or conclusive but as an outline of the pathways that the Robyn community has embarked on. This article aims to provide a structured introduction to these evolving practices, encouraging feedback and dialogue to ensure that Robyn's development aligns with the needs of the broader data science community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14674v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julian Runge, Igor Skokan, Gufeng Zhou</dc:creator>
    </item>
    <item>
      <title>Data Privacy Vocabulary (DPV) -- Version 2</title>
      <link>https://arxiv.org/abs/2404.13426</link>
      <description>arXiv:2404.13426v3 Announce Type: replace 
Abstract: The Data Privacy Vocabulary (DPV), developed by the W3C Data Privacy Vocabularies and Controls Community Group (DPVCG), enables the creation of machine-readable, interoperable, and standards-based representations for describing the processing of personal data. The group has also published extensions to the DPV to describe specific applications to support legislative requirements such as the EU's GDPR. The DPV fills a crucial niche in the state of the art by providing a vocabulary that can be embedded and used alongside other existing standards such as W3C ODRL, and which can be customised and extended for adapting to specifics of use-cases or domains. This article describes the version 2 iteration of the DPV in terms of its contents, methodology, current adoptions and uses, and future potential. It also describes the relevance and role of DPV in acting as a common vocabulary to support various regulatory (e.g. EU's DGA and AI Act) and community initiatives (e.g. Solid) emerging across the globe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13426v3</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harshvardhan J. Pandit, Beatriz Esteves, Georg P. Krog, Paul Ryan, Delaram Golpayegani, Julian Flake</dc:creator>
    </item>
    <item>
      <title>Personhood credentials: Artificial intelligence and the value of privacy-preserving tools to distinguish who is real online</title>
      <link>https://arxiv.org/abs/2408.07892</link>
      <description>arXiv:2408.07892v3 Announce Type: replace 
Abstract: Anonymity is an important principle online. However, malicious actors have long used misleading identities to conduct fraud, spread disinformation, and carry out other deceptive schemes. With the advent of increasingly capable AI, bad actors can amplify the potential scale and effectiveness of their operations, intensifying the challenge of balancing anonymity and trustworthiness online. In this paper, we analyze the value of a new tool to address this challenge: "personhood credentials" (PHCs), digital credentials that empower users to demonstrate that they are real people -- not AIs -- to online services, without disclosing any personal information. Such credentials can be issued by a range of trusted institutions -- governments or otherwise. A PHC system, according to our definition, could be local or global, and does not need to be biometrics-based. Two trends in AI contribute to the urgency of the challenge: AI's increasing indistinguishability from people online (i.e., lifelike content and avatars, agentic activity), and AI's increasing scalability (i.e., cost-effectiveness, accessibility). Drawing on a long history of research into anonymous credentials and "proof-of-personhood" systems, personhood credentials give people a way to signal their trustworthiness on online platforms, and offer service providers new tools for reducing misuse by bad actors. In contrast, existing countermeasures to automated deception -- such as CAPTCHAs -- are inadequate against sophisticated AI, while stringent identity verification solutions are insufficiently private for many use-cases. After surveying the benefits of personhood credentials, we also examine deployment risks and design challenges. We conclude with actionable next steps for policymakers, technologists, and standards bodies to consider in consultation with the public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07892v3</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Adler, Zo\"e Hitzig, Shrey Jain, Catherine Brewer, Wayne Chang, Ren\'ee DiResta, Eddy Lazzarin, Sean McGregor, Wendy Seltzer, Divya Siddarth, Nouran Soliman, Tobin South, Connor Spelliscy, Manu Sporny, Varya Srivastava, John Bailey, Brian Christian, Andrew Critch, Ronnie Falcon, Heather Flanagan, Kim Hamilton Duffy, Eric Ho, Claire R. Leibowicz, Srikanth Nadhamuni, Alan Z. Rozenshtein, David Schnurr, Evan Shapiro, Lacey Strahm, Andrew Trask, Zoe Weinberg, Cedric Whitney, Tom Zick</dc:creator>
    </item>
    <item>
      <title>Inference-Time Rule Eraser: Fair Recognition via Distilling and Removing Biased Rules</title>
      <link>https://arxiv.org/abs/2404.04814</link>
      <description>arXiv:2404.04814v4 Announce Type: replace-cross 
Abstract: Machine learning models often make predictions based on biased features such as gender, race, and other social attributes, posing significant fairness risks, especially in societal applications, such as hiring, banking, and criminal justice. Traditional approaches to addressing this issue involve retraining or fine-tuning neural networks with fairness-aware optimization objectives. However, these methods can be impractical due to significant computational resources, complex industrial tests, and the associated CO2 footprint. Additionally, regular users often fail to fine-tune models because they lack access to model parameters In this paper, we introduce the Inference-Time Rule Eraser (Eraser), a novel method designed to address fairness concerns by removing biased decision-making rules from deployed models during inference without altering model weights. We begin by establishing a theoretical foundation for modifying model outputs to eliminate biased rules through Bayesian analysis. Next, we present a specific implementation of Eraser that involves two stages: (1) distilling the biased rules from the deployed model into an additional patch model, and (2) removing these biased rules from the output of the deployed model during inference. Extensive experiments validate the effectiveness of our approach, showcasing its superior performance in addressing fairness concerns in AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04814v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Zhang, Dongyuan Lu, Jitao Sang</dc:creator>
    </item>
    <item>
      <title>The virtual CAT: A tool for algorithmic thinking assessment in Swiss compulsory education</title>
      <link>https://arxiv.org/abs/2408.01263</link>
      <description>arXiv:2408.01263v2 Announce Type: replace-cross 
Abstract: In today's digital era, holding algorithmic thinking (AT) skills is crucial, not only in computer science-related fields. These abilities enable individuals to break down complex problems into more manageable steps and create a sequence of actions to solve them. To address the increasing demand for AT assessments in educational settings and the limitations of current methods, this paper introduces the virtual Cross Array Task (CAT), a digital adaptation of an unplugged assessment activity designed to evaluate algorithmic skills in Swiss compulsory education. This tool offers scalable and automated assessment, reducing human involvement and mitigating potential data collection errors. The platform features gesture-based and visual block-based programming interfaces, ensuring its usability for diverse learners, further supported by multilingual capabilities. To evaluate the virtual CAT platform, we conducted a pilot evaluation in Switzerland involving a heterogeneous group of students. The findings show the platform's usability, proficiency and suitability for assessing AT skills among students of diverse ages, development stages, and educational backgrounds, as well as the feasibility of large-scale data collection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01263v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgia Adorni, Alberto Piatti</dc:creator>
    </item>
    <item>
      <title>Time Series Analysis for Education: Methods, Applications, and Future Directions</title>
      <link>https://arxiv.org/abs/2408.13960</link>
      <description>arXiv:2408.13960v2 Announce Type: replace-cross 
Abstract: Recent advancements in the collection and analysis of sequential educational data have brought time series analysis to a pivotal position in educational research, highlighting its essential role in facilitating data-driven decision-making. However, there is a lack of comprehensive summaries that consolidate these advancements. To the best of our knowledge, this paper is the first to provide a comprehensive review of time series analysis techniques specifically within the educational context. We begin by exploring the landscape of educational data analytics, categorizing various data sources and types relevant to education. We then review four prominent time series methods-forecasting, classification, clustering, and anomaly detection-illustrating their specific application points in educational settings. Subsequently, we present a range of educational scenarios and applications, focusing on how these methods are employed to address diverse educational tasks, which highlights the practical integration of multiple time series methods to solve complex educational problems. Finally, we conclude with a discussion on future directions, including personalized learning analytics, multimodal data fusion, and the role of large language models (LLMs) in educational time series. The contributions of this paper include a detailed taxonomy of educational data, a synthesis of time series techniques with specific educational applications, and a forward-looking perspective on emerging trends and future research opportunities in educational analysis. The related papers and resources are available and regularly updated at the project page.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13960v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 28 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shengzhong Mao, Chaoli Zhang, Yichi Song, Jindong Wang, Xiao-Jun Zeng, Zenglin Xu, Qingsong Wen</dc:creator>
    </item>
  </channel>
</rss>
