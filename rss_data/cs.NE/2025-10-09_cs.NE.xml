<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Oct 2025 01:56:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Associative Memory Model with Neural Networks: Memorizing multiple images with one neuron</title>
      <link>https://arxiv.org/abs/2510.06542</link>
      <description>arXiv:2510.06542v1 Announce Type: new 
Abstract: This paper presents a neural network model (associative memory model) for memory and recall of images. In this model, only a single neuron can memorize multi-images and when that neuron is activated, it is possible to recall all the memorized images at the same time. The system is composed of a single cluster of numerous neurons, referred to as the "Cue Ball," and multiple neural network layers, collectively called the "Recall Net." One of the features of this model is that several different images are stored simultaneously in one neuron, and by presenting one of the images stored in that neuron, all stored images are recalled. Furthermore, this model allows for complete recall of an image even when an incomplete image is presented</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06542v1</guid>
      <category>cs.NE</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hiroshi Inazawa</dc:creator>
    </item>
    <item>
      <title>Neuromorphic Computing -- An Overview</title>
      <link>https://arxiv.org/abs/2510.06721</link>
      <description>arXiv:2510.06721v1 Announce Type: new 
Abstract: With traditional computing technologies reaching their limit, a new field has emerged seeking to follow the example of the human brain into a new era: neuromorphic computing. This paper provides an introduction to neuromorphic computing, why this and other new computing systems are needed, and what technologies currently exist in the neuromorphic field. It begins with a general introduction into the history of traditional computing and its present problems, and then proceeds to a general overview of neuromorphic systems. It subsequently discusses the main technologies currently in development. For completeness, the paper first discusses neuromorphic-style computing on traditional hardware, and then discusses the two top branches of specialized hardware in this field; neuromorphic chips and photonic systems. Both branches are explained as well as their relative benefits and drawbacks. The paper concludes with a summary and an outlook on the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06721v1</guid>
      <category>cs.NE</category>
      <category>cs.ET</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Cognitive Science Student Journal 7 (2023) 1-10</arxiv:journal_reference>
      <dc:creator>Benedikt Jung, Maximilian Kalcher, Merlin Marinova, Piper Powell, Esma Sakalli</dc:creator>
    </item>
    <item>
      <title>The Effect of Label Noise on the Information Content of Neural Representations</title>
      <link>https://arxiv.org/abs/2510.06401</link>
      <description>arXiv:2510.06401v1 Announce Type: cross 
Abstract: In supervised classification tasks, models are trained to predict a label for each data point. In real-world datasets, these labels are often noisy due to annotation errors. While the impact of label noise on the performance of deep learning models has been widely studied, its effects on the networks' hidden representations remain poorly understood. We address this gap by systematically comparing hidden representations using the Information Imbalance, a computationally efficient proxy of conditional mutual information. Through this analysis, we observe that the information content of the hidden representations follows a double descent as a function of the number of network parameters, akin to the behavior of the test error. We further demonstrate that in the underparameterized regime, representations learned with noisy labels are more informative than those learned with clean labels, while in the overparameterized regime, these representations are equally informative. Our results indicate that the representations of overparameterized networks are robust to label noise. We also found that the information imbalance between the penultimate and pre-softmax layers decreases with cross-entropy loss in the overparameterized regime. This offers a new perspective on understanding generalization in classification tasks. Extending our analysis to representations learned from random labels, we show that these perform worse than random features. This indicates that training on random labels drives networks much beyond lazy learning, as weights adapt to encode labels information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06401v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NE</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Hussaini Umar, Franky Kevin Nando Tezoh, Jean Barbier, Santiago Acevedo, Alessandro Laio</dc:creator>
    </item>
    <item>
      <title>Visualizing Multimodality in Combinatorial Search Landscapes</title>
      <link>https://arxiv.org/abs/2510.06517</link>
      <description>arXiv:2510.06517v1 Announce Type: cross 
Abstract: This work walks through different visualization techniques for combinatorial search landscapes, focusing on multimodality. We discuss different techniques from the landscape analysis literature, and how they can be combined to provide a more comprehensive view of the search landscape. We also include examples and discuss relevant work to show how others have used these techniques in practice, based on the geometric and aesthetic elements of the Grammar of Graphics. We conclude that there is no free lunch in visualization, and provide recommendations for future work as there are several paths to continue the work in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06517v1</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xavier F. C. S\'anchez-D\'iaz, Ole Jakob Mengshoel</dc:creator>
    </item>
    <item>
      <title>Attacking the Spike: On the Transferability and Security of Spiking Neural Networks to Adversarial Examples</title>
      <link>https://arxiv.org/abs/2209.03358</link>
      <description>arXiv:2209.03358v4 Announce Type: replace 
Abstract: Spiking neural networks (SNNs) have drawn much attention for their high energy efficiency and recent advances in classification performance. However, unlike traditional deep learning, the robustness of SNNs to adversarial examples remains underexplored. This work advances the adversarial attack side of SNNs and makes three major contributions. First, we show that successful white-box attacks on SNNs strongly depend on the surrogate gradient estimation technique, even for adversarially trained models. Second, using the best single surrogate gradient estimator, we study the transferability of adversarial examples between SNNs and state-of-the-art architectures such as Vision Transformers (ViTs) and CNNs. Our analysis reveals two major gaps: no existing white-box attack leverages multiple surrogate estimators, and no single attack effectively fools both SNNs and non-SNN models simultaneously. Third, we propose the Mixed Dynamic Spiking Estimation (MDSE) attack, which dynamically combines multiple surrogate gradients to overcome these gaps. MDSE produces adversarial examples that fool both SNN and non-SNN models, achieving up to 91.4% higher effectiveness on SNN/ViT ensembles and a 3x boost on adversarially trained SNN ensembles over Auto-PGD. Experiments span three datasets (CIFAR-10, CIFAR-100, ImageNet) and nineteen classifiers, and we will release code and models upon publication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.03358v4</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.neucom.2025.131506</arxiv:DOI>
      <arxiv:journal_reference>Neurocomputing, Volume 656, 2025, 131506</arxiv:journal_reference>
      <dc:creator>Nuo Xu, Kaleel Mahmood, Haowen Fang, Ethan Rathbun, Caiwen Ding, Wujie Wen</dc:creator>
    </item>
    <item>
      <title>Lagrange Oscillatory Neural Networks for Constraint Satisfaction and Optimization</title>
      <link>https://arxiv.org/abs/2505.07179</link>
      <description>arXiv:2505.07179v2 Announce Type: replace-cross 
Abstract: Physics-inspired computing paradigms are receiving renewed attention to enhance efficiency in compute-intensive tasks such as artificial intelligence and optimization. Similar to Hopfield neural networks, oscillatory neural networks (ONNs) minimize an Ising energy function that embeds the solutions of hard combinatorial optimization problems. Despite their success in solving unconstrained optimization problems, Ising machines still face challenges with constrained problems as they can become trapped in infeasible local minima. In this paper, we introduce a Lagrange ONN (LagONN) designed to escape infeasible states based on the theory of Lagrange multipliers. Unlike existing oscillatory Ising machines, LagONN employs additional Lagrange oscillators to guide the system towards feasible states in an augmented energy landscape, settling only when constraints are met. Taking the maximum satisfiability problem with three literals as a use case (Max-3-SAT), we harness LagONN's constraint satisfaction mechanism to find optimal solutions for random SATlib instances with up to 200 variables and 860 clauses, which provides a deterministic alternative to simulated annealing for coupled oscillators. We benchmark LagONN with SAT solvers and further discuss the potential of Lagrange oscillators to address other constraints, such as phase copying, which is useful in oscillatory Ising machines with limited connectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07179v2</guid>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1088/2634-4386/ae0eab</arxiv:DOI>
      <arxiv:journal_reference>Neuromorphic Computing and Engineering, 2025</arxiv:journal_reference>
      <dc:creator>Corentin Delacour, Bram Haverkort, Filip Sabo, Nadine Azemard, Aida Todri-Sanial</dc:creator>
    </item>
    <item>
      <title>Fully Spiking Neural Networks for Unified Frame-Event Object Tracking</title>
      <link>https://arxiv.org/abs/2505.20834</link>
      <description>arXiv:2505.20834v2 Announce Type: replace-cross 
Abstract: The integration of image and event streams offers a promising approach for achieving robust visual object tracking in complex environments. However, current fusion methods achieve high performance at the cost of significant computational overhead and struggle to efficiently extract the sparse, asynchronous information from event streams, failing to leverage the energy-efficient advantages of event-driven spiking paradigms. To address this challenge, we propose the first fully Spiking Frame-Event Tracking framework called SpikeFET. This network achieves synergistic integration of convolutional local feature extraction and Transformer-based global modeling within the spiking paradigm, effectively fusing frame and event data. To overcome the degradation of translation invariance caused by convolutional padding, we introduce a Random Patchwork Module (RPM) that eliminates positional bias through randomized spatial reorganization and learnable type encoding while preserving residual structures. Furthermore, we propose a Spatial-Temporal Regularization (STR) strategy that overcomes similarity metric degradation from asymmetric features by enforcing spatio-temporal consistency among temporal template features in latent space. Extensive experiments across multiple benchmarks demonstrate that the proposed framework achieves superior tracking accuracy over existing methods while significantly reducing power consumption, attaining an optimal balance between performance and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20834v2</guid>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingjun Yang, Liangwei Fan, Jinpu Zhang, Xiangkai Lian, Hui Shen, Dewen Hu</dc:creator>
    </item>
  </channel>
</rss>
