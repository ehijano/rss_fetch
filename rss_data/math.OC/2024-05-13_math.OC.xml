<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 May 2024 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Addressing Unboundedness in Quadratically-Constrained Mixed-Integer Problems</title>
      <link>https://arxiv.org/abs/2405.05978</link>
      <description>arXiv:2405.05978v1 Announce Type: new 
Abstract: Quadratically-constrained unbounded integer programs hold the distinction of being undecidable, suggesting a possible soft-spot for Mathematical Programming (MP) techniques, which otherwise constitute a good choice to treat integer or mixed-integer (MI) problems. We consider the challenge of minimizing MI convex quadratic objective functions subject to unbounded decision variables and quadratic constraints. Given the theoretical weakness of white-box MP solvers to handle such models, we turn to black-box meta-heuristics of the Evolution Strategies (ESs) family, and question their capacity to solve this challenge. Through an empirical assessment of quadratically-constrained quadratic objective functions, across varying Hessian forms and condition numbers, we compare the performance of the CPLEX solver to state-of-the-art MI ESs, which handle constraints by penalty. Our systematic investigation begins where the CPLEX solver encounters difficulties (timeouts as the search-space dimensionality increases, (D&gt;=30), on which we report by means of detailed analyses. Overall, the empirical observations confirm that black-box and white-box solvers can be competitive, especially when the constraint function is separable, and that two common ESs' mutation operators can effectively handle the integer unboundedness. We also conclude that conditioning and separability are not intuitive factors in determining the complexity of this class of MI problems, where regular versus rough landscape structures can pose mirrored degrees of challenge for MP versus ESs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05978v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guy Zepko, Ofer M. Shir</dc:creator>
    </item>
    <item>
      <title>On the Convergence Analysis of Yau-Yau Nonlinear Filtering Algorithm: from a Probabilistic Perspective</title>
      <link>https://arxiv.org/abs/2405.06162</link>
      <description>arXiv:2405.06162v1 Announce Type: new 
Abstract: At the beginning of this century, a real time solution of the nonlinear filtering problem without memory was proposed in [1, 2] by the third author and his collaborator, and it is later on referred to as Yau-Yau algorithm. During the last two decades, a great many nonlinear filtering algorithms have been put forward and studied based on this framework. In this paper, we will generalize the results in the original works and conduct a novel convergence analysis of Yau-Yau algorithm from a probabilistic perspective. Instead of considering a particular trajectory, we estimate the expectation of the approximation error, and show that commonly-used statistics of the conditional distribution (such as conditional mean and covariance matrix) can be accurately approximated with arbitrary precision by Yau-Yau algorithm, for general nonlinear filtering systems with very liberal assumptions. This novel probabilistic version of convergence analysis is more compatible with the development of modern stochastic control theory, and will provide a more valuable theoretical guidance for practical implementations of Yau-Yau algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06162v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeju Sun, Xiuqiong Chen, Stephen S. -T. Yau</dc:creator>
    </item>
    <item>
      <title>Adversarial neural network methods for topology optimization of eigenvalue problems</title>
      <link>https://arxiv.org/abs/2405.06248</link>
      <description>arXiv:2405.06248v1 Announce Type: new 
Abstract: This research presents a novel method using an adversarial neural network to solve the eigenvalue topology optimization problems. The study focuses on optimizing the first eigenvalues of second-order elliptic and fourth-order biharmonic operators subject to geometry constraints. These models are usually solved with topology optimization algorithms based on sensitivity analysis, in which it is expensive to repeatedly solve the nonlinear constrained eigenvalue problem with traditional numerical methods such as finite elements or finite differences. In contrast, our method leverages automatic differentiation within the deep learning framework. Furthermore, the adversarial neural networks enable different neural networks to train independently, which improves the training efficiency and achieve satisfactory optimization results. Numerical results are presented to verify effectiveness of the algorithms for maximizing and minimizing the first eigenvalues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06248v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xindi Hu, Jiaming Weng, Shengfeng Zhu</dc:creator>
    </item>
    <item>
      <title>Long-Time Asymptotics of the Sliced-Wasserstein Flow</title>
      <link>https://arxiv.org/abs/2405.06313</link>
      <description>arXiv:2405.06313v1 Announce Type: new 
Abstract: The sliced-Wasserstein flow is an evolution equation where a probability density evolves in time, advected by a velocity field computed as the average among directions in the unit sphere of the optimal transport displacements from its 1D projections to the projections of a fixed target measure. This flow happens to be the gradient flow in the usual Wasserstein space of the squared sliced-Wasserstein distance to the target. We consider the question whether in long-time the flow converges to the target (providing a positive result when the target is Gaussian) and the question of the long-time limit of the flow map obtained by following the trajectories of each particle. We prove that this limit is in general not the optimal transport map from the starting measure to the target. Both questions come from the folklore about sliced-Wasserstein and had never been properly treated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06313v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giacomo Cozzi, Filippo Santambogio</dc:creator>
    </item>
    <item>
      <title>Solving maximally comonotone inclusion problems via an implicit Newton-like inertial dynamical system and its discretization</title>
      <link>https://arxiv.org/abs/2405.06332</link>
      <description>arXiv:2405.06332v1 Announce Type: new 
Abstract: This paper deals with an implicit Newton-like inertial dynamical system governed by a maximally comonotone inclusion problem in a Hilbert space. Under suitable conditions, we establish not only pointwise estimates and integral estimates for the velocity and the value of the associated Yosida regularization operator along the trajectory of the system, but also the weak convergence of the trajectory to a zero of the maximally comonotone operator. Moreover, a new inertial algorithm is developed via a time discretization of the proposed system. Our analysis reveals that the resulting discrete algorithm exhibits fast convergence properties matching the ones of the continuous time counterpart. Finally, the theoretical results are illustrated by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06332v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Z. Z. Tan, R. Hu, Y. P. Fang</dc:creator>
    </item>
    <item>
      <title>Unbounded Hamilton-Jacobi-Bellman Equations with one co-dimensional discontinuities</title>
      <link>https://arxiv.org/abs/2405.06396</link>
      <description>arXiv:2405.06396v1 Announce Type: new 
Abstract: The aim of this work is to deal with a discontinuous Hamilton-Jacobi equation in the whole euclidian N-dimensional space, associated to a possibly unbounded optimal control problem. Here, the discontinuities are located on a hyperplane and the typical questions we address concern the existence and uniqueness of solutions, and of course the definition itself of solution. We consider viscosity solutions in the sense of Ishii. The convex Hamiltonians are associated to a control problem with specific cost and dynamics given on each side of the hyperplane. We assume that those are Lipschitz continuous but the main difficulty we deal with is that they are potentially unbounded, as well as the control spaces. Using Bellman's approach we construct two value functions which turn out to be the minimal and maximal solutions in the sense of Ishii. Moreover, we also build a whole family of value functions, which are still solutions in the sense of Ishii and connect continuously the minimal solution to the maximal one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06396v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Emmanuel Chasseigne, Robson Carlos Reis, Silvia Sastre-Gomez</dc:creator>
    </item>
    <item>
      <title>An Open Source Stochastic Unit Commitment Tool using the PyPSA-Framework</title>
      <link>https://arxiv.org/abs/2405.06490</link>
      <description>arXiv:2405.06490v1 Announce Type: new 
Abstract: This paper presents an open source stochastic unit commitment (UC) optimization tool, which is available on GitHub. In addition, it presents an example use case in which UC optimization is done for a waste-to-energy plant with heat storage and a battery energy storage system (BESS) in Germany, under uncertain day-ahead and balancing power (aFRR) market prices as well as heat load uncertainty. The tool consists of multiple modular extensions for the Python for Power System Analysis (PyPSA) framework, namely the implementation of market and bidding mechanisms, stochastic optimization and multistaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06490v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tom Welfonder, Johannes Lips, Alois Gmur, Hendrik Lens</dc:creator>
    </item>
    <item>
      <title>Optimal transport of measures via autonomous vector fields</title>
      <link>https://arxiv.org/abs/2405.06503</link>
      <description>arXiv:2405.06503v1 Announce Type: new 
Abstract: We study the problem of transporting one probability measure to another via an autonomous velocity field. We rely on tools from the theory of optimal transport. In one space-dimension, we solve a linear homogeneous functional equation to construct a suitable autonomous vector field that realizes the (unique) monotone transport map as the time-$1$ map of its flow. Generically, this vector field can be chosen to be Lipschitz continuous. We then use Sudakov's disintegration approach to deal with the multi-dimensional case by reducing it to a family of one-dimensional problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06503v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>math.CA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicola De Nitti, Xavier Fern\'andez-Real</dc:creator>
    </item>
    <item>
      <title>Gradient Descent for Noisy Optimization</title>
      <link>https://arxiv.org/abs/2405.06539</link>
      <description>arXiv:2405.06539v1 Announce Type: new 
Abstract: We study the use of gradient descent with backtracking line search (GD-BLS) to solve the noisy optimization problem $\theta_\star:=\mathrm{argmin}_{\theta\in\mathbb{R}^d} \mathbb{E}[f(\theta,Z)]$, imposing that the function $F(\theta):=\mathbb{E}[f(\theta,Z)]$ is strictly convex. Assuming that $\mathbb{E}[\|\nabla_\theta f(\theta_\star,Z)\|^2]&lt;\infty$ and that objective function is locally $L$-smooth, we first prove that GD-BLS allows to estimate $\theta_\star$ with an error of size $\mathcal{O}_{\mathbb{P}}(B^{-0.25})$, where $B$ is the available computational budget. We then show that we can improve upon this rate by stopping the optimization process earlier when the gradient of the objective function is sufficiently close to zero, and use the residual computational budget to optimize, again with GD-BLS, a finer approximation of $F$. By iteratively applying this strategy $J$ times, we establish that we can estimate $\theta_\star$ with an error of size $\mathcal{O}_{\mathbb{P}}(B^{-\frac{1}{2}(1-\delta^{J})})$, where $\delta\in(1/2,1)$ is a user-specified parameter. More generally, we show that if $\mathbb{E}[\|\nabla_\theta f(\theta_\star,Z)\|^{1+\alpha}]&lt;\infty$ for some known $\alpha\in (0,1]$ then this approach allows to learn $\theta_\star$ with an error of size $\mathcal{O}_{\mathbb{P}}(B^{-\frac{\alpha}{1+\alpha}(1-\delta^{J})})$, where $\delta\in(2\alpha/(1+3\alpha),1)$ is a tuning parameter. Beyond knowing $\alpha$, achieving the aforementioned convergence rates do not require to tune the algorithms parameters according to the specific functions $F$ and $f$ at hand, and we exhibit a simple noisy optimization problem for which stochastic gradient is not guaranteed to converge while the algorithms discussed in this work are.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06539v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annie Hu, Mathieu Gerber</dc:creator>
    </item>
    <item>
      <title>Geometric Approaches to Matrix Normalization and Graph Balancing</title>
      <link>https://arxiv.org/abs/2405.06190</link>
      <description>arXiv:2405.06190v1 Announce Type: cross 
Abstract: Normal matrices, or matrices which commute with their adjoints, are of fundamental importance in pure and applied mathematics. In this paper, we study a natural functional on the space of square complex matrices whose global minimizers are normal matrices. We show that this functional, which we refer to as the non-normal energy, has incredibly well-behaved gradient descent dynamics: despite it being non-convex, we show that the only critical points of the non-normal energy are the normal matrices, and that its gradient descent trajectories fix matrix spectra and preserve the subset of real matrices. We also show that, even when restricted to the subset of unit Frobenius norm matrices, the gradient flow of the non-normal energy retains many of these useful properties. This is applied to prove that low-dimensional homotopy groups of spaces of unit norm normal matrices vanish; for example, we show that the space of $d \times d$ complex unit norm normal matrices is simply connected for all $d \geq 2$. Finally, we consider the related problem of balancing a weighted directed graph -- that is, readjusting its edge weights so that the weighted in-degree and out-degree is the same at each node. We adapt the non-normal energy to define another natural functional whose global minima are balanced graphs and show that gradient descent of this functional always converges to a balanced graph, while preserving graph spectra and realness of the weights. Our results were inspired by concepts from symplectic geometry and Geometric Invariant Theory, but we mostly avoid invoking this machinery and our proofs are generally self-contained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06190v1</guid>
      <category>math.DG</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tom Needham, Clayton Shonkwiler</dc:creator>
    </item>
    <item>
      <title>Restricted isometric compression of sparse datasets into low-dimensional varieties</title>
      <link>https://arxiv.org/abs/2405.06200</link>
      <description>arXiv:2405.06200v1 Announce Type: cross 
Abstract: This article extends the known restricted isometric projection of sparse datasets in Euclidean spaces $\mathbb{R}^N$ down into low-dimensional subspaces $\mathbb{R}^k, k \ll N,$ to the case of low-dimensional varieties $\mathcal{M} \subset \mathbb{R}^N,$ of codimension $N - k = \omega(N)$. Applications to structured/hierarchical datasets are considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06200v1</guid>
      <category>math-ph</category>
      <category>math.FA</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <category>math.RT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasile Pop, Iuliana Teodorescu, Razvan Teodorescu</dc:creator>
    </item>
    <item>
      <title>On Characterizations of Potential and Ordinal Potential Games</title>
      <link>https://arxiv.org/abs/2405.06253</link>
      <description>arXiv:2405.06253v1 Announce Type: cross 
Abstract: This paper investigates some necessary and sufficient conditions for a game to be a potential game. At first, we extend the classical results of Slade and Monderer and Shapley from games with one-dimensional action spaces to games with multi-dimensional action spaces, which require differentiable cost functions. Then, we provide a necessary and sufficient conditions for a game to have a potential function by investigating the structure of a potential function in terms of the players' cost differences, as opposed to differentials. This condition provides a systematic way for construction of a potential function, which is applied to network congestion games, as an example. Finally, we provide some sufficient conditions for a game to be ordinal potential and generalized ordinal potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06253v1</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sina Arefizadeh, Angelia Nedich, Gautam Dasarathy</dc:creator>
    </item>
    <item>
      <title>Viscosity Solutions of Second Order Path-Dependent Partial Differential Equations and Applications</title>
      <link>https://arxiv.org/abs/2405.06309</link>
      <description>arXiv:2405.06309v1 Announce Type: cross 
Abstract: In this article, a notion of viscosity solutions is introduced for fully nonlinear second order path-dependent partial differential equations in the spirit of [Zhou, Ann. Appl. Probab., 33 (2023), 5564-5612]. We prove the existence, comparison principle, consistency and stability for the viscosity solutions. Application to path-dependent stochastic differential games is given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06309v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shanjian Tang, Jianjun Zhou</dc:creator>
    </item>
    <item>
      <title>Fair Mixed Effects Support Vector Machine</title>
      <link>https://arxiv.org/abs/2405.06433</link>
      <description>arXiv:2405.06433v1 Announce Type: cross 
Abstract: To ensure unbiased and ethical automated predictions, fairness must be a core principle in machine learning applications. Fairness in machine learning aims to mitigate biases present in the training data and model imperfections that could lead to discriminatory outcomes. This is achieved by preventing the model from making decisions based on sensitive characteristics like ethnicity or sexual orientation. A fundamental assumption in machine learning is the independence of observations. However, this assumption often does not hold true for data describing social phenomena, where data points are often clustered based. Hence, if the machine learning models do not account for the cluster correlations, the results may be biased. Especially high is the bias in cases where the cluster assignment is correlated to the variable of interest. We present a fair mixed effects support vector machine algorithm that can handle both problems simultaneously. With a reproducible simulation study we demonstrate the impact of clustered data on the quality of fair machine learning predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06433v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jo\~ao Vitor Pamplona, Jan Pablo Burgard</dc:creator>
    </item>
    <item>
      <title>IETI-based Low-Rank method for PDE-constrained optimization</title>
      <link>https://arxiv.org/abs/2405.06458</link>
      <description>arXiv:2405.06458v1 Announce Type: cross 
Abstract: Isogeometric Analysis (IgA) is a versatile method for the discretization of partial differential equations on complex domains, which arise in various applications of science and engineering. Some complex geometries can be better described as a computational domain by a multi-patch approach, where each patch is determined by a tensor product Non-Uniform Rational Basis Splines (NURBS) parameterization. This allows on the one hand to consider the problem of the complex assembly of mass or stiffness matrices (or tensors) over the whole geometry locally on the individual smaller patches, and on the other hand it is possible to perform local mesh refinements independently on each patch, allowing efficient local refinement in regions of high activity where higher accuracy is required, while coarser meshes can be used elsewhere. Furthermore, the information about differing material models or properties that are to apply in a subdomain of the geometry can be included in the patch in which this subdomain is located. For this it must be ensured that the approximate solution is continuous over the entire computational domain and therefore at the interfaces of two (or more) patches. The most promising approach for this problem, which transfers the idea of Finite Element Tearing and Interconnecting (FETI) methods into the isogeometric setup, was the IsogEometric Tearing and Interconnecting (IETI) method, where by introducing a constraints matrix and associated Lagrange multipliers and formulating it into a dual problem, depending only on the Lagrange multipliers, continuity at the interfaces was ensured in solving the resulting system. In this paper we illustrate that low-rank methods based on the tensor-train format can be generalised for a multi-patch IgA setup, which follows the IETI idea.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06458v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandra B\"unger, Tom-Christian Riemer, Martin Stoll</dc:creator>
    </item>
    <item>
      <title>Model-Based Adaptive Control of Modular Multilevel Converters</title>
      <link>https://arxiv.org/abs/2405.06469</link>
      <description>arXiv:2405.06469v1 Announce Type: cross 
Abstract: Electrical power conversions are common in a large variety of engineering applications. With reference to AC/DC and DC/AC power conversions, a strong research interest resides in multilevel converters, thanks to the many advantages they provide over standard two-level converters. In this paper, we first provide a power-oriented model of Modular Multilevel Converters (MMCs), followed by a detailed harmonic analysis. The model is given in the form of a block scheme that can be directly implemented in the Matlab/Simulink environment. The performed harmonic analysis gives a deep and exact understanding of the different terms affecting the evolution of the voltage trajectories in the upper and lower arms of the converter. Next, we propose a new model-based adaptive control scheme for MMCs. The proposed control allows to determine the optimal average capacitor voltages reference in real-time, thus allowing to properly track the desired load current while minimizing the harmonic content in the generated load current itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06469v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Davide Tebaldi, Roberto Zanasi</dc:creator>
    </item>
    <item>
      <title>Microstructures and anti-phase boundaries in long-range lattice systems</title>
      <link>https://arxiv.org/abs/2405.06542</link>
      <description>arXiv:2405.06542v1 Announce Type: cross 
Abstract: We study the effect of long-range interactions in non-convex one-dimensional lattice systems in the simplified yet meaningful assumption that the relevant long-range interactions are between $M$-neighbours for some $M\ge 2$ and are convex. If short-range interactions are non-convex we then have a competition between short-range oscillations and long-range ordering. In the case of a double-well nearest-neighbour potential, thanks to a recent result by Braides, Causin, Solci and Truskinovsky, we are able to show that such a competition generates $M$-periodic minimizers whose arrangements are driven by an interfacial energy. Given $M$, the shape of such minimizers is universal, and independent of the details of the energies, but the number and shapes of such minimizers increases as $M$ diverges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06542v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andrea Braides, Edoardo Voglino, Matteo Zanardini</dc:creator>
    </item>
    <item>
      <title>Optimal epidemic control by social distancing and vaccination of an infection structured by time since infection: the covid-19 case study</title>
      <link>https://arxiv.org/abs/2405.06628</link>
      <description>arXiv:2405.06628v1 Announce Type: cross 
Abstract: Motivated by the issue of COVID-19 mitigation, in this work we tackle the general problem of optimally controlling an epidemic outbreak of a communicable disease structured by time since exposure, by the aid of two types of control instruments namely, social distancing and vaccination by a vaccine at least partly effective in protecting from infection. Effective vaccines are assumed to be made available only in a subsequent period of the epidemic so that - in the first period - epidemic control only relies on social distancing, as it happened for the COVID-19 pandemic. By our analyses, we could prove the existence of (at least) one optimal control pair, we derived first-order necessary conditions for optimality, and proved some useful properties of such optimal solutions. A worked example provides a number of further insights on the relationships between key control and epidemic parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06628v1</guid>
      <category>q-bio.PE</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/22M1499406</arxiv:DOI>
      <arxiv:journal_reference>SIAM J. Appl. Math, S199-S224, 2023</arxiv:journal_reference>
      <dc:creator>Alberto d'Onofrio, Mimmo Iannelli, Piero Manfredi, Gabriela Marinoschi</dc:creator>
    </item>
    <item>
      <title>Distributionally robust halfspace depth</title>
      <link>https://arxiv.org/abs/2101.00726</link>
      <description>arXiv:2101.00726v2 Announce Type: replace 
Abstract: Tukey's halfspace depth can be seen as a stochastic program and as such it is not guarded against optimizer's curse, so that a limited training sample may easily result in a poor out-of-sample performance. We propose a generalized halfspace depth concept relying on the recent advances in distributionally robust optimization, where every halfspace is examined using the respective worst-case distribution in the Wasserstein ball of radius $\delta\geq 0$ centered at the empirical law. This new depth can be seen as a smoothed and regularized classical halfspace depth which is retrieved as $\delta\downarrow 0$. It inherits most of the main properties of the latter and, additionally, enjoys various new attractive features such as continuity and strict positivity beyond the convex hull of the support. We provide numerical illustrations of the new depth and its advantages, and develop some fundamental theory. In particular, we study the upper level sets and the median region including their breakdown properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.00726v2</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jevgenijs Ivanovs, Pavlo Mozharovskyi</dc:creator>
    </item>
    <item>
      <title>MGProx: A nonsmooth multigrid proximal gradient method with adaptive restriction for strongly convex optimization</title>
      <link>https://arxiv.org/abs/2302.04077</link>
      <description>arXiv:2302.04077v3 Announce Type: replace 
Abstract: We study the combination of proximal gradient descent with multigrid for solving a class of possibly nonsmooth strongly convex optimization problems. We propose a multigrid proximal gradient method called MGProx, which accelerates the proximal gradient method by multigrid, based on using hierarchical information of the optimization problem. MGProx applies a newly introduced adaptive restriction operator to simplify the Minkowski sum of subdifferentials of the nondifferentiable objective function across different levels. We provide a theoretical characterization of MGProx. First we show that the MGProx update operator exhibits a fixed-point property. Next, we show that the coarse correction is a descent direction for the fine variable of the original fine level problem in the general nonsmooth case. Lastly, under some assumptions we provide the convergence rate for the algorithm. In the numerical tests on the Elastic Obstacle Problem, which is an example of nonsmooth convex optimization problem where multigrid method can be applied, we show that MGProx has a faster convergence speed than competing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.04077v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andersen Ang, Hans De Sterck, Stephen Vavasis</dc:creator>
    </item>
    <item>
      <title>Understanding the Douglas-Rachford splitting method through the lenses of Moreau-type envelopes</title>
      <link>https://arxiv.org/abs/2303.16394</link>
      <description>arXiv:2303.16394v3 Announce Type: replace 
Abstract: We analyze the Douglas-Rachford splitting method for weakly convex optimization problems, by the token of the Douglas-Rachford envelope, a merit function akin to the Moreau envelope. First, we use epi-convergence techniques to show that this artifact approximates the original objective function via epigraphs. Secondly, we present how global convergence and local linear convergence rates for Douglas-Rachford can be obtained using such envelope, under mild regularity assumptions. The keystone of the convergence analysis is the fact that the Douglas-Rachford envelope evaluated at the generated iterates satisfies a sufficient descent inequality, a feature that allows us to use arguments usually employed for descent methods. We report the results of numerical experiments that use weakly convex penalty functions, which are comparable with the known behavior of the method in the convex case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.16394v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felipe Atenas</dc:creator>
    </item>
    <item>
      <title>Predict-then-Calibrate: A New Perspective of Robust Contextual LP</title>
      <link>https://arxiv.org/abs/2305.15686</link>
      <description>arXiv:2305.15686v4 Announce Type: replace 
Abstract: Contextual optimization, also known as predict-then-optimize or prescriptive analytics, considers an optimization problem with the presence of covariates (context or side information). The goal is to learn a prediction model (from the training data) that predicts the objective function from the covariates, and then in the test phase, solve the optimization problem with the covariates but without the observation of the objective function. In this paper, we consider a risk-sensitive version of the problem and propose a generic algorithm design paradigm called predict-then-calibrate. The idea is to first develop a prediction model without concern for the downstream risk profile or robustness guarantee, and then utilize calibration (or recalibration) methods to quantify the uncertainty of the prediction. While the existing methods suffer from either a restricted choice of the prediction model or strong assumptions on the underlying data, we show the disentangling of the prediction model and the calibration/uncertainty quantification has several advantages. First, it imposes no restriction on the prediction model and thus fully unleashes the potential of off-the-shelf machine learning methods. Second, the derivation of the risk and robustness guarantee can be made independent of the choice of the prediction model through a data-splitting idea. Third, our paradigm of predict-then-calibrate applies to both (risk-sensitive) robust and (risk-neutral) distributionally robust optimization (DRO) formulations. Theoretically, it gives new generalization bounds for the contextual LP problem and sheds light on the existing results of DRO for contextual LP. Numerical experiments further reinforce the advantage of the predict-then-calibrate paradigm in that an improvement on either the prediction model or the calibration model will lead to a better final performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.15686v4</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunlin Sun, Linyu Liu, Xiaocheng Li</dc:creator>
    </item>
    <item>
      <title>Forward-Reflected-Backward and Shadow-Douglas--Rachford with partial inverse for Solving Monotone Inclusions</title>
      <link>https://arxiv.org/abs/2305.17500</link>
      <description>arXiv:2305.17500v3 Announce Type: replace 
Abstract: In this article, we study two methods for solving monotone inclusions in real Hilbert spaces involving the sum of a maximally monotone operator, a monotone-Lipschitzian operator, a cocoercive operator, and a normal cone to a vector subspace. Our algorithms split and exploits the intrinsic properties of each operator involved in the inclusion. We derive our methods by combining partial inverse techniques with the forward-reflected-backward algorithm and with the shadow-Douglas--Rachford algorithm, respectively. Our methods inherit the advantages of those methods, requiring only one activation of the Lipschitzian operator, one activation of the cocoercive operator, two projections onto the closed vector subspace, and one calculation of the resolvent of the maximally monotone operator. Additionally, to allow larger step-sizes in one of the proposed methods, we revisit FSDR by extending its convergence for larger step-sizes. Furthermore, we provide methods for solving monotone inclusions involving a sum of maximally monotone operators and for solving a system of primal-dual inclusions involving a mixture of sums, linear compositions, parallel sums, Lipschitzian operators, cocoercive operators, and normal cones. We apply our methods to constrained composite convex optimization problems as a specific example. Finally, in order to compare our methods with existing methods in the literature, we provide numerical experiments on constrained total variation least-squares optimization problems and computed tomography inverse problems. We obtain promising numerical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17500v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fernando Rold\'an</dc:creator>
    </item>
    <item>
      <title>Learning-based State Estimation in Distribution Systems with Limited Real-Time Measurements</title>
      <link>https://arxiv.org/abs/2307.16822</link>
      <description>arXiv:2307.16822v2 Announce Type: replace 
Abstract: The task of state estimation in active distribution systems faces a major challenge due to the integration of different measurements with multiple reporting rates. As a result, distribution systems are essentially unobservable in real time, indicating the existence of multiple states that result in identical values for the available measurements. Certain existing approaches utilize historical data to infer the relationship between real-time available measurements and the state. Other learning-based methods aim to estimate the measurements acquired with a delay, generating pseudo-measurements. Our paper presents a methodology that utilizes the outcome of an unobservable state estimator to exploit information on the joint probability distribution between real-time available measurements and delayed ones. Through numerical simulations conducted on a realistic distribution grid with insufficient real-time measurements, the proposed procedure showcases superior performance compared to existing state forecasting approaches and those relying on inferred pseudo-measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16822v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. G. De la Varga, S. Pineda, J. M. Morales, \'A. Porras</dc:creator>
    </item>
    <item>
      <title>Boosting Data-Driven Mirror Descent with Randomization, Equivariance, and Acceleration</title>
      <link>https://arxiv.org/abs/2308.05045</link>
      <description>arXiv:2308.05045v3 Announce Type: replace 
Abstract: Learning-to-optimize (L2O) is an emerging research area in large-scale optimization with applications in data science. Recently, researchers have proposed a novel L2O framework called learned mirror descent (LMD), based on the classical mirror descent (MD) algorithm with learnable mirror maps parameterized by input-convex neural networks. The LMD approach has been shown to significantly accelerate convex solvers while inheriting the convergence properties of the classical MD algorithm. This work proposes several practical extensions of the LMD algorithm, addressing its instability, scalability, and feasibility for high-dimensional problems. We first propose accelerated and stochastic variants of LMD, leveraging classical momentum-based acceleration and stochastic optimization techniques for improving the convergence rate and per-iteration computational complexity. Moreover, for the particular application of training neural networks, we derive and propose a novel and efficient parameterization for the mirror potential, exploiting the equivariant structure of the training problems to significantly reduce the dimensionality of the underlying problem. We provide theoretical convergence guarantees for our schemes under standard assumptions and demonstrate their effectiveness in various computational imaging and machine learning applications such as image inpainting, and the training of support vector machines and deep neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05045v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Ye Tan, Subhadip Mukherjee, Junqi Tang, Carola-Bibiane Sch\"onlieb</dc:creator>
    </item>
    <item>
      <title>Accelerated linearized alternating direction method of multipliers with Nesterov extrapolation</title>
      <link>https://arxiv.org/abs/2310.16404</link>
      <description>arXiv:2310.16404v2 Announce Type: replace 
Abstract: The alternating direction method of multipliers (ADMM) has found widespread use in solving separable convex optimization problems. In this paper, by employing Nesterov extrapolation technique, we propose two families of accelerated linearized ADMMs for addressing two-block linearly constrained separable convex optimization problems where each block of the objective function exhibits a ``nonsmooth'' + ``smooth'' composite structure. Our proposed accelerated linearized ADMMs extend two classical Nesterov acceleration methods designed for unconstrained composite optimization problems to linearly constrained problems. These methods are capable of achieving non-ergodic convergence rates of $\mathcal{O}(1/k^2)$, provided that one block of the objective function exhibits strong convexity and the gradients of smooth terms are Lipschitz continuous. We show that the proposed methods can reduce to accelerated linearized augmented Lagrangian methods (ALMs) for solving one-block linearly constrained convex optimization problems. By choosing different extrapolation parameters, we explore the relationship between the proposed methods and some existing accelerated methods. Numerical results are presented to validate the efficacy and reliability of the proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16404v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>X. He, N. J. Huang, Y. P. Fang</dc:creator>
    </item>
    <item>
      <title>Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality</title>
      <link>https://arxiv.org/abs/2401.09556</link>
      <description>arXiv:2401.09556v2 Announce Type: replace 
Abstract: This work introduces a framework to address the computational complexity inherent in Mixed-Integer Programming (MIP) models by harnessing the potential of deep learning. By employing deep learning, we construct problem-specific heuristics that identify and exploit common structures across MIP instances. We train deep learning models to estimate complicating binary variables for target MIP problem instances. The resulting reduced MIP models are solved using standard off-the-shelf solvers. We present an algorithm for generating synthetic data enhancing the robustness and generalizability of our models across diverse MIP instances. We compare the effectiveness of (a) feed-forward neural networks (ANN) and (b) convolutional neural networks (CNN). To enhance the framework's performance, we employ Bayesian optimization for hyperparameter tuning, aiming to maximize the occurrence of global optimum solutions. We apply this framework to a flow-based facility location allocation MIP formulation that describes long-term investment planning and medium-term tactical scheduling in a personalized medicine supply chain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09556v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niki Triantafyllou, Maria M. Papathanasiou</dc:creator>
    </item>
    <item>
      <title>Solving the waste bin location problem with uncertain waste generation rate: a bi-objective robust optimization approach</title>
      <link>https://arxiv.org/abs/2403.19476</link>
      <description>arXiv:2403.19476v2 Announce Type: replace 
Abstract: An efficient Municipal solid waste (MSW) system is critical to modern cities in order to enhance sustainability and livability of urban life. With this aim, the planning phase of the MSW system should be carefully addressed by decision makers. However, planning success is dependent on many sources of uncertainty that can affect key parameters of the system, e.g., the waste generation rate in an urban area. With this in mind, this paper contributes with a robust optimization model to design the network of collection points (i.e., location and storage capacity), which are the first points of contact with the MSW system. A central feature of the model is a bi-objective function that aims at simultaneously minimizing the network costs of collection points and the required collection frequency to gather the accumulated waste (as a proxy of the collection cost). The value of the model is demonstrated by comparing its solutions with those obtained from its deterministic counterpart over a set of realistic instances considering different scenarios defined by different waste generation rates. The results show that the robust model finds competitive solutions in almost all cases investigated. An additional benefit of the model is that it allows the user to explore trade-offs between the two objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19476v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1177/0734242X241248729</arxiv:DOI>
      <dc:creator>Diego Rossit, Jonathan Bard</dc:creator>
    </item>
    <item>
      <title>Decision Machines: An Extension of Decision Trees</title>
      <link>https://arxiv.org/abs/2101.11347</link>
      <description>arXiv:2101.11347v2 Announce Type: replace-cross 
Abstract: Based on decision trees, it is efficient to handle tabular data. Conventional decision tree growth methods often result in suboptimal trees because of their greedy nature. Their inherent structure limits the options of hardware to implement decision trees in parallel. Here is a compact representation of binary decision trees to overcome these deficiencies. We explicitly formulate the dependence of prediction on binary tests for binary decision trees and construct a function to guide the input sample from the root to the appropriate leaf node. And based on this formulation we introduce a new interpretation of binary decision trees. Then we approximate this formulation via continuous functions. Finally, we interpret the decision tree as a model combination method. And we propose the selection-prediction scheme to unify a few learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.11347v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinxiong Zhang</dc:creator>
    </item>
  </channel>
</rss>
