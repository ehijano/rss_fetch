<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Oct 2024 02:08:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the simultaneous recovery of two coefficients in the Helmholtz equation for inverse scattering problems via neural networks</title>
      <link>https://arxiv.org/abs/2410.01041</link>
      <description>arXiv:2410.01041v1 Announce Type: new 
Abstract: Recently, deep neural networks (DNNs) have become powerful tools for solving inverse scattering problems. However, the approximation and generalization rates of DNNs for solving these problems remain largely under-explored. In this work, we introduce two types of combined DNNs (uncompressed and compressed) to reconstruct two coefficients in the Helmholtz equation for inverse scattering problems from the scattering data at two different frequencies. An analysis of the approximation and generalization capabilities of the proposed neural networks for simulating the regularized pseudo-inverses of the linearized forward operators in direct scattering problems is provided. The results show that, with sufficient training data and parameters, the proposed neural networks can effectively approximate the inverse process with desirable generalization. Preliminary numerical results show the feasibility of the proposed neural networks for recovering two types of isotropic inhomogeneous media. Furthermore, the trained neural network is capable of reconstructing the isotropic representation of certain types of anisotropic media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01041v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zehui Zhou</dc:creator>
    </item>
    <item>
      <title>A simple linear convergence analysis of the reshuffling Kaczmarz method</title>
      <link>https://arxiv.org/abs/2410.01140</link>
      <description>arXiv:2410.01140v1 Announce Type: new 
Abstract: The Kaczmarz method and its variants, which are types of stochastic gradient descent (SGD) methods, have been extensively studied for their simplicity and efficiency in solving linear systems. Random reshuffling (RR), also known as SGD without replacement, is typically faster in practice than traditional SGD method. Although some convergence analysis results for RR apply to the reshuffling Kaczmarz method, they do not comprehensively characterize its convergence. In this paper, we present a new convergence analysis of the reshuffling Kaczmarz method and demonstrate that it can converge linearly to the unique least-norm solution of the linear system. Furthermore, the convergence upper bound is tight and does not depend on the dimension of the coefficient matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01140v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deren Han, Jiaxin Xie</dc:creator>
    </item>
    <item>
      <title>Adaptive Finite Element Method for Phase Field Fracture Models Based on Recovery Error Estimates</title>
      <link>https://arxiv.org/abs/2410.01177</link>
      <description>arXiv:2410.01177v1 Announce Type: new 
Abstract: The phase field model is a widely used mathematical approach for describing crack propagation in continuum damage fractures. In the context of phase field fracture simulations, adaptive finite element methods (AFEM) are often employed to address the mesh size dependency of the model. However, existing AFEM approaches for this application frequently rely on heuristic adjustments and empirical parameters for mesh refinement. In this paper, we introduce an adaptive finite element method based on a recovery type posteriori error estimates approach grounded in theoretical analysis. This method transforms the gradient of the numerical solution into a smoother function space, using the difference between the recovered gradient and the original numerical gradient as an error indicator for adaptive mesh refinement. This enables the automatic capture of crack propagation directions without the need for empirical parameters. We have implemented this adaptive method for the Hybrid formulation of the phase field model using the open-source software package FEALPy. The accuracy and efficiency of the proposed approach are demonstrated through simulations of classical 2D and 3D brittle fracture examples, validating the robustness and effectiveness of our implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01177v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tian Tian, Chen Chunyu, He Liang, Wei Huayi</dc:creator>
    </item>
    <item>
      <title>Fast Summation of Radial Kernels via QMC Slicing</title>
      <link>https://arxiv.org/abs/2410.01316</link>
      <description>arXiv:2410.01316v1 Announce Type: new 
Abstract: The fast computation of large kernel sums is a challenging task, which arises as a subproblem in any kernel method. We approach the problem by slicing, which relies on random projections to one-dimensional subspaces and fast Fourier summation. We prove bounds for the slicing error and propose a quasi-Monte Carlo (QMC) approach for selecting the projections based on spherical quadrature rules. Numerical examples demonstrate that our QMC-slicing approach significantly outperforms existing methods like (QMC-)random Fourier features, orthogonal Fourier features or non-QMC slicing on standard test datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01316v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Hertrich, Tim Jahn, Michael Quellmalz</dc:creator>
    </item>
    <item>
      <title>A generalized spectral concentration problem and the varying masks algorithm</title>
      <link>https://arxiv.org/abs/2410.01465</link>
      <description>arXiv:2410.01465v1 Announce Type: new 
Abstract: In this paper we generalize the spectral concentration problem as formulated by Slepian, Pollak and Landau in the 1960s. We show that a generalized version with arbitrary space and Fourier masks is well-posed, and we prove some new results concerning general quadratic domains and gaussian filters. We also propose a more general splitting representation of the spectral concentration operator allowing to construct quasi-modes in some situations. We then study its discretization and we illustrate the fact that standard eigen-algorithms are not robust because of a clustering of eigenvalues. We propose a new alternative algorithm that can be implemented in any dimension and for any domain shape, and that gives very efficient results in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01465v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.SP</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erwan Faou (IRMAR, Inria), Yoann Le Henaff</dc:creator>
    </item>
    <item>
      <title>A fast numerical scheme for fractional viscoelastic models of wave propagation</title>
      <link>https://arxiv.org/abs/2410.01467</link>
      <description>arXiv:2410.01467v1 Announce Type: new 
Abstract: We propose a fast scheme for approximating the Mittag-Leffler function by an efficient sum-of-exponentials (SOE), and apply the scheme to the viscoelastic model of wave propagation with mixed finite element methods for the spatial discretization and the Newmark-beta scheme for the second-order temporal derivative. Compared with traditional L1 scheme for fractional derivative, our fast scheme reduces the memory complexity from $\mathcal O(N_sN) $ to $\mathcal O(N_sN_{exp})$ and the computation complexity from $\mathcal O(N_sN^2)$ to $\mathcal O(N_sN_{exp}N)$, where $N$ denotes the total number of temporal grid points, $N_{exp}$ is the number of exponentials in SOE, and $N_s$ represents the complexity of memory and computation related to the spatial discretization. Numerical experiments are provided to verify the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01467v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Yuan, Xiaoping Xie</dc:creator>
    </item>
    <item>
      <title>Towards Model Discovery Using Domain Decomposition and PINNs</title>
      <link>https://arxiv.org/abs/2410.01599</link>
      <description>arXiv:2410.01599v1 Announce Type: new 
Abstract: We enhance machine learning algorithms for learning model parameters in complex systems represented by ordinary differential equations (ODEs) with domain decomposition methods. The study evaluates the performance of two approaches, namely (vanilla) Physics-Informed Neural Networks (PINNs) and Finite Basis Physics-Informed Neural Networks (FBPINNs), in learning the dynamics of test models with a quasi-stationary longtime behavior. We test the approaches for data sets in different dynamical regions and with varying noise level. As results, we find a better performance for the FBPINN approach compared to the vanilla PINN approach, even in cases with data from only a quasi-stationary time domain with few dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01599v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tirtho S. Saha, Alexander Heinlein, Cordula Reisch</dc:creator>
    </item>
    <item>
      <title>PREPARE: PREdicting PAndemic's REcurring Waves Amidst Mutations, Vaccination, and Lockdowns</title>
      <link>https://arxiv.org/abs/2410.00921</link>
      <description>arXiv:2410.00921v1 Announce Type: cross 
Abstract: This study releases an adaptable framework that can provide insights to policymakers to predict the complex recurring waves of the pandemic in the medium postemergence of the virus spread, a phase marked by rapidly changing factors like virus mutations, lockdowns, and vaccinations, offering a way to forecast infection trends and stay ahead of future outbreaks even amidst uncertainty. The proposed model is validated on data from COVID-19 spread in Germany.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00921v1</guid>
      <category>q-bio.PE</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>nlin.CD</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Narges M. Shahtori, S. Farokh Atashzar</dc:creator>
    </item>
    <item>
      <title>Structure-Preserving Operator Learning</title>
      <link>https://arxiv.org/abs/2410.01065</link>
      <description>arXiv:2410.01065v1 Announce Type: cross 
Abstract: Learning complex dynamics driven by partial differential equations directly from data holds great promise for fast and accurate simulations of complex physical systems. In most cases, this problem can be formulated as an operator learning task, where one aims to learn the operator representing the physics of interest, which entails discretization of the continuous system. However, preserving key continuous properties at the discrete level, such as boundary conditions, and addressing physical systems with complex geometries is challenging for most existing approaches. We introduce a family of operator learning architectures, structure-preserving operator networks (SPONs), that allows to preserve key mathematical and physical properties of the continuous system by leveraging finite element (FE) discretizations of the input-output spaces. SPONs are encode-process-decode architectures that are end-to-end differentiable, where the encoder and decoder follows from the discretizations of the input-output spaces. SPONs can operate on complex geometries, enforce certain boundary conditions exactly, and offer theoretical guarantees. Our framework provides a flexible way of devising structure-preserving architectures tailored to specific applications, and offers an explicit trade-off between performance and efficiency, all thanks to the FE discretization of the input-output spaces. Additionally, we introduce a multigrid-inspired SPON architecture that yields improved performance at higher efficiency. Finally, we release a software to automate the design and training of SPON architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01065v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nacime Bouziani, Nicolas Boull\'e</dc:creator>
    </item>
    <item>
      <title>H-DES: a Quantum-Classical Hybrid Differential Equation Solver</title>
      <link>https://arxiv.org/abs/2410.01130</link>
      <description>arXiv:2410.01130v1 Announce Type: cross 
Abstract: In this article, we introduce an original hybrid quantum-classical algorithm based on a variational quantum algorithm for solving systems of differential equations. The algorithm relies on a spectral method, which involves encoding the solution functions in the amplitudes of the quantum states generated by different parametrized circuits and transforms the task of solving the differential equations into an optimization problem. We first describe the principle of the algorithm from a theoretical point of view. We provide a detailed pseudo-code of the algorithm, on which we conduct a complexity analysis to highlight its scaling properties. We apply it to a set of examples, showcasing its applicability across diverse sets of differential equations. We discuss the advantages of our method and potential avenues for further exploration and refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01130v1</guid>
      <category>quant-ph</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hamza Jaffali, Jonas Bastos de Araujo, Nadia Milazzo, Marta Reina, Henri de Boutray, Karla Baumann, Fr\'ed\'eric Holweck</dc:creator>
    </item>
    <item>
      <title>Ensembles provably learn equivariance through data augmentation</title>
      <link>https://arxiv.org/abs/2410.01452</link>
      <description>arXiv:2410.01452v1 Announce Type: cross 
Abstract: Recently, it was proved that group equivariance emerges in ensembles of neural networks as the result of full augmentation in the limit of infinitely wide neural networks (neural tangent kernel limit). In this paper, we extend this result significantly. We provide a proof that this emergence does not depend on the neural tangent kernel limit at all. We also consider stochastic settings, and furthermore general architectures. For the latter, we provide a simple sufficient condition on the relation between the architecture and the action of the group for our results to hold. We validate our findings through simple numeric experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01452v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Oskar Nordenfors, Axel Flinth</dc:creator>
    </item>
    <item>
      <title>Finite element method. Detailed proofs to be formalized in Coq</title>
      <link>https://arxiv.org/abs/2410.01538</link>
      <description>arXiv:2410.01538v1 Announce Type: cross 
Abstract: To obtain the highest confidence on the correction of numerical simulation programs for the resolution of Partial Differential Equations (PDEs), one has to formalize the mathematical notions and results that allow to establish the soundness of the approach. The finite element method is one of the popular tools for the numerical resolution of a wide range of PDEs. The purpose of this document is to provide the formal proof community with very detailed pen-and-paper proofs for the construction of the Lagrange finite elements of any degree on simplices in positive dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01538v1</guid>
      <category>cs.LO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Cl\'ement (SERENA, CERMICS), Vincent Martin (LMAC)</dc:creator>
    </item>
    <item>
      <title>Leray-Schauder Mappings for Operator Learning</title>
      <link>https://arxiv.org/abs/2410.01746</link>
      <description>arXiv:2410.01746v1 Announce Type: cross 
Abstract: We present an algorithm for learning operators between Banach spaces, based on the use of Leray-Schauder mappings to learn a finite-dimensional approximation of compact subspaces. We show that the resulting method is a universal approximator of (possibly nonlinear) operators. We demonstrate the efficiency of the approach on two benchmark datasets showing it achieves results comparable to state of the art models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01746v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuele Zappala</dc:creator>
    </item>
    <item>
      <title>Efficient $1$-bit tensor approximations</title>
      <link>https://arxiv.org/abs/2410.01799</link>
      <description>arXiv:2410.01799v1 Announce Type: cross 
Abstract: We present a spatially efficient decomposition of matrices and arbitrary-order tensors as linear combinations of tensor products of $\{-1, 1\}$-valued vectors. For any matrix $A \in \mathbb{R}^{m \times n}$, $$A - R_w = S_w C_w T_w^\top = \sum_{j=1}^w c_j \cdot \mathbf{s}_j \mathbf{t}_j^\top$$ is a {\it $w$-width signed cut decomposition of $A$}. Here $C_w = "diag"(\mathbf{c}_w)$ for some $\mathbf{c}_w \in \mathbb{R}^w,$ and $S_w, T_w$, and the vectors $\mathbf{s}_j, \mathbf{t}_j$ are $\{-1, 1\}$-valued. To store $(S_w, T_w, C_w)$, we may pack $w \cdot (m + n)$ bits, and require only $w$ floating point numbers. As a function of $w$, $\|R_w\|_F$ exhibits exponential decay when applied to #f32 matrices with i.i.d. $\mathcal N (0, 1)$ entries. Choosing $w$ so that $(S_w, T_w, C_w)$ has the same memory footprint as a \textit{f16} or \textit{bf16} matrix, the relative error is comparable. Our algorithm yields efficient signed cut decompositions in $20$ lines of pseudocode. It reflects a simple modification from a celebrated 1999 paper [1] of Frieze and Kannan. As a first application, we approximate the weight matrices in the open \textit{Mistral-7B-v0.1} Large Language Model to a $50\%$ spatial compression. Remarkably, all $226$ remainder matrices have a relative error $&lt;6\%$ and the expanded model closely matches \textit{Mistral-7B-v0.1} on the {\it huggingface} leaderboard [2]. Benchmark performance degrades slowly as we reduce the spatial compression from $50\%$ to $25\%$. We optimize our open source \textit{rust} implementation [3] with \textit{simd} instructions on \textit{avx2} and \textit{avx512} architectures. We also extend our algorithm from matrices to tensors of arbitrary order and use it to compress a picture of the first author's cat Angus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01799v1</guid>
      <category>math.CO</category>
      <category>cs.LG</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex W. Neal Riasanovsky, Sarah El Kazdadi</dc:creator>
    </item>
    <item>
      <title>Efficient error and variance estimation for randomized matrix computations</title>
      <link>https://arxiv.org/abs/2207.06342</link>
      <description>arXiv:2207.06342v5 Announce Type: replace 
Abstract: Randomized matrix algorithms have become workhorse tools in scientific computing and machine learning. To use these algorithms safely in applications, they should be coupled with posterior error estimates to assess the quality of the output. To meet this need, this paper proposes two diagnostics: a leave-one-out error estimator for randomized low-rank approximations and a jackknife resampling method to estimate the variance of the output of a randomized matrix computation. Both of these diagnostics are rapid to compute for randomized low-rank approximation algorithms such as the randomized SVD and randomized Nystr\"om approximation, and they provide useful information that can be used to assess the quality of the computed output and guide algorithmic parameter choices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.06342v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/23M1558537</arxiv:DOI>
      <arxiv:journal_reference>SIAM Journal on Scientific Computing, 46(1), A508-A528 (2024)</arxiv:journal_reference>
      <dc:creator>Ethan N. Epperly, Joel A. Tropp</dc:creator>
    </item>
    <item>
      <title>The Average Rate of Convergence of the Exact Line Search Gradient Descent Method</title>
      <link>https://arxiv.org/abs/2305.09140</link>
      <description>arXiv:2305.09140v2 Announce Type: replace 
Abstract: It is very well-known that when the exact line search gradient descent method is applied to a convex quadratic objective, the worst case rate of convergence (among all seed vectors) deteriorates as the condition number of the Hessian of the objective grows. By an elegant analysis by H. Akaike, it is generally believed -- but not proved -- that in the ill-conditioned regime the ROC for almost all initial vectors, and hence also the average ROC, is close to the worst case ROC. We complete Akaike's analysis using the theorem of center and stable manifolds. Our analysis also makes apparent the effect of an intermediate eigenvalue in the Hessian by establishing the following somewhat amusing result: In the absence of an intermediate eigenvalue, the average ROC gets arbitrarily fast -- not slow -- as the Hessian gets increasingly ill-conditioned.
  We discuss in passing some contemporary applications of exact line search GD to polynomial optimization problems arising from imaging and data sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.09140v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Yu</dc:creator>
    </item>
    <item>
      <title>The complex-step Newton method and its convergence</title>
      <link>https://arxiv.org/abs/2312.08395</link>
      <description>arXiv:2312.08395v2 Announce Type: replace 
Abstract: Considered herein is a modified Newton method for the numerical solution of nonlinear equations where the Jacobian is approximated using a complex-step derivative approximation. We show that this method converges for sufficiently small complex-step values, which need not be infinitesimal. Notably, when the individual derivatives in the Jacobian matrix are approximated using the complex-step method, the convergence is linear and becomes quadratic as the complex-step approaches zero. However, when the Jacobian matrix is approximated by the nonlinear complex-step derivative approximation, the convergence rate remains quadratic for any appropriately small complex-step value, not just in the limit as it approaches zero. This claim is supported by numerical experiments. Additionally, we demonstrate the method's robust applicability in solving nonlinear systems arising from differential equations, where it is implemented as a Jacobian-free Newton-Krylov method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08395v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimitrios Mitsotakis</dc:creator>
    </item>
    <item>
      <title>Helmholtz preconditioning for the compressible Euler equations using mixed finite elements with Lorenz staggering</title>
      <link>https://arxiv.org/abs/2403.04095</link>
      <description>arXiv:2403.04095v3 Announce Type: replace 
Abstract: Implicit solvers for atmospheric models are often accelerated via the solution of a preconditioned system. For block preconditioners this typically involves the factorisation of the (approximate) Jacobian resulting from linearization of the coupled system into a Helmholtz equation for some function of the pressure. Here we present a preconditioner for the compressible Euler equations with a flux form representation of the potential temperature on the Lorenz grid using mixed finite elements. This formulation allows for spatial discretisations that conserve both energy and potential temperature variance. By introducing the dry thermodynamic entropy as an auxiliary variable for the solution of the algebraic system, the resulting preconditioner is shown to have a similar block structure to an existing preconditioner for the material form transport of potential temperature on the Charney-Phillips grid. This new formulation is also shown to be more efficient and stable than both the material form transport of potential temperature on the Charney-Phillips grid, and a previous Helmholtz preconditioner for the flux form transport of density weighted potential temperature on the Lorenz grid for a 1D thermal bubble configuration. The new preconditioner is further verified against standard two dimensional test cases in a vertical slice geometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04095v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Lee, Alberto F. Mart\'in, Kieran Ricardo</dc:creator>
    </item>
    <item>
      <title>Statistical Rounding Error Analysis for the Computation of Random Vectors and Matrices</title>
      <link>https://arxiv.org/abs/2405.07537</link>
      <description>arXiv:2405.07537v2 Announce Type: replace 
Abstract: The conventional rounding error analysis provides worst-case bounds with an associated failure probability and ignores the statistical property of the rounding errors. In this paper, we develop a new statistical rounding error analysis for random vectors and matrices computation. By assuming the relative errors are independent random variables, we derive the approximate closed-form expressions for the expectation and variance of the rounding errors in various key computations for vectors and random matrices. Numerical experiments validate the accuracy of our derivations and demonstrate that our analytical expressions are generally at least two orders of magnitude tighter than alternative worst-case bounds, exemplified through the inner products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07537v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Fang, Li Chen</dc:creator>
    </item>
    <item>
      <title>Stable semi-implicit SDC methods for conservation laws</title>
      <link>https://arxiv.org/abs/2405.19969</link>
      <description>arXiv:2405.19969v2 Announce Type: replace 
Abstract: Semi-implicit spectral deferred correction (SDC) methods provide a systematic approach to construct time integration methods of arbitrarily high order for nonlinear evolution equations including conservation laws. They converge towards $A$- or even $L$-stable collocation methods, but are often not sufficiently robust themselves. In this paper, a family of SDC methods inspired by an implicit formulation of the Lax-Wendroff method is developed. Compared to fully implicit approaches, the methods have the advantage that they only require the solution of positive definite or semi-definite linear systems. Numerical evidence suggests that the proposed semi-implicit SDC methods with Radau points are $L$-stable up to order 11 and require very little diffusion for orders 13 and 15. The excellent stability and accuracy of these methods is confirmed by numerical experiments with 1D conservation problems, including the convection-diffusion, Burgers, Euler and Navier-Stokes equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19969v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joerg Stiller</dc:creator>
    </item>
    <item>
      <title>Improving sampling by modifying the effective diffusion</title>
      <link>https://arxiv.org/abs/2410.00525</link>
      <description>arXiv:2410.00525v2 Announce Type: replace 
Abstract: This is a preliminary version. Markov chain Monte Carlo samplers based on discretizations of (overdamped) Langevin dynamics are commonly used in the Bayesian inference and computational statistical physics literature to estimate high-dimensional integrals. One can introduce a non-constant diffusion matrix to precondition these dynamics, and recent works have optimized it in order to sooner reach stationarity by overcoming entropic and energy barriers. However, the methodology introduced to compute these optimal diffusions is not suited to high-dimensional settings, as it relies on costly optimization procedures. In this work, we propose a class of diffusion matrices, based on one-dimensional collective variables (CVs), which helps dynamics explore the latent space defined by the CV. The form of the diffusion matrix is such that the effective dynamics, which are approximations of the processes as observed on the latent space, are governed by the optimal effective diffusion coefficient in a homogenized limit, which possesses an analytical expression. We describe how this class of diffusion matrices can be constructed and learned during the simulation. We provide implementations of the Metropolis--Adjusted Langevin Algorithm and Riemann Manifold (Generalized) Hamiltonian Monte Carlo algorithms, and discuss numerical optimizations in the case when the CV depends only on a few number of components of the position of the system. We illustrate the efficiency gains of using this class of diffusion by computing mean transition durations between two configurations of a dimer in a solvent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00525v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tony Leli\`evre, R\'egis Santet, Gabriel Stoltz</dc:creator>
    </item>
    <item>
      <title>Reproducibility via neural fields of visual illusions induced by localized stimuli</title>
      <link>https://arxiv.org/abs/2401.09108</link>
      <description>arXiv:2401.09108v2 Announce Type: replace-cross 
Abstract: This paper focuses on the modeling of experiments conducted by Billock and Tsou [V. A. Billock and B. H. Tsou, Proc. Natl. Acad. Sci. USA, 104 (2007), pp. 8490--8495] using an Amari-type neural field that models the average membrane potential of neuronal activity in the primary visual cortex (V1). The study specifically focuses on a regular funnel pattern localized in the fovea or the peripheral visual field. It aims to comprehend and model the visual phenomena induced by this pattern, emphasizing their nonlinear nature. The research involves designing sensory inputs that mimic the visual stimuli from Billock and Tsou's experiments. The cortical outputs induced by these sensory inputs are then theoretically and numerically studied to assess their ability to model the experimentally observed visual effects at the V1 level. A crucial aspect of this study is the exploration of the effects induced by the nonlinear nature of neural responses. By highlighting the significance of excitatory and inhibitory neurons in the emergence of these visual phenomena, the research suggests that an interplay of both types of neuronal activities plays a crucial role in visual processes, challenging the assumption that the latter is primarily driven by excitatory activities alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09108v2</guid>
      <category>q-bio.NC</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>nlin.PS</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyprien Tamekue, Dario Prandi, Yacine Chitour</dc:creator>
    </item>
    <item>
      <title>HAMLET: Graph Transformer Neural Operator for Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2402.03541</link>
      <description>arXiv:2402.03541v2 Announce Type: replace-cross 
Abstract: We present a novel graph transformer framework, HAMLET, designed to address the challenges in solving partial differential equations (PDEs) using neural networks. The framework uses graph transformers with modular input encoders to directly incorporate differential equation information into the solution process. This modularity enhances parameter correspondence control, making HAMLET adaptable to PDEs of arbitrary geometries and varied input formats. Notably, HAMLET scales effectively with increasing data complexity and noise, showcasing its robustness. HAMLET is not just tailored to a single type of physical simulation, but can be applied across various domains. Moreover, it boosts model resilience and performance, especially in scenarios with limited data. We demonstrate, through extensive experiments, that our framework is capable of outperforming current techniques for PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03541v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of Machine Learning Research, Vol. 235, pp. 4624-4641, 2024</arxiv:journal_reference>
      <dc:creator>Andrey Bryutkin, Jiahao Huang, Zhongying Deng, Guang Yang, Carola-Bibiane Sch\"onlieb, Angelica Aviles-Rivero</dc:creator>
    </item>
    <item>
      <title>A reduced-order modeling of pattern formations</title>
      <link>https://arxiv.org/abs/2403.03632</link>
      <description>arXiv:2403.03632v2 Announce Type: replace-cross 
Abstract: Chemical and biochemical reactions can exhibit a wide range of complex behaviors, including multiple steady states, oscillatory patterns, and chaotic dynamics. These phenomena have captivated researchers for many decades. Notable examples of oscillating chemical systems include the Briggs--Rauscher, Belousov--Zhabotinskii, and Bray--Liebhafsky reactions, where periodic variations in concentration are often visualized through observable color changes. These systems are typically modeled by a set of partial differential equations coupled through nonlinear interactions.
  Upon closer analysis, it appears that the dynamics of these chemical/biochemical reactions may be governed by only a finite number of spatial Fourier modes. We can also draw the same conclusion in fluid dynamics, where it has been shown that, over long periods, the fluid velocity is determined by a finite set of Fourier modes, referred to as determining modes. In this article, we introduce the concept of determining modes for a two-species chemical models, which covers models such as the Brusselator, the Gray-Scott model, and the Glycolysis model \cite{ashkenazi1978spatial,segel1980mathematical}. We demonstrate that it is indeed sufficient to characterize the dynamic of the model using only a finite number of spatial Fourier modes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03632v2</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erika Hausenblas, Tsiry Avisoa Randrianasolo</dc:creator>
    </item>
    <item>
      <title>Alternating Maximization Algorithm for Mismatch Capacity with Oblivious Relaying</title>
      <link>https://arxiv.org/abs/2409.19674</link>
      <description>arXiv:2409.19674v3 Announce Type: replace-cross 
Abstract: Reliable communication over a discrete memoryless channel with the help of a relay has aroused interest due to its widespread applications in practical scenarios. By considering the system with a mismatched decoder, previous works have provided optimization models to evaluate the mismatch capacity in these scenarios. The proposed models, however, are difficult due to the complicated structure of the mismatched decoding problem with the information flows in hops given by the relay. Existing methods, such as the grid search, become impractical as they involve finding all roots of a nonlinear system, with the growing size of the alphabet. To address this problem, we reformulate the max-min optimization model as a consistent maximization form, by considering the dual form of the inner minimization problem and the Lagrangian with a fixed multiplier. Based on the proposed formulation, an alternating maximization framework is designed, which provides the closed-form solution with simple iterations in each step by introducing a suitable variable transformation. The effectiveness of the proposed approach is demonstrated by the simulations over practical scenarios, including Quaternary and Gaussian channels. Moreover, the simulation results of the transitional probability also shed light on the promising application attribute to the quantizer design in the relay node.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19674v3</guid>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinwei Li, Lingyi Chen, Shitong Wu, Huihui Wu, Hao Wu, Wenyi Zhang</dc:creator>
    </item>
  </channel>
</rss>
