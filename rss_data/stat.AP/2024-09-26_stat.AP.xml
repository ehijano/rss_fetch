<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Sep 2024 02:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Spatial extremal modelling: A case study on the interplay between margins and dependence</title>
      <link>https://arxiv.org/abs/2409.16373</link>
      <description>arXiv:2409.16373v1 Announce Type: new 
Abstract: It is no secret that statistical modelling often involves making simplifying assumptions when attempting to study complex stochastic phenomena. Spatial modelling of extreme values is no exception, with one of the most common such assumptions being stationarity in the marginal and/or dependence features. If non-stationarity has been detected in the marginal distributions, it is tempting to try to model this while assuming stationarity in the dependence, without necessarily putting this latter assumption through thorough testing. However, margins and dependence are often intricately connected and the detection of non-stationarity in one feature might affect the detection of non-stationarity in the other. This work is an in-depth case study of this interrelationship, with a particular focus on a spatio-temporal environmental application exhibiting well-documented marginal non-stationarity. Specifically, we compare and contrast four different marginal detrending approaches in terms of our post-detrending ability to detect temporal non-stationarity in the spatial extremal dependence structure of a sea surface temperature dataset from the Red Sea.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16373v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lydia Kakampakou, Emma S. Simpson, Jennifer L. Wadsworth</dc:creator>
    </item>
    <item>
      <title>Aggregating multiple test results to improve medical decision-making</title>
      <link>https://arxiv.org/abs/2409.16442</link>
      <description>arXiv:2409.16442v1 Announce Type: new 
Abstract: Gathering observational data for medical decision-making often involves uncertainties arising from both type I (false positive)and type II (false negative) errors. In this work, we develop a statistical model to study how medical decision-making can be improved by repeating diagnostic and screening tests, and aggregating their results. This approach is relevant not only in clinical settings, such as medical imaging, but also in public health, as highlighted by the need for rapid, cost-effective testing methods during the SARS-CoV-2pandemic. Our model enables the development of testing protocols with an arbitrary number of tests, which can be customized to meet requirements for type I and type II errors. This allows us to adjust sensitivity and specificity according to application-specific needs. Additionally, we derive generalized Rogan--Gladen estimates for estimating disease prevalence, accounting for an arbitrary number of tests with potentially different type I and type II errors. We also provide the corresponding uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16442v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas B\"ottcher, Maria R. D'Orsogna, Tom Chou</dc:creator>
    </item>
    <item>
      <title>Dependencies in Item-Adaptive CAT Data and Differential Item Functioning Detection: A Multilevel Framework</title>
      <link>https://arxiv.org/abs/2409.16534</link>
      <description>arXiv:2409.16534v1 Announce Type: new 
Abstract: This study investigates differential item functioning (DIF) detection in computerized adaptive testing (CAT) using multilevel modeling. We argue that traditional DIF methods have proven ineffective in CAT due to the hierarchical nature of the data. Our proposed two-level model accounts for dependencies between items via provisional ability estimates. Simulations revealed that our model outperformed others in Type-I error control and power, particularly in scenarios with high exposure rates and longer tests. Expanding item pools, incorporating item parameters, and exploring Bayesian estimation are recommended for future research to further enhance DIF detection in CAT. Balancing model complexity with convergence remains a key challenge for robust outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16534v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dandan Chen Kaptur, Justin Kern, Chingwei David Shin, Jinming Zhang</dc:creator>
    </item>
    <item>
      <title>A Novel Framework for Analyzing Structural Transformation in Data-Constrained Economies Using Bayesian Modeling and Machine Learning</title>
      <link>https://arxiv.org/abs/2409.16738</link>
      <description>arXiv:2409.16738v1 Announce Type: new 
Abstract: Structural transformation, the shift from agrarian economies to more diversified industrial and service-based systems, is a key driver of economic development. However, in low- and middle-income countries (LMICs), data scarcity and unreliability hinder accurate assessments of this process. This paper presents a novel statistical framework designed to address these challenges by integrating Bayesian hierarchical modeling, machine learning-based data imputation, and factor analysis. The framework is specifically tailored for conditions of data sparsity and is capable of providing robust insights into sectoral productivity and employment shifts across diverse economies. By utilizing Bayesian models, uncertainties in data are effectively managed, while machine learning techniques impute missing data points, ensuring the integrity of the analysis. Factor analysis reduces the dimensionality of complex datasets, distilling them into core economic structures. The proposed framework has been validated through extensive simulations, demonstrating its ability to predict structural changes even when up to 60\% of data is missing. This approach offers policymakers and researchers a valuable tool for making informed decisions in environments where data quality is limited, contributing to the broader understanding of economic development in LMICs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16738v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Katende</dc:creator>
    </item>
    <item>
      <title>Optimal starting point for time series forecasting</title>
      <link>https://arxiv.org/abs/2409.16843</link>
      <description>arXiv:2409.16843v1 Announce Type: new 
Abstract: Recent advances on time series forecasting mainly focus on improving the forecasting models themselves. However, managing the length of the input data can also significantly enhance prediction performance. In this paper, we introduce a novel approach called Optimal Starting Point Time Series Forecast (OSP-TSP) to capture the intrinsic characteristics of time series data. By adjusting the sequence length via leveraging the XGBoost and LightGBM models, the proposed approach can determine optimal starting point (OSP) of the time series and thus enhance the prediction performances. The performances of the OSP-TSP approach are then evaluated across various frequencies on the M4 dataset and other real-world datasets. Empirical results indicate that predictions based on the OSP-TSP approach consistently outperform those using the complete dataset. Moreover, recognizing the necessity of sufficient data to effectively train models for OSP identification, we further propose targeted solutions to address the issue of data insufficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16843v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiming Zhong, Yinuo Ren, Guangyao Cao, Feng Li, Haobo Qi</dc:creator>
    </item>
    <item>
      <title>The Impact of Geopolitical Risks on Bitcoin Volume Growth: Evidence from a Panel Data Analysis</title>
      <link>https://arxiv.org/abs/2409.17057</link>
      <description>arXiv:2409.17057v1 Announce Type: new 
Abstract: This paper investigates the relationship between geopolitical risks (GPR) and the growth rate of Bitcoin (BTC) volume. Our analysis utilizes dynamic panel data from 33 individual countries and the European economic region. Empirical results demonstrate that GPR has a significant positive impact on BTC volume growth, particularly in developing countries. Our results are confirmed by several robustness checks, like Lagged IV, and volatility check among others. Our study offers a new perspective on BTC, as the novelty of the data used helps us understand the dynamics of BTC volume.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17057v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivan Sergio, Danilo Petti</dc:creator>
    </item>
    <item>
      <title>Damage detection in an uncertain nonlinear beam based on stochastic Volterra series: an experimental application</title>
      <link>https://arxiv.org/abs/2409.16305</link>
      <description>arXiv:2409.16305v1 Announce Type: cross 
Abstract: The damage detection problem becomes a more difficult task when the intrinsically nonlinear behavior of the structures and the natural data variation are considered in the analysis because both phenomena can be confused with damage if linear and deterministic approaches are implemented. Therefore, this work aims the experimental application of a stochastic version of the Volterra series combined with a novelty detection approach to detect damage in an initially nonlinear system taking into account the measured data variation, caused by the presence of uncertainties. The experimental setup is composed by a cantilever beam operating in a nonlinear regime of motion, even in the healthy condition, induced by the presence of a magnet near to the free extremity. The damage associated with mass changes in a bolted connection (nuts loosed) is detected based on the comparison between linear and nonlinear contributions of the stochastic Volterra kernels in the total response, estimated in the reference and damaged conditions. The experimental measurements were performed on different days to add natural variation to the data measured. The results obtained through the stochastic proposed approach are compared with those obtained by the deterministic version of the Volterra series, showing the advantage of the stochastic model use when we consider the experimental data variation with the capability to detect the presence of the damage with statistical confidence. Besides, the nonlinear metric used presented a higher sensitivity to the occurrence of the damage compared with the linear one, justifying the application of a nonlinear metric when the system exhibits intrinsically nonlinear behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16305v1</guid>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ymssp.2019.03.045</arxiv:DOI>
      <arxiv:journal_reference>Mechanical Systems and Signal Processing, vol. 128, pp. 463-478, 2019</arxiv:journal_reference>
      <dc:creator>Luis Gustavo Gioacon Villani, Samuel da Silva, Americo Cunha Jr, Michael D. Todd</dc:creator>
    </item>
    <item>
      <title>DeepScore: A Comprehensive Approach to Measuring Quality in AI-Generated Clinical Documentation</title>
      <link>https://arxiv.org/abs/2409.16307</link>
      <description>arXiv:2409.16307v1 Announce Type: cross 
Abstract: Medical practitioners are rapidly adopting generative AI solutions for clinical documentation, leading to significant time savings and reduced stress. However, evaluating the quality of AI-generated documentation is a complex and ongoing challenge. This paper presents an overview of DeepScribe's methodologies for assessing and managing note quality, focusing on various metrics and the composite "DeepScore", an overall index of quality and accuracy. These methodologies aim to enhance the quality of patient care documentation through accountability and continuous improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16307v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jon Oleson</dc:creator>
    </item>
    <item>
      <title>Probabilistic Spatiotemporal Modeling of Day-Ahead Wind Power Generation with Input-Warped Gaussian Processes</title>
      <link>https://arxiv.org/abs/2409.16308</link>
      <description>arXiv:2409.16308v1 Announce Type: cross 
Abstract: We design a Gaussian Process (GP) spatiotemporal model to capture features of day-ahead wind power forecasts. We work with hourly-scale day-ahead forecasts across hundreds of wind farm locations, with the main aim of constructing a fully probabilistic joint model across space and hours of the day. To this end, we design a separable space-time kernel, implementing both temporal and spatial input warping to capture the non-stationarity in the covariance of wind power. We conduct synthetic experiments to validate our choice of the spatial kernel and to demonstrate the effectiveness of warping in addressing nonstationarity. The second half of the paper is devoted to a detailed case study using a realistic, fully calibrated dataset representing wind farms in the ERCOT region of Texas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16308v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiqi Li, Mike Ludkovski</dc:creator>
    </item>
    <item>
      <title>Identification of extreme weather events and impacts of the disasters in Brazil</title>
      <link>https://arxiv.org/abs/2409.16309</link>
      <description>arXiv:2409.16309v1 Announce Type: cross 
Abstract: An important consequence of human-induced climate change emerges through extreme weather events. The impact of extreme weather events is quantified in some parts of the globe, but it remains underestimated in several countries. In this work we first quantify the extreme temperature and precipitation events in Brazil using data from the Brazilian Institute of Meteorology, which includes 634 meteorological stations that have worked intermittently since 1961. We show that the anomaly in temperature has increased by more than 1{\deg}C in the last 60 years and that extreme events are heterogeneously distributed in the country. In terms of precipitation, our analyses show that it is getting drier in the Northwest region of Brazil while excessive precipitation events are increasing in the South, in agreement with previous works. We then use data from S2iD, an official database that registers disasters in Brazil to estimate their impact in terms of human damage and financial costs in the last ten years. The analysis shows that the drought extreme events are the most expensive, several of them reaching a cost of over a billion USD. Although we are not able to attribute the natural disasters registered in one database to the extreme weather events identified using the meteorological data, we discuss the possible correlations between them. Finally, we present a proposal of using extreme value theory to estimate the probability of having severe extreme events of precipitation in locations where there are already some natural disasters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16309v1</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Davi Lazzari, Am\'alia Garcez, Nicole Poltozi, Gianluca Pozzi, Carolina Brito</dc:creator>
    </item>
    <item>
      <title>New Insights into Global Warming: End-to-End Visual Analysis and Prediction of Temperature Variations</title>
      <link>https://arxiv.org/abs/2409.16311</link>
      <description>arXiv:2409.16311v1 Announce Type: cross 
Abstract: Global warming presents an unprecedented challenge to our planet however comprehensive understanding remains hindered by geographical biases temporal limitations and lack of standardization in existing research. An end to end visual analysis of global warming using three distinct temperature datasets is presented. A baseline adjusted from the Paris Agreements one point five degrees Celsius benchmark based on data analysis is employed. A closed loop design from visualization to prediction and clustering is created using classic models tailored to the characteristics of the data. This approach reduces complexity and eliminates the need for advanced feature engineering. A lightweight convolutional neural network and long short term memory model specifically designed for global temperature change is proposed achieving exceptional accuracy in long term forecasting with a mean squared error of three times ten to the power of negative six and an R squared value of zero point nine nine nine nine. Dynamic time warping and KMeans clustering elucidate national level temperature anomalies and carbon emission patterns. This comprehensive method reveals intricate spatiotemporal characteristics of global temperature variations and provides warming trend attribution. The findings offer new insights into climate change dynamics demonstrating that simplicity and precision can coexist in environmental analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16311v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meihua Zhou, Nan Wan, Tianlong Zheng, Hanwen Xu, Li Yang, Tingting Wang</dc:creator>
    </item>
    <item>
      <title>Refereeing the Referees: Evaluating Two-Sample Tests for Validating Generators in Precision Sciences</title>
      <link>https://arxiv.org/abs/2409.16336</link>
      <description>arXiv:2409.16336v1 Announce Type: cross 
Abstract: We propose a robust methodology to evaluate the performance and computational efficiency of non-parametric two-sample tests, specifically designed for high-dimensional generative models in scientific applications such as in particle physics. The study focuses on tests built from univariate integral probability measures: the sliced Wasserstein distance and the mean of the Kolmogorov-Smirnov statistics, already discussed in the literature, and the novel sliced Kolmogorov-Smirnov statistic. These metrics can be evaluated in parallel, allowing for fast and reliable estimates of their distribution under the null hypothesis. We also compare these metrics with the recently proposed unbiased Fr\'echet Gaussian Distance and the unbiased quadratic Maximum Mean Discrepancy, computed with a quartic polynomial kernel. We evaluate the proposed tests on various distributions, focusing on their sensitivity to deformations parameterized by a single parameter $\epsilon$. Our experiments include correlated Gaussians and mixtures of Gaussians in 5, 20, and 100 dimensions, and a particle physics dataset of gluon jets from the JetNet dataset, considering both jet- and particle-level features. Our results demonstrate that one-dimensional-based tests provide a level of sensitivity comparable to other multivariate metrics, but with significantly lower computational cost, making them ideal for evaluating generative models in high-dimensional settings. This methodology offers an efficient, standardized tool for model comparison and can serve as a benchmark for more advanced tests, including machine-learning-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16336v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>hep-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuele Grossi, Marco Letizia, Riccardo Torre</dc:creator>
    </item>
    <item>
      <title>Statistical tuning of artificial neural network</title>
      <link>https://arxiv.org/abs/2409.16426</link>
      <description>arXiv:2409.16426v1 Announce Type: cross 
Abstract: Neural networks are often regarded as "black boxes" due to their complex functions and numerous parameters, which poses significant challenges for interpretability. This study addresses these challenges by introducing methods to enhance the understanding of neural networks, focusing specifically on models with a single hidden layer. We establish a theoretical framework by demonstrating that the neural network estimator can be interpreted as a nonparametric regression model. Building on this foundation, we propose statistical tests to assess the significance of input neurons and introduce algorithms for dimensionality reduction, including clustering and (PCA), to simplify the network and improve its interpretability and accuracy. The key contributions of this study include the development of a bootstrapping technique for evaluating artificial neural network (ANN) performance, applying statistical tests and logistic regression to analyze hidden neurons, and assessing neuron efficiency. We also investigate the behavior of individual hidden neurons in relation to out-put neurons and apply these methodologies to the IDC and Iris datasets to validate their practical utility. This research advances the field of Explainable Artificial Intelligence by presenting robust statistical frameworks for interpreting neural networks, thereby facilitating a clearer understanding of the relationships between inputs, outputs, and individual network components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16426v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamad Yamen AL Mohamad, Hossein Bevrani, Ali Akbar Haydari</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification for Agent Based Models: A Tutorial</title>
      <link>https://arxiv.org/abs/2409.16776</link>
      <description>arXiv:2409.16776v1 Announce Type: cross 
Abstract: We explore the application of uncertainty quantification methods to agent-based models (ABMs) using a simple sheep and wolf predator-prey model. This work serves as a tutorial on how techniques like emulation can be powerful tools in this context. We also highlight the importance of advanced statistical methods in effectively utilising computationally expensive ABMs. Specifically, we implement stochastic Gaussian processes, Gaussian process classification, sequential design, and history matching to address uncertainties in model input parameters and outputs. Our results show that these methods significantly enhance the robustness, accuracy, and predictive power of ABMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16776v1</guid>
      <category>stat.OT</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louise Kimpton, Peter Challenor, James Salter</dc:creator>
    </item>
    <item>
      <title>Large Language Model Predicts Above Normal All India Summer Monsoon Rainfall in 2024</title>
      <link>https://arxiv.org/abs/2409.16799</link>
      <description>arXiv:2409.16799v1 Announce Type: cross 
Abstract: Reliable prediction of the All India Summer Monsoon Rainfall (AISMR) is pivotal for informed policymaking for the country, impacting the lives of billions of people. However, accurate simulation of AISMR has been a persistent challenge due to the complex interplay of various muti-scale factors and the inherent variability of the monsoon system. This research focuses on adapting and fine-tuning the latest LLM model, PatchTST, to accurately predict AISMR with a lead time of three months. The fine-tuned PatchTST model, trained with historical AISMR data, the Ni\~no3.4 index, and categorical Indian Ocean Dipole values, outperforms several popular neural network models and statistical models. This fine-tuned LLM model exhibits an exceptionally low RMSE percentage of 0.07% and a Spearman correlation of 0.976. This is particularly impressive, since it is nearly 80% more accurate than the best-performing NN models. The model predicts an above-normal monsoon for the year 2024, with an accumulated rainfall of 921.6 mm in the month of June-September for the entire country.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16799v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ujjawal Sharma, Madhav Biyani, Akhil Dev Suresh, Debi Prasad Bhuyan, Saroj Kanta Mishra, Tanmoy Chakraborty</dc:creator>
    </item>
    <item>
      <title>Bayesian Bivariate Conway-Maxwell-Poisson Regression Model for Correlated Count Data in Sports</title>
      <link>https://arxiv.org/abs/2409.17129</link>
      <description>arXiv:2409.17129v1 Announce Type: cross 
Abstract: Count data play a crucial role in sports analytics, providing valuable insights into various aspects of the game. Models that accurately capture the characteristics of count data are essential for making reliable inferences. In this paper, we propose the use of the Conway-Maxwell-Poisson (CMP) model for analyzing count data in sports. The CMP model offers flexibility in modeling data with different levels of dispersion. Here we consider a bivariate CMP model that models the potential correlation between home and away scores by incorporating a random effect specification. We illustrate the advantages of the CMP model through simulations. We then analyze data from baseball and soccer games before, during, and after the COVID-19 pandemic. The performance of our proposed CMP model matches or outperforms standard Poisson and Negative Binomial models, providing a good fit and an accurate estimation of the observed effects in count data with any level of dispersion. The results highlight the robustness and flexibility of the CMP model in analyzing count data in sports, making it a suitable default choice for modeling a diverse range of count data types in sports, where the data dispersion may vary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17129v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mauro Florez, Michele Guindani, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Unraveling stochastic fundamental diagrams considering empirical knowledge: modeling, limitation and further discussion</title>
      <link>https://arxiv.org/abs/2404.09318</link>
      <description>arXiv:2404.09318v2 Announce Type: replace 
Abstract: Traffic flow modeling relies heavily on fundamental diagrams. However, deterministic fundamental diagrams, such as single or multi-regime models, cannot capture the uncertainty pattern that underlies traffic flow. To address this limitation, a sparse non-parametric regression model is proposed in this paper to formulate the stochastic fundamental diagram. Unlike parametric stochastic fundamental diagram models, a non-parametric model is insensitive to parameters, flexible, and applicable. The computation complexity and the huge memory required for training in the Gaussian process regression have been reduced by introducing the sparse Gaussian process regression. The paper also discusses how empirical knowledge influences the modeling process. The paper analyzes the influence of modeling empirical knowledge in the prior of the stochastic fundamental diagram model and whether empirical knowledge can improve the robustness and accuracy of the proposed model. By introducing several well-known single-regime fundamental diagram models as the prior and testing the model's robustness and accuracy with different sampling methods given real-world data, the authors find that empirical knowledge can only benefit the model under small inducing samples given a relatively clean and large dataset. A pure data-driven approach is sufficient to estimate and describe the pattern of the density-speed relationship.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09318v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan-Zheng Lei, Yaobang Gong, Xianfeng Terry Yang</dc:creator>
    </item>
    <item>
      <title>Robust Distributed Learning of Functional Data From Simulators through Data Sketching</title>
      <link>https://arxiv.org/abs/2406.18751</link>
      <description>arXiv:2406.18751v2 Announce Type: replace 
Abstract: In environmental studies, realistic simulations are essential for understanding complex systems. Statistical emulation with Gaussian processes (GPs) in functional data models have become a standard tool for this purpose. Traditional centralized processing of such models requires substantial computational and storage resources, leading to emerging distributed Bayesian learning algorithms that partition data into shards for distributed computations. However, concerns about the sensitivity of distributed inference to shard selection arise. Instead of using data shards, our approach employs multiple random matrices to create random linear projections, or sketches, of the dataset. Posterior inference on functional data models is conducted using random data sketches on various machines in parallel. These individual inferences are combined across machines at a central server. The aggregation of inference across random matrices makes our approach resilient to the selection of data sketches, resulting in robust distributed Bayesian learning. An important advantage is its ability to maintain the privacy of sampling units, as random sketches prevent the recovery of raw data. We highlight the significance of our approach through simulation examples and showcase the performance of our approach as an emulator using surrogates of the Sea, Lake, and Overland Surges from Hurricanes (SLOSH) simulator - an important simulator for government agencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18751v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>R. Jacob Andros, Rajarshi Guhaniyogi, Devin Francom, Donatella Pasqualini</dc:creator>
    </item>
    <item>
      <title>Simple Macroeconomic Forecast Distributions for the G7 Economies</title>
      <link>https://arxiv.org/abs/2408.08304</link>
      <description>arXiv:2408.08304v2 Announce Type: replace 
Abstract: We present a simple method for predicting the distribution of output growth and inflation in the G7 economies. The method is based on point forecasts published by the International Monetary Fund (IMF), as well as robust statistics from the empirical distribution of the IMF's past forecast errors while imposing coherence of prediction intervals across horizons. We show that the technique yields calibrated prediction intervals and performs similar to, or better than, more complex time series models in terms of statistical loss functions. We provide a simple website with graphical illustrations of our forecasts, as well as time-stamped data files that document their real time character.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08304v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Friederike Becker, Fabian Kr\"uger, Melanie Schienle</dc:creator>
    </item>
    <item>
      <title>Graph-constrained Analysis for Multivariate Functional Data</title>
      <link>https://arxiv.org/abs/2209.06294</link>
      <description>arXiv:2209.06294v3 Announce Type: replace-cross 
Abstract: Functional Gaussian graphical models (GGM) used for analyzing multivariate functional data customarily estimate an unknown graphical model representing the conditional relationships between the functional variables. However, in many applications of multivariate functional data, the graph is known and existing functional GGM methods cannot preserve a given graphical constraint. In this manuscript, we demonstrate how to conduct multivariate functional analysis that exactly conforms to a given inter-variable graph. We first show the equivalence between partially separable functional GGM and graphical Gaussian processes (GP), proposed originally for constructing optimal covariance functions for multivariate spatial data that retain the conditional independence relations in a given graphical model. The theoretical connection help design a new algorithm that leverages Dempster's covariance selection to calculate the maximum likelihood estimate of the covariance function for multivariate functional data under graphical constraints. We also show that the finite term truncation of functional GGM basis expansion used in practice is equivalent to a low-rank graphical GP, which is known to oversmooth marginal distributions. To remedy this, we extend our algorithm to better preserve marginal distributions while still respecting the graph and retaining computational scalability. The insights obtained from the new results presented in this manuscript will help practitioners better understand the relationship between these graphical models and in deciding on the appropriate method for their specific multivariate data analysis task. The benefits of the proposed algorithms are illustrated using empirical experiments and an application to functional modeling of neuroimaging data using the connectivity graph among regions of the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.06294v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debangan Dey, Sudipto Banerjee, Martin Lindquist, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Estimation of finite population proportions for small areas -- a statistical data integration approach</title>
      <link>https://arxiv.org/abs/2305.12336</link>
      <description>arXiv:2305.12336v2 Announce Type: replace-cross 
Abstract: Empirical best prediction (EBP) is a well-known method for producing reliable proportion estimates when the primary data source provides only small or no sample from finite populations. There are potential challenges in implementing existing EBP methodology such as limited auxiliary variables in the frame (not adequate for building a reasonable working predictive model) or unable to accurately link the sample to the finite population frame due to absence of identifiers. In this paper, we propose a new data linkage approach where the finite population frame is replaced by a big probability sample, having a large set of auxiliary variables but not the outcome binary variable of interest. We fit an assumed model on the small probability sample and then impute the outcome variable for all units of the big sample to obtain standard weighted proportions. We develop a new adjusted maximum likelihood (ML) method so that the estimate of model variance doesn't fall on the boundary, which is otherwise encountered in commonly used ML method. We also propose an estimator of the mean squared prediction error using a parametric bootstrap method and address computational issues by developing an efficient Expectation Maximization algorithm. The proposed methodology is illustrated in the context of election projection for small areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12336v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditi Sen, Partha Lahiri</dc:creator>
    </item>
    <item>
      <title>Best Linear Unbiased Estimate from Privatized Histograms</title>
      <link>https://arxiv.org/abs/2409.04387</link>
      <description>arXiv:2409.04387v2 Announce Type: replace-cross 
Abstract: In differential privacy (DP) mechanisms, it can be beneficial to release "redundant" outputs, in the sense that a quantity can be estimated by combining different combinations of privatized values. Indeed, this structure is present in the DP 2020 Decennial Census products published by the U.S. Census Bureau. With this structure, the DP output can be improved by enforcing self-consistency (i.e., estimators obtained by combining different values result in the same estimate) and we show that the minimum variance processing is a linear projection. However, standard projection algorithms are too computationally expensive in terms of both memory and execution time for applications such as the Decennial Census. We propose the Scalable Efficient Algorithm for Best Linear Unbiased Estimate (SEA BLUE), based on a two step process of aggregation and differencing that 1) enforces self-consistency through a linear and unbiased procedure, 2) is computationally and memory efficient, 3) achieves the minimum variance solution under certain structural assumptions, and 4) is empirically shown to be robust to violations of these structural assumptions. We propose three methods of calculating confidence intervals from our estimates, under various assumptions. We apply SEA BLUE to two 2010 Census demonstration products, illustrating its scalability and validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04387v2</guid>
      <category>stat.CO</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Adam Edwards, Paul Bartholomew, Andrew Sillers</dc:creator>
    </item>
  </channel>
</rss>
