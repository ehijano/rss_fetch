<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Dec 2024 05:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Wall-Proximity Matters: Understanding the Effect of Device Placement with Respect to the Wall for Indoor Wireless Sensing</title>
      <link>https://arxiv.org/abs/2412.13208</link>
      <description>arXiv:2412.13208v1 Announce Type: new 
Abstract: Wi-Fi sensing has been extensively explored for various applications, including vital sign monitoring, human activity recognition, indoor localization, and tracking. However, practical implementation in real-world scenarios is hindered by unstable sensing performance and limited knowledge of wireless sensing coverage. While previous works have aimed to address these challenges, they have overlooked the impact of walls on sensing capabilities in indoor environments. To fill this gap, we present a theoretical model that accounts for the effect of wall-device distance on sensing coverage. By incorporating both the wall-reflected path and the line-of-sight (LoS) path, we develop a comprehensive sensing coverage model tailored for indoor environments. This model demonstrates that strategically deploying the transmitter and receiver in proximity to the wall within a specific range can significantly expand sensing coverage. We assess the properties of our model through experiments in respiratory monitoring and stationary crowd counting applications, showcasing a notable 11.2% improvement in counting accuracy. These findings pave the way for optimized deployment strategies in Wi-Fi sensing, facilitating more effective and accurate sensing solutions across various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13208v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>He Wang, Yunpeng Ge, Ivan Wang-Hei Ho</dc:creator>
    </item>
    <item>
      <title>Driving Innovation in 6G Wireless Technologies: The OpenAirInterface Approach</title>
      <link>https://arxiv.org/abs/2412.13295</link>
      <description>arXiv:2412.13295v1 Announce Type: new 
Abstract: The development of 6G wireless technologies is rapidly advancing, with the 3rd Generation Partnership Project (3GPP) entering the pre-standardization phase and aiming to deliver the first specifications by 2028. This paper explores the OpenAirInterface (OAI) project, an open-source initiative that plays a crucial role in the evolution of 5G and the future 6G networks. OAI provides a comprehensive implementation of 3GPP and O-RAN compliant networks, including Radio Access Network (RAN), Core Network (CN), and software-defined User Equipment (UE) components. The paper details the history and evolution of OAI, its licensing model, and the various projects under its umbrella, such as RAN, the CN, as well as the Operations, Administration and Maintenance (OAM) projects. It also highlights the development methodology, Continuous Integration/Continuous Delivery (CI/CD) processes, and end-to-end systems powered by OAI. Furthermore, the paper discusses the potential of OAI for 6G research, focusing on spectrum, reflective intelligent surfaces, and Artificial Intelligence (AI)/Machine Learning (ML) integration. The open-source approach of OAI is emphasized as essential for tackling the challenges of 6G, fostering community collaboration, and driving innovation in next-generation wireless technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13295v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Kaltenberger, Tommaso Melodia, Irfan Ghauri, Michele Polese, Raymond Knopp, Tien Thinh Nguyen, Sakthivel Velumani, Davide Villa, Leonardo Bonati, Robert Schmidt, Sagar Arora, Mikel Irazabal, Navid Nikaein</dc:creator>
    </item>
    <item>
      <title>Magnifier: Detecting Network Access via Lightweight Traffic-based Fingerprints</title>
      <link>https://arxiv.org/abs/2412.13428</link>
      <description>arXiv:2412.13428v1 Announce Type: new 
Abstract: Network access detection plays a crucial role in global network management, enabling efficient network monitoring and topology measurement by identifying unauthorized network access and gathering detailed information about mobile devices. Existing methods for endpoint-based detection primarily rely on deploying monitoring software to recognize network connections. However, the challenges associated with developing and maintaining such systems have limited their universality and coverage in practical deployments, especially given the cost implications of covering a wide array of devices with heterogeneous operating systems. To tackle the issues, we propose Magnifier for mobile device network access detection that, for the first time, passively infers access patterns from backbone traffic at the gateway level. Magnifier's foundation is the creation of device-specific access patterns using the innovative Domain Name Forest (dnForest) fingerprints. We then employ a two-stage distillation algorithm to fine-tune the weights of individual Domain Name Trees (dnTree) within each dnForest, emphasizing the unique device fingerprints. With these meticulously crafted fingerprints, Magnifier efficiently infers network access from backbone traffic using a lightweight fingerprint matching algorithm. Our experimental results, conducted in real-world scenarios, demonstrate that Magnifier exhibits exceptional universality and coverage in both initial and repetitive network access detection in real-time. To facilitate further research, we have thoughtfully curated the NetCess2023 dataset, comprising network access data from 26 different models across 7 brands, covering the majority of mainstream mobile devices. We have also made both the Magnifier prototype and the NetCess2023 dataset publicly available\footnote{https://github.com/SecTeamPolaris/Magnifier}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13428v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhao Li, Qiang Wang, Huaifeng Bao, Xiao-Yu Zhang, Lingyun Ying, Zhaoxuan Li</dc:creator>
    </item>
    <item>
      <title>Transmit What You Need: Task-Adaptive Semantic Communications for Visual Information</title>
      <link>https://arxiv.org/abs/2412.13646</link>
      <description>arXiv:2412.13646v1 Announce Type: new 
Abstract: Recently, semantic communications have drawn great attention as the groundbreaking concept surpasses the limited capacity of Shannon's theory. Specifically, semantic communications probably become crucial in realizing visual tasks that demand massive network traffic. Although highly distinctive forms of visual semantics exist for computer vision tasks, a thorough investigation of what visual semantics can be transmitted in time and which one is required for completing different visual tasks has not yet been reported. To this end, we first scrutinize the achievable throughput in transmitting existing visual semantics through the limited wireless communication bandwidth. In addition, we further demonstrate the resulting performance of various visual tasks for each visual semantic. Based on the empirical testing, we suggest a task-adaptive selection of visual semantics is crucial for real-time semantic communications for visual tasks, where we transmit basic semantics (e.g., objects in the given image) for simple visual tasks, such as classification, and richer semantics (e.g., scene graphs) for complex tasks, such as image regeneration. To further improve transmission efficiency, we suggest a filtering method for scene graphs, which drops redundant information in the scene graph, thus allowing the sending of essential semantics for completing the given task. We confirm the efficacy of our task-adaptive semantic communication approach through extensive simulations in wireless channels, showing more than 45 times larger throughput over a naive transmission of original data. Our work can be reproduced at the following source codes: https://github.com/jhpark2024/jhpark.github.io</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13646v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeonghun Park, Sung Whan Yoon</dc:creator>
    </item>
    <item>
      <title>Resilience of Networks to Spreading Computer Viruses: Optimal Anti-Virus Deployment (Extended Version)</title>
      <link>https://arxiv.org/abs/2412.13911</link>
      <description>arXiv:2412.13911v1 Announce Type: new 
Abstract: Deployment of anti-virus software is a common strategy for preventing and controlling the propagation of computer viruses and worms over a computer network. As the deployment of such programs is often limited due to monetary or operational costs, devising optimal strategies for their allocation and deployment can be of high value to the operation, performance, and resilience of the target networks.
  We study the effects of anti-virus deployment (i.e., "vaccination") strategies on the ability of a network to block the spread of a virus. Such ability is obtained when the network reaches "herd immunity", achieved when a large fraction of the network entities is immune to the infection, which provides protection even for entities which are not immune. We use a model that explicitly accounts for the inherent heterogeneity of network nodes activity and derive optimal strategies for anti-virus deployment.
  Numerical evaluations demonstrate that the system performance is very sensitive to the chosen strategy, and thus strategies which disregard the heterogeneous spread nature may perform significantly worse relatively to those derived in this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13911v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jhonatan Tavori, Hanoch Levy</dc:creator>
    </item>
    <item>
      <title>CoRa: A Collision-Resistant LoRa Symbol Detector of Low Complexity</title>
      <link>https://arxiv.org/abs/2412.13930</link>
      <description>arXiv:2412.13930v1 Announce Type: new 
Abstract: Long range communication with LoRa has become popular as it avoids the complexity of multi-hop communication at low cost and low energy consumption. LoRa is openly accessible, but its packets are particularly vulnerable to collisions due to long time on air in a shared band. This degrades communication performance. Existing techniques for demodulating LoRa symbols under collisions face challenges such as high computational complexity, reliance on accurate symbol boundary information, or error-prone peak detection methods. In this paper, we introduce CoRa , a symbol detector for demodulating LoRa symbols under severe collisions. CoRa employs a Bayesian classifier to accurately identify the true symbol amidst interference from other LoRa transmissions, leveraging empirically derived features from raw symbol data. Evaluations using real-world and simulated packet traces demonstrate that CoRa clearly outperforms the related state-of-the-art, i.e., up to 29% better decoding performance than TnB and 178% better than CIC. Compared to the LoRa baseline demodulator, CoRa magnifies the packet reception rate by up to 11.53x. CoRa offers a significant reduction in computational complexity compared to existing solutions by only adding a constant overhead to the baseline demodulator, while also eliminating the need for peak detection and accurately identifying colliding frames.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13930v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e \'Alamos, Thomas C. Schmidt, Matthias W\"ahlisch</dc:creator>
    </item>
    <item>
      <title>Learning and Reconstructing Conflicts in O-RAN: A Graph Neural Network Approach</title>
      <link>https://arxiv.org/abs/2412.14119</link>
      <description>arXiv:2412.14119v1 Announce Type: new 
Abstract: The Open Radio Access Network (O-RAN) architecture enables the deployment of third-party applications on the RAN Intelligent Controllers (RICs) to provide Mobile Network Operators (MNOs) with different functionality. However, the operation of third-party applications in the Near Real-Time RIC (Near-RT RIC), known as xApps, can result in conflicting interactions. Each xApp can independently modify the same control parameters to achieve distinct outcomes, which has the potential to cause performance degradation and network instability. The current conflict detection and mitigation solutions in the literature assume that all conflicts are known a priori, which does not always hold due to complex and often hidden relationships between control parameters and Key Performance Indicators (KPIs). In this paper, we introduce a novel data-driven Graph Neural Network (GNN)-based method for reconstructing conflict graphs. Specifically, we leverage GraphSAGE, an inductive learning framework, to dynamically learn the hidden relationships between xApps, control parameters, and KPIs. Our experimental results validate our proposed method for reconstructing conflict graphs and identifying all types of conflicts in O-RAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14119v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arshia Zolghadr, Joao F. Santos, Luiz A. DaSilva, Jacek Kibi{\l}da</dc:creator>
    </item>
    <item>
      <title>Optimizing Age of Information in Internet of Vehicles Over Error-Prone Channels</title>
      <link>https://arxiv.org/abs/2412.13204</link>
      <description>arXiv:2412.13204v1 Announce Type: cross 
Abstract: In the Internet of Vehicles (IoV), Age of Information (AoI) has become a vital performance metric for evaluating the freshness of information in communication systems. Although many studies aim to minimize the average AoI of the system through optimized resource scheduling schemes, they often fail to adequately consider the queue characteristics. Moreover, the vehicle mobility leads to rapid changes in network topology and channel conditions, making it difficult to accurately reflect the unique characteristics of vehicles with the calculated AoI under ideal channel conditions. This paper examines the impact of Doppler shifts caused by vehicle speeds on data transmission in error-prone channels. Based on the M/M/1 and D/M/1 queuing theory models, we derive expressions for the Age of Information and optimize the system's average AoI by adjusting the data extraction rates of vehicles (which affect system utilization). We propose an online optimization algorithm that dynamically adjusts the vehicles' data extraction rates based on environmental changes to ensure optimal AoI. Simulation results have demonstrated that adjusting the data extraction rates of vehicles can significantly reduce the system's AoI. Additionally, in the network scenario of this work, the AoI of the D/M/1 system is lower than that of the M/M/1 system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13204v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cui Zhang, Maoxin Ji, Qiong Wu, Pingyi Fan, Qiang Fan</dc:creator>
    </item>
    <item>
      <title>Accelerating the Operation of Complex Workflows through Standard Data Interfaces</title>
      <link>https://arxiv.org/abs/2412.13339</link>
      <description>arXiv:2412.13339v1 Announce Type: cross 
Abstract: In this position paper we argue for standardizing how we share and process data in scientific workflows at the network-level to maximize step re-use and workflow portability across platforms and networks in pursuit of a foundational workflow stack. We look to evolve workflows from steps connected point-to-point in a directed acyclic graph (DAG) to steps connected via shared channels in a message system implemented as a network service. To start this evolution, we contribute: a preliminary reference model, architecture, and open tools to implement the architecture today. Our goal stands to improve the deployment and operation of complex workflows by decoupling data sharing and data processing in workflow steps. We seek the workflow community's input on this approach's merit, related research to explore and initial requirements from the workflows community to inform future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13339v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taylor Paul, William Regli</dc:creator>
    </item>
    <item>
      <title>CATO: End-to-End Optimization of ML-Based Traffic Analysis Pipelines</title>
      <link>https://arxiv.org/abs/2402.06099</link>
      <description>arXiv:2402.06099v2 Announce Type: replace 
Abstract: Machine learning has shown tremendous potential for improving the capabilities of network traffic analysis applications, often outperforming simpler rule-based heuristics. However, ML-based solutions remain difficult to deploy in practice. Many existing approaches only optimize the predictive performance of their models, overlooking the practical challenges of running them against network traffic in real time. This is especially problematic in the domain of traffic analysis, where the efficiency of the serving pipeline is a critical factor in determining the usability of a model. In this work, we introduce CATO, a framework that addresses this problem by jointly optimizing the predictive performance and the associated systems costs of the serving pipeline. CATO leverages recent advances in multi-objective Bayesian optimization to efficiently identify Pareto-optimal configurations, and automatically compiles end-to-end optimized serving pipelines that can be deployed in real networks. Our evaluations show that compared to popular feature optimization techniques, CATO can provide up to 3600x lower inference latency and 3.7x higher zero-loss throughput while simultaneously achieving better model performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06099v2</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gerry Wan, Shinan Liu, Francesco Bronzino, Nick Feamster, Zakir Durumeric</dc:creator>
    </item>
    <item>
      <title>Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)</title>
      <link>https://arxiv.org/abs/2403.07573</link>
      <description>arXiv:2403.07573v3 Announce Type: replace 
Abstract: In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent requirements. ACNC encompasses two primary functionalities: state recognition and context detection. Given the intricate nature of the user-service-computing-network space, the paper employs dimension reduction to generate live, holistic, abstract system states in a hierarchical structure. To address the challenges posed by dynamic changes, Continual Learning (CL) is employed, classifying the system state into contexts controlled by dedicated ML agents, enabling them to operate efficiently. These two functionalities are intricately linked within a closed loop overseen by the End-to-End (E2E) orchestrator to allocate resources. The paper introduces the components of ACNC, proposes a Metaverse scenario to exemplify ACNC's role in resource provisioning with Segment Routing v6 (SRv6), outlines ACNC's workflow, details a numerical analysis for efficiency assessment, and concludes with discussions on relevant challenges and potential avenues for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07573v3</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masoud Shokrnezhad, Hao Yu, Tarik Taleb, Richard Li, Kyunghan Lee, Jaeseung Song, Cedric Westphal</dc:creator>
    </item>
    <item>
      <title>RIS-aided Wireless-Powered Backscatter Communications for Sustainable Internet of Underground Things</title>
      <link>https://arxiv.org/abs/2412.07542</link>
      <description>arXiv:2412.07542v2 Announce Type: replace 
Abstract: Wireless-powered underground sensor networks (WPUSNs), which enable wireless energy transfer to sensors located underground, is a promising approach for establishing sustainable internet of underground things (IoUT). To support urgent information transmission and improve resource utilization within WPUSNs, backscatter communication (BC) is introduced, resulting in what is known as wireless-powered backscatter underground sensor networks (WPBUSNs). Nevertheless, the performance of WPBUSNs is significantly limited by severe channel impairments caused by the underground soil and the blockage of direct links. To overcome this challenge, in this work, we propose integrating reconfigurable intelligent surface (RIS) with WPBUSNs, leading to the development of RIS-aided WPBUSNs. We begin by reviewing the recent advancements in BC-WPUSNs and RIS. Then, we propose a general architecture of RIS-aided WPBUSNs across various IoUT scenarios, and discuss its advantages and implementation challenges. To illustrate the effectiveness of RIS-aided WPBUSNs, we focus on a realistic farming case study, demonstrating that our proposed framework outperforms the three benchmarks in terms of the sum throughput. Finally, we discuss the open challenges and future research directions for translating this study into practical IoUT applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07542v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiqiang Lin, Yijie Mao</dc:creator>
    </item>
    <item>
      <title>From Capture to Display: A Survey on Volumetric Video</title>
      <link>https://arxiv.org/abs/2309.05658</link>
      <description>arXiv:2309.05658v2 Announce Type: replace-cross 
Abstract: Volumetric video, which offers immersive viewing experiences, is gaining increasing prominence. With its six degrees of freedom, it provides viewers with greater immersion and interactivity compared to traditional videos. Despite their potential, volumetric video services pose significant challenges. This survey conducts a comprehensive review of the existing literature on volumetric video. We firstly provide a general framework of volumetric video services, followed by a discussion on prerequisites for volumetric video, encompassing representations, open datasets, and quality assessment metrics. Then we delve into the current methodologies for each stage of the volumetric video service pipeline, detailing capturing, compression, transmission, rendering, and display techniques. Lastly, we explore various applications enabled by this pioneering technology and we present an array of research challenges and opportunities in the domain of volumetric video services. This survey aspires to provide a holistic understanding of this burgeoning field and shed light on potential future research trajectories, aiming to bring the vision of volumetric video to fruition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05658v2</guid>
      <category>cs.MM</category>
      <category>cs.NI</category>
      <category>eess.IV</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yili Jin, Kaiyuan Hu, Junhua Liu, Fangxin Wang, Xue Liu</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Aware Hybrid Inference with On-Device Small and Remote Large Language Models</title>
      <link>https://arxiv.org/abs/2412.12687</link>
      <description>arXiv:2412.12687v2 Announce Type: replace-cross 
Abstract: This paper studies a hybrid language model (HLM) architecture that integrates a small language model (SLM) operating on a mobile device with a large language model (LLM) hosted at the base station (BS) of a wireless network. The HLM token generation process follows the speculative inference principle: the SLM's vocabulary distribution is uploaded to the LLM, which either accepts or rejects it, with rejected tokens being resampled by the LLM. While this approach ensures alignment between the vocabulary distributions of the SLM and LLM, it suffers from low token throughput due to uplink transmission and the computation costs of running both language models. To address this, we propose a novel HLM structure coined Uncertainty-aware opportunistic HLM (U-HLM), wherein the SLM locally measures its output uncertainty and skips both uplink transmissions and LLM operations for tokens that are likely to be accepted. This opportunistic skipping is enabled by our empirical finding of a linear correlation between the SLM's uncertainty and the LLM's rejection probability. We analytically derive the uncertainty threshold and evaluate its expected risk of rejection. Simulations show that U-HLM reduces uplink transmissions and LLM computations by 45.93%, while achieving up to 97.54% of the LLM's inference accuracy and 2.54$\times$ faster token throughput than HLM without skipping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12687v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 19 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seungeun Oh, Jinhyuk Kim, Jihong Park, Seung-Woo Ko, Tony Q. S. Quek, Seong-Lyun Kim</dc:creator>
    </item>
  </channel>
</rss>
