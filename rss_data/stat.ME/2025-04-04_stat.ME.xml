<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Apr 2025 04:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Testing independence and conditional independence in high dimensions via coordinatewise Gaussianization</title>
      <link>https://arxiv.org/abs/2504.02233</link>
      <description>arXiv:2504.02233v1 Announce Type: new 
Abstract: We propose new statistical tests, in high-dimensional settings, for testing the independence of two random vectors and their conditional independence given a third random vector. The key idea is simple, i.e., we first transform each component variable to standard normal via its marginal empirical distribution, and we then test for independence and conditional independence of the transformed random vectors using appropriate $L_\infty$-type test statistics. While we are testing some necessary conditions of the independence or the conditional independence, the new tests outperform the 13 frequently used testing methods in a large scale simulation comparison. The advantage of the new tests can be summarized as follows: (i) they do not require any moment conditions, (ii) they allow arbitrary dependence structures of the components among the random vectors, and (iii) they allow the dimensions of random vectors diverge at the exponential rates of the sample size. The critical values of the proposed tests are determined by a computationally efficient multiplier bootstrap procedure. Theoretical analysis shows that the sizes of the proposed tests can be well controlled by the nominal significance level, and the proposed tests are also consistent under certain local alternatives. The finite sample performance of the new tests is illustrated via extensive simulation studies and a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02233v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Yue Du, Jing He, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>Exceedance and force of centrality for functional data</title>
      <link>https://arxiv.org/abs/2504.02296</link>
      <description>arXiv:2504.02296v1 Announce Type: new 
Abstract: Exceedance refers to instances where a dynamic process surpasses given thresholds, e.g., the occurrence of a heat wave. We propose a novel exceedance framework for functional data, where each observed random trajectory is transformed into an exceedance function, which quantifies exceedance durations as a function of threshold levels. An inherent relationship between exceedance functions and probability distributions makes it possible to draw on distributional data analysis techniques such as Fr\'echet regression to study the dependence of exceedances on Euclidean predictors, e.g., calendar year when the exceedances are observed. We use local linear estimators to obtain exceedance functions from discretely observed functional data with noise and study the convergence of the proposed estimators. New concepts of interest include the force of centrality that quantifies the propensity of a system to revert to lower levels when a given threshold has been exceeded, conditional exceedance functions when conditioning on Euclidean covariates, and threshold exceedance functions, which characterize the size of exceedance sets in dependence on covariates for any fixed threshold. We establish consistent estimation with rates of convergence for these targets. The practical merits of the proposed methodology are illustrated through simulations and applications for annual temperature curves and medfly activity profiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02296v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Poorbita Kundu, Hang Zhou, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Impact of Global Warming on Extreme Rainfall in Taiwan</title>
      <link>https://arxiv.org/abs/2504.02470</link>
      <description>arXiv:2504.02470v1 Announce Type: new 
Abstract: The relationship between global warming and extreme rainfalls in Taiwan was examined in this study. Taiwan rainfall data from TCCIP, a project led by MOST, were analyzed. North Hemisphere reference temperature data from NCEI led by NOAA. The yearly maximum of daily rainfall was focused on and the PGEV model, as proposed by Olafsdottir et al. \citep{olafsdottir2021extreme}, was used to fit the extreme values and make inferences. The PGEV model integrates the General Extreme Value (GEV) and Peak over Threshold (PoT) approaches, which are commonly used to analyze extreme data. Relative intensity and return value were used to show the connection between temperature and extreme rainfall.
  Results indicated that the intensity of extreme rainfall in Taiwan increases as the temperature rises. However, the effects of global warming on the frequency and intensity of extreme rainfalls varied by region. In the north and south regions, the frequency of extreme rainfalls changed, while in the center and east regions, the intensity of extreme rainfalls changed. Furthermore, according to the return value analysis, extreme rainfalls are likely to occur more frequently in the future.
  To account for differences between locations, Gaussian Process was used to smooth the results obtained using the PGEV model. In addition, simulations using the Gaussian copula and Gaussian Process were conducted to determine the quantile confidence intervals for each PGEV model. The simulations showed that all tests comparing with models with and without covariates are significant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02470v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng-Ching Lin</dc:creator>
    </item>
    <item>
      <title>A smooth multi-group Gaussian Mixture Model for cellwise robust covariance estimation</title>
      <link>https://arxiv.org/abs/2504.02547</link>
      <description>arXiv:2504.02547v1 Announce Type: new 
Abstract: Are data groups which are pre-defined by expert opinions or medical diagnoses corresponding to groups based on statistical modeling? For which reason might observations be inconsistent? This contribution intends to answer both questions by proposing a novel multi-group Gaussian mixture model that accounts for the given group context while allowing high flexibility. This is achieved by assuming that the observations of a particular group originate not from a single distribution but from a Gaussian mixture of all group distributions. Moreover, the model provides robustness against cellwise outliers, thus against atypical data cells of the observations. The objective function can be formulated as a likelihood problem and optimized efficiently. We also derive the theoretical breakdown point of the estimators, an innovative result in this context to quantify the degree of robustness to cellwise outliers. Simulations demonstrate the excellent performance and the advantages to alternative models and estimators. Applications from different areas illustrate the strength of the method, particularly in investigating observations which are on the overlap of different groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02547v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patricia Puchhammer, Ines Wilms, Peter Filzmoser</dc:creator>
    </item>
    <item>
      <title>A Dynamic, Ordinal Gaussian Process Item Response Theoretic Model</title>
      <link>https://arxiv.org/abs/2504.02643</link>
      <description>arXiv:2504.02643v1 Announce Type: new 
Abstract: Social scientists are often interested in using ordinal indicators to estimate latent traits that change over time. Frequently, this is done with item response theoretic (IRT) models that describe the relationship between those latent traits and observed indicators. We combine recent advances in Bayesian nonparametric IRT, which makes minimal assumptions on shapes of item response functions, and Gaussian process time series methods to capture dynamic structures in latent traits from longitudinal observations. We propose a generalized dynamic Gaussian process item response theory (GD-GPIRT) as well as a Markov chain Monte Carlo sampling algorithm for estimation of both latent traits and response functions. We evaluate GD-GPIRT in simulation studies against baselines in dynamic IRT, and apply it to various substantive studies, including assessing public opinions on economy environment and congressional ideology related to abortion debate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02643v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yehu Chen, Jacob Montgomery, Roman Garnett</dc:creator>
    </item>
    <item>
      <title>Semiparametric Counterfactual Regression</title>
      <link>https://arxiv.org/abs/2504.02694</link>
      <description>arXiv:2504.02694v1 Announce Type: new 
Abstract: We study counterfactual regression, which aims to map input features to outcomes under hypothetical scenarios that differ from those observed in the data. This is particularly useful for decision-making when adapting to sudden shifts in treatment patterns is essential. We propose a doubly robust-style estimator for counterfactual regression within a generalizable framework that accommodates a broad class of risk functions and flexible constraints, drawing on tools from semiparametric theory and stochastic optimization. Our approach uses incremental interventions to enhance adaptability while maintaining consistency with standard methods. We formulate the target estimand as the optimal solution to a stochastic optimization problem and develop an efficient estimation strategy, where we can leverage rapid development of modern optimization algorithms. We go on to analyze the rates of convergence and characterize the asymptotic distributions. Our analysis shows that the proposed estimators can achieve $\sqrt{n}$-consistency and asymptotic normality for a broad class of problems. Numerical illustrations highlight their effectiveness in adapting to unseen counterfactual scenarios while maintaining parametric convergence rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02694v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwangho Kim</dc:creator>
    </item>
    <item>
      <title>Estimation of the complier causal hazard ratio under dependent censoring</title>
      <link>https://arxiv.org/abs/2504.02096</link>
      <description>arXiv:2504.02096v1 Announce Type: cross 
Abstract: In this work, we are interested in studying the causal effect of an endogenous binary treatment on a dependently censored duration outcome. By dependent censoring, it is meant that the duration time ($T$) and right censoring time ($C$) are not statistically independent of each other, even after conditioning on the measured covariates. The endogeneity issue is handled by making use of a binary instrumental variable for the treatment. To deal with the dependent censoring problem, it is assumed that on the stratum of compliers: (i) $T$ follows a semiparametric proportional hazards model; (ii) $C$ follows a fully parametric model; and (iii) the relation between $T$ and $C$ is modeled by a parametric copula, such that the association parameter can be left unspecified. In this framework, the treatment effect of interest is the complier causal hazard ratio (CCHR). We devise an estimation procedure that is based on a weighted maximum likelihood approach, where the weights are the probabilities of an observation coming from a complier. The weights are estimated non-parametrically in a first stage, followed by the estimation of the CCHR. Novel conditions under which the model is identifiable are given, a two-step estimation procedure is proposed and some important asymptotic properties are established. Simulations are used to assess the validity and finite-sample performance of the estimation procedure. Finally, we apply the approach to estimate the CCHR of both job training programs on unemployment duration and periodic screening examinations on time until death from breast cancer. The data come from the National Job Training Partnership Act study and the Health Insurance Plan of Greater New York experiment respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02096v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gilles Crommen, Jad Beyhum, Ingrid Van Keilegom</dc:creator>
    </item>
    <item>
      <title>The Amenability Framework: Rethinking Causal Ordering Without Estimating Causal Effects</title>
      <link>https://arxiv.org/abs/2504.02456</link>
      <description>arXiv:2504.02456v1 Announce Type: cross 
Abstract: Who should we prioritize for intervention when we cannot estimate intervention effects? In many applied domains (e.g., advertising, customer retention, and behavioral nudging) prioritization is guided by predictive models that estimate outcome probabilities rather than causal effects. This paper investigates when these predictions (scores) can effectively rank individuals by their intervention effects, particularly when direct effect estimation is infeasible or unreliable. We propose a conceptual framework based on amenability: an individual's latent proclivity to be influenced by an intervention. We then formalize conditions under which predictive scores serve as effective proxies for amenability. These conditions justify using non-causal scores for intervention prioritization, even when the scores do not directly estimate effects. We further show that, under plausible assumptions, predictive models can outperform causal effect estimators in ranking individuals by intervention effects. Empirical evidence from an advertising context supports our theoretical findings, demonstrating that predictive modeling can offer a more robust approach to targeting than effect estimation. Our framework suggests a shift in focus, from estimating effects to inferring who is amenable, as a practical and theoretically grounded strategy for prioritizing interventions in resource-constrained environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02456v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Fern\'andez-Lor\'ia, Jorge Lor\'ia</dc:creator>
    </item>
    <item>
      <title>Universal Log-Optimality for General Classes of e-processes and Sequential Hypothesis Tests</title>
      <link>https://arxiv.org/abs/2504.02818</link>
      <description>arXiv:2504.02818v1 Announce Type: cross 
Abstract: We consider the problem of sequential hypothesis testing by betting. For a general class of composite testing problems -- which include bounded mean testing, equal mean testing for bounded random tuples, and some key ingredients of two-sample and independence testing as special cases -- we show that any $e$-process satisfying a certain sublinear regret bound is adaptively, asymptotically, and almost surely log-optimal for a composite alternative. This is a strong notion of optimality that has not previously been established for the aforementioned problems and we provide explicit test supermartingales and $e$-processes satisfying this notion in the more general case. Furthermore, we derive matching lower and upper bounds on the expected rejection time for the resulting sequential tests in all of these cases. The proofs of these results make weak, algorithm-agnostic moment assumptions and rely on a general-purpose proof technique involving the aforementioned regret and a family of numeraire portfolios. Finally, we discuss how all of these theorems hold in a distribution-uniform sense, a notion of log-optimality that is stronger still and seems to be new to the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02818v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Waudby-Smith, Ricardo Sandoval, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Adaptive Student's t-distribution with method of moments moving estimator for nonstationary time series</title>
      <link>https://arxiv.org/abs/2304.03069</link>
      <description>arXiv:2304.03069v3 Announce Type: replace 
Abstract: The real life time series are usually nonstationary, bringing a difficult question of model adaptation. Classical approaches like ARMA-ARCH assume arbitrary type of dependence. To avoid their bias, we will focus on recently proposed agnostic philosophy of moving estimator: in time $t$ finding parameters optimizing e.g. $F_t=\sum_{\tau&lt;t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$ moving log-likelihood, evolving in time. It allows for example to estimate parameters using inexpensive exponential moving averages (EMA), like absolute central moments $m_p=E[|x-\mu|^p]$ evolving for one or multiple powers $p\in\mathbb{R}^+$ using $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$. Application of such general adaptive methods of moments will be presented on Student's t-distribution, popular especially in economical applications, here applied to log-returns of DJIA companies. While standard ARMA-ARCH approaches provide evolution of $\mu$ and $\sigma$, here we also get evolution of $\nu$ describing $\rho(x)\sim |x|^{-\nu-1}$ tail shape, probability of extreme events - which might turn out catastrophic, destabilizing the market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03069v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jarek Duda</dc:creator>
    </item>
    <item>
      <title>Proximal Causal Inference for Conditional Separable Effects</title>
      <link>https://arxiv.org/abs/2402.11020</link>
      <description>arXiv:2402.11020v3 Announce Type: replace 
Abstract: Scientists regularly pose questions about treatment effects on outcomes conditional on a post-treatment event. However, defining, identifying, and estimating causal effects conditional on post-treatment events requires care, even in perfectly executed randomized experiments. Recently, the conditional separable effect (CSE) was proposed as an interventionist estimand that corresponds to scientifically meaningful questions in these settings. However, while being a single-world estimand, which can be queried experimentally, existing identification results for the CSE require no unmeasured confounding between the outcome and post-treatment event. This assumption can be violated in many applications. In this work, we address this concern by developing new identification and estimation results for the CSE in the presence of unmeasured confounding. We establish nonparametric identification of the CSE in observational and experimental settings when time-varying confounders are present, and certain proxy variables are available for hidden common causes of the post-treatment event and outcome. For inference, we characterize an influence function for the CSE under a semiparametric model in which nuisance functions are a priori unrestricted. Moreover, we develop a consistent, asymptotically linear, and locally semiparametric efficient estimator of the CSE using modern machine learning theory. We illustrate our framework with simulation studies and a real-world cancer therapy trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11020v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chan Park, Mats Stensrud, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Nonparametric efficient causal estimation of the intervention-specific expected number of recurrent events with continuous-time targeted maximum likelihood and highly adaptive lasso estimation</title>
      <link>https://arxiv.org/abs/2404.01736</link>
      <description>arXiv:2404.01736v3 Announce Type: replace 
Abstract: Longitudinal settings involving outcome, competing risks and censoring events occurring and recurring in continuous time are common in medical research, but are often analyzed with methods that do not allow for taking post-baseline information into account. In this work, we define statistical and causal target parameters via the g-computation formula by carrying out interventions directly on the product integral representing the observed data distribution in a continuous-time counting process model framework. In recurrent events settings our target parameter identifies the expected number of recurrent events also in settings where the censoring mechanism or post-baseline treatment decisions depend on past information of post-baseline covariates such as the recurrent event process. We propose a flexible estimation procedure based on targeted maximum likelihood estimation coupled with highly adaptive lasso estimation to provide a novel approach for double robust and nonparametric inference for the considered target parameter. We illustrate the methods in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01736v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Helene C. W. Rytgaard, Mark J. van der Laan</dc:creator>
    </item>
    <item>
      <title>BRcal: An R Package to Boldness-Recalibrate Probability Predictions</title>
      <link>https://arxiv.org/abs/2409.13858</link>
      <description>arXiv:2409.13858v3 Announce Type: replace 
Abstract: When probability predictions are too cautious for decision making, boldness-recalibration enables responsible emboldening while maintaining the probability of calibration required by the user. We formulate boldness-recalibration as a nonlinear optimization of boldness with a nonlinear inequality constraint on calibration. We further show that recalibration based on the maximized linear log odds likelihood also maximizes the posterior probability of calibration. We introduce BRcal, an R package implementing boldness-recalibration and supporting methodology as recently proposed. The BRcal package provides direct control of the calibration-boldness tradeoff and visualizes how different calibration levels change individual predictions. We present a new real world case study involving housing foreclosure predictions. The BRcal package is available on the Comprehensive R Archive Network (CRAN) (https://cran.r-project.org/web/packages/BRcal/index.html) and on Github (https://github.com/apguthrie/BRcal).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13858v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adeline P. Guthrie, Christopher T. Franck</dc:creator>
    </item>
    <item>
      <title>Bayesian Covariate-Dependent Graph Learning with a Dual Group Spike-and-Slab Prior</title>
      <link>https://arxiv.org/abs/2409.17404</link>
      <description>arXiv:2409.17404v2 Announce Type: replace 
Abstract: Covariate-dependent graph learning has gained increasing interest in the graphical modeling literature for the analysis of heterogeneous data. This task, however, poses challenges to modeling, computational efficiency, and interpretability. The parameter of interest can be naturally represented as a three-dimensional array with elements that can be grouped according to two directions, corresponding to node level and covariate level, respectively. In this article, we propose a novel dual group spike-and-slab prior that enables multi-level selection at covariate-level and node-level, as well as individual (local) level sparsity. We introduce a nested strategy with specific choices to address distinct challenges posed by the various grouping directions. For posterior inference, we develop a tuning-free Gibbs sampler for all parameters, which mitigates the difficulties of parameter tuning often encountered in high-dimensional graphical models and facilitates routine implementation. Through simulation studies, we demonstrate that the proposed model outperforms existing methods in its accuracy of graph recovery. We show the practical utility of our model via an application to microbiome data where we seek to better understand the interactions among microbes as well as how these are affected by relevant covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17404v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zijian Zeng, Meng Li, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Combining BART and Principal Stratification to estimate the effect of intermediate variables on primary outcomes with application to estimating the effect of family planning on employment in Nigeria and Senegal</title>
      <link>https://arxiv.org/abs/2412.16320</link>
      <description>arXiv:2412.16320v3 Announce Type: replace 
Abstract: There is interest in learning about the causal effects of family planning (FP) on empowerment-related outcomes. Data related to this question are available from studies in which FP programs increase access to FP, but such interventions do not necessarily result in uptake of FP. In addition, women impacted by such programs may differ systematically from target populations of interest in ways that alter the effect of FP. To assess the causal effect of FP on empowerment-related outcomes, we developed a 2-step approach. We use principal stratification and Bayesian Additive Regression Trees (BART) to non-parametrically estimate the effect in the source population among women affected by a FP program. We generalize the results to a broader population by taking the expectation of conditional average treatment effects from the selective sample over the covariate distribution in the target population. To estimate (uncertainty in) the covariate distribution from survey data with a complex sampling design, we use a Bayesian bootstrap (BB). We apply the approach to estimate the causal effect of modern contraceptive use on employment among urban women in Nigeria and Senegal and find strong effects and effect heterogeneity. Sensitivity analyses suggest robustness to violations of assumptions for internal and external validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16320v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Godoy Garraza, Ilene Speizer, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>Comparative Review of Modern Competing Risk Methods in High-dimensional Settings</title>
      <link>https://arxiv.org/abs/2503.12824</link>
      <description>arXiv:2503.12824v2 Announce Type: replace 
Abstract: Competing risk analysis accounts for multiple mutually exclusive events, improving risk estimation over traditional survival analysis. Despite methodological advancements, a comprehensive comparison of competing risk methods, especially in high-dimensional settings, remains limited. This study evaluates penalized regression (LASSO, SCAD, MCP), boosting (CoxBoost, CB), random forest (RF), and deep learning (DeepHit, DH) methods for competing risk analysis through extensive simulations, assessing variable selection, estimation accuracy, discrimination, and calibration under diverse data conditions. Our results show that CB achieves the best variable selection, estimation stability, and discriminative ability, particularly in high-dimensional settings. while MCP and SCAD provide superior calibration in $n&gt;p$ scenarios. RF and DH capture nonlinear effects but exhibit instability, with RF showing high false discovery rates and DH suffering from poor calibration. Further, we compare the flexibility of these methods through the analysis of a melanoma gene expression data with survival information. This study provides practical guidelines for selecting competing risk models to ensure robust and interpretable analysis in high-dimensional settings and outlines important directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12824v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul M. Djangang, Summer S. Han, Nilotpal Sanyal</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for High-dimensional Time Series with a Directed Acyclic Graphical Structure</title>
      <link>https://arxiv.org/abs/2503.23563</link>
      <description>arXiv:2503.23563v2 Announce Type: replace 
Abstract: In multivariate time series analysis, understanding the underlying causal relationships among variables is often of interest for various applications. Directed acyclic graphs (DAGs) provide a powerful framework for representing causal dependencies. This paper proposes a novel Bayesian approach for modeling multivariate time series where conditional independencies and causal structure are encoded by a DAG. The proposed model allows structural properties such as stationarity to be easily accommodated. Given the application, we further extend the model for matrix-variate time series. We take a Bayesian approach to inference, and a ``projection-posterior'' based efficient computational algorithm is developed. The posterior convergence properties of the proposed method are established along with two identifiability results for the unrestricted structural equation models. The utility of the proposed method is demonstrated through simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23563v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>Adaptive adequacy testing of high-dimensional factor-augmented regression model</title>
      <link>https://arxiv.org/abs/2504.01432</link>
      <description>arXiv:2504.01432v2 Announce Type: replace 
Abstract: In this paper, we investigate the adequacy testing problem of high-dimensional factor-augmented regression model. Existing test procedures perform not well under dense alternatives. To address this critical issue, we introduce a novel quadratic-type test statistic which can efficiently detect dense alternative hypotheses. We further propose an adaptive test procedure to remain powerful under both sparse and dense alternative hypotheses. Theoretically, under the null hypothesis, we establish the asymptotic normality of the proposed quadratic-type test statistic and asymptotic independence of the newly introduced quadratic-type test statistic and a maximum-type test statistic. We also prove that our adaptive test procedure is powerful to detect signals under either sparse or dense alternative hypotheses. Simulation studies and an application to an FRED-MD macroeconomics dataset are carried out to illustrate the merits of our introduced procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01432v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanmei Shi, Leheng Cai, Xu Guo, Shurong Zheng</dc:creator>
    </item>
    <item>
      <title>Optimal Shrinkage Estimation of Fixed Effects in Linear Panel Data Models</title>
      <link>https://arxiv.org/abs/2308.12485</link>
      <description>arXiv:2308.12485v3 Announce Type: replace-cross 
Abstract: Shrinkage methods are frequently used to estimate fixed effects to reduce the noisiness of the least squares estimators. However, widely used shrinkage estimators guarantee such noise reduction only under strong distributional assumptions. I develop an estimator for the fixed effects that obtains the best possible mean squared error within a class of shrinkage estimators. This class includes conventional shrinkage estimators and the optimality does not require distributional assumptions. The estimator has an intuitive form and is easy to implement. Moreover, the fixed effects are allowed to vary with time and to be serially correlated, and the shrinkage optimally incorporates the underlying correlation structure in this case. In such a context, I also provide a method to forecast fixed effects one period ahead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12485v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soonwoo Kwon</dc:creator>
    </item>
    <item>
      <title>Contextual Dynamic Pricing: Algorithms, Optimality, and Local Differential Privacy Constraints</title>
      <link>https://arxiv.org/abs/2406.02424</link>
      <description>arXiv:2406.02424v2 Announce Type: replace-cross 
Abstract: We study contextual dynamic pricing problems where a firm sells products to $T$ sequentially-arriving consumers, behaving according to an unknown demand model. The firm aims to minimize its regret over a clairvoyant that knows the model in advance. The demand follows a generalized linear model (GLM), allowing for stochastic feature vectors in $\mathbb R^d$ encoding product and consumer information. We first show the optimal regret is of order $\sqrt{dT}$, up to logarithmic factors, improving existing upper bounds by a $\sqrt{d}$ factor. This optimal rate is materialized by two algorithms: a confidence bound-type algorithm and an explore-then-commit (ETC) algorithm. A key insight is an intrinsic connection between dynamic pricing and contextual multi-armed bandit problems with many arms with a careful discretization. We further study contextual dynamic pricing under local differential privacy (LDP) constraints. We propose a stochastic gradient descent-based ETC algorithm achieving regret upper bounds of order $d\sqrt{T}/\epsilon$, up to logarithmic factors, where $\epsilon&gt;0$ is the privacy parameter. The upper bounds with and without LDP constraints are matched by newly constructed minimax lower bounds, characterizing costs of privacy. Moreover, we extend our study to dynamic pricing under mixed privacy constraints, improving the privacy-utility tradeoff by leveraging public data. This is the first time such setting is studied in the dynamic pricing literature and our theoretical results seamlessly bridge dynamic pricing with and without LDP. Extensive numerical experiments and real data applications are conducted to illustrate the efficiency and practical value of our algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02424v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zifeng Zhao, Feiyu Jiang, Yi Yu</dc:creator>
    </item>
  </channel>
</rss>
