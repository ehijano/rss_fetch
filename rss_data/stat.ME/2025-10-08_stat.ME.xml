<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Oct 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Procrustes Problems on Random Matrices</title>
      <link>https://arxiv.org/abs/2510.05182</link>
      <description>arXiv:2510.05182v1 Announce Type: new 
Abstract: Meaningful comparison between sets of observations often necessitates alignment or registration between them, and the resulting optimization problems range in complexity from those admitting simple closed-form solutions to those requiring advanced and novel techniques. We compare different Procrustes problems in which we align two sets of points after various perturbations by minimizing the norm of the difference between one matrix and an orthogonal transformation of the other. The minimization problem depends significantly on the choice of matrix norm; we highlight recent developments in nonsmooth Riemannian optimization and characterize which choices of norm work best for each perturbation. We show that in several applications, from low-dimensional alignments to hypothesis testing for random networks, when Procrustes alignment with the spectral or robust norm is the appropriate choice, it is often feasible to replace the computationally more expensive spectral and robust minimizers with their closed-form Frobenius-norm counterpart. Our work reinforces the synergy between optimization, geometry, and statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05182v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hajg Jasa, Ronny Bergmann, Christian K\"ummerle, Avanti Athreya, Zachary Lubberts</dc:creator>
    </item>
    <item>
      <title>An efficient hybrid approach of quantile and expectile regression</title>
      <link>https://arxiv.org/abs/2510.05268</link>
      <description>arXiv:2510.05268v1 Announce Type: new 
Abstract: Quantiles and expectiles are determined by different loss functions: asymmetric least absolute deviation for quantiles and asymmetric squared loss for expectiles. This distinction ensures that quantile regression methods are robust to outliers but somewhat less effective than expectile regression, especially for normally distributed data. However, expectile regression is vulnerable to lack of robustness, especially for heavy-tailed distributions. To address this trade-off between robustness and effectiveness, we propose a novel approach. By introducing a parameter $\gamma$ that ranges between 0 and 1, we combine the aforementioned loss functions, resulting in a hybrid approach of quantiles and expectiles. This fusion leads to the estimation of a new type of location parameter family within the linear regression framework, termed Hybrid of Quantile and Expectile Regression (HQER). The asymptotic properties of the resulting estimaror are then established. Through simulation studies, we compare the asymptotic relative efficiency of the HQER estimator with its competitors, namely the quantile, expectile, and $k$th power expectile regression estimators. Our results show that HQER outperforms its competitors in several simulation scenarios. In addition, we apply HQER to a real dataset to illustrate its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05268v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdellah Atanane, Abdallah Mkhadri, Karim Oualkacha</dc:creator>
    </item>
    <item>
      <title>A new composite Mann-Whitney test for two-sample survival comparisons with right-censored data</title>
      <link>https://arxiv.org/abs/2510.05353</link>
      <description>arXiv:2510.05353v1 Announce Type: new 
Abstract: A fundamental challenge in comparing two survival distributions with right censored data is the selection of an appropriate nonparametric test, as the power of standard tests like the Log rank and Wilcoxon is highly dependent on the often unknown nature of the alternative hypothesis. This paper introduces a new, distribution free two sample test designed to overcome this limitation. The proposed method is based on a strategic decomposition of the data into uncensored and censored subsets, from which a composite test statistic is constructed as the sum of two independent Mann Whitney statistics. This design allows the test to automatically and inherently adapt to various patterns of difference including early, late, and crossing hazards without requiring pre specified parameters, pre testing, or complex weighting schemes. An extensive Monte Carlo simulation study demonstrates that the proposed test robustly maintains the nominal Type I error rate. Crucially, its power is highly competitive with the optimal traditional tests in standard scenarios and superior in complex settings with crossing survival curves, while also exhibiting remarkable robustness to high levels of censoring. The test power effectively approximates the maximum power achievable by either the Log rank or Wilcoxon tests across a wide range of alternatives, offering a powerful, versatile, and computationally simple tool for survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05353v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abid Hussain, Touqeer Ahmad</dc:creator>
    </item>
    <item>
      <title>Sparse-Group Factor Analysis for High-Dimensional Time Series</title>
      <link>https://arxiv.org/abs/2510.05370</link>
      <description>arXiv:2510.05370v1 Announce Type: new 
Abstract: Factor analysis is a widely used technique for dimension reduction in high-dimensional data. However, a key challenge in factor models lies in the interpretability of the latent factors. One intuitive way to interpret these factors is through their associated loadings. Liu and Wang proposed a novel framework that redefines factor models with sparse loadings to enhance interpretability. In many high-dimensional time series applications, variables exhibit natural group structures. Building on this idea, our paper incorporates domain knowledge and prior information by modeling both individual sparsity and group sparsity in the loading matrix. This dual-sparsity framework further improves the interpretability of the estimated factors. We develop an algorithm to estimate both the loading matrix and the common component, and we establish the asymptotic properties of the resulting estimators. Simulation studies demonstrate the strong performance of the proposed method, and a real-data application illustrates how incorporating prior knowledge leads to more interpretable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05370v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Wang, Xialu Liu</dc:creator>
    </item>
    <item>
      <title>Can language models boost the power of randomized experiments without statistical bias?</title>
      <link>https://arxiv.org/abs/2510.05545</link>
      <description>arXiv:2510.05545v1 Announce Type: new 
Abstract: Randomized experiments or randomized controlled trials (RCTs) are gold standards for causal inference, yet cost and sample-size constraints limit power. Meanwhile, modern RCTs routinely collect rich, unstructured data that are highly prognostic of outcomes but rarely used in causal analyses. We introduce CALM (Causal Analysis leveraging Language Models), a statistical framework that integrates large language models (LLMs) predictions with established causal estimators to increase precision while preserving statistical validity. CALM treats LLM outputs as auxiliary prognostic information and corrects their potential bias via a heterogeneous calibration step that residualizes and optimally reweights predictions. We prove that CALM remains consistent even when LLM predictions are biased and achieves efficiency gains over augmented inverse probability weighting estimators for various causal effects. In particular, CALM develops a few-shot variant that aggregates predictions across randomly sampled demonstration sets. The resulting U-statistic-like predictor restores i.i.d. structure and also mitigates prompt-selection variability. Empirically, in simulations calibrated to a mobile-app depression RCT, CALM delivers lower variance relative to other benchmarking methods, is effective in zero- and few-shot settings, and remains stable across prompt designs. By principled use of LLMs to harness unstructured data and external knowledge learned during pretraining, CALM provides a practical path to more precise causal analyses in RCTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05545v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinrui Ruan, Xinwei Ma, Yingfei Wang, Waverly Wei, Jingshen Wang</dc:creator>
    </item>
    <item>
      <title>A Bivariate DAR($1$) model for ordinal time series</title>
      <link>https://arxiv.org/abs/2510.05680</link>
      <description>arXiv:2510.05680v1 Announce Type: new 
Abstract: We present a bivariate vector valued discrete autoregressive model of order $1$ (BDAR($1$)) for discrete time series. The BDAR($1$) model assumes that each time series follows its own univariate DAR($1$) model with dependent random mechanisms that determine from which component the current status occurs and dependent innovations. The joint distribution of the random mechanisms which are expressed by Bernoulli vectors are proposed to be defined through copulas. The same holds for the joint distribution of innovation terms. Properties of the model are provided, while special focus is given to the case of bivariate ordinal time series. A simulation study is presented, indicating that model provides efficient estimates even in case of moderate sample size. Finally, a real data application on unemployment state of two countries is presented, for illustrating the proposed model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05680v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Nalpantidi, Dimitris Karlis</dc:creator>
    </item>
    <item>
      <title>Extension of Wald-Wolfowitz Runs Test for Regression Validity Testing with Repeated Measures of Independent Variable</title>
      <link>https://arxiv.org/abs/2510.05861</link>
      <description>arXiv:2510.05861v1 Announce Type: new 
Abstract: The Wald-Wolfowitz runs test can assess the correctness of a regression curve fitted to a data set with one independent parameter. The assessment is performed through examination of the residuals, where the signs of the residuals would appear randomly if the regression curve were correct. We propose extending the test to the case where multiple data points were measured for specific independent parameter values. By randomly permutating the data points corresponding to each independent parameter value and treating their residuals as occurring in their permutated sequence and then executing the runs test, results are shown to be equivalent to those of a data set containing the same number of points with no repeated measurements. This approach avoids the loss of points, and hence loss of test sensitivity, were the means at each independent parameter value used. It also avoids the problem of weighting each mean differently if the number of data points measured at each parameter value is not identical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05861v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo-Yao Lian, Nelson G. Chen</dc:creator>
    </item>
    <item>
      <title>A subsampling approach for large data sets when the Generalised Linear Model is potentially misspecified</title>
      <link>https://arxiv.org/abs/2510.05902</link>
      <description>arXiv:2510.05902v1 Announce Type: new 
Abstract: Subsampling is a computationally efficient and scalable method to draw inference in large data settings based on a subset of the data rather than needing to consider the whole dataset. When employing subsampling techniques, a crucial consideration is how to select an informative subset based on the queries posed by the data analyst. A recently proposed method for this purpose involves randomly selecting samples from the large dataset based on subsampling probabilities. However, a major drawback of this approach is that the derived subsampling probabilities are typically based on an assumed statistical model which may be difficult to correctly specify in practice. To address this limitation, we propose to determine subsampling probabilities based on a statistical model that we acknowledge may be misspecified. To do so, we propose to evaluate the subsampling probabilities based on the Mean Squared Error (MSE) of the predictions from a model that is not assumed to completely describe the large dataset. We apply our subsampling approach in a simulation study and for the analysis of two real-world large datasets, where its performance is benchmarked against existing subsampling techniques. The findings suggest that there is value in adopting our approach over current practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05902v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amalan Mahendran, Helen Thompson, James M. McGree</dc:creator>
    </item>
    <item>
      <title>Automated Gating for Flow Cytometry Data Using a Kernel-Smoothed EM Algorithm</title>
      <link>https://arxiv.org/abs/2510.06051</link>
      <description>arXiv:2510.06051v1 Announce Type: new 
Abstract: Phytoplankton are microscopic algae responsible for roughly half of the world's photosynthesis that play a critical role in global carbon cycles and oxygen production, and measuring the abundance of their subtypes across a wide range of spatiotemporal scales is of great relevance to oceanography. High-frequency flow cytometry is a powerful technique in which oceanographers at sea can rapidly record the optical properties of tens of thousands of individual phytoplankton cells every few minutes. Identifying distinct subpopulations within these vast datasets (a process known as "gating") remains a major challenge and has largely been performed manually so far. In this paper, we introduce a fast, automated gating method, which accurately identifies phytoplankton populations by fitting a time-evolving mixture of Gaussians model using an expectation-maximization-like algorithm with kernel smoothing. We use simulated data to demonstrate the validity and robustness of this approach, and use oceanographic cruise data to highlight the method's ability to not only replicate but surpass expert manual gating. Finally, we provide the flowkernel R package, written in literate programming, that implements the algorithm efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06051v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farhad de Sousa, Fran\c{c}ois Ribalet, Jacob Bien</dc:creator>
    </item>
    <item>
      <title>Geometric Model Selection for Latent Space Network Models: Hypothesis Testing via Multidimensional Scaling and Resampling Techniques</title>
      <link>https://arxiv.org/abs/2510.06136</link>
      <description>arXiv:2510.06136v1 Announce Type: new 
Abstract: Latent space models assume that network ties are more likely between nodes that are closer together in an underlying latent space. Euclidean space is a popular choice for the underlying geometry, but hyperbolic geometry can mimic more realistic patterns of ties in complex networks. To identify the underlying geometry, past research has applied non-Euclidean extensions of multidimensional scaling (MDS) to the observed geodesic distances: the shortest path lengths between nodes. The difference in stress, a standard goodness-of-fit metric for MDS, across the geometries is then used to select a latent geometry with superior model fit (lower stress). The effectiveness of this method is assessed through simulations of latent space networks in Euclidean and hyperbolic geometries. To better account for uncertainty, we extend permutation-based hypothesis tests for MDS to the latent network setting. However, these tests do not incorporate any network structure. We propose a parametric bootstrap distribution of networks, conditioned on observed geodesic distances and the Gaussian Latent Position Model (GLPM). Our method extends the Davidson-MacKinnon J-test to latent space network models with differing latent geometries. We pay particular attention to large and sparse networks, and both the permutation test and the bootstrapping methods show an improvement in detecting the underlying geometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06136v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jieyun Wang, Anna L. Smith</dc:creator>
    </item>
    <item>
      <title>A GNAR-Based Framework for Spectral Estimation of Network Time Series: Application to Global Bank Network Connectedness</title>
      <link>https://arxiv.org/abs/2510.06157</link>
      <description>arXiv:2510.06157v1 Announce Type: new 
Abstract: Patterns of dependence in financial networks, such as global bank connectedness, evolve over time and across frequencies. Analysing these systems requires statistical tools that jointly capture temporal dynamics and the underlying network topology. This work develops a novel spectral analysis framework for Generalized Network Autoregressive (GNAR) processes, modeling dependencies beyond direct neighbours by incorporating r-stage neighbourhood effects, unlike existing methods that at best rely solely on adjacency-based interactions. We define the GNAR spectral density and related quantities, such as coherence and partial coherence, for which we propose both parametric and network-penalized nonparametric estimators. Extensive simulations demonstrate the strong performance of the parametric spectral estimator, as also backed up by theoretical arguments. The proposed framework has wide applications, and here we focus on the analysis of global bank network connectedness. The findings illustrate how the GNAR spectral quantities effectively capture the frequency-specific cross-nodal dependencies, thus yielding estimates consistent with established measures, while also uncovering richer temporal and structural patterns of volatility transmission.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06157v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristian F. Jim\'enez-Var\'on, Marina I. Knight</dc:creator>
    </item>
    <item>
      <title>Power-divergence copulas: A new class of Archimedean copulas, with an insurance application</title>
      <link>https://arxiv.org/abs/2510.06177</link>
      <description>arXiv:2510.06177v1 Announce Type: new 
Abstract: This paper demonstrates that, under a particular convention, the convex functions that characterise the phi divergences also generate Archimedean copulas in at least two dimensions. As a special case, we develop the family of Archimedean copulas associated with the important family of power divergences, which we call the power-divergence copulas. The properties of the family are extensively studied, including the subfamilies that are absolutely continuous or have a singular component, the ordering of the family, limiting cases (i.e., the Frechet-Hoeffding lower bound and Frechet-Hoeffding upper bound), the Kendall's tau and tail-dependence coefficients, and cases that extend to three or more dimensions. In an illustrative application, the power-divergence copulas are used to model a Danish fire insurance dataset. It is shown that the power-divergence copulas provide an adequate fit to the bivariate distribution of two kinds of fire-related losses claimed by businesses, while several benchmarks (a suite of well known Archimedean, extreme-value, and elliptical copulas) do not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06177v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alan R. Pearse, Howard Bondell</dc:creator>
    </item>
    <item>
      <title>Tensor time series change-point detection in cryptocurrency network data</title>
      <link>https://arxiv.org/abs/2510.06211</link>
      <description>arXiv:2510.06211v1 Announce Type: new 
Abstract: Financial fraud has been growing exponentially in recent years. The rise of cryptocurrencies as an investment asset has simultaneously seen a parallel growth in cryptocurrency scams. To detect possible cryptocurrency fraud, and in particular market manipulation, previous research focused on the detection of changes in the network of trades; however, market manipulators are now trading across multiple cryptocurrency platforms, making their detection more difficult. Hence, it is important to consider the identification of changes across several trading networks or a `network of networks' over time. To this end, in this article, we propose a new change-point detection method in the network structure of tensor-variate data. This new method, labeled TenSeg, first employs a tensor decomposition, and second detects multiple change-points in the second-order (cross-covariance or network) structure of the decomposed data. It allows for change-point detection in the presence of frequent changes of possibly small magnitudes and is computationally fast. We apply our method to several simulated datasets and to a cryptocurrency dataset, which consists of network tensor-variate data from the Ethereum blockchain. We demonstrate that our approach substantially outperforms other state-of-the-art change-point techniques, and the detected change-points in the Ethereum data set coincide with changes across several trading networks or a `network of networks' over time. Finally, all the relevant \textsf{R} code implementing the method in the article are available on https://github.com/Anastasiou-Andreas/TenSeg.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06211v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andreas Anastasiou, Ivor Cribben</dc:creator>
    </item>
    <item>
      <title>Estimating Treatment Effects Under Bounded Heterogeneity</title>
      <link>https://arxiv.org/abs/2510.05454</link>
      <description>arXiv:2510.05454v1 Announce Type: cross 
Abstract: Researchers often use specifications that correctly estimate the average treatment effect under the assumption of constant effects. When treatment effects are heterogeneous, however, such specifications generally fail to recover this average effect. Augmenting these specifications with interaction terms between demeaned covariates and treatment eliminates this bias, but often leads to imprecise estimates and becomes infeasible under limited overlap. We propose a generalized ridge regression estimator, $\texttt{regulaTE}$, that penalizes the coefficients on the interaction terms to achieve an optimal trade-off between worst-case bias and variance in estimating the average effect under limited treatment effect heterogeneity. Building on this estimator, we construct confidence intervals that remain valid under limited overlap and can also be used to assess sensitivity to violations of the constant effects assumption. We illustrate the method in empirical applications under unconfoundedness and staggered adoption, providing a practical approach to inference under limited overlap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05454v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soonwoo Kwon, Liyang Sun</dc:creator>
    </item>
    <item>
      <title>Robust Inference for Convex Pairwise Difference Estimators</title>
      <link>https://arxiv.org/abs/2510.05991</link>
      <description>arXiv:2510.05991v1 Announce Type: cross 
Abstract: This paper develops distribution theory and bootstrap-based inference methods for a broad class of convex pairwise difference estimators. These estimators minimize a kernel-weighted convex-in-parameter function over observation pairs that are similar in terms of certain covariates, where the similarity is governed by a localization (bandwidth) parameter. While classical results establish asymptotic normality under restrictive bandwidth conditions, we show that valid Gaussian and bootstrap-based inference remains possible under substantially weaker assumptions. First, we extend the theory of small bandwidth asymptotics to convex pairwise estimation settings, deriving robust Gaussian approximations even when a smaller than standard bandwidth is used. Second, we employ a debiasing procedure based on generalized jackknifing to enable inference with larger bandwidths, while preserving convexity of the objective function. Third, we construct a novel bootstrap method that adjusts for bandwidth-induced variance distortions, yielding valid inference across a wide range of bandwidth choices. Our proposed inference method enjoys demonstrable more robustness, while retaining the practical appeal of convex pairwise difference estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05991v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Michael Jansson, Kenichi Nagasawa</dc:creator>
    </item>
    <item>
      <title>The analysis of paired comparison data in the presence of cyclicality and intransitivity</title>
      <link>https://arxiv.org/abs/2406.11584</link>
      <description>arXiv:2406.11584v2 Announce Type: replace 
Abstract: A principled approach to cyclicality and intransitivity in paired comparison data is developed. The proposed methodology enables more precise estimation of the underlying preference profile and facilitates the identification of all cyclic patterns and potential intransitivities. Consequently, it improves upon existing methods for ranking and prediction, including enhanced performance in betting and wagering systems. Fundamental to our development is a detailed understanding and study of the parameter space that accommodates cyclicality and intransitivity. It is shown that identifying cyclicality and intransitivity reduces to a model selection problem, and a new method for model selection employing geometrical insights, unique to the problem at hand, is proposed. The large sample properties of the estimators and guarantees on the selected model are provided. Thus, it is shown that in large samples all cyclical relations and consequent intransitivities can be identified. The method is exemplified using simulations and analysis of an illustrative example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11584v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rahul Singh, Ori Davidov</dc:creator>
    </item>
    <item>
      <title>YEAST: Yet Another Sequential Test</title>
      <link>https://arxiv.org/abs/2406.16523</link>
      <description>arXiv:2406.16523v5 Announce Type: replace 
Abstract: Online evaluation of machine learning models is typically conducted through A/B experiments. Sequential statistical tests are valuable tools for analysing these experiments, as they enable researchers to stop data collection early without increasing the risk of false discoveries. However, existing sequential tests either limit the number of interim analyses or suffer from low statistical power. In this paper, we introduce a novel sequential test designed for continuous monitoring of A/B experiments. We validate our method using semi-synthetic simulations and demonstrate that it outperforms current state-of-the-art sequential testing approaches. Our method is derived using a new technique that inverts a bound on the probability of threshold crossing, based on a classical maximal inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16523v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexey Kurennoy, Majed Dodin, Tural Gurbanov, Ana Peleteiro Ramallo</dc:creator>
    </item>
    <item>
      <title>A Deterministic Information Bottleneck Method for Clustering Mixed-Type Data</title>
      <link>https://arxiv.org/abs/2407.03389</link>
      <description>arXiv:2407.03389v5 Announce Type: replace 
Abstract: In this paper, we present an information-theoretic method for clustering mixed-type data, that is, data consisting of both continuous and categorical variables. The proposed approach extends the Information Bottleneck principle to heterogeneous data through generalised product kernels, integrating continuous, nominal, and ordinal variables within a unified optimization framework. We address the following challenges: developing a systematic bandwidth selection strategy that equalises contributions across variable types, and proposing an adaptive hyperparameter updating scheme that ensures a valid solution into a predetermined number of potentially imbalanced clusters. Through simulations on 28,800 synthetic data sets and ten publicly available benchmarks, we demonstrate that the proposed method, named DIBmix, achieves superior performance compared to four established methods (KAMILA, K-Prototypes, FAMD with K-Means, and PAM with Gower's dissimilarity). Results show DIBmix particularly excels when clusters exhibit size imbalances, data contain low or moderate cluster overlap, and categorical and continuous variables are equally represented. The method presents a significant advantage over traditional centroid-based algorithms, establishing DIBmix as a competitive and theoretically grounded alternative for mixed-type data clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03389v5</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Efthymios Costa, Ioanna Papatsouma, Angelos Markos</dc:creator>
    </item>
    <item>
      <title>Inequality Constrained Minimum Density Power Divergence Estimation in Panel Count Data</title>
      <link>https://arxiv.org/abs/2503.21534</link>
      <description>arXiv:2503.21534v4 Announce Type: replace 
Abstract: The analysis of panel count data has garnered considerable attention in the literature, leading to the development of multiple statistical techniques. In inferential analysis, most works focus on leveraging estimating equation-based techniques or conventional maximum likelihood estimation. However, the robustness of these methods is largely questionable. In this paper, we present a robust density power divergence estimation method for panel count data arising from non-homogeneous Poisson processes correlated through a latent frailty variable. To cope with real-world incidents, it is often desirable to impose certain inequality constraints on the parameter space, leading to the constrained minimum density power divergence estimator. Being incorporated with inequality restrictions, coupled with the inherent complexity of our objective function, standard computational algorithms are inadequate for estimation purposes. To overcome this, we adopt sequential convex programming, which approximates the original problem through a series of subproblems. Further, we study the asymptotic properties of the resultant estimator, making a significant contribution to this work. The proposed method ensures high efficiency in the model estimation while providing reliable inference despite data contamination. Moreover, the density power divergence measure is governed by a tuning parameter $\gamma$, which controls the trade-off between robustness and efficiency. To effectively determine the optimal value of $\gamma$, this study employs a generalized score-matching technique, marking considerable progress in the data analysis. Simulation studies and real data examples are provided to illustrate the performance of the estimator and to substantiate the theory developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21534v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Udita Goswami, Shuvashree Mondal</dc:creator>
    </item>
    <item>
      <title>Modeling Large Nonstationary Spatial Data with the Full-Scale Basis Graphical Lasso</title>
      <link>https://arxiv.org/abs/2505.01318</link>
      <description>arXiv:2505.01318v2 Announce Type: replace 
Abstract: We propose a new approach for the modeling large datasets of nonstationary spatial processes that combines a latent low rank process and a sparse covariance model. The low rank component coefficients are endowed with a flexible graphical Gaussian Markov random field model. The utilization of a low rank and compactly-supported covariance structure combines the full-scale approximation and the basis graphical lasso; we term this new approach the full-scale basis graphical lasso (FSBGL). Estimation employs a graphical lasso-penalized likelihood, which is optimized using a difference-of-convex scheme. We illustrate the proposed approach on synthetic fields as well as with a challenging high-resolution simulation dataset of the thermosphere. In a comparison against state-of-the-art spatial models, the FSBGL performs better at capturing salient features of the thermospheric temperature fields, even with limited available training data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01318v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew LeDuc, William Kleiber, Tomoko Matsuo</dc:creator>
    </item>
    <item>
      <title>Estimation of Treatment Effects Under Nonstationarity via the Truncated Policy Gradient Estimator</title>
      <link>https://arxiv.org/abs/2506.05308</link>
      <description>arXiv:2506.05308v2 Announce Type: replace 
Abstract: Randomized experiments (or A/B tests) are widely used to evaluate interventions in dynamic systems such as recommendation platforms, marketplaces, and digital health. In these settings, interventions affect both current and future system states, so estimating the global average treatment effect (GATE) requires accounting for temporal dynamics, which is especially challenging in the presence of nonstationarity; existing approaches suffer from high bias, high variance, or both. In this paper, we address this challenge via the novel Truncated Policy Gradient (TPG) estimator, which replaces instantaneous outcomes with short-horizon outcome trajectories. The estimator admits a policy-gradient interpretation: it is a truncation of the first-order approximation to the GATE, yielding provable reductions in bias and variance in nonstationary Markovian settings. We further establish a central limit theorem for the TPG estimator and develop a consistent variance estimator that remains valid under nonstationarity with single-trajectory data. We validate our theory with two real-world case studies. The results show that a well-calibrated TPG estimator attains low bias and variance in practical nonstationary settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05308v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramesh Johari, Tianyi Peng, Wenqian Xing</dc:creator>
    </item>
    <item>
      <title>Conditional Local Independence Testing for It\^o processes with Applications to Dynamic Causal Discovery</title>
      <link>https://arxiv.org/abs/2506.07844</link>
      <description>arXiv:2506.07844v3 Announce Type: replace 
Abstract: Inferring causal relationships from dynamical systems is the central interest of many scientific inquiries. Conditional local independence, which describes whether the evolution of one process is influenced by another process given additional processes, is important for causal learning in such systems. In this paper, we propose a hypothesis test for conditional local independence in It\^o processes. Our test is grounded in the semimartingale decomposition of the It\^o process, with which we introduce a stochastic integral process that is a martingale under the null hypothesis. We then apply a test for the martingale property, quantifying potential deviation from local independence. The test statistics is estimated using the optimal filtering equation. We show the consistency of the estimation, thereby establishing the level and power of our test. Numerical verification and a real-world application to causal discovery in brain resting-state fMRIs are conducted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07844v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingzhou Liu, Xinwei Sun, Yizhou Wang</dc:creator>
    </item>
    <item>
      <title>Does $K$-fold CV based penalty perform variable selection or does it lead to $n^{1/2}$-consistency in Lasso?</title>
      <link>https://arxiv.org/abs/2507.12457</link>
      <description>arXiv:2507.12457v3 Announce Type: replace 
Abstract: Least absolute shrinkage and selection operator or Lasso, introduced by Tibshirani (1996), is one of the widely used regularization methods in regression. It is observed that the properties of Lasso vary wildly depending on the choice of the penalty parameter. The recent results of Lahiri (2021) suggest that, depending on the nature of the penalty parameter, Lasso can either be variable selection consistent or be $n^{1/2}-$consistent. However, practitioners generally implement Lasso by choosing the penalty parameter in a data-dependent way, the most popular being the $K$-fold cross-validation. In this paper, we explore the variable selection consistency and $n^{1/2}-$consistency of Lasso when the penalty is chosen based on $K$-fold cross-validation with $K$ being fixed. We consider the fixed-dimensional heteroscedastic linear regression model and show that Lasso with $K$-fold cross-validation based penalty is $n^{1/2}-$consistent, but not variable selection consistent. We also establish the $n^{1/2}-$consistency of the $K$-fold cross-validation based penalty as an intermediate result. Additionally, as a consequence of $n^{1/2}-$consistency, we establish the validity of Bootstrap to approximate the distribution of the Lasso estimator based on $K-$fold cross-validation. We validate the Bootstrap approximation in finite samples based on a moderate simulation study. Thus, our results essentially justify the use of $K$-fold cross-validation in practice to draw inferences based on $n^{1/2}-$scaled pivotal quantities in Lasso regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12457v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mayukh Choudhury, Debraj Das</dc:creator>
    </item>
    <item>
      <title>Structural Nested Mean Models for Modified Treatment Policies</title>
      <link>https://arxiv.org/abs/2509.22916</link>
      <description>arXiv:2509.22916v2 Announce Type: replace 
Abstract: There is a growing literature on estimating effects of treatment strategies based on the natural treatment that would have been received in the absence of intervention, often dubbed `modified treatment policies' (MTPs). MTPs are sometimes of interest because they are more realistic than interventions setting exposure to an ideal level for all members of a population. In the general time-varying setting, Richardson and Robins (2013) provided exchangeability conditions for nonparametric identification of MTP effects that could be deduced from Single World Intervention Graphs (SWIGs). Diaz (2023) provided multiply robust estimators under these identification assumptions that allow for machine learning nuisance regressions. In this paper, we fill a remaining gap by extending Structural Nested Mean Models (SNMMs) to MTP settings, which enables characterization of (time-varying) heterogeneity of MTP effects. We do this both under the exchangeability assumptions of Richardson and Robins (2013) and under parallel trends assumptions, which enables investigation of (time-varying heterogeneous) MTP effects in the presence of some unobserved confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22916v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Shahn</dc:creator>
    </item>
    <item>
      <title>SKADA-Bench: Benchmarking Unsupervised Domain Adaptation Methods with Realistic Validation On Diverse Modalities</title>
      <link>https://arxiv.org/abs/2407.11676</link>
      <description>arXiv:2407.11676v4 Announce Type: replace-cross 
Abstract: Unsupervised Domain Adaptation (DA) consists of adapting a model trained on a labeled source domain to perform well on an unlabeled target domain with some data distribution shift. While many methods have been proposed in the literature, fair and realistic evaluation remains an open question, particularly due to methodological difficulties in selecting hyperparameters in the unsupervised setting. With SKADA-bench, we propose a framework to evaluate DA methods on diverse modalities, beyond computer vision task that have been largely explored in the literature. We present a complete and fair evaluation of existing shallow algorithms, including reweighting, mapping, and subspace alignment. Realistic hyperparameter selection is performed with nested cross-validation and various unsupervised model selection scores, on both simulated datasets with controlled shifts and real-world datasets across diverse modalities, such as images, text, biomedical, and tabular data. Our benchmark highlights the importance of realistic validation and provides practical guidance for real-life applications, with key insights into the choice and impact of model selection approaches. SKADA-bench is open-source, reproducible, and can be easily extended with novel DA methods, datasets, and model selection criteria without requiring re-evaluating competitors. SKADA-bench is available on Github at https://github.com/scikit-adaptation/skada-bench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11676v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanis Lalou, Th\'eo Gnassounou, Antoine Collas, Antoine de Mathelin, Oleksii Kachaiev, Ambroise Odonnat, Alexandre Gramfort, Thomas Moreau, R\'emi Flamary</dc:creator>
    </item>
    <item>
      <title>Maximum Ideal Likelihood Estimation: A Unified Inference Framework for Latent Variable Models</title>
      <link>https://arxiv.org/abs/2410.01194</link>
      <description>arXiv:2410.01194v2 Announce Type: replace-cross 
Abstract: This paper develops a unified estimation framework, the Maximum Ideal Likelihood Estimation (MILE), for general parametric models with latent variables. Unlike traditional approaches relying on the marginal likelihood of the observed data, MILE directly exploits the joint distribution of the complete data by treating the latent variables as parameters (the ideal likelihood). Borrowing strength from optimisation techniques and algorithms, MILE is a broadly applicable framework in case that traditional methods fail, such as when the marginal likelihood has non-finite expectations. MILE offers a flexible and robust alternative to established techniques, including the Expectation-Maximisation algorithm and Markov chain Monte Carlo. We facilitate statistical inference of MILE on consistency, asymptotic distribution, and equivalence to the Maximum Likelihood Estimation, under some mild conditions. Extensive simulations illustrative real-data applications illustrate the empirical advantages of MILE, outperforming existing methods on computational feasibility and scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01194v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizhou Cai, Ting Fung Ma</dc:creator>
    </item>
    <item>
      <title>Conjugate gradient methods for high-dimensional GLMMs</title>
      <link>https://arxiv.org/abs/2411.04729</link>
      <description>arXiv:2411.04729v2 Announce Type: replace-cross 
Abstract: Generalized linear mixed models (GLMMs) are a widely used tool in statistical analysis. The main bottleneck of many computational approaches lies in the inversion of the high dimensional precision matrices associated with the random effects. Such matrices are typically sparse; however, the sparsity pattern resembles a multi partite random graph, which does not lend itself well to default sparse linear algebra techniques. Notably, we show that, for typical GLMMs, the Cholesky factor is dense even when the original precision is sparse. We thus turn to approximate iterative techniques, in particular to the conjugate gradient (CG) method. We combine a detailed analysis of the spectrum of said precision matrices with results from random graph theory to show that CG-based methods applied to high-dimensional GLMMs typically achieve a fixed approximation error with a total cost that scales linearly with the number of parameters and observations. Numerical illustrations with both real and simulated data confirm the theoretical findings, while at the same time illustrating situations, such as nested structures, where CG-based methods struggle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04729v2</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Pandolfi, Omiros Papaspiliopoulos, Giacomo Zanella</dc:creator>
    </item>
    <item>
      <title>Offline changepoint localization using a matrix of conformal p-values</title>
      <link>https://arxiv.org/abs/2505.00292</link>
      <description>arXiv:2505.00292v4 Announce Type: replace-cross 
Abstract: Changepoint localization is the problem of estimating the index at which a change occurred in the data generating distribution of an ordered list of data, or declaring that no change occurred. We present the broadly applicable MCP algorithm, which uses a matrix of conformal p-values to produce a confidence interval for a (single) changepoint under the mild assumption that the pre-change and post-change distributions are each exchangeable. We prove a novel conformal Neyman-Pearson lemma, motivating practical classifier-based choices for our conformal score function. Finally, we exemplify the MCP algorithm on a variety of synthetic and real-world datasets, including using black-box pre-trained classifiers to detect changes in sequences of images, text, and accelerometer data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00292v4</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>FARS: Factor Augmented Regression Scenarios in R</title>
      <link>https://arxiv.org/abs/2507.10679</link>
      <description>arXiv:2507.10679v3 Announce Type: replace-cross 
Abstract: In the context of macroeconomic/financial time series, the FARS package provides a comprehensive framework in R for the construction of conditional densities of the variable of interest based on the factor-augmented quantile regressions (FA-QRs) methodology, with the factors extracted from multi-level dynamic factor models (ML-DFMs) with potential overlapping group-specific factors. Furthermore, the package also allows the construction of measures of risk as well as modeling and designing economic scenarios based on the conditional densities. In particular, the package enables users to: (i) extract global and group-specific factors using a flexible multi-level factor structure; (ii) compute asymptotically valid confidence regions for the estimated factors, accounting for uncertainty in the factor loadings; (iii) obtain estimates of the parameters of the FA-QRs together with their standard deviations; (iv) recover full predictive conditional densities from estimated quantiles; (v) obtain risk measures based on extreme quantiles of the conditional densities; and (vi) estimate the conditional density and the corresponding extreme quantiles when the factors are stressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10679v3</guid>
      <category>stat.CO</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gian Pietro Bellocca, Ignacio Garr\'on, Vladimir Rodr\'iguez-Caballero, Esther Ruiz</dc:creator>
    </item>
    <item>
      <title>Assessing (im)balance in signed brain networks</title>
      <link>https://arxiv.org/abs/2508.00542</link>
      <description>arXiv:2508.00542v2 Announce Type: replace-cross 
Abstract: Many complex systems - be they financial, natural or social - are composed by units - such as stocks, neurons or agents - whose joint activity can be represented as a multivariate time series. An issue of both practical and theoretical importance concerns the possibility of inferring the presence of a static relationships between any two units solely from their dynamic state. The present contribution aims at tackling such an issue within the frame of traditional hypothesis testing: briefly speaking, our suggestion is that of linking any two units if behaving in a sufficiently similar way. To achieve such a goal, we project a multivariate time series onto a signed graph by i) comparing the empirical properties of the former with those expected under a suitable benchmark and ii) linking any two units with a positive (negative) edge in case the corresponding series share a significantly large number of concordant (discordant) values. To define our benchmarks, we adopt an information-theoretic approach that is rooted into the constrained maximisation of Shannon entropy, a procedure inducing an ensemble of multivariate time series that preserves some of the empirical properties on average while randomising everything else. We showcase the possible applications of our method by addressing one of the most timely issues in the domain of neurosciences, i.e. that of determining if brain networks are frustrated or not - and, in case, to what extent. As our results suggest, this is indeed the case, the structure of the negative subgraph being more prone to inter-subject variability than the complementary, positive subgraph. At the mesoscopic level, instead, the minimisation of the Bayesian Information Criterion instantiated with the Signed Stochastic Block Model reveals that brain areas gather into modules aligning with the statistical variant of the Relaxed Balance Theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00542v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.data-an</category>
      <category>physics.med-ph</category>
      <category>stat.ME</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marzio Di Vece, Emanuele Agrimi, Samuele Tatullo, Tommaso Gili, Miguel Ib\'a\~nez-Berganza, Tiziano Squartini</dc:creator>
    </item>
    <item>
      <title>Quickest Change Detection with Cost-Constrained Experiment Design</title>
      <link>https://arxiv.org/abs/2509.14186</link>
      <description>arXiv:2509.14186v2 Announce Type: replace-cross 
Abstract: In the classical quickest change detection problem, an observer performs a single experiment to monitor a stochastic process. The goal in the classical problem is to detect a change in the statistical properties of the process, with the minimum possible delay, subject to a constraint on the rate of false alarms. This paper considers the case where, at each observation time, the decision-maker must choose between multiple experiments with varying information qualities and costs. The change can be detected using any of the experiments. The goal here is to detect the change with the minimum delay, subject to constraints on the rate of false alarms and the fraction of time each experiment is performed before the time of change. The constraint on the fraction of time can be used to control the overall cost of using the system of experiments. An algorithm called the two-experiment cumulative sum (2E-CUSUM) algorithm is first proposed to solve the problem when there are only two experiments. The algorithm for the case of multiple experiments, starting with three experiments, is then designed iteratively using the 2E-CUSUM algorithm. Two key ideas used in the design are the scaling of undershoots and the truncation of tests. The multiple-experiment algorithm can be designed to satisfy the constraints and can achieve the delay performance of the experiment with the highest quality within a constant. The important concept of data efficiency, where the observer has the choice of not performing any experiment, is explored as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14186v2</guid>
      <category>eess.SP</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 08 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Vincent N. Lubenia, Taposh Banerjee</dc:creator>
    </item>
  </channel>
</rss>
