<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Jul 2025 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On Fine-Grained Distinct Element Estimation</title>
      <link>https://arxiv.org/abs/2506.22608</link>
      <description>arXiv:2506.22608v1 Announce Type: new 
Abstract: We study the problem of distributed distinct element estimation, where $\alpha$ servers each receive a subset of a universe $[n]$ and aim to compute a $(1+\varepsilon)$-approximation to the number of distinct elements using minimal communication. While prior work establishes a worst-case bound of $\Theta\left(\alpha\log n+\frac{\alpha}{\varepsilon^2}\right)$ bits, these results rely on assumptions that may not hold in practice. We introduce a new parameterization based on the number $C = \frac{\beta}{\varepsilon^2}$ of pairwise collisions, i.e., instances where the same element appears on multiple servers, and design a protocol that uses only $\mathcal{O}\left(\alpha\log n+\frac{\sqrt{\beta}}{\varepsilon^2} \log n\right)$ bits, breaking previous lower bounds when $C$ is small. We further improve our algorithm under assumptions on the number of distinct elements or collisions and provide matching lower bounds in all regimes, establishing $C$ as a tight complexity measure for the problem. Finally, we consider streaming algorithms for distinct element estimation parameterized by the number of items with frequency larger than $1$. Overall, our results offer insight into why statistical problems with known hardness results can be efficiently solved in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22608v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Daniel M. Kane, Jasper C. H. Lee, Thanasis Pittas, David P. Woodruff, Samson Zhou</dc:creator>
    </item>
    <item>
      <title>On Finding $\ell$-th Smallest Perfect Matchings</title>
      <link>https://arxiv.org/abs/2506.22619</link>
      <description>arXiv:2506.22619v1 Announce Type: new 
Abstract: Given an undirected weighted graph $G$ and an integer $k$, Exact-Weight Perfect Matching (EWPM) is the problem of finding a perfect matching of weight exactly $k$ in $G$. In this paper, we study EWPM and its variants. The EWPM problem is famous, since in the case of unary encoded weights, Mulmuley, Vazirani, and Vazirani showed almost 40 years ago that the problem can be solved in randomized polynomial time. However, up to this date no derandomization is known.
  Our first result is a simple deterministic algorithm for EWPM that runs in time $n^{O(\ell)}$, where $\ell$ is the number of distinct weights that perfect matchings in $G$ can take. In fact, we show how to find an $\ell$-th smallest perfect matching in any weighted graph (even if the weights are encoded in binary, in which case EWPM in general is known to be NP-complete) in time $n^{O(\ell)}$ for any integer $\ell$. Similar next-to-optimal variants have also been studied recently for the shortest path problem.
  For our second result, we extend the list of problems that are known to be equivalent to EWPM. We show that EWPM is equivalent under a weight-preserving reduction to the Exact Cycle Sum problem (ECS) in undirected graphs with a conservative (i.e. no negative cycles) weight function. To the best of our knowledge, we are the first to study this problem. As a consequence, the latter problem is contained in RP if the weights are encoded in unary. Finally, we identify a special case of EWPM, called BCPM, which was recently studied by El Maalouly, Steiner and Wulf. We show that BCPM is equivalent under a weight-preserving transformation to another problem recently studied by Schlotter and Seb\H{o} as well as Geelen and Kapadia: the Shortest Odd Cycle problem (SOC) in undirected graphs with conservative weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22619v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas El Maalouly, Sebastian Haslebacher, Adrian Taubner, Lasse Wulf</dc:creator>
    </item>
    <item>
      <title>Counting distinct (non-)crossing substrings</title>
      <link>https://arxiv.org/abs/2506.22728</link>
      <description>arXiv:2506.22728v1 Announce Type: new 
Abstract: Let $w$ be a string of length $n$. The problem of counting factors crossing a position - Problem 64 from the textbook ``125 Problems in Text Algorithms'' [Crochemore, Leqroc, and Rytter, 2021], asks to count the number $\mathcal{C}(w,k)$ (resp. $\mathcal{N}(w,k)$) of distinct substrings in $w$ that have occurrences containing (resp. not containing) a position $k$ in $w$. The solutions provided in their textbook compute $\mathcal{C}(w,k)$ and $\mathcal{N}(w,k)$ in $O(n)$ time for a single position $k$ in $w$, and thus a direct application would require $O(n^2)$ time for all positions $k = 1, \ldots, n$ in $w$. Their solution is designed for constant-size alphabets. In this paper, we present new algorithms which compute $\mathcal{C}(w,k)$ in $O(n)$ total time for general ordered alphabets, and $\mathcal{N}(w,k)$ in $O(n)$ total time for linearly sortable alphabets, for all positions $k = 1, \ldots, n$ in $w$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22728v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haruki Umezaki, Hiroki Shibata, Dominik K\"oppl, Yuto Nakashima, Shunsuke Inenaga, Hideo Bannai</dc:creator>
    </item>
    <item>
      <title>Tight Additive Sensitivity on LZ-style Compressors and String Attractors</title>
      <link>https://arxiv.org/abs/2506.22778</link>
      <description>arXiv:2506.22778v1 Announce Type: new 
Abstract: The worst-case additive sensitivity of a string repetitiveness measure $c$ is defined to be the largest difference between $c(w)$ and $c(w')$, where $w$ is a string of length $n$ and $w'$ is a string that can be obtained by performing a single-character edit operation on $w$. We present $O(\sqrt{n})$ upper bounds for the worst-case additive sensitivity of the smallest string attractor size $\gamma$ and the smallest bidirectional scheme size $b$, which match the known lower bounds $\Omega(\sqrt{n})$ for $\gamma$ and $b$ [Akagi et al. 2023]. Further, we present matching upper and lower bounds for the worst-case additive sensitivity of the Lempel-Ziv family - $\Theta(n^{\frac{2}{3}})$ for LZSS and LZ-End, and $\Theta(n)$ for LZ78.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22778v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuto Fujie, Hiroki Shibata, Yuto Nakashima, Shunsuke Inenaga</dc:creator>
    </item>
    <item>
      <title>Global Predecessor Indexing: Avoiding Binary Search in Weighted Job Scheduling</title>
      <link>https://arxiv.org/abs/2506.22922</link>
      <description>arXiv:2506.22922v1 Announce Type: new 
Abstract: We present an improved solution to the Weighted Job Scheduling (WJS) problem. While the classical dynamic programming (DP) solution runs in $O(n \log(n))$ time due to comparison-based sorting and per-job binary search, we eliminate the binary search bottleneck. In its place, we introduce a novel multi-phase preprocessing technique called Global Predecessor Indexing (GPI), which computes the latest non-overlapping job (i.e., the predecessor) for all jobs via a two-pointer linear-time pass. GPI enables direct use in the classical DP recurrence. When combined with linear-time sorting, GPI yields a complete $O(n)$ solution. Even with comparison-based sorting, GPI significantly outperforms the classical solution in practice by avoiding repeated binary searches. Keywords: Weighted Job Scheduling, Interval Scheduling, Dynamic Programming, Linear Sorting, Two Pointers, Preprocessing</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22922v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amit Joshi</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Vertex Fault-Tolerant Labels for Steiner Connectivity</title>
      <link>https://arxiv.org/abs/2506.23215</link>
      <description>arXiv:2506.23215v1 Announce Type: new 
Abstract: We present a compact labeling scheme for determining whether a designated set of terminals in a graph remains connected after any $f$ (or less) vertex failures occur. An $f$-FT Steiner connectivity labeling scheme for an $n$-vertex graph $G=(V,E)$ with terminal set $U \subseteq V$ provides labels to the vertices of $G$, such that given only the labels of any subset $F \subseteq V$ with $|F| \leq f$, one can determine if $U$ remains connected in $G-F$. The main complexity measure is the maximum label length.
  The special case $U=V$ of global connectivity has been recently studied by Jiang, Parter, and Petruschka, who provided labels of $n^{1-1/f} \cdot \mathrm{poly}(f,\log n)$ bits. This is near-optimal (up to $\mathrm{poly}(f,\log n)$ factors) by a lower bound of Long, Pettie and Saranurak. Our scheme achieves labels of $|U|^{1-1/f} \cdot \mathrm{poly}(f, \log n)$ for general $U \subseteq V$, which is near-optimal for any given size $|U|$ of the terminal set. To handle terminal sets, our approach differs from Jiang et al. We use a well-structured Steiner tree for $U$ produced by a decomposition theorem of Duan and Pettie, and bypass the need for Nagamochi-Ibaraki sparsification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23215v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Koustav Bhanja, Asaf Petruschka</dc:creator>
    </item>
    <item>
      <title>Parameterized Critical Node Cut Revisited</title>
      <link>https://arxiv.org/abs/2506.23363</link>
      <description>arXiv:2506.23363v1 Announce Type: new 
Abstract: Given a graph $G$ and integers $k, x \geq 0$, the Critical Node Cut problem asks whether it is possible to delete at most $k$ vertices from $G$ such that the number of remaining pairs of connected vertices is at most $x$. This problem generalizes Vertex Cover (when $x = 0$), and has applications in network design, epidemiology, and social network analysis. We investigate the parameterized complexity of Critical Node Cut under various structural parameters. We first significantly strengthen existing hardness results by proving W[1]-hardness even when parameterized by the combined parameter $k + \mathrm{fes} + \Delta + \mathrm{pw}$, where $\mathrm{fes}$ is the feedback edge set number, $\Delta$ the maximum degree, and $\mathrm{pw}$ the pathwidth of the input graph. We then identify three structural parameters--max-leaf number, vertex integrity, and modular-width--that render the problem fixed-parameter tractable. Furthermore, leveraging a technique introduced by Lampis [ICALP '14], we develop an FPT approximation scheme that, for any $\varepsilon &gt; 0$, computes a $(1+\varepsilon)$-approximate solution in time $(\mathrm{tw} / \varepsilon)^{\mathcal{O}(\mathrm{tw})} n^{\mathcal{O}(1)}$, where $\mathrm{tw}$ denotes the treewidth of the input graph. Finally, we show that Critical Node Cut does not admit a polynomial kernel when parameterized by vertex cover number, unless standard complexity assumptions fail. Overall, our results significantly sharpen the known complexity landscape of Critical Node Cut.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23363v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Du\v{s}an Knop, Nikolaos Melissinos, Manolis Vasilakis</dc:creator>
    </item>
    <item>
      <title>Planar Multiway Cut with Terminals on Few Faces</title>
      <link>https://arxiv.org/abs/2506.23399</link>
      <description>arXiv:2506.23399v1 Announce Type: new 
Abstract: We consider the \textsc{Edge Multiway Cut} problem on planar graphs. It is known that this can be solved in $n^{O(\sqrt{t})}$ time [Klein, Marx, ICALP 2012] and not in $n^{o(\sqrt{t})}$ time under the Exponential Time Hypothesis [Marx, ICALP 2012], where $t$ is the number of terminals. A stronger parameter is the number $k$ of faces of the planar graph that jointly cover all terminals. For the related {\sc Steiner Tree} problem, an $n^{O(\sqrt{k})}$ time algorithm was recently shown [Kisfaludi-Bak et al., SODA 2019]. By a completely different approach, we prove in this paper that \textsc{Edge Multiway Cut} can be solved in $n^{O(\sqrt{k})}$ time as well.
  Our approach employs several major concepts on planar graphs, including homotopy and sphere-cut decomposition. We also mix a global treewidth dynamic program with a Dreyfus-Wagner style dynamic program to locally deal with large numbers of terminals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23399v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sukanya Pandey, Erik Jan van Leeuwen</dc:creator>
    </item>
    <item>
      <title>Efficient Resource Allocation under Adversary Attacks: A Decomposition-Based Approach</title>
      <link>https://arxiv.org/abs/2506.23442</link>
      <description>arXiv:2506.23442v1 Announce Type: new 
Abstract: We address the problem of allocating limited resources in a network under persistent yet statistically unknown adversarial attacks. Each node in the network may be degraded, but not fully disabled, depending on its available defensive resources. The objective is twofold: to minimize total system damage and to reduce cumulative resource allocation and transfer costs over time. We model this challenge as a bi-objective optimization problem and propose a decomposition-based solution that integrates chance-constrained programming with network flow optimization. The framework separates the problem into two interrelated subproblems: determining optimal node-level allocations across time slots, and computing efficient inter-node resource transfers. We theoretically prove the convergence of our method to the optimal solution that would be obtained with full statistical knowledge of the adversary. Extensive simulations demonstrate that our method efficiently learns the adversarial patterns and achieves substantial gains in minimizing both damage and operational costs, comparing three benchmark strategies under various parameter settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23442v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mansoor Davoodi, Setareh Maghsudi</dc:creator>
    </item>
    <item>
      <title>Towards practical FPRAS for #NFA: Exploiting the Power of Dependence</title>
      <link>https://arxiv.org/abs/2506.23561</link>
      <description>arXiv:2506.23561v1 Announce Type: new 
Abstract: #NFA refers to the problem of counting the words of length $n$ accepted by a non-deterministic finite automaton. #NFA is #P-hard, and although fully-polynomial-time randomized approximation schemes (FPRAS) exist, they are all impractical. The first FPRAS for #NFA had a running time of $\tilde{O}(n^{17}m^{17}\varepsilon^{-14}\log(\delta^{-1}))$, where $m$ is the number of states in the automaton, $\delta \in (0,1]$ is the confidence parameter, and $\varepsilon &gt; 0$ is the tolerance parameter (typically smaller than $1$). The current best FPRAS achieved a significant improvement in the time complexity relative to the first FPRAS and obtained FPRAS with time complexity $\tilde{O}((n^{10}m^2 + n^6m^3)\varepsilon^{-4}\log^2(\delta^{-1}))$. The complexity of the improved FPRAS is still too intimidating to attempt any practical implementation.
  In this paper, we pursue the quest for practical FPRAS for #NFA by presenting a new algorithm with a time complexity of $O(n^2m^3\log(nm)\varepsilon^{-2}\log(\delta^{-1}))$. Observe that evaluating whether a word of length $n$ is accepted by an NFA has a time complexity of $O(nm^2)$. Therefore, our proposed FPRAS achieves sub-quadratic complexity with respect to membership checks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23561v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kuldeep S. Meel, Alexis de Colnet</dc:creator>
    </item>
    <item>
      <title>Simple Approximations for General Spanner Problems</title>
      <link>https://arxiv.org/abs/2506.23638</link>
      <description>arXiv:2506.23638v1 Announce Type: new 
Abstract: Consider a graph with n nodes and m edges, independent edge weights and lengths, and arbitrary distance demands for node pairs. The spanner problem asks for a minimum-weight subgraph that satisfies these demands via sufficiently short paths w.r.t. the edge lengths. For multiplicative alpha-spanners (where demands equal alpha times the original distances) and assuming that each edge's weight equals its length, the simple Greedy heuristic by Alth\"ofer et al. (1993) is known to yield strong solutions, both in theory and practice. To obtain guarantees in more general settings, recent approximations typically abandon this simplicity and practicality. Still, so far, there is no known non-trivial approximation algorithm for the spanner problem in its most general form. We provide two surprisingly simple approximations algorithms. In general, our Adapted Greedy achieves the first unconditional approximation ratio of m, which is non-trivial due to the independence of weights and lengths. Crucially, it maintains all size and weight guarantees Greedy is known for, i.e., in the aforementioned multiplicative alpha-spanner scenario and even for additive +beta-spanners. Further, it generalizes some of these size guarantees to derive new weight guarantees. Our second approach, Randomized Rounding, establishes a graph transformation that allows a simple rounding scheme over a standard multicommodity flow LP. It yields an O(n log n)-approximation, assuming integer lengths and polynomially bounded distance demands. The only other known approximation guarantee in this general setting requires several complex subalgorithms and analyses, yet we match it up to a factor of O(n^{1/5-eps}) using standard tools. Further, on bounded-degree graphs, we yield the first O(log n) approximation ratio for constant-bounded distance demands (beyond multiplicative 2-spanners in unit-length graphs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23638v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fritz B\"okler, Markus Chimani, Henning Jasper</dc:creator>
    </item>
    <item>
      <title>Segmented Operations using Matrix Multiplications</title>
      <link>https://arxiv.org/abs/2506.23906</link>
      <description>arXiv:2506.23906v1 Announce Type: new 
Abstract: Specialized computational units that perform small matrix multiplications as primitive operations are typically present in modern accelerators. However, these units are often underutilized for many fundamental operations besides dense matrix multiplications. The analysis of algorithms for such architectures is currently stagnated due to the lack of a rigorous theoretical model of computation that captures their characteristics. In this work, we propose MMV-RAM, a computational model tailored to matrix multiplication accelerators. MMV-RAM judiciously extends the Vector-RAM model with an additional processing unit that multiplies two matrices of sizes $n\times s$ and $s\times s$ in a single parallel step, where $s$ is a model parameter. We provide a detailed theoretical analysis of the model, and carefully balance the computational power between the matrix and vector units, guided by the circuit complexity lower bound that parity is not in AC[0].
  In MMV-RAM, we study algorithms for segmented scan and sum, two fundamental parallel primitives. We propose a segmented scan algorithm that uses matrix multiplications to perform speculative block-scan computations, which runs in $O(\log_s(n))$ steps. In contrast, we show that any algorithm that uses only the vector unit of MMV-RAM requires $\Omega\left(\frac{\log_2(n)}{\log_2\log_2(n)}\right)$ steps. We further apply these techniques to obtain similar theoretical speedups for element-wise vector multiplication and matrix multiplication. Beyond the worst-case complexity analysis, we propose algorithms for segmented operations that could lead to highly efficient and pragmatic implementations. For example, we observe that segmented sum is a combination of three elementary parallel primitives: scan, compress, and vector differentiation. As a case study, we implement...</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23906v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleksandros Sobczyk, Giuseppe Sorrentino, Anastasios Zouzias</dc:creator>
    </item>
    <item>
      <title>Fantastic Flips and Where to Find Them: A General Framework for Parameterized Local Search on Partitioning Problem</title>
      <link>https://arxiv.org/abs/2506.24001</link>
      <description>arXiv:2506.24001v1 Announce Type: new 
Abstract: Parameterized local search combines classic local search heuristics with the paradigm of parameterized algorithmics. While most local search algorithms aim to improve given solutions by performing one single operation on a given solution, the parameterized approach aims to improve a solution by performing $k$ simultaneous operations. Herein, $k$ is a parameter called search radius for which the value can be chosen by a user. One major goal in the field of parameterized local search is to outline the trade-off between the size of $k$ and the running time of the local search step. In this work, we introduce an abstract framework that generalizes natural parameterized local search approaches for a large class of partitioning problems: Given $n$ items that are partitioned into $b$ bins and a target function that evaluates the quality of the current partition, one asks whether it is possible to improve the solution by removing up to $k$ items from their current bins and reassigning them to other bins. Among others, our framework applies for the local search versions of problems like Cluster Editing, Vector Bin Packing, and Nash Social Welfare. Motivated by a real-world application of the problem Vector Bin Packing, we introduce a parameter called number of types $\tau \le n$ and show that all problems fitting in our framework can be solved in $\tau^k 2^{O(k)} |I|^{O(1)}$ time, where $|I|$ denotes the total input size. In case of Cluster Editing, the parameter $\tau$ generalizes the well-known parameter neighborhood diversity of the input graph. We complement this by showing that for all considered problems, an algorithm significantly improving over our algorithm with running time $\tau^k 2^{O(k)} |I|^{O(1)}$ would contradict the ETH. Additionally, we show that even on very restricted instances, all considered problems are W[1]-hard when parameterized by the search radius $k$ alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24001v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niels Gr\"uttemeier, Nils Morawietz, Frank Sommer</dc:creator>
    </item>
    <item>
      <title>Dominating Set Knapsack: Profit Optimization on Dominating Sets</title>
      <link>https://arxiv.org/abs/2506.24032</link>
      <description>arXiv:2506.24032v1 Announce Type: new 
Abstract: In a large-scale network, we want to choose some influential nodes to make a profit by paying some cost within a limited budget so that we do not have to spend more budget on some nodes adjacent to the chosen nodes; our problem is the graph-theoretic representation of it. We define our problem Dominating Set Knapsack by attaching Knapsack Problem with Dominating Set on graphs. Each vertex is associated with a cost factor and a profit amount. We aim to choose some vertices within a fixed budget that gives maximum profit so that we do not need to choose their 1-hop neighbors. We show that the Dominating Set Knapsack problem is strongly NP-complete even when restricted to Bipartite graphs but weakly NP-complete for Star graphs. We present a pseudo-polynomial time algorithm for Trees in time $O(n\cdot min\{s^2, (\alpha(V))^2\})$. We show that Dominating Set Knapsack is very unlikely to be Fixed Parameter Tractable(FPT) by proving that it is in W[2]-hard parameterized by the solution size. We developed FPT algorithms with running time $O(4^{tw}\cdot n^{O(1)} \cdot min\{s^2,{\alpha(V)}^2\})$ and $O(2^{vck-1}\cdot n^{O(1)} \cdot min\{s^2,{\alpha(V)}^2\})$, where $tw$ represents the treewidth of the given graph, $vck$ is the solution size of the Vertex Cover Knapsack, $s$ is the size of the knapsack and $\alpha(V)=\sum_{v\in V}\alpha(v)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24032v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sipra Singh</dc:creator>
    </item>
    <item>
      <title>Translating between the representations of an acyclic convex geometry of bounded degree</title>
      <link>https://arxiv.org/abs/2506.24052</link>
      <description>arXiv:2506.24052v1 Announce Type: new 
Abstract: We consider the problem of enumerating the irreducible closed sets of a closure system given by an implicational base. In the context of Horn logic, these correspond to Horn expressions and characteristic models, respectively. To date, the complexity status of this problem is widely open, and it is further known to generalize the notorious hypergraph dualization problem, even in the context of acyclic convex geometries, i.e., closure systems admitting an acyclic implicational base. This paper studies this later class with a focus on the degree, which corresponds to the maximal number of implications in which an element occurs. We show that the problem is tractable for bounded values of this parameter, even when relaxed to the notions of premise- and conclusion-degree. Our algorithms rely on structural properties of acyclic convex geometries and involve various techniques from algorithmic enumeration such as solution graph traversal, saturation techniques, and a sequential approach leveraging from acyclicity. They are shown to perform in incremental-polynomial time for the computation of irreducible closed sets, and in polynomial time for the construction of an implicational base. Finally, we argue that our running times cannot be improved to polynomial delay using the standard framework of flashlight search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24052v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Defrain, Arthur Ohana, Simon Vilmin</dc:creator>
    </item>
    <item>
      <title>A Refined Kernel for $d$-Hitting Set</title>
      <link>https://arxiv.org/abs/2506.24114</link>
      <description>arXiv:2506.24114v1 Announce Type: new 
Abstract: The $d$-Hitting Set problem is a fundamental problem in parameterized complexity, which asks whether a given hypergraph contains a vertex subset $S$ of size at most $k$ that intersects every hyperedge (i.e., $S \cap e \neq \emptyset$ for each hyperedge $e$). The best known kernel for this problem, established by Abu-Khzam [1], has $(2d - 1)k^{d - 1} + k$ vertices. This result has been very widely used in the literature as many problems can be modeled as a special $d$-Hitting Set problem. In this work, we present a refinement to this result by employing linear programming techniques to construct crown decompositions in hypergraphs. This approach yields a slight but notable improvement, reducing the size to $(2d - 2)k^{d - 1} + k$ vertices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24114v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxi Liu, Mingyu Xiao</dc:creator>
    </item>
    <item>
      <title>Inventory Control Using a L\'evy Process for Evaluating Total Costs under Intermittent Demand</title>
      <link>https://arxiv.org/abs/2506.22524</link>
      <description>arXiv:2506.22524v1 Announce Type: cross 
Abstract: Products with intermittent demand are characterized by a high risk of sales losses and obsolescence due to the sporadic occurrence of demand events. Generally, both point forecasting and probabilistic forecasting approaches are applied to intermittent demand. In particular, probabilistic forecasting, which models demand as a stochastic process, is capable of capturing uncertainty. An example of such modeling is the use of L\'evy processes, which possess independent increments and accommodate discontinuous changes (jumps). However, to the best of our knowledge, in inventory control using L\'evy processes, no studies have investigated how the order quantity and reorder point affect the total cost. One major difficulty has been the mathematical formulation of inventory replenishment triggered at reorder points. To address this challenge, the present study formulates a reorder-point policy by modeling cumulative demand as a drifted Poisson process and introducing a stopping time to represent the timing at which the reorder point is reached. Furthermore, the validity of the proposed method is verified by comparing the total cost with that obtained from a case where an ARIMA model is combined with a reorder-point policy. As a main result, while the total cost under ARIMA-based forecasting increases linearly over time, the L\'evy process-based formulation provides an analytical expression for the total cost, revealing that random demand fluctuations cause the expected total cost to grow at a rate faster than linear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22524v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryoya Koide, Yurika Ono, Aya Ishigaki</dc:creator>
    </item>
    <item>
      <title>A Rigorous Error Bound for the TG Kernel in Prime Counting</title>
      <link>https://arxiv.org/abs/2506.22634</link>
      <description>arXiv:2506.22634v1 Announce Type: cross 
Abstract: We establish rigorous error bounds for prime counting using a truncated Gaussian (TG) kernel in the explicit formula framework. Our main theorem proves that the approximation error remains globally below 1/2 for all sufficiently large arguments, guaranteeing exact computation of {\pi}(x) through simple rounding, without relying on unproven hypotheses.
  The TG kernel construction employs Gaussian-like test functions with compact support, engineered with vanishing moments to eliminate main terms. For x with 10^8 decimal digits, we demonstrate that only ~1200 nontrivial zeta zeros suffice to achieve the error bound, enabling computation in seconds on modern hardware - a dramatic improvement over classical methods.
  Key contributions include: (1) Explicit tail truncation bounds using Taylor remainder analysis, showing exponential decay; (2) Zero-sum truncation error bounds via unconditional density estimates; (3) Rigorous treatment of trivial zero contributions. All constants are made explicit, ensuring full verifiability.
  The method bridges analytic number theory and practical computation, with potential applications to record-breaking prime counting computations. We discuss algorithmic implications including FFT-based arithmetic for ~330 million bit numbers. The framework's flexibility suggests connections to deeper structures in prime distribution, particularly regarding optimized kernel designs and the interplay between smoothing parameters {\alpha} and truncation heights.
  This work exemplifies how classical analytic techniques, when carefully implemented with modern computational perspectives, yield practical algorithms for problems previously considered purely theoretical. The rigorous error analysis ensures reliability even at astronomical scales, opening new avenues for computational number theory research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22634v1</guid>
      <category>math.NT</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bugra Kilictas, Faruk Alpay</dc:creator>
    </item>
    <item>
      <title>Lower bounds for trace estimation via Block Krylov and other methods</title>
      <link>https://arxiv.org/abs/2506.22701</link>
      <description>arXiv:2506.22701v1 Announce Type: cross 
Abstract: This paper studies theoretical lower bounds for estimating the trace of a matrix function, $\text{tr}(f(A))$, focusing on methods that use Hutchinson's method along with Block Krylov techniques. These methods work by approximating matrix-vector products like $f(A)V$ using a Block Krylov subspace. This is closely related to approximating functions with polynomials. We derive theoretical upper bounds on how many Krylov steps are needed for functions such as $A^{-1/2}$ and $A^{-1}$ by analyzing the upper bounds from the polynomial approximation of their scalar equivalent. In addition, we also develop lower limits on the number of queries needed for trace estimation, specifically for $\text{tr}(W^{-p})$ where $W$ is a Wishart matrix. Our study clarifies the connection between the number of steps in Block Krylov methods and the degree of the polynomial used for approximation. This links the total cost of trace estimation to basic limits in polynomial approximation and how much information is needed for the computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22701v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shi Jie Yu</dc:creator>
    </item>
    <item>
      <title>Shifted Composition IV: Underdamped Langevin and Numerical Discretizations with Partial Acceleration</title>
      <link>https://arxiv.org/abs/2506.23062</link>
      <description>arXiv:2506.23062v1 Announce Type: cross 
Abstract: Quantifying the convergence rate of the underdamped Langevin dynamics (ULD) is a classical topic, in large part due to the possibility for diffusive-to-ballistic speedups -- as was recently established for the continuous-time dynamics via space-time Poincare inequalities. A central challenge for analyzing ULD is that its degeneracy necessitates the development of new analysis approaches, e.g., the theory of hypocoercivity. In this paper, we give a new coupling-based framework for analyzing ULD and its numerical discretizations. First, in the continuous-time setting, we use this framework to establish new parabolic Harnack inequalities for ULD. These are the first Harnack inequalities that decay to zero in contractive settings, thereby reflecting the convergence properties of ULD in addition to just its regularity properties.
  Second, we build upon these Harnack inequalities to develop a local error framework for analyzing discretizations of ULD in KL divergence. This extends our framework in part III from uniformly elliptic diffusions to degenerate diffusions, and shares its virtues: the framework is user-friendly, applies to sophisticated discretization schemes, and does not require contractivity. Applying this framework to the randomized midpoint discretization of ULD establishes (i) the first ballistic acceleration result for log-concave sampling (i.e., sublinear dependence on the condition number), and (ii) the first $d^{1/3}$ iteration complexity guarantee for sampling to constant total variation error in dimension $d$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23062v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason M. Altschuler, Sinho Chewi, Matthew S. Zhang</dc:creator>
    </item>
    <item>
      <title>Moving Matter: Using a Single, Simple Robot to Reconfigure a Connected Set of Building Blocks</title>
      <link>https://arxiv.org/abs/2506.23333</link>
      <description>arXiv:2506.23333v1 Announce Type: cross 
Abstract: We implement and evaluate different methods for the reconfiguration of a connected arrangement of tiles into a desired target shape, using a single active robot that can move along the tile structure. This robot can pick up, carry, or drop off one tile at a time, but it must maintain a single connected configuration at all times.
  Becker et al. (CCCG 2025) recently proposed an algorithm that uses histograms as canonical intermediate configurations, guaranteeing performance within a constant factor of the optimal solution if the start and target configuration are well-separated. We implement and evaluate this algorithm, both in a simulated and practical setting, using an inchworm type robot to compare it with two existing heuristic algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23333v1</guid>
      <category>cs.RO</category>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javier Garcia, Jonas Friemel, Ramin Kosfeld, Michael Yannuzzi, Peter Kramer, Christian Rieck, Christian Scheffer, Arne Schmidt, Harm Kube, Dan Biediger, S\'andor P. Fekete, Aaron T. Becker</dc:creator>
    </item>
    <item>
      <title>Sampling and Identity-Testing Without Approximate Tensorization of Entropy</title>
      <link>https://arxiv.org/abs/2506.23456</link>
      <description>arXiv:2506.23456v1 Announce Type: cross 
Abstract: Certain tasks in high-dimensional statistics become easier when the underlying distribution satisfies a local-to-global property called approximate tensorization of entropy (ATE). For example, the Glauber dynamics Markov chain of an ATE distribution mixes fast and can produce approximate samples in a small amount of time, since such a distribution satisfies a modified log-Sobolev inequality. Moreover, identity-testing for an ATE distribution requires few samples if the tester is given coordinate conditional access to the unknown distribution, as shown by Blanca, Chen, \v{S}tefankovi\v{c}, and Vigoda (COLT 2023).
  A natural class of distributions that do not satisfy ATE consists of mixtures of (few) distributions that do satisfy ATE. We study the complexity of identity-testing and sampling for these distributions. Our main results are the following:
  1. We show fast mixing of Glauber dynamics from a data-based initialization, with optimal sample complexity, for mixtures of distributions satisfying modified log-Sobolev inequalities. This extends work of Huang, Koehler, Lee, Mohanty, Rajaraman, Vuong, and Wu (STOC 2025, COLT 2025) for mixtures of distributions satisfying Poincar\'e inequalities.
  2. Answering an open question posed by Blanca et al., we give efficient identity-testers for mixtures of ATE distributions in the coordinate-conditional sampling access model. We also give some simplifications and improvements to the original algorithm of Blanca et al.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23456v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Gay, William He, Nicholas Kocurek, Ryan O'Donnell</dc:creator>
    </item>
    <item>
      <title>Optimized methods for composite optimization: a reduction perspective</title>
      <link>https://arxiv.org/abs/2506.23756</link>
      <description>arXiv:2506.23756v1 Announce Type: cross 
Abstract: Recent advances in convex optimization have leveraged computer-assisted proofs to develop optimized first-order methods that improve over classical algorithms. However, each optimized method is specially tailored for a particular problem setting, and it is a well-documented challenge to extend optimized methods to other settings due to their highly bespoke design and analysis. We provide a general framework that derives optimized methods for composite optimization directly from those for unconstrained smooth optimization. The derived methods naturally extend the original methods, generalizing how proximal gradient descent extends gradient descent. The key to our result is certain algebraic identities that provide a unified and straightforward way of extending convergence analyses from unconstrained to composite settings. As concrete examples, we apply our framework to establish (1) the phenomenon of stepsize acceleration for proximal gradient descent; (2) a convergence rate for the proximal optimized gradient method which is faster than FISTA; (3) a new method that improves the state-of-the-art rate for minimizing gradient norm in the composite setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23756v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinho Bok, Jason M. Altschuler</dc:creator>
    </item>
    <item>
      <title>A Graph Width Perspective on Partially Ordered Hamiltonian Paths and Cycles I: Treewidth, Pathwidth, and Grid Graphs</title>
      <link>https://arxiv.org/abs/2506.23790</link>
      <description>arXiv:2506.23790v1 Announce Type: cross 
Abstract: We consider the problem of finding a Hamiltonian path or a Hamiltonian cycle with precedence constraints in the form of a partial order on the vertex set. We show that the path problem is $\mathsf{NP}$-complete for graphs of pathwidth 4 while the cycle problem is $\mathsf{NP}$-complete on graphs of pathwidth 5. We complement these results by giving polynomial-time algorithms for graphs of pathwidth 3 and treewidth 2 for Hamiltonian paths as well as pathwidth 4 and treewidth 3 for Hamiltonian cycles. Furthermore, we study the complexity of the path and cycle problems on rectangular grid graphs of bounded height. For these, we show that the path and cycle problems are $\mathsf{NP}$-complete when the height of the grid is greater or equal to 7 and 9, respectively. In the variant where we look for minimum edge-weighted Hamiltonian paths and cycles, the problems are $\mathsf{NP}$-hard for heights 5 and 6, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23790v1</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jesse Beisegel, Katharina Klost, Kristin Knorr, Fabienne Ratajczak, Robert Scheffler</dc:creator>
    </item>
    <item>
      <title>Randomized Communication and Implicit Graph Representations</title>
      <link>https://arxiv.org/abs/2111.03639</link>
      <description>arXiv:2111.03639v5 Announce Type: replace 
Abstract: We initiate the focused study of constant-cost randomized communication, with emphasis on its connection to graph representations. We observe that constant-cost randomized communication problems are equivalent to hereditary (i.e. closed under taking induced subgraphs) graph classes which admit constant-size adjacency sketches and probabilistic universal graphs (PUGs), which are randomized versions of the well-studied adjacency labeling schemes and induced-universal graphs. This gives a new perspective on long-standing questions about the existence of these objects, including new methods of constructing adjacency labeling schemes.
  We ask three main questions about constant-cost communication, or equivalently, constant-size PUGs: (1) Are there any natural, non-trivial problems aside from Equality and k-Hamming Distance which have constant-cost communication? We provide a number of new examples, including deciding whether two vertices have path-distance at most k in a planar graph, and showing that constant-size PUGs are preserved by the Cartesian product operation. (2) What structures of a problem explain the existence or non-existence of a constant-cost protocol? We show that in many cases a Greater-Than subproblem is such a structure. (3) Is the Equality problem complete for constant-cost randomized communication? We show that it is not: there are constant-cost problems which do not reduce to Equality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.03639v5</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Harms, Sebastian Wild, Viktor Zamaraev</dc:creator>
    </item>
    <item>
      <title>A $4/3$ Approximation for $2$-Vertex-Connectivity</title>
      <link>https://arxiv.org/abs/2305.02240</link>
      <description>arXiv:2305.02240v4 Announce Type: replace 
Abstract: The 2-Vertex-Connected Spanning Subgraph problem (2VCSS) is among the most basic NP-hard (Survivable) Network Design problems: we are given an (unweighted) undirected graph $G$. Our goal is to find a spanning subgraph $S$ of $G$ with the minimum number of edges which is $2$-vertex-connected, namely $S$ remains connected after the deletion of an arbitrary node. 2VCSS is well-studied in terms of approximation algorithms, and the current best (polynomial-time) approximation factor is $10/7$ by Heeger and Vygen [SIDMA'17] (improving on earlier results by Khuller and Vishkin [STOC'92] and Garg, Vempala and Singla [SODA'93]).
  Here we present an improved $4/3$ approximation. Our main technical ingredient is an approximation preserving reduction to a conveniently structured subset of instances which are ``almost'' 3-vertex-connected. The latter reduction might be helpful in future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.02240v4</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.25.13</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 4 (2025), Article 13, 1-44</arxiv:journal_reference>
      <dc:creator>Miguel Bosch-Calvo, Fabrizio Grandoni, Afrouz Jabal Ameli</dc:creator>
    </item>
    <item>
      <title>Value-Compressed Sparse Column (VCSC): Sparse Matrix Storage for Redundant Data</title>
      <link>https://arxiv.org/abs/2309.04355</link>
      <description>arXiv:2309.04355v2 Announce Type: replace 
Abstract: Compressed Sparse Column (CSC) and Coordinate (COO) are popular compression formats for sparse matrices. However, both CSC and COO are general purpose and cannot take advantage of any of the properties of the data other than sparsity, such as data redundancy. Highly redundant sparse data is common in many machine learning applications, such as genomics, and is often too large for in-core computation using conventional sparse storage formats. In this paper, we present two extensions to CSC: (1) Value-Compressed Sparse Column (VCSC) and (2) Index- and Value-Compressed Sparse Column (IVCSC). VCSC takes advantage of high redundancy within a column to further compress data up to 3-fold over COO and 2.25-fold over CSC, without significant negative impact to performance characteristics. IVCSC extends VCSC by compressing index arrays through delta encoding and byte-packing, achieving a 10-fold decrease in memory usage over COO and 7.5-fold decrease over CSC. Our benchmarks on simulated and real data show that VCSC and IVCSC can be read in compressed form with little added computational cost. These two novel compression formats offer a broadly useful solution to encoding and reading redundant sparse data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04355v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/BigData62323.2024.10825091</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE International Conference on Big Data (BigData), 2024, pp. 4952-4958</arxiv:journal_reference>
      <dc:creator>Skyler Ruiter, Seth Wolfgang, Marc Tunnell, Timothy Triche Jr., Erin Carrier, Zachary DeBruine</dc:creator>
    </item>
    <item>
      <title>Aligning Multiple Inhomogeneous Random Graphs: Fundamental Limits of Exact Recovery</title>
      <link>https://arxiv.org/abs/2405.12293</link>
      <description>arXiv:2405.12293v2 Announce Type: replace 
Abstract: This work studies fundamental limits for recovering the underlying correspondence among multiple correlated graphs. In the setting of inhomogeneous random graphs, we present and analyze a matching algorithm: first partially match the graphs pairwise and then combine the partial matchings by transitivity. Our analysis yields a sufficient condition on the problem parameters to exactly match all nodes across all the graphs. In the setting of homogeneous (Erd\H{o}s-R\'enyi) graphs, we show that this condition is also necessary, i.e. the algorithm works down to the information theoretic threshold. This reveals a scenario where exact matching between two graphs alone is impossible, but leveraging more than two graphs allows exact matching among all the graphs. Converse results are also given in the inhomogeneous setting and transitivity again plays a role. Along the way, we derive independent results about the k-core of inhomogeneous random graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12293v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taha Ameen, Bruce Hajek</dc:creator>
    </item>
    <item>
      <title>Cuckoo Heavy Keeper and the balancing act of maintaining heavy hitters in stream processing</title>
      <link>https://arxiv.org/abs/2412.12873</link>
      <description>arXiv:2412.12873v3 Announce Type: replace 
Abstract: Finding heavy hitters in databases and data streams is a fundamental problem with applications ranging from network monitoring to database query optimization, machine learning, and more. Approximation algorithms offer practical solutions, but they present trade-offs involving throughput, memory usage, and accuracy. Moreover, modern applications further complicate these trade-offs by demanding capabilities beyond sequential processing that require both parallel scaling and support for concurrent queries and updates.
  Analysis of these trade-offs led us to the key idea behind our proposed streaming algorithm, Cuckoo Heavy Keeper (CHK). The approach introduces an inverted process for distinguishing frequent from infrequent items, which unlocks new algorithmic synergies that were previously inaccessible with conventional approaches. By further analyzing the competing metrics with a focus on parallelism, we propose an algorithmic framework that balances scalability aspects and provides options to optimize query and insertion efficiency based on their relative frequencies. The framework is capable of parallelizing any heavy-hitter detection algorithm.
  Besides the algorithms' analysis, we present an extensive evaluation on both real-world and synthetic data across diverse distributions and query selectivity, representing the broad spectrum of application needs. Compared to state-of-the-art methods, CHK improves throughput by 1.7-5.7$\times$ and accuracy by up to four orders of magnitude even under low-skew data and tight memory constraints. These properties allow its parallel instances to achieve near-linear scale-up and low latency for heavy-hitter queries, even under a high query rate. We expect the versatility of CHK and its parallel instances to impact a broad spectrum of tools and applications in large-scale data analytics and stream processing systems</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12873v3</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vinh Quang Ngo, Marina Papatriantafilou</dc:creator>
    </item>
    <item>
      <title>The Parameterized Landscape of Labeled Graph Contractions</title>
      <link>https://arxiv.org/abs/2502.16096</link>
      <description>arXiv:2502.16096v2 Announce Type: replace 
Abstract: In this work, we study the problem of computing a maximum common contraction of two vertex-labeled graphs, i.e. how to make them identical by contracting as little edges as possible in the two graphs. We study the problem from a parameterized complexity point of view, using parameters such as the maximum degree, the degeneracy, the clique-width or treewidth of the input graphs as well as the number of allowed contractions. We put this complexity in perspective with that of the labeled contractibility problem, i.e determining whether a labeled graph is a contraction of another. Surprisingly, our results indicate very little difference between these problems in terms of parameterized complexity status. We only prove their status to differ when parameterizing by both the degeneracy and the number of allowed contractions, showing W[1]-hardness of the maximum common contraction problem in this case, whereas the contractibility problem is FPT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16096v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Manuel Lafond, Bertrand Marchand</dc:creator>
    </item>
    <item>
      <title>Solving Partial Dominating Set and Related Problems Using Twin-Width</title>
      <link>https://arxiv.org/abs/2504.18218</link>
      <description>arXiv:2504.18218v2 Announce Type: replace 
Abstract: Partial vertex cover and partial dominating set are two well-investigated optimization problems. While they are $\rm W[1]$-hard on general graphs, they have been shown to be fixed-parameter tractable on many sparse graph classes, including nowhere-dense classes. In this paper, we demonstrate that these problems are also fixed-parameter tractable with respect to the twin-width of a graph. Indeed, we establish a more general result: every graph property that can be expressed by a logical formula of the form $\phi\equiv\exists x_1\cdots \exists x_k \sum_{\alpha \in I} \#y\,\psi_\alpha(x_1,\ldots,x_k,y)\ge t$, where $\psi_\alpha$ is a quantifier-free formula for each $\alpha \in I$, $t$ is an arbitrary number, and $\#y$ is a counting quantifier, can be evaluated in time $f(d,k)n$, where $n$ is the number of vertices and $d$ is the width of a contraction sequence that is part of the input. In addition to the aforementioned problems, this includes also connected partial dominating set and independent partial dominating set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18218v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.LO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakub Balab\'an, Daniel Mock, Peter Rossmanith</dc:creator>
    </item>
    <item>
      <title>New Sorting Algorithm Wave Sort (W-Sort)</title>
      <link>https://arxiv.org/abs/2505.13552</link>
      <description>arXiv:2505.13552v2 Announce Type: replace 
Abstract: Modern comparison sorts like quicksort suffer from performance inconsistencies due to suboptimal pivot selection, leading to $O(N^2)$ worst-case complexity, while in-place merge sort variants face challenges with data movement overhead. We introduce Wave Sort, a novel in-place sorting algorithm that addresses these limitations through a dynamic pivot selection strategy. Wave Sort iteratively expands a sorted region and selects pivots from this growing sorted portion to partition adjacent unsorted data. This approach ensures robust pivot selection irrespective of dataset size, guarantees a logarithmic recursion stack depth, and enables efficient in-place sorting. Our analysis shows a worst-case comparison complexity bounded by $O(N(\log N)^2)$ with a small constant factor. Experimental results demonstrate that Wave Sort requires significantly fewer comparisons than quicksort on average (approximately 24% less) and performs close to the theoretical minimum, while also incorporating adaptive techniques for efficient handling of presorted sequences. Wave Sort offers a compelling alternative for applications demanding consistent, predictable, and in-place sorting performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13552v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jia Xu Wei</dc:creator>
    </item>
    <item>
      <title>Notes on the Linear Algebraic View of Regularity Lemmas</title>
      <link>https://arxiv.org/abs/2505.18740</link>
      <description>arXiv:2505.18740v2 Announce Type: replace 
Abstract: When regularity lemmas were first developed in the 1970s, they were described as results that promise a partition of any graph into a ``small'' number of parts, such that the graph looks ``similar'' to a random graph on its edge subsets going between parts. Regularity lemmas have been repeatedly refined and reinterpreted in the years since, and the modern perspective is that they can instead be seen as purely linear-algebraic results about sketching a large, complicated matrix with a smaller, simpler one. These matrix sketches then have a nice interpretation about partitions when applied to the adjacency matrix of a graph.
  In these notes we will develop regularity lemmas from scratch, under the linear-algebraic perspective, and then use the linear-algebraic versions to derive the familiar graph versions. We do not assume any prior knowledge of regularity lemmas, and we recap the relevant linear-algebraic definitions as we go, but some comfort with linear algebra will definitely be helpful to read these notes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18740v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Greg Bodwin, Tuong Le</dc:creator>
    </item>
    <item>
      <title>Courcelle's Theorem for Lipschitz Continuity</title>
      <link>https://arxiv.org/abs/2506.21118</link>
      <description>arXiv:2506.21118v2 Announce Type: replace 
Abstract: Lipschitz continuity of algorithms, introduced by Kumabe and Yoshida (FOCS'23), measures the stability of an algorithm against small input perturbations. Algorithms with small Lipschitz continuity are desirable, as they ensure reliable decision-making and reproducible scientific research. Several studies have proposed Lipschitz continuous algorithms for various combinatorial optimization problems, but these algorithms are problem-specific, requiring a separate design for each problem.
  To address this issue, we provide the first algorithmic meta-theorem in the field of Lipschitz continuous algorithms. Our result can be seen as a Lipschitz continuous analogue of Courcelle's theorem, which offers Lipschitz continuous algorithms for problems on bounded-treewidth graphs. Specifically, we consider the problem of finding a vertex set in a graph that maximizes or minimizes the total weight, subject to constraints expressed in monadic second-order logic (MSO_2). We show that for any $\varepsilon&gt;0$, there exists a $(1\pm \varepsilon)$-approximation algorithm for the problem with a polylogarithmic Lipschitz constant on bounded treewidth graphs. On such graphs, our result outperforms most existing Lipschitz continuous algorithms in terms of approximability and/or Lipschitz continuity. Further, we provide similar results for problems on bounded-clique-width graphs subject to constraints expressed in MSO_1. Additionally, we construct a Lipschitz continuous version of Baker's decomposition using our meta-theorem as a subroutine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21118v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuya Gima, Soh Kumabe, Yuichi Yoshida</dc:creator>
    </item>
    <item>
      <title>A Formal Analysis of Algorithms for Matroids and Greedoids</title>
      <link>https://arxiv.org/abs/2505.19816</link>
      <description>arXiv:2505.19816v2 Announce Type: replace-cross 
Abstract: We present a formal analysis, in Isabelle/HOL, of optimisation algorithms for matroids, which are useful generalisations of combinatorial structures that occur in optimisation, and greedoids, which are a generalisation of matroids. Although some formalisation work has been done earlier on matroids, our work here presents the first formalisation of results on greedoids, and many results we formalise in relation to matroids are also formalised for the first time in this work. We formalise the analysis of a number of optimisation algorithms for matroids and greedoids. We also derive from those algorithms executable implementations of Kruskal's algorithm for minimum spanning trees, an algorithm for maximum cardinality matching for bi-partite graphs, and Prim's algorithm for computing minimum weight spanning trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19816v2</guid>
      <category>cs.LO</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Abdulaziz, Thomas Ammer, Shriya Meenakshisundaram, Adem Rimpapa</dc:creator>
    </item>
    <item>
      <title>New aspects of quantum topological data analysis: Betti number estimation, and testing and tracking of homology and cohomology classes</title>
      <link>https://arxiv.org/abs/2506.01432</link>
      <description>arXiv:2506.01432v2 Announce Type: replace-cross 
Abstract: The application of quantum computation to topological data analysis (TDA) has received growing attention. While estimating Betti numbers is a central task in TDA, general complexity theoretic limitations restrict the possibility of quantum speedups. To address this, we explore quantum algorithms under a more structured input model. We show that access to additional topological information enables improved quantum algorithms for estimating Betti and persistent Betti numbers. Building on this, we introduce a new approach based on homology tracking, which avoids computing the kernel of combinatorial Laplacians used in prior methods. This yields a framework that remains efficient even when Betti numbers are small, offering substantial and sometimes exponential speedups. Beyond Betti number estimation, we formulate and study the homology property testing problem, and extend our approach to the cohomological setting. We present quantum algorithms for testing triviality and distinguishing homology classes, revealing new avenues for quantum advantage in TDA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01432v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <category>math.AT</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junseo Lee, Nhat A. Nghiem</dc:creator>
    </item>
  </channel>
</rss>
