<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.IT</link>
    <description>cs.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 01:20:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Convergent Primal-Dual Algorithm for Computing Rate-Distortion-Perception Functions</title>
      <link>https://arxiv.org/abs/2508.13486</link>
      <description>arXiv:2508.13486v1 Announce Type: new 
Abstract: Recent advances in Rate-Distortion-Perception (RDP) theory highlight the importance of balancing compression level, reconstruction quality, and perceptual fidelity. While previous work has explored numerical approaches to approximate the information RDP function, the lack of theoretical guarantees remains a major limitation, especially in the presence of complex perceptual constraints that introduce non-convexity and computational intractability. Inspired by our previous constrained Blahut-Arimoto algorithm for solving the rate-distortion function, in this paper, we present a new theoretical framework for computing the information RDP function by relaxing the constraint on the reconstruction distribution and replacing it with an alternative optimization approach over the reconstruction distribution itself. This reformulation significantly simplifies the optimization and enables a rigorous proof of convergence. Based on this formulation, we develop a novel primal-dual algorithm with provable convergence guarantees. Our analysis establishes, for the first time, a rigorous convergence rate of $O(1/n)$ for the computation of RDP functions. The proposed method not only bridges a key theoretical gap in the existing literature but also achieves competitive empirical performance in representative settings. These results lay the groundwork for more reliable and interpretable optimization in RDP-constrained compression systems. Experimental results demonstrate the efficiency and accuracy of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13486v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunhui Chen, Linyi Chen, Xueyan Niu, Hao Wu</dc:creator>
    </item>
    <item>
      <title>On optimal quantum LRCs from the Hermitian construction and $t$-designs</title>
      <link>https://arxiv.org/abs/2508.13553</link>
      <description>arXiv:2508.13553v1 Announce Type: new 
Abstract: In a recent work, quantum locally recoverable codes (qLRCs) have been introduced for their potential application in large-scale quantum data storage and implication for quantum LDPC codes. This work focuses on the bounds and constructions of qLRCs derived from the Hermitian construction, which solves an open problem proposed by Luo $et~al.$ (IEEE Trans. Inf. Theory, 71 (3): 1794-1802, 2025). We present four bounds for qLRCs and give comparisons in terms of their asymptotic formulas. We construct several new infinite families of NMDS codes, with general and flexible dimensions, that support t-designs for $t\in \{2,3\}$, and apply them to obtain Hermitian dual-containing classical LRCs (cLRCs). As a result, we derive three explicit families of optimal qLRCs. Compared to the known qLRCs obtained by the CSS construction, our optimal qLRCs offer new and more flexible parameters. It is also worth noting that the constructed cLRCs themselves are interesting as they are optimal with respect to four distinct bounds for cLRCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13553v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Li, Shitao Li, Huimin Lao, Gaojun Luo, San Ling</dc:creator>
    </item>
    <item>
      <title>Power and Rate Allocations for Positive-rate Covert Communications in Block-Fading Channels</title>
      <link>https://arxiv.org/abs/2508.13555</link>
      <description>arXiv:2508.13555v1 Announce Type: new 
Abstract: We aim to achieve keyless covert communication with a positive-rate in Rayleigh block-fading channels. Specifically, the transmitter and the legitimate receiver are assumed to have either causal or non-causal knowledge of the \ac{CSI} for both the legitimate and the warden channels, while the warden only knows the statistical distribution of the \ac{CSI}. Two problem formulations are considered in this work: (a) Power allocation: maximizing the sum covert rate subject to a maximum power constraint, and (b) Rate allocation: minimizing the power consumption subject to a minimum covert rate constraint. Both problems are formulated based on recent information theoretical results on covert communication over state-dependent channels. When the \ac{CSI} of each fading block is known non-causally, we propose a novel three-step method to solve both the power and rate allocation problems. In the case where the \ac{CSI} is known causally, the power allocation problem can be formulated as \ac{MDP} and be solved using a \ac{DDQN} approach. Although the rate allocation problem under causal \ac{CSI} does not directly conform to an \ac{MDP} structure, it can be approximately solved using the \ac{DDQN} trained for power allocation. Simulation results demonstrate the effectiveness of the proposed power and rate allocation methods and provide comprehensive performance comparisons across different allocation schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13555v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yubo Zhang, Hassan ZivariFard, Xiaodong Wang</dc:creator>
    </item>
    <item>
      <title>Repeater Swarm-Assisted Cellular Systems: Interaction Stability and Performance Analysis</title>
      <link>https://arxiv.org/abs/2508.13593</link>
      <description>arXiv:2508.13593v1 Announce Type: new 
Abstract: We consider a cellular massive MIMO system where swarms of wireless repeaters are deployed to improve coverage. These repeaters are full-duplex relays with small form factors that receive and instantaneously retransmit signals. They can be deployed in a plug-and-play manner at low cost, while being transparent to the network--conceptually they are active channel scatterers with amplification capabilities. Two fundamental questions need to be addressed in repeater deployments: (I) How can we prevent destructive effects of positive feedback caused by inter-repeater interaction (i.e., each repeater receives and amplifies signals from others)? (ii) How much performance improvement can be achieved given that repeaters also inject noise and may introduce more interference? To answer these questions, we first derive a generalized Nyquist stability criterion for the repeater swarm system, and provide an easy-to-check stability condition. Then, we study the uplink performance and develop an efficient iterative algorithm that jointly optimizes the repeater gains, user transmit powers, and receive combining weights to maximize the weighted sum rate while ensuring system stability. Numerical results corroborate our theoretical findings and show that the repeaters can significantly improve the system performance, both in sub-6 GHz and millimeter-wave bands. The results also warrant careful deployment to fully realize the benefits of repeaters, for example, by ensuring a high probability of line-of-sight links between repeaters and the base station.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13593v1</guid>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianan Bai, Anubhab Chowdhury, Anders Hansson, Erik G. Larsson</dc:creator>
    </item>
    <item>
      <title>Joint Beamforming Design for RIS-Empowered NOMA-ISAC Systems</title>
      <link>https://arxiv.org/abs/2508.13842</link>
      <description>arXiv:2508.13842v1 Announce Type: new 
Abstract: This paper investigates a reconfigurable intelligent surface (RIS)-assisted integrated sensing and communication (ISAC) system and proposes a joint communication and sensing beamforming design based on non-orthogonal multiple access (NOMA) technology. The system employs a dual-functional base station (DFBS) to simultaneously serve multiple users and sense multiple targets with the aid of RIS. To maximize the sum-rate of users, we jointly optimize the DFBS's active beamforming, the RIS's reflection coefficients, and the radar receive filters. The optimization is performed under constraints including the radar signal-to-noise ratio thresholds, the user signal-to-interference-plus-noise ratio requirements, the phase shifts of the RIS, the total transmit power, the receive filters, and the successive interference cancellation decoding order. To tackle the complex interdependencies and non-convex nature of the optimization problem, we introduce an effective iterative algorithm based on the alternating optimization framework. Simulation results demonstrate that the proposed algorithm outperforms baseline algorithms, highlighting its distinct advantages in the considered RIS-empowered NOMA-ISAC systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13842v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunjie Wang, Xuhui Zhang, Jinke Ren, Wenchao Liu, Shuqiang Wang, Yanyan Shen, Kejiang Ye, Chengzhong Xu, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>The Hidden Cost of Correlation: Rethinking Privacy Leakage in Local Differential Privacy</title>
      <link>https://arxiv.org/abs/2508.12539</link>
      <description>arXiv:2508.12539v1 Announce Type: cross 
Abstract: Local differential privacy (LDP) has emerged as a promising paradigm for privacy-preserving data collection in distributed systems, where users contribute multi-dimensional records with potentially correlated attributes. Recent work has highlighted that correlation-induced privacy leakage (CPL) plays a critical role in shaping the privacy-utility trade-off under LDP, especially when correlations exist among attributes. Nevertheless, it remains unclear to what extent the prevailing assumptions and proposed solutions are valid and how significant CPL is in real-world data. To address this gap, we first perform a comprehensive statistical analysis of five widely used LDP mechanisms -- GRR, RAPPOR, OUE, OLH and Exponential mechanism -- to assess CPL across four real-world datasets. We identify that many primary assumptions and metrics in current approaches fall short of accurately characterising these leakages. Moreover, current studies have been limited to a set of pure LDP (i.e., {\delta = 0}) mechanisms. In response, we develop the first algorithmic framework to theoretically quantify CPL for any general approximated LDP (({\varepsilon},{\delta})-LDP) mechanism. We validate our theoretical results against empirical statistical results and provide a theoretical explanation for the observed statistical patterns. Finally, we propose two novel benchmarks to validate correlation analysis algorithms and evaluate the utility vs CPL of LDP mechanisms. Further, we demonstrate how these findings can be applied to achieve an efficient privacy-utility trade-off in real-world data governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12539v1</guid>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandaru Jayawardana, Sennur Ulukus, Ming Ding, Kanchana Thilakarathna</dc:creator>
    </item>
    <item>
      <title>Order Optimal Regret Bounds for Sharpe Ratio Optimization in the Bandit Setting</title>
      <link>https://arxiv.org/abs/2508.13749</link>
      <description>arXiv:2508.13749v1 Announce Type: cross 
Abstract: In this paper, we investigate the problem of sequential decision-making for Sharpe ratio (SR) maximization in a stochastic bandit setting. We focus on the Thompson Sampling (TS) algorithm, a Bayesian approach celebrated for its empirical performance and exploration efficiency, under the assumption of Gaussian rewards with unknown parameters. Unlike conventional bandit objectives focusing on maximizing cumulative reward, Sharpe ratio optimization instead introduces an inherent tradeoff between achieving high returns and controlling risk, demanding careful exploration of both mean and variance. Our theoretical contributions include a novel regret decomposition specifically designed for the Sharpe ratio, highlighting the role of information acquisition about the reward distribution in driving learning efficiency. Then, we establish fundamental performance limits for the proposed algorithm \texttt{SRTS} in terms of an upper bound on regret. We also derive the matching lower bound and show the order-optimality. Our results show that Thompson Sampling achieves logarithmic regret over time, with distribution-dependent factors capturing the difficulty of distinguishing arms based on risk-adjusted performance. Empirical simulations show that our algorithm significantly outperforms existing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13749v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Taha Shah, Sabrina Khurshid, Gourab Ghatak</dc:creator>
    </item>
    <item>
      <title>Adversarially robust quantum state learning and testing</title>
      <link>https://arxiv.org/abs/2508.13959</link>
      <description>arXiv:2508.13959v1 Announce Type: cross 
Abstract: Quantum state learning is a fundamental problem in physics and computer science. As near-term quantum devices are error-prone, it is important to design error-resistant algorithms. Apart from device errors, other unexpected factors could also affect the algorithm, such as careless human read-out error, or even a malicious hacker deliberately altering the measurement results. Thus, we want our algorithm to work even in the worst case when things go against our favor.
  We consider the practical setting of single-copy measurements and propose the $\gamma$-adversarial corruption model where an imaginary adversary can arbitrarily change $\gamma$-fraction of the measurement outcomes. This is stronger than the $\gamma$-bounded SPAM noise model, where the post-measurement state changes by at most $\gamma$ in trace distance. Under our stronger model of corruption, we design an algorithm using non-adaptive measurements that can learn an unknown rank-$r$ state up to $\tilde{O}(\gamma\sqrt{r})$ in trace distance, provided that the number of copies is sufficiently large. We further prove an information-theoretic lower bound of $\Omega(\gamma\sqrt{r})$ for non-adaptive measurements, demonstrating the optimality of our algorithm. Our upper and lower bounds also hold for quantum state testing, where the goal is to test whether an unknown state is equal to a given state or far from it.
  Our results are intriguingly optimistic and pessimistic at the same time. For general states, the error is dimension-dependent and $\gamma\sqrt{d}$ in the worst case, meaning that only corrupting a very small fraction ($1/\sqrt{d}$) of the outcomes could totally destroy any non-adaptive learning algorithm. However, for constant-rank states that are useful in many quantum algorithms, it is possible to achieve dimension-independent error, even in the worst-case adversarial setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13959v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maryam Aliakbarpour, Vladimir Braverman, Nai-Hui Chia, Yuhan Liu</dc:creator>
    </item>
    <item>
      <title>Nearly Optimal Bounds on the Fourier sampling numbers of Besov Spaces</title>
      <link>https://arxiv.org/abs/2508.13991</link>
      <description>arXiv:2508.13991v1 Announce Type: cross 
Abstract: Let $\mathbb{T}^d$ denote the $d$-dimensional torus. We consider the problem of optimally recovering a target function $f^*:\mathbb{T}^d\rightarrow \mathbb{C}$ from samples of its Fourier coefficients. We make classical smoothness assumptions on $f^*$, specifically that $f^*$ lies in a Besov space $B^s_\infty(L_q)$ with $s &gt; 0$ and $1\leq q\leq \infty$, and measure recovery error in the $L_p$-norm with $1\leq p\leq \infty$. Abstractly, the optimal recovery error is characterized by a `restricted' version of the Gelfand widths, which we call the Fourier sampling numbers. Up to logarithmic factors, we determine the correct asymptotics of the Fourier sampling numbers in the regime $s/d &gt; 1 - 1/p$. We also give a description of nearly optimal Fourier measurements and recovery algorithms in each of these cases. In the other direction, we prove a novel lower bound showing that there is an asymptotic gap between the Fourier sampling numbers and the Gelfand widths when $q = 1$ and $p_0 &lt; p\leq 2$ with $p_0 \approx 1.535$. Finally, we discuss the practical implications of our results, which imply a sharper recovery of edges, and provide numerical results demonstrating this phenomenon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13991v1</guid>
      <category>math.FA</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan W. Siegel</dc:creator>
    </item>
    <item>
      <title>GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks</title>
      <link>https://arxiv.org/abs/2508.14004</link>
      <description>arXiv:2508.14004v1 Announce Type: cross 
Abstract: Quantized neural networks can be viewed as a chain of noisy channels, where rounding in each layer reduces capacity as bit-width shrinks; the floating-point (FP) checkpoint sets the maximum input rate. We track capacity dynamics as the average bit-width decreases and identify resulting quantization bottlenecks by casting fine-tuning as a smooth, constrained optimization problem. Our approach employs a fully differentiable Straight-Through Estimator (STE) with learnable bit-width, noise scale and clamp bounds, and enforces a target bit-width via an exterior-point penalty; mild metric smoothing (via distillation) stabilizes training. Despite its simplicity, the method attains competitive accuracy down to the extreme W1A1 setting while retaining the efficiency of STE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14004v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sergey Salishev, Ian Akhremchik</dc:creator>
    </item>
    <item>
      <title>Channel Coding for Gaussian Channels with Mean and Variance Constraints</title>
      <link>https://arxiv.org/abs/2501.10953</link>
      <description>arXiv:2501.10953v2 Announce Type: replace 
Abstract: We consider channel coding for Gaussian channels with the recently introduced mean and variance cost constraints. Through matching converse and achievability bounds, we characterize the optimal first- and second-order performance. The main technical contribution of this paper is an achievability scheme which uses random codewords drawn from a mixture of three uniform distributions on $(n-1)$-spheres of radii $R_1, R_2$ and $R_3$, where $R_i = O(\sqrt{n})$ and $|R_i - R_j| = O(1)$. To analyze such a mixture distribution, we prove a lemma giving a uniform $O(\log n)$ bound, which holds with high probability, on the log ratio of the output distributions $Q_i^{cc}$ and $Q_j^{cc}$, where $Q_i^{cc}$ is induced by a random channel input uniformly distributed on an $(n-1)$-sphere of radius $R_i$. To facilitate the application of the usual central limit theorem, we also give a uniform $O(\log n)$ bound, which holds with high probability, on the log ratio of the output distributions $Q_i^{cc}$ and $Q^*_i$, where $Q_i^*$ is induced by a random channel input with i.i.d. components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10953v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adeel Mahmood, Aaron B. Wagner</dc:creator>
    </item>
    <item>
      <title>On de Bruijn Array Codes Part II: Linear Codes</title>
      <link>https://arxiv.org/abs/2501.12124</link>
      <description>arXiv:2501.12124v4 Announce Type: replace 
Abstract: An M-sequence generated by a primitive polynomial has many interesting and desirable properties. A pseudo-random array is the two-dimensional generalization of an M-sequence. There are non-primitive polynomials all of whose non-zero sequences have the same period. These polynomials generate \emph{sets} of sequences with properties similar to M-sequences. In this paper, a two-dimensional generalization for such sequences is given. This generalization is for a pseudo-random array code, which is a set of $r_1 \times r_2$ arrays in which each $n_1 \times n_2$ nonzero matrix is contained exactly once as a window in one of the arrays. Moreover, these arrays have the shift-and-add property, i.e., the bitwise addition of two arrays (or a nontrivial shift of such arrays) is another array (or a shift of another array) from the code. All the known arrays can be formed by folding sequences generated from an irreducible polynomial or a reducible polynomial whose factors have the same degree and the same exponent. Two proof techniques are used to prove the constructions are indeed of pseudo-random array codes. The first technique is based on another method, different from folding, for constructing some of these arrays. The second technique is a generalization of a known proof technique. This generalization enables the construction of pseudo-random arrays with parameters not known before, and also provides a variety of pseudo-random array codes which cannot be generated by the first method. The two techniques also suggest two different hierarchies between pseudo-random array codes. Finally, two methods to verify whether a folding of sequences, generated by these polynomials, yields a pseudo-random array or a pseudo-random array code, will be presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12124v4</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Blackburn, Yeow Meng Chee, Tuvi Etzion, Huimin Lao</dc:creator>
    </item>
    <item>
      <title>Neural Coding Is Not Always Semantic: Toward the Standardized Coding Workflow in Semantic Communications</title>
      <link>https://arxiv.org/abs/2505.18637</link>
      <description>arXiv:2505.18637v3 Announce Type: replace 
Abstract: Semantic communication, leveraging advanced deep learning techniques, emerges as a new paradigm that meets the requirements of next-generation wireless networks. However, current semantic communication systems, which employ neural coding for feature extraction from raw data, have not adequately addressed the fundamental question: Is general feature extraction through deep neural networks sufficient for understanding semantic meaning within raw data in semantic communication? This article is thus motivated to clarify two critical aspects: semantic understanding and general semantic representation. This article presents a standardized definition on semantic coding, an extensive neural coding scheme for general semantic representation that clearly represents underlying data semantics based on contextual modeling. With these general semantic representations obtained, both human- and machine-centric end-to-end data transmission can be achieved through only minimal specialized modifications, such as fine-tuning and regularization. This article contributes to establishing a commonsense that semantic communication extends far beyond mere feature transmission, focusing instead on conveying compact semantic representations through context-aware coding schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18637v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hai-Long Qin, Jincheng Dai, Sixian Wang, Xiaoqi Qin, Shuo Shao, Kai Niu, Wenjun Xu, Ping Zhang</dc:creator>
    </item>
    <item>
      <title>Cell-Free Massive MIMO SWIPT with Beyond Diagonal Reconfigurable Intelligent Surfaces</title>
      <link>https://arxiv.org/abs/2507.23702</link>
      <description>arXiv:2507.23702v3 Announce Type: replace 
Abstract: We investigate the integration of beyond diagonal reconfigurable intelligent surfaces (BDRISs) into cell free massive multiple input multiple output (CFmMIMO) systems to enhance simultaneous wireless information and power transfer (SWIPT). To simultaneously support two groups of users energy receivers (ERs) and information receivers (IRs) without sacrificing time frequency resources, a subset of access points (APs) is dedicated to serving ERs with the aid of a BDRIS, while the remaining APs focus on supporting IRs. A protective partial zero forcing precoding technique is implemented at the APs to manage the non coherent interference between the ERs and IRs. Subsequently, closed form expressions for the spectral efficiency of the IRs and the average sum of harvested energy at the ERs are leveraged to formulate a comprehensive optimization problem. This problem jointly optimizes the AP selection, AP power control, and scattering matrix design at the BDRIS, all based on long term statistical channel state information. This challenging problem is then effectively transformed into more tractable forms. To solve these sub problems, efficient algorithms are proposed, including a heuristic search for the scattering matrix design, as well as successive convex approximation and deep reinforcement learning methods for the joint AP mode selection and power control design. Numerical results show that a BDRIS with a group or fully connected architecture achieves significant energy harvesting gains over the conventional diagonal RIS, especially delivering up to a seven fold increase in the average sum of harvested energy when a heuristic based scattering matrix design is employed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23702v3</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TCOMM.2025.3597660</arxiv:DOI>
      <dc:creator>Duc Thien Hua, Mohammadali Mohammadi, Hien Quoc Ngo, Michail Matthaiou</dc:creator>
    </item>
    <item>
      <title>Explicit Function-Correcting Code Constructions for Lee Metric Channels</title>
      <link>https://arxiv.org/abs/2508.01702</link>
      <description>arXiv:2508.01702v2 Announce Type: replace 
Abstract: Function-Correcting Codes (FCCs) are a novel class of codes designed to protect function evaluations of messages against errors while minimizing redundancy. A theoretical framework for systematic FCCs to channels matched to the Lee metric has been studied recently, which introduced function-correcting Lee codes (FCLCs) and also derived upper and lower bounds on their optimal redundancy. In this paper, we first propose a Plotkin-like bound for irregular Lee-distance codes. We then construct explicit FCLCs for specific classes of functions, including the Lee weight, Lee weight distribution, modular sum, and locally bounded function. For these functions, lower bounds on redundancy are obtained, and our constructions are shown to be optimal in certain cases. Finally, a comparative analysis with classical Lee error-correcting codes and codes correcting errors in function values, demonstrates that FCLCs can significantly reduce redundancy while preserving function correctness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01702v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hareesh K., Rashid Ummer N. T., B. Sundar Rajan</dc:creator>
    </item>
    <item>
      <title>On the reconstruction limits of complex networks</title>
      <link>https://arxiv.org/abs/2501.01437</link>
      <description>arXiv:2501.01437v3 Announce Type: replace-cross 
Abstract: Network reconstruction consists in retrieving the hidden interaction structure of a system from observations. Many reconstruction algorithms have been proposed, although less research has been devoted to describe their theoretical limitations. In this work, we take a first-principles approach and build on our earlier definition of reconstructability-the fraction of structural information recoverable from data. We relate this quantity to the true data-generating (TDG) process and delineate an information-theoretic reconstruction limit, i.e., the upper bound of the mutual information between the true underlying graph and any graph reconstructed from observations. These concepts lead us to a principled numerical method to assess the validity of empirically reconstructed networks, based on model selection and a quantity we introduce: the reconstruction index. This index approximates the reconstructability from data, quantifies the variability of the reconstructed network ensemble, and is shown to predict reconstruction error without requiring knowledge of the true underlying network. We characterize this method and test it on empirical time series and networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01437v3</guid>
      <category>stat.AP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles Murphy, Simon Lizotte, Fran\c{c}ois Thibault, Vincent Thibeault, Patrick Desrosiers, Antoine Allard</dc:creator>
    </item>
  </channel>
</rss>
