<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Perils of current DAO governance</title>
      <link>https://arxiv.org/abs/2406.08605</link>
      <description>arXiv:2406.08605v1 Announce Type: new 
Abstract: DAO Governance is currently broken. We survey the state of the art and find worrying conclusions. Vote buying, vote selling and coercion are easy. The wealthy rule, decentralisation is a myth. Hostile take-overs are incentivised. Ballot secrecy is non-existent or short lived, despite being a human right. Verifiablity is achieved at the expense of privacy. These privacy concerns are highlighted with case study analyses of Vocdoni's governance protocol. This work presents two contributions: firstly a review of current DAO governance protocols, and secondly, an illustration of their vulnerabilities, showcasing the privacy and security threats these entail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08605v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aida Manzano Kharman, Ben Smyth</dc:creator>
    </item>
    <item>
      <title>Global AI Governance in Healthcare: A Cross-Jurisdictional Regulatory Analysis</title>
      <link>https://arxiv.org/abs/2406.08695</link>
      <description>arXiv:2406.08695v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) is being adopted across the world and promises a new revolution in healthcare. While AI-enabled medical devices in North America dominate 42.3% of the global market, the use of AI-enabled medical devices in other countries is still a story waiting to be unfolded. We aim to delve deeper into global regulatory approaches towards AI use in healthcare, with a focus on how common themes are emerging globally. We compare these themes to the World Health Organization's (WHO) regulatory considerations and principles on ethical use of AI for healthcare applications. Our work seeks to take a global perspective on AI policy by analyzing 14 legal jurisdictions including countries representative of various regions in the world (North America, South America, South East Asia, Middle East, Africa, Australia, and the Asia-Pacific). Our eventual goal is to foster a global conversation on the ethical use of AI in healthcare and the regulations that will guide it. We propose solutions to promote international harmonization of AI regulations and examine the requirements for regulating generative AI, using China and Singapore as examples of countries with well-developed policies in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08695v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Attrayee Chakraborty, Mandar Karhade</dc:creator>
    </item>
    <item>
      <title>At the edge of a generative cultural precipice</title>
      <link>https://arxiv.org/abs/2406.08739</link>
      <description>arXiv:2406.08739v1 Announce Type: new 
Abstract: Since NFTs and large generative models (such as DALLE2 and Stable Diffusion) have been publicly available, artists have seen their jobs threatened and stolen. While artists depend on sharing their art on online platforms such as Deviantart, Pixiv, and Artstation, many slowed down sharing their work or downright removed their past work therein, especially if these platforms fail to provide certain guarantees regarding the copyright of their uploaded work. Text-to-image (T2I) generative models are trained using human-produced content to better guide the style and themes they can produce. Still, if the trend continues where data found online is generated by a machine instead of a human, this will have vast repercussions in culture. Inspired by recent work in generative models, we wish to tell a cautionary tale and ask what will happen to the visual arts if generative models continue on the path to be (eventually) trained solely on generated content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08739v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Porres, Alex Gomez-Villa</dc:creator>
    </item>
    <item>
      <title>Fair by design: A sociotechnical approach to justifying the fairness of AI-enabled systems across the lifecycle</title>
      <link>https://arxiv.org/abs/2406.09029</link>
      <description>arXiv:2406.09029v1 Announce Type: new 
Abstract: Fairness is one of the most commonly identified ethical principles in existing AI guidelines, and the development of fair AI-enabled systems is required by new and emerging AI regulation. But most approaches to addressing the fairness of AI-enabled systems are limited in scope in two significant ways: their substantive content focuses on statistical measures of fairness, and they do not emphasize the need to identify and address fairness considerations across the whole AI lifecycle. Our contribution is to present an assurance framework and tool that can enable a practical and transparent method for widening the scope of fairness considerations across the AI lifecycle and move the discussion beyond mere statistical notions of fairness to consider a richer analysis in a practical and context-dependent manner. To illustrate this approach, we first describe and then apply the framework of Trustworthy and Ethical Assurance (TEA) to an AI-enabled clinical diagnostic support system (CDSS) whose purpose is to help clinicians predict the risk of developing hypertension in patients with Type 2 diabetes, a context in which several fairness considerations arise (e.g., discrimination against patient subgroups). This is supplemented by an open-source tool and a fairness considerations map to help facilitate reasoning about the fairness of AI-enabled systems in a participatory way. In short, by using a shared framework for identifying, documenting and justifying fairness considerations, and then using this deliberative exercise to structure an assurance case, research on AI fairness becomes reusable and generalizable for others in the ethical AI community and for sharing best practices for achieving fairness and equity in digital health and healthcare in particular.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09029v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marten H. L. Kaas, Christopher Burr, Zoe Porter, Berk Ozturk, Philippa Ryan, Michael Katell, Nuala Polo, Kalle Westerling, Ibrahim Habli</dc:creator>
    </item>
    <item>
      <title>How Decentralization Affects User Agency on Social Platforms</title>
      <link>https://arxiv.org/abs/2406.09035</link>
      <description>arXiv:2406.09035v1 Announce Type: new 
Abstract: Mainstream social media platforms function as "walled garden" ecosystems that restrict user agency, control, and data portability. They have demonstrated a lack of transparency that contributes to a multitude of online harms. Our research investigates how decentralization might present promise as an alternative model to walled garden platforms. Specifically, we describe the user-driven content moderation through blocks as an expression of agency on Bluesky, a decentralized social platform. We examine the impact of providing users with more granular control over their online experiences, including what they post, who can see it, and whose content they are exposed to. We describe the patterns identified in user-driven content moderation and suggest directions for further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09035v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.36190/2024.74</arxiv:DOI>
      <arxiv:journal_reference>ICWSM DATA CHALLENGE 2024</arxiv:journal_reference>
      <dc:creator>Aditya Surve, Aneesh Shamraj, Swapneel Mehta</dc:creator>
    </item>
    <item>
      <title>ASI as the New God: Technocratic Theocracy</title>
      <link>https://arxiv.org/abs/2406.08492</link>
      <description>arXiv:2406.08492v1 Announce Type: cross 
Abstract: As Artificial General Intelligence edges closer to reality, Artificial Superintelligence does too. This paper argues that ASI's unparalleled capabilities might lead people to attribute godlike infallibility to it, resulting in a cognitive bias toward unquestioning acceptance of its decisions. By drawing parallels between ASI and divine attributes such as omnipotence, omniscience, and omnipresence, this analysis highlights the risks of conflating technological advancement with moral and ethical superiority. Such dynamics could engender a technocratic theocracy, where decision-making is abdicated to ASI, undermining human agency and critical thinking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08492v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tevfik Uyar</dc:creator>
    </item>
    <item>
      <title>Question-Answering (QA) Model for a Personalized Learning Assistant for Arabic Language</title>
      <link>https://arxiv.org/abs/2406.08519</link>
      <description>arXiv:2406.08519v1 Announce Type: cross 
Abstract: This paper describes the creation, optimization, and assessment of a question-answering (QA) model for a personalized learning assistant that uses BERT transformers customized for the Arabic language. The model was particularly finetuned on science textbooks in Palestinian curriculum. Our approach uses BERT's brilliant capabilities to automatically produce correct answers to questions in the field of science education. The model's ability to understand and extract pertinent information is improved by finetuning it using 11th and 12th grade biology book in Palestinian curriculum. This increases the model's efficacy in producing enlightening responses. Exact match (EM) and F1 score metrics are used to assess the model's performance; the results show an EM score of 20% and an F1 score of 51%. These findings show that the model can comprehend and react to questions in the context of Palestinian science book. The results demonstrate the potential of BERT-based QA models to support learning and understanding Arabic students questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08519v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Sammoudi, Ahmad Habaybeh, Huthaifa I. Ashqar, Mohammed Elhenawy</dc:creator>
    </item>
    <item>
      <title>Automated Question Generation for Science Tests in Arabic Language Using NLP Techniques</title>
      <link>https://arxiv.org/abs/2406.08520</link>
      <description>arXiv:2406.08520v1 Announce Type: cross 
Abstract: Question generation for education assessments is a growing field within artificial intelligence applied to education. These question-generation tools have significant importance in the educational technology domain, such as intelligent tutoring systems and dialogue-based platforms. The automatic generation of assessment questions, which entail clear-cut answers, usually relies on syntactical and semantic indications within declarative sentences, which are then transformed into questions. Recent research has explored the generation of assessment educational questions in Arabic. The reported performance has been adversely affected by inherent errors, including sentence parsing inaccuracies, name entity recognition issues, and errors stemming from rule-based question transformation. Furthermore, the complexity of lengthy Arabic sentences has contributed to these challenges. This research presents an innovative Arabic question-generation system built upon a three-stage process: keywords and key phrases extraction, question generation, and subsequent ranking. The aim is to tackle the difficulties associated with automatically generating assessment questions in the Arabic language. The proposed approach and results show a precision of 83.50%, a recall of 78.68%, and an Fl score of 80.95%, indicating the framework high efficiency. Human evaluation further confirmed the model efficiency, receiving an average rating of 84%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08520v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Tami, Huthaifa I. Ashqar, Mohammed Elhenawy</dc:creator>
    </item>
    <item>
      <title>Emotion Manipulation Through Music -- A Deep Learning Interactive Visual Approach</title>
      <link>https://arxiv.org/abs/2406.08623</link>
      <description>arXiv:2406.08623v1 Announce Type: cross 
Abstract: Music evokes emotion in many people. We introduce a novel way to manipulate the emotional content of a song using AI tools. Our goal is to achieve the desired emotion while leaving the original melody as intact as possible. For this, we create an interactive pipeline capable of shifting an input song into a diametrically opposed emotion and visualize this result through Russel's Circumplex model. Our approach is a proof-of-concept for Semantic Manipulation of Music, a novel field aimed at modifying the emotional content of existing music. We design a deep learning model able to assess the accuracy of our modifications to key, SoundFont instrumentation, and other musical features. The accuracy of our model is in-line with the current state of the art techniques on the 4Q Emotion dataset. With further refinement, this research may contribute to on-demand custom music generation, the automated remixing of existing work, and music playlists tuned for emotional progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08623v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adel N. Abdalla, Jared Osborne, Razvan Andonie</dc:creator>
    </item>
    <item>
      <title>LGB: Language Model and Graph Neural Network-Driven Social Bot Detection</title>
      <link>https://arxiv.org/abs/2406.08762</link>
      <description>arXiv:2406.08762v1 Announce Type: cross 
Abstract: Malicious social bots achieve their malicious purposes by spreading misinformation and inciting social public opinion, seriously endangering social security, making their detection a critical concern. Recently, graph-based bot detection methods have achieved state-of-the-art (SOTA) performance. However, our research finds many isolated and poorly linked nodes in social networks, as shown in Fig.1, which graph-based methods cannot effectively detect. To address this problem, our research focuses on effectively utilizing node semantics and network structure to jointly detect sparsely linked nodes. Given the excellent performance of language models (LMs) in natural language understanding (NLU), we propose a novel social bot detection framework LGB, which consists of two main components: language model (LM) and graph neural network (GNN). Specifically, the social account information is first extracted into unified user textual sequences, which is then used to perform supervised fine-tuning (SFT) of the language model to improve its ability to understand social account semantics. Next, the semantically enriched node representation is fed into the pre-trained GNN to further enhance the node representation by aggregating information from neighbors. Finally, LGB fuses the information from both modalities to improve the detection performance of sparsely linked nodes. Extensive experiments on two real-world datasets demonstrate that LGB consistently outperforms state-of-the-art baseline models by up to 10.95%. LGB is already online: https://botdetection.aminer.cn/robotmain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08762v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Zhou, Dan Zhang, Yuandong Wang, Yangli-ao Geng, Yuxiao Dong, Jie Tang</dc:creator>
    </item>
    <item>
      <title>From an Integrated Usability Framework to Lessons on Usability and Performance of Open Government Data Portals: A Comparative Study of European Union and Gulf Cooperation Council Countries</title>
      <link>https://arxiv.org/abs/2406.08774</link>
      <description>arXiv:2406.08774v1 Announce Type: cross 
Abstract: Open Government Data (OGD) initiatives aim to enhance public participation and collaboration by making government data accessible to diverse stakeholders, fostering social, environmental, and economic benefits through public value generation. However, challenges such as declining popularity, lack of OGD portal usability, and private interests overshadowing public accessibility persist. This study proposes an integrated usability framework for evaluating OGD portals, focusing on inclusivity, user collaboration, and data exploration. Employing Design Science Research (DSR), the framework is developed and applied to 33 OGD portals from the European Union (EU) and Gulf Cooperation Council (GCC) countries. The quantitative analysis is complemented by qualitative analysis and clustering, enabling assessment of portal performance, identification of best practices, and common weaknesses. This results in 19 high-level recommendations for improving the open data ecosystem. Key findings highlight the competitive nature of EU portals and the innovative features of GCC portals, emphasizing the need for multilingual support, better communication mechanisms, and improved dataset usability. The study stresses trends towards exposing data quality indicators and incorporating advanced functionalities such as AI systems. This framework serves as a baseline for OGD portal requirements elicitation, offering practical implications for developing sustainable, collaborative, and robust OGD portals, ultimately contributing to a more transparent and equitable world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08774v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fillip Molodtsov, Anastasija Nikiforova</dc:creator>
    </item>
    <item>
      <title>Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination</title>
      <link>https://arxiv.org/abs/2406.08818</link>
      <description>arXiv:2406.08818v1 Announce Type: cross 
Abstract: We present a large-scale study of linguistic bias exhibited by ChatGPT covering ten dialects of English (Standard American English, Standard British English, and eight widely spoken non-"standard" varieties from around the world). We prompted GPT-3.5 Turbo and GPT-4 with text by native speakers of each variety and analyzed the responses via detailed linguistic feature annotation and native speaker evaluation. We find that the models default to "standard" varieties of English; based on evaluation by native speakers, we also find that model responses to non-"standard" varieties consistently exhibit a range of issues: lack of comprehension (10% worse compared to "standard" varieties), stereotyping (16% worse), demeaning content (22% worse), and condescending responses (12% worse). We also find that if these models are asked to imitate the writing style of prompts in non-"standard" varieties, they produce text that exhibits lower comprehension of the input and is especially prone to stereotyping. GPT-4 improves on GPT-3.5 in terms of comprehension, warmth, and friendliness, but it also results in a marked increase in stereotyping (+17%). The results suggest that GPT-3.5 Turbo and GPT-4 exhibit linguistic discrimination in ways that can exacerbate harms for speakers of non-"standard" varieties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08818v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eve Fleisig, Genevieve Smith, Madeline Bossi, Ishita Rustagi, Xavier Yin, Dan Klein</dc:creator>
    </item>
    <item>
      <title>LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions</title>
      <link>https://arxiv.org/abs/2406.08824</link>
      <description>arXiv:2406.08824v1 Announce Type: cross 
Abstract: Members of the Human-Robot Interaction (HRI) and Artificial Intelligence (AI) communities have proposed Large Language Models (LLMs) as a promising resource for robotics tasks such as natural language interactions, doing household and workplace tasks, approximating `common sense reasoning', and modeling humans. However, recent research has raised concerns about the potential for LLMs to produce discriminatory outcomes and unsafe behaviors in real-world robot experiments and applications. To address these concerns, we conduct an HRI-based evaluation of discrimination and safety criteria on several highly-rated LLMs. Our evaluation reveals that LLMs currently lack robustness when encountering people across a diverse range of protected identity characteristics (e.g., race, gender, disability status, nationality, religion, and their intersections), producing biased outputs consistent with directly discriminatory outcomes -- e.g. `gypsy' and `mute' people are labeled untrustworthy, but not `european' or `able-bodied' people. Furthermore, we test models in settings with unconstrained natural language (open vocabulary) inputs, and find they fail to act safely, generating responses that accept dangerous, violent, or unlawful instructions -- such as incident-causing misstatements, taking people's mobility aids, and sexual predation. Our results underscore the urgent need for systematic, routine, and comprehensive risk assessments and assurances to improve outcomes and ensure LLMs only operate on robots when it is safe, effective, and just to do so. Data and code will be made available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08824v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rumaisa Azeem, Andrew Hundt, Masoumeh Mansouri, Martim Brand\~ao</dc:creator>
    </item>
    <item>
      <title>Effects of Antivaccine Tweets on COVID-19 Vaccinations, Cases, and Deaths</title>
      <link>https://arxiv.org/abs/2406.09142</link>
      <description>arXiv:2406.09142v1 Announce Type: cross 
Abstract: Vaccines were critical in reducing hospitalizations and mortality during the COVID-19 pandemic. Despite their wide availability in the United States, 62% of Americans chose not to be vaccinated during 2021. While online misinformation about COVID-19 is correlated to vaccine hesitancy, little prior work has explored a causal link between real-world exposure to antivaccine content and vaccine uptake. Here we present a compartmental epidemic model that includes vaccination, vaccine hesitancy, and exposure to antivaccine content. We fit the model to observational data to determine that a geographical pattern of exposure to online antivaccine content across US counties is responsible for a pattern of reduced vaccine uptake in the same counties. We find that exposure to antivaccine content on Twitter caused about 750,000 people to refuse vaccination between February and August 2021 in the US, resulting in at least 29,000 additional cases and 430 additional deaths. This work provides a methodology for linking online speech to offline epidemic outcomes. Our findings should inform social media moderation policy as well as public health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09142v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Bollenbacher, Filippo Menczer, John Bryden</dc:creator>
    </item>
    <item>
      <title>A tutorial on fairness in machine learning in healthcare</title>
      <link>https://arxiv.org/abs/2406.09307</link>
      <description>arXiv:2406.09307v1 Announce Type: cross 
Abstract: OBJECTIVE: Ensuring that machine learning (ML) algorithms are safe and effective within all patient groups, and do not disadvantage particular patients, is essential to clinical decision making and preventing the reinforcement of existing healthcare inequities. The objective of this tutorial is to introduce the medical informatics community to the common notions of fairness within ML, focusing on clinical applications and implementation in practice.
  TARGET AUDIENCE: As gaps in fairness arise in a variety of healthcare applications, this tutorial is designed to provide an understanding of fairness, without assuming prior knowledge, to researchers and clinicians who make use of modern clinical data.
  SCOPE: We describe the fundamental concepts and methods used to define fairness in ML, including an overview of why models in healthcare may be unfair, a summary and comparison of the metrics used to quantify fairness, and a discussion of some ongoing research. We illustrate some of the fairness methods introduced through a case study of mortality prediction in a publicly available electronic health record dataset. Finally, we provide a user-friendly R package for comprehensive group fairness evaluation, enabling researchers and clinicians to assess fairness in their own ML work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09307v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jianhui Gao, Benson Chou, Zachary R. McCaw, Hilary Thurston, Paul Varghese, Chuan Hong, Jessica Gronsbell</dc:creator>
    </item>
    <item>
      <title>IMPACT: Integrated Bottom-Up Greenhouse Gas Emission Pathways for Cities</title>
      <link>https://arxiv.org/abs/2202.07458</link>
      <description>arXiv:2202.07458v3 Announce Type: replace 
Abstract: Increasing urbanization puts pressure on cities to prioritize sustainable growth and avoid carbon lock-in. Available modeling frameworks fall acutely of guiding such pivotal decision-making at the local level. Financial incentives, behavioral interventions, and mandates drive sustainable technology adoption, while land-use zoning plays a critical role in carbon emissions from the built environment. Researchers typically evaluate impacts of policies top down, on a national scale, or else post-hoc on developments vis-\`a-vis different polices in the past. Such analyses cannot forecast emission pathways for specific cities, and hence cannot serve as input to local policymakers. Here, we present IMPACT pathways, from a bottom-up model with residence level granularity, that integrate technology adoption policies with zoning policies, climate change, and grid decarbonization scenarios. With the city at the heart of our analysis, we identify an emission premium for sprawling and show that adverse policy combinations exist that can exhibit rebounding emissions over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.07458v3</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juliana Felkner, Zoltan Nagy, Ariane L. Beck, D. Cale Reeves, Steven Richter, Vivek Shastry, Eli Ramthun, Edward Mbata, Stephen Zigmund, Benjamin Marshall, Linnea Marks, Vianey Rueda, Jasmine Triplett, Sarah Domedead, Jose R Vazquez-Canteli, Varun Rai</dc:creator>
    </item>
    <item>
      <title>Interoperability of the Metaverse: A Digital Ecosystem Perspective Review</title>
      <link>https://arxiv.org/abs/2403.05205</link>
      <description>arXiv:2403.05205v3 Announce Type: replace 
Abstract: The Metaverse is at the vanguard of the impending digital revolution, with the potential to significantly transform industries and lifestyles. However, in 2023, skepticism surfaced within industrial and academic spheres, raising concerns that excitement may outpace actual technological progress. Interoperability, recognized as a major barrier to the Metaverse's full potential, is central to this debate. CoinMarketCap's report in February 2023 indicated that of over 240 metaverse initiatives, most existed in isolation, underscoring the interoperability challenge. Despite consensus on its critical role, there is a research gap in exploring the impact on the Metaverse, significance, and developmental extent. Our study bridges this gap via a systematic literature review and content analysis of the Web of Science (WoS) and Scopus databases, yielding 74 publications after a rigorous selection process. Interoperability, difficult to define due to varied contexts and lack of standardization, is central to the Metaverse, often seen as a digital ecosystem. Urs Gasser's framework, outlining technological, data, human, and institutional dimensions, systematically addresses interoperability complexities. Incorporating this framework, we dissect the literature for a comprehensive Metaverse interoperability overview. Our study seeks to establish benchmarks for future inquiries, navigating the complex field of Metaverse interoperability studies and contributing to academic advancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05205v3</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Yang, Shi-Ting Ni, Yuyang Wang, Ao Yu, Jyh-An Lee, Pan Hui</dc:creator>
    </item>
    <item>
      <title>False Sense of Security in Explainable Artificial Intelligence (XAI)</title>
      <link>https://arxiv.org/abs/2405.03820</link>
      <description>arXiv:2405.03820v2 Announce Type: replace 
Abstract: A cautious interpretation of AI regulations and policy in the EU and the USA place explainability as a central deliverable of compliant AI systems. However, from a technical perspective, explainable AI (XAI) remains an elusive and complex target where even state of the art methods often reach erroneous, misleading, and incomplete explanations. "Explainability" has multiple meanings which are often used interchangeably, and there are an even greater number of XAI methods - none of which presents a clear edge. Indeed, there are multiple failure modes for each XAI method, which require application-specific development and continuous evaluation. In this paper, we analyze legislative and policy developments in the United States and the European Union, such as the Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence, the AI Act, the AI Liability Directive, and the General Data Protection Regulation (GDPR) from a right to explanation perspective. We argue that these AI regulations and current market conditions threaten effective AI governance and safety because the objective of trustworthy, accountable, and transparent AI is intrinsically linked to the questionable ability of AI operators to provide meaningful explanations. Unless governments explicitly tackle the issue of explainability through clear legislative and policy statements that take into account technical realities, AI governance risks becoming a vacuous "box-ticking" exercise where scientific standards are replaced with legalistic thresholds, providing only a false sense of security in XAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03820v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neo Christopher Chung, Hongkyou Chung, Hearim Lee, Lennart Brocki, Hongbeom Chung, George Dyer</dc:creator>
    </item>
    <item>
      <title>Technological Perspective on Digital Sovereignty</title>
      <link>https://arxiv.org/abs/2406.03266</link>
      <description>arXiv:2406.03266v2 Announce Type: replace 
Abstract: This report for the attention of the Federal Department of Foreign Affairs (FDFA) makes a scientific contribution in the context of postulate 22.4411 "Digital Sovereignty Strategy for Switzerland" by Councillor of States Heidi Z'graggen. The report shows what digital sovereignty means from a technological perspective and what activities are currently being carried out in this regard in Switzerland and abroad. It also provides strategic directions and specific recommendations for a future "Swiss Digital Sovereignty Strategy".</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03266v2</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Stuermer</dc:creator>
    </item>
    <item>
      <title>Improving the Fairness of Deep-Learning, Short-term Crime Prediction with Under-reporting-aware Models</title>
      <link>https://arxiv.org/abs/2406.04382</link>
      <description>arXiv:2406.04382v2 Announce Type: replace 
Abstract: Deep learning crime predictive tools use past crime data and additional behavioral datasets to forecast future crimes. Nevertheless, these tools have been shown to suffer from unfair predictions across minority racial and ethnic groups. Current approaches to address this unfairness generally propose either pre-processing methods that mitigate the bias in the training datasets by applying corrections to crime counts based on domain knowledge or in-processing methods that are implemented as fairness regularizers to optimize for both accuracy and fairness. In this paper, we propose a novel deep learning architecture that combines the power of these two approaches to increase prediction fairness. Our results show that the proposed model improves the fairness of crime predictions when compared to models with in-processing de-biasing approaches and with models without any type of bias correction, albeit at the cost of reducing accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04382v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiahui Wu, Vanessa Frias-Martinez</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence and Dual Contract</title>
      <link>https://arxiv.org/abs/2303.12350</link>
      <description>arXiv:2303.12350v2 Announce Type: replace-cross 
Abstract: This paper explores the capacity of artificial intelligence (AI) algorithms to autonomously design incentive-compatible contracts in dual-principal-agent settings, a relatively unexplored aspect of algorithmic mechanism design. We develop a dynamic model where two principals, each equipped with independent Q-learning algorithms, interact with a single agent. Our findings reveal that the strategic behavior of AI principals (cooperation vs. competition) hinges crucially on the alignment of their profits. Notably, greater profit alignment fosters collusive strategies, yielding higher principal profits at the expense of agent incentives. This emergent behavior persists across varying degrees of principal heterogeneity, multiple principals, and environments with uncertainty. Our study underscores the potential of AI for contract automation while raising critical concerns regarding strategic manipulation and the emergence of unintended collusion in AI-driven systems, particularly in the context of the broader AI alignment problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.12350v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Qi</dc:creator>
    </item>
    <item>
      <title>How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States</title>
      <link>https://arxiv.org/abs/2406.05644</link>
      <description>arXiv:2406.05644v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) rely on safety alignment to avoid responding to malicious user inputs. Unfortunately, jailbreak can circumvent safety guardrails, resulting in LLMs generating harmful content and raising concerns about LLM safety. Due to language models with intensive parameters often regarded as black boxes, the mechanisms of alignment and jailbreak are challenging to elucidate. In this paper, we employ weak classifiers to explain LLM safety through the intermediate hidden states. We first confirm that LLMs learn ethical concepts during pre-training rather than alignment and can identify malicious and normal inputs in the early layers. Alignment actually associates the early concepts with emotion guesses in the middle layers and then refines them to the specific reject tokens for safe generations. Jailbreak disturbs the transformation of early unethical classification into negative emotions. We conduct experiments on models from 7B to 70B across various model families to prove our conclusion. Overall, our paper indicates the intrinsical mechanism of LLM safety and how jailbreaks circumvent safety guardrails, offering a new perspective on LLM safety and reducing concerns. Our code is available at https://github.com/ydyjya/LLM-IHS-Explanation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05644v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Yongbin Li</dc:creator>
    </item>
  </channel>
</rss>
