<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Nov 2025 05:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Inferential Theory for Pricing Errors with Latent Factors and Firm Characteristics</title>
      <link>https://arxiv.org/abs/2511.03076</link>
      <description>arXiv:2511.03076v1 Announce Type: new 
Abstract: We study factor models that combine latent factors with firm characteristics and propose a new framework for modeling, estimating, and inferring pricing errors. Following Zhang (2024), our approach decomposes mispricing into two distinct components: inside alpha, explained by firm characteristics but orthogonal to factor exposures, and outside alpha, orthogonal to both factors and characteristics. Our model generalizes those developed recently such as Kelly et al. (2019) and Zhang (2024), resolving issues of orthogonality, basis dependence, and unit sensitivity. Methodologically, we develop estimators grounded in low-rank methods with explicit debiasing, providing closed-form solutions and a rigorous inferential theory that accommodates a growing number of characteristics and relaxes standard assumptions on sample dimensions. Empirically, using U.S. stock returns from 2000-2019, we document strong evidence of both inside and outside alphas, with the former showing industry-level co-movements and the latter reflecting idiosyncratic shocks beyond firm fundamentals. Our framework thus unifies statistical and characteristic-based approaches to factor modeling, offering both theoretical advances and new insights into the structure of pricing errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03076v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jungjun Choi, Ming Yuan</dc:creator>
    </item>
    <item>
      <title>The Economics of Spatial Coordination in Critical Infrastructure Investment</title>
      <link>https://arxiv.org/abs/2511.03091</link>
      <description>arXiv:2511.03091v1 Announce Type: new 
Abstract: We develop a hybrid approach to estimate spatial coordination mechanisms in structural dynamic discrete choice models by combining nested fixed-point (NFXP) dynamic programming with method of simulated moments (MSM), achieving computational tractability in spatial settings while preserving structural interpretation. Applying this framework to GPU replacement data from 12,915 equipment locations in Oak Ridge National Laboratory's Titan supercomputer, we identify two distinct coordination mechanisms: sequential replacement cascades (gamma_lag = -0.793) and contemporaneous failure batching (gamma_fail = -0.265). Sequential coordination dominates - approximately three times stronger than failure batching - indicating that operators engage in deliberate strategic behavior rather than purely reactive responses. Spatial interdependencies account for 5.3% of variation unexplained by independent-decision models, with coordination concentrated in high-risk thermal environments exhibiting effects more than 10 times stronger than cool zones. Formal tests decisively reject spatial independence (chi-squared(2) = 685.38, p &lt; 0.001), demonstrating that infrastructure policies ignoring spatial coordination will systematically mistime interventions and forgo available coordination gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03091v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L Kaili Diamond, Benjamin Gilbert</dc:creator>
    </item>
    <item>
      <title>Large Bayesian Tensor Autoregressions</title>
      <link>https://arxiv.org/abs/2511.03097</link>
      <description>arXiv:2511.03097v1 Announce Type: new 
Abstract: The availability of multidimensional economic datasets has grown significantly in recent years. An example is bilateral trade values across goods among countries, comprising three dimensions -- importing countries, exporting countries, and goods -- forming a third-order tensor time series. This paper introduces a general Bayesian tensor autoregressive framework to analyze the dynamics of large, multidimensional time series with a particular focus on international trade across different countries and sectors. Departing from the standard homoscedastic assumption in this literature, we incorporate flexible stochastic volatility into the tensor autoregressive models. The proposed models can capture time-varying volatility due to the COVID-19 pandemic and recent outbreaks of war. To address computational challenges and mitigate overfitting, we develop an efficient sampling method based on low-rank Tucker decomposition and hierarchical shrinkage priors. Additionally, we provide a factor interpretation of the model showing how the Tucker decomposition projects large-dimensional disaggregated trade flows onto global factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03097v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaling Qi</dc:creator>
    </item>
    <item>
      <title>Unbiased Regression-Adjusted Estimation of Average Treatment Effects in Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2511.03236</link>
      <description>arXiv:2511.03236v1 Announce Type: new 
Abstract: This article introduces a leave-one-out regression adjustment estimator (LOORA) for estimating average treatment effects in randomized controlled trials. The method removes the finite-sample bias of conventional regression adjustment and provides exact variance expressions for LOORA versions of the Horvitz-Thompson and difference-in-means estimators under simple and complete random assignment. Ridge regularization limits the influence of high-leverage observations, improving stability and precision in small samples. In large samples, LOORA attains the asymptotic efficiency of regression-adjusted estimator as characterized by Lin (2013, Annals of Applied Statistics), while remaining exactly unbiased. To construct confidence intervals, we rely on asymptotic variance estimates that treat the estimator as a two-step procedure, accounting for both the regression adjustment and the random assignment stages. Two within-subject experimental applications that provide realistic joint distributions of potential outcomes as ground truth show that LOORA eliminates substantial biases and achieves close-to-nominal confidence interval coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03236v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alberto Abadie, Mehrdad Ghadiri, Ali Jadbabaie, Mahyar JafariNodeh</dc:creator>
    </item>
    <item>
      <title>Using spatial modeling to address covariate measurement error</title>
      <link>https://arxiv.org/abs/2511.03306</link>
      <description>arXiv:2511.03306v1 Announce Type: new 
Abstract: We propose a new estimation methodology to address the presence of covariate measurement error by exploiting the availability of spatial data. The approach uses neighboring observations as repeated measurements, after suitably controlling for the random distance between the observations in a way that allows the use of operator diagonalization methods to establish identification. The method is applicable to general nonlinear models with potentially nonclassical errors and does not rely on a priori distributional assumptions regarding any of the variables. The method's implementation combines a sieve semiparametric maximum likelihood with a first-step kernel estimator and simulation methods. The method's effectiveness is illustrated through both controlled simulations and an application to the assessment of the effect of pre-colonial political structure on current economic development in Africa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03306v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susanne M. Schennach, Vincent Starck</dc:creator>
    </item>
    <item>
      <title>Multivariate Ordered Discrete Response Models with Lattice Structures</title>
      <link>https://arxiv.org/abs/2511.03418</link>
      <description>arXiv:2511.03418v1 Announce Type: new 
Abstract: We analyze multivariate ordered discrete response models with a lattice structure, modeling decision makers who narrowly bracket choices across multiple dimensions. These models map latent continuous processes into discrete responses using functionally independent decision thresholds. In a semiparametric framework, we model latent processes as sums of covariate indices and unobserved errors, deriving conditions for identifying parameters, thresholds, and the joint cumulative distribution function of errors. For the parametric bivariate probit case, we separately derive identification of regression parameters and thresholds, and the correlation parameter, with the latter requiring additional covariate conditions. We outline estimation approaches for semiparametric and parametric models and present simulations illustrating the performance of estimators for lattice models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03418v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiana Komarova, William Matcham</dc:creator>
    </item>
    <item>
      <title>The moment is here: a generalised class of estimators for fuzzy regression discontinuity designs</title>
      <link>https://arxiv.org/abs/2511.03424</link>
      <description>arXiv:2511.03424v1 Announce Type: new 
Abstract: The standard fuzzy regression discontinuity (FRD) estimator is a ratio of differences of local polynomial estimators. I show that this estimator does not have finite moments of any order in finite samples, regardless of the choice of kernel function, bandwidth, or order of polynomial. This leads to an imprecise estimator with a heavy-tailed sampling distribution, and inaccurate inference with small sample sizes or when the discontinuity in the probability of treatment assignment at the cutoff is small. I present a generalised class of computationally simple FRD estimators, which contains a continuum of estimators with finite moments of all orders in finite samples, and nests both the standard FRD and sharp (SRD) estimators. The class is indexed by a single tuning parameter, and I provide simple values that lead to substantial improvements in median bias, median absolute deviation and root mean squared error. These new estimators remain very stable in small samples, or when the discontinuity in the probability of treatment assignment at the cutoff is small. Simple confidence intervals that have strong coverage and length properties in small samples are also developed. The improvements are seen across a wide range of models and using common bandwidth selection algorithms in extensive Monte Carlo simulations. The improved stability and performance of the estimators and confidence intervals is also demonstrated using data on class size effects on educational attainment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03424v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stuart Lane</dc:creator>
    </item>
    <item>
      <title>Leniency Designs: An Operator's Manual</title>
      <link>https://arxiv.org/abs/2511.03572</link>
      <description>arXiv:2511.03572v1 Announce Type: new 
Abstract: We develop a step-by-step guide to leniency (a.k.a. judge or examiner instrument) designs, drawing on recent econometric literatures. The unbiased jackknife instrumental variables estimator (UJIVE) is purpose-built for leveraging exogenous leniency variation, avoiding subtle biases even in the presence of many decision-makers or controls. We show how UJIVE can also be used to assess key assumptions underlying leniency designs, including quasi-random assignment and average first-stage monotonicity, and to probe the external validity of treatment effect estimates. We further discuss statistical inference, arguing that non-clustered standard errors are often appropriate. A reanalysis of Farre-Mensa et al. (2020), using quasi-random examiner assignment to estimate the value of patents to startups, illustrates our checklist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03572v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Goldsmith-Pinkham, Peter Hull, Michal Koles\'ar</dc:creator>
    </item>
    <item>
      <title>Testing Inequalities Linear in Nuisance Parameters</title>
      <link>https://arxiv.org/abs/2510.27633</link>
      <description>arXiv:2510.27633v1 Announce Type: cross 
Abstract: This paper proposes a new test for inequalities that are linear in possibly partially identified nuisance parameters. This type of hypothesis arises in a broad set of problems, including subvector inference for linear unconditional moment (in)equality models, specification testing of such models, and inference for parameters bounded by linear programs. The new test uses a two-step test statistic and a chi-squared critical value with data-dependent degrees of freedom that can be calculated by an elementary formula. Its simple structure and tuning-parameter-free implementation make it attractive for practical use. We establish uniform asymptotic validity of the test, demonstrate its finite-sample size and power in simulations, and illustrate its use in an empirical application that analyzes women's labor supply in response to a welfare policy reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27633v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Fletcher Cox, Xiaoxia Shi, Yuya Shimizu</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Synthetic Control: Ensuring Robustness Against Highly Correlated Controls and Weight Shifts</title>
      <link>https://arxiv.org/abs/2511.02632</link>
      <description>arXiv:2511.02632v1 Announce Type: cross 
Abstract: The synthetic control method estimates the causal effect by comparing the outcomes of a treated unit to a weighted average of control units that closely match the pre-treatment outcomes of the treated unit. This method presumes that the relationship between the potential outcomes of the treated and control units remains consistent before and after treatment. However, the estimator may become unreliable when these relationships shift or when control units are highly correlated. To address these challenges, we introduce the Distributionally Robust Synthetic Control (DRoSC) method by accommodating potential shifts in relationships and addressing high correlations among control units. The DRoSC method targets a new causal estimand defined as the optimizer of a worst-case optimization problem that checks through all possible synthetic weights that comply with the pre-treatment period. When the identification conditions for the classical synthetic control method hold, the DRoSC method targets the same causal effect as the synthetic control. When these conditions are violated, we show that this new causal estimand is a conservative proxy of the non-identifiable causal effect. We further show that the limiting distribution of the DRoSC estimator is non-normal and propose a novel inferential approach to characterize this non-normal limiting distribution. We demonstrate its finite-sample performance through numerical studies and an analysis of the economic impact of terrorism in the Basque Country.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02632v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taehyeon Koo, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>Privacy-aware identification</title>
      <link>https://arxiv.org/abs/2006.14732</link>
      <description>arXiv:2006.14732v3 Announce Type: replace 
Abstract: The paper redefines econometric identification under formal privacy constraints, particularly differential privacy (DP). Traditionally, econometrics focuses on point or partial identification, aiming to recover parameters precisely or within a deterministic set. However, DP introduces a fundamental challenge: information asymmetry between researchers and data curators results in DP outputs belonging to a potentially large collection of differentially private statistics, which is naturally described as a random set. Due to the finite-sample nature of the DP notion and mechanisms, identification must be reinterpreted as the ability to recover parameters in the limit of this random set. In the DP setting this limit may remain random which necessitates new theoretical tools, such as random set theory, to characterize parameter properties and practical methods, like proposed decision mappings by data curators, to restore point identification. We argue that privacy constraints push econometrics toward a broader framework where randomness and uncertainty are intrinsic features of identification, moving beyond classical approaches. By integrating DP, identification, and random sets, we offer a privacy-aware identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.14732v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiana Komarova, Denis Nekipelov</dc:creator>
    </item>
    <item>
      <title>Multivariate ordered discrete response models with two layers of dependence</title>
      <link>https://arxiv.org/abs/2205.05779</link>
      <description>arXiv:2205.05779v3 Announce Type: replace 
Abstract: We develop a class of multivariate ordered discrete response models featuring general rectangular structures, which allow for functionally interdependent thresholds across dimensions, extending beyond traditional (lattice) models that assume threshold independence. The new models incorporate two layers of dependence: one arising from the interdependence of decision rules (capturing broad bracketing behaviors) and another from the correlation of latent utilities conditional on observables. We provide microfoundations, explore semiparametric and parametric specifications, and establish identification conditions under logical consistency in decision-making. An empirical application to health insurance markets demonstrates the advantages of this new framework, showing how it disentangles moral hazard (captured via threshold dependence) from adverse selection (isolated in unobservable correlations), offering insights into behavioral responses obscured by lattice models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.05779v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatiana Komarova, William Matcham</dc:creator>
    </item>
    <item>
      <title>Tests of exogeneity in duration models with censored data</title>
      <link>https://arxiv.org/abs/2510.26613</link>
      <description>arXiv:2510.26613v2 Announce Type: replace 
Abstract: Consider the setting in which a researcher is interested in the causal effect of a treatment $Z$ on a duration time $T$, which is subject to right censoring. We assume that $T=\varphi(X,Z,U)$, where $X$ is a vector of baseline covariates, $\varphi(X,Z,U)$ is strictly increasing in the error term $U$ for each $(X,Z)$ and $U\sim \mathcal{U}[0,1]$. Therefore, the model is nonparametric and nonseparable. We propose nonparametric tests for the hypothesis that $Z$ is exogenous, meaning that $Z$ is independent of $U$ given $X$. The test statistics rely on an instrumental variable $W$ that is independent of $U$ given $X$. We assume that $X,W$ and $Z$ are all categorical. Test statistics are constructed for the hypothesis that the conditional rank $V_T= F_{T \mid X,Z}(T \mid X,Z)$ is independent of $(X,W)$ jointly. Under an identifiability condition on $\varphi$, this hypothesis is equivalent to $Z$ being exogenous. However, note that $V_T$ is censored by $V_C =F_{T \mid X,Z}(C \mid X,Z)$, which complicates the construction of the test statistics significantly. We derive the limiting distributions of the proposed tests and prove that our estimator of the distribution of $V_T$ converges to the uniform distribution at a rate faster than the usual parametric $n^{-1/2}$-rate. We demonstrate that the test statistics and bootstrap approximations for the critical values have a good finite sample performance in various Monte Carlo settings. Finally, we illustrate the tests with an empirical application to the National Job Training Partnership Act (JTPA) Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26613v2</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gilles Crommen, Ingrid Van Keilegom, Jean-Pierre Florens</dc:creator>
    </item>
    <item>
      <title>FARS: Factor Augmented Regression Scenarios in R</title>
      <link>https://arxiv.org/abs/2507.10679</link>
      <description>arXiv:2507.10679v5 Announce Type: replace-cross 
Abstract: In the context of macroeconomic/financial time series, the FARS package provides a comprehensive framework in R for the construction of conditional densities of the variable of interest based on the factor-augmented quantile regressions (FA-QRs) methodology, with the factors extracted from multi-level dynamic factor models (ML-DFMs) with potential overlapping group-specific factors. Furthermore, the package also allows the construction of measures of risk as well as modeling and designing economic scenarios based on the conditional densities. In particular, the package enables users to: (i) extract global and group-specific factors using a flexible multi-level factor structure; (ii) compute asymptotically valid confidence regions for the estimated factors, accounting for uncertainty in the factor loadings; (iii) obtain estimates of the parameters of the FA-QRs together with their standard deviations; (iv) recover full predictive conditional densities from estimated quantiles; (v) obtain risk measures based on extreme quantiles of the conditional densities; and (vi) estimate the conditional density and the corresponding extreme quantiles when the factors are stressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10679v5</guid>
      <category>stat.CO</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gian Pietro Bellocca, Ignacio Garr\'on, Vladimir Rodr\'iguez-Caballero, Esther Ruiz</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap between Empirical Welfare Maximization and Conditional Average Treatment Effect Estimation in Policy Learning</title>
      <link>https://arxiv.org/abs/2510.26723</link>
      <description>arXiv:2510.26723v2 Announce Type: replace-cross 
Abstract: The goal of policy learning is to train a policy function that recommends a treatment given covariates to maximize population welfare. There are two major approaches in policy learning: the empirical welfare maximization (EWM) approach and the plug-in approach. The EWM approach is analogous to a classification problem, where one first builds an estimator of the population welfare, which is a functional of policy functions, and then trains a policy by maximizing the estimated welfare. In contrast, the plug-in approach is based on regression, where one first estimates the conditional average treatment effect (CATE) and then recommends the treatment with the highest estimated outcome. This study bridges the gap between the two approaches by showing that both are based on essentially the same optimization problem. In particular, we prove an exact equivalence between EWM and least squares over a reparameterization of the policy class. As a consequence, the two approaches are interchangeable in several respects and share the same theoretical guarantees under common conditions. Leveraging this equivalence, we propose a regularization method for policy learning. The reduction to least squares yields a smooth surrogate that is typically easier to optimize in practice. At the same time, for many natural policy classes the inherent combinatorial hardness of exact EWM generally remains, so the reduction should be viewed as an optimization aid rather than a universal bypass of NP-hardness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26723v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
  </channel>
</rss>
