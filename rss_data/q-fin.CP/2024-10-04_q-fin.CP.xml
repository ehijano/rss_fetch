<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.CP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.CP</link>
    <description>q-fin.CP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.CP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Oct 2024 04:06:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Nonnegativity preserving convolution kernels. Application to Stochastic Volterra Equations in closed convex domains and their approximation</title>
      <link>https://arxiv.org/abs/2302.07758</link>
      <description>arXiv:2302.07758v2 Announce Type: replace-cross 
Abstract: This work defines and studies one-dimensional convolution kernels that preserve nonnegativity. When the past dynamics of a process is integrated with a convolution kernel like in Stochastic Volterra Equations or in the jump intensity of Hawkes processes, this property allows to get the nonnegativity of the integral. We give characterizations of these kernels and show in particular that completely monotone kernels preserve nonnegativity. We then apply these results to analyze the stochastic invariance of a closed convex set by Stochastic Volterra Equations. We also get a comparison result in dimension one. Last, when the kernel is a positive linear combination of decaying exponential functions, we present a second order approximation scheme for the weak error that stays in the closed convex domain under suitable assumptions. We apply these results to the rough Heston model and give numerical illustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07758v2</guid>
      <category>math.PR</category>
      <category>q-fin.CP</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aur\'elien Alfonsi</dc:creator>
    </item>
    <item>
      <title>Regret-Optimal Federated Transfer Learning for Kernel Regression with Applications in American Option Pricing</title>
      <link>https://arxiv.org/abs/2309.04557</link>
      <description>arXiv:2309.04557v2 Announce Type: replace-cross 
Abstract: We propose an optimal iterative scheme for federated transfer learning, where a central planner has access to datasets ${\cal D}_1,\dots,{\cal D}_N$ for the same learning model $f_{\theta}$. Our objective is to minimize the cumulative deviation of the generated parameters $\{\theta_i(t)\}_{t=0}^T$ across all $T$ iterations from the specialized parameters $\theta^\star_{1},\ldots,\theta^\star_N$ obtained for each dataset, while respecting the loss function for the model $f_{\theta(T)}$ produced by the algorithm upon halting. We only allow for continual communication between each of the specialized models (nodes/agents) and the central planner (server), at each iteration (round). For the case where the model $f_{\theta}$ is a finite-rank kernel regression, we derive explicit updates for the regret-optimal algorithm. By leveraging symmetries within the regret-optimal algorithm, we further develop a nearly regret-optimal heuristic that runs with $\mathcal{O}(Np^2)$ fewer elementary operations, where $p$ is the dimension of the parameter space. Additionally, we investigate the adversarial robustness of the regret-optimal algorithm showing that an adversary which perturbs $q$ training pairs by at-most $\varepsilon&gt;0$, across all training sets, cannot reduce the regret-optimal algorithm's regret by more than $\mathcal{O}(\varepsilon q \bar{N}^{1/2})$, where $\bar{N}$ is the aggregate number of training pairs. To validate our theoretical findings, we conduct numerical experiments in the context of American option pricing, utilizing a randomly generated finite-rank kernel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04557v2</guid>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <category>q-fin.CP</category>
      <pubDate>Fri, 04 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuwei Yang, Anastasis Kratsios, Florian Krach, Matheus Grasselli, Aurelien Lucchi</dc:creator>
    </item>
  </channel>
</rss>
