<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Dec 2025 02:39:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Predicting COVID-19 Prevalence Using Wastewater RNA Surveillance: A Semi-Supervised Learning Approach with Temporal Feature Trust</title>
      <link>https://arxiv.org/abs/2512.00100</link>
      <description>arXiv:2512.00100v1 Announce Type: new 
Abstract: As COVID-19 transitions into an endemic disease that remains constantly present in the population at a stable level, monitoring its prevalence without invasive measures becomes increasingly important. In this paper, we present a deep neural network estimator for the COVID-19 daily case count based on wastewater surveillance data and other confounding factors. This work builds upon the study by Jiang, Kolozsvary, and Li (2024), which connects the COVID-19 case counts with testing data collected early in the pandemic. Using the COVID-19 testing data and the wastewater surveillance data during the period when both data were highly reliable, one can train an artificial neural network that learns the nonlinear relation between the COVID-19 daily case count and the wastewater viral RNA concentration. From a machine learning perspective, the main challenge lies in addressing temporal feature reliability, as the training data has different reliability over different time periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00100v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Chen, Eric Liang</dc:creator>
    </item>
    <item>
      <title>RadDiff: Retrieval-Augmented Denoising Diffusion for Protein Inverse Folding</title>
      <link>https://arxiv.org/abs/2512.00126</link>
      <description>arXiv:2512.00126v1 Announce Type: new 
Abstract: Protein inverse folding, the design of an amino acid sequence based on a target 3D structure, is a fundamental problem of computational protein engineering. Existing methods either generate sequences without leveraging external knowledge or relying on protein language models (PLMs). The former omits the evolutionary information stored in protein databases, while the latter is parameter-inefficient and inflexible to adapt to ever-growing protein data. To overcome the above drawbacks, in this paper we propose a novel method, called retrieval-augmented denoising diffusion (RadDiff), for protein inverse folding. Given the target protein backbone, RadDiff uses a hierarchical search strategy to efficiently retrieve structurally similar proteins from large protein databases. The retrieved structures are then aligned residue-by-residue to the target to construct a position-specific amino acid profile, which serves as an evolutionary-informed prior that conditions the denoising process. A lightweight integration module is further designed to incorporate this prior effectively. Experimental results on the CATH, PDB, and TS50 datasets show that RadDiff consistently outperforms existing methods, improving sequence recovery rate by up to 19%. Experimental results also demonstrate that RadDiff generates highly foldable sequences and scales effectively with database size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00126v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jin Han, Tianfan Fu, Wu-Jun Li</dc:creator>
    </item>
    <item>
      <title>HyperADRs: A Hierarchical Hypergraph Framework for Drug-Gene-ADR Prediction</title>
      <link>https://arxiv.org/abs/2512.00137</link>
      <description>arXiv:2512.00137v1 Announce Type: new 
Abstract: Adverse drug reactions (ADRs) are a major barrier to safe and effective pharmacotherapy and increasingly reflect higher order interactions between drugs, genetic background, and clinical phenotypes. Existing graph based approaches usually predict ADRs as properties of drugs or drug pairs, leaving the causal gene implicit and limiting their value for pharmacogenomic decision making. We introduce HyperADRs, a hierarchical hypergraph framework that predicts ADR risk at the level of drug-gene-ADR triads. Starting from curated pharmacogenomic annotations in PharmGKB and the pharmacogenomics subdatabase of DrugBank, we construct high confidence triplets and integrate them with auxiliary molecular, functional, and disease relations from precision-medicine-oriented knowledge graphs. Drugs, genes, and ADR concepts are embedded with modality appropriate pretrained models (UniMol, ESM2, SapBERT) and propagated through a hypergraph convolutional network. A FiLM based, query conditioned contrastive learning module learns context specific representations so that, given any two entities, the model retrieves the correct third entity against many candidates. To improve robustness and interpretability, we propose a nine category ADR macro system scheme that reduces large heterogeneous "other" bins while aligning with organ system reasoning in clinical pharmacology. Across drug-, gene-, and ADR-held-out evaluations on PharmGKB, HyperADRs matches or exceeds strong baselines on ranking based metrics. When trained on PharmGKB and tested on unseen DrugBank triplets, HyperADRs maintains its ranking advantage, indicating that the learned representations capture transferable biological mechanisms and can support mechanistically grounded pharmacogenomic hypothesis generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00137v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ze Cai, Haotian Tang, Shuai Gao, Binbin Zhou, Junhan Zhao, Jun Wen</dc:creator>
    </item>
    <item>
      <title>Layer Probing Improves Kinase Functional Prediction with Protein Language Models</title>
      <link>https://arxiv.org/abs/2512.00376</link>
      <description>arXiv:2512.00376v1 Announce Type: new 
Abstract: Protein language models (PLMs) have transformed sequence-based protein analysis, yet most applications rely only on final-layer embeddings, which may overlook biologically meaningful information encoded in earlier layers. We systematically evaluate all 33 layers of ESM-2 for kinase functional prediction using both unsupervised clustering and supervised classification. We show that mid-to-late transformer layers (layers 20-33) outperform the final layer by 32 percent in unsupervised Adjusted Rand Index and improve homology-aware supervised accuracy to 75.7 percent. Domain-level extraction, calibrated probability estimates, and a reproducible benchmarking pipeline further strengthen reliability. Our results demonstrate that transformer depth contains functionally distinct biological signals and that principled layer selection significantly improves kinase function prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00376v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajit Kumar, IndraPrakash Jha</dc:creator>
    </item>
    <item>
      <title>Tissue stress measurements with Bayesian Inversion Stress Microscopy</title>
      <link>https://arxiv.org/abs/2512.00550</link>
      <description>arXiv:2512.00550v1 Announce Type: new 
Abstract: Cells within biological tissue are constantly subjected to dynamic mechanical forces. Measuring the internal stress of tissues has proven crucial for our understanding of the role of mechanical forces in fundamental biological processes like morphogenesis, collective migration, cell division or cell elimination and death. Previously, we have introduced Bayesian Inversion Stress Microscopy (BISM), which is relying on measuring cell-generated traction forces in vitro and has proven particularly useful to measure absolute stresses in confined cell monolayers. We further demonstrate the applicability and robustness of BISM across various experimental settings with different boundary conditions, ranging from confined tissues of arbitrary shape to monolayers composed of different cell types. Importantly, BISM does not require assumptions on cell rheology. Therefore, it can be applied to complex heterogeneous tissues consisting of different cell types, as long as they can be grown on a flat substrate. Finally, we compare BISM to other common stress measurement techniques using a coherent experimental setup, followed by a discussion on its limitations and further perspectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00550v1</guid>
      <category>q-bio.QM</category>
      <category>physics.bio-ph</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L. Anger, A. Schoenit, F. Wodrascka, C. Ross\'e, R. M. M\`ege, B. Ladoux, P. Marcq</dc:creator>
    </item>
    <item>
      <title>Disentangling Proxies of Demographic Adjustments in Clinical Equations</title>
      <link>https://arxiv.org/abs/2512.00905</link>
      <description>arXiv:2512.00905v1 Announce Type: new 
Abstract: The use of coarse demographic adjustments in clinical equations has been increasingly scrutinized. In particular, adjustments for race have sparked significant debate with several medical professional societies recommending race-neutral equations in recent years. However, current approaches to remove race from clinical equations do not address the underlying causes of observed differences. Here, we present ARC (Approach for identifying pRoxies of demographic Correction), a framework to identify explanatory factors of group-level differences, which may inform the development of more accurate and precise clinical equations. We apply ARC to spirometry tests across two observational cohorts, CDC NHANES and UK Biobank, comprising 159,893 participants. Cross-sectional sociodemographic or exposure measures did not explain differences in reference lung function across race groups beyond those already explained by age, sex, and height. By contrast, sitting height accounted for up to 26% of the remaining differences in lung volumes between healthy Black and White adults. We then demonstrate how pulmonary function test (PFT) reference equations can incorporate these factors in a new set of equations called $ARC_{PFT}$, surpassing the predictive performance of the race-neutral GLI-Global equation recommended by major pulmonary societies. When compared to GLI-Global, inclusion of sitting height and waist circumference in $ARC_{PFT}$ decreased mean absolute error by 13% among Black participants in the UK Biobank and by 24% in NHANES. $ARC_{PFT}$ also had reduced vulnerability to domain shift compared to race-based methods, with mean absolute error 19.3% and 35.6% lower than race-stratified models in out-of-sample Asian and Hispanic populations, respectively. This approach provides a path for understanding the proxies of imprecise demographic adjustments and developing personalized clinical equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00905v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aashna P. Shah, James A. Diao, Emma Pierson, Chirag J. Patel, Arjun K. Manrai</dc:creator>
    </item>
    <item>
      <title>Sleep Apnea Detection on a Wireless Multimodal Wearable Device Without Oxygen Flow Using a Mamba-based Deep Learning Approach</title>
      <link>https://arxiv.org/abs/2512.00989</link>
      <description>arXiv:2512.00989v1 Announce Type: new 
Abstract: Objectives: We present and evaluate a Mamba-based deep-learning model for diagnosis and event-level characterization of sleep disordered breathing based on signals from the ANNE One, a non-intrusive dual-module wireless wearable system measuring chest electrocardiography, triaxial accelerometry, chest and finger temperature, and finger phototplethysmography.
  Methods: We obtained concurrent PSG and wearable sensor recordings from 384 adults attending a tertiary care sleep laboratory. Respiratory events in the PSG were manually annotated in accordance with AASM guidelines. Wearable sensor and PSG recordings were automatically aligned based on the ECG signal, alignment confirmed by visual inspection, and PSG-derived respiratory event labels were used to train and evaluate a deep sequential neural network based on the Mamba architecture.
  Results: In 57 recordings in our test set (mean age 56, mean AHI 10.8, 43.86\% female) the model-predicted AHI was highly correlated with that derived form the PSG labels (R=0.95, p=8.3e-30, men absolute error 2.83). This performance did not vary with age or sex. At a threshold of AHI$&gt;$5, the model had a sensitivity of 0.96, specificity of 0.87, and kappa of 0.82, and at a threshold of AHI$&gt;$15, the model had a sensitivity of 0.86, specificity of 0.98, and kappa of 0.85. At the level of 30-sec epochs, the model had a sensitivity of 0.93 and specificity of 0.95, with a kappa of 0.68 regarding whether any given epoch contained a respiratory event.
  Conclusions: Applied to data from the ANNE One, a Mamba-based deep learning model can accurately predict AHI and identify SDB at clinically relevant thresholds, achieves good epoch- and event-level identification of individual respiratory events, and shows promise at physiological characterization of these events including event type (central vs. other) and event duration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00989v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominik Luszczynski, Richard Fei Yin, Nicholas Afonin, Andrew S. P. Lim</dc:creator>
    </item>
    <item>
      <title>Personalized optimization of pediatric HD-tDCS for dose consistency and target engagement</title>
      <link>https://arxiv.org/abs/2512.01406</link>
      <description>arXiv:2512.01406v1 Announce Type: new 
Abstract: High-definition transcranial direct current stimulation (HD-tDCS) dosing in children remains largely empirical, relying on one-size-fits-all protocols despite rapid developmental changes in head anatomy and tissue properties that strongly modulate how currents reach the developing brain. Using 70 pediatric head models and commonly used cortical targets, our forward simulations find that standard montages produce marked age-dependent reductions in target electric-field intensity and systematic sex differences linked to tissue-volume covariation, underscoring the profound limitations of conventional uniform montages. To overcome these limitations, we introduce a developmentally informed, dual-objective optimization framework designed to generate personalized Pareto fronts summarizing the trade-off between electric-field intensity and focality. From these optimized solutions, we derive two practical dosing prescriptions: a dose-consistency strategy that, for the first time, enforces fixed target intensity across individuals to implicitly mitigate demographic effects, and a target-engagement strategy that maximizes target intensity under safety limits. Both strategies remain robust to large conductivity variations, and we further show that dense HD-tDCS solutions admit sparse equivalents without performance loss under the target-engagement strategy. We also find that tissue conductivity sensitivity is depth-dependent, with Pareto-front distributions for superficial cortical targets most influenced by gray matter, scalp, and bone conductivities, and those for a deep target predominantly shaped by gray and white matter conductivities. Together, these results establish a principled framework for pediatric HD-tDCS planning that explicitly accounts for developmental anatomy and physiological uncertainty, enabling reliable and individualized neuromodulation dosing in pediatric populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01406v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeming Liu, Mo Wang, Xuanye Pan, Yuan Yang, Wilson Truccolo, Quanying Liu</dc:creator>
    </item>
    <item>
      <title>A robust generalizable device-agnostic deep learning model for sleep-wake determination from triaxial wrist accelerometry</title>
      <link>https://arxiv.org/abs/2512.01986</link>
      <description>arXiv:2512.01986v1 Announce Type: new 
Abstract: Study Objectives: Wrist accelerometry is widely used for inferring sleep-wake state. Previous works demonstrated poor wake detection, without cross-device generalizability and validation in different age range and sleep disorders. We developed a robust deep learning model for to detect sleep-wakefulness from triaxial accelerometry and evaluated its validity across three devices and in a large adult population spanning a wide range of ages with and without sleep disorders. Methods: We collected wrist accelerometry simultaneous to polysomnography (PSG) in 453 adults undergoing clinical sleep testing at a tertiary care sleep laboratory, using three devices. We extracted features in 30-second epochs and trained a 3-class model to detect wake, sleep, and sleep with arousals, which was then collapsed into wake vs. sleep using a decision tree. To enhance wake detection, the model was specifically trained on randomly selected subjects with low sleep efficiency and/or high arousal index from one device recording and then tested on the remaining recordings. Results: The model showed high performance with F1 Score of 0.86, sensitivity (sleep) of 0.87, and specificity (wakefulness) of 0.78, and significant and moderate correlation to PSG in predicting total sleep time (R=0.69) and sleep efficiency (R=0.63). Model performance was robust to the presence of sleep disorders, including sleep apnea and periodic limb movements in sleep, and was consistent across all three models of accelerometer. Conclusions: We present a deep model to detect sleep-wakefulness from actigraphy in adults with relative robustness to the presence of sleep disorders and generalizability across diverse commonly used wrist accelerometers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01986v1</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nasim Montazeri, Stone Yang, Dominik Luszczynski, John Zhang, Dharmendra Gurve, Andrew Centen, Maged Goubran, Andrew Lim</dc:creator>
    </item>
    <item>
      <title>AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI</title>
      <link>https://arxiv.org/abs/2512.00194</link>
      <description>arXiv:2512.00194v1 Announce Type: cross 
Abstract: We introduce EEG Autoclean Vision Language AI (ICVision) a first-of-its-kind system that emulates expert-level EEG ICA component classification through AI-agent vision and natural language reasoning. Unlike conventional classifiers such as ICLabel, which rely on handcrafted features, ICVision directly interprets ICA dashboard visualizations topography, time series, power spectra, and ERP plots, using a multimodal large language model (GPT-4 Vision). This allows the AI to see and explain EEG components the way trained neurologists do, making it the first scientific implementation of AI-agent visual cognition in neurophysiology. ICVision classifies each component into one of six canonical categories (brain, eye, heart, muscle, channel noise, and other noise), returning both a confidence score and a human-like explanation. Evaluated on 3,168 ICA components from 124 EEG datasets, ICVision achieved k = 0.677 agreement with expert consensus, surpassing MNE ICLabel, while also preserving clinically relevant brain signals in ambiguous cases. Over 97% of its outputs were rated as interpretable and actionable by expert reviewers. As a core module of the open-source EEG Autoclean platform, ICVision signals a paradigm shift in scientific AI, where models do not just classify, but see, reason, and communicate. It opens the door to globally scalable, explainable, and reproducible EEG workflows, marking the emergence of AI agents capable of expert-level visual decision-making in brain science and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00194v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Conference ICMI2026</arxiv:journal_reference>
      <dc:creator>Zag ElSayed, Grace Westerkamp, Gavin Gammoh, Yanchen Liu, Peyton Siekierski, Craig Erickson, Ernest Pedapati</dc:creator>
    </item>
    <item>
      <title>BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models</title>
      <link>https://arxiv.org/abs/2512.00283</link>
      <description>arXiv:2512.00283v2 Announce Type: cross 
Abstract: Foundation models have revolutionized various fields such as natural language processing (NLP) and computer vision (CV). While efforts have been made to transfer the success of the foundation models in general AI domains to biology, existing works focus on directly adopting the existing foundation model architectures from general machine learning domains without a systematic design considering the unique physicochemical and structural properties of each biological data modality. This leads to suboptimal performance, as these repurposed architectures struggle to capture the long-range dependencies, sparse information, and complex underlying ``grammars'' inherent to biological data. To address this gap, we introduce BioArc, a novel framework designed to move beyond intuition-driven architecture design towards principled, automated architecture discovery for biological foundation models. Leveraging Neural Architecture Search (NAS), BioArc systematically explores a vast architecture design space, evaluating architectures across multiple biological modalities while rigorously analyzing the interplay between architecture, tokenization, and training strategies. This large-scale analysis identifies novel, high-performance architectures, allowing us to distill a set of empirical design principles to guide future model development. Furthermore, to make the best of this set of discovered principled architectures, we propose and compare several architecture prediction methods that effectively and efficiently predict optimal architectures for new biological tasks. Overall, our work provides a foundational resource and a principled methodology to guide the creation of the next generation of task-specific and foundation models for biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00283v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yi Fang, Haoran Xu, Jiaxin Han, Sirui Ding, Yizhi Wang, Yue Wang, Xuan Wang</dc:creator>
    </item>
    <item>
      <title>Rep3Net: An Approach Exploiting Multimodal Representation for Molecular Bioactivity Prediction</title>
      <link>https://arxiv.org/abs/2512.00521</link>
      <description>arXiv:2512.00521v1 Announce Type: cross 
Abstract: In early stage drug discovery, bioactivity prediction of molecules against target proteins plays a crucial role. Trdaitional QSAR models that utilizes molecular descriptor based data often struggles to predict bioactivity of molecules effectively due to its limitation in capturing structural and contextual information embedded within each compound. To address this challenge, we propose Rep3Net, a unified deep learning architecture that not only incorporates descriptor data but also includes spatial and relational information through graph-based represenation of compounds and contextual information through ChemBERTa generated embeddings from SMILES strings. Our model employing multimodal concatenated features produce reliable bioactivity prediction on Poly [ADP-ribose] polymerase 1 (PARP-1) dataset. PARP-1 is a crucial agent in DNA damage repair and has become a significant theraputic target in malignancies that depend on it for survival and growth. A comprehensive analysis and comparison with conventional standalone models including GCN, GAT, XGBoost, etc. demonstrates that our architecture achieves the highest predictive performance. In computational screening of compounds in drug discovery, our architecture provides a scalable framework for bioactivity prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00521v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabrina Islam, Md. Atiqur Rahman, Md. Bakhtiar Hasan, Md. Hasanul Kabir</dc:creator>
    </item>
    <item>
      <title>Statistical NLP for Optimization of Clinical Trial Success Prediction in Pharmaceutical R&amp;D</title>
      <link>https://arxiv.org/abs/2512.00586</link>
      <description>arXiv:2512.00586v1 Announce Type: cross 
Abstract: This work presents the development and evaluation of an NLP-enabled probabilistic classifier designed to estimate the probability of technical and regulatory success (pTRS) for clinical trials in the field of neuroscience. While pharmaceutical R&amp;D is plagued by high attrition rates and enormous costs, particularly within neuroscience, where success rates are below 10%, timely identification of promising programs can streamline resource allocation and reduce financial risk. Leveraging data from the ClinicalTrials.gov database and success labels from the recently developed Clinical Trial Outcome dataset, the classifier extracts text-based clinical trial features using statistical NLP techniques. These features were integrated into several non-LLM frameworks (logistic regression, gradient boosting, and random forest) to generate calibrated probability scores. Model performance was assessed on a retrospective dataset of 101,145 completed clinical trials spanning 1976-2024, achieving an overall ROC-AUC of 0.64. An LLM-based predictive model was then built using BioBERT, a domain-specific language representation encoder. The BioBERT-based model achieved an overall ROC-AUC of 0.74 and a Brier Score of 0.185, indicating its predictions had, on average, 40% less squared error than would be observed using industry benchmarks. The BioBERT-based model also made trial outcome predictions that were superior to benchmark values 70% of the time overall. By integrating NLP-driven insights into drug development decision-making, this work aims to enhance strategic planning and optimize investment allocation in neuroscience programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00586v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Doane</dc:creator>
    </item>
    <item>
      <title>Associative Syntax and Maximal Repetitions reveal context-dependent complexity in fruit bat communication</title>
      <link>https://arxiv.org/abs/2512.01033</link>
      <description>arXiv:2512.01033v1 Announce Type: cross 
Abstract: This study presents an unsupervised method to infer discreteness, syntax and temporal structures of fruit-bats vocalizations, as a case study of graded vocal systems, and evaluates the complexity of communication patterns in relation with behavioral context. The method improved the baseline for unsupervised labeling of vocal units (i.e. syllables) through manifold learning, by investigating how dimen- sionality reduction on mel-spectrograms affects labeling, and comparing it with unsupervised labels based on acoustic similarity. We then encoded vocalizations as syllabic sequences to analyze the type of syntax, and extracted the Maximal Repetitions (MRs) to evaluate syntactical structures. We found evidence for: i) associative syntax, rather than combinatorial (context classification is unaffected by permutation of sequences, F 1 &gt; 0.9); ii) context-dependent use of syllables (Wilcoxon rank-sum tests, p-value &lt; 0.05); iii) heavy-tail distribution of MRs (truncated power-law, exponent {\alpha} &lt; 2), indicative of mechanism encoding com- binatorial complexity. Analysis of MRs and syllabic transition networks revealed that mother-pupil interactions were characterized by repetitions, while commu- nication in conflict-contexts exhibited higher complexity (longer MRs and more interconnected vocal sequences) than non-agonistic contexts. We propose that communicative complexity is higher in scenarios of disagreement, reflecting lower compressibility of information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01033v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luigi Assom</dc:creator>
    </item>
    <item>
      <title>COVID-19 Forecasting from U.S. Wastewater Surveillance Data: A Retrospective Multi-Model Study (2022-2024)</title>
      <link>https://arxiv.org/abs/2512.01074</link>
      <description>arXiv:2512.01074v1 Announce Type: cross 
Abstract: Accurate and reliable forecasting models are critical for guiding public health responses and policy decisions during pandemics such as COVID-19. Retrospective evaluation of model performance is essential for improving epidemic forecasting capabilities. In this study, we used COVID-19 wastewater data from CDC's National Wastewater Surveillance System to generate sequential weekly retrospective forecasts for the United States from March 2022 through September 2024, both at the national level and for four major regions (Northeast, Midwest, South, and West). We produced 133 weekly forecasts using 11 models, including ARIMA, generalized additive models (GAM), simple linear regression (SLR), Prophet, and the n-sub-epidemic framework (top-ranked, weighted-ensemble, and unweighted-ensemble variants). Forecast performance was assessed using mean absolute error (MAE), mean squared error (MSE), weighted interval score (WIS), and 95% prediction interval coverage. The n-sub-epidemic unweighted ensembles outperformed all other models at 3-4-week horizons, particularly at the national level and in the Midwest and West. ARIMA and GAM performed best at 1-2-week horizons in most regions, whereas Prophet and SLR consistently underperformed across regions and horizons. These findings highlight the value of region-specific modeling strategies and demonstrate the utility of the n-sub-epidemic framework for real-time outbreak forecasting using wastewater surveillance data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01074v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faharudeen Alhassan, Hamed Karami, Amanda Bleichrodt, James M. Hyman, Isaac C. H. Fung, Ruiyan Luo, Gerardo Chowell</dc:creator>
    </item>
    <item>
      <title>Robustness and resilience of dynamical networks in biology and epidemiology</title>
      <link>https://arxiv.org/abs/2512.01462</link>
      <description>arXiv:2512.01462v1 Announce Type: cross 
Abstract: Natural systems are remarkably robust and resilient, maintaining essential functions despite variability, uncertainty, and hostile conditions. Understanding these nonlinear, dynamic behaviours is challenging because such systems involve many interacting parameters, yet it is crucial for explaining processes from cellular regulation to disease onset and epidemic spreading. Robustness and resilience describe a system's ability to preserve and recover desired behaviours in the presence of intrinsic and extrinsic fluctuations. This survey reviews how different disciplines define these concepts, examines methods for assessing whether key properties of uncertain, networked dynamical systems are structural (parameter-free) or robust (preserved for parameter variations within an uncertainty bounding set), and discusses integrated structural and probabilistic techniques for biological and epidemiological models. The text introduces formal definitions of resilience for families of systems obtained by adding stochastic perturbations to a nominal deterministic model, enabling a probabilistic characterisation of the ability to remain within or return to a prescribed attractor. These definitions generalise probabilistic robustness and shed new light on classical biological examples. In addition, the survey summarises resilience indicators and data-driven tools for detecting resilience loss and regime shifts, drawing on bifurcation analysis to anticipate qualitative changes in system behaviour. Together, these methodologies support the study and control of complex natural systems, guiding the design of biomolecular feedback architectures, the identification of therapeutic targets, the forecasting and management of epidemics, and the detection of tipping points in ecological and biological networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01462v1</guid>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1561/260000003.</arxiv:DOI>
      <dc:creator>Daniele Proverbio, Rami Katz, Giulia Giordano</dc:creator>
    </item>
    <item>
      <title>From Toggle to Tuning: Controlling Turing Patterns in Gene Circuits</title>
      <link>https://arxiv.org/abs/2512.01652</link>
      <description>arXiv:2512.01652v1 Announce Type: cross 
Abstract: Controlling spatial patterns in synthetic biological systems remains challenging due to poor parameter robustness and limited experimental tunability. We introduce two complementary mechanisms-the pattern switch and the pattern dial-to systematically control Turing pattern formation in gene circuits. The switch toggles pattern onset via a single parameter, while the dial enables transitions between distinct pattern types using weakly nonlinear amplitude equations. Analyzing network size reveals a key trade-off: small networks are easier to control but less robust, while larger networks gain robustness at the cost of tunability-suggesting a sweet spot for both evolvability and designability. Our results offer practical design rules for engineering programmable patterns in living systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01652v1</guid>
      <category>physics.bio-ph</category>
      <category>math-ph</category>
      <category>math.DS</category>
      <category>math.MP</category>
      <category>nlin.PS</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Antonio Matas-Gil, Robert G. Endres</dc:creator>
    </item>
    <item>
      <title>TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</title>
      <link>https://arxiv.org/abs/2512.01885</link>
      <description>arXiv:2512.01885v1 Announce Type: cross 
Abstract: Tracking cells in time-lapse videos is an essential technique for monitoring cell population dynamics at a single-cell level. Current methods for cell tracking are developed on videos with mostly single, constant signals and do not detect pivotal events such as cell death. Here, we present TransientTrack, a deep learning-based framework for cell tracking in multi-channel microscopy video data with transient fluorescent signals that fluctuate over time following processes such as the circadian rhythm of cells. By identifying key cellular events - mitosis (cell division) and apoptosis (cell death) our method allows us to build complete trajectories, including cell lineage information. TransientTrack is lightweight and performs matching on cell detection embeddings directly, without the need for quantification of tracking-specific cell features. Furthermore, our approach integrates Transformer Networks, multi-stage matching using all detection boxes, and the interpolation of missing tracklets with the Kalman Filter. This unified framework achieves strong performance across diverse conditions, effectively tracking cells and capturing cell division and death. We demonstrate the use of TransientTrack in an analysis of the efficacy of a chemotherapeutic drug at a single-cell level. The proposed framework could further advance quantitative studies of cancer cell dynamics, enabling detailed characterization of treatment response and resistance mechanisms. The code is available at https://github.com/bozeklab/TransientTrack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01885v1</guid>
      <category>cs.CV</category>
      <category>q-bio.CB</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian B\"urger, Martim Dias Gomes, Nica Gutu, Adri\'an E. Granada, No\'emie Moreau, Katarzyna Bozek</dc:creator>
    </item>
    <item>
      <title>Velocity Spectrum Imaging using velocity encoding preparation pulses</title>
      <link>https://arxiv.org/abs/2508.20218</link>
      <description>arXiv:2508.20218v2 Announce Type: replace 
Abstract: Purpose: The goal of this article is to introduce a technique to measure the velocity distribution of water inside each voxel of an MR image. The method is based on the use of motion sensitizing gradients with changing first moment to encode velocity. As such, it is completely non-invasive and requires no contrast injections.
  Methods: The technique consists of acquiring a series of images preceded by preparatory RF pulses that encode velocity information, analogously to k-space encoding. The velocity distribution can be decoded via the Fourier transform. We demonstrate its use on a simple flow phantom with known flow characteristics. We demonstrate the technique on the brains of five human participants from whom we collected the velocity distribution along each of the three laboratory axes.
  Results: Velocity distribution measurements on simple phantoms yielded velocity distributions consistent with theory. Human velocity spectra identified specific anatomical features at different velocity bins. The largest fraction of spins was in the lowest velocity bands. Movement in the CSF spaces could be clearly identified at different velocity bands.
  Conclusion: Velocity Spectrum Imaging has great potential as a tool to study the movement of fluids in the human body without contrast agents. In addition to a useful tool for validating computational fluid dynamic models in vivo, it can be used to study the complex movement of water in the glymphatic system and its involvement in neurodegenerative disorders. However, further development is needed to probe the velocity spectrum in the ultra-low velocity regime of the perivascular spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20218v2</guid>
      <category>q-bio.QM</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis Hernandez-Garcia, Alberto L. Vazquez, Doug C. Noll</dc:creator>
    </item>
    <item>
      <title>Prediction of Distant Metastasis in Head and Neck Cancer Patients Using Tumor and Peritumoral Multi-Modal Deep Learning</title>
      <link>https://arxiv.org/abs/2508.20469</link>
      <description>arXiv:2508.20469v2 Announce Type: replace 
Abstract: Although the combined treatment of surgery, radiotherapy, chemotherapy, and emerging target therapy has significantly improved the outcomes of patients with head and neck cancer, distant metastasis remains the leading cause of treatment failure. In this study, we propose a deep learning-based multimodal framework integrating CT imaging, radiomics, and clinical data to predict metastasis risk in HNSCC. A total of 1497 patients were retrospectively analyzed. Tumor and organ masks were generated from pretreatment CT scans, from which a 3D Swin Transformer extracted deep imaging features, while 1562 radiomics features were reduced to 36 via correlation filtering and random forest selection. Clinical data (age, sex, smoking, and alcohol status) were encoded and fused with imaging features, and the multimodal representation was fed into a fully connected network for prediction. Five-fold cross-validation was used to assess performance via AUC, accuracy, sensitivity, and specificity. The multimodal model outperformed all single-modality baselines. The deep learning module alone achieved an AUC of 0.715, whereas multimodal fusion significantly improved performance (AUC = 0.803, ACC = 0.752, SEN = 0.730, SPE = 0.758). Stratified analyses confirmed good generalizability across tumor subtypes. Ablation experiments demonstrated complementary contributions from each modality, and the 3D Swin Transformer provided more robust representations than conventional architectures. This multimodal deep learning model enables accurate, non-invasive metastasis prediction in HNSCC and shows strong potential for individualized treatment planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20469v2</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nuo Tong (School of Artificial Intelligence, Xidian University), Changhao Liu (Department of Radiotherapy, Xijing Hospital, Air Force Medical University of PLA), Zizhao Tang (School of Artificial Intelligence, Xidian University), Feifan Sun (Department of Oncology, The General Hospital of Western Theater Command), Yingping Li (School of Artificial Intelligence, Xidian University), Shuiping Gou (School of Artificial Intelligence, Xidian University), Mei Shi (Department of Radiotherapy, Xijing Hospital, Air Force Medical University of PLA)</dc:creator>
    </item>
    <item>
      <title>SynCell: Contextualized Drug Synergy Prediction</title>
      <link>https://arxiv.org/abs/2511.17695</link>
      <description>arXiv:2511.17695v2 Announce Type: replace 
Abstract: Motivation: Drug synergy is strongly influenced by cellular context. Variations in protein interaction landscapes and pathway activities across cell types can reshape how drugs act in combination. However, most existing models overlook this heterogeneity and rely on static or bulk level protein protein interaction networks that ignore cell specific molecular wiring. With the availability of single cell transcriptomic data, it is now possible to reconstruct cell line specific interactomes, offering a new foundation for contextualized drug synergy modeling.
  Results: We present SynCell, a contextualized drug synergy framework that integrates drug protein, protein protein, and protein cell line relations within a unified graph architecture. SynCell leverages single cell derived, cell line specific PPI networks to embed the molecular context in which drugs act, and employs graph convolutional learning to model how pharmacological effects propagate through cell specific signaling networks. This formulation treats synergy prediction as a cell line contextualized drug drug interaction problem. Across two large scale benchmarks (NCI ALMANAC and ONeil), SynCell consistently outperforms state of the art baselines including DeepDDS, HypergraphSynergy, and HERMES, especially in predicting synergies involving unseen drugs or novel cell lines. Ablation analyses show that contextualizing PPIs with single cell resolution yields substantial gains in generalization and biological interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17695v2</guid>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keqin Peng, Guangxin Su, Qinshan Shi, Shuai Gao, Ren Wang, Can Chen, Jun Wen</dc:creator>
    </item>
    <item>
      <title>Global-to-local image quality assessment in optical microscopy via fast and robust deep learning predictions</title>
      <link>https://arxiv.org/abs/2510.04859</link>
      <description>arXiv:2510.04859v2 Announce Type: replace-cross 
Abstract: Optical microscopy is one of the most widely used techniques in research studies for life sciences and biomedicine. These applications require reliable experimental pipelines to extract valuable knowledge from the measured samples and must be supported by image quality assessment (IQA) to ensure correct processing and analysis of the image data. IQA methods are implemented with variable complexity. However, while most quality metrics have a straightforward implementation, they might be time consuming and computationally expensive when evaluating a large dataset. In addition, quality metrics are often designed for well-defined image features and may be unstable for images out of the ideal domain. To overcome these limitations, recent works have proposed deep learning-based IQA methods, which can provide superior performance, increased generalizability and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by previous studies and applies a deep convolutional neural network designed for IQA on natural images to optical microscopy measurements. We retrained the same architecture to predict individual quality metrics and global quality scores for optical microscopy data. The resulting models provide fast and stable predictions of image quality by generalizing quality estimation even outside the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA provides patch-wise prediction of image quality and can be used to visualize spatially varying quality in a single image. Our study demonstrates that optical microscopy-based studies can benefit from the generalizability of deep learning models due to their stable performance in the presence of outliers, the ability to assess small image patches, and rapid predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04859v2</guid>
      <category>cs.CV</category>
      <category>physics.data-an</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Corbetta, Thomas Bocklitz</dc:creator>
    </item>
    <item>
      <title>TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2511.09605</link>
      <description>arXiv:2511.09605v2 Announce Type: replace-cross 
Abstract: The sharp rise in medical tomography examinations has created a demand for automated systems that can reliably extract informative features for downstream tasks such as tumor characterization. Although 3D volumes contain richer information than individual slices, effective 3D classification remains difficult: volumetric data encode complex spatial dependencies, and the scarcity of large-scale 3D datasets has constrained progress toward 3D foundation models. As a result, many recent approaches rely on 2D vision foundation models trained on natural images, repurposing them as feature extractors for medical scans with surprisingly strong performance. Despite their practical success, current methods that apply 2D foundation models to 3D scans via slice-based decomposition remain fundamentally limited. Standard slicing along axial, sagittal, and coronal planes often fails to capture the true spatial extent of a structure when its orientation does not align with these canonical views. More critically, most approaches aggregate slice features independently, ignoring the underlying 3D geometry and losing spatial coherence across slices. To overcome these limitations, we propose TomoGraphView, a novel framework that integrates omnidirectional volume slicing with spherical graph-based feature aggregation. Instead of restricting the model to axial, sagittal, or coronal planes, our method samples both canonical and non-canonical cross-sections generated from uniformly distributed points on a sphere enclosing the volume. We publicly share our accessible code base at http://github.com/compai-lab/2025-MedIA-kiechle and provide a user-friendly library for omnidirectional volume slicing at https://pypi.org/project/OmniSlicer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09605v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Kiechle, Stefan M. Fischer, Daniel M. Lang, Cosmin I. Bercea, Matthew J. Nyflot, Lina Felsner, Julia A. Schnabel, Jan C. Peeken</dc:creator>
    </item>
  </channel>
</rss>
