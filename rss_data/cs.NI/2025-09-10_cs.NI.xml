<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Sep 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>SCION Path Performance Toolkit and Benchmark for Advancing Machine Learning in Next-Generation Networks: ScionPathML</title>
      <link>https://arxiv.org/abs/2509.07154</link>
      <description>arXiv:2509.07154v1 Announce Type: new 
Abstract: Path-aware networks promise enhanced performance and resilience through multipath transport, but a lack of empirical data on their real-world dynamics hinders the design of effective protocols. This paper presents a longitudinal measurement study of the SCION architecture on the global SCIONLab testbed, characterizing the path stability, diversity, and performance crucial for protocols like Multipath QUIC (MPQUIC). Our measurements reveal a dynamic environment, with significant control-plane churn and short path lifetimes in parts of the testbed. We identify and characterize path discrepancy, a phenomenon where routing policies create asymmetric path availability between endpoints. Furthermore, we observe a performance trade-off where concurrent multipath transmissions can improve aggregate throughput but may degrade the latency and reliability of individual paths. These findings demonstrate that protocols such as MPQUIC should explicitly account for high churn and path asymmetry, challenging common assumptions in multipath protocol design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07154v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Damien Rossi, Lars Herschbach, Sina Keshvadi</dc:creator>
    </item>
    <item>
      <title>DORA: Dynamic O-RAN Resource Allocation for Multi-Slice 5G Networks</title>
      <link>https://arxiv.org/abs/2509.07242</link>
      <description>arXiv:2509.07242v1 Announce Type: new 
Abstract: The fifth generation (5G) of wireless networks must simultaneously support heterogeneous service categories, including Ultra-Reliable Low-Latency Communications (URLLC), enhanced Mobile Broadband (eMBB), and massive Machine-Type Communications (mMTC), each with distinct Quality of Service (QoS) requirements. Meeting these demands under limited spectrum resources requires adaptive and standards-compliant radio resource management. We present DORA (Dynamic O-RAN Resource Allocation), a deep reinforcement learning (DRL) framework for dynamic slice-level Physical Resource Block (PRB) allocation in Open RAN. DORA employs a PPO-based RL agent to allocate PRBs across URLLC, eMBB, and mMTC slices based on observed traffic demands and channel conditions. Intra-slice PRB scheduling is handled deterministically via round-robin among active UEs, simplifying control complexity and improving training stability. Unlike prior work, DORA supports online training and adapts continuously to evolving traffic patterns and cross-slice contention. Implemented in the standards-compliant OpenAirInterface (OAI) RAN stack and designed for deployment as an O-RAN xApp, DORA integrates seamlessly with RAN Intelligent Controllers (RICs). Extensive evaluation under congested regimes shows that DORA outperforms three non-learning baselines and a \texttt{DQN} agent, achieving lower URLLC latency, higher eMBB throughput with fewer SLA violations, and broader mMTC coverage without starving high-priority slices. To our knowledge, this is the first fully online DRL framework for adaptive, slice-aware PRB allocation in O-RAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07242v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alireza Ebrahimi Dorcheh, Tolunay Seyfi, Fatemeh Afghah</dc:creator>
    </item>
    <item>
      <title>TEGRA: A Flexible &amp; Scalable NextGen Mobile Core</title>
      <link>https://arxiv.org/abs/2509.07410</link>
      <description>arXiv:2509.07410v1 Announce Type: new 
Abstract: To support emerging mobile use cases (e.g., AR/VR, autonomous driving, and massive IoT), next-generation mobile cores for 5G and 6G are being re-architected as service-based architectures (SBAs) running on both private and public clouds. However, current performance optimization strategies for scaling these cores still revert to traditional NFV-based techniques, such as consolidating functions into rigid, monolithic deployments on dedicated servers. This raises a critical question: Is there an inherent tradeoff between flexibility and scalability in an SBA-based mobile core, where improving performance (and resiliency) inevitably comes at the cost of one or the other?
  To explore this question, we introduce resilient SBA microservices design patterns and state-management strategies, and propose TEGRA -- a high-performance, flexible, and scalable SBA-based mobile core. By leveraging the mobile core's unique position in the end-to-end internet ecosystem (i.e., at the last-mile edge), TEGRA optimizes performance without compromising adaptability. Our evaluation demonstrates that TEGRA achieves significantly lower latencies, processing requests 20x, 11x, and 1.75x faster than traditional SBA core implementations -- free5GC, Open5GS, and Aether, respectively -- all while matching the performance of state-of-the-art cores (e.g., CoreKube) while retaining flexibility. Furthermore, it reduces the complexity of deploying new features, requiring orders of magnitude fewer lines of code (LoCs) compared to existing cores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07410v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bilal Saleem, Omar Basit, Jiayi Meng, Iftekhar Alam, Ajay Thakur, Christian Maciocco, Muhammad Shahbaz, Y. Charlie Hu, Larry Peterson</dc:creator>
    </item>
    <item>
      <title>Network-accelerated Active Messages</title>
      <link>https://arxiv.org/abs/2509.07431</link>
      <description>arXiv:2509.07431v1 Announce Type: new 
Abstract: Remote Direct Memory Access (RDMA) improves host networking performance by eliminating software and server CPU involvement. However, RDMA has a limited set of operations, is difficult to program, and often requires multiple round trips to perform simple application operations. Programmable SmartNICs provide a different means to offload work from host CPUs to a NIC. This leaves applications with the complex choice of embedding logic as RPC handlers at servers, using RDMA's limited interface to access server structures via client-side logic, or running some logic on SmartNICs. The best choice varies between workloads and over time. To solve this dilemma, we present NAAM, network-accelerated active messages. NAAM applications specify small, portable eBPF functions associated with messages. Each message specifies what data it accesses using an RDMA-like interface. NAAM runs at various places in the network, including at clients, on server-attached SmartNICs, and server host CPU cores. Due to eBPF's portability, the code associated with a message can be run at any location. Hence, the NAAM runtime can dynamically steer any message to execute its associated logic wherever it makes the most sense. To demonstrate NAAM's flexibility, we built several applications, including the MICA hash table and lookups from a Cell-style B-tree. With an NVIDIA BlueField-2 SmartNIC and integrating its NIC-embedded switch, NAAM can run any of these operations on client, server, and NIC cores, shifting load in tens of milliseconds on server compute congestion. NAAM dynamically offloads up to 1.8 million MICA ops/s for YCSB-B and 750,000 Cell lookups/s from server CPUs. Finally, whereas iPipe, the state-of-the-art SmartNIC offload framework, only scales to 8 application offloads on BlueField-2, NAAM scales to hundreds of application offloads with minimal impact on tail latency due to eBPF's low overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07431v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Ashfaqur Rahaman (University of Utah), Alireza Sanaee (University of Cambridge), Todd Thornley (University of Utah), Sebastiano Miano (Politecnico di Milano), Gianni Antichi (Politecnico di Milano,Queen Mary University of London), Brent E. Stephens (Google,University of Utah), Ryan Stutsman (University of Utah)</dc:creator>
    </item>
    <item>
      <title>Constraint-Compliant Network Optimization through Large Language Models</title>
      <link>https://arxiv.org/abs/2509.07492</link>
      <description>arXiv:2509.07492v1 Announce Type: new 
Abstract: This work develops an LLM-based optimization framework ensuring strict constraint satisfaction in network optimization. While LLMs possess contextual reasoning capabilities, existing approaches often fail to enforce constraints, causing infeasible solutions. Unlike conventional methods that address average constraints, the proposed framework integrates a natural language-based input encoding strategy to restrict the solution space and guarantee feasibility. For multi-access edge computing networks, task allocation is optimized while minimizing worst-case latency. Numerical evaluations demonstrate LLMs as a promising tool for constraint-aware network optimization, offering insights into their inference capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07492v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youngjin Song, Wookjin Lee, Hong Ki Kim, Sang Hyun Lee</dc:creator>
    </item>
    <item>
      <title>FlexSAN: A Flexible Regenerative Satellite Access Network Architecture</title>
      <link>https://arxiv.org/abs/2509.07548</link>
      <description>arXiv:2509.07548v1 Announce Type: new 
Abstract: The regenerative satellite access network (SAN) architecture deploys next-generation NodeB (gNBs) on satellites to enable enhanced network management capabilities. It supports two types of regenerative payload, on-board gNB and on-board gNB-Distributed Unit (gNB-DU). Measurement results based on our prototype implementation show that the on-board gNB offers lower latency, while the on-board gNB-DU is more cost-effective, and there is often a trade-off between Quality-of-Service (QoS) and operational expenditure (OPEX) when choosing between the two payload types. However, current SAN configurations are static and inflexible -- either deploying the full on-board gNB or only the on-board gNB-DU. This rigidity can lead to resource waste or poor user experiences. In this paper, we propose Flexible SAN (FlexSAN), an adaptive satellite access network architecture that dynamically configures the optimal regenerative payload based on real-time user demands. FlexSAN selects the lowest OPEX payload configuration when all user demands are satisfied, and otherwise maximizes the number of admitted users while ensuring QoS for connected users. To address the computational complexity of dynamic payload selection, we design an adaptive greedy heuristic algorithm. Extensive experiments validate FlexSAN's effectiveness, showing a 36.1% average improvement in user admission rates and a 15% OPEX reduction over static SANs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07548v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weize Kong, Chaoqun You, Xuming Pei,  YueGao</dc:creator>
    </item>
    <item>
      <title>Quantum Computing for Large-scale Network Optimization: Opportunities and Challenges</title>
      <link>https://arxiv.org/abs/2509.07773</link>
      <description>arXiv:2509.07773v1 Announce Type: new 
Abstract: The complexity of large-scale 6G-and-beyond networks demands innovative approaches for multi-objective optimization over vast search spaces, a task often intractable. Quantum computing (QC) emerges as a promising technology for efficient large-scale optimization. We present our vision of leveraging QC to tackle key classes of problems in future mobile networks. By analyzing and identifying common features, particularly their graph-centric representation, we propose a unified strategy involving QC algorithms. Specifically, we outline a methodology for optimization using quantum annealing as well as quantum reinforcement learning. Additionally, we discuss the main challenges that QC algorithms and hardware must overcome to effectively optimize future networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07773v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>quant-ph</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Macaluso, Giovanni Geraci, El\'ias F. Combarro, Sergi Abadal, Ioannis Arapakis, Sofia Vallecorsa, Eduard Alarc\'on</dc:creator>
    </item>
    <item>
      <title>Making congestion control robust to per-packet load balancing in datacenters</title>
      <link>https://arxiv.org/abs/2509.07907</link>
      <description>arXiv:2509.07907v1 Announce Type: new 
Abstract: Per-packet load-balancing approaches are increasingly deployed in datacenter networks. However, their combination with existing congestion control algorithms (CCAs) may lead to poor performance, and even state-of-the-art CCAs can collapse due to duplicate ACKs. A typical approach to handle this collapse is to make CCAs resilient to duplicate ACKs.
  In this paper, we first model the throughput collapse of a wide array of CCAs when some of the paths are congested. We show that addressing duplicate ACKs is insufficient. Instead, we explain that since CCAs are typically designed for single-path routing, their estimation function focuses on the latest feedback and mishandles feedback that reflects multiple paths. We propose to use a median feedback that is more robust to the varying signals that come with multiple paths. We introduce MSwift, which applies this principle to make Google's Swift robust to multi-path routing while keeping its incast tolerance and single-path performance. Finally, we demonstrate that MSwift improves the 99th-percentile FCT by up to 25\%, both with random packet spraying and adaptive routing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07907v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Barak Gerstein, Mark Silberstein, Isaac Keslassy</dc:creator>
    </item>
    <item>
      <title>Influence Maximization Considering Influence, Cost and Time</title>
      <link>https://arxiv.org/abs/2509.07625</link>
      <description>arXiv:2509.07625v1 Announce Type: cross 
Abstract: Influence maximization has been studied for social network analysis, such as viral marketing (advertising), rumor prevention, and opinion leader identification. However, most studies neglect the interplay between influence spread, cost efficiency, and temporal urgency. In practical scenarios such as viral marketing and information campaigns, jointly optimizing Influence, Cost, and Time is essential, yet remaining largely unaddressed in current literature. To bridge the gap, this paper proposes a new multi-objective influence maximization problem that simultaneously optimizes influence, cost, and time. We show the intuitive and empirical evidence to prove the feasibility and necessity of this multi-objective problem. We also develop an evolutionary variable-length search algorithm that can effectively search for optimal node combinations. The proposed EVEA algorithm outperforms all baselines, achieving up to 19.3% higher hypervolume and 25 to 40% faster convergence across four real-world networks, while maintaining a diverse and balanced Pareto front among influence, cost, and time objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07625v1</guid>
      <category>cs.SI</category>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingyang Feng, Qi Zhao, Shan He, Yuhui Shi</dc:creator>
    </item>
    <item>
      <title>Disruption-aware Microservice Re-orchestration for Cost-efficient Multi-cloud Deployments</title>
      <link>https://arxiv.org/abs/2501.16143</link>
      <description>arXiv:2501.16143v4 Announce Type: replace 
Abstract: Multi-cloud environments enable a cost-efficient scaling of cloud-native applications across geographically distributed virtual nodes with different pricing models. In this context, the resource fragmentation caused by frequent changes in the resource demands of deployed microservices, along with the allocation or termination of new and existing microservices, increases the deployment cost. Therefore, re-orchestrating deployed microservices on a cheaper configuration of multi-cloud nodes offers a practical solution to restore the cost efficiency of deployment. However, the rescheduling procedure causes frequent service interruptions due to the continuous termination and rebooting of the containerized microservices. Moreover, it may potentially interfere with and delay other deployment operations, compromising the stability of the running applications. To address this issue, we formulate a multi-objective integer linear programming (ILP) problem that computes a microservice rescheduling solution capable of providing minimum deployment cost without significantly affecting the service continuity. At the same time, the proposed formulation also preserves the quality of service (QoS) requirements, including latency, expressed through microservice co-location constraints. Additionally, we present a heuristic algorithm to approximate the optimal solution, striking a balance between cost reduction and service disruption mitigation. We integrate the proposed approach as a custom plugin of the Kubernetes (K8s) scheduler. Results reveal that our approach significantly reduces multi-cloud deployment costs and service disruptions compared to the benchmark schemes, while ensuring QoS requirements are consistently met.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16143v4</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TSC.2025.3604373</arxiv:DOI>
      <dc:creator>Marco Zambianco, Silvio Cretti, Domenico Siracusa</dc:creator>
    </item>
    <item>
      <title>Joint Resource Estimation and Trajectory Optimization for eVTOL-involved CR network: A Monte Carlo Tree Search-based Approach</title>
      <link>https://arxiv.org/abs/2504.18031</link>
      <description>arXiv:2504.18031v2 Announce Type: replace 
Abstract: Electric Vertical Take-Off and Landing (eVTOL) aircraft, pivotal to Advanced Air Mobility (AAM), are emerging as a transformative transportation paradigm with the potential to redefine urban and regional mobility. While these systems offer unprecedented efficiency in transporting people and goods, they rely heavily on computation capability, safety-critical operations such as real-time navigation, environmental sensing, and trajectory tracking--necessitating robust offboard computational support. A widely adopted solution involves offloading these tasks to terrestrial base stations (BSs) along the flight path. However, air-to-ground connectivity is often constrained by spectrum conflicts with terrestrial users, which poses a significant challenge to maintaining reliable task execution. Cognitive radio (CR) techniques offer promising capabilities for dynamic spectrum access, making them a natural fit for addressing this issue. Existing studies often overlook the time-varying nature of BS resources, such as spectrum availability and CPU cycles, which leads to inaccurate trajectory planning, suboptimal offloading success rates, excessive energy consumption, and operational delays. To address these challenges, we propose a trajectory optimization framework for eVTOL swarms that maximizes task offloading success probability while minimizing both energy consumption and resource competition (e.g., spectrum and CPU cycles) with primary terrestrial users. The proposed algorithm integrates a Multi-Armed Bandit (MAB) model to dynamically estimate BS resource availability and a Monte Carlo Tree Search (MCTS) algorithm to determine optimal offloading decisions, selecting both the BSs and access time windows that align with energy and temporal constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18031v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Xiong, Chenxin Yang, Yujie Qin, Wanzhi Ma, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>GCN-Driven Reinforcement Learning for Probabilistic Real-Time Guarantees in Industrial URLLC</title>
      <link>https://arxiv.org/abs/2506.15011</link>
      <description>arXiv:2506.15011v3 Announce Type: replace 
Abstract: Ensuring packet-level communication quality is vital for ultra-reliable, low-latency communications (URLLC) in large-scale industrial wireless networks. We enhance the Local Deadline Partition (LDP) algorithm by introducing a Graph Convolutional Network (GCN) integrated with a Deep Q-Network (DQN) reinforcement learning framework for improved interference coordination in multi-cell, multi-channel networks. Unlike LDP's static priorities, our approach dynamically learns link priorities based on real-time traffic demand, network topology, remaining transmission opportunities, and interference patterns. The GCN captures spatial dependencies, while the DQN enables adaptive scheduling decisions through reward-guided exploration. Simulation results show that our GCN-DQN model achieves mean SINR improvements of 179.6\%, 197.4\%, and 175.2\% over LDP across three network configurations. Additionally, the GCN-DQN model demonstrates mean SINR improvements of 31.5\%, 53.0\%, and 84.7\% over our previous CNN-based approach across the same configurations. These results underscore the effectiveness of our GCN-DQN model in addressing complex URLLC requirements with minimal overhead and superior network performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15011v3</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eman Alqudah, Ashfaq Khokhar</dc:creator>
    </item>
    <item>
      <title>Enhancements to P4TG: Histogram-Based RTT Monitoring in the Data Plane</title>
      <link>https://arxiv.org/abs/2507.15382</link>
      <description>arXiv:2507.15382v2 Announce Type: replace 
Abstract: Modern traffic generators are essential tools for evaluating the performance of network environments. P4TG is a P4-based traffic generator implemented for Intel Tofino switches that offers high-speed packet generation with fine-grained measurement capabilities. However, P4TG samples time-based metrics such as the round-trip time (RTT) in the data plane and collects them at the controller. This leads to a reduced accuracy. In this paper, we introduce a histogram-based RTT measurement feature for P4TG. It enables accurate analysis at line rate without sampling. Generally, histogram bins are modeled as ranges, and values are matched to a bin. Efficient packet matching in hardware is typically achieved using ternary content addressable memory (TCAM). However, representing range matching rules in TCAM poses a challenge. Therefore, we implemented a range-to-prefix conversion algorithm that models range matching with multiple ternary entries. This paper describes the data plane implementation and runtime configuration of RTT histograms in P4TG. Further, we discuss the efficiency of the ternary decomposition. Our evaluation demonstrates the applicability of the histogram-based RTT analysis by comparing the measured values with a configured theoretical distribution of RTTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15382v2</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.14279/depositonce-24399</arxiv:DOI>
      <dc:creator>Fabian Ihle, Etienne Zink, Michael Menth</dc:creator>
    </item>
    <item>
      <title>VariSAC: V2X Assured Connectivity in RIS-Aided ISAC via GNN-Augmented Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2509.06763</link>
      <description>arXiv:2509.06763v2 Announce Type: replace 
Abstract: The integration of Reconfigurable Intelligent Surfaces (RIS) and Integrated Sensing and Communication (ISAC) in vehicular networks enables dynamic spatial resource management and real-time adaptation to environmental changes. However, the coexistence of distinct vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) connectivity requirements, together with highly dynamic and heterogeneous network topologies, presents significant challenges for unified reliability modeling and resource optimization. To address these issues, we propose VariSAC, a graph neural network (GNN)-augmented deep reinforcement learning framework for assured, time-continuous connectivity in RIS-assisted, ISAC-enabled vehicle-to-everything (V2X) systems. Specifically, we introduce the Continuous Connectivity Ratio (CCR), a unified metric that characterizes the sustained temporal reliability of V2I connections and the probabilistic delivery guarantees of V2V links, thus unifying their continuous reliability semantics. Next, we employ a GNN with residual adapters to encode complex, high-dimensional system states, capturing spatial dependencies among vehicles, base stations (BS), and RIS nodes. These representations are then processed by a Soft Actor-Critic (SAC) agent, which jointly optimizes channel allocation, power control, and RIS configurations to maximize CCR-driven long-term rewards. Extensive experiments on real-world urban datasets demonstrate that VariSAC consistently outperforms existing baselines in terms of continuous V2I ISAC connectivity and V2V delivery reliability, enabling persistent connectivity in highly dynamic vehicular environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06763v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Huijun Tang, Wang Zeng, Ming Du, Pinlong Zhao, Pengfei Jiao, Huaming Wu, Hongjian Sun</dc:creator>
    </item>
  </channel>
</rss>
