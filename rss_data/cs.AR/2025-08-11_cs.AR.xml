<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Aug 2025 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>ConiQ: Enabling Concatenated Quantum Error Correction on Neutral Atom Arrays</title>
      <link>https://arxiv.org/abs/2508.05779</link>
      <description>arXiv:2508.05779v1 Announce Type: new 
Abstract: Recent progress on concatenated codes, especially many-hypercube codes, achieves unprecedented space efficiency. Yet two critical challenges persist in practice. First, these codes lack efficient implementations of addressable logical gates. Second, the required high degree of parallelism and long-range interactions pose significant challenges for current hardware platforms. In this paper, we propose an efficient compilation approach for concatenated codes, specifically many-hypercube codes, targeted at neutral atom arrays, which provide the necessary parallelism and long-range interactions. Our approach builds on two key innovations. First, we introduce Automorphism-assisted Hierarchical Addressing (AHA) logical CNOT gates that significantly reduce spacetime overhead compared to conventional distillation-based methods. Second, we develop Virtual Atom Intermediate Representation (VAIR) that enables level-wise optimization and legalization. We implement these innovations in ConiQ, a hardware-aware quantum compiler designed to compile fault-tolerant quantum circuits for neutral atom arrays using many-hypercube codes. Our evaluation demonstrates that ConiQ achieves up to 2000x reduction in spacetime overhead and up to 10^6x reduction in compilation time compared to state-of-the-art compilers, with our AHA gates providing an additional overhead reduction of up to 20x. These results establish concatenated codes as a promising approach for fault-tolerant quantum computing in the near future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05779v1</guid>
      <category>cs.AR</category>
      <category>quant-ph</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pengyu Liu, Mingkuan Xu, Hengyun Zhou, Hanrui Wang, Umut A. Acar, Yunong Shi</dc:creator>
    </item>
    <item>
      <title>ArchXBench: A Complex Digital Systems Benchmark Suite for LLM Driven RTL Synthesis</title>
      <link>https://arxiv.org/abs/2508.06047</link>
      <description>arXiv:2508.06047v1 Announce Type: new 
Abstract: Modern SoC datapaths include deeply pipelined, domain-specific accelerators, but their RTL implementation and verification are still mostly done by hand. While large language models (LLMs) exhibit advanced code-generation abilities for programming languages like Python, their application to Verilog-like RTL remains in its nascent stage. This is reflected in the simple arithmetic and control circuits currently used to evaluate generative capabilities in existing benchmarks. In this paper, we introduce ArchXBench, a six-level benchmark suite that encompasses complex arithmetic circuits and other advanced digital subsystems drawn from domains such as cryptography, image processing, machine learning, and signal processing. Architecturally, some of these designs are purely combinational, others are multi-cycle or pipelined, and many require hierarchical composition of modules. For each benchmark, we provide a problem description, design specification, and testbench, enabling rapid research in the area of LLM-driven agentic approaches for complex digital systems design.
  Using zero-shot prompting with Claude Sonnet 4, GPT 4.1, o4-mini-high, and DeepSeek R1 under a pass@5 criterion, we observed that o4-mini-high successfully solves the largest number of benchmarks, 16 out of 30, spanning Levels 1, 2, and 3. From Level 4 onward, however, all models consistently fail, highlighting a clear gap in the capabilities of current state-of-the-art LLMs and prompting/agentic approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06047v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suresh Purini, Siddhant Garg, Mudit Gaur, Sankalp Bhat, Sohan Mupparapu, Arun Ravindran</dc:creator>
    </item>
    <item>
      <title>Nail: Not Another Fault-Injection Framework for Chisel-generated RTL</title>
      <link>https://arxiv.org/abs/2508.06344</link>
      <description>arXiv:2508.06344v1 Announce Type: new 
Abstract: Fault simulation and emulation are essential techniques for evaluating the dependability of integrated circuits, enabling early-stage vulnerability analysis and supporting the implementation of effective mitigation strategies. High-level hardware description languages such as Chisel facilitate the rapid development of complex fault scenarios with minimal modification to the design. However, existing Chisel-based fault injection (FI) frameworks are limited by coarse-grained, instruction-level controllability, restricting the precision of fault modeling. This work introduces Nail, a Chisel-based open-source FI framework that overcomes these limitations by introducing state-based faults. This approach enables fault scenarios that depend on specific system states, rather than solely on instruction-level triggers, thereby removing the need for precise timing of fault activation. For greater controllability, Nail allows users to arbitrarily modify internal trigger states via software at runtime. To support this, Nail automatically generates a software interface, offering straightforward access to the instrumented design. This enables fine-tuning of fault parameters during active FI campaigns - a feature particularly beneficial for FPGA emulation, where synthesis is time-consuming. Utilizing these features, Nail narrows the gap between the high speed of emulation-based FI frameworks, the usability of software-based approaches, and the controllability achieved in simulation. We demonstrate Nail's state-based FI and software framework by modeling a faulty general-purpose register in a RISC-V processor. Although this might appear straightforward, it requires state-dependent FI and was previously impossible without fundamental changes to the design. The approach was validated in both simulation and FPGA emulation, where the addition of Nail introduced less than 1% resource overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06344v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robin Sehm, Christian Ewert, Rainer Buchty, Mladen Berekovic, Saleh Mulhem</dc:creator>
    </item>
    <item>
      <title>Accelerating Data Chunking in Deduplication Systems using Vector Instructions</title>
      <link>https://arxiv.org/abs/2508.05797</link>
      <description>arXiv:2508.05797v1 Announce Type: cross 
Abstract: Content-defined Chunking (CDC) algorithms dictate the overall space savings that deduplication systems achieve. However, due to their need to scan each file in its entirety, they are slow and often the main performance bottleneck within data deduplication. We present VectorCDC, a method to accelerate hashless CDC algorithms using vector CPU instructions, such as SSE / AVX. Our evaluation shows that VectorCDC is effective on Intel, AMD, ARM, and IBM CPUs, achieving 8.35x - 26.2x higher throughput than existing vector-accelerated techniques without affecting the deduplication space savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05797v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sreeharsha Udayashankar, Abdelrahman Baba, Samer Al-Kiswany</dc:creator>
    </item>
    <item>
      <title>DECA: A Near-Core LLM Decompression Accelerator Grounded on a 3D Roofline Model</title>
      <link>https://arxiv.org/abs/2505.19349</link>
      <description>arXiv:2505.19349v2 Announce Type: replace 
Abstract: To alleviate the memory bandwidth bottleneck in Large Language Model (LLM) inference workloads, weight matrices are stored in memory in quantized and sparsified formats. Hence, before tiles of these matrices can be processed by in-core generalized matrix multiplication (GeMM) hardware engines, they need to be dequantized and de-sparsified. This is currently performed in software with vector operations. Unfortunately, this approach delivers only modest performance. Moreover, it is hard to understand how to improve the system, as the overall GeMM performance depends on the interaction between memory resources, vector units, and hardware matrix engines.
  To improve the performance of LLM inference in advanced platforms equipped with in-core GeMM engines and HBM, this paper makes three main contributions. First, it develops an analytical performance model with a 3D visual representation that provides insights into how memory resources, vector units, and hardware matrix engines interact to deliver compressed GeMM performance. Second, it proposes DECA, a new near-core ML-model decompression accelerator. DECA offloads tile de-sparsification and dequantization from the CPU, producing ready-to-use tiles for in-core GeMM engines. Third, it introduces a new ISA extension that enables out-of-order invocation of the near-core accelerator. With this extension, accelerator and core computations can interleave and overlap with high-performance. Our evaluation shows that, in a simulated 56-core Xeon 4 server with HBM, DECA accelerates the execution of compressed GeMMs by up to 4x over the use of optimized Intel software kernels. Further, DECA reduces the next-token generation time of Llama2-70B and OPT-66B by 1.6x-2.6x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19349v2</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gerasimos Gerogiannis (Intel Corporation,University of Illinois at Urbana-Champaign), Stijn Eyerman (Intel Corporation), Evangelos Georganas (Intel Labs), Wim Heirman (Intel Corporation), Josep Torrellas (University of Illinois at Urbana-Champaign)</dc:creator>
    </item>
    <item>
      <title>A Reconfigurable Time-Domain In-Memory Computing Macro using FeFET-Based CAM with Multilevel Delay Calibration in 28 nm CMOS</title>
      <link>https://arxiv.org/abs/2504.03925</link>
      <description>arXiv:2504.03925v2 Announce Type: replace-cross 
Abstract: Time-domain nonvolatile in-memory computing (TD-nvIMC) offers a promising pathway to reduce data movement and improve energy efficiency by encoding computation in delay rather than voltage or current. This work presents a fully integrated and reconfigurable TD-nvIMC macro, fabricated in 28 nm CMOS, that combines a ferroelectric FET (FeFET)-based content-addressable memory array, a cascaded delay element chain, and a time-to-digital converter. The architecture supports binary multiply-and-accumulate (MAC) operations using XOR- and AND-based matching, as well as in-memory Boolean logic and arithmetic functions. Sub-nanosecond MAC resolution is achieved through experimentally demonstrated 550 ps delay steps, representing a 2000$\times$ improvement over prior FeFET TD-nvIMC work, enabled by multilevel-state calibration with $\leq$100 ps resolution. Write-disturb resilience is ensured via isolated triple-well bulks. The proposed macro achieves a measured throughput of 222.2 MOPS/cell and energy efficiency of 1887 TOPS/W at 0.85 V, establishing a viable path toward scalable, energy-efficient TD-nvIMC accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03925v2</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Mon, 11 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jeries Mattar, Mor M. Dahan, Stefan Dunkel, Halid Mulaosmanovic, Gunda Beernink, Sven Beyer, Eilam Yalon, Nicol\'as Wainstein</dc:creator>
    </item>
  </channel>
</rss>
