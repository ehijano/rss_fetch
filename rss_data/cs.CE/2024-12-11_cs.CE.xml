<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2024 02:42:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>equilibrium-c: A Lightweight Modern Equilibrium Chemistry Calculator for Hypersonic Flow Applications</title>
      <link>https://arxiv.org/abs/2412.07166</link>
      <description>arXiv:2412.07166v1 Announce Type: new 
Abstract: equilibrium-c (eqc) is a program for computing the composition of gas mixtures in chemical equilibrium. In typical usage, the program is given a known thermodynamic state, such as fixed temperature and pressure, as well as an initial composition of gaseous species, and computes the final composition in the limit of a large amount of time relative to the reaction speeds. eqc includes a database of thermodynamic properties taken from the literature, a set of core routines written the C programming language to solve the equilibrium problems, and a Python wrapper layer to organise the solution process and interface with user code. Dependencies are extremely minimal, and the API is designed to be easily embedded in multi-physics codes that solve problems in fluid dynamics, combustion, and chemical processing. In this paper, I first introduce the equations of chemical equilibrium, then spend some time discussing their numerical solution, and finally present a series of example problems, with an emphasis on verification and validation of the solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07166v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas N. Gibbons</dc:creator>
    </item>
    <item>
      <title>Quantitative Comparison of the Total Focusing Method, Reverse Time Migration, and Full Waveform Inversion for Ultrasonic Imaging</title>
      <link>https://arxiv.org/abs/2412.07347</link>
      <description>arXiv:2412.07347v1 Announce Type: new 
Abstract: Phased array ultrasound is a widely used technique in non-destructive testing. Using piezoelectric elements as both sources and receivers provides a significant gain in information and enables more accurate defect detection. When all source-receiver combinations are used, the process is called full matrix capture. The total focusing method~(TFM), which exploits such datasets, relies on a delay and sum algorithm to sum up the signals on a pixel grid. However, TFM only uses the first arriving p-waves, making it challenging to size complex-shaped defects. By contrast, more advanced methods such as reverse time migration~(RTM) and full waveform inversion~(FWI) use full waveforms to reconstruct defects. Both methods compare measured signals with ultrasound simulations. While RTM identifies defects by convolving forward and backward wavefields once, FWI iteratively updates material models to reconstruct the actual distribution of material properties. This study compares TFM, RTM, and FWI for six specimens featuring circular defects or Y-shaped notches. The reconstructed results are first evaluated qualitatively using different thresholds and then quantitatively using metrics such as AUPRC, AUROC, and F1-score. The results show that FWI performs best in most cases, both qualitatively and quantitatively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07347v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim B\"urchner, Simon Schmid, Lukas Bergbreiter, Ernst Rank, Stefan Kollmannsberger, Christian U. Grosse</dc:creator>
    </item>
    <item>
      <title>A Robust Sustainability Assessment Methodology for Aircraft Parts: Application to a Fuselage Panel</title>
      <link>https://arxiv.org/abs/2412.07421</link>
      <description>arXiv:2412.07421v1 Announce Type: new 
Abstract: The paper presents a cradle-to-gate sustainability assessment methodology specifically designed to evaluate aircraft components in a robust and systematic manner. This methodology integrates multi-criteria decision-making (MCDM) analysis across ten criteria, categorized under environmental impact, cost, and performance. Environmental impact is analyzed through life cycle assessment and cost through life cycle costing, with both analyses facilitated by SimaPro software. Performance is measured in terms of component mass and specific stiffness. The robustness of this methodology is tested through various MCDM techniques, normalization approaches, and objective weighting methods. To demonstrate the methodology, the paper assesses the sustainability of a fuselage panel, comparing nine variants that differ in materials, joining techniques, and part thicknesses. All approaches consistently identify thermoplastic CFRP panels as the most sustainable option, with the geometric mean aggregation of weights providing balanced criteria consideration across environmental, cost, and performance aspects. The adaptability of this proposed methodology is illustrated, showing its applicability to any aircraft component with the requisite data. This structured approach offers critical insights to support sustainable decision-making in aircraft component design and procurement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07421v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <category>math.NA</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aikaterini A. Anagnostopoulou, Dimitris G. Sotiropoulos, Konstantinos I. Tserpes</dc:creator>
    </item>
    <item>
      <title>Systematically Examining Reproducibility: A Case Study for High Throughput Sequencing using the PRIMAD Model and BioCompute Object</title>
      <link>https://arxiv.org/abs/2412.07502</link>
      <description>arXiv:2412.07502v1 Announce Type: new 
Abstract: The reproducibility of computational pipelines is an expectation in biomedical science, particularly in critical domains like human health. In this context, reporting next generation genome sequencing methods used in precision medicine spurred the development of the IEEE 2791-2020 standard for Bioinformatics Analyses Generated by High Throughput Sequencing (HTS), known as the BioCompute Object (BCO). Championed by the USA's Food and Drug Administration, the BCO is a pragmatic framework for documenting pipelines; however, it has not been systematically assessed for its reproducibility claims.
  This study uses the PRIMAD model, a conceptual framework for describing computational experiments for reproducibility purposes, to systematically review the BCO for depth and coverage. A meticulous mapping of BCO and PRIMAD elements onto a published BCO use case reveals potential omissions and necessary extensions within both frameworks. This underscores the significance of systematically validating claims of reproducibility for published digital objects, thereby enhancing the reliability of scientific research in bioscience and related disciplines.
  This study, along with its artifacts, is reported as a RO-Crate, providing a structured reporting approach, which is available at https://doi.org/10.5281/zenodo.14317922.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07502v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meznah Aloqalaa, Stian Soiland-Reyes, Carole Goble</dc:creator>
    </item>
    <item>
      <title>Knowledge-based model validation using a custom metric</title>
      <link>https://arxiv.org/abs/2412.07521</link>
      <description>arXiv:2412.07521v1 Announce Type: new 
Abstract: Vehicle models have a long history of research and as of today are able to model the involved physics in a reasonable manner. However, each new vehicle has its new characteristics or parameters. The identification of these is the main task of an engineer. To validate whether the correct parameter set has been chosen is a tedious task and often can only be performed by experts. Metrics known commonly used in literature are able to compare different results under certain aspects. However, they fail to answer the question: Are the models accurate enough? In this article, we propose the usage of a custom metric trained on the knowledge of experts to tackle this problem. Our approach involves three main steps: first, the formalized collection of subject matter experts' opinion on the question: Having seen the measurement and simulation time series in comparison, is the model quality sufficient? From this step, we obtain a data set that is able to quantify the sufficiency of a simulation result based on a comparison to corresponding experimental data. In a second step, we compute common model metrics on the measurement and simulation time series and use these model metrics as features to a regression model. Third, we fit a regression model to the experts' opinions. This regression model, i.e., our custom metric, can than predict the sufficiency of a new simulation result and gives a confidence on this prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07521v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicola Henkelmann, Stephan Rhode, Johannes von Keler</dc:creator>
    </item>
    <item>
      <title>CoinCLIP: A Multimodal Framework for Evaluating the Viability of Memecoins in the Web3 Ecosystem</title>
      <link>https://arxiv.org/abs/2412.07591</link>
      <description>arXiv:2412.07591v1 Announce Type: new 
Abstract: The rapid growth of memecoins within the Web3 ecosystem, driven by platforms like Pump.fun, has made it easier for anyone to create tokens. However, this democratization has also led to an explosion of low-quality or bot-generated projects, often motivated by short-term financial gain. This overwhelming influx of speculative tokens creates a challenge in distinguishing viable memecoins from those that are unlikely to succeed. To address this issue, we introduce CoinVibe, a comprehensive multimodal dataset designed to evaluate the viability of memecoins. CoinVibe integrates textual descriptions, visual content (logos), and community data (user comments, timestamps, and number of likes) to provide a holistic view of a memecoin's potential. In addition, we present CoinCLIP, a novel framework that leverages the Contrastive Language-Image Pre-Training (CLIP) model, augmented with lightweight modules and community data integration, to improve classification accuracy. By combining visual and textual representations with community insights, CoinCLIP provides a robust, data-driven approach to filter out low-quality or bot-driven projects. This research aims to help creators and investors identify high-potential memecoins, while also offering valuable insights into the factors that contribute to their long-term success. The code and dataset are publicly available at https://github.com/hwlongCUHK/CoinCLIP.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07591v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hou-Wan Long, Hongyang Li, Wei Cai</dc:creator>
    </item>
    <item>
      <title>Real-Time Performance Optimization of Travel Reservation Systems Using AI and Microservices</title>
      <link>https://arxiv.org/abs/2412.06874</link>
      <description>arXiv:2412.06874v1 Announce Type: cross 
Abstract: The rapid growth of the travel industry has increased the need for real-time optimization in reservation systems that could take care of huge data and transaction volumes. This study proposes a hybrid framework that ut folds an Artificial Intelligence and a Microservices approach for the performance optimization of the system. The AI algorithms forecast demand patterns, optimize the allocation of resources, and enhance decision-making driven by Microservices architecture, hence decentralizing system components for scalability, fault tolerance, and reduced downtime. The model provided focuses on major problems associated with the travel reservation systems such as latency of systems, load balancing and data consistency. It endows the systems with predictive models based on AI improved ability to forecast user demands. Microservices would also take care of different scales during uneven traffic patterns. Hence, both aspects ensure better handling of peak loads and spikes while minimizing delays and ensuring high service quality. A comparison was made between traditional reservation models, which are monolithic and the new model of AI-Microservices. Comparatively, the analysis results state that there is a drastic improvement in processing times where the system uptime and resource utilization proved the capability of AI and the microservices in transforming the travel industry in terms of reservation. This research work focused on AI and Microservices towards real-time optimization, providing critical insight into how to move forward with practical recommendations for upgrading travel reservation systems with this technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06874v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.PL</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biman Barua, M. Shamim Kaiser</dc:creator>
    </item>
    <item>
      <title>Multi-fidelity Bayesian Optimization in Engineering Design</title>
      <link>https://arxiv.org/abs/2311.13050</link>
      <description>arXiv:2311.13050v2 Announce Type: replace 
Abstract: Resided at the intersection of multi-fidelity optimization (MFO) and Bayesian optimization (BO), MF BO has found a niche in solving expensive engineering design optimization problems, thanks to its advantages in incorporating physical and mathematical understandings of the problems, saving resources, addressing exploitation-exploration trade-off, considering uncertainty, and processing parallel computing. The increasing number of works dedicated to MF BO suggests the need for a comprehensive review of this advanced optimization technique. In this paper, we survey recent developments of two essential ingredients of MF BO: Gaussian process (GP) based MF surrogates and acquisition functions. We first categorize the existing MF modeling methods and MFO strategies to locate MF BO in a large family of surrogate-based optimization and MFO algorithms. We then exploit the common properties shared between the methods from each ingredient of MF BO to describe important GP-based MF surrogate models and review various acquisition functions. By doing so, we expect to provide a structured understanding of MF BO. Finally, we attempt to reveal important aspects that require further research for applications of MF BO in solving intricate yet important design optimization problems, including constrained optimization, high-dimensional optimization, optimization under uncertainty, and multi-objective optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13050v2</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bach Do, Ruda Zhang</dc:creator>
    </item>
    <item>
      <title>Theory and Computation of Substructure Characteristic Modes</title>
      <link>https://arxiv.org/abs/2403.00792</link>
      <description>arXiv:2403.00792v3 Announce Type: replace 
Abstract: The problem of substructure characteristic modes is developed using a scattering matrix-based formulation, generalizing subregion characteristic mode decomposition to arbitrary computational tools. It is shown that the modes of the scattering formulation are identical to the modes of the classical formulation based on the background Green's function for lossless systems under conditions where both formulations can be applied. The scattering formulation, however, opens a variety of new subregion scenarios unavailable within previous formulations, including cases with lumped or wave ports or subregions in circuits. Thanks to its scattering nature, the formulation is solver-agnostic with the possibility to utilize an arbitrary full-wave method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00792v3</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mats Gustafsson, Lukas Jelinek, Miloslav Capek, Johan Lundgren, Kurt Schab</dc:creator>
    </item>
    <item>
      <title>Payment Scheduling in the Interval Debt Model</title>
      <link>https://arxiv.org/abs/2403.02198</link>
      <description>arXiv:2403.02198v2 Announce Type: replace-cross 
Abstract: The network-based study of financial systems has received considerable attention in recent years but has seldom explicitly incorporated the dynamic aspects of such systems. We consider this problem setting from the temporal point of view and introduce the Interval Debt Model (IDM) and some scheduling problems based on it, namely: Bankruptcy Minimization/Maximization, in which the aim is to produce a payment schedule with at most/at least a given number of bankruptcies; Perfect Scheduling, the special case of the minimization variant where the aim is to produce a schedule with no bankruptcies (that is, a perfect schedule); and Bailout Minimization, in which a financial authority must allocate a smallest possible bailout package to enable a perfect schedule. We show that each of these problems is NP-complete, in many cases even on very restricted input instances. On the positive side, we provide for Perfect Scheduling a polynomial-time algorithm on (rooted) out-trees although in contrast we prove NP-completeness on directed acyclic graphs, as well as on instances with a constant number of nodes (and hence also constant treewidth). When we allow non-integer payments, we show by a linear programming argument that the problem Bailout Minimization can be solved in polynomial time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02198v2</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.CE</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Friedetzky, David C. Kutner, George B. Mertzios, Iain A. Stewart, Amitabh Trehan</dc:creator>
    </item>
    <item>
      <title>A Resolution Independent Neural Operator</title>
      <link>https://arxiv.org/abs/2407.13010</link>
      <description>arXiv:2407.13010v3 Announce Type: replace-cross 
Abstract: The Deep Operator Network (DeepONet) is a powerful neural operator architecture that uses two neural networks to map between infinite-dimensional function spaces. This architecture allows for the evaluation of the solution field at any location within the domain but requires input functions to be discretized at identical locations, limiting practical applications. We introduce a general framework for operator learning from input-output data with arbitrary sensor locations and counts. This begins by introducing a resolution-independent DeepONet (RI-DeepONet), which handles input functions discretized arbitrarily but sufficiently finely. To achieve this, we propose two dictionary learning algorithms that adaptively learn continuous basis functions, parameterized as implicit neural representations (INRs), from correlated signals on arbitrary point clouds. These basis functions project input function data onto a finite-dimensional embedding space, making it compatible with DeepONet without architectural changes. We specifically use sinusoidal representation networks (SIRENs) as trainable INR basis functions. Similarly, the dictionary learning algorithms identify basis functions for output data, defining a new neural operator architecture: the Resolution Independent Neural Operator (RINO). In RINO, the operator learning task reduces to mapping coefficients of input basis functions to output basis functions. We demonstrate RINO's robustness and applicability in handling arbitrarily sampled input and output functions during both training and inference through several numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13010v3</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bahador Bahmani, Somdatta Goswami, Ioannis G. Kevrekidis, Michael D. Shields</dc:creator>
    </item>
  </channel>
</rss>
