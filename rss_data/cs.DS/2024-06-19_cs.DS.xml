<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Jun 2024 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Tight Streaming Lower Bounds for Deterministic Approximate Counting</title>
      <link>https://arxiv.org/abs/2406.12149</link>
      <description>arXiv:2406.12149v1 Announce Type: new 
Abstract: We study the streaming complexity of $k$-counter approximate counting. In the $k$-counter approximate counting problem, we are given an input string in $[k]^n$, and we are required to approximate the number of each $j$'s ($j\in[k]$) in the string. Typically we require an additive error $\leq\frac{n}{3(k-1)}$ for each $j\in[k]$ respectively, and we are mostly interested in the regime $n\gg k$. We prove a lower bound result that the deterministic and worst-case $k$-counter approximate counting problem requires $\Omega(k\log(n/k))$ bits of space in the streaming model, while no non-trivial lower bounds were known before. In contrast, trivially counting the number of each $j\in[k]$ uses $O(k\log n)$ bits of space. Our main proof technique is analyzing a novel potential function.
  Our lower bound for $k$-counter approximate counting also implies the optimality of some other streaming algorithms. For example, we show that the celebrated Misra-Gries algorithm for heavy hitters [MG82] has achieved optimal space usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12149v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichuan Wang</dc:creator>
    </item>
    <item>
      <title>Implementation Of Dynamic De Bruijn Graphs Via Learned Index</title>
      <link>https://arxiv.org/abs/2406.12339</link>
      <description>arXiv:2406.12339v1 Announce Type: new 
Abstract: De Bruijn graphs are essential for sequencing data analysis and must be efficiently constructed and stored for large-scale population studies. They also need to be dynamic to allow updates such as adding or removing edges and nodes. Existing dynamic implementations include DynamicBOSS and dynamicDBG. In 2018, a new family of data structures called learned indexes was introduced by Tim Kraska and Alex Beutel, with a particularly efficient implementation proposed by Paolo Ferragina and Giorgio Vinciguerra in 2020. This paper presents a new method for implementing De Bruijn graphs using learned indexes and compares its performance with current implementations. The new method shows improved time and memory efficiency for edge and node insertions, particularly with large datasets (over 110 million k-mers).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12339v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Nigrelli</dc:creator>
    </item>
    <item>
      <title>Biased Pareto Optimization for Subset Selection with Dynamic Cost Constraints</title>
      <link>https://arxiv.org/abs/2406.12383</link>
      <description>arXiv:2406.12383v1 Announce Type: new 
Abstract: Subset selection with cost constraints aims to select a subset from a ground set to maximize a monotone objective function without exceeding a given budget, which has various applications such as influence maximization and maximum coverage. In real-world scenarios, the budget, representing available resources, may change over time, which requires that algorithms must adapt quickly to new budgets. However, in this dynamic environment, previous algorithms either lack theoretical guarantees or require a long running time. The state-of-the-art algorithm, POMC, is a Pareto optimization approach designed for static problems, lacking consideration for dynamic problems. In this paper, we propose BPODC, enhancing POMC with biased selection and warm-up strategies tailored for dynamic environments. We focus on the ability of BPODC to leverage existing computational results while adapting to budget changes. We prove that BPODC can maintain the best known $(\alpha_f/2)(1-e^{-\alpha_f})$-approximation guarantee when the budget changes. Experiments on influence maximization and maximum coverage show that BPODC adapts more effectively and rapidly to budget changes, with a running time that is less than that of the static greedy algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12383v1</guid>
      <category>cs.DS</category>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan-Xuan Liu, Chao Qian</dc:creator>
    </item>
    <item>
      <title>Massively Parallel Ruling Set Made Deterministic</title>
      <link>https://arxiv.org/abs/2406.12727</link>
      <description>arXiv:2406.12727v1 Announce Type: new 
Abstract: We study the deterministic complexity of the $2$-Ruling Set problem in the model of Massively Parallel Computation (MPC) with linear and strongly sublinear local memory.
  Linear MPC: We present a constant-round deterministic algorithm for the $2$-Ruling Set problem that matches the randomized round complexity recently settled by Cambus, Kuhn, Pai, and Uitto [DISC'23], and improves upon the deterministic $O(\log \log n)$-round algorithm by Pai and Pemmaraju [PODC'22]. Our main ingredient is a simpler analysis of CKPU's algorithm based solely on bounded independence, which makes its efficient derandomization possible.
  Sublinear MPC: We present a deterministic algorithm that computes a $2$-Ruling Set in $\tilde O(\sqrt{\log n})$ rounds deterministically. Notably, this is the first deterministic ruling set algorithm with sublogarithmic round complexity, improving on the $O(\log \Delta + \log \log^* n)$-round complexity that stems from the deterministic MIS algorithm of Czumaj, Davies, and Parter [TALG'21]. Our result is based on a simple and fast randomness-efficient construction that achieves the same sparsification as that of the randomized $\tilde O(\sqrt{\log n})$-round LOCAL algorithm by Kothapalli and Pemmaraju [FSTTCS'12].</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12727v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeff Giliberti, Zahra Parsaeian</dc:creator>
    </item>
    <item>
      <title>Sample-Based Matroid Prophet Inequalities</title>
      <link>https://arxiv.org/abs/2406.12799</link>
      <description>arXiv:2406.12799v1 Announce Type: new 
Abstract: We study matroid prophet inequalities when distributions are unknown and accessible only through samples. While single-sample prophet inequalities for special matroids are known, no constant-factor competitive algorithm with even a sublinear number of samples was known for general matroids. Adding more to the stake, the single-sample version of the question for general matroids has close (two-way) connections with the long-standing matroid secretary conjecture.
  In this work, we give a $(\frac14 - \varepsilon)$-competitive matroid prophet inequality with only $O_\varepsilon(\mathrm{poly} \log n)$ samples. Our algorithm consists of two parts: (i) a novel quantile-based reduction from matroid prophet inequalities to online contention resolution schemes (OCRSs) with $O_\varepsilon(\log n)$ samples, and (ii) a $(\frac14 - \varepsilon)$-selectable matroid OCRS with $O_\varepsilon(\mathrm{poly} \log n)$ samples which carefully addresses an adaptivity challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12799v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hu Fu, Pinyan Lu, Zhihao Gavin Tang, Hongxun Wu, Jinzhao Wu, Qianfan Zhang</dc:creator>
    </item>
    <item>
      <title>Variational ground-state quantum adiabatic theorem</title>
      <link>https://arxiv.org/abs/2406.12392</link>
      <description>arXiv:2406.12392v1 Announce Type: cross 
Abstract: We present a variational quantum adiabatic theorem, which states that, under certain assumptions, the adiabatic dynamics projected onto a variational manifold follow the instantaneous variational ground state. We focus on low-entanglement variational manifolds and target Hamiltonians with classical ground states. Despite the presence of highly entangled intermediate states along the exact quantum annealing path, the variational evolution converges to the target ground state. We demonstrate this approach with several examples that align with our theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12392v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bojan \v{Z}unkovi\v{c}, Pietro Torta, Giovanni Pecci, Guglielmo Lami, Mario Collura</dc:creator>
    </item>
    <item>
      <title>Parameterized Shortest Path Reconfiguration</title>
      <link>https://arxiv.org/abs/2406.12717</link>
      <description>arXiv:2406.12717v1 Announce Type: cross 
Abstract: An st-shortest path, or st-path for short, in a graph G is a shortest (induced) path from s to t in G. Two st-paths are said to be adjacent if they differ on exactly one vertex. A reconfiguration sequence between two st-paths P and Q is a sequence of adjacent st-paths starting from P and ending at Q. Deciding whether there exists a reconfiguration sequence between two given $st$-paths is known to be PSPACE-complete, even on restricted classes of graphs such as graphs of bounded bandwidth (hence pathwidth). On the positive side, and rather surprisingly, the problem is polynomial-time solvable on planar graphs. In this paper, we study the parameterized complexity of the Shortest Path Reconfiguration (SPR) problem. We show that SPR is W[1]-hard parameterized by k + \ell, even when restricted to graphs of bounded (constant) degeneracy; here k denotes the number of edges on an st-path, and \ell denotes the length of a reconfiguration sequence from P to Q. We complement our hardness result by establishing the fixed-parameter tractability of SPR parameterized by \ell and restricted to nowhere-dense classes of graphs. Additionally, we establish fixed-parameter tractability of SPR when parameterized by the treedepth, by the cluster-deletion number, or by the modular-width of the input graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12717v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Bousquet, Kshitij Gajjar, Abhiruk Lahiri, Amer E. Mouawad</dc:creator>
    </item>
    <item>
      <title>Stay or Switch: Competitive Online Algorithms for Energy Plan Selection in Energy Markets with Retail Choice</title>
      <link>https://arxiv.org/abs/1905.07145</link>
      <description>arXiv:1905.07145v3 Announce Type: replace 
Abstract: Energy markets with retail choice enable customers to switch energy plans among competitive retail suppliers. Despite the promising benefits of more affordable prices and better savings to customers, there appears subsided participation in energy retail markets from residential customers. One major reason is the complex online decision-making process for selecting the best energy plan from a multitude of options that hinders average consumers. In this paper, we shed light on the online energy plan selection problem by providing effective competitive online algorithms. We first formulate the online energy plan selection problem as a metrical task system problem with temporally dependent switching costs. For the case of constant cancellation fee, we present a 3-competitive deterministic online algorithm and a 2-competitive randomized online algorithm for solving the energy plan selection problem. We show that the two competitive ratios are the best possible among deterministic and randomized online algorithms, respectively. We further extend our online algorithms to the case where the cancellation fee is linearly proportional to the residual contract duration. Through empirical evaluations using real-world household and energy plan data, we show that our deterministic online algorithm can produce on average 14.6% cost saving, as compared to 16.2% by the offline optimal algorithm, while our randomized online algorithm can further improve cost saving by up to 0.5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:1905.07145v3</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianing Zhai, Sid Chi-Kin Chau, Minghua Chen</dc:creator>
    </item>
    <item>
      <title>Space-Efficient Graph Coarsening with Applications to Succinct Planar Encodings</title>
      <link>https://arxiv.org/abs/2205.06128</link>
      <description>arXiv:2205.06128v3 Announce Type: replace 
Abstract: We present a novel space-efficient graph coarsening technique for $n$-vertex planar graphs $G$, called cloud partition, which partitions the vertices $V(G)$ into disjoint sets $C$ of size $O(\log n)$ such that each $C$ induces a connected subgraph of $G$. Using this partition $P$ we construct a so-called structure-maintaining minor $F$ of $G$ via specific contractions within the disjoint sets such that $F$ has $O(n/\log n)$ vertices. The combination of $(F, P)$ is referred to as a cloud decomposition.
  For planar graphs we show that a cloud decomposition can be constructed in $O(n)$ time and using $O(n)$ bits. Given a cloud decomposition $(F, P)$ constructed for a planar graph $G$ we are able to find a balanced separator of $G$ in $O(n/\log n)$ time. Contrary to related publications, we do not make use of an embedding of the planar input graph. We generalize our cloud decomposition from planar graphs to $H$-minor-free graphs for any fixed graph $H$. This allows us to construct the succinct encoding scheme for $H$-minor-free graphs due to Blelloch and Farzan (CPM 2010) in $O(n)$ time and $O(n)$ bits improving both runtime and space by a factor of $\Theta(\log n)$.
  As an additional application of our cloud decomposition we show that, for $H$-minor-free graphs, a tree decomposition of width $O(n^{1/2 + \epsilon})$ for any $\epsilon &gt; 0$ can be constructed in $O(n)$ bits and a time linear in the size of the tree decomposition. Finally, we implemented our cloud decomposition algorithm and experimentally verified its practical effectiveness on both randomly generated graphs and real-world graphs such as road networks. The obtained data shows that a simplified version of our algorithms suffices in a practical setting, as many of the theoretical worst-case scenarios are not present in the graphs we encountered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.06128v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nina Hammer, Frank Kammer, Johannes Meintrup</dc:creator>
    </item>
    <item>
      <title>Isomorphism for Tournaments of Small Twin Width</title>
      <link>https://arxiv.org/abs/2312.02048</link>
      <description>arXiv:2312.02048v2 Announce Type: replace 
Abstract: We prove that isomorphism of tournaments of twin width at most $k$ can be decided in time $k^{O(\log k)}n^{O(1)}$. This implies that the isomorphism problem for classes of tournaments of bounded or moderately growing twin width is in polynomial time. By comparison, there are classes of undirected graphs of bounded twin width that are isomorphism complete, that is, the isomorphism problem for the classes is as hard as the general graph isomorphism problem. Twin width is a graph parameter that has been introduced only recently (Bonnet et al., FOCS 2020), but has received a lot of attention in structural graph theory since then. On directed graphs, it is functionally smaller than clique width. We prove that on tournaments (but not on general directed graphs) it is also functionally smaller than directed tree width (and thus, the same also holds for cut width and directed path width). Hence, our result implies that tournament isomorphism testing is also fixed-parameter tractable when parameterized by any of these parameters.
  Our isomorphism algorithm heavily employs group-theoretic techniques. This seems to be necessary: as a second main result, we show that the combinatorial Weisfeiler-Leman algorithm does not decide isomorphism of tournaments of twin width at most 35 if its dimension is $o(n)$. (Throughout this abstract, $n$ is the order of the input graphs.)</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02048v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Grohe, Daniel Neuen</dc:creator>
    </item>
    <item>
      <title>An overview of some single machine scheduling problems: polynomial algorithms, complexity and approximability</title>
      <link>https://arxiv.org/abs/2405.18789</link>
      <description>arXiv:2405.18789v2 Announce Type: replace 
Abstract: Since the publication of the first scheduling paper in 1954, a huge number of works dealing with different types of single machine problems appeared. They addressed many heuristics and enumerative procedures, complexity results or structural properties of certain problems. Regarding surveys, often particular subjects like special objective functions are discussed, or more general scheduling problems were surveyed, where a substantial part is devoted to single machine problems. In this paper we present some results on polynomial algorithms, complexity and approximation issues, where the main focus is on results, which have been published during the last decades in papers, where at least one of the first two authors of this paper was involved. We hope that the reviewed results will stimulate further investigation in related research fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18789v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nodari Vakhania, Frank Werner, Kevin Johedan Ram\'irez-Fuentes, V\'ictor Pacheco-Valencia</dc:creator>
    </item>
    <item>
      <title>Optimized Deletion From an AVL Tree</title>
      <link>https://arxiv.org/abs/2406.05162</link>
      <description>arXiv:2406.05162v3 Announce Type: replace 
Abstract: An AVL tree is a binary search tree that guarantees $ O\left( \log n \right ) $ search. The guarantee is obtained at the cost of rebalancing the AVL tree, potentially after every insertion or deletion. This article proposes a deletion algorithm that reduces rebalancing after deletion by 19 percent compared to previously reported deletion algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05162v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Russell A. Brown</dc:creator>
    </item>
    <item>
      <title>Randomized Binary and Tree Search under Pressure</title>
      <link>https://arxiv.org/abs/2406.06468</link>
      <description>arXiv:2406.06468v2 Announce Type: replace 
Abstract: We study a generalized binary search problem on the line and general trees. On the line (e.g., a sorted array), binary search finds a target node in $O(\log n)$ queries in the worst case, where $n$ is the number of nodes. In situations with limited budget or time, we might only be able to perform a few queries, possibly sub-logarithmic many. In this case, it is impossible to guarantee that the target will be found regardless of its position. Our main result is the construction of a randomized strategy that maximizes the minimum (over the target position) probability of finding the target. Such a strategy provides a natural solution where there is no apriori (stochastic) information of the target's position. As with regular binary search, we can find and run the strategy in $O(\log n)$ time (and using only $O(\log n)$ random bits). Our construction is obtained by reinterpreting the problem as a two-player (\textit{seeker} and \textit{hider}) zero-sum game and exploiting an underlying number theoretical structure.
  Furthermore, we generalize the setting to study a search game on trees. In this case, a query returns the edge's endpoint closest to the target. Again, when the number of queries is bounded by some given $k$, we quantify a \emph{the-less-queries-the-better} approach by defining a seeker's profit $p$ depending on the number of queries needed to locate the hider. For the linear programming formulation of the corresponding zero-sum game, we show that computing the best response for the hider (i.e., the separation problem of the underlying dual LP) can be done in time $O(n^2 2^{2k})$, where $n$ is the size of the tree. This result allows to compute a Nash equilibrium in polynomial time whenever $k=O(\log n)$. In contrast, computing the best response for the hider is NP-hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06468v2</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agust\'in Caracci, Christoph D\"urr, Jos\'e Verschae</dc:creator>
    </item>
    <item>
      <title>A New Construction of the Vietoris-Rips Complex</title>
      <link>https://arxiv.org/abs/2301.07191</link>
      <description>arXiv:2301.07191v3 Announce Type: replace-cross 
Abstract: We present a new, inductive construction of the Vietoris-Rips complex, in which we take advantage of a small amount of unexploited combinatorial structure in the $k$-skeleton of the complex in order to avoid unnecessary comparisons when identifying its $(k+1)$-simplices. In doing so, we achieve a significant reduction in the number of comparisons required to construct the Vietoris-Rips compared to state-of-the-art algorithms, which is seen here by examining the computational complexity of the critical step in the algorithms. In experiments comparing a C/C++ implementation of our algorithm to the GUDHI v3.9.0 software package, this results in an observed $5$-$10$-fold improvement in speed of on sufficiently sparse Erd\H{o}s-R\'enyi graphs with the best advantages as the graphs become sparser, as well as for higher dimensional Vietoris-Rips complexes. We further clarify that the algorithm described in Boissonnat and Maria (https://doi.org/10.1007/978-3-642-33090-2_63) for the construction of the Vietoris-Rips complex is exactly the Incremental Algorithm from Zomorodian (https://doi.org/10.1016/j.cag.2010.03.007), albeit with the additional requirement that the result be stored in a tree structure, and we explain how these techniques are different from the algorithm presented here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07191v3</guid>
      <category>math.CO</category>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <category>math.AT</category>
      <category>math.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Rieser</dc:creator>
    </item>
    <item>
      <title>Gap-Free Clustering: Sensitivity and Robustness of SDP</title>
      <link>https://arxiv.org/abs/2308.15642</link>
      <description>arXiv:2308.15642v2 Announce Type: replace-cross 
Abstract: We study graph clustering in the Stochastic Block Model (SBM) in the presence of both large clusters and small, unrecoverable clusters. Previous convex relaxation approaches achieving exact recovery do not allow any small clusters of size $o(\sqrt{n})$, or require a size gap between the smallest recovered cluster and the largest non-recovered cluster. We provide an algorithm based on semidefinite programming (SDP) which removes these requirements and provably recovers large clusters regardless of the remaining cluster sizes. Mid-sized clusters pose unique challenges to the analysis, since their proximity to the recovery threshold makes them highly sensitive to small noise perturbations and precludes a closed-form candidate solution. We develop novel techniques, including a leave-one-out-style argument which controls the correlation between SDP solutions and noise vectors even when the removal of one row of noise can drastically change the SDP solution. We also develop improved eigenvalue perturbation bounds of potential independent interest. Our results are robust to certain semirandom settings that are challenging for alternative algorithms. Using our gap-free clustering procedure, we obtain efficient algorithms for the problem of clustering with a faulty oracle with superior query complexities, notably achieving $o(n^2)$ sample complexity even in the presence of a large number of small clusters. Our gap-free clustering procedure also leads to improved algorithms for recursive clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15642v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Zurek, Yudong Chen</dc:creator>
    </item>
    <item>
      <title>On Differentially Private Subspace Estimation in a Distribution-Free Setting</title>
      <link>https://arxiv.org/abs/2402.06465</link>
      <description>arXiv:2402.06465v2 Announce Type: replace-cross 
Abstract: Private data analysis faces a significant challenge known as the curse of dimensionality, leading to increased costs. However, many datasets possess an inherent low-dimensional structure. For instance, during optimization via gradient descent, the gradients frequently reside near a low-dimensional subspace. If the low-dimensional structure could be privately identified using a small amount of points, we could avoid paying for the high ambient dimension.
  On the negative side, Dwork, Talwar, Thakurta, and Zhang (STOC 2014) proved that privately estimating subspaces, in general, requires an amount of points that has a polynomial dependency on the dimension. However, their bound do not rule out the possibility to reduce the number of points for "easy'' instances. Yet, providing a measure that captures how much a given dataset is "easy'' for this task turns out to be challenging, and was not properly addressed in prior works.
  Inspired by the work of Singhal and Steinke (NeurIPS 2021), we provide the first measures that quantify easiness as a function of multiplicative singular-value gaps in the input dataset, and support them with new upper and lower bounds. In particular, our results determine the first type of gap that is sufficient and necessary for estimating a subspace with an amount of points that is independent of the dimension. Furthermore, we realize our upper bounds using a practical algorithm and demonstrate its advantage in high-dimensional regimes compared to prior approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06465v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eliad Tsfadia</dc:creator>
    </item>
  </channel>
</rss>
