<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jan 2026 05:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mind the Gap: Why Neural Memory Fails Under Semantic Density</title>
      <link>https://arxiv.org/abs/2601.15313</link>
      <description>arXiv:2601.15313v1 Announce Type: new 
Abstract: The brain solves a problem that current AI architectures struggle to manage: storing specific episodic facts without corrupting general semantic knowledge. Neuroscience explains this through Complementary Learning Systems theory - a fast hippocampal system for episodic storage using pattern-separated representations, and a slow neocortical system for extracting statistical regularities. Current AI systems lack this separation, attempting both functions through neural weights alone. We identify the 'Stability Gap' in online neural memory: fast-weight mechanisms that write facts into shared continuous parameters collapse to near-random accuracy within tens of semantically related facts. Through semantic density (rho), we show collapse occurs with as few as N=5 facts at high density (rho &gt; 0.6) or N ~ 20-75 at moderate density - a phenomenon we formalise as the Orthogonality Constraint. This failure persists even with perfect attention and unlimited context, arising from write-time interference when storage and retrieval share the same substrate. We also identify schema drift and version ambiguity as primary failure modes in production systems, observing 40-70% schema consistency and 0-100% clean correction rates. Context-based memory incurs 30-300% cost premium over selective retrieval. We propose Knowledge Objects (KOs): discrete, typed memory units with controlled vocabularies and explicit version chains. Paired with neural weights, KOs enable a true complementary learning architecture, suggesting reliable AI memory may require this bicameral design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15313v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matt Beton, Simran Chana</dc:creator>
    </item>
    <item>
      <title>Beyond the Einstein-Bohr Debate: Cognitive Complementarity and the Emergence of Quantum Intuition</title>
      <link>https://arxiv.org/abs/2601.15314</link>
      <description>arXiv:2601.15314v1 Announce Type: new 
Abstract: Recent high-precision experimental confirmations of quantum complementarity have revitalized foundational debates about measurement, description, and realism. This article argues that complementarity is most productively interpreted as an epistemic principle--constraining what can be simultaneously accessed and represented--rather than as an ontological claim about quantum reality. Reexamining the Einstein-Bohr debate through this lens reveals a persistent tension between descriptive completeness and contextual meaning, a tension experiments clarify but do not dissolve. Building on this analysis, we introduce cognitive complementarity as a structural principle governing reasoning under non-classical uncertainty, where mutually constraining representations cannot be jointly optimized. Within this framework, we propose quantum intuition as a testable cognitive capacity: the ability to sustain representational plurality, regulate commitment timing, and resolve perspective-incompatibilities in a context-sensitive manner. Formulated as a naturalistic construct grounded in shared informational constraints, quantum intuition offers a principled bridge between quantum measurement theory and cognition. This work reframes the historical debate, extends epistemic lessons from quantum foundations into cognitive science, and outlines empirical pathways for studying decision-making in contexts of irreducible uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15314v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>quant-ph</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lalit Kumar Shukla</dc:creator>
    </item>
    <item>
      <title>Large Language Models as Simulative Agents for Neurodivergent Adult Psychometric Profiles</title>
      <link>https://arxiv.org/abs/2601.15319</link>
      <description>arXiv:2601.15319v1 Announce Type: new 
Abstract: Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder (ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive Disengagement Syndrome (CDS), is marked by substantial symptom overlap that limits the discriminant sensitivity of standard psychometric instruments. While recent work suggests that Large Language Models (LLMs) can simulate human psychometric responses from qualitative data, it remains unclear whether they can accurately and stably model neurodevelopmental traits rather than broad personality characteristics. This study examines whether LLMs can generate psychometric responses that approximate those of real individuals when grounded in a structured qualitative interview, and whether such simulations are sensitive to variations in trait intensity. Twenty-six adults completed a 29-item open-ended interview and four standardized self-report measures (ASRS, BAARS-IV, AQ, RAADS-R). Two LLMs (GPT-4o and Qwen3-235B-A22B) were prompted to infer an individual psychological profile from interview content and then respond to each questionnaire in-role. Accuracy, reliability, and sensitivity were assessed using group-level comparisons, error metrics, exact-match scoring, and a randomized baseline. Both models outperformed random responses across instruments, with GPT-4o showing higher accuracy and reproducibility. Simulated responses closely matched human data for ASRS, BAARS-IV, and RAADS-R, while the AQ revealed subscale-specific limitations, particularly in Attention to Detail. Overall, the findings indicate that interview-grounded LLMs can produce coherent and above-chance simulations of neurodevelopmental traits, supporting their potential use as synthetic participants in early-stage psychometric research, while highlighting clear domain-specific constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15319v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Chiappone, Davide Marocco, Nicola Milano</dc:creator>
    </item>
    <item>
      <title>On Brain as a Mathematical Manifold: Neural Manifolds, Sheaf Semantics, and Leibnizian Harmony</title>
      <link>https://arxiv.org/abs/2601.15320</link>
      <description>arXiv:2601.15320v1 Announce Type: new 
Abstract: We present a mathematical and philosophical framework in which brain function is modeled using sheaf theory over neural state spaces. Local neural or cognitive functions are represented as sections of a sheaf, while global coherence corresponds to the existence of global sections. Brain pathologies are interpreted as obstructions to such global integration and are classified using tools from sheaf cohomology. The framework builds on the neural manifold program in contemporary neuroscience and on standard results in sheaf theory, and is further interpreted through a Leibnizian lens \cite{Churchland2012, Leibniz1714, MacLaneMoerdijk, Perich2025}. This paper is intended as a conceptual and formal proposal rather than a complete empirical theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15320v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takao Inou\'e</dc:creator>
    </item>
    <item>
      <title>Analysis of the Ventriloquism Aftereffect Using Network Theory Techniques</title>
      <link>https://arxiv.org/abs/2601.15321</link>
      <description>arXiv:2601.15321v1 Announce Type: new 
Abstract: Ventriloquism After-Effect is the phenomenon where sustained exposure to the ventriloquist illusion causes a change in unisensory auditory localization towards the location where the visual stimulus was present. We investigate the recalibration in EEG networks that causes this change and the track the timeline of changes in the auditory processing pathway. Our results obtained using network analysis, non-stationary time series analysis and multivariate pattern classification show that recalibration takes place early in the auditory processing pathway and the after-effect decays with time after exposure to the illusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15321v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sayan Saha</dc:creator>
    </item>
    <item>
      <title>Learning Discrete Successor Transitions in Continuous Attractor Networks: Emergence, Limits, and Topological Constraints</title>
      <link>https://arxiv.org/abs/2601.15336</link>
      <description>arXiv:2601.15336v1 Announce Type: new 
Abstract: Continuous attractor networks (CANs) are a well-established class of models for representing low-dimensional continuous variables such as head direction, spatial position, and phase. In canonical spatial domains, transitions along the attractor manifold are driven by continuous displacement signals, such as angular velocity-provided by sensorimotor systems external to the CAN itself. When such signals are not explicitly provided as dedicated displacement inputs, it remains unclear whether attractor-based circuits can reliably acquire recurrent dynamics that support stable state transitions, or whether alternative predictive strategies dominate.
  In this work, we present an experimental framework for training CANs to perform successor-like transitions between stable attractor states in the absence of externally provided displacement signals. We compare two recurrent topologies, a circular ring and a folded snake manifold, and systematically vary the temporal regime under which stability is evaluated. We find that, under short evaluation windows, networks consistently converge to impulse-driven associative solutions that achieve high apparent accuracy yet lack persistent attractor dynamics. Only when stability is explicitly enforced over extended free-run periods do genuine attractor-based transition dynamics emerge. This suggests that shortcut solutions are the default outcome of local learning in recurrent networks, while attractor dynamics represent a constrained regime rather than a generic result.
  Furthermore, we demonstrate that topology strictly limits the capacity for learned transitions. While the continuous ring topology achieves perfect stability over long horizons, the folded snake topology hits a geometric limit characterized by failure at manifold discontinuities, which neither curriculum learning nor basal ganglia-inspired gating can fully overcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15336v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Brownell</dc:creator>
    </item>
    <item>
      <title>A Dual-Head Transformer-State-Space Architecture for Neurocircuit Mechanism Decomposition from fMRI</title>
      <link>https://arxiv.org/abs/2601.15344</link>
      <description>arXiv:2601.15344v1 Announce Type: new 
Abstract: Precision psychiatry aspires to elucidate brain-based biomarkers of psychopathology to bolster disease risk assessment and treatment development. To this end, functional magnetic resonance imaging (fMRI) has helped triangulate brain circuits whose functional features are correlated with or even predictive of forms of psychopathology. Yet, fMRI biomarkers to date remain largely descriptive identifiers of where, rather than how, neurobiology is aberrant, limiting their utility for guiding treatment. We present a method for decomposing fMRI-based functional connectivity (FC) into constituent biomechanisms - output drive, input responsivity, modulator gating - with clearer alignment to differentiable therapeutic interventions. Neurocircuit mechanism decomposition (NMD) integrates (i) a graph-constrained, lag-aware transformer to estimate directed, pathway-specific routing distributions and drive signals, with (ii) a measurement-aware state-space model (SSM) that models hemodynamic convolution and recovers intrinsic latent dynamics. This dual-head architecture yields interpretable circuit parameters that may provide a more direct bridge from fMRI to treatment strategy selection. We instantiate the model in an anatomically and electrophysiologically well-defined circuit: the cortico-basal ganglia-thalamo-cortical loop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15344v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cole Korponay</dc:creator>
    </item>
    <item>
      <title>Dynamic Mean Field Theories for Nonlinear Noise in Recurrent Neuronal Networks</title>
      <link>https://arxiv.org/abs/2601.15462</link>
      <description>arXiv:2601.15462v1 Announce Type: new 
Abstract: Strong, correlated noise in recurrent neural circuits often passes through nonlinear transfer functions, complicating dynamical mean-field analyses of complex phenomena such as transients and bifurcations. We introduce a method that replaces nonlinear functions of Ornstein-Uhlenbeck (OU) noise with a Gaussian-equivalent process matched in mean and covariance, and combine this with a lognormal moment closure for expansive nonlinearities to derive a closed dynamical mean-field theory for recurrent neuronal networks. The resulting theory captures order-one transients, fixed points, and noise-induced shifts of bifurcation structure, and outperforms standard linearization-based approximations in the strong-fluctuation regime. More broadly, the approach applies whenever dynamics depend smoothly on OU processes via nonlinear transformations, offering a tractable route to noise-dependent phase diagrams in computational neuroscience models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15462v1</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shoshana Chipman, Brent Doiron</dc:creator>
    </item>
    <item>
      <title>Resting-State Functional Connectivity Correlates of Emotional Memory Control under Cognitive load in Subclinical Anxiety</title>
      <link>https://arxiv.org/abs/2601.15689</link>
      <description>arXiv:2601.15689v1 Announce Type: new 
Abstract: Volitional memory control supports adaptive cognition by enabling intentional Recall of goal-relevant information and Suppression of unwanted memories. While neural mechanisms underlying Recall and Suppression have been studied largely in isolation, less is known about the large-scale brain networks supporting these processes under competing cognitive demands, particularly as a function of subclinical anxiety. Here, we examined control of emotionally valenced memories during directed Recall and Suppression while 47 participants concurrently performed an independent visual working memory task. Cognitive control efficiency was quantified using the Balanced Integration Score (BIS), and seed-to-voxel resting-state functional connectivity (rsFC) was used to characterize intrinsic network organization. Dissociable rsFC profiles were associated with memory control efficiency across emotional valences and were selectively moderated by anxiety. More efficient Suppression of positive memories was linked to reduced connectivity between the anterior cingulate cortex and posterior perceptual-midline regions, as well as diminished hippocampal-frontal pole coupling. In contrast, efficient Suppression of negative memories was associated with increased connectivity between posterior parietal and lateral occipital regions. Anxiety moderated relationships between cognitive efficiency and prefrontal connectivity during Suppression of positive memories and Recall of positive and neutral memories. Direct comparisons further revealed stronger hippocampal-thalamic rsFC during Suppression relative to Recall of positive memories. Together, these findings delineate the functional brain architecture supporting volitional control of emotional memories under cognitive load and demonstrate that anxiety severity selectively shapes these network-level mechanisms across the anxiety continuum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15689v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shruti Kinger, Mrinmoy Chakrabarty</dc:creator>
    </item>
    <item>
      <title>Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis from MRI and clinical features</title>
      <link>https://arxiv.org/abs/2601.15530</link>
      <description>arXiv:2601.15530v1 Announce Type: cross 
Abstract: Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques and tau tangles in the brain can be diagnosed with high accuracy based on protein biomarkers via PET or CSF analysis. However, due to the invasive nature of biomarker collection, most AD diagnoses are made in memory clinics using cognitive tests and evaluation of hippocampal atrophy based on MRI. While clinical assessment and hippocampal volume show high diagnostic accuracy for amnestic or typical AD (tAD), a substantial subgroup of AD patients with atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis of atAD patients, we propose a machine learning approach to distinguish between atAD and non-AD cognitive impairment using clinical testing battery and MRI data collected as standard-of-care. We develop and evaluate our approach using 1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685 cognitively normal) collected from one private data set and two public data sets from the National Alzheimer's Coordinating Center (NACC) and the Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD vs. non-AD classification experiments using clinical features and hippocampal volume as well as a comprehensive set of MRI features from across the brain. The best performance is achieved by incorporating additional important MRI features, which outperforms using hippocampal volume alone. Furthermore, we use the Boruta statistical approach to identify and visualize significant brain regions distinguishing between diagnostic groups. Our ML approach improves the percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed approach has important implications for improving diagnostic accuracy for non-amnestic atAD in clinical settings using only clinical testing battery and MRI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15530v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Megan A. Witherow, Michael L. Evans, Ahmed Temtam, Hamid Okhravi, Khan M. Iftekharuddin</dc:creator>
    </item>
    <item>
      <title>Functional dissociations versus post-hoc selection: Moving beyond the Stockart et al. (2025) compromise</title>
      <link>https://arxiv.org/abs/2411.15078</link>
      <description>arXiv:2411.15078v4 Announce Type: replace 
Abstract: Stockart et al. (2025) recommend guidelines for best practices in the field of unconscious cognition. However, they condone the repeatedly criticized technique of excluding trials with high visibility ratings or of participants with high sensitivity for the critical stimulus. Based on standard signal detection theory for discrimination judgments, we show that post-hoc trial selection only isolates points of neutral response bias but remains consistent with uncomfortably high levels of sensitivity. We argue that post-hoc selection constitutes a sampling fallacy that capitalizes on chance, generates regression artifacts, and wrongly ascribes unconscious processing to stimulus conditions that may be far from indiscriminable. As an alternative, we advocate the study of functional dissociations, where direct (D) and indirect (I) measures are conceptualized as spanning up a two-dimensional D-I-space and where single, sensitivity, and double dissociations appear as distinct curve patterns. While Stockart et al.'s recommendations cover only a single line of that space where D is close to zero, functional dissociations can utilize the entire space, circumventing requirements like null visibility and exhaustive reliability, and allowing for the planful measurement of theoretically meaningful functional relationships between experimentally controlled variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15078v4</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thomas Schmidt, Xin Ying Lee, Maximilian P. Wolkersdorfer</dc:creator>
    </item>
    <item>
      <title>A comprehensive framework for statistical testing of brain dynamics</title>
      <link>https://arxiv.org/abs/2505.02541</link>
      <description>arXiv:2505.02541v3 Announce Type: replace 
Abstract: Neural activity data can be associated with behavioral and physiological variables by analyzing their changes in the temporal domain. However, such relationships are often difficult to quantify and test, requiring advanced computational modeling approaches. Here, we provide a protocol for the statistical analysis of brain dynamics and for testing their associations with behavioral, physiological and other non-imaging variables. The protocol is based on an open-source Python package built on a generalization of the hidden Markov model (HMM) - the Gaussian-linear HMM - and supports multiple experimental modalities, including task-based and resting-state studies, often used to explore a wide range of questions in neuroscience and mental health. Our toolbox is available as both a Python library and a graphical interface, so it can be used by researchers with or without programming experience. Statistical inference is performed by using permutation-based methods and structured Monte Carlo resampling, and the framework can easily handle confounding variables, multiple testing corrections and hierarchical relationships within the data, among other features. The package includes tools developed to facilitate the intuitive visualization of statistical results, along with comprehensive documentation and step-by-step tutorials for data interpretation. Overall, the protocol covers the full workflow for the statistical analysis of functional neural data and their temporal dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02541v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41596-025-01300-2</arxiv:DOI>
      <arxiv:journal_reference>Nature Protocols, 2026</arxiv:journal_reference>
      <dc:creator>Nick Yao Larsen, Laura Paulsen, Christine Ahrends, Anderson M. Winkler, Diego Vidaurre</dc:creator>
    </item>
    <item>
      <title>Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning</title>
      <link>https://arxiv.org/abs/2506.03088</link>
      <description>arXiv:2506.03088v2 Announce Type: replace 
Abstract: The mapping from sound to neural activity that underlies hearing is highly non-linear. The first few stages of this mapping in the cochlea have been modelled successfully, with biophysical models built by hand and, more recently, with DNN models trained on datasets simulated by biophysical models. Modelling the auditory brain has been a challenge because central auditory processing is too complex for models to be built by hand, and datasets for training DNN models directly have not been available. Recent work has taken advantage of large-scale high resolution neural recordings from the auditory midbrain to build a DNN model of normal hearing with great success. But this model assumes that auditory processing is the same in all brains, and therefore it cannot capture the widely varying effects of hearing loss.
  We propose a novel variational-conditional model to learn to encode the space of hearing loss directly from recordings of neural activity in the auditory midbrain of healthy and noise exposed animals. With hearing loss parametrised by only 6 free parameters per animal, our model accurately predicts 62% of the explainable variance in neural responses from normal hearing animals and 68% for hearing impaired animals, within a few percentage points of state of the art animal specific models. We demonstrate that the model can be used to simulate realistic activity from out of sample animals by fitting only the learned conditioning parameters with Bayesian optimisation, achieving crossentropy loss within 2% of the optimum in 15-30 iterations. Including more animals in the training data slightly improved the performance on unseen animals. This model will enable future development of parametrised hearing loss compensation models trained to directly restore normal neural coding in hearing impaired brains, which can be quickly fitted for a new user by human in the loop optimisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03088v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lloyd Pellatt, Fotios Drakopoulos, Shievanie Sabesan, Nicholas A. Lesica</dc:creator>
    </item>
    <item>
      <title>Dynamical Mechanisms for Coordinating Long-term Working Memory Based on the Precision of Spike-timing in Cortical Neurons</title>
      <link>https://arxiv.org/abs/2512.15891</link>
      <description>arXiv:2512.15891v3 Announce Type: replace 
Abstract: In the last century, most sensorimotor studies of cortical neurons relied on average firing rates. Rate coding is efficient for fast sensorimotor processing that occurs within a few seconds. Much less is known about long-term working memory with a time scale of hours (Ericsson and Kintsch, 1995). The discovery of millisecond-precision spike initiation in cortical neurons was unexpected (Mainen and Sejnowski, 1995). Even more striking was the precision of spiking in vivo, in response to rapidly fluctuating sensory inputs, suggesting that neural circuits could preserve and manipulate sensory information through spike timing. High temporal resolution enables a broader range of neural codes. It could also support spike-timing-dependent plasticity (STDP), which is triggered by the relative timing of spikes between presynaptic and postsynaptic neurons in the millisecond range. What spike-timing mechanisms could regulate STDP in vivo? Cortical traveling waves have been observed across many frequency bands with high temporal precision. Traveling waves have wave fronts that could link spike timing to STDP. As a wave front passes through a cortical column, excitatory synapses on the dendrites of both pyramidal and basket cells are stimulated synchronously. Inhibitory basket cells form a calyx on pyramidal cell bodies, and inhibitory rebound following a strong transient hyperpolarization can trigger a backpropagating action potential, which arrives shortly after the excitatory inputs on pyramidal dendrites. STDP activated in this way could persist for hours, creating a second-tier network. This temporary network could support long-term working memory, a cognitive network riding above the long-term sensorimotor network. On their own, traveling waves and STDP have not yet yielded new insights into cortical function. Together, they could be responsible for how we think (Sejnowski, 2025).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15891v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Terrence J. Sejnowski</dc:creator>
    </item>
    <item>
      <title>Graph Neural Network Reveals the Local Cortical Morphology of Brain Aging in Normal Cognition and Alzheimers Disease</title>
      <link>https://arxiv.org/abs/2601.10912</link>
      <description>arXiv:2601.10912v3 Announce Type: replace 
Abstract: Estimating brain age (BA) from T1-weighted magnetic resonance images (MRIs) provides a useful approach to map the anatomic features of brain senescence. Whereas global BA (GBA) summarizes overall brain health, local BA (LBA) can reveal spatially localized patterns of aging. Although previous studies have examined anatomical contributors to GBA, no framework has been established to compute LBA using cortical morphology. To address this gap, we introduce a novel graph neural network (GNN) that uses morphometric features (cortical thickness, curvature, surface area, gray/white matter intensity ratio and sulcal depth) to estimate LBA across the cortical surface at high spatial resolution (mean inter-vertex distance = 1.37 mm). Trained on cortical surface meshes extracted from the MRIs of cognitively normal adults (N = 14,250), our GNN identifies prefrontal and parietal association cortices as early sites of morphometric aging, in concordance with biological theories of brain aging. Feature comparison using integrated gradients reveals that morphological aging is driven primarily by changes in surface area (gyral crowns and highly folded regions) and cortical thickness (occipital lobes), with additional contributions from gray/white matter intensity ratio (frontal lobes and sulcal troughs) and curvature (sulcal troughs). In Alzheimers disease (AD), as expected, the model identifies widespread, excessive morphological aging in parahippocampal gyri and related temporal structures. Significant associations are found between regional LBA gaps and neuropsychological measures descriptive of AD-related cognitive impairment, suggesting an intimate relationship between morphological cortical aging and cognitive decline. These results highlight the ability of GNN-derived gero-morphometry to provide insights into local brain aging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10912v3</guid>
      <category>q-bio.NC</category>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel D. Anderson, Nikhil N. Chaudhari, Nahian F. Chowdhury, Jordan Jomsky, Andrei Irimia, Xiaoyu Rayne Zheng, Alzheimers Disease Neuroimaging Initiative</dc:creator>
    </item>
    <item>
      <title>Dynamical stability for dense patterns in discrete attractor neural networks</title>
      <link>https://arxiv.org/abs/2507.10383</link>
      <description>arXiv:2507.10383v4 Announce Type: replace-cross 
Abstract: Neural networks storing multiple discrete attractors are canonical models of biological memory. Previously, the dynamical stability of such networks could only be guaranteed under highly restrictive conditions. Here, we derive a theory of the local stability of discrete fixed points in a broad class of networks with graded neural activities and in the presence of noise. By directly analyzing the bulk and the outliers of the Jacobian spectrum, we show that all fixed points are stable below a critical load that is distinct from the classical \textit{critical capacity} and depends on the statistics of neural activities in the fixed points as well as the single-neuron activation function. Our analysis highlights the computational benefits of threshold-linear activation and sparse-like patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10383v4</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uri Cohen, M\'at\'e Lengyel</dc:creator>
    </item>
  </channel>
</rss>
