<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Nov 2025 02:51:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>SALPA: Spaceborne LiDAR Point Adjustment for Enhanced GEDI Footprint Geolocation</title>
      <link>https://arxiv.org/abs/2511.17600</link>
      <description>arXiv:2511.17600v1 Announce Type: new 
Abstract: Spaceborne Light Detection and Ranging (LiDAR) systems, such as NASA's Global Ecosystem Dynamics Investigation (GEDI), provide forest structure for global carbon assessments. However, geolocation uncertainties (typically 5-15 m) propagate systematically through derived products, undermining forest profile estimates, including carbon stock assessments. Existing correction methods face critical limitations: waveform simulation approaches achieve meter-level accuracy but require high-resolution LiDAR data unavailable in most regions, while terrain-based methods employ deterministic grid searches that may overlook optimal solutions in continuous solution spaces. We present SALPA (Spaceborne LiDAR Point Adjustment), a multi-algorithm optimization framework integrating three optimization paradigms with five distance metrics. Operating exclusively with globally available digital elevation models and geoid data, SALPA explores continuous solution spaces through gradient-based, evolutionary, and swarm intelligence approaches. Validation across contrasting sites: topographically complex Nikko, Japan, and flat Landes, France, demonstrates 15-16% improvements over original GEDI positions and 0.5-2% improvements over the state-of-the-art GeoGEDI algorithm. L-BFGS-B with Area-based metrics achieves optimal accuracy-efficiency trade-offs, while population-based algorithms (genetic algorithms, particle swarm optimization) excel in complex terrain. The platform-agnostic framework facilitates straightforward adaptation to emerging spaceborne LiDAR missions, providing a generalizable foundation for universal geolocation correction essential for reliable global forest monitoring and climate policy decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17600v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Narumasa Tsutsumida, Rei Mitsuhashi, Yoshito Sawada, Akira Kato</dc:creator>
    </item>
    <item>
      <title>Reconfigurable, large-format D-ToF/photon-counting SPAD image sensors with embedded FPGA for scene adaptability</title>
      <link>https://arxiv.org/abs/2511.17651</link>
      <description>arXiv:2511.17651v1 Announce Type: new 
Abstract: CMOS-compatible single-photon avalanche diodes (SPADs) have emerged in many systems as the solution of choice for cameras with photon-number resolution and photon counting capabilities. Being natively digital optical interfaces, SPADs are naturally drawn to in situ logic processing and event-driven computation; they are usually coupled to discrete FPGAs to enable reconfigurability. In this work, we propose to bring the FPGA on-chip, in direct contact with the SPADs at pixel or cluster level. To demonstrate the suitability of this approach, we created an architecture for processing timestamps and photon counts using programmable weighted sums based on an efficient use of look-up tables. The outputs are processed hierarchically, similarly to what is done in FPGAs, reducing power consumption and simplifying I/Os. Finally, we show how artificial neural networks can be designed and reprogrammed by using look-up tables in an efficient way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17651v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tommaso Milanese, Baris Can Efe, Claudio Bruschini, Nobukazu Teranishi, Edoardo Charbon</dc:creator>
    </item>
    <item>
      <title>Robust Detection of Retinal Neovascularization in Widefield Optical Coherence Tomography</title>
      <link>https://arxiv.org/abs/2511.17744</link>
      <description>arXiv:2511.17744v1 Announce Type: new 
Abstract: Retinal neovascularization (RNV) is a vision threatening development in diabetic retinopathy (DR). Vision loss associated with RNV is preventable with timely intervention, making RNV clinical screening and monitoring a priority. Optical coherence tomography (OCT) angiography (OCTA) provides high-resolution imaging and high-sensitivity detection of RNV lesions. With recent commercial devices introducing widefield OCTA imaging to the clinic, the technology stands to improve early detection of RNV pathology. However, to meet clinical requirements these imaging capabilities must be combined with effective RNV detection and quantification, but existing algorithms for OCTA images are optimized for conventional, i.e. narrow, fields of view. Here, we present a novel approach for RNV diagnosis and staging on widefield OCT/OCTA. Unlike conventional methods dependent on multi-layer retinal segmentation, our model reframes RNV identification as a direct binary localization task. Our fully automated approach was trained and validated on 589 widefield scans (17x17-mm to 26x21-mm) collected from multiple devices at multiple clinics. Our method achieved a device-dependent area under curve (AUC) ranging from 0.96 to 0.99 for RNV diagnosis, and mean intersection over union (IOU) ranging from 0.76 to 0.88 for segmentation. We also demonstrate our method's ability to monitor lesion growth longitudinally. Our results indicate that deep learning-based analysis for widefield OCTA images could offer a valuable means for improving RNV screening and management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17744v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyi Hao (Casey Eye Institute, Oregon Health &amp; Science University, Portland, Oregon 97239, USA), Jie Wang (Casey Eye Institute, Oregon Health &amp; Science University, Portland, Oregon 97239, USA), Kotaro Tsuboi (Department of Ophthalmology, Aichi Medical University, 1-1, Yazako Karimata, Nagakute, Aichi, 480- 1195, Japan), Liqin Gao (Casey Eye Institute, Oregon Health &amp; Science University, Portland, Oregon 97239, USA), Tristan T. Hormel (Casey Eye Institute, Oregon Health &amp; Science University, Portland, Oregon 97239, USA), Yukun Guo (Casey Eye Institute, Oregon Health &amp; Science University, Portland, Oregon 97239, USA, Department of Biomedical Engineering, Oregon Health &amp; Science University, Portland, Oregon 97239, USA), An-Lun Wu (Casey Eye Institute, Oregon Health &amp; Science University, Portland, Oregon 97239, USA, Department of Ophthalmology, Mackay Memorial Hospital, Hsinchu 300044, Taiwan), Min Gao (Casey Eye Institute, Oregon Health &amp; Science University, Portland, Oregon 97239, USA, Department of Biomedical Engineering, Oregon Health &amp; Science University, Portland, Oregon 97239, USA), Christina J. Flaxel (Casey Eye Institute, Oregon Health &amp; Science University, Portland, Oregon 97239, USA), Steven T. Bailey (Casey Eye Institute, Oregon Health &amp; Science University, Portland, Oregon 97239, USA), Thomas S. Hwang (Casey Eye Institute, Oregon Health &amp; Science University, Portland, Oregon 97239, USA), Yali Jia (Casey Eye Institute, Oregon Health &amp; Science University, Portland, Oregon 97239, USA, Department of Biomedical Engineering, Oregon Health &amp; Science University, Portland, Oregon 97239, USA)</dc:creator>
    </item>
    <item>
      <title>Generative MR Multitasking with complex-harmonic cardiac encoding: Bridging the gap between gated imaging and real-time imaging</title>
      <link>https://arxiv.org/abs/2511.17847</link>
      <description>arXiv:2511.17847v1 Announce Type: new 
Abstract: Purpose: To develop a unified image reconstruction framework that bridges real-time and gated cardiac MRI, including quantitative MRI. Methods: We introduce Generative Multitasking, which learns an implicit neural temporal basis from sequence timings and an interpretable latent space for cardiac and respiratory motion. Cardiac motion is modeled as a complex harmonic, with phase encoding timing and a latent amplitude capturing beat-to-beat functional variability, linking cardiac phase-resolved ("gated-like") and time-resolved ("real-time-like") views. We implemented the framework using a conditional variational autoencoder (CVAE) and evaluated it for free-breathing, non-ECG-gated radial GRE in three settings: steady-state cine imaging, multicontrast T2prep/IR imaging, and dual-flip-angle T1/T2 mapping, compared with conventional Multitasking. Results: Generative Multitasking provided flexible cardiac motion representation, enabling reconstruction of archetypal cardiac phase-resolved cines (like gating) as well as time-resolved series that reveal beat-to-beat variability (like real-time imaging). Conditioning on the previous k-space angle and modifying this term at inference removed eddy-current artifacts without globally smoothing high temporal frequencies. For quantitative mapping, Generative Multitasking reduced intraseptal T1 and T2 coefficients of variation compared with conventional Multitasking (T1: 0.13 vs. 0.31; T2: 0.12 vs. 0.32; p&lt;0.001), indicating higher SNR. Conclusion: Generative Multitasking uses a CVAE with complex harmonic cardiac coordinates to unify gated and real-time CMR within a single free-breathing, non-ECG-gated acquisition. It allows flexible cardiac motion representation, suppresses trajectory-dependent artifacts, and improves T1 and T2 mapping, suggesting a path toward cine, multicontrast, and quantitative imaging without separate gated and real-time scans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17847v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinguo Fang, Anthony G. Christodoulou</dc:creator>
    </item>
    <item>
      <title>A Versatile Optical Frontend for Multicolor Fluorescence Imaging with Miniaturized Lensless Sensors</title>
      <link>https://arxiv.org/abs/2511.17860</link>
      <description>arXiv:2511.17860v1 Announce Type: new 
Abstract: Lensless imaging enables exceptionally compact fluorescence sensors, advancing applications in \textit{in vivo} imaging and low-cost, point-of-care diagnostics. These sensors require a filter to block the excitation light while passing fluorescent emissions. However, conventional thin-film interference filters are sensitive to angle of incidence (AOI), complicating their use in lensless systems. Here we thoroughly analyze and optimize a technique using a fiber optic plate (FOP) to absorb off-axis light that would bleed through the interference filter while improving image resolution. Through simulations, we show that the numerical aperture (NA) of the FOP drives inherent design tradeoffs: collection efficiency improves rapidly with a higher NA, but at the cost of resolution, increased device thickness, and fluorescence excitation efficiency. To illustrate this, we optimize two optical frontends with full-width at half maximums (FWHMs) of 8.3{\deg} and 45.7{\deg}. Implementing these designs, we show that angle-insensitivity requires filters on both sides of the FOP, due to scattering. In imaging experiments, the 520-$\mu$m-thick high-NA design is 59$\times$ more sensitive to fluorescence while only degrading resolution by 3.2$\times$. Alternatively, the low-NA design is capable of three-color fluorescence imaging with 110-$\mu$m resolution at a 1-mm working distance. Overall, we demonstrate a versatile optical frontend that is adaptable to a range of applications using different fluorophores, illumination configurations, and lensless imaging techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17860v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Harris, Micah Roschelle, Jack Bartley, Mekhail Anwar</dc:creator>
    </item>
    <item>
      <title>INT-DTT+: Low-Complexity Data-Dependent Transforms for Video Coding</title>
      <link>https://arxiv.org/abs/2511.17867</link>
      <description>arXiv:2511.17867v1 Announce Type: new 
Abstract: Discrete trigonometric transforms (DTTs), such as the DCT-2 and the DST-7, are widely used in video codecs for their balance between coding performance and computational efficiency. In contrast, data-dependent transforms, such as the Karhunen-Lo\`eve transform (KLT) and graph-based separable transforms (GBSTs), offer better energy compaction but lack symmetries that can be exploited to reduce computational complexity. This paper bridges this gap by introducing a general framework to design low-complexity data-dependent transforms. Our approach builds on DTT+, a family of GBSTs derived from rank-one updates of the DTT graphs, which can adapt to signal statistics while retaining a structure amenable to fast computation. We first propose a graph learning algorithm for DTT+ that estimates the rank-one updates for rows and column graphs jointly, capturing the statistical properties of the overall block. Then, we exploit the progressive structure of DTT+ to decompose the kernel into a base DTT and a structured Cauchy matrix. By leveraging low-complexity integer DTTs and sparsifying the Cauchy matrix, we construct an integer approximation to DTT+, termed INT-DTT+. This approximation significantly reduces both computational and memory complexities with respect to the separable KLT with minimal performance loss. We validate our approach in the context of mode-dependent transforms for the VVC standard, following a rate-distortion optimized transform (RDOT) design approach. Integrated into the explicit multiple transform selection (MTS) framework of VVC in a rate-distortion optimization setup, INT-DTT+ achieves more than 3% BD-rate savings over the VVC MTS baseline, with complexity comparable to the integer DCT-2 once the base DTT coefficients are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17867v1</guid>
      <category>eess.IV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samuel Fern\'andez-Mendui\~na, Eduardo Pavez, Antonio Ortega, Tsung-Wei Huang, Thuong Nguyen Canh, Guan-Ming Su, Peng Yin</dc:creator>
    </item>
    <item>
      <title>TransLK-Net: Entangling Transformer and Large Kernel for Progressive and Collaborative Feature Encoding and Decoding in Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2511.17873</link>
      <description>arXiv:2511.17873v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) and vision transformers (ViTs) are widely employed for medical image segmentation, but they are still challenged by their intrinsic characteristics. CNNs are limited from capturing varying-scaled features and global contextual information due to the employment of fixed-sized kernels. In contrast, ViTs employ self-attention and MLP for global information modeling, but they lack mechanisms to learn spatial-wise local information. Additionally, self-attention leads the network to show high computational complexity. To tackle these limitations, we propose Progressively Entangled Transformer Large Kernel (PTLK) and Collaboratively Entangled Transformer Large Kernel (CTLK) modules to leverage the benefits of self-attention and large kernel convolutions and overcome shortcomings. Specifically, PTLK and CTLK modules employ the Multi-head Large Kernel to capture multi-scale local features and the Efficient Decomposed Self-attention to model global information efficiently. Subsequently, they employ the Attention Entanglement mechanism to enable local and global features to enhance and calibrate each other progressively and collaboratively. Additionally, an Attention-gated Channel MLP (AG-MLP) module is proposed to equip the standard MLP module with the capabilities of modeling spatial information. PTLK and CTLK modules are further incorporated as a Cross Entanglement Decoding (CED) block for efficient feature fusion and decoding. Finally, we propose a novel network for volumetric medical image segmentation that employs an encoder-decoder architecture, termed TransLK-Net. The encoder employs a hierarchical ViT architecture whose block is built by incorporating PTLK and CTLK with AG-MLP into a ViT block, and the decoder employs the CED block.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17873v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jin Yang, Daniel S. Marcus, Aristeidis Sotiras</dc:creator>
    </item>
    <item>
      <title>Spectral Super-Resolution Neural Operator with Atmospheric Radiative Transfer Prior</title>
      <link>https://arxiv.org/abs/2511.17895</link>
      <description>arXiv:2511.17895v1 Announce Type: new 
Abstract: Spectral super-resolution (SSR) aims to reconstruct hyperspectral images (HSIs) from multispectral observations, with broad applications in remote sensing. Data-driven methods are widely used, but they often overlook physical principles, leading to unrealistic spectra, particularly in atmosphere-affected bands. To address this challenge, we propose the Spectral Super-Resolution Neural Operator (SSRNO), which incorporates atmospheric radiative transfer (ART) prior into the data-driven procedure, yielding more physically consistent predictions. The proposed SSRNO framework consists of three stages: upsampling, reconstruction, and refinement. In the upsampling stage, we leverage prior information to expand the input multispectral image, producing a physically plausible hyperspectral estimate. Subsequently, we utilize a neural operator in the reconstruction stage to learn a continuous mapping across the spectral domain. Finally, the refinement stage imposes a hard constraint on the output HSI to eliminate color distortion. The upsampling and refinement stages are implemented via the proposed guidance matrix projection (GMP) method, and the reconstruction neural operator adopts U-shaped spectral-aware convolution (SAC) layers to capture multi-scale features. Moreover, we theoretically demonstrate the optimality of the GMP method. With the neural operator and ART priors, SSRNO also achieves continuous spectral reconstruction and zero-shot extrapolation. Various experiments validate the effectiveness and generalization ability of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17895v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziye Zhang, Bin Pan, Zhenwei Shi</dc:creator>
    </item>
    <item>
      <title>Diverse Instance Generation via Diffusion Models for Enhanced Few-Shot Object Detection in Remote Sensing Images</title>
      <link>https://arxiv.org/abs/2511.18031</link>
      <description>arXiv:2511.18031v1 Announce Type: new 
Abstract: Few-shot object detection (FSOD) aims to detect novel instances with only a limited number of labeled training samples, presenting a challenge that is particularly prominent in numerous remote sensing applications such as endangered species monitoring and disaster assessment. Existing FSOD methods for remote sensing images (RSIs) have achieved promising progress but remain constrained by the limited diversity of instances. To address this issue, we propose a novel framework that can leverage a diffusion model pretrained on large-scale natural images to synthesize diverse remote sensing instances, thereby improving the performance of few-shot object detectors. Instead of directly synthesizing complete remote sensing images, we first generate instance-level slices via a specialized slice-to-slice module, and then embed these slices into full-scale imagery for enhanced data augmentation. To further adapt diffusion models for remote sensing scenarios, we develop a class-agnostic image inversion module that can invert remote sensing instance slices into semantic space. Additionally, we introduce contrastive loss to semantically align the synthesized images with their corresponding classes. Experimental results show that our method hasachieved an average performance improvement of 4.4% across multiple datasets and various approaches. Ablation experiments indicate that the elaborately designed inversion module can effectively enhance the performance of FSOD methods, and the semantic contrastive loss can further boost the performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18031v1</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/LGRS.2025.3624148</arxiv:DOI>
      <arxiv:journal_reference>IEEE Geoscience and Remote Sensing Letters, vol. 22, 2025, pp. 1-5, Art no. 6015405</arxiv:journal_reference>
      <dc:creator>Yanxing Liu, Jiancheng Pan, Jianwei Yang, Tiancheng Chen, Peiling Zhou, Bingchen Zhang</dc:creator>
    </item>
    <item>
      <title>Linear Algebraic Approaches to Neuroimaging Data Compression: A Comparative Analysis of Matrix and Tensor Decomposition Methods for High-Dimensional Medical Images</title>
      <link>https://arxiv.org/abs/2511.18197</link>
      <description>arXiv:2511.18197v1 Announce Type: new 
Abstract: This paper evaluates Tucker decomposition and Singular Value Decomposition (SVD) for compressing neuroimaging data. Tucker decomposition preserves multi-dimensional relationships, achieving superior reconstruction fidelity and perceptual similarity. SVD excels in extreme compression but sacrifices fidelity. The results highlight Tucker decomposition's suitability for applications requiring the preservation of structural and temporal relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18197v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaeho Kim, Daniel David, Ana Vizitiv</dc:creator>
    </item>
    <item>
      <title>Shape-Adapting Gated Experts: Dynamic Expert Routing for Colonoscopic Lesion Segmentation</title>
      <link>https://arxiv.org/abs/2511.18493</link>
      <description>arXiv:2511.18493v2 Announce Type: new 
Abstract: The substantial diversity in cell scale and form remains a primary challenge in computer-aided cancer detection on gigapixel Whole Slide Images (WSIs), attributable to cellular heterogeneity. Existing CNN-Transformer hybrids rely on static computation graphs with fixed routing, which consequently causes redundant computation and limits their adaptability to input variability. We propose Shape-Adapting Gated Experts (SAGE), an input-adaptive framework that enables dynamic expert routing in heterogeneous visual networks. SAGE reconfigures static backbones into dynamically routed expert architectures. SAGE's dual-path design features a backbone stream that preserves representation and selectively activates an expert path through hierarchical gating. This gating mechanism operates at multiple hierarchical levels, performing a two-level, hierarchical selection between shared and specialized experts to modulate model logits for Top-K activation. Our Shape-Adapting Hub (SA-Hub) harmonizes structural and semantic representations across the CNN and the Transformer module, effectively bridging diverse modules. Embodied as SAGE-UNet, our model achieves superior segmentation on three medical benchmarks: EBHI, DigestPath, and GlaS, yielding state-of-the-art Dice Scores of 95.57%, 95.16%, and 94.17%, respectively, and robustly generalizes across domains by adaptively balancing local refinement and global context. SAGE provides a scalable foundation for dynamic expert routing, enabling flexible visual reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18493v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gia Huy Thai, Hoang-Nguyen Vu, Anh-Minh Phan, Quang-Thinh Ly, Tram Dinh, Thi-Ngoc-Truc Nguyen, Nhat Ho</dc:creator>
    </item>
    <item>
      <title>Equivariant Deep Equilibrium Models for Imaging Inverse Problems</title>
      <link>https://arxiv.org/abs/2511.18667</link>
      <description>arXiv:2511.18667v1 Announce Type: new 
Abstract: Equivariant imaging (EI) enables training signal reconstruction models without requiring ground truth data by leveraging signal symmetries. Deep equilibrium models (DEQs) are a powerful class of neural networks where the output is a fixed point of a learned operator. However, training DEQs with complex EI losses requires implicit differentiation through fixed-point computations, whose implementation can be challenging. We show that backpropagation can be implemented modularly, simplifying training. Experiments demonstrate that DEQs trained with implicit differentiation outperform those trained with Jacobian-free backpropagation and other baseline methods. Additionally, we find evidence that EI-trained DEQs approximate the proximal map of an invariant prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18667v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Mehta, Ruangrawee Kitichotkul, Vivek K Goyal, Juli\'an Tachella</dc:creator>
    </item>
    <item>
      <title>Evaluation of Hardware-based Video Encoders on Modern GPUs for UHD Live-Streaming</title>
      <link>https://arxiv.org/abs/2511.18686</link>
      <description>arXiv:2511.18686v1 Announce Type: new 
Abstract: Many GPUs have incorporated hardware-accelerated video encoders, which allow video encoding tasks to be offloaded from the main CPU and provide higher power efficiency. Over the years, many new video codecs such as H.265/HEVC, VP9, and AV1 were added to the latest GPU boards. Recently, the rise of live video content such as VTuber, game live-streaming, and live event broadcasts, drives the demand for high-efficiency hardware encoders in the GPUs to tackle these real-time video encoding tasks, especially at higher resolutions such as 4K/8K UHD. In this paper, RD performance, encoding speed, as well as power consumption of hardware encoders in several generations of NVIDIA, Intel GPUs as well as Qualcomm Snapdragon Mobile SoCs were evaluated and compared to the software counterparts, including the latest H.266/VVC codec, using several metrics including PSNR, SSIM, and machine-learning based VMAF. The results show that modern GPU hardware encoders can match the RD performance of software encoders in real-time encoding scenarios, and while encoding speed increased in newer hardware, there is mostly negligible RD performance improvement between hardware generations. Finally, the bitrate required for each hardware encoder to match YouTube transcoding quality was also calculated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18686v1</guid>
      <category>eess.IV</category>
      <category>cs.AR</category>
      <category>cs.MM</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICCCN61486.2024.10637525</arxiv:DOI>
      <dc:creator>Kasidis Arunruangsirilert, Jiro Katto</dc:creator>
    </item>
    <item>
      <title>Neural B-Frame Coding: Tackling Domain Shift Issues with Lightweight Online Motion Resolution Adaptation</title>
      <link>https://arxiv.org/abs/2511.18724</link>
      <description>arXiv:2511.18724v1 Announce Type: new 
Abstract: Learned B-frame codecs with hierarchical temporal prediction often encounter the domain-shift issue due to mismatches between the Group-of-Pictures (GOP) sizes for training and testing, leading to inaccurate motion estimates, particularly for large motion. A common solution is to turn large motion into small motion by downsampling video frames during motion estimation. However, determining the optimal downsampling factor typically requires costly rate-distortion optimization. This work introduces lightweight classifiers to predict downsampling factors. These classifiers leverage simple state signals from current and reference frames to balance rate-distortion performance with computational cost. Three variants are proposed: (1) a binary classifier (Bi-Class) trained with Focal Loss to choose between high and low resolutions, (2) a multi-class classifier (Mu-Class) trained with novel soft labels based on rate-distortion costs, and (3) a co-class approach (Co-Class) that combines the predictive capability of the multi-class classifier with the selective search of the binary classifier. All classifier methods can work seamlessly with existing B-frame codecs without requiring codec retraining. Experimental results show that they achieve coding performance comparable to exhaustive search methods while significantly reducing computational complexity. The code is available at: https://github.com/NYCU-MAPL/Fast-OMRA.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18724v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sang NguyenQuang, Xiem HoangVan, Wen-Hsiao Peng</dc:creator>
    </item>
    <item>
      <title>Semantic-driven Wireless Environment Knowledge Representation for Efficiency-Accuracy Balanced Beam Prediction in Vehicular Networks</title>
      <link>https://arxiv.org/abs/2511.17552</link>
      <description>arXiv:2511.17552v1 Announce Type: cross 
Abstract: The rapid evolution of the internet of vehicles demands ultra-reliable low-latency communication in high-mobility environments, where conventional beam prediction methods suffer from high-dimensional inputs, prolonged training times, and limited interpretability. To address these challenges, the propagation environment semantics-aware wireless environment knowledge beam prediction (PES-WEKBP) framework is proposed. PES-WEKBP pioneers a novel electromagnetic (EM)-grounded knowledge distillation method, transforming raw visual data into an ultra-lean, interpretable material and location-related wireless environment knowledge matrix. This matrix explicitly encodes critical propagation environment semantics, which is material EM properties and spatial relationships through a physics-informed parameterization process, distilling the environment and channel interplay into a minimal yet information-dense representation. A lightweight decision network then leverages this highly compressed knowledge for low-complexity beam prediction. To holistically evaluate the performance of PES-WEKBP, we first design the prediction consistency-efficiency index (PCEI), which combines prediction accuracy with a stability-penalized logarithmic training time to ensure a balanced optimization of reliability and computational efficiency. Experiments validate that PES-WEKBP achieves a 99.75% to 99.96% dimension reduction and improves accuracy by 5.52% to 8.19%, which outperforms state-of-the-art methods in PCEI scores across diverse vehicular scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17552v1</guid>
      <category>eess.SP</category>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialin Wang, Jianhua Zhang, Yu Li, Yutong Sun, Yuxiang Zhang</dc:creator>
    </item>
    <item>
      <title>Speed Control Security System For safety of Driver and Surroundings</title>
      <link>https://arxiv.org/abs/2511.18445</link>
      <description>arXiv:2511.18445v1 Announce Type: cross 
Abstract: The speed control security system is best suited for the task of slowing the speed of a vehicle during rash driving as the Driver is over speeding the circuit captures the images of the lanes witch decides the speed of the road the car is currently on this input is further provided to the ESP-32 micro Prosser module in the car switch compiles this data with the data received for the RPM sensor of the car and decides whether the car is over speeding or not in case of over speeding a signal is send by the ESP to the Arduino witch actuates the dc motor used in the car to reduce the speed of the car by the use of a hydraulic brake system actuated by a DC motor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18445v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Vishesh Vishal Ahire, Yash Badrinarayan Amle, Akshada Nanasaheb Waditke, Ojas Nitin Ahire, Amey Mahesh Warnekar, Ayush Ganesh Ahire, Prashant Anerao</dc:creator>
    </item>
    <item>
      <title>Data Augmentation Strategies for Robust Lane Marking Detection</title>
      <link>https://arxiv.org/abs/2511.18668</link>
      <description>arXiv:2511.18668v1 Announce Type: cross 
Abstract: Robust lane detection is essential for advanced driver assistance and autonomous driving, yet models trained on public datasets such as CULane often fail to generalise across different camera viewpoints. This paper addresses the challenge of domain shift for side-mounted cameras used in lane-wheel monitoring by introducing a generative AI-based data enhancement pipeline. The approach combines geometric perspective transformation, AI-driven inpainting, and vehicle body overlays to simulate deployment-specific viewpoints while preserving lane continuity. We evaluated the effectiveness of the proposed augmentation in two state-of-the-art models, SCNN and UFLDv2. With the augmented data trained, both models show improved robustness to different conditions, including shadows. The experimental results demonstrate gains in precision, recall, and F1 score compared to the pre-trained model.
  By bridging the gap between widely available datasets and deployment-specific scenarios, our method provides a scalable and practical framework to improve the reliability of lane detection in a pilot deployment scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18668v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Flora Lian, Dinh Quang Huynh, Hector Penades, J. Stephany Berrio Perez, Mao Shan, Stewart Worrall</dc:creator>
    </item>
    <item>
      <title>PrismAudio: Decomposed Chain-of-Thoughts and Multi-dimensional Rewards for Video-to-Audio Generation</title>
      <link>https://arxiv.org/abs/2511.18833</link>
      <description>arXiv:2511.18833v2 Announce Type: cross 
Abstract: Video-to-Audio (V2A) generation requires balancing four critical perceptual dimensions: semantic consistency, audio-visual temporal synchrony, aesthetic quality, and spatial accuracy; yet existing methods suffer from objective entanglement that conflates competing goals in single loss functions and lack human preference alignment. We introduce PrismAudio, the first framework to integrate Reinforcement Learning into V2A generation with specialized Chain-of-Thought (CoT) planning. Our approach decomposes monolithic reasoning into four specialized CoT modules (Semantic, Temporal, Aesthetic, and Spatial CoT), each paired with targeted reward functions. This CoT-reward correspondence enables multidimensional RL optimization that guides the model to jointly generate better reasoning across all perspectives, solving the objective entanglement problem while preserving interpretability. To make this optimization computationally practical, we propose Fast-GRPO, which employs hybrid ODE-SDE sampling that dramatically reduces the training overhead compared to existing GRPO implementations. We also introduce AudioCanvas, a rigorous benchmark that is more distributionally balanced and covers more realistically diverse and challenging scenarios than existing datasets, with 300 single-event classes and 501 multi-event samples. Experimental results demonstrate that PrismAudio achieves state-of-the-art performance across all four perceptual dimensions on both the in-domain VGGSound test set and out-of-domain AudioCanvas benchmark. The project page is available at https://PrismAudio-Project.github.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.18833v2</guid>
      <category>cs.SD</category>
      <category>cs.CV</category>
      <category>eess.AS</category>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Huadai Liu, Kaicheng Luo, Wen Wang, Qian Chen, Peiwen Sun, Rongjie Huang, Xiangang Li, Jieping Ye, Wei Xue</dc:creator>
    </item>
    <item>
      <title>Automatic nodule identification and differentiation in ultrasound videos to facilitate per-nodule examination</title>
      <link>https://arxiv.org/abs/2310.06339</link>
      <description>arXiv:2310.06339v2 Announce Type: replace 
Abstract: Ultrasound is a vital diagnostic technique in health screening, with the advantages of non-invasive, cost-effective, and radiation free, and therefore is widely applied in the diagnosis of nodules. However, it relies heavily on the expertise and clinical experience of the sonographer. In ultrasound images, a single nodule might present heterogeneous appearances in different cross-sectional views which makes it hard to perform per-nodule examination. Sonographers usually discriminate different nodules by examining the nodule features and the surrounding structures like gland and duct, which is cumbersome and time-consuming. To address this problem, we collected hundreds of breast ultrasound videos and built a nodule reidentification system that consists of two parts: an extractor based on the deep learning model that can extract feature vectors from the input video clips and a real-time clustering algorithm that automatically groups feature vectors by nodules. The system obtains satisfactory results and exhibits the capability to differentiate ultrasound videos. As far as we know, it's the first attempt to apply re-identification technique in the ultrasonic field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06339v2</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyuan Jiang, Yan Ding, Yuling Wang, Lei Xu, Wenli Dai, Wanru Chang, Jianfeng Zhang, Jie Yu, Jianqiao Zhou, Chunquan Zhang, Ping Liang, Dexing Kong</dc:creator>
    </item>
    <item>
      <title>FCDM: A Physics-Guided Bidirectional Frequency Aware Convolution and Diffusion-Based Model for Sinogram Inpainting</title>
      <link>https://arxiv.org/abs/2409.06714</link>
      <description>arXiv:2409.06714v5 Announce Type: replace 
Abstract: Computed tomography (CT) is widely used in scientific imaging systems such as synchrotron and laboratory-based nano-CT, but acquiring full-view sinograms requires high radiation dose and long scan times. Sparse-view CT alleviates this burden but yields incomplete sinograms with structured signal loss, hampering accurate reconstruction. Unlike RGB images, sinograms encode overlapping features along projection paths and exhibit distinct directional spectral patterns, which make conventional RGB-oriented inpainting approaches--including diffusion models--ineffective for sinogram restoration, as they disregard the angular dependencies and physical constraints inherent to tomographic data. To overcome these limitations, we propose FCDM, a diffusion-based framework tailored for sinograms, which restores global structure through bidirectional frequency reasoning and angular-aware masking, while enforcing physical plausibility via physics-guided constraints and frequency-adaptive noise control. Experiments on real-world datasets show that FCDM consistently outperforms baselines, achieving SSIM over 0.93 and PSNR above 31 dB across diverse sparse-view scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06714v5</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaze E, Srutarshi Banerjee, Tekin Bicer, Guannan Wang, Yanfu Zhang, Bin Ren</dc:creator>
    </item>
    <item>
      <title>RN-SDEs: Limited-Angle CT Reconstruction with Residual Null-Space Diffusion Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2409.13930</link>
      <description>arXiv:2409.13930v3 Announce Type: replace 
Abstract: Computed tomography is a widely used imaging modality with applications ranging from medical imaging to material analysis. One major challenge arises from the lack of scanning information at certain angles, resulting in distortion or artifacts in the reconstructed images. This is referred to as the Limited Angle Computed Tomography (LACT) reconstruction problem. To address this problem, we propose the use of Residual Null-Space Diffusion Stochastic Differential Equations (RN-SDEs), which are a variant of diffusion models that characterize the diffusion process with mean-reverting (MR) stochastic differential equations. To demonstrate the generalizability of RN-SDEs, we conducted experiments with two different LACT datasets, ChromSTEM and C4KC-KiTS. Through extensive experiments, we demonstrate that by leveraging learned MR-SDEs as a prior and emphasizing data consistency using Range-Null Space Decomposition (RNSD) based rectification, we can recover high-quality images from severely degraded ones and achieve state-of-the-art performance in most LACT tasks. Additionally, we present a quantitative comparison of RN-SDE with other networks, in terms of computational complexity and runtime efficiency, highlighting the superior effectiveness of our proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13930v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaqi Guo, Santiago Lopez-Tapia, Wing Shun Li, Yunan Wu, Marcelo Carignano, Martin Kr\"oger, Vinayak P. Dravid, Igal Szleifer, Vadim Backman, Aggelos K. Katsaggelos</dc:creator>
    </item>
    <item>
      <title>Full-scale Representation Guided Network for Retinal Vessel Segmentation</title>
      <link>https://arxiv.org/abs/2501.18921</link>
      <description>arXiv:2501.18921v2 Announce Type: replace 
Abstract: The U-Net architecture and its variants have remained state-of-the-art (SOTA) for retinal vessel segmentation over the past decade. In this study, we introduce a Full-Scale Guided Network (FSG-Net), where a novel feature representation module using modernized convolution blocks effectively captures full-scale structural information, while a guided convolution block subsequently refines this information. Specifically, we introduce an attention-guided filter within the guided convolution block, leveraging its similarity to unsharp masking to enhance fine vascular structures. Passing full-scale information to the attention block facilitates the generation of more contextually relevant attention maps, which are then passed to the attention-guided filter, providing further refinement to the segmentation performance. The structure preceding the guided convolution block can be replaced by any U-Net variant, ensuring flexibility and scalability across various segmentation tasks. For a fair comparison, we re-implemented recent studies available in public repositories to evaluate their scalability and reproducibility. Our experiments demonstrate that, despite its compact architecture, FSG-Net delivers performance competitive with SOTA methods across multiple public datasets. Ablation studies further demonstrate that each proposed component meaningfully contributes to this competitive performance. Our code is available on https://github.com/ZombaSY/FSG-Net-pytorch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18921v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1186/s12880-025-02021-4</arxiv:DOI>
      <dc:creator>Sunyong Seo, Sangwook Yoo, Huisu Yoon</dc:creator>
    </item>
    <item>
      <title>SPASHT: An image-enhancement method for sparse-view MPI SPECT</title>
      <link>https://arxiv.org/abs/2511.06203</link>
      <description>arXiv:2511.06203v2 Announce Type: replace 
Abstract: Single-photon emission computed tomography for myocardial perfusion imaging (MPI SPECT) is a widely used diagnostic tool for coronary artery disease. However, the procedure requires considerable scanning time, leading to patient discomfort and the potential for motion-induced artifacts. Reducing the number of projection views while keeping the time per view unchanged provides a mechanism to shorten the scanning time. However, this approach leads to increased sampling artifacts, higher noise, and hence limited image quality. To address these issues, we propose sparseview SPECT image enhancement (SPASHT), inherently training the algorithm to improve performance on defect-detection tasks. We objectively evaluated SPASHT on the clinical task of detecting perfusion defects in a retrospective clinical study using data from patients who underwent MPI SPECT, where the defects were clinically realistic and synthetically inserted. The study was conducted for different numbers of fewer projection views, including 1/6, 1/3, and 1/2 of the typical projection views for MPI SPECT. Performance on the detection task was quantified using area under the receiver operating characteristic curve (AUC). Images obtained with SPASHT yielded significantly improved AUC compared to those obtained with the sparse-view protocol for all the considered numbers of fewer projection views. To further assess performance, a human observer study on the task of detecting perfusion defects was conducted. Results from the human observer study showed improved detection performance with images reconstructed using SPASHT compared to those from the sparse-view protocol. The results provide evidence of the efficacy of SPASHT in improving the quality of sparse-view MPI SPECT images and motivate further clinical validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06203v2</guid>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Zezhang Yang, Zitong Yu, Nuri Choi, Janice Tania, Wenxuan Xue, Barry A. Siegel, Abhinav K. Jha</dc:creator>
    </item>
    <item>
      <title>Smooth Total variation Regularization for Interference Detection and Elimination (STRIDE) for MRI</title>
      <link>https://arxiv.org/abs/2511.13628</link>
      <description>arXiv:2511.13628v2 Announce Type: replace 
Abstract: MRI is increasingly desired to function near electronic devices that emit potentially dynamic electromagnetic interference (EMI). To accommodate for this, we propose the STRIDE method, which improves on previous external-sensor-based EMI removal methods by exploiting inherent MR image smoothness in its total variation. STRIDE measures data from both EMI detectors and primary MR imaging coils, transforms this data into the image domain, and for each column of the resulting image array, combines and subtracts data from the EMI detectors in a way that optimizes for total-variation smoothness. Performance was tested on phantom and in-vivo datasets with a 0.5T scanner. STRIDE resulted in visually better EMI removal, higher temporal SNR, larger EMI removal percentage, and lower RMSE than standard implementations. STRIDE is a robust technique that leverages inherent MR image properties to provide improved EMI removal performance over standard algorithms, particularly for time-varying noise sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13628v2</guid>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Mertens, Diego Martinez, Amgad Louka, Ying Yang, Chad Harris, Ian Connell</dc:creator>
    </item>
    <item>
      <title>OmniLens++: Blind Lens Aberration Correction via Large LensLib Pre-Training and Latent PSF Representation</title>
      <link>https://arxiv.org/abs/2511.17126</link>
      <description>arXiv:2511.17126v3 Announce Type: replace 
Abstract: Emerging deep-learning-based lens library pre-training (LensLib-PT) pipeline offers a new avenue for blind lens aberration correction by training a universal neural network, demonstrating strong capability in handling diverse unknown optical degradations. This work proposes the OmniLens++ framework, which resolves two challenges that hinder the generalization ability of existing pipelines: the difficulty of scaling data and the absence of prior guidance characterizing optical degradation. To improve data scalability, we expand the design specifications to increase the degradation diversity of the lens source, and we sample a more uniform distribution by quantifying the spatial-variation patterns and severity of optical degradation. In terms of model design, to leverage the Point Spread Functions (PSFs), which intuitively describe optical degradation, as guidance in a blind paradigm, we propose the Latent PSF Representation (LPR). The VQVAE framework is introduced to learn latent features of LensLib's PSFs, which is assisted by modeling the optical degradation process to constrain the learning of degradation priors. Experiments on diverse aberrations of real-world lenses and synthetic LensLib show that OmniLens++ exhibits state-of-the-art generalization capacity in blind aberration correction. Beyond performance, the AODLibpro is verified as a scalable foundation for more effective training across diverse aberrations, and LPR can further tap the potential of large-scale LensLib. The source code and datasets will be made publicly available at https://github.com/zju-jiangqi/OmniLens2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17126v3</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>physics.optics</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Jiang, Xiaolong Qian, Yao Gao, Lei Sun, Kailun Yang, Zhonghua Yi, Wenyong Li, Ming-Hsuan Yang, Luc Van Gool, Kaiwei Wang</dc:creator>
    </item>
    <item>
      <title>Improvement of Spiking Neural Network with Bit Planes and Color Models</title>
      <link>https://arxiv.org/abs/2410.08229</link>
      <description>arXiv:2410.08229v5 Announce Type: replace-cross 
Abstract: Spiking neural network (SNN) has emerged as a promising paradigm in computational neuroscience and artificial intelligence, offering advantages such as low energy consumption and small memory footprint. However, their practical adoption is constrained by several challenges, prominently among them being performance optimization. In this study, we present a novel approach to enhance the performance of SNN for images through a new coding method that exploits bit plane representation. Our proposed technique is designed to improve the accuracy of SNN without increasing model size. Also, we investigate the impacts of color models of the proposed coding process. Through extensive experimental validation, we demonstrate the effectiveness of our coding strategy in achieving performance gain across multiple datasets. To the best of our knowledge, this is the first research that considers bit planes and color models in the context of SNN. By leveraging the unique characteristics of bit planes, we hope to unlock new potentials in SNNs performance, potentially paving the way for more efficient and effective SNNs models in future researches and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08229v5</guid>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2025.3635297</arxiv:DOI>
      <dc:creator>Nhan T. Luu, Duong T. Luu, Nam N. Pham, Thang C. Truong</dc:creator>
    </item>
    <item>
      <title>Splats in Splats: Robust and Effective 3D Steganography towards Gaussian Splatting</title>
      <link>https://arxiv.org/abs/2412.03121</link>
      <description>arXiv:2412.03121v2 Announce Type: replace-cross 
Abstract: 3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction performance with explicit scene representations. Given the widespread application of 3DGS in 3D reconstruction and generation tasks, there is an urgent need to protect the copyright of 3DGS assets. However, existing copyright protection techniques for 3DGS overlook the usability of 3D assets, posing challenges for practical deployment. Here we describe splats in splats, the first 3DGS steganography framework that embeds 3D content in 3DGS itself without modifying any attributes. To achieve this, we take a deep insight into spherical harmonics (SH) and devise an importance-graded SH coefficient encryption strategy to embed the hidden SH coefficients. Furthermore, we employ a convolutional autoencoder to establish a mapping between the original Gaussian primitives' opacity and the hidden Gaussian primitives' opacity. Extensive experiments indicate that our method significantly outperforms existing 3D steganography techniques, with 5.31% higher scene fidelity and 3x faster rendering speed, while ensuring security, robustness, and user experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03121v2</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijia Guo, Wenkai Huang, Yang Li, Gaolei Li, Hang Zhang, Liwen Hu, Jianhua Li, Tiejun Huang, Lei Ma</dc:creator>
    </item>
    <item>
      <title>Can Knowledge Improve Security? A Coding-Enhanced Jamming Approach for Semantic Communication</title>
      <link>https://arxiv.org/abs/2504.16960</link>
      <description>arXiv:2504.16960v5 Announce Type: replace-cross 
Abstract: As semantic communication (SemCom) attracts growing attention as a novel communication paradigm, ensuring the security of transmitted semantic information over open wireless channels has become a critical issue. However, traditional encryption methods often introduce significant additional communication overhead to maintain reliability, and conventional learning-based secure SemCom methods typically rely on a channel capacity advantage for the legitimate receiver, which is challenging to guarantee in real-world scenarios. In this paper, we propose a coding-enhanced jamming method that eliminates the need to transmit a secret key by utilizing shared knowledge, which may be part of the training set of the SemCom system, between the legitimate receiver and the transmitter. Specifically, we leverage the shared private knowledge base to generate a set of private digital codebooks in advance using neural network (NN)-based encoders. For each transmission, we encode the transmitted data into a digital sequence Y1 and associate Y1 with a sequence randomly picked from the private codebook, denoted as Y2, through superposition coding. Here, Y1 serves as the outer code and Y2 as the inner code. By optimizing the power allocation between the inner and outer codes, the legitimate receiver can reconstruct the transmitted data using successive decoding based on the shared index of Y2, while the eavesdropper's decoding performance is severely degraded, potentially to the point of random guessing. Experimental results demonstrate that our method achieves security comparable to state-of-the-art approaches while significantly improving the reconstruction performance of the legitimate receiver by more than 1 dB across varying channel signal-to-noise ratios (SNRs) and compression ratios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16960v5</guid>
      <category>cs.IT</category>
      <category>eess.IV</category>
      <category>math.IT</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weixuan Chen (Sherman), Qianqian Yang (Sherman), Shuo Shao (Sherman), Zhiguo Shi (Sherman), Jiming Chen (Sherman),  Xuemin (Sherman),  Shen</dc:creator>
    </item>
    <item>
      <title>Smooth optimization using global and local low-rank regularizers</title>
      <link>https://arxiv.org/abs/2505.06073</link>
      <description>arXiv:2505.06073v2 Announce Type: replace-cross 
Abstract: Many inverse problems and signal processing problems involve low-rank regularizers based on the nuclear norm. Commonly, proximal gradient methods (PGM) are adopted to solve this type of non-smooth problems as they can offer fast and guaranteed convergence. However, PGM methods cannot be simply applied in settings where low-rank models are imposed locally on overlapping patches; therefore, heuristic approaches have been proposed that lack convergence guarantees. In this work we propose to replace the nuclear norm with a smooth approximation in which a Huber-type function is applied to each singular value. By providing a theoretical framework based on singular value function theory, we show that important properties can be established for the proposed regularizer, such as: convexity, differentiability, and Lipschitz continuity of the gradient. Moreover, we provide a closed-form expression for the regularizer gradient, enabling the use of standard iterative gradient-based optimization algorithms (e.g., nonlinear conjugate gradient) that can easily address the case of overlapping patches and have well-known convergence guarantees. In addition, we provide a novel step-size selection strategy based on a quadratic majorizer of the line-search function that leverages the Huber characteristics of the proposed regularizer. Finally, we assess the proposed optimization framework by providing empirical results in dynamic magnetic resonance imaging (MRI) reconstruction in the context of local low-rank models with overlapping patches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06073v2</guid>
      <category>eess.SP</category>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rodrigo A. Lobos, Javier Salazar Cavazos, Raj Rao Nadakuditi, Jeffrey A. Fessler</dc:creator>
    </item>
    <item>
      <title>Intraoperative 2D/3D Registration via Spherical Similarity Learning and Differentiable Levenberg-Marquardt Optimization</title>
      <link>https://arxiv.org/abs/2509.06890</link>
      <description>arXiv:2509.06890v3 Announce Type: replace-cross 
Abstract: Intraoperative 2D/3D registration aligns preoperative 3D volumes with real-time 2D radiographs, enabling accurate localization of instruments and implants. A recent fully differentiable similarity learning framework approximates geodesic distances on SE(3), expanding the capture range of registration and mitigating the effects of substantial disturbances, but existing Euclidean approximations distort manifold structure and slow convergence. To address these limitations, we explore similarity learning in non-Euclidean spherical feature spaces to better capture and fit complex manifold structure. We extract feature embeddings using a CNN-Transformer encoder, project them into spherical space, and approximate their geodesic distances with Riemannian distances in the bi-invariant SO(4) space. This enables a more expressive and geometrically consistent deep similarity metric, enhancing the ability to distinguish subtle pose differences. During inference, we replace gradient descent with fully differentiable Levenberg-Marquardt optimization to accelerate convergence. Experiments on real and synthetic datasets show superior accuracy in both patient-specific and patient-agnostic scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06890v3</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minheng Chen, Youyong Kong</dc:creator>
    </item>
  </channel>
</rss>
