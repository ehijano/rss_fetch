<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Robustly encoding certainty in a metastable neural circuit model</title>
      <link>https://arxiv.org/abs/2405.13182</link>
      <description>arXiv:2405.13182v1 Announce Type: new 
Abstract: Localized persistent neural activity has been shown to serve delayed estimation of continuous variables. Common experiments require that subjects store and report the feature value (e.g., orientation) of a particular cue (e.g., oriented bar on a screen) after a delay. Visualizing recorded activity of neurons according to their feature tuning reveals activity bumps whose centers wander stochastically, degrading the estimate over time. Bump position therefore represents the remembered estimate. Recent work suggests that bump amplitude may represent estimate certainty reflecting a probabilistic population code for a Bayesian posterior. Idealized models of this type are fragile due to the fine tuning common to constructed continuum attractors in dynamical systems. Here we propose an alternative metastable model for robustly supporting multiple bump amplitudes by extending neural circuit models to include quantized nonlinearities. Asymptotic projections of circuit activity produce low-dimensional evolution equations for the amplitude and position of bump solutions in response to external stimuli and noise perturbations. Analysis of reduced equations accurately characterizes phase variance and the dynamics of amplitude transitions between stable discrete values. More salient cues generate bumps of higher amplitude which wander less, consistent with the experimental finding that greater certainty correlates with more accurate memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13182v1</guid>
      <category>q-bio.NC</category>
      <category>nlin.PS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heather L Cihak, Zachary P Kilpatrick</dc:creator>
    </item>
    <item>
      <title>A theory of neural emulators</title>
      <link>https://arxiv.org/abs/2405.13394</link>
      <description>arXiv:2405.13394v1 Announce Type: new 
Abstract: A central goal in neuroscience is to provide explanations for how animal nervous systems can generate actions and cognitive states such as consciousness while artificial intelligence (AI) and machine learning (ML) seek to provide models that are increasingly better at prediction. Despite many decades of research we have made limited progress on providing neuroscience explanations yet there is an increased use of AI and ML methods in neuroscience for prediction of behavior and even cognitive states. Here we propose emulator theory (ET) and neural emulators as circuit- and scale-independent predictive models of biological brain activity and emulator theory (ET) as an alternative research paradigm in neuroscience. ET proposes that predictive models trained solely on neural dynamics and behaviors can generate functionally indistinguishable systems from their sources. That is, compared to the biological organisms which they model, emulators may achieve indistinguishable behavior and cognitive states - including consciousness - without any mechanistic explanations. We posit ET via several conjectures, discuss the nature of endogenous and exogenous activation of neural circuits, and discuss neural causality of phenomenal states. ET provides the conceptual and empirical framework for prediction-based models of neural dynamics and behavior without explicit representations of idiosyncratically evolved nervous systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13394v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Catalin C. Mitelut</dc:creator>
    </item>
    <item>
      <title>Contribute to balance, wire in accordance: Emergence of backpropagation from a simple, bio-plausible neuroplasticity rule</title>
      <link>https://arxiv.org/abs/2405.14139</link>
      <description>arXiv:2405.14139v1 Announce Type: new 
Abstract: Backpropagation (BP) has been pivotal in advancing machine learning and remains essential in computational applications and comparative studies of biological and artificial neural networks. Despite its widespread use, the implementation of BP in the brain remains elusive, and its biological plausibility is often questioned due to inherent issues such as the need for symmetry of weights between forward and backward connections, and the requirement of distinct forward and backward phases of computation. Here, we introduce a novel neuroplasticity rule that offers a potential mechanism for implementing BP in the brain. Similar in general form to the classical Hebbian rule, this rule is based on the core principles of maintaining the balance of excitatory and inhibitory inputs as well as on retrograde signaling, and operates over three progressively slower timescales: neural firing, retrograde signaling, and neural plasticity. We hypothesize that each neuron possesses an internal state, termed credit, in addition to its firing rate. After achieving equilibrium in firing rates, neurons receive credits based on their contribution to the E-I balance of postsynaptic neurons through retrograde signaling. As the network's credit distribution stabilizes, connections from those presynaptic neurons are strengthened that significantly contribute to the balance of postsynaptic neurons. We demonstrate mathematically that our learning rule precisely replicates BP in layered neural networks without any approximations. Simulations on artificial neural networks reveal that this rule induces varying community structures in networks, depending on the learning rate. This simple theoretical framework presents a biologically plausible implementation of BP, with testable assumptions and predictions that may be evaluated through biological experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14139v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinhao Fan, Shreesh P Mysore</dc:creator>
    </item>
    <item>
      <title>Prospective and retrospective coding in cortical neurons</title>
      <link>https://arxiv.org/abs/2405.14810</link>
      <description>arXiv:2405.14810v1 Announce Type: new 
Abstract: Brains can process sensory information from different modalities at astonishing speed, this is surprising as already the integration of inputs through the membrane causes a delayed response. Neuronal recordings in vitro reveal a possible explanation for the fast processing through an advancement of the output firing rates of individual neurons with respect to the input, a concept which we refer to as prospective coding. The underlying mechanisms of prospective coding, however, is not completely understood. We propose a mechanistic explanation for individual neurons advancing their output on the level of single action potentials and instantaneous firing rates. Using the Hodgkin-Huxley model, we show that the spike generation mechanism can be the source for prospective (advanced) or retrospective (delayed) responses with respect the underlying somatic voltage. A simplified Hodgkin-Huxley model identifies the sodium inactivation as a source for the prospective firing, controlling the timing of the neuron's output as a function the voltage and its derivative. We also consider a slower spike-frequency adaptation as a mechanisms that generates prospective firings to inputs that undergo slow temporal modulations. In general, we show that adaptation processes at different time scales can cause advanced neuronal responses to time varying inputs that are modulated on the corresponding time scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14810v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Simon Brandt, Mihai Alexandru Petrovici, Walter Senn, Katharina Anna Wilmes, Federico Benitez</dc:creator>
    </item>
    <item>
      <title>Launching Your VR Neuroscience Laboratory</title>
      <link>https://arxiv.org/abs/2405.13171</link>
      <description>arXiv:2405.13171v1 Announce Type: cross 
Abstract: The proliferation and refinement of affordable virtual reality (VR) technologies and wearable sensors have opened new frontiers in cognitive and behavioral neuroscience. This chapter offers a broad overview of VR for anyone interested in leveraging it as a research tool. In the first section, it examines the fundamental functionalities of VR and outlines important considerations that inform the development of immersive content that stimulates the senses. In the second section, the focus of the discussion shifts to the implementation of VR in the context of the neuroscience lab. Practical advice is offered on adapting commercial, off-theshelf devices to specific research purposes. Further, methods are explored for recording, synchronizing, and fusing heterogeneous forms of data obtained through the VR system or add-on sensors, as well as for labeling events and capturing game play.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13171v1</guid>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/7854_2023_420</arxiv:DOI>
      <arxiv:journal_reference>In Virtual Reality in Behavioral Neuroscience: New Insights and Methods (pp. 25-46). Cham: Springer International Publishing (2023)</arxiv:journal_reference>
      <dc:creator>Ying Choon Wu, Christopher Maymon, Jonathon Paden, Weichen Liu</dc:creator>
    </item>
    <item>
      <title>On the dynamics of convolutional recurrent neural networks near their critical point</title>
      <link>https://arxiv.org/abs/2405.13854</link>
      <description>arXiv:2405.13854v1 Announce Type: cross 
Abstract: We examine the dynamical properties of a single-layer convolutional recurrent network with a smooth sigmoidal activation function, for small values of the inputs and when the convolution kernel is unitary, so all eigenvalues lie exactly at the unit circle. Such networks have a variety of hallmark properties: the outputs depend on the inputs via compressive nonlinearities such as cubic roots, and both the timescales of relaxation and the length-scales of signal propagation depend sensitively on the inputs as power laws, both diverging as the input to 0. The basic dynamical mechanism is that inputs to the network generate ongoing activity, which in turn controls how additional inputs or signals propagate spatially or attenuate in time. We present analytical solutions for the steady states when the network is forced with a single oscillation and when a background value creates a steady state of ongoing activity, and derive the relationships shaping the value of the temporal decay and spatial propagation length as a function of this background value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13854v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditi Chandra, Marcelo O. Magnasco</dc:creator>
    </item>
    <item>
      <title>Poisson Variational Autoencoder</title>
      <link>https://arxiv.org/abs/2405.14473</link>
      <description>arXiv:2405.14473v1 Announce Type: cross 
Abstract: Variational autoencoders (VAE) employ Bayesian inference to interpret sensory inputs, mirroring processes that occur in primate vision across both ventral (Higgins et al., 2021) and dorsal (Vafaii et al., 2023) pathways. Despite their success, traditional VAEs rely on continuous latent variables, which deviates sharply from the discrete nature of biological neurons. Here, we developed the Poisson VAE (P-VAE), a novel architecture that combines principles of predictive coding with a VAE that encodes inputs into discrete spike counts. Combining Poisson-distributed latent variables with predictive coding introduces a metabolic cost term in the model loss function, suggesting a relationship with sparse coding which we verify empirically. Additionally, we analyze the geometry of learned representations, contrasting the P-VAE to alternative VAE models. We find that the P-VAEencodes its inputs in relatively higher dimensions, facilitating linear separability of categories in a downstream classification task with a much better (5x) sample efficiency. Our work provides an interpretable computational framework to study brain-like sensory processing and paves the way for a deeper understanding of perception as an inferential process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14473v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hadi Vafaii, Dekel Galor, Jacob L. Yates</dc:creator>
    </item>
    <item>
      <title>Discretization of continuous input spaces in the hippocampal autoencoder</title>
      <link>https://arxiv.org/abs/2405.14600</link>
      <description>arXiv:2405.14600v1 Announce Type: cross 
Abstract: The hippocampus has been associated with both spatial cognition and episodic memory formation, but integrating these functions into a unified framework remains challenging. Here, we demonstrate that forming discrete memories of visual events in sparse autoencoder neurons can produce spatial tuning similar to hippocampal place cells. We then show that the resulting very high-dimensional code enables neurons to discretize and tile the underlying image space with minimal overlap. Additionally, we extend our results to the auditory domain, showing that neurons similarly tile the frequency space in an experience-dependent manner. Lastly, we show that reinforcement learning agents can effectively perform various visuo-spatial cognitive tasks using these sparse, very high-dimensional representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14600v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrian F. Amil, Ismael T. Freire, Paul F. M. J. Verschure</dc:creator>
    </item>
    <item>
      <title>Discontinuous transition to chaos in a canonical random neural network</title>
      <link>https://arxiv.org/abs/2405.14607</link>
      <description>arXiv:2405.14607v1 Announce Type: cross 
Abstract: We study a paradigmatic random recurrent neural network introduced by Sompolinsky, Crisanti, and Sommers (SCS). In the infinite size limit, this system exhibits a direct transition from a homogeneous rest state to chaotic behavior, with the Lyapunov exponent gradually increasing from zero. We generalize the SCS model considering odd saturating nonlinear transfer functions, beyond the usual choice $\phi(x)=\tanh x$. A discontinuous transition to chaos occurs whenever the slope of $\phi$ at 0 is a local minimum (i.e.,~for $\phi'''(0)&gt;0$). Chaos appears out of the blue, by an attractor-repeller fold. Accordingly, the Lyapunov exponent stays away from zero at the birth of chaos.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14607v1</guid>
      <category>nlin.CD</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diego Paz\'o</dc:creator>
    </item>
    <item>
      <title>Dynamical properties and mechanisms of metastability: a perspective in neuroscience</title>
      <link>https://arxiv.org/abs/2305.05328</link>
      <description>arXiv:2305.05328v2 Announce Type: replace 
Abstract: Metastability, characterized by a variability of regimes in time, is a ubiquitous type of neural dynamics. It has been formulated in many different ways in the neuroscience literature, however, which may cause some confusion. In this Perspective, we discuss metastability from the point of view of dynamical systems theory. We extract from the literature a very simple but general definition through the concept of metastable regimes as long-lived but transient epochs of activity with unique dynamical properties. This definition serves as an umbrella term that encompasses formulations from other works, and readily connects to concepts from dynamical systems theory. This allows us to examine general dynamical properties of metastable regimes, propose in a didactic manner several dynamics-based mechanisms that generate them, and discuss a theoretical tool to characterize them quantitatively. This perspective leads to insights that help to address issues debated in the literature and also suggest pathways for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.05328v2</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <category>nlin.CD</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kalel L. Rossi, Roberto C. Budzinski, Everton S. Medeiros, Bruno R. R. Boaretto, Lyle Muller, Ulrike Feudel</dc:creator>
    </item>
    <item>
      <title>MorphGrower: A Synchronized Layer-by-layer Growing Approach for Plausible Neuronal Morphology Generation</title>
      <link>https://arxiv.org/abs/2401.09500</link>
      <description>arXiv:2401.09500v2 Announce Type: replace 
Abstract: Neuronal morphology is essential for studying brain functioning and understanding neurodegenerative disorders. As acquiring real-world morphology data is expensive, computational approaches for morphology generation have been studied. Traditional methods heavily rely on expert-set rules and parameter tuning, making it difficult to generalize across different types of morphologies. Recently, MorphVAE was introduced as the sole learning-based method, but its generated morphologies lack plausibility, i.e., they do not appear realistic enough and most of the generated samples are topologically invalid. To fill this gap, this paper proposes MorphGrower, which mimicks the neuron natural growth mechanism for generation. Specifically, MorphGrower generates morphologies layer by layer, with each subsequent layer conditioned on the previously generated structure. During each layer generation, MorphGrower utilizes a pair of sibling branches as the basic generation block and generates branch pairs synchronously. This approach ensures topological validity and allows for fine-grained generation, thereby enhancing the realism of the final generated morphologies. Results on four real-world datasets demonstrate that MorphGrower outperforms MorphVAE by a notable margin. Importantly, the electrophysiological response simulation demonstrates the plausibility of our generated samples from a neuroscience perspective. Our code is available at https://github.com/Thinklab-SJTU/MorphGrower.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09500v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nianzu Yang, Kaipeng Zeng, Haotian Lu, Yexin Wu, Zexin Yuan, Danni Chen, Shengdian Jiang, Jiaxiang Wu, Yimin Wang, Junchi Yan</dc:creator>
    </item>
    <item>
      <title>Extreme value statistics of nerve transmission delay</title>
      <link>https://arxiv.org/abs/2402.00484</link>
      <description>arXiv:2402.00484v2 Announce Type: replace 
Abstract: Nerve transmission delay is an important topic in neuroscience. Spike signals fired or received at the dendrites of a neuron travel from the axon to the presynaptic cell. The spike signal triggers a chemical reaction at the synapse, wherein a presynaptic cell transfers neurotransmitters to the postsynaptic cell, and regenerates electrical signals by a chemical reaction process through ion channels and transmits it to neighboring neurons. In the context of describing the complex physiological reaction process as a stochastic process, this study aimed to show that the distribution of the maximum time interval of spike signals follows extreme order statistics. By considering the statistical variance in the time constant of the Leaky Integrate-and-Fire model, which is a deterministic time evolution model of spike signals, we enabled randomness in the time interval of spike signals. When the time constant follows an exponential distribution function, the time interval of the spike signal also follows an exponential distribution. In this case, our theory and simulations confirmed that the histogram of the maximum time interval follows the Gumbel distribution, which is one of the three types of extreme value statistics. We also confirmed that the histogram of the maximum time interval follows a Fr\'{e}chet distribution when the time interval of the spike signal follows a Pareto distribution. These findings confirm that nerve transmission delay can be described using extreme value statistics and could, therefore, be used as a new indicator for transmission delay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00484v2</guid>
      <category>q-bio.NC</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satori Tsuzuki</dc:creator>
    </item>
    <item>
      <title>Broken detailed balance and entropy production in directed networks</title>
      <link>https://arxiv.org/abs/2402.19157</link>
      <description>arXiv:2402.19157v3 Announce Type: replace-cross 
Abstract: The structure of a complex network plays a crucial role in determining its dynamical properties. In this work, we show that the the degree to which a network is directed and hierarchically organised is closely associated with the degree to which its dynamics break detailed balance and produce entropy. We consider a range of dynamical processes and show how different directed network features affect their entropy production rate. We begin with an analytical treatment of a 2-node network followed by numerical simulations of synthetic networks using the preferential attachment and Erd\"os-Renyi algorithms. Next, we analyse a collection of 97 empirical networks to determine the effect of complex real-world topologies. Finally, we present a simple method for inferring broken detailed balance and directed network structure from multivariate time-series and apply our method to identify non-equilibrium dynamics and hierarchical organisation in both human neuroimaging and financial time-series. Overall, our results shed light on the consequences of directed network structure on non-equilibrium dynamics and highlight the importance and ubiquity of hierarchical organisation and non-equilibrium dynamics in real-world systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.19157v3</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ram\'on Nartallo-Kaluarachchi, Malbor Asllani, Gustavo Deco, Morten L. Kringelbach, Alain Goriely, Renaud Lambiotte</dc:creator>
    </item>
  </channel>
</rss>
