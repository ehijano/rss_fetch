<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Sep 2025 01:27:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>XASDB -- Design and Implementation of an Open-Access Spectral Database</title>
      <link>https://arxiv.org/abs/2509.13566</link>
      <description>arXiv:2509.13566v1 Announce Type: cross 
Abstract: The increasing volume and complexity of X-ray absorption spectroscopy (XAS) data generated at synchrotron facilities worldwide require robust infrastructure for data management, sharing, and analysis. This paper introduces the XAS Database (XASDB), a comprehensive web-based platform developed and hosted by the Canadian Light Source (CLS). The database houses more than 1000 reference spectra spanning 40 elements and 324 chemical compounds. The platform employs a Node.js/MongoDB architecture designed to handle diverse data formats from multiple beamlines and synchrotron facilities. A key innovation is the XASproc JavaScript library, which enables browser-based XAS data processing including normalization, background sub- traction, extended X-ray absorption fine structure (EXAFS) extraction, and preliminary analysis traditionally limited to desktop applications. The integrated XASVue spectral viewer provides installation-free data visualization and analysis with broad accessibility across devices and operating systems. By offering standardized data output, comprehensive metadata, and integrated analytical ca- pabilities, XASDB facilitates collaborative research and promotes FAIR (Findable, Accessible, In- teroperable, and Reusable) data principles. The platform serves as a valuable resource for linear combination fitting (LCF) analysis, machine learning applications, and educational purposes. This initiative demonstrates the potential for web-centric approaches in XAS data analysis, accelerating advances in materials science, environmental research, chemistry, and biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13566v1</guid>
      <category>cs.DB</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denis Spasyuk</dc:creator>
    </item>
    <item>
      <title>Nucleation regions in the Large-Scale Structure II: Morphology and dynamical state of supercluster cores</title>
      <link>https://arxiv.org/abs/2509.13573</link>
      <description>arXiv:2509.13573v1 Announce Type: cross 
Abstract: This work explores the morphology and dynamical properties of cores within rich superclusters, highlighting their role as transitional structures in the large-scale structure of the Universe. Using projected and radial velocity distributions of member galaxies, we identify cores as dense structures that, despite being gravitationally bound, are not yet dynamically relaxed. However, they exhibit a tendency toward virialisation, evolving in a self-similar manner to massive galaxy clusters but on a larger scale. Morphological analysis reveals that cores are predominantly filamentary, reflecting quasi-linear formation processes consistent with the Zeldovich approximation. Our estimates of the entropy confirm their intermediate dynamical state, with relaxation levels varying across the sample. Mass estimates indicate efficient accretion processes, concentrating matter into gravitationally bound systems. We conclude that cores are important environments where galaxy evolution and hierarchical assembly occur, bridging the gap between supercluster-scale structures and virialised clusters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13573v1</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.GA</category>
      <category>math.DG</category>
      <category>math.PR</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. M. Z\'u\~niga, C. A. Caretta, H. Andernach</dc:creator>
    </item>
    <item>
      <title>Artificial neural networks ensemble methodology to predict significant wave height</title>
      <link>https://arxiv.org/abs/2509.14020</link>
      <description>arXiv:2509.14020v1 Announce Type: cross 
Abstract: The forecast of wave variables are important for several applications that depend on a better description of the ocean state. Due to the chaotic behaviour of the differential equations which model this problem, a well know strategy to overcome the difficulties is basically to run several simulations, by for instance, varying the initial condition, and averaging the result of each of these, creating an ensemble. Moreover, in the last few years, considering the amount of available data and the computational power increase, machine learning algorithms have been applied as surrogate to traditional numerical models, yielding comparative or better results. In this work, we present a methodology to create an ensemble of different artificial neural networks architectures, namely, MLP, RNN, LSTM, CNN and a hybrid CNN-LSTM, which aims to predict significant wave height on six different locations in the Brazilian coast. The networks are trained using NOAA's numerical reforecast data and target the residual between observational data and the numerical model output. A new strategy to create the training and target datasets is demonstrated. Results show that our framework is capable of producing high efficient forecast, with an average accuracy of $80\%$, that can achieve up to $88\%$ in the best case scenario, which means $5\%$ reduction in error metrics if compared to NOAA's numerical model, and a increasingly reduction of computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14020v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.oceaneng.2024.117479</arxiv:DOI>
      <arxiv:journal_reference>Ocean Engineering, 300 (2024) 117479</arxiv:journal_reference>
      <dc:creator>Felipe Crivellaro Minuzzi, Leandro Farina</dc:creator>
    </item>
    <item>
      <title>Radar Maxima: calibrated area-based probabilistic forecasts for heavy precipitation</title>
      <link>https://arxiv.org/abs/2509.14081</link>
      <description>arXiv:2509.14081v1 Announce Type: cross 
Abstract: We present, motivate, and evaluate Radar Maxima, a calibrated area-based probabilistic forecast product for heavy precipitation. It is designed to overcome inherent limitations of point-based forecasts, which often yield low probabilities for extreme events due to spatial displacement errors. The method aggregates radar-derived precipitation within 40 km neighbourhoods to statistically upscale forecasts from the ensemble system ICON-D2-EPS of DWD. Evaluation considers both objective verification metrics and feedback from operational weather forecasters based on case studies. Verification shows improved predictability, reliability and forecast sharpness. Evaluation of forecasters confirmed operational value in some cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14081v1</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reinhold Hess</dc:creator>
    </item>
    <item>
      <title>An Attention-Based Stochastic Simulator for Multisite Extremes to Evaluate Nonstationary, Cascading Flood Risk</title>
      <link>https://arxiv.org/abs/2509.14162</link>
      <description>arXiv:2509.14162v1 Announce Type: cross 
Abstract: Compound flood risks from spatially and temporally clustered extremes challenge traditional risk models and insurance portfolios that often neglect correlated risks across regions. Spatiotemporally clustered floods exhibit fat-tail behavior, modulated by low-frequency hydroclimatic variability and large-scale moisture transport. Nonstationary stochastic simulators and regional compound event models aim to capture such tail risk, but have not yet unified spatial and temporal extremes under low-frequency hydroclimatic variability. We introduce a novel attention-based framework for multisite flood generation conditional on a multivariate hydroclimatic signal with explainable attribution to global sub-decadal to multi-decadal climate variability. Our simulator combines wavelet signal processing, transformer-based multivariate time series forecasting, and modified Neyman-Scott joint clustering to simulate climate-informed spatially compounding and temporally cascading floods. Applied to a Mississippi River Basin case study, the model generates distributed portfolios of plausibly clustered flood risks across space and time, providing a basis for simulating spatiotemporally correlated losses characteristic of flood-induced damage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14162v1</guid>
      <category>physics.geo-ph</category>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Nayak, Pierre Gentine, Upmanu Lall</dc:creator>
    </item>
    <item>
      <title>Large deviations for probability graphons</title>
      <link>https://arxiv.org/abs/2509.14204</link>
      <description>arXiv:2509.14204v1 Announce Type: cross 
Abstract: We establish a large deviation principle (LDP) for probability graphons, which are symmetric functions from the unit square into the space of probability measures. This notion extends classical graphons and provides a flexible framework for studying the limit behavior of large dense weighted graphs. In particular, our result generalizes the seminal work of Chatterjee and Varadhan (2011), who derived an LDP for Erd\H{o}s-R\'enyi random graphs via graphon theory. We move beyond their binary (Bernoulli) setting to encompass arbitrary edge-weight distributions. Specifically, we analyze the distribution on probability graphons induced by random weighted graphs in which edges are sampled independently from a common reference probability measure supported on a compact Polish space. We prove that this distribution satisfies an LDP with a good rate function, expressed as an extension of the Kullback-Leibler divergence between probability graphons and the reference measure. This theorem can also be viewed as a Sanov-type result in the graphon setting. Our work provides a rigorous foundation for analyzing rare events in weighted networks and supports statistical inference in structured random graph models under distributional edge uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14204v1</guid>
      <category>math.PR</category>
      <category>cond-mat.stat-mech</category>
      <category>math.CO</category>
      <category>math.FA</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pierfrancesco Dionigi, Giulio Zucal</dc:creator>
    </item>
    <item>
      <title>Tailor-Made Metasurface Camouflage</title>
      <link>https://arxiv.org/abs/2504.00620</link>
      <description>arXiv:2504.00620v2 Announce Type: replace-cross 
Abstract: Reducing electromagnetic scattering from an object has always been a task, inspiring efforts across disciplines such as materials science and electromagnetic theory. The pursuit of electromagnetic cloaking significantly advanced the field of metamaterials, yet achieving broadband, conformal cloaking for complex, non-trivial objects remains an unresolved challenge. Here, we introduce the concept of 'tailor-made metasurfaces' - machine-designed aperiodic structures optimized to suppress scattering from arbitrary objects by accounting for their layout, including resonant or large-scale features. Specifically, we demonstrated a wideband ~20% fractional bandwidth scattering suppression of more than 20-30 dB for various generic test objects, including randomly distributed wire meshes, spheres, and polygons. The demonstrated evolutionary optimization marks a leap forward in electromagnetic design, enabling the development of high-performance structures to meet complex technological demands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00620v2</guid>
      <category>physics.optics</category>
      <category>physics.app-ph</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Tsukerman, K. Grotov, A. Mikhailovskaya, P. Bezrukov, S. Geyman, A. Kharchevskii, A. Maximenko, V. Bobrovs, P. Ginzburg</dc:creator>
    </item>
    <item>
      <title>Noise2Ghost: Self-supervised deep convolutional reconstruction for ghost imaging</title>
      <link>https://arxiv.org/abs/2504.10288</link>
      <description>arXiv:2504.10288v2 Announce Type: replace-cross 
Abstract: We present a new self-supervised deep-learning-based Ghost Imaging (GI) reconstruction method, which provides unparalleled reconstruction performance for noisy acquisitions among unsupervised methods. We present the supporting mathematical framework and results from theoretical and real data use cases. Self-supervision removes the need for clean reference data while offering strong noise reduction. This provides the necessary tools for addressing signal-to-noise ratio concerns for GI acquisitions in emerging and cutting-edge low-light GI scenarios. Notable examples include micro- and nano-scale x-ray emission imaging, e.g., x-ray fluorescence imaging of dose-sensitive samples. Their applications include in-vivo and in-operando case studies for biological samples and batteries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10288v2</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathieu Manni, Dmitry Karpov, K. Joost Batenburg, Sharon Shwartz, Nicola Vigan\`o</dc:creator>
    </item>
    <item>
      <title>Particle identification in the GlueX detector with machine learning</title>
      <link>https://arxiv.org/abs/2505.14706</link>
      <description>arXiv:2505.14706v3 Announce Type: replace-cross 
Abstract: In particle physics experiments, identifying the types of particles registered in a detector is essential for the accurate reconstruction of particle collisions. At Thomas Jefferson National Accelerator Facility (Jefferson Lab), the GlueX experiment performs particle identification (PID) by setting specific thresholds, known as cuts, on the kinematic properties of tracks and showers obtained from detector hits. Our research aims to enhance this cut-based method by employing machine-learning algorithms based on multi-layer perceptrons and boosted decision trees. Similar approaches have been applied in other particle physics experiments and offer an opportunity to increase PID accuracies using reconstructed kinematic data. Our study illustrates that both multilayered perceptrons and boosted decision trees can identify charged and neutral particles in Monte Carlo simulated GlueX data with significantly improved accuracy over the current cuts-based PID method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14706v3</guid>
      <category>physics.ins-det</category>
      <category>hep-ex</category>
      <category>nucl-ex</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Habjan, Richard Dube, James McIntyre, Mezmur Edo, Richard Jones</dc:creator>
    </item>
    <item>
      <title>Lost Data in Electron Microscopy</title>
      <link>https://arxiv.org/abs/2508.18217</link>
      <description>arXiv:2508.18217v2 Announce Type: replace-cross 
Abstract: The goal of this study is to estimate the amount of lost data in electron microscopy and to analyze the extent to which experimentally acquired images are utilized in peer-reviewed scientific publications. Analysis of the number of images taken on electron microscopes at a core user facility and the number of images subsequently included in peer-reviewed scientific journals revealed low efficiency of data utilization. Up to around 90% of electron microscopy data generated during routine instrument operation remain unused. Of the more than 150 000 electron microscopy images evaluated in this study, only approximately 3 500 (just over 2%) were made available in publications. For the analyzed dataset, the amount of lost data in electron microscopy can be estimated as &gt;90% (in terms of data being recorded but not being published in peer-reviewed literature). On the one hand, these results highlight a shortcoming in the optimal use of microscopy images; on the other hand, they indicate the existence of a large pool of electron microscopy data that can facilitate research in data science and the development of AI-based projects. The considerations important to unlock the potential of lost data are discussed in the present article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18217v2</guid>
      <category>cs.DB</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.DL</category>
      <category>physics.chem-ph</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nina M. Ivanova, Alexey S. Kashin, Valentine P. Ananikov</dc:creator>
    </item>
  </channel>
</rss>
