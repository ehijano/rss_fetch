<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Jan 2025 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Introducing the generalized gamma distribution: a flexible distribution for index standardization</title>
      <link>https://arxiv.org/abs/2501.05618</link>
      <description>arXiv:2501.05618v1 Announce Type: new 
Abstract: Fisheries scientists use regression models to estimate population quantities, such as biomass or abundance, for use in climate, habitat, stock, and ecosystem assessments. However, these models are sensitive to the chosen probability distribution used to characterize observation error. Here, we introduce the generalized gamma distribution (GGD), which has not been widely used in fisheries science. The GGD has useful properties: (1) it reduces to the lognormal distribution when the shape parameter approaches zero; (2) it reduces to the gamma distribution when the shape and scale parameters are equal; and (3) the coefficient of variation is independent of the mean. We assess the relative performance and robustness of the GGD to estimate biomass density across different observation error types in a simulation experiment. When fit to data generated from the GGD, lognormal, gamma, and Tweedie families, the GGD had low bias and high predictive accuracy. Finally, we fit spatiotemporal index standardization models using the R package sdmTMB to 15 species from three trawl surveys from the Gulf of Alaska and coast of British Columbia, Canada. When the Akaike information criterion (AIC) weight was compared among fits using the lognormal, gamma, and Tweedie families the GGD was the most commonly selected model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05618v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jillian C. Dunic, Jason Conner, Sean C. Anderson, James T. Thorson</dc:creator>
    </item>
    <item>
      <title>An Efficient Dual ADMM for Huber Regression with Fused Lasso Penalty</title>
      <link>https://arxiv.org/abs/2501.05676</link>
      <description>arXiv:2501.05676v1 Announce Type: new 
Abstract: The ordinary least squares estimate in linear regression is sensitive to the influence of errors with large variance, which reduces its robustness, especially when dealing with heavy-tailed errors or outliers frequently encountered in real-world scenarios. To address this issue and accommodate the sparsity of coefficients along with their sequential disparities, we combine the adaptive robust Huber loss function with a fused lasso penalty. This combination yields a robust estimator capable of simultaneously achieving estimation and variable selection. Furthermore, we utilize an efficient alternating direction method of multipliers to solve this regression model from a dual perspective. The effectiveness and efficiency of our proposed approach is demonstrated through numerical experiments carried out on both simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05676v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengjiao Shi, Yunhai Xiao</dc:creator>
    </item>
    <item>
      <title>Causal survival analysis, Estimation of the Average Treatment Effect (ATE): Practical Recommendations</title>
      <link>https://arxiv.org/abs/2501.05836</link>
      <description>arXiv:2501.05836v1 Announce Type: new 
Abstract: Causal survival analysis combines survival analysis and causal inference to evaluate the effect of a treatment or intervention on a time-to-event outcome, such as survival time. It offers an alternative to relying solely on Cox models for assessing these effects. In this paper, we present a comprehensive review of estimators for the average treatment effect measured with the restricted mean survival time, including regression-based methods, weighting approaches, and hybrid techniques. We investigate their theoretical properties and compare their performance through extensive numerical experiments. Our analysis focuses on the finite-sample behavior of these estimators, the influence of nuisance parameter selection, and their robustness and stability under model misspecification. By bridging theoretical insights with practical evaluation, we aim to equip practitioners with both state-of-the-art implementations of these methods and practical guidelines for selecting appropriate estimators for treatment effect estimation. Among the approaches considered, G-formula two-learners, AIPCW-AIPTW, Buckley-James estimators, and causal survival forests emerge as particularly promising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05836v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charlotte Voinot (PREMEDICAL, Sanofi Gentilly), Cl\'ement Berenfeld (Sanofi Gentilly), Imke Mayer (Sanofi Gentilly), Bernard Sebastien (Sanofi Gentilly), Julie Josse (PREMEDICAL)</dc:creator>
    </item>
    <item>
      <title>An approach to non-homogenous phase-type distributions through multiple cut-points</title>
      <link>https://arxiv.org/abs/2501.05863</link>
      <description>arXiv:2501.05863v1 Announce Type: new 
Abstract: A new class of distributions based on phase-type distributions is introduced in the current paper to model lifetime data in the field of reliability analysis. This one is the natural extension of the distribution proposed by Acal et al. (2021) for more than one cut-point. Multiple interesting measures such as density function, hazard rate or moments, among others, were worked out both for the continuous and discrete case. Besides, a new EM-algorithm is provided to estimate the parameters by maximum likelihood. The results have been implemented computationally in R and simulation studies reveal that this new distribution reduces the number of parameters to be estimated in the optimization process and, in addition, it improves the fitting accuracy in comparison with the classical phase-type distributions, especially in heavy tailed distributions. An application is presented in the context of resistive memories with a new set of electron devices for non-volatile memory circuits. In particular, the voltage associated with the resistive switching processes that control the internal behaviour of resistive memories has been modelled with this new distribution to shed light on the physical mechanisms behind the operation of these memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05863v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/08982112.2023.2168202</arxiv:DOI>
      <arxiv:journal_reference>Quality Engineering, Volume 35, Issue 4, Pages 619 - 638, Year 2023</arxiv:journal_reference>
      <dc:creator>Juan Eloy Ruiz-Castro, Christian Acal, Juan B. Rold\'an</dc:creator>
    </item>
    <item>
      <title>Doubly-Robust Functional Average Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2501.06024</link>
      <description>arXiv:2501.06024v1 Announce Type: new 
Abstract: Understanding causal relationships in the presence of complex, structured data remains a central challenge in modern statistics and science in general. While traditional causal inference methods are well-suited for scalar outcomes, many scientific applications demand tools capable of handling functional data -- outcomes observed as functions over continuous domains such as time or space. Motivated by this need, we propose DR-FoS, a novel method for estimating the Functional Average Treatment Effect (FATE) in observational studies with functional outcomes. DR-FoS exhibits double robustness properties, ensuring consistent estimation of FATE even if either the outcome or the treatment assignment model is misspecified. By leveraging recent advances in functional data analysis and causal inference, we establish the asymptotic properties of the estimator, proving its convergence to a Gaussian process. This guarantees valid inference with simultaneous confidence bands across the entire functional domain. Through extensive simulations, we show that DR-FoS achieves robust performance under a wide range of model specifications. Finally, we illustrate the utility of DR-FoS in a real-world application, analyzing functional outcomes to uncover meaningful causal insights in the SHARE (Survey of Health, Aging and Retirement in Europe) dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06024v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Testa, Tobia Boschi, Francesca Chiaromonte, Edward H. Kennedy, Matthew Reimherr</dc:creator>
    </item>
    <item>
      <title>Identification and Scaling of Latent Variables in Ordinal Factor Analysis</title>
      <link>https://arxiv.org/abs/2501.06094</link>
      <description>arXiv:2501.06094v1 Announce Type: new 
Abstract: Social science researchers are generally accustomed to treating ordinal variables as though they are continuous. In this paper, we consider how identification constraints in ordinal factor analysis can mimic the treatment of ordinal variables as continuous. We describe model constraints that lead to latent variable predictions equaling the average of ordinal variables. This result leads us to propose minimal identification constraints, which we call "integer constraints," that center the latent variables around the scale of the observed, integer-coded ordinal variables. The integer constraints lead to intuitive model parameterizations because researchers are already accustomed to thinking about ordinal variables as though they are continuous. We provide a proof that our proposed integer constraints are indeed minimal identification constraints, as well as an illustration of how integer constraints work with real data. We also provide simulation results indicating that integer constraints are similar to other identification constraints in terms of estimation convergence and admissibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06094v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edgar C. Merkle, Sonja D. Winter, Ellen Fitzsimmons</dc:creator>
    </item>
    <item>
      <title>Testing conditional independence under isotonicity</title>
      <link>https://arxiv.org/abs/2501.06133</link>
      <description>arXiv:2501.06133v1 Announce Type: new 
Abstract: We propose a test of the conditional independence of random variables $X$ and $Y$ given $Z$ under the additional assumption that $X$ is stochastically increasing in $Z$. The well-documented hardness of testing conditional independence means that some further restriction on the null hypothesis parameter space is required, but in contrast to existing approaches based on parametric models, smoothness assumptions, or approximations to the conditional distribution of $X$ given $Z$ and/or $Y$ given $Z$, our test requires only the stochastic monotonicity assumption. Our procedure, called PairSwap-ICI, determines the significance of a statistic by randomly swapping the $X$ values within ordered pairs of $Z$ values. The matched pairs and the test statistic may depend on both $Y$ and $Z$, providing the analyst with significant flexibility in constructing a powerful test. Our test offers finite-sample Type I error control, and provably achieves high power against a large class of alternatives that are not too close to the null. We validate our theoretical findings through a series of simulations and real data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06133v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohan Hore, Jake A. Soloff, Rina Foygel Barber, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>k-Sample inference via Multimarginal Optimal Transport</title>
      <link>https://arxiv.org/abs/2501.05645</link>
      <description>arXiv:2501.05645v1 Announce Type: cross 
Abstract: This paper proposes a Multimarginal Optimal Transport ($MOT$) approach for simultaneously comparing $k\geq 2$ measures supported on finite subsets of $\mathbb{R}^d$, $d \geq 1$. We derive asymptotic distributions of the optimal value of the empirical $MOT$ program under the null hypothesis that all $k$ measures are same, and the alternative hypothesis that at least two measures are different. We use these results to construct the test of the null hypothesis and provide consistency and power guarantees of this $k$-sample test. We consistently estimate asymptotic distributions using bootstrap, and propose a low complexity linear program to approximate the test cut-off. We demonstrate the advantages of our approach on synthetic and real datasets, including the real data on cancers in the United States in 2004 - 2020.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05645v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia Kravtsova</dc:creator>
    </item>
    <item>
      <title>On factor copula-based mixed regression models</title>
      <link>https://arxiv.org/abs/2305.02789</link>
      <description>arXiv:2305.02789v3 Announce Type: replace 
Abstract: In this article, a copula-based method for mixed regression models is proposed, where the conditional distribution of the response variable, given covariates, is modelled by a parametric family of continuous or discrete distributions, and the effect of a common latent variable pertaining to a cluster is modelled with a factor copula. We show how to estimate the parameters of the copula and the parameters of the margins, and we find the asymptotic behaviour of the estimation errors. Numerical experiments are performed to assess the precision of the estimators for finite samples. An example of an application is given using COVID-19 vaccination hesitancy from several countries. Computations are based on R package CopulaGAMM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.02789v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pavel Krupskii, Bouchra R Nasri, Bruno N Remillard</dc:creator>
    </item>
    <item>
      <title>Test-negative designs with various reasons for testing: statistical bias and solution</title>
      <link>https://arxiv.org/abs/2312.03967</link>
      <description>arXiv:2312.03967v3 Announce Type: replace 
Abstract: Test-negative designs are widely used for post-market evaluation of vaccine effectiveness, particularly in cases when randomized trials are not feasible. Differing from classical test-negative designs where only healthcare-seekers with symptoms are included, recent test-negative designs have involved individuals with various reasons for testing, especially in an outbreak setting. While including these data can increase sample size and hence improve precision, concerns have been raised about whether they introduce bias into the current framework of test-negative designs, thereby demanding a formal statistical examination of this modified design. In this article, using statistical derivations, causal graphs, and numerical demonstrations, we show that the standard odds ratio estimator may be biased if various reasons for testing are not accounted for. To eliminate this bias, we identify three categories of reasons for testing, including symptoms, mandatory screening, and case contact tracing, and characterize associated statistical properties and estimands. Based on our characterization, we show how to consistently estimate each estimand via stratification. Furthermore, we describe when these estimands correspond to the same vaccine effectiveness parameter, and, when appropriate, propose a stratified estimator that can incorporate multiple reasons for testing and improve precision. The performance of our proposed method is demonstrated through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03967v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengxin Yu, Tom Hongyi Liu, Kendrick Qijun Li, Nicholas Jewell, Eric Tchetgen Tchetgen, Dylan Small, Xu Shi, Bingkai Wang</dc:creator>
    </item>
    <item>
      <title>Robust Point Matching with Distance Profiles</title>
      <link>https://arxiv.org/abs/2312.12641</link>
      <description>arXiv:2312.12641v5 Announce Type: replace 
Abstract: We show the outlier robustness and noise stability of practical matching procedures based on distance profiles. Although the idea of matching points based on invariants like distance profiles has a long history in the literature, there has been little understanding of the theoretical properties of such procedures, especially in the presence of outliers and noise. We provide a theoretical analysis showing that under certain probabilistic settings, the proposed matching procedure is successful with high probability even in the presence of outliers and noise. We demonstrate the performance of the proposed method using a real data example and provide simulation studies to complement the theoretical findings. Lastly, we extend the concept of distance profiles to the abstract setting and connect the proposed matching procedure to the Gromov-Wasserstein distance and its lower bound, with a new sample complexity result derived based on the properties of distance profiles. This paper contributes to the literature by providing theoretical underpinnings of the matching procedures based on invariants like distance profiles, which have been widely used in practice but have rarely been analyzed theoretically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12641v5</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>YoonHaeng Hur, Yuehaw Khoo</dc:creator>
    </item>
    <item>
      <title>On GEE for Mean-Variance-Correlation Models: Variance Estimation and Model Selection</title>
      <link>https://arxiv.org/abs/2401.08172</link>
      <description>arXiv:2401.08172v2 Announce Type: replace 
Abstract: Generalized estimating equations (GEE) are of great importance in analyzing clustered data without full specification of multivariate distributions. A recent approach jointly models the mean, variance, and correlation coefficients of clustered data through three sets of regressions (Luo and Pan, 2022). We observe that these estimating equations, however, are a special case of those of Yan and Fine (2004) which further allows the variance to depend on the mean through a variance function. The proposed variance estimators may be incorrect for the variance and correlation parameters because of a subtle dependence induced by the nested structure of the estimating equations. We characterize model settings where their variance estimation is invalid and show the variance estimators in Yan and Fine (2004) correctly account for such dependence. In addition, we introduce a novel model selection criterion that enables the simultaneous selection of the mean-scale-correlation model. The sandwich variance estimator and the proposed model selection criterion are tested by several simulation studies and real data analysis, which validate its effectiveness in variance estimation and model selection. Our work also extends the R package geepack with the flexibility to apply different working covariance matrices for the variance and correlation structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08172v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenyu Xu, Jason P. Fine, Wenling Song, Jun Yan</dc:creator>
    </item>
    <item>
      <title>Positivity violations in marginal structural survival models with time-dependent confounding: a simulation study on IPTW-estimator performance</title>
      <link>https://arxiv.org/abs/2403.19606</link>
      <description>arXiv:2403.19606v2 Announce Type: replace 
Abstract: In longitudinal observational studies, marginal structural models (MSMs) are a class of causal models used to analyse the effect of an exposure on the (time-to-event) outcome of interest, while accounting for exposure-affected time-dependent confounding. In the applied literature, inverse probability of treatment weighting (IPTW) has been widely adopted to estimate MSMs. An essential assumption for IPTW-based MSMs is the positivity assumption, which ensures that, for any combination of measured confounders among individuals, there is a non-zero probability of receiving each possible treatment strategy. Positivity is crucial for valid causal inference through IPTW-based MSMs, but is often overlooked compared to confounding bias. Positivity violations may also arise due to randomness, in situations where the assignment to a specific treatment is theoretically possible but is either absent or rarely observed in the data, leading to near violations. These situations are common in practical applications, particularly when the sample size is small, and they pose significant challenges for causal inference. This study investigates the impact of near-positivity violations on estimates from IPTW-based MSMs in survival analysis. Two algorithms are proposed for simulating longitudinal data from hazard-MSMs, accommodating near-positivity violations, a time-varying binary exposure, and a time-to-event outcome. Cases of near-positivity violations, where remaining unexposed is rare within certain confounder levels, are analysed across various scenarios and weight truncation (WT) strategies. This work aims to serve as a critical warning against overlooking the positivity assumption or naively applying WT in causal studies using longitudinal observational data and IPTW.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19606v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marta Spreafico</dc:creator>
    </item>
    <item>
      <title>Multi-objective Bayesian optimization for Likelihood-Free inference in sequential sampling models of decision making</title>
      <link>https://arxiv.org/abs/2409.01735</link>
      <description>arXiv:2409.01735v3 Announce Type: replace 
Abstract: Scientifically motivated statistical models can sometimes be defined by a generative process for simulating synthetic data. Models specified this way can have likelihoods which are intractable, and this is the case for many sequential sampling models (SSMs) widely used in psychology and consumer behavior modelling. Researchers have developed likelihood-free inference (LFI) methods to make Bayesian inferences on parameters in models with intractable likelihood. Extending a popular approach to simulation efficient LFI for single-source data, we propose Multi-objective Bayesian Optimization for Likelihood-Free Inference (MOBOLFI) to estimate the parameters of SSMs calibrated using multi-source data, such as those based on response times and choice outcomes. MOBOLFI models a multi-dimensional discrepancy between observed and simulated data, using a discrepancy for each data source. Multi-objective Bayesian Optimization is then used to ensure simulation efficient approximation of the SSM likelihood. The use of a multivariate discrepancy allows for approximations to individual data source likelihoods in addition to the joint likelihood, enabling both the detection of conflicting information and a deeper understanding of the importance of different data sources in estimating individual SSM parameters. We illustrate the advantages of our approach in comparison with the use of a single discrepancy in a simple synthetic data example and an SSM example with real-world data assessing preferences of ride-hailing drivers in Singapore to rent electric vehicles. Although we focus on applications to SSMs, our approach applies to the likelihood-free calibration of other models using multi-source data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01735v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Chen, Xinwei Li, Eui-Jin Kim, Prateek Bansal, David Nott</dc:creator>
    </item>
    <item>
      <title>Bayesian Transfer Learning for Artificially Intelligent Geospatial Systems: A Predictive Stacking Approach</title>
      <link>https://arxiv.org/abs/2410.09504</link>
      <description>arXiv:2410.09504v2 Announce Type: replace 
Abstract: Building artificially intelligent geospatial systems require rapid delivery of spatial data analysis at massive scales with minimal human intervention. Depending upon their intended use, data analysis may also entail model assessment and uncertainty quantification. This article devises transfer learning frameworks for deployment in artificially intelligent systems, where a massive data set is split into smaller data sets that stream into the analytical framework to propagate learning and assimilate inference for the entire data set. Specifically, we introduce Bayesian predictive stacking for multivariate spatial data and demonstrate its effectiveness in rapidly analyzing massive data sets. Furthermore, we make inference feasible in a reasonable amount of time, and without excessively demanding hardware settings. We illustrate the effectiveness of this approach in extensive simulation experiments and subsequently analyze massive data sets in climate science on sea surface temperatures and on vegetation index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09504v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Presicce, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>The Conflict Graph Design: Estimating Causal Effects under Arbitrary Neighborhood Interference</title>
      <link>https://arxiv.org/abs/2411.10908</link>
      <description>arXiv:2411.10908v2 Announce Type: replace 
Abstract: A fundamental problem in network experiments is selecting an appropriate experimental design in order to precisely estimate a given causal effect of interest. In this work, we propose the Conflict Graph Design, a general approach for constructing experiment designs under network interference with the goal of precisely estimating a pre-specified causal effect. A central aspect of our approach is the notion of a conflict graph, which captures the fundamental unobservability associated with the causal effect and the underlying network. In order to estimate effects, we propose a modified Horvitz--Thompson estimator. We show that its variance under the Conflict Graph Design is bounded as $O(\lambda(H) / n )$, where $\lambda(H)$ is the largest eigenvalue of the adjacency matrix of the conflict graph. These rates depend on both the underlying network and the particular causal effect under investigation. Not only does this yield the best known rates of estimation for several well-studied causal effects (e.g. the global and direct effects) but it also provides new methods for effects which have received less attention from the perspective of experiment design (e.g. spill-over effects). Finally, we construct conservative variance estimators which facilitate asymptotically valid confidence intervals for the causal effect of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10908v2</guid>
      <category>stat.ME</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vardis Kandiros, Charilaos Pipis, Constantinos Daskalakis, Christopher Harshaw</dc:creator>
    </item>
    <item>
      <title>Algorithmic modelling of a complex redundant multi-state system subject to multiple events, preventive maintenance, loss of units and a multiple vacation policy through a MMAP</title>
      <link>https://arxiv.org/abs/2411.19104</link>
      <description>arXiv:2411.19104v2 Announce Type: replace 
Abstract: A complex multi-state redundant system undergoing preventive maintenance and experiencing multiple events is being considered in a continuous time frame. The online unit is susceptible to various types of failures, both internal and external in nature, with multiple degradation levels present, both internally and externally. Random inspections are continuously monitoring these degradation levels, and if they reach a critical state, the unit is directed to a repair facility for preventive maintenance. The repair facility is managed by a single repairperson, who follows a multiple vacation policy dependent on the operational status of the units. The repairperson is responsible for two primary tasks: corrective repairs and preventive maintenance. The time durations within the system follow phase-type distributions, and the model is constructed using Markovian Arrival Processes with marked arrivals. A variety of performance measures, including transient and stationary distributions, are calculated using matrix-analytic methods. This approach enables the expression of key results and overall system behaviour in a matrix-algorithmic format. In order to optimize the model, costs and rewards are integrated into the analysis. A numerical example is presented to showcase the model's flexibility and effectiveness in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19104v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.matcom.2024.11.005</arxiv:DOI>
      <arxiv:journal_reference>Mathematics and Computers in Simulation, Volume 230, April 2025, Pages 165-192</arxiv:journal_reference>
      <dc:creator>Juan Eloy Ruiz-Castro, Hugo Ala\'in Zapata-Ceballos</dc:creator>
    </item>
    <item>
      <title>A shiny app for modeling the lifetime in primary breast cancer patients through phase-type distributions</title>
      <link>https://arxiv.org/abs/2412.03975</link>
      <description>arXiv:2412.03975v2 Announce Type: replace 
Abstract: Phase-type distributions (PHDs), which are defined as the distribution of the lifetime up to the absorption in an absorbent Markov chain, are an appropriate candidate to model the lifetime of any system, since any non-negative probability distribution can be approximated by a PHD with sufficient precision. Despite PHD potential, friendly statistical programs do not have a module implemented in their interfaces to handle PHD. Thus, researchers must consider others statistical software such as R, Matlab or Python that work with the compilation of code chunks and functions. This fact might be an important handicap for those researchers who do not have sufficient knowledge in programming environments. In this paper, a new interactive web application developed with shiny is introduced in order to adjust PHD to an experimental dataset. This open access app does not require any kind of knowledge about programming or major mathematical concepts. Users can easily compare the graphic fit of several PHDs while estimating their parameters and assess the goodness of fit with just several clicks. All these functionalities are exhibited by means of a numerical simulation and modeling the time to live since the diagnostic in primary breast cancer patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03975v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3934/mbe.2024065 10.3934/mbe.2024065</arxiv:DOI>
      <arxiv:journal_reference>Mathematical Biosciences and Engineering. 2024, Volume 21, Issue 1: 1508-1526</arxiv:journal_reference>
      <dc:creator>Christian Acal, Elena Contreras, Ismael Montero, Juan Eloy Ruiz-Castro</dc:creator>
    </item>
    <item>
      <title>Geometric Sampling</title>
      <link>https://arxiv.org/abs/2308.07715</link>
      <description>arXiv:2308.07715v2 Announce Type: replace-cross 
Abstract: This paper introduces an innovative and intuitive finite population sampling method that have been developed using a unique geometric framework. In this approach, I represent first-order inclusion probabilities as bars on a two-dimensional graph. By manipulating the positions of these bars, researchers can create a wide range of different sampling designs. This geometric visualization of sampling designs not only leads to increased creativity for researchers to provide new efficient designs but also eliminates the need for complex mathematical algorithms. This novel approach holds significant promise for tackling complex challenges in sampling, such as maximizing entropy and achieving an optimal design. By applying a version of the greedy best-first search algorithm to this geometric approach for finding an optimal design, I have demonstrated the potential for integrating intelligent algorithms into finite population sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.07715v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bardia Panahbehagh</dc:creator>
    </item>
    <item>
      <title>Bayesian Joint Additive Factor Models for Multiview Learning</title>
      <link>https://arxiv.org/abs/2406.00778</link>
      <description>arXiv:2406.00778v3 Announce Type: replace-cross 
Abstract: It is increasingly common in a wide variety of applied settings to collect data of multiple different types on the same set of samples. Our particular focus in this article is on studying relationships between such multiview features and responses. A motivating application arises in the context of precision medicine where multi-omics data are collected to correlate with clinical outcomes. It is of interest to infer dependence within and across views while combining multimodal information to improve the prediction of outcomes. The signal-to-noise ratio can vary substantially across views, motivating more nuanced statistical tools beyond standard late and early fusion. This challenge comes with the need to preserve interpretability, select features, and obtain accurate uncertainty quantification. We propose a joint additive factor regression model (JAFAR) with a structured additive design, accounting for shared and view-specific components. We ensure identifiability via a novel dependent cumulative shrinkage process (D-CUSP) prior. We provide an efficient implementation via a partially collapsed Gibbs sampler and extend our approach to allow flexible feature and outcome distributions. Prediction of time-to-labor onset from immunome, metabolome, and proteome data illustrates performance gains against state-of-the-art competitors. Our open-source software (R package) is available at https://github.com/niccoloanceschi/jafar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00778v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niccolo Anceschi, Federico Ferrari, David B. Dunson, Himel Mallick</dc:creator>
    </item>
    <item>
      <title>Gradual changes in functional time series</title>
      <link>https://arxiv.org/abs/2407.07996</link>
      <description>arXiv:2407.07996v2 Announce Type: replace-cross 
Abstract: We consider the problem of detecting gradual changes in the sequence of mean functions from a not necessarily stationary functional time series. Our approach is based on the maximum deviation (calculated over a given time interval) between a benchmark function and the mean functions at different time points. We speak of a gradual change of size $\Delta $, if this quantity exceeds a given threshold $\Delta&gt;0$. For example, the benchmark function could represent an average of yearly temperature curves from the pre-industrial time, and we are interested in the question if the yearly temperature curves afterwards deviate from the pre-industrial average by more than $\Delta =1.5$ degrees Celsius, where the deviations are measured with respect to the sup-norm. Using Gaussian approximations for high-dimensional data we develop a test for hypotheses of this type and estimators for the time where a deviation of size larger than $\Delta$ appears for the first time. We prove the validity of our approach and illustrate the new methods by a simulation study and a data example, where we analyze yearly temperature curves at different stations in Australia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07996v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Bastian, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Exploring the Potential Role of Generative AI in the TRAPD Procedure for Survey Translation</title>
      <link>https://arxiv.org/abs/2411.14472</link>
      <description>arXiv:2411.14472v2 Announce Type: replace-cross 
Abstract: This paper explores and assesses in what ways generative AI can assist in translating survey instruments. Writing effective survey questions is a challenging and complex task, made even more difficult for surveys that will be translated and deployed in multiple linguistic and cultural settings. Translation errors can be detrimental, with known errors rendering data unusable for its intended purpose and undetected errors leading to incorrect conclusions. A growing number of institutions face this problem as surveys deployed by private and academic organizations globalize, and the success of their current efforts depends heavily on researchers' and translators' expertise and the amount of time each party has to contribute to the task. Thus, multilinguistic and multicultural surveys produced by teams with limited expertise, budgets, or time are at significant risk for translation-based errors in their data. We implement a zero-shot prompt experiment using ChatGPT to explore generative AI's ability to identify features of questions that might be difficult to translate to a linguistic audience other than the source language. We find that ChatGPT can provide meaningful feedback on translation issues, including common source survey language, inconsistent conceptualization, sensitivity and formality issues, and nonexistent concepts. In addition, we provide detailed information on the practicality of the approach, including accessing the necessary software, associated costs, and computational run times. Lastly, based on our findings, we propose avenues for future research that integrate AI into survey translation practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14472v2</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erica Ann Metheney, Lauren Yehle</dc:creator>
    </item>
    <item>
      <title>Choosing the Right Norm for Change Point Detection in Functional Data</title>
      <link>https://arxiv.org/abs/2501.04476</link>
      <description>arXiv:2501.04476v2 Announce Type: replace-cross 
Abstract: We consider the problem of detecting a change point in a sequence of mean functions from a functional time series. We propose an $L^1$ norm based methodology and establish its theoretical validity both for classical and for relevant hypotheses. We compare the proposed method with currently available methodology that is based on the $L^2$ and supremum norms. Additionally we investigate the asymptotic behaviour under the alternative for all three methods and showcase both theoretically and empirically that the $L^1$ norm achieves the best performance in a broad range of scenarios. We also propose a power enhancement component that improves the performance of the $L^1$ test against sparse alternatives. Finally we apply the proposed methodology to both synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04476v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Bastian</dc:creator>
    </item>
  </channel>
</rss>
