<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Oct 2025 04:01:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A coupling-based approach to f-divergences diagnostics for Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2510.07559</link>
      <description>arXiv:2510.07559v1 Announce Type: new 
Abstract: A long-standing gap exists between the theoretical analysis of Markov chain Monte Carlo convergence, which is often based on statistical divergences, and the diagnostics used in practice. We introduce the first general convergence diagnostics for Markov chain Monte Carlo based on any f-divergence, allowing users to directly monitor, among others, the Kullback--Leibler and the $\chi^2$ divergences as well as the Hellinger and the total variation distances. Our first key contribution is a coupling-based `weight harmonization' scheme that produces a direct, computable, and consistent weighting of interacting Markov chains with respect to their target distribution. The second key contribution is to show how such consistent weightings of empirical measures can be used to provide upper bounds to f-divergences in general. We prove that these bounds are guaranteed to tighten over time and converge to zero as the chains approach stationarity, providing a concrete diagnostic. Numerical experiments demonstrate that our method is a practical and competitive diagnostic tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07559v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos, Hai-Dang Dau</dc:creator>
    </item>
    <item>
      <title>Rotated Mean-Field Variational Inference and Iterative Gaussianization</title>
      <link>https://arxiv.org/abs/2510.07732</link>
      <description>arXiv:2510.07732v1 Announce Type: new 
Abstract: We propose to perform mean-field variational inference (MFVI) in a rotated coordinate system that reduces correlations between variables. The rotation is determined by principal component analysis (PCA) of a cross-covariance matrix involving the target's score function. Compared with standard MFVI along the original axes, MFVI in this rotated system often yields substantially more accurate approximations with negligible additional cost.
  MFVI in a rotated coordinate system defines a rotation and a coordinatewise map that together move the target closer to Gaussian. Iterating this procedure yields a sequence of transformations that progressively transforms the target toward Gaussian. The resulting algorithm provides a computationally efficient way to construct flow-like transport maps: it requires only MFVI subproblems, avoids large-scale optimization, and yields transformations that are easy to invert and evaluate. In Bayesian inference tasks, we demonstrate that the proposed method achieves higher accuracy than standard MFVI, while maintaining much lower computational cost than conventional normalizing flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07732v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Chen, Sifan Liu</dc:creator>
    </item>
    <item>
      <title>Large-scale spatial variable gene atlas for spatial transcriptomics</title>
      <link>https://arxiv.org/abs/2510.07653</link>
      <description>arXiv:2510.07653v1 Announce Type: cross 
Abstract: Spatial variable genes (SVGs) reveal critical information about tissue architecture, cellular interactions, and disease microenvironments. As spatial transcriptomics (ST) technologies proliferate, accurately identifying SVGs across diverse platforms, tissue types, and disease contexts has become both a major opportunity and a significant computational challenge. Here, we present a comprehensive benchmarking study of 20 state-of-the-art SVG detection methods using human slides from STimage-1K4M, a large-scale resource of ST data comprising 662 slides from more than 18 tissue types. We evaluate each method across a range of biologically and technically meaningful criteria, including recovery of pathologist-annotated domain-specific markers, cross-slide reproducibility, scalability to high-resolution data, and robustness to technical variation. Our results reveal marked differences in performance depending on tissue type, spatial resolution, and study design. Beyond benchmarking, we construct the first cross-tissue atlas of SVGs, enabling comparative analysis of spatial gene programs across cancer and normal tissues. We observe similarities between pairs of tissues that reflect developmental and functional relationships, such as high overlap between thymus and lymph node, and uncover spatial gene programs associated with metastasis, immune infiltration, and tissue-of-origin identity in cancer. Together, our work defines a framework for evaluating and interpreting spatial gene expression and establishes a reference resource for the ST community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07653v1</guid>
      <category>stat.AP</category>
      <category>cs.DB</category>
      <category>q-bio.GN</category>
      <category>q-bio.TO</category>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawen Chen, Jinwei Zhang, Dongshen Peng, Yutong Song, Aitong Ruan, Yun Li, Didong Li</dc:creator>
    </item>
    <item>
      <title>Adjusted Random Effect Block Bootstraps for Highly Unbalanced Clustered Data</title>
      <link>https://arxiv.org/abs/2510.07770</link>
      <description>arXiv:2510.07770v1 Announce Type: cross 
Abstract: Clustered data arise naturally in many scientific and applied research settings where units are grouped within clusters. They are commonly analyzed using linear mixed models to account for within-cluster correlations. This article focuses on the scenario in which cluster sizes might be highly unbalanced and proposes a proportional random effect block bootstrap and a modified random effect block bootstrap, which are applicable in such cases and accommodate general distributions of random effects and error terms. These methods generalize the random effect block bootstrap, originally designed for the balanced case, and can be used for inference on parameters of linear mixed models or functions thereof. Both proposed bootstraps are shown to enjoy Fisher consistency under general cluster sizes, while the original random effect block bootstrap is consistent only for balanced clusters. Simulations demonstrate strong finite sample inferential performance of the proposed bootstraps relative to the random effect block bootstrap and other existing bootstrap methods for clustered data. Application to the Oman rainfall enhancement trial dataset, with cluster sizes ranging from 1 to 58, shows improved bootstrap confidence intervals using the proposed bootstraps over the random effect block bootstrap and a statistically significant effect of the ionization technology on rainfall.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07770v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Yang Tho, Raymond Chambers, A. H. Welsh</dc:creator>
    </item>
    <item>
      <title>Empirical evaluation of normalizing flows in Markov Chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2412.17136</link>
      <description>arXiv:2412.17136v2 Announce Type: replace-cross 
Abstract: Recent advances in MCMC use normalizing flows to precondition target distributions and enable jumps to distant regions. However, there is currently no systematic comparison of different normalizing flow architectures for MCMC. As such, many works choose simple flow architectures that are readily available and do not consider other models. Guidelines for choosing an appropriate architecture would reduce analysis time for practitioners and motivate researchers to take the recommended models as foundations to be improved. We provide the first such guideline by extensively evaluating many normalizing flow architectures on various flow-based MCMC methods and target distributions. When the target density gradient is available, we show that flow-based MCMC outperforms classic MCMC for suitable NF architecture choices with minor hyperparameter tuning. When the gradient is unavailable, flow-based MCMC wins with off-the-shelf architectures. We find contractive residual flows to be the best general-purpose models with relatively low sensitivity to hyperparameter choice. We also provide various insights into normalizing flow behavior within MCMC when varying their hyperparameters, properties of target distributions, and the overall computational budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17136v2</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Nabergoj, Erik \v{S}trumbelj</dc:creator>
    </item>
    <item>
      <title>Efficient Inference in First Passage Time Models</title>
      <link>https://arxiv.org/abs/2503.18381</link>
      <description>arXiv:2503.18381v2 Announce Type: replace-cross 
Abstract: First passage time models describe the time it takes for a random process to exit a region of interest and are widely used across various scientific fields. Fast and accurate numerical methods for computing the likelihood function in these models are essential for efficient statistical inference of model parameters. Specifically, in computational cognitive neuroscience, generalized drift diffusion models (GDDMs) are an important class of first passage time models that describe the latent psychological processes underlying simple decision-making scenarios. GDDMs model the joint distribution over choices and response times as the first hitting time of a one-dimensional stochastic differential equation (SDE) to possibly time-varying upper and lower boundaries. They are widely applied to extract parameters associated with distinct cognitive and neural mechanisms. However, current likelihood computation methods struggle in common application scenarios in which drift rates dynamically vary within trials as a function of exogenous covariates (e.g., brain activity in specific regions or visual fixations). In this work, we propose a fast and flexible algorithm for computing the likelihood function of GDDMs based on a large class of SDEs satisfying the Cherkasov condition. Our method divides each trial into discrete stages, employs fast analytical results to compute stage-wise densities, and integrates these to compute the overall trial-wise likelihood. Numerical examples demonstrate that our method not only yields accurate likelihood evaluations for efficient statistical inference, but also considerably outperforms existing approaches in terms of speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18381v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sicheng Liu, Alexander Fengler, Michael J. Frank, Matthew T. Harrison</dc:creator>
    </item>
  </channel>
</rss>
