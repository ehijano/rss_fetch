<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Jan 2026 05:00:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Reevaluating Causal Estimation Methods with Data from a Product Release</title>
      <link>https://arxiv.org/abs/2601.11845</link>
      <description>arXiv:2601.11845v1 Announce Type: new 
Abstract: Recent developments in causal machine learning methods have made it easier to estimate flexible relationships between confounders, treatments and outcomes, making unconfoundedness assumptions in causal analysis more palatable. How successful are these approaches in recovering ground truth baselines? In this paper we analyze a new data sample including an experimental rollout of a new feature at a large technology company and a simultaneous sample of users who endogenously opted into the feature. We find that recovering ground truth causal effects is feasible -- but only with careful modeling choices. Our results build on the observational causal literature beginning with LaLonde (1986), offering best practices for more credible treatment effect estimation in modern, high-dimensional datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11845v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Young, Muthoni Ngatia, Eleanor Wiske Dillon</dc:creator>
    </item>
    <item>
      <title>Public Education Spending and Income Inequality</title>
      <link>https://arxiv.org/abs/2601.11928</link>
      <description>arXiv:2601.11928v1 Announce Type: new 
Abstract: This paper investigates the relationship between public education spending and income inequality across U.S. counties from 2010 to 2022 using quantile regression methods. The analysis shows that total per pupil education spending is consistently associated with a small increase in income inequality, with stronger effects in high inequality counties. In contrast, the composition of education spending plays a substantially more important role. Reallocating budgets toward instructional, support service, and other current expenditures significantly reduces income inequality, particularly at the upper quantiles of the Gini distribution. Capital outlays and interest payments exhibit weaker and mixed effects. Economic and demographic factors, especially poverty, median income, and educational attainment, remain dominant drivers of inequality. Overall, the results demonstrate that how education funds are allocated matters more than how much is spent, underscoring the importance of budget composition in using public education policy to promote equity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11928v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ishmael Amartey</dc:creator>
    </item>
    <item>
      <title>Nonlinear Dynamic Factor Analysis With a Transformer Network</title>
      <link>https://arxiv.org/abs/2601.12039</link>
      <description>arXiv:2601.12039v1 Announce Type: new 
Abstract: The paper develops a Transformer architecture for estimating dynamic factors from multivariate time series data under flexible identification assumptions. Performance on small datasets is improved substantially by using a conventional factor model as prior information via a regularization term in the training objective. The results are interpreted with Attention matrices that quantify the relative importance of variables and their lags for the factor estimate. Time variation in Attention patterns can help detect regime switches and evaluate narratives. Monte Carlo experiments suggest that the Transformer is more accurate than the linear factor model, when the data deviate from linear-Gaussian assumptions. An empirical application uses the Transformer to construct a coincident index of U.S. real economic activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12039v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver Snellman</dc:creator>
    </item>
    <item>
      <title>A Robust Similarity Estimator</title>
      <link>https://arxiv.org/abs/2601.12198</link>
      <description>arXiv:2601.12198v1 Announce Type: new 
Abstract: We construct and analyze an estimator of association between random variables based on their similarity in both direction and magnitude. Under special conditions, the proposed measure becomes a robust and consistent estimator of the linear correlation, for which an exact sampling distribution is available. This distribution is intrinsically insensitive to heavy tails and outliers, thereby facilitating robust inference for correlations. The measure can be naturally extended to higher dimensions, where it admits an interpretation as an indicator of joint similarity among multiple random variables. We investigate the empirical performance of the proposed measure with financial return data at both high and low frequencies. Specifically, we apply the new estimator to construct confidence intervals for correlations based on intraday returns and to develop a new specification for multivariate GARCH models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12198v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilya Archakov</dc:creator>
    </item>
    <item>
      <title>How Well Do LLMs Predict Human Behavior? A Measure of their Pretrained Knowledge</title>
      <link>https://arxiv.org/abs/2601.12343</link>
      <description>arXiv:2601.12343v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly used to predict human behavior. We propose a measure for evaluating how much knowledge a pretrained LLM brings to such a prediction: its equivalent sample size, defined as the amount of task-specific data needed to match the predictive accuracy of the LLM. We estimate this measure by comparing the prediction error of a fixed LLM in a given domain to that of flexible machine learning models trained on increasing samples of domain-specific data. We further provide a statistical inference procedure by developing a new asymptotic theory for cross-validated prediction error. Finally, we apply this method to the Panel Study of Income Dynamics. We find that LLMs encode considerable predictive information for some economic variables but much less for others, suggesting that their value as substitutes for domain-specific data differs markedly across settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12343v1</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wayne Gao, Sukjin Han, Annie Liang</dc:creator>
    </item>
    <item>
      <title>Partial Identification under Stratified Randomization</title>
      <link>https://arxiv.org/abs/2601.12566</link>
      <description>arXiv:2601.12566v1 Announce Type: new 
Abstract: This paper develops a unified framework for partial identification and inference in stratified experiments with attrition, accommodating both equal and heterogeneous treatment shares across strata. For equal-share designs, we apply recent theory for finely stratified experiments to Lee bounds, yielding closed-form, design-consistent variance estimators and properly sized confidence intervals. Simulations show that the conventional formula can overstate uncertainty, while our approach delivers tighter intervals. When treatment shares differ across strata, we propose a new strategy, which combines inverse probability weighting and global trimming to construct valid bounds even when strata are small or unbalanced. We establish identification, introduce a moment estimator, and extend existing inference results to stratified designs with heterogeneous shares, covering a broad class of moment-based estimators which includes the one we formulate. We also generalize our results to designs in which strata are defined solely by observed labels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12566v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bruno Ferman, Davi Siqueira, Vitor Possebom</dc:creator>
    </item>
    <item>
      <title>Quantitative Methods in Finance</title>
      <link>https://arxiv.org/abs/2601.12896</link>
      <description>arXiv:2601.12896v1 Announce Type: new 
Abstract: These lecture notes provide a comprehensive introduction to Quantitative Methods in Finance (QMF), designed for graduate students in finance and economics with heterogeneous programming backgrounds. The material develops a unified toolkit combining probability theory, statistics, numerical methods, and empirical modeling, with a strong emphasis on implementation in Python. Core topics include random variables and distributions, moments and dependence, simulation and Monte Carlo methods, numerical optimization, root-finding, and time-series models commonly used in finance and macro-finance. Particular attention is paid to translating theoretical concepts into reproducible code, emphasizing vectorization, numerical stability, and interpretation of outputs. The notes progressively bridge theory and practice through worked examples and exercises covering asset pricing intuition, risk measurement, forecasting, and empirical analysis. By focusing on clarity, minimal prerequisites, and hands-on computation, these lecture notes aim to serve both as a pedagogical entry point for non-programmers and as a practical reference for applied researchers seeking transparent and replicable quantitative methods in finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12896v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Vansteenberghe</dc:creator>
    </item>
    <item>
      <title>Realised quantile-based estimation of the integrated variance</title>
      <link>https://arxiv.org/abs/2601.13006</link>
      <description>arXiv:2601.13006v1 Announce Type: new 
Abstract: In this paper, we propose a new jump robust quantile-based realised variance measure of ex-post return variation that can be computed using potentially noisy data. The estimator is consistent for the integrated variance and we present feasible central limit theorems which show that it converges at the best attainable rate and has excellent efficiency. Asymptotically, the quantile-based realised variance is immune to finite activity jumps and outliers in the price series, while in modified form the estimator is applicable with market microstructure noise and therefore operational on high-frequency data. Simulations show that it has superior robustness properties in finite sample, while an empirical application illustrates its use on equity data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13006v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jeconom.2010.04.008</arxiv:DOI>
      <dc:creator>Kim Christensen, Roel Oomen, Mark Podolskij</dc:creator>
    </item>
    <item>
      <title>A machine learning approach to volatility forecasting</title>
      <link>https://arxiv.org/abs/2601.13014</link>
      <description>arXiv:2601.13014v1 Announce Type: new 
Abstract: We inspect how accurate machine learning (ML) is at forecasting realized variance of the Dow Jones Industrial Average index constituents. We compare several ML algorithms, including regularization, regression trees, and neural networks, to multiple Heterogeneous AutoRegressive (HAR) models. ML is implemented with minimal hyperparameter tuning. In spite of this, ML is competitive and beats the HAR lineage, even when the only predictors are the daily, weekly, and monthly lags of realized variance. The forecast gains are more pronounced at longer horizons. We attribute this to higher persistence in the ML models, which helps to approximate the long-memory of realized variance. ML also excels at locating incremental information about future volatility from additional predictors. Lastly, we propose a ML measure of variable importance based on accumulated local effects. This shows that while there is agreement about the most important predictors, there is disagreement on their ranking, helping to reconcile our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13014v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/jjfinec/nbac032</arxiv:DOI>
      <dc:creator>Kim Christensen, Mathias Siggaard, Bezirgen Veliyev</dc:creator>
    </item>
    <item>
      <title>Spectral Dynamics and Regularization for High-Dimensional Copulas</title>
      <link>https://arxiv.org/abs/2601.13281</link>
      <description>arXiv:2601.13281v1 Announce Type: new 
Abstract: We introduce a novel model for time-varying, asymmetric, tail-dependent copulas in high dimensions that incorporates both spectral dynamics and regularization. The dynamics of the dependence matrix' eigenvalues are modeled in a score-driven way, while biases in the unconditional eigenvalue spectrum are resolved by non-linear shrinkage. The dynamic parameterization of the copula dependence matrix ensures that it satisfies the appropriate restrictions at all times and for any dimension. The model is parsimonious, computationally efficient, easily scalable to high dimensions, and performs well for both simulated and empirical data. In an empirical application to financial market dynamics using 100 stocks from 10 different countries and 10 different industry sectors, we find that our copula model captures both geographic and industry related co-movements and outperforms recent computationally more intensive clustering-based factor copula alternatives. Both the spectral dynamics and the regularization contribute to the new model's performance. During periods of market stress, we find that the spectral dynamics reveal strong increases in international stock market dependence, which causes reductions in diversification potential and increases in systemic risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13281v1</guid>
      <category>econ.EM</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koos B. Gubbels, Andre Lucas</dc:creator>
    </item>
    <item>
      <title>The Collapse of Multilayer Predation and the Emergence of a Monolithic Leviathan</title>
      <link>https://arxiv.org/abs/2601.13544</link>
      <description>arXiv:2601.13544v1 Announce Type: cross 
Abstract: This paper constructs a multilayer recursive game model to demonstrate that in a rule vacuum environment, hierarchical predatory structures inevitably collapse into a monolithic political strongman system due to the conflict between exponentially growing rent dissipation and the rigidity of bottom-level survival constraints. We propose that the rise of a monolithic political strongman is essentially an "algorithmic entropy reduction" achieved through forceful means by the system to counteract the "informational entropy increase" generated by multilayer agency. However, the order gained at the expense of social complexity results in the stagnation of social evolutionary functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13544v1</guid>
      <category>physics.soc-ph</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>On the Anchoring Effect of Monetary Policy on the Labor Share of Income and the Rationality of Its Setting Mechanism</title>
      <link>https://arxiv.org/abs/2601.13675</link>
      <description>arXiv:2601.13675v1 Announce Type: cross 
Abstract: Modern macroeconomic monetary theory suggests that the labor share of income has effectively become a core macroe-conomic parameter anchored by top policymakers through Open Market Operations (OMO). However, the setting of this parameter remains a subject of intense economic debate. This paper provides a detailed summary of these controversies, analyzes the scope of influence exerted by market agents other than the top policymakers on the labor share, and explores the rationality of its setting mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13675v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>Policy Learning under Endogeneity Using Instrumental Variables</title>
      <link>https://arxiv.org/abs/2206.09883</link>
      <description>arXiv:2206.09883v4 Announce Type: replace 
Abstract: I propose a framework for learning individualized policy rules in observational data settings characterized by endogenous treatment selection and the availability of an instrumental variable. I introduce encouragement rules that manipulate the instrument. By incorporating the marginal treatment effect (MTE) as a policy invariant parameter, I establish the identification of the social welfare criterion for the optimal encouragement rule. Focusing on binary encouragement rules, I propose to estimate the optimal encouragement rule via the Empirical Welfare Maximization (EWM) method and derive the welfare loss convergence rate. I apply my method to advise on the optimal tuition subsidy assignment in Indonesia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.09883v4</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Liu</dc:creator>
    </item>
    <item>
      <title>Identification in Multiple Treatment Models under Discrete Variation</title>
      <link>https://arxiv.org/abs/2307.06174</link>
      <description>arXiv:2307.06174v2 Announce Type: replace 
Abstract: We develop a marginal treatment effect based method to learn about causal effects in multiple treatment models with discrete instruments. We allow selection into treatment to be governed by a general class of threshold crossing models that permit multidimensional unobserved heterogeneity. An inherent complication is that the primitives characterizing the selection model are not generally point-identified. Allowing these primitives to be point-identified up to a finite-dimensional parameter, we show how a two-step computational program can be used to obtain sharp bounds for a number of treatment effect parameters when the marginal treatment response functions are allowed to satisfy only nonparametric shape restrictions or are additionally parameterized. We demonstrate the benefits of our method by revisiting Kline and Walters' (2016) empirical analysis of the Head Start program. Our approach relaxes their point-identifying assumptions on the selection model and marginal treatment response functions, allowing us to assess the robustness of their conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.06174v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vishal Kamat, Samuel Norris, Matthew Pecenco</dc:creator>
    </item>
    <item>
      <title>Interpreting Event-Studies from Recent Difference-in-Differences Methods</title>
      <link>https://arxiv.org/abs/2401.12309</link>
      <description>arXiv:2401.12309v2 Announce Type: replace 
Abstract: This note discusses the interpretation of event-study plots produced by recent difference-in-differences methods. I show that even when specialized to the case of non-staggered treatment timing, the default plots produced by software for several of the most popular recent methods do not match those of traditional two-way fixed effects (TWFE) event-studies. The plots produced by the new methods may show a kink or jump at the time of treatment even when the TWFE event-study shows a straight line. This difference stems from the fact that the new methods construct the pre-treatment coefficients asymmetrically from the post-treatment coefficients. As a result, visual heuristics for evaluating violations of parallel trends using TWFE event-study plots should not be immediately applied to those from these methods. I conclude with practical recommendations for constructing and interpreting event-study plots when using these methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12309v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Roth</dc:creator>
    </item>
    <item>
      <title>To be or not to be: Roughness or long memory in volatility?</title>
      <link>https://arxiv.org/abs/2403.12653</link>
      <description>arXiv:2403.12653v2 Announce Type: replace 
Abstract: We develop a framework for composite likelihood estimation of parametric continuous-time stationary Gaussian processes. We derive the asymptotic theory of the associated maximum composite likelihood estimator. We implement our approach on a pair of models that have been proposed to describe the random log-spot variance of financial asset returns. A simulation study shows that it delivers good performance in these settings and improves upon a method-of-moments estimation. In an empirical investigation, we inspect the dynamic of an intraday measure of the spot log-realized variance computed with high-frequency data from the cryptocurrency market. The evidence supports a mechanism, where the short- and long-term correlation structure of stochastic volatility are decoupled in order to capture its properties at different time scales. This is further backed by an analysis of the associated spot log-trading volume.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12653v2</guid>
      <category>econ.EM</category>
      <category>q-fin.MF</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikkel Bennedsen, Kim Christensen, Peter Christensen</dc:creator>
    </item>
    <item>
      <title>Potential weights and implicit causal designs in linear regression</title>
      <link>https://arxiv.org/abs/2407.21119</link>
      <description>arXiv:2407.21119v4 Announce Type: replace 
Abstract: When we interpret linear regression as estimating causal effects justified by quasi-experimental treatment variation, what do we mean? This paper formalizes a minimal criterion for quasi-experimental interpretation and characterizes its necessary implications. A minimal requirement is that the regression always estimates some contrast of potential outcomes under the true treatment assignment process. This requirement implies linear restrictions on the true distribution of treatment. If the regression were to be interpreted quasi-experimentally, these restrictions imply candidates for the true distribution of treatment, which we call implicit designs. Regression estimators are numerically equivalent to augmented inverse propensity weighting (AIPW) estimators using an implicit design. Implicit designs serve as a framework that unifies and extends existing theoretical results on causal interpretation of regression across starkly distinct settings (including multiple treatment, panel, and instrumental variables). They lead to new theoretical insights for widely used but less understood specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21119v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Revisiting the Identification of the Conduct Parameter in Homogeneous Goods Markets</title>
      <link>https://arxiv.org/abs/2410.16998</link>
      <description>arXiv:2410.16998v3 Announce Type: replace 
Abstract: We revisit the identification of the conduct parameter in homogeneous goods markets. Lau (1982) argues that the conduct parameter is not identified if and only if the inverse demand function is separable, except for a specific separable function. This result has been regarded as an extension of the result in Bresnahan (1982) to more general settings. However, we show that Lau's claim is incorrect and provide a new characterization of the non-identification. Our characterization shows that a demand function with demand rotation instruments is the necessary and sufficient condition for the identification of the conduct parameter. Therefore, our result properly generalizes the role of demand rotation instruments in identifying the conduct parameter, as highlighted by Bresnahan (1982), to more general settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16998v3</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuri Matsumura, Suguru Otani</dc:creator>
    </item>
    <item>
      <title>Kotlarski's lemma for dyadic models</title>
      <link>https://arxiv.org/abs/2502.02734</link>
      <description>arXiv:2502.02734v2 Announce Type: replace 
Abstract: We show how to identify the distributions of the latent components in the two-way dyadic model for bipartite networks $y_{i,\ell}= \alpha_i+\eta_{\ell}+\varepsilon_{i,\ell}$. This is achieved by a repeated application of the extension of the classical lemma of Kotlarski (1967) in Evdokimov and White (2012). We provide two separate sets of assumptions under which all the latent distributions are identified. Both rely on some of the latent components being identically distributed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02734v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigory Franguridi, Hyungsik Roger Moon</dc:creator>
    </item>
    <item>
      <title>Policy Learning with Confidence</title>
      <link>https://arxiv.org/abs/2502.10653</link>
      <description>arXiv:2502.10653v3 Announce Type: replace 
Abstract: This paper introduces a rule for policy selection in the presence of estimation uncertainty, explicitly accounting for estimation risk. The rule belongs to the class of risk-aware rules on the efficient decision frontier, characterized as policies offering maximal estimated welfare for a given level of estimation risk. Among this class, the proposed rule is chosen to provide a reporting guarantee, ensuring that the welfare delivered exceeds a threshold with a pre-specified confidence level. We apply this approach to the allocation of a limited budget among social programs using estimates of their marginal value of public funds and associated standard errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10653v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Sokbae Lee, Adam M. Rosen, Liyang Sun</dc:creator>
    </item>
    <item>
      <title>Probabilistic Forecasting of Climate Policy Uncertainty: The Role of Macro-financial Variables and Google Search Data</title>
      <link>https://arxiv.org/abs/2507.12276</link>
      <description>arXiv:2507.12276v3 Announce Type: replace 
Abstract: Accurately forecasting Climate Policy Uncertainty (CPU) is essential for designing climate strategies that balance economic growth with environmental objectives. Elevated CPU levels can delay regulatory implementation, hinder investment in green technologies, and amplify public resistance to policy reforms, particularly during periods of economic stress. Despite the growing literature documenting the economic relevance of CPU, forecasting its evolution and understanding the role of macro-financial drivers in shaping its fluctuations have not been explored. This study addresses this gap by presenting the first effort to forecast CPU and identify its key drivers. We employ various statistical tools to identify macro-financial exogenous drivers, alongside Google search data to capture early public attention to climate policy. Local projection impulse response analysis quantifies the dynamic effects of these variables, revealing that household financial vulnerability, housing market activity, business confidence, credit conditions, and financial market sentiment exert the most substantial impacts. These predictors are incorporated into a Bayesian Structural Time Series (BSTS) framework to produce probabilistic forecasts for both US and Global CPU indices. Extensive experiments and statistical validation demonstrate that BSTS with time-invariant regression coefficients achieves superior forecasting performance. We demonstrate that this performance stems from its variable selection mechanism, which identifies exogenous predictors that are empirically significant and theoretically grounded, as confirmed by the feature importance analysis. From a policy perspective, the findings underscore the importance of adaptive climate policies that remain effective across shifting economic conditions while supporting long-term environmental and growth objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12276v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Donia Besher, Anirban Sengupta, Tanujit Chakraborty</dc:creator>
    </item>
    <item>
      <title>Making Event Study Plots Honest: A Functional Data Approach to Causal Inference</title>
      <link>https://arxiv.org/abs/2512.06804</link>
      <description>arXiv:2512.06804v2 Announce Type: replace 
Abstract: Event study plots are the centerpiece of Difference-in-Differences (DiD) analysis, but current plotting methods cannot provide honest causal inference when the parallel trends and/or no-anticipation assumption fails. We introduce a novel functional data approach to DiD that directly enables honest causal inference via event study plots. Our DiD estimator converges to a Gaussian process in the Banach space of continuous functions, enabling powerful simultaneous confidence bands. This theoretical contribution allows us to turn an event study plot into a rigorous honest causal inference tool through equivalence and relevance testing: Honest reference bands can be validated using equivalence testing in the pre-treatment period, and honest causal effects can be tested using relevance testing in the post-treatment period. We demonstrate the performance of our method in simulations and two case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.06804v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chencheng Fang, Dominik Liebl</dc:creator>
    </item>
    <item>
      <title>From Many Models, One: Macroeconomic Forecasting with Reservoir Ensembles</title>
      <link>https://arxiv.org/abs/2512.13642</link>
      <description>arXiv:2512.13642v2 Announce Type: replace 
Abstract: Model combination is a powerful approach for achieving superior performance compared to selecting a single model. We study both theoretically and empirically the effectiveness of ensembles of Multi-Frequency Echo State Networks (MFESNs), which have been shown to achieve state-of-the-art macroeconomic time series forecasting results (Ballarin et al., 2024a). The Hedge and Follow-the-Leader schemes are discussed, and their online learning guarantees are extended to settings with dependent data. In empirical applications, the proposed Ensemble Echo State Networks demonstrate significantly improved predictive performance relative to individual MFESN models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13642v2</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni Ballarin, Lyudmila Grigoryeva, Yui Ching Li</dc:creator>
    </item>
    <item>
      <title>The Promise of Time-Series Foundation Models for Agricultural Forecasting: Evidence from Commodity Prices</title>
      <link>https://arxiv.org/abs/2601.06371</link>
      <description>arXiv:2601.06371v2 Announce Type: replace 
Abstract: Forecasting agricultural markets remains challenging due to nonlinear dynamics, structural breaks, and sparse data. A long-standing belief holds that simple time-series methods outperform more advanced alternatives. This paper provides the first systematic evidence that this belief no longer holds with modern time-series foundation models (TSFMs). Using USDA ERS monthly commodity price data from 1997-2025, we evaluate 17 forecasting approaches across four model classes, including traditional time-series, machine learning, deep learning, and five state-of-the-art TSFMs (Chronos, Chronos-2, TimesFM 2.5, Time-MoE, Moirai-2), and construct annual marketing year price predictions to compare with USDA's futures-based season-average price (SAP) forecasts. We show that zero-shot foundation models consistently outperform traditional time-series methods, machine learning, and deep learning architectures trained from scratch in both monthly and annual forecasting. Furthermore, foundation models remarkably outperform USDA's futures-based forecasts on three of four major commodities despite USDA's information advantage from forward-looking futures markets. Time-MoE delivers the largest accuracy gains, achieving 54.9% improvement on wheat and 18.5% improvement on corn relative to USDA ERS benchmarks on recent data (2017-2024 excluding COVID). These results point to a paradigm shift in agricultural forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06371v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Le Wang, Boyuan Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal Conditional Inference in Adaptive Experiments</title>
      <link>https://arxiv.org/abs/2309.12162</link>
      <description>arXiv:2309.12162v2 Announce Type: replace-cross 
Abstract: We study batched bandit experiments and consider the problem of inference conditional on the realized stopping time, assignment probabilities, and target parameter, where all of these may be chosen adaptively using information up to the last batch of the experiment. Absent further restrictions on the experiment, we show that inference using only the results of the last batch is optimal. When the adaptive aspects of the experiment are known to be location-invariant, in the sense that they are unchanged when we shift all batch-arm means by a constant, we show that there is additional information in the data, captured by one additional linear function of the batch-arm means. In the more restrictive case where the stopping time, assignment probabilities, and target parameter are known to depend on the data only through a collection of polyhedral events, we derive computationally tractable and optimal conditional inference procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12162v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen, Isaiah Andrews</dc:creator>
    </item>
    <item>
      <title>Assessing Utility of Differential Privacy for RCTs</title>
      <link>https://arxiv.org/abs/2309.14581</link>
      <description>arXiv:2309.14581v2 Announce Type: replace-cross 
Abstract: Randomized controlled trials (RCTs) have become powerful tools for assessing the impact of interventions and policies in many contexts. They are considered the gold standard for causal inference in the biomedical fields and many social sciences. Researchers have published an increasing number of studies that rely on RCTs for at least part of their inference. These studies typically include the response data that has been collected, de-identified, and sometimes protected through traditional disclosure limitation methods. In this paper, we empirically assess the impact of privacy-preserving synthetic data generation methodologies on published RCT analyses by leveraging available replication packages (research compendia) in economics and policy analysis. We implement three privacy-preserving algorithms, that use as a base one of the basic differentially private (DP) algorithms, the perturbed histogram, to support the quality of statistical inference. We highlight challenges with the straight use of this algorithm and the stability-based histogram in our setting and described the adjustments needed. We provide simulation studies and demonstrate that we can replicate the analysis in a published economics article on privacy-protected data under various parameterizations. We find that relatively straightforward (at a high-level) privacy-preserving methods influenced by DP techniques allow for inference-valid protection of published data. The results have applicability to researchers wishing to share RCT data, especially in the context of low- and middle-income countries, with strong privacy protection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14581v2</guid>
      <category>stat.AP</category>
      <category>cs.CR</category>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaitlyn R. Webb, Soumya Mukherjee, Aratrika Mustafi, Aleksandra Slavkovi\'c, Lars Vilhuber</dc:creator>
    </item>
    <item>
      <title>Sectorial Exclusion Criteria in the Marxist Analysis of the Average Rate of Profit: The United States Case (1960-2020)</title>
      <link>https://arxiv.org/abs/2501.06270</link>
      <description>arXiv:2501.06270v2 Announce Type: replace-cross 
Abstract: The long term estimation of the Marxist average rate of profit does not adhere to a theoretically grounded standard regarding which economic activities should or should not be included for such purposes, which is relevant because methodological non uniformity can be a significant source of overestimation or underestimation, generating a less accurate reflection of the capital accumulation dynamics. This research aims to provide a standard Marxist decision criterion regarding the inclusion and exclusion of economic activities for the calculation of the Marxist average profit rate for the case of United States economic sectors from 1960 to 2020, based on the Marxist definition of productive labor, its location in the circuit of capital, and its relationship with the production of surplus value. Using wavelet transformed Daubechies filters with increased symmetry, empirical mode decomposition, Hodrick Prescott filter embedded in unobserved components model, and a wide variety of unit root tests the internal theoretical consistency of the presented criteria is evaluated. Also, the objective consistency of the theory is evaluated by a dynamic factor autoregressive model, Principal Component Analysis via Singular Value Decomposition, and regularized Horseshoe regression. The results are consistent both theoretically and econometrically with the logic of Classical Marxist political economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06270v2</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Mauricio Gomez Julian</dc:creator>
    </item>
    <item>
      <title>Semiparametric Off-Policy Inference for Optimal Policy Values under Possible Non-Uniqueness</title>
      <link>https://arxiv.org/abs/2505.13809</link>
      <description>arXiv:2505.13809v4 Announce Type: replace-cross 
Abstract: Off-policy evaluation (OPE) constructs confidence intervals for the value of a target policy using data generated under a different behavior policy. Most existing inference methods focus on fixed target policies and may fail when the target policy is estimated as optimal, particularly when the optimal policy is non-unique or nearly deterministic.
  We study inference for the value of optimal policies in Markov decision processes. We characterize the existence of the efficient influence function and show that non-regularity arises under policy non-uniqueness. Motivated by this analysis, we propose a novel \textit{N}onparametric \textit{S}equenti\textit{A}l \textit{V}alue \textit{E}valuation (NSAVE) method, which achieves semiparametric efficiency and retains the double robustness property when the optimal policy is unique, and remains stable in degenerate regimes beyond the scope of existing asymptotic theory. We further develop a smoothing-based approach for valid inference under non-unique optimal policies, and a post-selection procedure with uniform coverage for data-selected optimal policies.
  Simulation studies support the theoretical results. An application to the OhioT1DM mobile health dataset provides patient-specific confidence intervals for optimal policy values and their improvement over observed treatment policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13809v4</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyu Wei</dc:creator>
    </item>
    <item>
      <title>Bias correction for Chatterjee's graph-based correlation coefficient</title>
      <link>https://arxiv.org/abs/2508.09040</link>
      <description>arXiv:2508.09040v2 Announce Type: replace-cross 
Abstract: Azadkia and Chatterjee (2021) recently introduced a simple nearest neighbor (NN) graph-based correlation coefficient that consistently detects both independence and functional dependence. Specifically, it approximates a measure of dependence that equals 0 if and only if the variables are independent, and 1 if and only if they are functionally dependent. However, this NN estimator includes a bias term that may vanish at a rate slower than root-$n$, preventing root-$n$ consistency in general. In this article, we (i) analyze this bias term closely and show that it could become asymptotically negligible when the dimension is smaller than four; and (ii) propose a bias-correction procedure for more general settings. In both regimes, we obtain estimators (either the original or the bias-corrected version) that are root-$n$ consistent and asymptotically normal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09040v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mona Azadkia, Leihao Chen, Fang Han</dc:creator>
    </item>
    <item>
      <title>In Defense of the Pre-Test: Valid Inference when Testing Violations of Parallel Trends for Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2510.26470</link>
      <description>arXiv:2510.26470v2 Announce Type: replace-cross 
Abstract: The difference-in-differences (DID) research design is a key identification strategy which allows researchers to estimate causal effects under the parallel trends assumption. While the parallel trends assumption is counterfactual and cannot be tested directly, researchers often examine pre-treatment periods to check whether the time trends are parallel before treatment is administered. Recently, researchers have been cautioned against using preliminary tests which aim to detect violations of parallel trends in the pre-treatment period. In this paper, we argue that preliminary testing can -- and should -- play an important role within the DID research design. We propose a new and more substantively appropriate conditional extrapolation assumption, which requires an analyst to conduct a preliminary test to determine whether the severity of pre treatment parallel trend violations falls below an acceptable level before extrapolation to the post-treatment period is justified. This stands in contrast to prior work which can be interpreted as either setting the acceptable level to be exactly zero (in which case preliminary tests lack power) or assuming that extrapolation is always justified (in which case preliminary tests are not required). Under mild assumptions on how close the actual violation is to the acceptable level, we provide a consistent preliminary test as well confidence intervals which are valid when conditioned on the result of the test. The conditional coverage of these intervals overcomes a common critique made against the use of preliminary testing within the DID research design. To illustrate the performance of the proposed methods, we use synthetic data as well as data on recentralization of public services in Vietnam and right-to-carry laws in Virginia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26470v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 21 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas M. Mikhaeil, Christopher Harshaw</dc:creator>
    </item>
  </channel>
</rss>
