<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Oct 2024 04:00:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>User Experience Evaluation of AR Assisted Industrial Maintenance and Support Applications</title>
      <link>https://arxiv.org/abs/2410.17348</link>
      <description>arXiv:2410.17348v1 Announce Type: new 
Abstract: The paper introduces an innovative approach to industrial maintenance leveraging augmented reality (AR) technology, focusing on enhancing the user experience and efficiency. The shift from traditional to proactive maintenance strategies underscores the significance of maintenance in industrial systems. The proposed solution integrates AR interfaces, particularly through Head-Mounted Display (HMD) devices, to provide expert personnel-aided decision support for maintenance technicians, with the association of Artificial Intelligence (AI) solutions. The study explores the user experience aspect of AR interfaces in a simulated industrial environment, aiming to improve the maintenance processes' intuitiveness and effectiveness. Evaluation metrics such as the NASA Task Load Index (NASA-TLX) and the System Usability Scale (SUS) are employed to assess the usability, performance, and workload implications of the AR maintenance system. Additionally, the paper discusses the technical implementation, methodology, and results of experiments conducted to evaluate the effectiveness of the proposed solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17348v1</guid>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akos Nagy, Yannis Spyridis, Gregory J Mills, Vasileios Argyriou</dc:creator>
    </item>
    <item>
      <title>Harnessing Visualization for Climate Action and Sustainable Future</title>
      <link>https://arxiv.org/abs/2410.17411</link>
      <description>arXiv:2410.17411v1 Announce Type: new 
Abstract: The urgency of climate change is now recognized globally. As humanity confronts the critical need to mitigate climate change and foster sustainability, data visualization emerges as a powerful tool with a unique capacity to communicate insights crucial for understanding environmental complexities. This paper explores the critical need for designing and investigating responsible data visualization that can act as a catalyst for engaging communities within global climate action and sustainability efforts. Grounded in prior work and reflecting on a decade of community engagement research, I propose five critical considerations: (1) inclusive and accessible visualizations for enhancing climate education and communication, (2) interactive visualizations for fostering agency and deepening engagement,
  (3) in-situ visualizations for reducing spatial indirection,
  (4) shared immersive experiences for catalyzing collective action, and (5) accurate, transparent, and credible visualizations for ensuring trust and integrity. These considerations offer strategies and new directions for visualization research, aiming to enhance community engagement, deepen involvement, and foster collective action on critical socio-technical including and beyond climate change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17411v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Narges Mahyar</dc:creator>
    </item>
    <item>
      <title>AdaptoML-UX: An Adaptive User-centered GUI-based AutoML Toolkit for Non-AI Experts and HCI Researchers</title>
      <link>https://arxiv.org/abs/2410.17469</link>
      <description>arXiv:2410.17469v1 Announce Type: new 
Abstract: The increasing integration of machine learning across various domains has underscored the necessity for accessible systems that non-experts can utilize effectively. To address this need, the field of automated machine learning (AutoML) has developed tools to simplify the construction and optimization of ML pipelines. However, existing AutoML solutions often lack efficiency in creating online pipelines and ease of use for Human-Computer Interaction (HCI) applications. Therefore, in this paper, we introduce AdaptoML-UX, an adaptive framework that incorporates automated feature engineering, machine learning, and incremental learning to assist non-AI experts in developing robust, user-centered ML models. Our toolkit demonstrates the capability to adapt efficiently to diverse problem domains and datasets, particularly in HCI, thereby reducing the necessity for manual experimentation and conserving time and resources. Furthermore, it supports model personalization through incremental learning, customizing models to individual user behaviors. HCI researchers can employ AdaptoML-UX (\url{https://github.com/MichaelSargious/AdaptoML_UX}) without requiring specialized expertise, as it automates the selection of algorithms, feature engineering, and hyperparameter tuning based on the unique characteristics of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17469v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amr Gomaa, Michael Sargious, Antonio Kr\"uger</dc:creator>
    </item>
    <item>
      <title>Efficient and Aesthetic UI Design with a Deep Learning-Based Interface Generation Tree Algorithm</title>
      <link>https://arxiv.org/abs/2410.17586</link>
      <description>arXiv:2410.17586v1 Announce Type: new 
Abstract: This paper presents a novel method for user interface (UI) generation based on the Transformer architecture, addressing the increasing demand for efficient and aesthetically pleasing UI designs in software development. Traditional UI design relies heavily on designers' expertise, which can be time-consuming and costly. Leveraging the capabilities of Transformers, particularly their ability to capture complex design patterns and long-range dependencies, we propose a Transformer-based interface generation tree algorithm. This method constructs a hierarchical representation of UI components as nodes in a tree structure, utilizing pre-trained Transformer models for encoding and decoding. We define a markup language to describe UI components and their properties and use a rich dataset of real-world web and mobile application interfaces for training. The experimental results demonstrate that our approach not only significantly enhances design quality and efficiency but also outperforms traditional models in user satisfaction and aesthetic appeal. We also provide a comparative analysis with existing models, illustrating the advantages of our method in terms of accuracy, user ratings, and design similarity. Overall, our study underscores the potential of the Transformer based approach to revolutionize the UI design process, making it accessible for non-professionals while maintaining high standards of quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17586v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiyu Duan, Runsheng Zhang, Mengmeng Chen, Ziyi Wang, Shixiao Wang</dc:creator>
    </item>
    <item>
      <title>AI as a Bridge Across Ages: Exploring The Opportunities of Artificial Intelligence in Supporting Inter-Generational Communication in Virtual Reality</title>
      <link>https://arxiv.org/abs/2410.17909</link>
      <description>arXiv:2410.17909v1 Announce Type: new 
Abstract: Inter-generational communication is essential for bridging generational gaps and fostering mutual understanding. However, maintaining it is complex due to cultural, communicative, and geographical differences. Recent research indicated that while Virtual Reality (VR) creates a relaxed atmosphere and promotes companionship, it inadequately addresses the complexities of inter-generational dialogue, including variations in values and relational dynamics. To address this gap, we explored the opportunities of Artificial Intelligence (AI) in supporting inter-generational communication in VR. We developed three technology probes (e.g., Content Generator, Communication Facilitator, and Info Assistant) in VR and employed them in a probe-based participatory design study with twelve inter-generational pairs. Our results show that AI-powered VR facilitates inter-generational communication by enhancing mutual understanding, fostering conversation fluency, and promoting active participation. We also introduce several challenges when using AI-powered VR in supporting inter-generational communication and derive design implications for future VR platforms, aiming to improve inter-generational communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17909v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiuxin Du, Xiaoying Wei, Jiawei Li, Emily Kuang, Jie Hao, Dongdong Weng, Mingming Fan</dc:creator>
    </item>
    <item>
      <title>A Comparative Assessment of Technology Acceptance and Learning Outcomes in Computer-based versus VR-based Pedagogical Agents</title>
      <link>https://arxiv.org/abs/2410.18048</link>
      <description>arXiv:2410.18048v1 Announce Type: new 
Abstract: As educational technology evolves, the potential of Pedagogical Agents (PAs) in supporting education is extensively explored. Typically, research on PAs has primarily focused on computer-based learning environments, but their use in VR-based environments and integration into education is still in its infancy. To address this gap, this paper presents a mixed method comparative study that has been conducted to evaluate and examine how these computer-based PAs and VR-based PAs compare, towards their learning efficacy and technology acceptance. 92 Computing and Engineering undergraduate students were recruited and participated in an educational experience focusing on computing machinery education. The findings of this study revealed that both approaches can effectively facilitate learning acquisition, and both technologies have been positively perceived by participants toward acceptance, without any significant differences. The findings of this study shed light on the potential of utilizing intelligent PAs to support education, contributing towards the advancement of our understanding of how to integrate such technologies to develop learning interventions, and establishing the foundation for future investigations that aim to successfully integrate and use PAs in education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18048v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aimilios Hadjiliasi, Louis Nisiotis, Irene Polycarpou</dc:creator>
    </item>
    <item>
      <title>Audio-Driven Emotional 3D Talking-Head Generation</title>
      <link>https://arxiv.org/abs/2410.17262</link>
      <description>arXiv:2410.17262v1 Announce Type: cross 
Abstract: Audio-driven video portrait synthesis is a crucial and useful technology in virtual human interaction and film-making applications. Recent advancements have focused on improving the image fidelity and lip-synchronization. However, generating accurate emotional expressions is an important aspect of realistic talking-head generation, which has remained underexplored in previous works. We present a novel system in this paper for synthesizing high-fidelity, audio-driven video portraits with accurate emotional expressions. Specifically, we utilize a variational autoencoder (VAE)-based audio-to-motion module to generate facial landmarks. These landmarks are concatenated with emotional embeddings to produce emotional landmarks through our motion-to-emotion module. These emotional landmarks are then used to render realistic emotional talking-head video using a Neural Radiance Fields (NeRF)-based emotion-to-video module. Additionally, we propose a pose sampling method that generates natural idle-state (non-speaking) videos in response to silent audio inputs. Extensive experiments demonstrate that our method obtains more accurate emotion generation with higher fidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17262v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenqing Wang, Yun Fu</dc:creator>
    </item>
    <item>
      <title>Behavior Matters: An Alternative Perspective on Promoting Responsible Data Science</title>
      <link>https://arxiv.org/abs/2410.17273</link>
      <description>arXiv:2410.17273v1 Announce Type: cross 
Abstract: Data science pipelines inform and influence many daily decisions, from what we buy to who we work for and even where we live. When designed incorrectly, these pipelines can easily propagate social inequity and harm. Traditional solutions are technical in nature; e.g., mitigating biased algorithms. In this vision paper, we introduce a novel lens for promoting responsible data science using theories of behavior change that emphasize not only technical solutions but also the behavioral responsibility of practitioners. By integrating behavior change theories from cognitive psychology with data science workflow knowledge and ethics guidelines, we present a new perspective on responsible data science. We present example data science interventions in machine learning and visual data analysis, contextualized in behavior change theories that could be implemented to interrupt and redirect potentially suboptimal or negligent practices while reinforcing ethically conscious behaviors. We conclude with a call to action to our community to explore this new research area of behavior change interventions for responsible data science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17273v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziwei Dong, Ameya Patil, Yuichi Shoda, Leilani Battle, Emily Wall</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Survey and Classification of Evaluation Criteria for Trustworthy Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2410.17281</link>
      <description>arXiv:2410.17281v1 Announce Type: cross 
Abstract: This paper presents a systematic review of the literature on evaluation criteria for Trustworthy Artificial Intelligence (TAI), with a focus on the seven EU principles of TAI. This systematic literature review identifies and analyses current evaluation criteria, maps them to the EU TAI principles and proposes a new classification system for each principle. The findings reveal both a need for and significant barriers to standardising criteria for TAI evaluation. The proposed classification contributes to the development, selection and standardization of evaluation criteria for TAI governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17281v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louise McCormack, Malika Bendechache</dc:creator>
    </item>
    <item>
      <title>Leveraging the Domain Adaptation of Retrieval Augmented Generation Models for Question Answering and Reducing Hallucination</title>
      <link>https://arxiv.org/abs/2410.17783</link>
      <description>arXiv:2410.17783v1 Announce Type: cross 
Abstract: While ongoing advancements in Large Language Models have demonstrated remarkable success across various NLP tasks, Retrieval Augmented Generation Model stands out to be highly effective on downstream applications like Question Answering. Recently, RAG-end2end model further optimized the architecture and achieved notable performance improvements on domain adaptation. However, the effectiveness of these RAG-based architectures remains relatively unexplored when fine-tuned on specialized domains such as customer service for building a reliable conversational AI system. Furthermore, a critical challenge persists in reducing the occurrence of hallucinations while maintaining high domain-specific accuracy. In this paper, we investigated the performance of diverse RAG and RAG-like architectures through domain adaptation and evaluated their ability to generate accurate and relevant response grounded in the contextual knowledge base. To facilitate the evaluation of the models, we constructed a novel dataset HotelConvQA, sourced from wide range of hotel-related conversations and fine-tuned all the models on our domain specific dataset. We also addressed a critical research gap on determining the impact of domain adaptation on reducing hallucinations across different RAG architectures, an aspect that was not properly measured in prior work. Our evaluation shows positive results in all metrics by employing domain adaptation, demonstrating strong performance on QA tasks and providing insights into their efficacy in reducing hallucinations. Our findings clearly indicate that domain adaptation not only enhances the models' performance on QA tasks but also significantly reduces hallucination across all evaluated RAG architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17783v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Salman Rakin, Md. A. R. Shibly, Zahin M. Hossain, Zeeshan Khan, Md. Mostofa Akbar</dc:creator>
    </item>
    <item>
      <title>The Double-Edged Sword of Behavioral Responses in Strategic Classification: Theory and User Studies</title>
      <link>https://arxiv.org/abs/2410.18066</link>
      <description>arXiv:2410.18066v1 Announce Type: cross 
Abstract: When humans are subject to an algorithmic decision system, they can strategically adjust their behavior accordingly (``game'' the system). While a growing line of literature on strategic classification has used game-theoretic modeling to understand and mitigate such gaming, these existing works consider standard models of fully rational agents. In this paper, we propose a strategic classification model that considers behavioral biases in human responses to algorithms. We show how misperceptions of a classifier (specifically, of its feature weights) can lead to different types of discrepancies between biased and rational agents' responses, and identify when behavioral agents over- or under-invest in different features. We also show that strategic agents with behavioral biases can benefit or (perhaps, unexpectedly) harm the firm compared to fully rational strategic agents. We complement our analytical results with user studies, which support our hypothesis of behavioral biases in human responses to the algorithm. Together, our findings highlight the need to account for human (cognitive) biases when designing AI systems, and providing explanations of them, to strategic human in the loop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18066v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raman Ebrahimi, Kristen Vaccaro, Parinaz Naghizadeh</dc:creator>
    </item>
    <item>
      <title>An intelligent sociotechnical systems (iSTS) concept: Toward a sociotechnically-based hierarchical human-centered AI approach</title>
      <link>https://arxiv.org/abs/2401.03223</link>
      <description>arXiv:2401.03223v4 Announce Type: replace 
Abstract: While artificial intelligence (AI) offers significant benefits, it also has negatively impacted humans and society. A human-centered AI (HCAI) approach has been proposed to address these issues. However, current HCAI practices have shown limited contributions due to a lack of sociotechnical thinking. To overcome these challenges, we conducted a literature review and comparative analysis of sociotechnical characteristics with respect to AI. Then, we propose updated sociotechnical systems (STS) design principles. Based on these findings, this paper introduces an intelligent sociotechnical systems (iSTS) framework to extend traditional STS theory and meet the demands with respect to AI. The iSTS framework emphasizes human-centered joint optimization across individual, organizational, ecosystem, and societal levels. The paper further integrates iSTS with current HCAI practices, proposing a hierarchical HCAI (hHCAI) approach. This hHCAI approach offers a structured approach to address challenges in HCAI practices from a broader sociotechnical perspective. Finally, we provide recommendations for future iSTS and hHCAI work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03223v4</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Xu, Zaifeng Gao</dc:creator>
    </item>
    <item>
      <title>Gamification of Motor Imagery Brain-Computer Interface Training Protocols: a systematic review</title>
      <link>https://arxiv.org/abs/2407.04610</link>
      <description>arXiv:2407.04610v2 Announce Type: replace 
Abstract: Current Motor Imagery Brain-Computer Interfaces (MI-BCI) require a lengthy and monotonous training procedure to train both the system and the user. Considering many users struggle with effective control of MI-BCI systems, a more user-centered approach to training might help motivate users and facilitate learning, alleviating inefficiency of the BCI system. With the increase of BCI-controlled games, researchers have suggested using game principles for BCI training, as games are naturally centered on the player. This review identifies and evaluates the application of game design elements to MI-BCI training, a process known as gamification. Through a systematic literature search, we examined how MI-BCI training protocols have been gamified and how specific game elements impacted the training outcomes. We identified 86 studies that employed gamified MI-BCI protocols in the past decade. The prevalence and reported effects of individual game elements on user experience and performance were extracted and synthesized. Results reveal that MI-BCI training protocols are most often gamified by having users move an avatar in a virtual environment that provides visual feedback. Furthermore, in these virtual environments, users were provided with goals that guided their actions. Using gamification, the reviewed protocols allowed users to reach effective MI-BCI control, with studies reporting positive effects of four individual elements on user performance and experience, namely: feedback, avatars, assistance, and social interaction. Based on these elements, this review makes current and future recommendations for effective gamification, such as the use of virtual reality and adaptation of game difficulty to user skill level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04610v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.chbr.2024.100508</arxiv:DOI>
      <dc:creator>Fred Atilla, Marie Postma, Maryam Alimardani</dc:creator>
    </item>
    <item>
      <title>Investigating the effects of housing instability on depression, anxiety, and mental health treatment in childhood and adolescence</title>
      <link>https://arxiv.org/abs/2409.06011</link>
      <description>arXiv:2409.06011v2 Announce Type: replace 
Abstract: Housing instability is a widespread phenomenon in the United States. In combination with other social determinants of health, housing instability affects children's overall health and development. Drawing on data from the 2022 National Survey of Children's Health, we employed multiple logistic regression models to understand how sociodemographic factors, especially housing instability, affect mental health outcomes and treatment access for youth aged 6-17 years. Our results show that youth facing housing instability have a higher likelihood of experiencing anxiety (OR: 1.42, p&lt;0.001) and depression (OR: 1.57, p&lt;0.001). Furthermore, youth experiencing both mental health conditions and housing instability are significantly less likely to receive mental health services in the past year, indicating the substantial barriers they face in accessing mental health care. Based on our findings, we highlight opportunities for digital mental health interventions to provide children experiencing housing instability with more accessible and consistent mental health services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06011v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachael Zehrung, Di Hu, Yawen Guo, Kai Zheng, Yunan Chen</dc:creator>
    </item>
    <item>
      <title>Privacy as Social Norm: Systematically Reducing Dysfunctional Privacy Concerns on Social Media</title>
      <link>https://arxiv.org/abs/2410.16137</link>
      <description>arXiv:2410.16137v2 Announce Type: replace 
Abstract: Privacy is essential to fully enjoying the benefits of social media. While fear around privacy risks can sometimes motivate privacy management, the negative impact of such fear, particularly when it is perceived as unaddressable (i.e., "dysfunctional" fear), can significantly harm teen well-being. In a co-design study with 136 participants aged 13-18, we explored how teens can protect their privacy without experiencing heightened fear. We identified seven different sources of dysfunctional fear, such as `fear of a hostile environment' and `fear of overstepping privacy norms.' We also evaluated ten designs, co-created with teen participants, that address these fears. Our findings suggest that social media platforms can mitigate dysfunctional fear without compromising privacy by creating a culture where privacy protection is the norm through default privacy-protective features. However, we also found that even the most effective privacy features are not likely to be adopted unless they balance the multifaceted and diverse needs of teens. Individual teens have different needs -- for example, public and private account users have different needs -- and teens often want to enjoy the benefits they get from slightly reducing privacy and widening their social reach. Given these considerations, augmenting default privacy features by allowing them to be toggled on and off will allow individual users to choose their own balance while still maintaining a privacy-focused norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16137v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>JaeWon Kim, Soobin Cho, Robert Wolfe, Jishnu Hari Nair, Alexis Hiniker</dc:creator>
    </item>
    <item>
      <title>STAR: SocioTechnical Approach to Red Teaming Language Models</title>
      <link>https://arxiv.org/abs/2406.11757</link>
      <description>arXiv:2406.11757v4 Announce Type: replace-cross 
Abstract: This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for human red teamers, leading to improved coverage of the risk surface. Parameterised instructions also provide more detailed insights into model failures at no increased cost. Second, STAR improves signal quality by matching demographics to assess harms for specific groups, resulting in more sensitive annotations. STAR further employs a novel step of arbitration to leverage diverse viewpoints and improve label reliability, treating disagreement not as noise but as a valuable contribution to signal quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11757v4</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Weidinger, John Mellor, Bernat Guillen Pegueroles, Nahema Marchal, Ravin Kumar, Kristian Lum, Canfer Akbulut, Mark Diaz, Stevie Bergman, Mikel Rodriguez, Verena Rieser, William Isaac</dc:creator>
    </item>
    <item>
      <title>Accessible, At-Home Detection of Parkinson's Disease via Multi-task Video Analysis</title>
      <link>https://arxiv.org/abs/2406.14856</link>
      <description>arXiv:2406.14856v2 Announce Type: replace-cross 
Abstract: Limited accessibility to neurological care leads to underdiagnosed Parkinson's Disease (PD), preventing early intervention. Existing AI-based PD detection methods primarily focus on unimodal analysis of motor or speech tasks, overlooking the multifaceted nature of the disease. To address this, we introduce a large-scale, multi-task video dataset consisting of 1102 sessions (each containing videos of finger tapping, facial expression, and speech tasks captured via webcam) from 845 participants (272 with PD). We propose a novel Uncertainty-calibrated Fusion Network (UFNet) that leverages this multimodal data to enhance diagnostic accuracy. UFNet employs independent task-specific networks, trained with Monte Carlo Dropout for uncertainty quantification, followed by self-attended fusion of features, with attention weights dynamically adjusted based on task-specific uncertainties. To ensure patient-centered evaluation, the participants were randomly split into three sets: 60% for training, 20% for model selection, and 20% for final performance evaluation. UFNet significantly outperformed single-task models in terms of accuracy, area under the ROC curve (AUROC), and sensitivity while maintaining non-inferior specificity. Withholding uncertain predictions further boosted the performance, achieving 88.0+-0.3%$ accuracy, 93.0+-0.2% AUROC, 79.3+-0.9% sensitivity, and 92.6+-0.3% specificity, at the expense of not being able to predict for 2.3+-0.3% data (+- denotes 95% confidence interval). Further analysis suggests that the trained model does not exhibit any detectable bias across sex and ethnic subgroups and is most effective for individuals aged between 50 and 80. Requiring only a webcam and microphone, our approach facilitates accessible home-based PD screening, especially in regions with limited healthcare resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14856v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Saiful Islam, Tariq Adnan, Jan Freyberg, Sangwu Lee, Abdelrahman Abdelkader, Meghan Pawlik, Cathe Schwartz, Karen Jaffe, Ruth B. Schneider, E Ray Dorsey, Ehsan Hoque</dc:creator>
    </item>
    <item>
      <title>Collaborative AI in Sentiment Analysis: System Architecture, Data Prediction and Deployment Strategies</title>
      <link>https://arxiv.org/abs/2410.13247</link>
      <description>arXiv:2410.13247v2 Announce Type: replace-cross 
Abstract: The advancement of large language model (LLM) based artificial intelligence technologies has been a game-changer, particularly in sentiment analysis. This progress has enabled a shift from highly specialized research environments to practical, widespread applications within the industry. However, integrating diverse AI models for processing complex multimodal data and the associated high costs of feature extraction presents significant challenges. Motivated by the marketing oriented software development +needs, our study introduces a collaborative AI framework designed to efficiently distribute and resolve tasks across various AI systems to address these issues. Initially, we elucidate the key solutions derived from our development process, highlighting the role of generative AI models like \emph{chatgpt}, \emph{google gemini} in simplifying intricate sentiment analysis tasks into manageable, phased objectives. Furthermore, we present a detailed case study utilizing our collaborative AI system in edge and cloud, showcasing its effectiveness in analyzing sentiments across diverse online media channels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13247v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 24 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaofeng Zhang, Jia Hou, Xueting Tan, Gaolei Li, Caijuan Chen</dc:creator>
    </item>
  </channel>
</rss>
