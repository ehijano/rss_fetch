<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Jul 2024 01:44:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>3/2-Approximation for the Matching Augmentation Problem</title>
      <link>https://arxiv.org/abs/2407.11101</link>
      <description>arXiv:2407.11101v1 Announce Type: new 
Abstract: We describe a $\frac{3}{2}$-approximation algorithm for the Matching Augmentation Problem, which is a special case of the weighted 2-edge-connected spanning subgraph problem. This improves upon the previous best ratio $\frac{13}{8}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11101v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ali \c{C}ivril</dc:creator>
    </item>
    <item>
      <title>Trace reconstruction from local statistical queries</title>
      <link>https://arxiv.org/abs/2407.11177</link>
      <description>arXiv:2407.11177v1 Announce Type: new 
Abstract: The goal of trace reconstruction is to reconstruct an unknown $n$-bit string $x$ given only independent random traces of $x$, where a random trace of $x$ is obtained by passing $x$ through a deletion channel. A Statistical Query (SQ) algorithm for trace reconstruction is an algorithm which can only access statistical information about the distribution of random traces of $x$ rather than individual traces themselves. Such an algorithm is said to be $\ell$-local if each of its statistical queries corresponds to an $\ell$-junta function over some block of $\ell$ consecutive bits in the trace. Since several -- but not all -- known algorithms for trace reconstruction fall under the local statistical query paradigm, it is interesting to understand the abilities and limitations of local SQ algorithms for trace reconstruction.
  In this paper we establish nearly-matching upper and lower bounds on local Statistical Query algorithms for both worst-case and average-case trace reconstruction. For the worst-case problem, we show that there is an $\tilde{O}(n^{1/5})$-local SQ algorithm that makes all its queries with tolerance $\tau \geq 2^{-\tilde{O}(n^{1/5})}$, and also that any $\tilde{O}(n^{1/5})$-local SQ algorithm must make some query with tolerance $\tau \leq 2^{-\tilde{\Omega}(n^{1/5})}$. For the average-case problem, we show that there is an $O(\log n)$-local SQ algorithm that makes all its queries with tolerance $\tau \geq 1/\mathrm{poly}(n)$, and also that any $O(\log n)$-local SQ algorithm must make some query with tolerance $\tau \leq 1/\mathrm{poly}(n).$</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11177v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Chen, Anindya De, Chin Ho Lee, Rocco A. Servedio</dc:creator>
    </item>
    <item>
      <title>Almost-linear Time Approximation Algorithm to Euclidean $k$-median and $k$-means</title>
      <link>https://arxiv.org/abs/2407.11217</link>
      <description>arXiv:2407.11217v1 Announce Type: new 
Abstract: Clustering is one of the staples of data analysis and unsupervised learning. As such, clustering algorithms are often used on massive data sets, and they need to be extremely fast. We focus on the Euclidean $k$-median and $k$-means problems, two of the standard ways to model the task of clustering.
  For these, the go-to algorithm is $k$-means++, which yields an $O(\log k)$-approximation in time $\tilde O(nkd)$. While it is possible to improve either the approximation factor [Lattanzi and Sohler, ICML19] or the running time [Cohen-Addad et al., NeurIPS 20], it is unknown how precise a linear-time algorithm can be.
  In this paper, we almost answer this question by presenting an almost linear-time algorithm to compute a constant-factor approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11217v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Dupr\'e la Tour, David Saulpic</dc:creator>
    </item>
    <item>
      <title>On the Houdr\'e-Tetali conjecture about an isoperimetric constant of graphs</title>
      <link>https://arxiv.org/abs/2407.11357</link>
      <description>arXiv:2407.11357v1 Announce Type: new 
Abstract: Houdr\'e and Tetali defined a class of isoperimetric constants $\varphi_p$ of graphs for $0 \leq p \leq 1$, and conjectured a Cheeger-type inequality for $\varphi_\frac12$ of the form $$\lambda_2 \lesssim \varphi_\frac12 \lesssim \sqrt{\lambda_2}$$ where $\lambda_2$ is the second smallest eigenvalue of the normalized Laplacian matrix. If true, the conjecture would be a strengthening of the hard direction of the classical Cheeger's inequality. Morris and Peres proved Houdr\'e and Tetali's conjecture up to an additional log factor, using techniques from evolving sets. We present the following related results on this conjecture.
  - We provide a family of counterexamples to the conjecture of Houdr\'e and Tetali, showing that the logarithmic factor is needed.
  - We match Morris and Peres's bound using standard spectral arguments.
  - We prove that Houdr\'e and Tetali's conjecture is true for any constant $p$ strictly bigger than $\frac12$, which is also a strengthening of the hard direction of Cheeger's inequality.
  Furthermore, our results can be extended to directed graphs using Chung's definition of eigenvalues for directed graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11357v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lap Chi Lau, Dante Tjowasi</dc:creator>
    </item>
    <item>
      <title>Learning-augmented Maximum Independent Set</title>
      <link>https://arxiv.org/abs/2407.11364</link>
      <description>arXiv:2407.11364v1 Announce Type: new 
Abstract: We study the Maximum Independent Set (MIS) problem on general graphs within the framework of learning-augmented algorithms. The MIS problem is known to be NP-hard and is also NP-hard to approximate to within a factor of $n^{1-\delta}$ for any $\delta&gt;0$. We show that we can break this barrier in the presence of an oracle obtained through predictions from a machine learning model that answers vertex membership queries for a fixed MIS with probability $1/2+\varepsilon$. In the first setting we consider, the oracle can be queried once per vertex to know if a vertex belongs to a fixed MIS, and the oracle returns the correct answer with probability $1/2 + \varepsilon$. Under this setting, we show an algorithm that obtains an $\tilde{O}(\sqrt{\Delta}/\varepsilon)$-approximation in $O(m)$ time where $\Delta$ is the maximum degree of the graph. In the second setting, we allow multiple queries to the oracle for a vertex, each of which is correct with probability $1/2 + \varepsilon$. For this setting, we show an $O(1)$-approximation algorithm using $O(n/\varepsilon^2)$ total queries and $\tilde{O}(m)$ runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11364v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Braverman, Prathamesh Dharangutte, Vihan Shah, Chen Wang</dc:creator>
    </item>
    <item>
      <title>Speed-robust scheduling revisited</title>
      <link>https://arxiv.org/abs/2407.11670</link>
      <description>arXiv:2407.11670v1 Announce Type: new 
Abstract: Speed-robust scheduling is the following two-stage problem of scheduling $n$ jobs on $m$ uniformly related machines. In the first stage, the algorithm receives the value of $m$ and the processing times of $n$ jobs; it has to partition the jobs into $b$ groups called bags. In the second stage, the machine speeds are revealed and the bags are assigned to the machines, i.e., the algorithm produces a schedule where all the jobs in the same bag are assigned to the same machine. The objective is to minimize the makespan (the length of the schedule). The algorithm is compared to the optimal schedule and it is called $\rho$-robust, if its makespan is always at most $\rho$ times the optimal one.
  Our main result is an improved bound for equal-size jobs for $b=m$. We give an upper bound of $1.6$. This improves previous bound of $1.8$ and it is almost tight in the light of previous lower bound of $1.58$. Second, for infinitesimally small jobs, we give tight upper and lower bounds for the case when $b\geq m$. This generalizes and simplifies the previous bounds for $b=m$. Finally, we introduce a new special case with relatively small jobs for which we give an algorithm whose robustness is close to that of infinitesimal jobs and thus gives better than $2$-robust for a large class of inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11670v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josef Mina\v{r}\'ik, Ji\v{r}\'i Sgall</dc:creator>
    </item>
    <item>
      <title>IID Prophet Inequality with Random Horizon: Going Beyond Increasing Hazard Rates</title>
      <link>https://arxiv.org/abs/2407.11752</link>
      <description>arXiv:2407.11752v1 Announce Type: new 
Abstract: Prophet inequalities are a central object of study in optimal stopping theory. In the iid model, a gambler sees values in an online fashion, sampled independently from a given distribution. Upon observing each value, the gambler either accepts it as a reward or irrevocably rejects it and proceeds to observe the next value. The goal of the gambler, who cannot see the future, is maximising the expected value of the reward while competing against the expectation of a prophet (the offline maximum). In other words, one seeks to maximise the gambler-to-prophet ratio of the expectations.
  This model has been studied with infinite, finite and unknown number of values. When the gambler faces a random number of values, the model is said to have random horizon. We consider the model in which the gambler is given a priori knowledge of the horizon's distribution. Alijani et al. (2020) designed a single-threshold algorithms achieving a ratio of $1/2$ when the random horizon has an increasing hazard rate and is independent of the values. We prove that with a single-threshold, a ratio of $1/2$ is actually achievable for several larger classes of horizon distributions, with the largest being known as the $\mathcal{G}$ class in reliability theory. Moreover, we extend this result to its dual, the $\overline{\mathcal{G}}$ class (which includes the decreasing hazard rate class), and to low-variance horizons. Finally, we construct the first example of a family of horizons, for which multiple thresholds are necessary to achieve a nonzero ratio. We establish that the Secretary Problem optimal stopping rule provides one such algorithm, paving the way towards the study of the model beyond single-threshold algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11752v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giordano Giambartolomei, Frederik Mallmann-Trenn, Raimundo Saona</dc:creator>
    </item>
    <item>
      <title>Independent Set Reconfiguration Under Bounded-Hop Token</title>
      <link>https://arxiv.org/abs/2407.11768</link>
      <description>arXiv:2407.11768v1 Announce Type: new 
Abstract: The independent set reconfiguration problem (ISReconf) is the problem of determining, for given independent sets I_s and I_t of a graph G, whether I_s can be transformed into I_t by repeatedly applying a prescribed reconfiguration rule that transforms an independent set to another. As reconfiguration rules for the ISReconf, the Token Sliding (TS) model and the Token Jumping (TJ) model are commonly considered. While the TJ model admits the addition of any vertex (as far as the addition yields an independent set), the TS model admits the addition of only a neighbor of the removed vertex. It is known that the complexity status of the ISReconf differs between the TS and TJ models for some graph classes.
  In this paper, we analyze how changes in reconfiguration rules affect the computational complexity of reconfiguration problems. To this end, we generalize the TS and TJ models to a unified reconfiguration rule, called the k-Jump model, which admits the addition of a vertex within distance k from the removed vertex. Then, the TS and TJ models are the 1-Jump and D(G)-Jump models, respectively, where D(G) denotes the diameter of a connected graph G. We give the following three results: First, we show that the computational complexity of the ISReconf under the k-Jump model for general graphs is equivalent for all k &gt;= 3. Second, we present a polynomial-time algorithm to solve the ISReconf under the 2-Jump model for split graphs. We note that the ISReconf under the 1-Jump (i.e., TS) model is PSPACE-complete for split graphs, and hence the complexity status of the ISReconf differs between k = 1 and k = 2. Third, we consider the optimization variant of the ISReconf, which computes the minimum number of steps of any transformation between Is and It. We prove that this optimization variant under the k-Jump model is NP-complete for chordal graphs of diameter at most 2k + 1, for any k &gt;=3.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11768v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroki Hatano, Naoki Kitamura, Taisuke Izumi, Takehiro Ito, Toshimitsu Masuzawa</dc:creator>
    </item>
    <item>
      <title>Text Indexing for Long Patterns using Locally Consistent Anchors</title>
      <link>https://arxiv.org/abs/2407.11819</link>
      <description>arXiv:2407.11819v1 Announce Type: new 
Abstract: In many real-world database systems, a large fraction of the data is represented by strings: sequences of letters over some alphabet. This is because strings can easily encode data arising from different sources. It is often crucial to represent such string datasets in a compact form but also to simultaneously enable fast pattern matching queries. This is the classic text indexing problem. The four absolute measures anyone should pay attention to when designing or implementing a text index are: (i) index space; (ii) query time; (iii) construction space; and (iv) construction time. Unfortunately, however, most (if not all) widely-used indexes (e.g., suffix tree, suffix array, or their compressed counterparts) are not optimized for all four measures simultaneously, as it is difficult to have the best of all four worlds. Here, we take an important step in this direction by showing that text indexing with sampling based on locally consistent anchors (lc-anchors) offers remarkably good performance in all four measures, when we have at hand a lower bound $\ell$ on the length of the queried patterns -- which is arguably a quite reasonable assumption in practical applications. Our index offers average-case guarantees. In our experiments using real benchmark datasets, we show that it compares favorably based on the four measures to all classic indexes: (compressed) suffix tree; (compressed) suffix array; and the FM-index. Notably, we also present a counterpart of our index with worst-case guarantees based on the lc-anchors notion of partitioning sets. To the best of our knowledge, this is the first index achieving the best of all worlds in the regime where we have at hand a lower bound $\ell$ on the length of the queried patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11819v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorraine A. K. Ayad, Grigorios Loukides, Solon P. Pissis</dc:creator>
    </item>
    <item>
      <title>Faster Algorithms for Schatten-p Low Rank Approximation</title>
      <link>https://arxiv.org/abs/2407.11959</link>
      <description>arXiv:2407.11959v1 Announce Type: new 
Abstract: We study algorithms for the Schatten-$p$ Low Rank Approximation (LRA) problem. First, we show that by using fast rectangular matrix multiplication algorithms and different block sizes, we can improve the running time of the algorithms in the recent work of Bakshi, Clarkson and Woodruff (STOC 2022). We then show that by carefully combining our new algorithm with the algorithm of Li and Woodruff (ICML 2020), we can obtain even faster algorithms for Schatten-$p$ LRA.
  While the block-based algorithms are fast in the real number model, we do not have a stability analysis which shows that the algorithms work when implemented on a machine with polylogarithmic bits of precision. We show that the LazySVD algorithm of Allen-Zhu and Li (NeurIPS 2016) can be implemented on a floating point machine with only logarithmic, in the input parameters, bits of precision. As far as we are aware, this is the first stability analysis of any algorithm using $O((k/\sqrt{\varepsilon})\text{poly}(\log n))$ matrix-vector products with the matrix $A$ to output a $1+\varepsilon$ approximate solution for the rank-$k$ Schatten-$p$ LRA problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11959v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Praneeth Kacham, David P. Woodruff</dc:creator>
    </item>
    <item>
      <title>Paralleling and Accelerating Arc Consistency Enforcement with Recurrent Tensor Computations</title>
      <link>https://arxiv.org/abs/2407.11388</link>
      <description>arXiv:2407.11388v1 Announce Type: cross 
Abstract: We propose a new arc consistency enforcement paradigm that transforms arc consistency enforcement into recurrent tensor operations. In each iteration of the recurrence, all involved processes can be fully parallelized with tensor operations. And the number of iterations is quite small. Based on these benefits, the resulting algorithm fully leverages the power of parallelization and GPU, and therefore is extremely efficient on large and densely connected constraint networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11388v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingqi Yang</dc:creator>
    </item>
    <item>
      <title>Opponent Indifference in Rating Systems: A Theoretical Case for Sonas</title>
      <link>https://arxiv.org/abs/2209.03950</link>
      <description>arXiv:2209.03950v3 Announce Type: replace 
Abstract: In competitive games, it is common to assign each player a real number rating signifying their skill level. A rating system is a procedure by which player ratings are adjusted upwards each time they win, or downwards each time they lose. Many matchmaking systems give players some control over their opponent's rating; for example, a player might be able to selectively initiate matches against opponents whose ratings are publicly visible, or abort a match without penalty before it begins but after glimpsing their opponent's rating. It is natural to ask whether one can design a rating system that does not incentivize a rating-maximizing player to act strategically, seeking matches against opponents of one rating over another. We show the following:
  - The full version of this "opponent indifference" property is unfortunately too strong to be feasible. Although it is satisfied by some rating systems, these systems lack certain desirable expressiveness properties, suggesting that they are not suitable to capture most games of interest.
  - However, there is a natural relaxation, roughly requiring indifference between any two opponents who are "reasonably evenly matched" with the choosing player. We prove that this relaxed variant of opponent indifference, which we call $P$ opponent indifference, is viable. In fact, a certain strong version of $P$ opponent indifference precisely characterizes the rating system Sonas, which was originally proposed for its empirical predictive accuracy on the outcomes of high-level chess matches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.03950v3</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Greg Bodwin, Forest Zhang</dc:creator>
    </item>
    <item>
      <title>The Complexity of Diameter on H-free graphs</title>
      <link>https://arxiv.org/abs/2402.16678</link>
      <description>arXiv:2402.16678v2 Announce Type: replace 
Abstract: The intensively studied Diameter problem is to find the diameter of a given connected graph. We investigate, for the first time in a structured manner, the complexity of Diameter for H-free graphs, that is, graphs that do not contain a fixed graph H as an induced subgraph. We first show that if H is not a linear forest with small components, then Diameter cannot be solved in subquadratic time for H-free graphs under SETH. For some small linear forests, we do show linear-time algorithms for solving Diameter. For other linear forests H, we make progress towards linear-time algorithms by considering specific diameter values. If H is a linear forest, the maximum value of the diameter of any graph in a connected H-free graph class is some constant dmax dependent only on H. We give linear-time algorithms for deciding if a connected H-free graph has diameter dmax, for several linear forests H. In contrast, for one such linear forest H, Diameter cannot be solved in subquadratic time for H-free graphs under SETH. Moreover, we even show that, for several other linear forests H, one cannot decide in subquadratic time if a connected H-free graph has diameter dmax under SETH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16678v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jelle J. Oostveen, Dani\"el Paulusma, Erik Jan van Leeuwen</dc:creator>
    </item>
    <item>
      <title>Combinatorial Correlation Clustering</title>
      <link>https://arxiv.org/abs/2404.05433</link>
      <description>arXiv:2404.05433v2 Announce Type: replace 
Abstract: Correlation Clustering is a classic clustering objective arising in numerous machine learning and data mining applications. Given a graph $G=(V,E)$, the goal is to partition the vertex set into clusters so as to minimize the number of edges between clusters plus the number of edges missing within clusters. The problem is APX-hard and the best known polynomial time approximation factor is 1.73 by Cohen-Addad, Lee, Li, and Newman [FOCS'23]. They use an LP with $|V|^{1/\epsilon^{\Theta(1)}}$ variables for some small $\epsilon$. However, due to the practical relevance of correlation clustering, there has also been great interest in getting more efficient sequential and parallel algorithms. The classic combinatorial \emph{pivot} algorithm of Ailon, Charikar and Newman [JACM'08] provides a 3-approximation in linear time. Like most other algorithms discussed here, this uses randomization. Recently, Behnezhad, Charikar, Ma and Tan [FOCS'22] presented a $3+\epsilon$-approximate solution for solving problem in a constant number of rounds in the Massively Parallel Computation (MPC) setting. Very recently, Cao, Huang, Su [SODA'24] provided a 2.4-approximation in a polylogarithmic number of rounds in the MPC model and in $\tilde{O} (|E|^{1.5})$ time in the classic sequential setting. They asked whether it is possible to get a better than 3-approximation in near-linear time?
  We resolve this problem with an efficient combinatorial algorithm providing a drastically better approximation factor. It achieves a $\sim 2-2/13 &lt; 1.847$-approximation in sub-linear ($\tilde O(|V|)$) sequential time or in sub-linear ($\tilde O(|V|)$) space in the streaming setting. In the MPC model, we give an algorithm using only a constant number of rounds that achieves a $\sim 2-1/8 &lt; 1.876$-approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05433v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3618260.3649712</arxiv:DOI>
      <dc:creator>Vincent Cohen-Addad, David Rasmussen Lolck, Marcin Pilipczuk, Mikkel Thorup, Shuyi Yan, Hanwen Zhang</dc:creator>
    </item>
    <item>
      <title>Rectangle Tiling Binary Arrays</title>
      <link>https://arxiv.org/abs/2007.14142</link>
      <description>arXiv:2007.14142v2 Announce Type: replace-cross 
Abstract: The problem of rectangle tiling binary arrays is defined as follows. Given an $n \times n$ array $A$ of zeros and ones and a natural number $p$, our task is to partition $A$ into at most $p$ rectangular tiles, so that the maximal weight of a tile is minimized. A tile is any rectangular subarray of $A$. The weight of a tile is the sum of elements that fall within it. We present a linear $(O(n^2))$ time $(\frac{3}{2}+\frac{p^2}{w(A)})$-approximation algorithm (where $\frac{p^2}{w(A)} &lt; \frac{1}{2}$) for this problem, where $w(A)$ denotes the weight of the whole array $A$. This improves on the previously known approximation with the ratio $2$.
  The result is best possible in the following sense. The algorithm employs the lower bound of $L=\lceil \frac{w(A)}{p} \rceil$, which is the only known and used bound on the optimum in all algorithms for rectangle tiling. We prove that a better approximation factor for the binary \RTILE cannot be achieved using $L$, because there exist arrays, whose every partition contains a tile with weight at least $(\frac{3}{2}+\frac{p^2}{w(A)})L$. We also consider the dual problem of rectangle tiling for binary arrays, where we are given an upper bound on the weight of the tiles, and we have to cover the array $A$ with the minimum number of non-overlapping tiles. Both problems have natural extensions to $d$-dimensional versions, for which we provide analogous results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.14142v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratik Ghosal, Syed Mohammad Meesum, Katarzyna Paluch</dc:creator>
    </item>
    <item>
      <title>Learning in Repeated Multi-Unit Pay-As-Bid Auctions</title>
      <link>https://arxiv.org/abs/2307.15193</link>
      <description>arXiv:2307.15193v2 Announce Type: replace-cross 
Abstract: Motivated by Carbon Emissions Trading Schemes, Treasury Auctions, and Procurement Auctions, which all involve the auctioning of homogeneous multiple units, we consider the problem of learning how to bid in repeated multi-unit pay-as-bid auctions. In each of these auctions, a large number of (identical) items are to be allocated to the largest submitted bids, where the price of each of the winning bids is equal to the bid itself. The problem of learning how to bid in pay-as-bid auctions is challenging due to the combinatorial nature of the action space. We overcome this challenge by focusing on the offline setting, where the bidder optimizes their vector of bids while only having access to the past submitted bids by other bidders. We show that the optimal solution to the offline problem can be obtained using a polynomial time dynamic programming (DP) scheme. We leverage the structure of the DP scheme to design online learning algorithms with polynomial time and space complexity under full information and bandit feedback settings. We achieve an upper bound on regret of $O(M\sqrt{T\log |\mathcal{B}|})$ and $O(M\sqrt{|\mathcal{B}|T\log |\mathcal{B}|})$ respectively, where $M$ is the number of units demanded by the bidder, $T$ is the total number of auctions, and $|\mathcal{B}|$ is the size of the discretized bid space. We accompany these results with a regret lower bound, which match the linear dependency in $M$. Our numerical results suggest that when all agents behave according to our proposed no regret learning algorithms, the resulting market dynamics mainly converge to a welfare maximizing equilibrium where bidders submit uniform bids. Lastly, our experiments demonstrate that the pay-as-bid auction consistently generates significantly higher revenue compared to its popular alternative, the uniform price auction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15193v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rigel Galgana, Negin Golrezaei</dc:creator>
    </item>
    <item>
      <title>The Maximum Clique Problem in a Disk Graph Made Easy</title>
      <link>https://arxiv.org/abs/2404.03751</link>
      <description>arXiv:2404.03751v2 Announce Type: replace-cross 
Abstract: A disk graph is an intersection graph of disks in $\mathbb{R}^2$. Determining the computational complexity of finding a maximum clique in a disk graph is a long-standing open problem. In 1990, Clark, Colbourn, and Johnson gave a polynomial-time algorithm for computing a maximum clique in a unit disk graph. However, finding a maximum clique when disks are of arbitrary size is widely believed to be a challenging open problem. The problem is open even if we restrict the disks to have at most two different sizes of radii, or restrict the radii to be within $[1,1+\varepsilon]$ for some $\epsilon&gt;0$. In this paper, we provide a new perspective to examine adjacencies in a disk graph that helps obtain the following results.
  - We design an $O(2^k n^{2k} poly(n))$-time algorithm to find a maximum clique in a $n$-vertex disk graph with $k$ different sizes of radii. This is polynomial for every fixed $k$, and thus settles the open question for the case when $k=2$.
  - Given a set of $n$ unit disks, we show how to compute a maximum clique inside each possible axis-aligned rectangle determined by the disk centers in $O(n^5\log n)$-time. This is at least a factor of $n^{4/3}$ faster than applying the fastest known algorithm for finding a maximum clique in a unit disk graph for each rectangle independently.
  - We give an $O(2^kn^{2rk} poly(n,r))$-time algorithm to find a maximum clique in a $n$-vertex ball graph with $k$ different sizes of radii where the ball centers lie on $r$ parallel planes. This is polynomial for every fixed $k$ and $r$, and thus contrasts the previously known NP-hardness result for finding a maximum clique in an arbitrary ball graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03751v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Mark Keil, Debajyoti Mondal</dc:creator>
    </item>
    <item>
      <title>Obstructions to Erd\H{o}s-P\'osa Dualities for Minors</title>
      <link>https://arxiv.org/abs/2407.09671</link>
      <description>arXiv:2407.09671v2 Announce Type: replace-cross 
Abstract: Let ${\cal G}$ and ${\cal H}$ be minor-closed graph classes. The pair $({\cal H},{\cal G})$ is an Erd\H{o}s-P\'osa pair (EP-pair) if there is a function $f$ where, for every $k$ and every $G\in{\cal G},$ either $G$ has $k$ pairwise vertex-disjoint subgraphs not belonging to ${\cal H},$ or there is a set $S\subseteq V(G)$ where $|S|\leq f(k)$ and $G-S\in{\cal H}.$ The classic result of Erd\H{o}s and P\'osa says that if $\mathcal{F}$ is the class of forests, then $({\cal F},{\cal G})$ is an EP-pair for every ${\cal G}$. The class ${\cal G}$ is an EP-counterexample for ${\cal H}$ if ${\cal G}$ is minimal with the property that $({\cal H},{\cal G})$ is not an EP-pair. We prove that for every ${\cal H}$ the set $\mathfrak{C}_{\cal H}$ of all EP-counterexamples for ${\cal H}$ is finite. In particular, we provide a complete characterization of $\mathfrak{C}_{\cal H}$ for every ${\cal H}$ and give a constructive upper bound on its size. Each class ${\cal G}\in \mathfrak{C}_{\cal H}$ can be described as all minors of a sequence of grid-like graphs $\langle \mathscr{W}_{k} \rangle_{k\in \mathbb{N}}.$ Moreover, each $\mathscr{W}_{k}$ admits a half-integral packing: $k$ copies of some $H\not\in{\cal H}$ where no vertex is used more than twice. This gives a complete delineation of the half-integrality threshold of the Erd\H{o}s-P\'osa property for minors and yields a constructive proof of Thomas' conjecture on the half-integral Erd\H{o}s-P\'osa property for minors (recently confirmed, non-constructively, by Liu). Let $h$ be the maximum size of a graph in ${\cal H}.$ For every class ${\cal H},$ we construct an algorithm that, given a graph $G$ and a $k,$ either outputs a half-integral packing of $k$ copies of some $H \not\in {\cal H}$ or outputs a set of at most ${2^{k^{\cal O}_h(1)}}$ vertices whose deletion creates a graph in ${\cal H}$ in time $2^{2^{k^{{\cal O}_h(1)}}}\cdot |G|^4\log |G|.$</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09671v2</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Paul, Evangelos Protopapas, Dimitrios M. Thilikos, Sebastian Wiederrecht</dc:creator>
    </item>
  </channel>
</rss>
