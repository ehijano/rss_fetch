<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Jun 2024 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Temporal Complexity of a Hopfield-Type Neural Model in Random and Scale-Free Graphs</title>
      <link>https://arxiv.org/abs/2406.12895</link>
      <description>arXiv:2406.12895v1 Announce Type: new 
Abstract: The Hopfield network model and its generalizations were introduced as a model of associative, or content-addressable, memory. They were widely investigated both as a unsupervised learning method in artificial intelligence and as a model of biological neural dynamics in computational neuroscience. The complexity features of biological neural networks are attracting the interest of scientific community since the last two decades. More recently, concepts and tools borrowed from complex network theory were applied to artificial neural networks and learning, thus focusing on the topological aspects. However, the temporal structure is also a crucial property displayed by biological neural networks and investigated in the framework of systems displaying complex intermittency. The Intermittency-Driven Complexity (IDC) approach indeed focuses on the metastability of self-organized states, whose signature is a power-decay in the inter-event time distribution or a scaling behavior in the related event-driven diffusion processes. The investigation of IDC in neural dynamics and its relationship with network topology is still in its early stages. In this work we present the preliminary results of a IDC analysis carried out on a bio-inspired Hopfield-type neural network comparing two different connectivities, i.e., scale-free vs. random network topology. We found that random networks can trigger complexity features similar to that of scale-free networks, even if with some differences and for different parameter values, in particular for different noise levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12895v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <category>nlin.AO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marco Cafiso, Paolo Paradisi</dc:creator>
    </item>
    <item>
      <title>Entropy-statistical approach to phase-locking detection of pulse oscillations: application for the analysis of biosignal synchronization</title>
      <link>https://arxiv.org/abs/2406.12906</link>
      <description>arXiv:2406.12906v1 Announce Type: new 
Abstract: In this study a new method for analyzing synchronization in oscillator systems is proposed using the example of modeling the dynamics of a circuit of two resistively coupled pulse oscillators. The dynamic characteristic of synchronization is fuzzy entropy (FuzzyEn) calculated a time series composed of the ratios of the number of pulse periods (subharmonic ratio, SHR) during phase-locking intervals. Low entropy values indicate strong synchronization, whereas high entropy values suggest weak synchronization between the two oscillators. This method effectively visualizes synchronized modes of the circuit using entropy maps of synchronization states. Additionally, a classification of synchronization states is proposed based on the dependencies of FuzzyEn on the length of embedding vectors of SHR time series. An extension of this method for analyzing non-relaxation (non-spike) type signals is illustrated using the example of phase-phase coupling rhythms of local field potential of rat hippocampus. The entropy-statistical approach using rational fractions and pulse signal forms makes this method promising for analyzing biosignal synchronization and implementing the algorithm in mobile digital platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12906v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>nlin.AO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Petr Boriskov, Vadim Putrolaynen, Andrei Velichko, Kristina Peltonen</dc:creator>
    </item>
    <item>
      <title>Association of neighborhood disadvantage with cognitive function and cortical disorganization in an unimpaired cohort</title>
      <link>https://arxiv.org/abs/2406.13822</link>
      <description>arXiv:2406.13822v1 Announce Type: new 
Abstract: Neighborhood disadvantage is associated with worse health and cognitive outcomes. Morphological similarity network (MSN) is a promising approach to elucidate cortical network patterns underlying complex cognitive functions. We hypothesized that MSNs could capture changes in cortical patterns related to neighborhood disadvantage and cognitive function. This cross-sectional study included cognitively unimpaired participants from two large Alzheimers studies at University of Wisconsin-Madison. Neighborhood disadvantage status was obtained using the Area Deprivation Index (ADI). Cognitive performance was assessed on memory, processing speed and executive function. Morphological Similarity Networks (MSN) were constructed for each participant based on the similarity in distribution of cortical thickness of brain regions, followed by computation of local and global network features. Association of ADI with cognitive scores and MSN features were examined using linear regression and mediation analysis. ADI showed negative association with category fluency,implicit learning speed, story recall and modified pre-clinical Alzheimers cognitive composite scores, indicating worse cognitive function among those living in more disadvantaged neighborhoods. Local network features of frontal and temporal regions differed based on ADI status. Centrality of left lateral orbitofrontal region showed a partial mediating effect between association of neighborhood disadvantage and story recall performance. Our preliminary findings suggest differences in local cortical organization by neighborhood disadvantage, which partially mediated the relationship between ADI and cognitive performance, providing a possible network-based mechanism to, in-part, explain the risk for poor cognitive functioning associated with disadvantaged neighborhoods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13822v1</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Apoorva Safai, Erin Jonaitis, Rebecca E Langhough, William R Buckingham, Sterling C. Johnson, W. Ryan Powell, Amy J. H. Kind, Barbara B. Bendlin, Pallavi Tiwari</dc:creator>
    </item>
    <item>
      <title>Self-Attention in Transformer Networks Explains Monkeys' Gaze Pattern in Pac-Man Game</title>
      <link>https://arxiv.org/abs/2406.14100</link>
      <description>arXiv:2406.14100v1 Announce Type: new 
Abstract: We proactively direct our eyes and attention to collect information during problem solving and decision making. Understanding gaze patterns is crucial for gaining insights into the computation underlying the problem-solving process. However, there is a lack of interpretable models that can account for how the brain directs the eyes to collect information and utilize it, especially in the context of complex problem solving. In the current study, we analyzed the gaze patterns of two monkeys playing the Pac-Man game. We trained a transformer network to mimic the monkeys' gameplay and found its attention pattern captures the monkeys' eye movements. In addition, the prediction based on the transformer network's attention outperforms the human subjects' predictions. Importantly, we dissected the computation underlying the attention mechanism of the transformer network, revealing its layered structures reflecting a value-based attention component and a component that captures the interactions between Pac-Man and other game objects. Based on these findings, we built a condensed attention model that is not only as accurate as the transformer network but also fully interpretable. Our results highlight the potential of using transformer neural networks to model and understand the cognitive processes underlying complex problem solving in the brain, opening new avenues for investigating the neural basis of cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14100v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhongqiao Lin, Yunwei Li, Tianming Yang</dc:creator>
    </item>
    <item>
      <title>The neural correlates of logical-mathematical symbol systems processing resemble that of spatial cognition more than natural language processing</title>
      <link>https://arxiv.org/abs/2406.14358</link>
      <description>arXiv:2406.14358v1 Announce Type: new 
Abstract: The ability to manipulate logical-mathematical symbols (LMS), encompassing tasks such as calculation, reasoning, and programming, is a cognitive skill arguably unique to humans. Considering the relatively recent emergence of this ability in human evolutionary history, it has been suggested that LMS processing may build upon more fundamental cognitive systems, possibly through neuronal recycling. Previous studies have pinpointed two primary candidates, natural language processing and spatial cognition. Existing comparisons between these domains largely relied on task-level comparison, which may be confounded by task idiosyncrasy. The present study instead compared the neural correlates at the domain level with both automated meta-analysis and synthesized maps based on three representative LMS tasks, reasoning, calculation, and mental programming. Our results revealed a more substantial cortical overlap between LMS processing and spatial cognition, in contrast to language processing. Furthermore, in regions activated by both spatial and language processing, the multivariate activation pattern for LMS processing exhibited greater multivariate similarity to spatial cognition than to language processing. A hierarchical clustering analysis further indicated that typical LMS tasks were indistinguishable from spatial cognition tasks at the neural level, suggesting an inherent connection between these two cognitive processes. Taken together, our findings support the hypothesis that spatial cognition is likely the basis of LMS processing, which may shed light on the limitations of large language models in logical reasoning, particularly those trained exclusively on textual data without explicit emphasis on spatial content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14358v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuannan Li, Shan Xu, Jia Liu</dc:creator>
    </item>
    <item>
      <title>CU-Net: a U-Net architecture for efficient brain-tumor segmentation on BraTS 2019 dataset</title>
      <link>https://arxiv.org/abs/2406.13113</link>
      <description>arXiv:2406.13113v1 Announce Type: cross 
Abstract: Accurately segmenting brain tumors from MRI scans is important for developing effective treatment plans and improving patient outcomes. This study introduces a new implementation of the Columbia-University-Net (CU-Net) architecture for brain tumor segmentation using the BraTS 2019 dataset. The CU-Net model has a symmetrical U-shaped structure and uses convolutional layers, max pooling, and upsampling operations to achieve high-resolution segmentation. Our CU-Net model achieved a Dice score of 82.41%, surpassing two other state-of-the-art models. This improvement in segmentation accuracy highlights the robustness and effectiveness of the model, which helps to accurately delineate tumor boundaries, which is crucial for surgical planning and radiation therapy, and ultimately has the potential to improve patient outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13113v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qimin Zhang, Weiwei Qi, Huili Zheng, Xinyu Shen</dc:creator>
    </item>
    <item>
      <title>Self-organized transport in noisy dynamic networks</title>
      <link>https://arxiv.org/abs/2406.13504</link>
      <description>arXiv:2406.13504v1 Announce Type: cross 
Abstract: We present a numerical study of multi-commodity transport in a noisy, nonlinear network. The nonlinearity determines the dynamics of the edge capacities, which can be amplified or suppressed depending on the local current flowing across an edge. We consider network self-organization for three different nonlinear functions: For all three we identify parameter regimes where noise leads to self-organization into more robust topologies, that are not found by the sole noiseless dynamics. Moreover, the interplay between noise and specific functional behavior of the nonlinearity gives rise to different features, such as (i) continuous or discontinuous responses to the demand strength and (ii) either single or multi-stable solutions. Our study shows the crucial role of the activation function on noise-assisted phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13504v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>nlin.AO</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Frederic Folz, Kurt Mehlhorn, Giovanna Morigi</dc:creator>
    </item>
    <item>
      <title>Prescribed exponential stabilization of a one-layer neural network with delayed feedback: Insights in seizure prevention and neural control</title>
      <link>https://arxiv.org/abs/2406.13730</link>
      <description>arXiv:2406.13730v1 Announce Type: cross 
Abstract: This paper provides control-oriented delay-based modelling of a one-layer neural network of Hopfield-type subject to an external input designed as delayed feedback. The specificity of such a model is that it makes the considered neuron less susceptible to seizure caused by its inherent dynamic instability. This modelling exploits a recently set partial pole placement for linear functional differential equations, which relies on the coexistence of real spectral values, allowing the explicit prescription of the closed-loop solution's exponential decay. The proposed framework improves some pioneering and scarce results from the literature on the characterization of the exact solution's exponential decay when a simple real spectral value exists. Indeed, it improves neural stability when the inherent dynamic is stable and provides insights into the design of a one-layer neural network that can be stabilized exponentially with delayed feedback and with a prescribed decay rate regardless of whether the inherent neuron dynamic is stable or unstable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13730v1</guid>
      <category>math.SP</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyprien Tamekue, Islam Boussaada, Karim Trabelsi</dc:creator>
    </item>
    <item>
      <title>Zero field active shielding</title>
      <link>https://arxiv.org/abs/2406.14234</link>
      <description>arXiv:2406.14234v1 Announce Type: cross 
Abstract: Ambient field suppression is critical for accurate magnetic field measurements, and a requirement for certain low-field sensors to operate. The difference in magnitude between noise and signal (up to 10$^9$) makes the problem challenging, and solutions such as passive shielding, post-hoc processing, and most active shielding designs do not address it completely. Zero field active shielding (ZFS) achieves accurate field suppression with a feed-forward structure in which correction coils are fed by reference sensors via a matrix found using data-driven methods. Requirements are a sufficient number of correction coils and reference sensors to span the ambient field at the sensors, and to zero out the coil-to-reference sensor coupling. The solution assumes instantaneous propagation and mixing, but it can be extended to handle convolutional effects. Precise calculations based on sensor and coil geometries are not necessary, other than to improve efficiency and usability. The solution is simulated here but not implemented in hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14234v1</guid>
      <category>physics.med-ph</category>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <category>physics.ins-det</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain de Cheveign\'e</dc:creator>
    </item>
    <item>
      <title>Control when confidence is costly</title>
      <link>https://arxiv.org/abs/2406.14427</link>
      <description>arXiv:2406.14427v1 Announce Type: cross 
Abstract: We develop a version of stochastic control that accounts for computational costs of inference. Past studies identified efficient coding without control, or efficient control that neglects the cost of synthesizing information. Here we combine these concepts into a framework where agents rationally approximate inference for efficient control. Specifically, we study Linear Quadratic Gaussian (LQG) control with an added internal cost on the relative precision of the posterior probability over the world state. This creates a trade-off: an agent can obtain more utility overall by sacrificing some task performance, if doing so saves enough bits during inference. We discover that the rational strategy that solves the joint inference and control problem goes through phase transitions depending on the task demands, switching from a costly but optimal inference to a family of suboptimal inferences related by rotation transformations, each misestimate the stability of the world. In all cases, the agent moves more to think less. This work provides a foundation for a new type of rational computations that could be used by both brains and machines for efficient but computationally constrained control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14427v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Itzel Olivos-Castillo, Paul Schrater, Xaq Pitkow</dc:creator>
    </item>
    <item>
      <title>Revealing Vision-Language Integration in the Brain with Multimodal Networks</title>
      <link>https://arxiv.org/abs/2406.14481</link>
      <description>arXiv:2406.14481v1 Announce Type: cross 
Abstract: We use (multi)modal deep neural networks (DNNs) to probe for sites of multimodal integration in the human brain by predicting stereoencephalography (SEEG) recordings taken while human subjects watched movies. We operationalize sites of multimodal integration as regions where a multimodal vision-language model predicts recordings better than unimodal language, unimodal vision, or linearly-integrated language-vision models. Our target DNN models span different architectures (e.g., convolutional networks and transformers) and multimodal training techniques (e.g., cross-attention and contrastive learning). As a key enabling step, we first demonstrate that trained vision and language models systematically outperform their randomly initialized counterparts in their ability to predict SEEG signals. We then compare unimodal and multimodal models against one another. Because our target DNN models often have different architectures, number of parameters, and training sets (possibly obscuring those differences attributable to integration), we carry out a controlled comparison of two models (SLIP and SimCLR), which keep all of these attributes the same aside from input modality. Using this approach, we identify a sizable number of neural sites (on average 141 out of 1090 total sites or 12.94%) and brain regions where multimodal integration seems to occur. Additionally, we find that among the variants of multimodal training techniques we assess, CLIP-style training is the best suited for downstream prediction of the neural activity in these sites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14481v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vighnesh Subramaniam, Colin Conwell, Christopher Wang, Gabriel Kreiman, Boris Katz, Ignacio Cases, Andrei Barbu</dc:creator>
    </item>
    <item>
      <title>Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models</title>
      <link>https://arxiv.org/abs/2406.14549</link>
      <description>arXiv:2406.14549v1 Announce Type: cross 
Abstract: The proliferation of large language models has revolutionized natural language processing tasks, yet it raises profound concerns regarding data privacy and security. Language models are trained on extensive corpora including potentially sensitive or proprietary information, and the risk of data leakage -- where the model response reveals pieces of such information -- remains inadequately understood. This study examines susceptibility to data leakage by quantifying the phenomenon of memorization in machine learning models, focusing on the evolution of memorization patterns over training. We investigate how the statistical characteristics of training data influence the memories encoded within the model by evaluating how repetition influences memorization. We reproduce findings that the probability of memorizing a sequence scales logarithmically with the number of times it is present in the data. Furthermore, we find that sequences which are not apparently memorized after the first encounter can be uncovered throughout the course of training even without subsequent encounters. The presence of these latent memorized sequences presents a challenge for data privacy since they may be hidden at the final checkpoint of the model. To this end, we develop a diagnostic test for uncovering these latent memorized sequences by considering their cross entropy loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14549v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sunny Duan, Mikail Khona, Abhiram Iyer, Rylan Schaeffer, Ila R Fiete</dc:creator>
    </item>
    <item>
      <title>Pre-frontal cortex guides dimension-reducing transformations in the occipito-ventral pathway for categorization behaviors</title>
      <link>https://arxiv.org/abs/2205.04393</link>
      <description>arXiv:2205.04393v3 Announce Type: replace 
Abstract: To interpret our surroundings, the brain uses a visual categorization process. Current theories and models suggest that this process comprises a hierarchy of different computations that transforms complex, high-dimensional inputs into lower-dimensional representations (i.e. manifolds) in support of multiple categorization behaviors. Here, we tested this hypothesis by analyzing these transformations reflected in dynamic MEG source activity while individual participants actively categorized the same stimuli according to different tasks: face expression, face gender, pedestrian gender, vehicle type. Results reveal three transformation stages guided by pre-frontal cortex. At Stage 1 (high-dimensional, 50-120ms), occipital sources represent both task-relevant and task-irrelevant stimulus features; task-relevant features advance into higher ventral/dorsal regions whereas task-irrelevant features halt at the occipital-temporal junction. At Stage 2 (121-150ms), stimulus feature representations reduce to lower-dimensional manifolds, which then transform into the task-relevant features underlying categorization behavior over Stage 3 (161-350ms).</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.04393v3</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Y. Duan (School of Psychology and Neuroscience, University of Glasgow, United Kingdom), J. Zhan (School of Psychology and Neuroscience, University of Glasgow, United Kingdom), J. Gross (Institute for Biomagnetism and Biosignalanalysis, University of M\"unster, Germany), R. A. A. Ince (School of Psychology and Neuroscience, University of Glasgow, United Kingdom), P. G. Schyns (School of Psychology and Neuroscience, University of Glasgow, United Kingdom)</dc:creator>
    </item>
    <item>
      <title>Tensor formalism for predicting synaptic connections with ensemble modeling or optimization</title>
      <link>https://arxiv.org/abs/2310.20309</link>
      <description>arXiv:2310.20309v2 Announce Type: replace 
Abstract: Theoretical neuroscientists often try to understand how the structure of a neural network relates to its function by focusing on structural features that would either follow from optimization or occur consistently across possible implementations. Both optimization theories and ensemble modeling approaches have repeatedly proven their worth, and it would simplify theory building considerably if predictions from both theory types could be derived and tested simultaneously. Here we show how tensor formalism from theoretical physics can be used to unify and solve many optimization and ensemble modeling approaches to predicting synaptic connectivity from neuronal responses. We specifically focus on analyzing the solution space of synaptic weights that allow a threshold-linear neural network to respond in a prescribed way to a limited number of input conditions. For optimization purposes, we compute the synaptic weight vector that minimizes an arbitrary quadratic loss function. For ensemble modeling, we identify synaptic weight features that occur consistently across all solutions bounded by an arbitrary ellipsoid. We derive a common solution to this suite of nonlinear problems by showing how each of them reduces to an equivalent linear problem that can be solved analytically. Although identifying the equivalent linear problem is nontrivial, our tensor formalism provides an elegant geometrical perspective that allows us to solve the problem approximately in an analytical way or exactly using numeric methods. The final algorithm is applicable to a wide range of interesting neuroscience problems, and the associated geometric insights may carry over to other scientific problems that require constrained optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20309v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.bio-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tirthabir Biswas, Tianzhi Lambus Li, James E. Fitzgerald</dc:creator>
    </item>
    <item>
      <title>Decomposing Thermodynamic Dissipation of Linear Langevin Systems via Oscillatory Modes and Its Application to Neural Dynamics</title>
      <link>https://arxiv.org/abs/2312.03489</link>
      <description>arXiv:2312.03489v4 Announce Type: replace 
Abstract: Recent developments in stochastic thermodynamics have elucidated various relations between the entropy production rate (thermodynamic dissipation) and the physical limits of information processing in nonequilibrium dynamical systems. These findings have opened new perspectives in analyzing real biological systems. In neuroscience, the importance of quantifying entropy production has attracted attention for understanding information processing in the brain. However, the relationship between the entropy production rate and oscillations, which are common in many biological systems, remains unclear. For instance, neural oscillations like delta, theta, and alpha waves play crucial roles in brain information processing. Here, we derive a novel decomposition of the entropy production rate of linear Langevin systems. We show that one component of the entropy production rate, called the housekeeping entropy production rate, can be decomposed into independent positive contributions from oscillatory modes. Our decomposition enables us to calculate the contribution of oscillatory modes to the housekeeping entropy production rate. In addition, when the noise matrix is diagonal, the contribution of each oscillatory mode can be further decomposed into the contribution of each system element. To demonstrate the utility of our decomposition, we applied it to an electrocorticography (ECoG) dataset recorded during awake and anesthetized conditions in monkeys, where the oscillatory properties change drastically. We showed consistent trends across different monkeys: the contribution of delta band was larger in the anesthetized condition than in the awake condition, while those from higher frequency bands, such as the theta band, were smaller. These results allow us to interpret the changes in neural oscillation in terms of stochastic thermodynamics and the physical limits of information processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03489v4</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daiki Sekizawa, Sosuke Ito, Masafumi Oizumi</dc:creator>
    </item>
    <item>
      <title>Hyperbolic embedding of brain networks as a tool for epileptic seizures forecasting</title>
      <link>https://arxiv.org/abs/2406.10184</link>
      <description>arXiv:2406.10184v2 Announce Type: replace 
Abstract: The evidence indicates that intracranial EEG connectivity, as estimated from daily resting state recordings from epileptic patients, may be capable of identifying preictal states. In this study, we employed hyperbolic embedding of brain networks to capture non-trivial patterns that discriminate between connectivity networks from days with (preictal) and without (interictal) seizure. A statistical model was constructed by combining hyperbolic geometry and machine learning tools, which allowed for the estimation of the probability of an upcoming seizure. The results demonstrated that representing brain networks in a hyperbolic space enabled an accurate discrimination (85%) between interictal (no-seizure) and preictal (seizure within the next 24 hours) states. The proposed method also demonstrated excellent prediction performances, with an overall accuracy of 87% and an F1-score of 89% (mean Brier score and Brier skill score of 0.12 and 0.37, respectively). In conclusion, our findings indicate that representations of brain connectivity in a latent geometry space can reveal a daily and reliable signature of the upcoming seizure(s), thus providing a promising biomarker for seizure forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10184v2</guid>
      <category>q-bio.NC</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Guillemaud, Louis Cousyn, Vincent Navarro, Mario Chavez</dc:creator>
    </item>
    <item>
      <title>Compressed representation of brain genetic transcription</title>
      <link>https://arxiv.org/abs/2310.16113</link>
      <description>arXiv:2310.16113v3 Announce Type: replace-cross 
Abstract: The architecture of the brain is too complex to be intuitively surveyable without the use of compressed representations that project its variation into a compact, navigable space. The task is especially challenging with high-dimensional data, such as gene expression, where the joint complexity of anatomical and transcriptional patterns demands maximum compression. Established practice is to use standard principal component analysis (PCA), whose computational felicity is offset by limited expressivity, especially at great compression ratios. Employing whole-brain, voxel-wise Allen Brain Atlas transcription data, here we systematically compare compressed representations based on the most widely supported linear and non-linear methods-PCA, kernel PCA, non-negative matrix factorization (NMF), t-stochastic neighbour embedding (t-SNE), uniform manifold approximation and projection (UMAP), and deep auto-encoding-quantifying reconstruction fidelity, anatomical coherence, and predictive utility with respect to signalling, microstructural, and metabolic targets. We show that deep auto-encoders yield superior representations across all metrics of performance and target domains, supporting their use as the reference standard for representing transcription patterns in the human brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16113v3</guid>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James K Ruffle, Henry Watkins, Robert J Gray, Harpreet Hyare, Michel Thiebaut de Schotten, Parashkev Nachev</dc:creator>
    </item>
    <item>
      <title>Multi-intention Inverse Q-learning for Interpretable Behavior Representation</title>
      <link>https://arxiv.org/abs/2311.13870</link>
      <description>arXiv:2311.13870v3 Announce Type: replace-cross 
Abstract: In advancing the understanding of natural decision-making processes, inverse reinforcement learning (IRL) methods have proven instrumental in reconstructing animal's intentions underlying complex behaviors. Given the recent development of a continuous-time multi-intention IRL framework, there has been persistent inquiry into inferring discrete time-varying rewards with IRL. To address this challenge, we introduce the class of hierarchical inverse Q-learning (HIQL) algorithms. Through an unsupervised learning process, HIQL divides expert trajectories into multiple intention segments, and solves the IRL problem independently for each. Applying HIQL to simulated experiments and several real animal behavior datasets, our approach outperforms current benchmarks in behavior prediction and produces interpretable reward functions. Our results suggest that the intention transition dynamics underlying complex decision-making behavior is better modeled by a step function instead of a smoothly varying function. This advancement holds promise for neuroscience and cognitive science, contributing to a deeper understanding of decision-making and uncovering underlying brain mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13870v3</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Zhu, Brice De La Crompe, Gabriel Kalweit, Artur Schneider, Maria Kalweit, Ilka Diester, Joschka Boedecker</dc:creator>
    </item>
    <item>
      <title>Action potential propagation properties of 4D, 3D and 2D Hodgkin-Huxley type models</title>
      <link>https://arxiv.org/abs/2402.16711</link>
      <description>arXiv:2402.16711v4 Announce Type: replace-cross 
Abstract: We explore the relationship between sodium (Na$^+$) and potassium (K$^+$) gating variables in the 4-dimensional (4D) Hodgkin-Huxley (HH) electrophysiology model and reduce its complexity by deriving new 3D and 2D models that retain the original model's dynamic properties. The new 3D and 2D models are based on the relationship $h \simeq c - n$ between the gating variables $h$ and $n$ of the 4D HH model, where $c$ is a constant, which suggests an interdependence between the dynamics of Na$^+$ and K$^+$ transmembrane voltage-gated channels. We derive the corresponding cable equations for the three HH-type models and demonstrate that the action potential propagates along the axon at a speed described by $v(R, C_m) = \alpha / (C_m R^{\beta}) = \gamma D^{\beta}$, where $\alpha &gt; 0$, $0 &lt; \beta &lt; 1$, and $\gamma$ are constants independent of the local stimulus intensity, $D$ is the diffusion coefficient of the electric signal along the axon, $C_m$ is the axon transmembrane capacitance, and $R$ is the axon conducting resistivity. The width $w$ of the action potential spikes is inversely related to the resistivity of the axon, with $w = \alpha_2 / R^{\beta_2}$, where $\alpha_2 &gt; 0$ and $\beta_2 &gt; 0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16711v4</guid>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L\'izia Branco, Rui Dil\~ao</dc:creator>
    </item>
  </channel>
</rss>
