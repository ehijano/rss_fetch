<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Sep 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Contributions to Robust and Efficient Methods for Analysis of High Dimensional Data</title>
      <link>https://arxiv.org/abs/2509.08155</link>
      <description>arXiv:2509.08155v1 Announce Type: new 
Abstract: A ubiquitous feature of data of our era is their extra-large sizes and dimensions. Analyzing such high-dimensional data poses significant challenges, since the feature dimension is often much larger than the sample size. This thesis introduces robust and computationally efficient methods to address several common challenges associated with high-dimensional data. In my first manuscript, I propose a coherent approach to variable screening that accommodates nonlinear associations. I develop a novel variable screening method that transcends traditional linear assumptions by leveraging mutual information, with an intended application in neuroimaging data. This approach allows for accurate identification of important variables by capturing nonlinear as well as linear relationships between the outcome and covariates. Building on this foundation, I develop new optimization methods for sparse estimation using nonconvex penalties in my second manuscript. These methods address notable challenges in current statistical computing practices, facilitating computationally efficient and robust analyses of complex datasets. The proposed method can be applied to a general class of optimization problems. In my third manuscript, I contribute to robust modeling of high-dimensional correlated observations by developing a mixed-effects model based on Tsallis power-law entropy maximization and discussed the theoretical properties of such distribution. This model surpasses the constraints of conventional Gaussian models by accommodating a broader class of distributions with enhanced robustness to outliers. Additionally, I develop a proximal nonlinear conjugate gradient algorithm that accelerates convergence while maintaining numerical stability, along with rigorous statistical properties for the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08155v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kai Yang</dc:creator>
    </item>
    <item>
      <title>Convergence and Optimality of the EM Algorithm Under Multi-Component Gaussian Mixture Models</title>
      <link>https://arxiv.org/abs/2509.08237</link>
      <description>arXiv:2509.08237v1 Announce Type: new 
Abstract: Gaussian mixture models (GMMs) are fundamental statistical tools for modeling heterogeneous data. Due to the nonconcavity of the likelihood function, the Expectation-Maximization (EM) algorithm is widely used for parameter estimation of each Gaussian component. Existing analyses of the EM algorithm's convergence to the true parameter focus primarily on either the two-component case or multi-component settings with both known mixing probabilities and known, isotropic covariance matrices. In this work, we establish the minimax optimal rate of convergence of the EM algorithm for multi-component GMMs in full generality. The required separation condition between Gaussian components for EM to converge is the weakest known to date. We develop two distinct analytical approaches, each tailored to a different regime of separation, reflecting two complementary perspectives on the use of EM: parameter estimation and clustering. As a byproduct of our analysis, we show that the EM algorithm, when used for community detection, also achieves the minimax optimal rate of misclustering error under milder separation conditions than spectral clustering and Lloyd's algorithm, an interesting result in its own right. Our analysis allows the number of components, the minimal mixing probabilities, the separation between Gaussian components as well as the dimension to grow with the sample size. Simulation studies corroborate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08237v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Bing, Dehan Kong, Bingqing Li</dc:creator>
    </item>
    <item>
      <title>kNNSampler: Stochastic Imputations for Recovering Missing Value Distributions</title>
      <link>https://arxiv.org/abs/2509.08366</link>
      <description>arXiv:2509.08366v1 Announce Type: cross 
Abstract: We study a missing-value imputation method, termed kNNSampler, that imputes a given unit's missing response by randomly sampling from the observed responses of the $k$ most similar units to the given unit in terms of the observed covariates. This method can sample unknown missing values from their distributions, quantify the uncertainties of missing values, and be readily used for multiple imputation. Unlike popular kNNImputer, which estimates the conditional mean of a missing response given an observed covariate, kNNSampler is theoretically shown to estimate the conditional distribution of a missing response given an observed covariate. Experiments demonstrate its effectiveness in recovering the distribution of missing values. The code for kNNSampler is made publicly available (https://github.com/SAP/knn-sampler).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08366v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parastoo Pashmchi, Jerome Benoit, Motonobu Kanagawa</dc:creator>
    </item>
    <item>
      <title>A transport approach to the cutoff phenomenon</title>
      <link>https://arxiv.org/abs/2509.08560</link>
      <description>arXiv:2509.08560v1 Announce Type: cross 
Abstract: Substantial progress has recently been made in the understanding of the cutoff phenomenon for Markov processes, using an information-theoretic statistics known as varentropy [Sal23; Sal24; Sal25a; PS25]. In the present paper, we propose an alternative approach which bypasses the use of varentropy and exploits instead a new W-TV transport inequality, combined with a classical parabolic regularization estimate [BGL01; OV01]. While currently restricted to non-negatively curved processes on smooth spaces, our argument no longer requires the chain rule, nor any approximate version thereof. As applications, we recover the main result of [Sal25a] establishing cutoff for the log-concave Langevin dynamics, and extend the conclusion to a widely-used discrete-time sampling algorithm known as the Proximal Sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08560v1</guid>
      <category>math.PR</category>
      <category>math.AP</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Pedrotti, Justin Salez</dc:creator>
    </item>
    <item>
      <title>On the Sample Complexity of Set Membership Estimation for Linear Systems with Disturbances Bounded by Convex Sets</title>
      <link>https://arxiv.org/abs/2406.00574</link>
      <description>arXiv:2406.00574v3 Announce Type: replace-cross 
Abstract: This paper revisits the set membership identification for linear control systems and establishes its convergence rates under relaxed assumptions on (i) the persistent excitation requirement and (ii) the system disturbances. In particular, instead of assuming persistent excitation exactly, this paper adopts the block-martingale small-ball condition enabled by randomly perturbed control policies to establish the convergence rates of SME with high probability. Further, we relax the assumptions on the shape of the bounded disturbance set and the boundary-visiting condition. Our convergence rates hold for disturbances bounded by general convex sets, which bridges the gap between the previous convergence analysis for general convex sets and the existing convergence rate analysis for $\ell_\infty$ balls. Further, we validate our convergence rates by several numerical experiments.
  This manuscript contains supplementary content in the Appendix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00574v3</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haonan Xu, Yingying Li</dc:creator>
    </item>
    <item>
      <title>Statistical-Computational Trade-offs for Recursive Adaptive Partitioning Estimators</title>
      <link>https://arxiv.org/abs/2411.04394</link>
      <description>arXiv:2411.04394v3 Announce Type: replace-cross 
Abstract: Models based on recursive adaptive partitioning such as decision trees and their ensembles are popular for high-dimensional regression as they can potentially avoid the curse of dimensionality. Because empirical risk minimization (ERM) is computationally infeasible, these models are typically trained using greedy algorithms. Although effective in many cases, these algorithms have been empirically observed to get stuck at local optima. We explore this phenomenon in the context of learning sparse regression functions over $d$ binary features, showing that when the true regression function $f^*$ does not satisfy Abbe et al. (2022)'s Merged Staircase Property (MSP), greedy training requires $\exp(\Omega(d))$ to achieve low estimation error. Conversely, when $f^*$ does satisfy MSP, greedy training can attain small estimation error with only $O(\log d)$ samples. This dichotomy mirrors that of two-layer neural networks trained with stochastic gradient descent (SGD) in the mean-field regime, thereby establishing a head-to-head comparison between SGD-trained neural networks and greedy recursive partitioning estimators. Furthermore, ERM-trained recursive partitioning estimators achieve low estimation error with $O(\log d)$ samples irrespective of whether $f^*$ satisfies MSP, thereby demonstrating a statistical-computational trade-off for greedy training. Our proofs are based on a novel interpretation of greedy recursive partitioning using stochastic process theory and a coupling technique that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04394v3</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Shuo Tan, Jason M. Klusowski, Krishnakumar Balasubramanian</dc:creator>
    </item>
    <item>
      <title>On the maximal correlation of some stochastic processes</title>
      <link>https://arxiv.org/abs/2411.17109</link>
      <description>arXiv:2411.17109v3 Announce Type: replace-cross 
Abstract: We study the maximal correlation coefficient $R(X,Y)$ between two stochastic processes $X$ and $Y$. In the case when $(X,Y)$ is a random walk, we find $R(X,Y)$ using the Cs\'{a}ki-Fischer identity and the lower semicontinuity of the map $\text{Law}(X,Y) \to R(X,Y)$. When $(X,Y)$ is a two-dimensional L\'{e}vy process, we express $R(X,Y)$ in terms of the L\'{e}vy measure of the process and the covariance matrix of the diffusion part of the process. Consequently, for a two-dimensional $\alpha$-stable random vector $(X,Y)$ with $0&lt;\alpha&lt;2$, we express $R(X,Y)$ in terms of $\alpha$ and the spectral measure $\tau$ of the $\alpha$-stable distribution. We also establish analogs and extensions of the Dembo-Kagan-Shepp-Yu inequality and the Madiman-Barron inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17109v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinshan Chang, Qinwei Chen</dc:creator>
    </item>
    <item>
      <title>Manifolds with kinks and the asymptotic behavior of the graph Laplacian operator with Gaussian kernel</title>
      <link>https://arxiv.org/abs/2507.07751</link>
      <description>arXiv:2507.07751v2 Announce Type: replace-cross 
Abstract: We introduce manifolds with kinks, a class of manifolds with possibly singular boundary that notably contains manifolds with smooth boundary and corners. We derive the asymptotic behavior of the Graph Laplace operator with Gaussian kernel and its deterministic limit on these spaces as bandwidth goes to zero. We show that this asymptotic behavior is determined by the inward sector of the tangent space and, as special cases, we derive its behavior near interior and singular points. Lastly, we show the validity of our theoretical results using numerical simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07751v2</guid>
      <category>math.DG</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susovan Pal, David Tewodrose</dc:creator>
    </item>
    <item>
      <title>Variance Estimation for Weighted Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2508.08167</link>
      <description>arXiv:2508.08167v2 Announce Type: replace-cross 
Abstract: Common variance estimation methods for weighted average treatment effects (WATEs) in observational studies include nonparametric bootstrap and model-based, closed-form sandwich variance estimation. However, the computational cost of bootstrap increases with the size of the data at hand. Besides, some replicates may exhibit random violations of the positivity assumption even when the original data do not. Sandwich variance estimation relies on regularity conditions that may be structurally violated. Moreover, the sandwich variance estimation is model-dependent on the propensity score model, the outcome model, or both; thus it does not have a unified closed-form expression. Recent studies have explored the use of wild bootstrap to estimate the variance of the average treatment effect on the treated (ATT). This technique adopts a one-dimensional, nonparametric, and computationally efficient resampling strategy. In this article, we propose a "post-weighting" bootstrap approach as an alternative to the conventional bootstrap, which helps avoid random positivity violations in replicates and improves computational efficiency. We also generalize the wild bootstrap algorithm from ATT to the broader class of WATEs by providing new justification for correctly accounting for sampling variability from multiple sources under different weighting functions. We evaluate the performance of all four methods through extensive simulation studies and demonstrate their application using data from the National Health and Nutrition Examination Survey (NHANES). Our findings offer several practical recommendations for the variance estimation of WATE estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08167v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huiyue Li, Yi Liu, Yunji Zhou, Jiajun Liu, Dezhao Fu, Roland A. Matsouaka</dc:creator>
    </item>
  </channel>
</rss>
