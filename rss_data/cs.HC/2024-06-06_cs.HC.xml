<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Jun 2024 01:46:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Impacts of Illuminance and Correlated Color Temperature on Cognitive Performance: A VR-Lighting Study</title>
      <link>https://arxiv.org/abs/2406.02728</link>
      <description>arXiv:2406.02728v1 Announce Type: new 
Abstract: This study contributes to the ongoing exploration of methods to enhance the environmental design, cognitive function, and overall wellbeing, primarily focusing on understanding the modulation of human cognitive performance by artificial lighting conditions. In this investigation, participants (N=35) engaged with two distinct architectural contexts, each featuring five different lighting conditions within a virtual environment during specific daytime scenarios. Responding to a series of cognitive memory tests, we measured participant test scores and the corresponding reaction time. The study's findings, particularly in Backward Digit Span Tasks (BDST) and Visual Memory Tasks (VMT), indicate that diverse lighting conditions significantly impacted cognitive performance at different times of the day. Notably, the BDST scores were mainly affected by lighting conditions in the afternoon session, whereas the VMT scores were primarily influenced in the morning sessions. This research offers support for architects and engineers as they develop lighting designs that are sensitive to the cognitive performance of occupants. It highlights the advantages of utilizing VR simulations in the AEC industry to assess the impact of lighting designs on users. Further research can lead to the development of lighting systems that can promote better cognitive function and overall wellbeing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02728v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Armin Mostafavi, Milica Vujovic, Tong Bill Xu, Michael Hensel</dc:creator>
    </item>
    <item>
      <title>ArguMentor: Augmenting User Experiences with Counter-Perspectives</title>
      <link>https://arxiv.org/abs/2406.02795</link>
      <description>arXiv:2406.02795v1 Announce Type: new 
Abstract: Opinion pieces often represent only one side of any story, which can influence users and make them susceptible to confirmation bias and echo chambers in society. Moreover, humans are also bad at reading long articles -- often indulging in idle reading and re-reading. To solve this, we design ArguMentor, an end-to-end system that highlights claims in opinion pieces, generates counter-arguments for them using an LLM, and generates a context-based summary of the passage based on current events. It further enhances user interaction and understanding through additional features like Q&amp;A bot, DebateMe and highlighting trigger windows. Our survey and results show that users can generate more counterarguments and on an average have more neutralized views after engaging with the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02795v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Priya Pitre, Kurt Luther</dc:creator>
    </item>
    <item>
      <title>A Design Experience for Interactive Narrative Based on The User Behavior</title>
      <link>https://arxiv.org/abs/2406.02866</link>
      <description>arXiv:2406.02866v1 Announce Type: new 
Abstract: Research on interactive narrative experiences in physical spaces is becoming more popular, growing into an established new media art format with the development of technology and evolution of audience aesthetics. However, the methods of designing interactive narratives are still similar to the basic video narratology of traditional designers, directors, and producers. This paper provides a design method based on the user's physical behavior and proposes an art installation by this method, where the aim of the installation is to transmit a more vivid story to users, presenting a new research inspiration of interactive narratology for designers and researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02866v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Cumulus Conference Proceedings Roma 2021</arxiv:journal_reference>
      <dc:creator>Yuan Yao, Haipeng Mi</dc:creator>
    </item>
    <item>
      <title>Temperature Illusions in Mixed Reality using Color and Dynamic Graphics</title>
      <link>https://arxiv.org/abs/2406.03241</link>
      <description>arXiv:2406.03241v1 Announce Type: new 
Abstract: Sensory illusions - where a sensory stimulus causes people to perceive effects that are altered by a different sensory stimulus - have the potential to enrich mixed-reality based interactions. The well-known colour-temperature illusion is a sensory illusion that causes people to, somewhat counterintuitively, perceive blue objects to feel warmer and red objects to feel colder. There is currently little information about whether this illusion can be recreated in mixed reality (MR). Additionally, it is unknown whether dynamic graphical effects made possible by mixed-reality systems could create a similar or potentially stronger effect to the color-temperature illusion. The results of our study (n=30) support that the color-temperature illusion can be recreated in MR and that dynamic graphics can create a new temperature-sensory illusion. Our dynamic-graphics-temperature illusion creates a stronger effect than the color-temperature illusion and has more intuitive relationship between the stimulus and the effect: cold graphical effects (a virtual ice ball) are perceived as colder and hot graphical effects (a virtual fire ball) as hotter. Our results demonstrate that mixed reality has the potential to create novel and stronger temperature-based illusions and encourage further investigation into graphical effects to shape user perception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03241v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of Graphics Interface 2023. Victoria, Canada. 6 pages</arxiv:journal_reference>
      <dc:creator>Connor Wilson, Daniel J. Rea, Scott Bateman</dc:creator>
    </item>
    <item>
      <title>Reconfiguring Participatory Design to Resist AI Realism</title>
      <link>https://arxiv.org/abs/2406.03245</link>
      <description>arXiv:2406.03245v1 Announce Type: new 
Abstract: The growing trend of artificial intelligence (AI) as a solution to social and technical problems reinforces AI Realism -- the belief that AI is an inevitable and natural order. In response, this paper argues that participatory design (PD), with its focus on democratic values and processes, can play a role in questioning and resisting AI Realism. I examine three concerning aspects of AI Realism: the facade of democratization that lacks true empowerment, demands for human adaptability in contrast to AI systems' inflexibility, and the obfuscation of essential human labor enabling the AI system. I propose resisting AI Realism by reconfiguring PD to continue engaging with value-centered visions, increasing its exploration of non-AI alternatives, and making the essential human labor underpinning AI systems visible. I position PD as a means to generate friction against AI Realism and open space for alternative futures centered on human needs and values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03245v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3661455.3669867</arxiv:DOI>
      <arxiv:journal_reference>Participatory Design Conference 2024</arxiv:journal_reference>
      <dc:creator>Aakash Gautam</dc:creator>
    </item>
    <item>
      <title>Save It for the "Hot" Day: An LLM-Empowered Visual Analytics System for Heat Risk Management</title>
      <link>https://arxiv.org/abs/2406.03317</link>
      <description>arXiv:2406.03317v1 Announce Type: new 
Abstract: The escalating frequency and intensity of heat-related climate events, particularly heatwaves, emphasize the pressing need for advanced heat risk management strategies. Current approaches, primarily relying on numerical models, face challenges in spatial-temporal resolution and in capturing the dynamic interplay of environmental, social, and behavioral factors affecting heat risks. This has led to difficulties in translating risk assessments into effective mitigation actions. Recognizing these problems, we introduce a novel approach leveraging the burgeoning capabilities of Large Language Models (LLMs) to extract rich and contextual insights from news reports. We hence propose an LLM-empowered visual analytics system, Havior, that integrates the precise, data-driven insights of numerical models with nuanced news report information. This hybrid approach enables a more comprehensive assessment of heat risks and better identification, assessment, and mitigation of heat-related threats. The system incorporates novel visualization designs, such as "thermoglyph" and news glyph, enhancing intuitive understanding and analysis of heat risks. The integration of LLM-based techniques also enables advanced information retrieval and semantic knowledge extraction that can be guided by experts' analytics needs. Our case studies on two cities that faced significant heatwave events and interviews with five experts have demonstrated the usefulness of our system in providing in-depth and actionable insights for heat risk management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03317v1</guid>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haobo Li, Wong Kam-Kwai, Yan Luo, Juntong Chen, Chengzhong Liu, Yaxuan Zhang, Alexis Kai Hon Lau, Huamin Qu, Dongyu Liu</dc:creator>
    </item>
    <item>
      <title>RemixTape: Enriching Narratives about Metrics with Semantic Alignment and Contextual Recommendation</title>
      <link>https://arxiv.org/abs/2406.03415</link>
      <description>arXiv:2406.03415v1 Announce Type: new 
Abstract: The temporal dynamics of quantitative metrics or key performance indicators (KPIs) are central to decision making within enterprise organizations. Recently, major business intelligence providers have introduced new infrastructure for defining, sharing, and monitoring metric values. However, these values are often presented in isolation and appropriate context is seldom externalized. In this design study, we present RemixTape, an application for constructing structured narratives around metrics. With design imperatives grounded in an formative interview study, RemixTape provides a hierarchical canvas for collecting and coordinating sequences of line chart representations of metrics, along with the ability to externalize situational context around them. RemixTape incorporates affordances to semantically align and annotate juxtaposed charts and text, as well as recommendations of complementary charts based on metrics already present on the canvas. We evaluated RemixTape in a user study in which six enterprise data professionals reproduced and extended partial narratives, with participants appreciating RemixTape as a novel alternative to dashboards, galleries, and slide presentations for supporting conversations about metrics. We conclude with a reflection on how aspects of RemixTape could generalize beyond metrics, with a call to define a conceptual foundation for remixing in the context of visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03415v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Brehmer, Margaret Drouhard, Arjun Srinivasan</dc:creator>
    </item>
    <item>
      <title>Cluster-to-Predict Affect Contours from Speech</title>
      <link>https://arxiv.org/abs/2406.02569</link>
      <description>arXiv:2406.02569v1 Announce Type: cross 
Abstract: Continuous emotion recognition (CER) aims to track the dynamic changes in a person's emotional state over time. This paper proposes a novel approach to translating CER into a prediction problem of dynamic affect-contour clusters from speech, where the affect-contour is defined as the contour of annotated affect attributes in a temporal window. Our approach defines a cluster-to-predict (C2P) framework that learns affect-contour clusters, which are predicted from speech with higher precision. To achieve this, C2P runs an unsupervised iterative optimization process to learn affect-contour clusters by minimizing both clustering loss and speech-driven affect-contour prediction loss. Our objective findings demonstrate the value of speech-driven clustering for both arousal and valence attributes. Experiments conducted on the RECOLA dataset yielded promising classification results, with F1 scores of 0.84 for arousal and 0.75 for valence in our four-class speech-driven affect-contour prediction model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02569v1</guid>
      <category>eess.AS</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>G\"okhan Ku\c{s}\c{c}u, Engin Erzin</dc:creator>
    </item>
    <item>
      <title>Immersive Robot Programming Interface for Human-Guided Automation and Randomized Path Planning</title>
      <link>https://arxiv.org/abs/2406.02799</link>
      <description>arXiv:2406.02799v1 Announce Type: cross 
Abstract: Researchers are exploring Augmented Reality (AR) interfaces for online robot programming to streamline automation and user interaction in variable manufacturing environments. This study introduces an AR interface for online programming and data visualization that integrates the human in the randomized robot path planning, reducing the inherent randomness of the methods with human intervention. The interface uses holographic items which correspond to physical elements to interact with a redundant manipulator. Utilizing Rapidly Random Tree Star (RRT*) and Spherical Linear Interpolation (SLERP) algorithms, the interface achieves end-effector s progression through collision-free path with smooth rotation. Next, Sequential Quadratic Programming (SQP) achieve robot s configurations for this progression. The platform executes the RRT* algorithm in a loop, with each iteration independently exploring the shortest path through random sampling, leading to variations in the optimized paths produced. These paths are then demonstrated to AR users, who select the most appropriate path based on the environmental context and their intuition. The accuracy and effectiveness of the interface are validated through its implementation and testing with a seven Degree-OF-Freedom (DOF) manipulator, indicating its potential to advance current practices in robot programming. The validation of this paper include two implementations demonstrating the value of human-in-the-loop and context awareness in robotics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02799v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaveh Malek, Claus Danielson, Fernando Moreu</dc:creator>
    </item>
    <item>
      <title>The Task-oriented Queries Benchmark (ToQB)</title>
      <link>https://arxiv.org/abs/2406.02943</link>
      <description>arXiv:2406.02943v1 Announce Type: cross 
Abstract: Task-oriented queries (e.g., one-shot queries to play videos, order food, or call a taxi) are crucial for assessing the quality of virtual assistants, chatbots, and other large language model (LLM)-based services. However, a standard benchmark for task-oriented queries is not yet available, as existing benchmarks in the relevant NLP (Natural Language Processing) fields have primarily focused on task-oriented dialogues. Thus, we present a new methodology for efficiently generating the Task-oriented Queries Benchmark (ToQB) using existing task-oriented dialogue datasets and an LLM service. Our methodology involves formulating the underlying NLP task to summarize the original intent of a speaker in each dialogue, detailing the key steps to perform the devised NLP task using an LLM service, and outlining a framework for automating a major part of the benchmark generation process. Through a case study encompassing three domains (i.e., two single-task domains and one multi-task domain), we demonstrate how to customize the LLM prompts (e.g., omitting system utterances or speaker labels) for those three domains and characterize the generated task-oriented queries. The generated ToQB dataset is made available to the public. We further discuss new domains that can be added to ToQB by community contributors and its practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02943v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keun Soo Yim</dc:creator>
    </item>
    <item>
      <title>The Impossibility of Fair LLMs</title>
      <link>https://arxiv.org/abs/2406.03198</link>
      <description>arXiv:2406.03198v1 Announce Type: cross 
Abstract: The need for fair AI is increasingly clear in the era of general-purpose systems such as ChatGPT, Gemini, and other large language models (LLMs). However, the increasing complexity of human-AI interaction and its social impacts have raised questions of how fairness standards could be applied. Here, we review the technical frameworks that machine learning researchers have used to evaluate fairness, such as group fairness and fair representations, and find that their application to LLMs faces inherent limitations. We show that each framework either does not logically extend to LLMs or presents a notion of fairness that is intractable for LLMs, primarily due to the multitudes of populations affected, sensitive attributes, and use cases. To address these challenges, we develop guidelines for the more realistic goal of achieving fairness in particular use cases: the criticality of context, the responsibility of LLM developers, and the need for stakeholder participation in an iterative process of design and evaluation. Moreover, it may eventually be possible and even necessary to use the general-purpose capabilities of AI systems to address fairness challenges as a form of scalable AI-assisted alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03198v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacy Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Alexander D'Amour, Chenhao Tan</dc:creator>
    </item>
    <item>
      <title>SelfReDepth: Self-Supervised Real-Time Depth Restoration for Consumer-Grade Sensors</title>
      <link>https://arxiv.org/abs/2406.03388</link>
      <description>arXiv:2406.03388v1 Announce Type: cross 
Abstract: Depth maps produced by consumer-grade sensors suffer from inaccurate measurements and missing data from either system or scene-specific sources. Data-driven denoising algorithms can mitigate such problems. However, they require vast amounts of ground truth depth data. Recent research has tackled this limitation using self-supervised learning techniques, but it requires multiple RGB-D sensors. Moreover, most existing approaches focus on denoising single isolated depth maps or specific subjects of interest, highlighting a need for methods to effectively denoise depth maps in real-time dynamic environments. This paper extends state-of-the-art approaches for depth-denoising commodity depth devices, proposing SelfReDepth, a self-supervised deep learning technique for depth restoration, via denoising and hole-filling by inpainting full-depth maps captured with RGB-D sensors. The algorithm targets depth data in video streams, utilizing multiple sequential depth frames coupled with color data to achieve high-quality depth videos with temporal coherence. Finally, SelfReDepth is designed to be compatible with various RGB-D sensors and usable in real-time scenarios as a pre-processing step before applying other depth-dependent algorithms. Our results demonstrate our approach's real-time performance on real-world datasets. They show that it outperforms state-of-the-art denoising and restoration performance at over 30fps on Commercial Depth Cameras, with potential benefits for augmented and mixed-reality applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03388v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Real-Time Image Processing 2024</arxiv:journal_reference>
      <dc:creator>Alexandre Duarte, Francisco Fernandes, Jo\~ao M. Pereira, Catarina Moreira, Jacinto C. Nascimento, Joaquim Jorge</dc:creator>
    </item>
    <item>
      <title>Improving Users' Passwords with DPAR: a Data-driven Password Recommendation System</title>
      <link>https://arxiv.org/abs/2406.03423</link>
      <description>arXiv:2406.03423v1 Announce Type: cross 
Abstract: Passwords are the primary authentication method online, but even with password policies and meters, users still find it hard to create strong and memorable passwords. In this paper, we propose DPAR: a Data-driven PAssword Recommendation system based on a dataset of 905 million leaked passwords. DPAR generates password recommendations by analyzing the user's given password and suggesting specific tweaks that would make it stronger while still keeping it memorable and similar to the original password. We conducted two studies to evaluate our approach: verifying the memorability of generated passwords (n=317), and evaluating the strength and recall of DPAR recommendations against password meters (n=441). In a randomized experiment, we show that DPAR increased password strength by 34.8 bits on average and did not significantly affect the ability to recall their password. Furthermore, 36.6% of users accepted DPAR's recommendations verbatim. We discuss our findings and their implications for enhancing password management with recommendation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03423v1</guid>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Assaf Morag, Liron David, Eran Toch, Avishai Wool</dc:creator>
    </item>
    <item>
      <title>How many labelers do you have? A closer look at gold-standard labels</title>
      <link>https://arxiv.org/abs/2206.12041</link>
      <description>arXiv:2206.12041v2 Announce Type: replace-cross 
Abstract: The construction of most supervised learning datasets revolves around collecting multiple labels for each instance, then aggregating the labels to form a type of "gold-standard". We question the wisdom of this pipeline by developing a (stylized) theoretical model of this process and analyzing its statistical consequences, showing how access to non-aggregated label information can make training well-calibrated models more feasible than it is with gold-standard labels. The entire story, however, is subtle, and the contrasts between aggregated and fuller label information depend on the particulars of the problem, where estimators that use aggregated information exhibit robust but slower rates of convergence, while estimators that can effectively leverage all labels converge more quickly if they have fidelity to (or can learn) the true labeling process. The theory makes several predictions for real-world datasets, including when non-aggregate labels should improve learning performance, which we test to corroborate the validity of our predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.12041v2</guid>
      <category>math.ST</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Cheng, Hilal Asi, John Duchi</dc:creator>
    </item>
    <item>
      <title>Visualization for Recommendation Explainability: A Survey and New Perspectives</title>
      <link>https://arxiv.org/abs/2305.11755</link>
      <description>arXiv:2305.11755v3 Announce Type: replace-cross 
Abstract: Providing system-generated explanations for recommendations represents an important step towards transparent and trustworthy recommender systems. Explainable recommender systems provide a human-understandable rationale for their outputs. Over the last two decades, explainable recommendation has attracted much attention in the recommender systems research community. This paper aims to provide a comprehensive review of research efforts on visual explanation in recommender systems. More concretely, we systematically review the literature on explanations in recommender systems based on four dimensions, namely explanation goal, explanation scope, explanation style, and explanation format. Recognizing the importance of visualization, we approach the recommender system literature from the angle of explanatory visualizations, that is using visualizations as a display style of explanation. As a result, we derive a set of guidelines that might be constructive for designing explanatory visualizations in recommender systems and identify perspectives for future work in this field. The aim of this review is to help recommendation researchers and practitioners better understand the potential of visually explainable recommendation research and to support them in the systematic design of visual explanations in current and future recommender systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.11755v3</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohamed Amine Chatti, Mouadh Guesmi, Arham Muslim</dc:creator>
    </item>
    <item>
      <title>Large Language Models Can Infer Psychological Dispositions of Social Media Users</title>
      <link>https://arxiv.org/abs/2309.08631</link>
      <description>arXiv:2309.08631v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate increasingly human-like abilities across a wide variety of tasks. In this paper, we investigate whether LLMs like ChatGPT can accurately infer the psychological dispositions of social media users and whether their ability to do so varies across socio-demographic groups. Specifically, we test whether GPT-3.5 and GPT-4 can derive the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r = .29 (range = [.22, .33]) between LLM-inferred and self-reported trait scores - a level of accuracy that is similar to that of supervised machine learning models specifically trained to infer personality. Our findings also highlight heterogeneity in the accuracy of personality inferences across different age groups and gender categories: predictions were found to be more accurate for women and younger individuals on several traits, suggesting a potential bias stemming from the underlying training data or differences in online self-expression. The ability of LLMs to infer psychological dispositions from user-generated text has the potential to democratize access to cheap and scalable psychometric assessments for both researchers and practitioners. On the one hand, this democratization might facilitate large-scale research of high ecological validity and spark innovation in personalized services. On the other hand, it also raises ethical concerns regarding user privacy and self-determination, highlighting the need for stringent ethical frameworks and regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08631v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heinrich Peters, Sandra Matz</dc:creator>
    </item>
  </channel>
</rss>
