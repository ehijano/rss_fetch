<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Sep 2025 01:56:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Open Political Corpora: Structuring, Searching, and Analyzing Political Text Collections with PoliCorp</title>
      <link>https://arxiv.org/abs/2509.17465</link>
      <description>arXiv:2509.17465v1 Announce Type: new 
Abstract: In this work, we present PoliCorp (https://demo-pollux.gesis.org/), a web portal designed to facilitate the search and analysis of political text corpora. PoliCorp provides researchers with access to rich textual data, enabling in-depth analysis of parliamentary discourse over time. The platform currently features a collection of transcripts from debates in the German parliament, spanning 76 years of proceedings. With the advanced search functionality, researchers can apply logical operations to combine or exclude search criteria, making it easier to filter through vast amounts of parliamentary debate data. The search can be customised by combining multiple fields and applying logical operators to uncover complex patterns and insights within the data. Additional data processing steps were performed to enable web-based search and incorporate extra features. A key feature that differentiates PoliCorp is its intuitive web-based interface that enables users to query processed political texts without requiring programming skills. The user-friendly platform allows for the creation of custom subcorpora via search parameters, which can be freely downloaded in JSON format for further analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17465v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nina Smirnova, Muhammad Ahsan Shahid, Philipp Mayr</dc:creator>
    </item>
    <item>
      <title>Survivors, Complainers, and Borderliners: Upward Bias in Online Discussions of Academic Conference Reviews</title>
      <link>https://arxiv.org/abs/2509.16831</link>
      <description>arXiv:2509.16831v1 Announce Type: cross 
Abstract: Online discussion platforms, such as community Q&amp;A sites and forums, have become important hubs where academic conference authors share and seek information about the peer review process and outcomes. However, these discussions involve only a subset of all submissions, raising concerns about the representativeness of the self-reported review scores. In this paper, we conduct a systematic study comparing the review score distributions of self-reported submissions in online discussions (based on data collected from Zhihu and Reddit) with those of all submissions. We reveal a consistent upward bias: the score distribution of self-reported samples is shifted upward relative to the population score distribution, with this difference statistically significant in most cases. Our analysis identifies three distinct contributors to this bias: (1) survivors, authors of accepted papers who are more likely to share good results than those of rejected papers who tend to conceal bad ones; (2) complainers, authors of high-scoring rejected papers who are more likely to voice complaints about the peer review process or outcomes than those of low scores; and (3) borderliners, authors with borderline scores who face greater uncertainty prior to decision announcements and are more likely to seek advice during the rebuttal period. These findings have important implications for how information seekers should interpret online discussions of academic conference reviews.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16831v1</guid>
      <category>cs.SI</category>
      <category>cs.DL</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hangxiao Zhu, Yian Yin, Yu Zhang</dc:creator>
    </item>
    <item>
      <title>Archiving and Replaying Current Web Advertisements: Challenges and Opportunities</title>
      <link>https://arxiv.org/abs/2502.01525</link>
      <description>arXiv:2502.01525v3 Announce Type: replace 
Abstract: Although web advertisements represent an inimitable part of digital cultural heritage, serious archiving and replay challenges persist. To explore these challenges, we created a dataset of 279 archived ads. We encountered five problems in archiving and replaying them. For one, prior to August 2023, Internet Archive's Save Page Now service excluded not only well-known ad services' ads, but also URLs with ad related file and directory names. Although after August 2023, Save Page Now still blocked the archiving of ads loaded on a web page, it permitted the archiving of an ad's resources if the user directly archived the URL(s) associated with the ad. Second, Brozzler's incompatibility with Chrome prevented ads from being archived. Third, during crawling and replay sessions, Google's and Amazon's ad scripts generated URLs with different random values. This precluded archived ads' replay. Updating replay systems' fuzzy matching approach should enable the replay of these ads. Fourth, when loading Flashtalking web page ads outside of ad iframes, the ad script requested a non-existent URL. This, prevented the replay of ad resources. But as was the case with Google and Amazon ads, updating replay systems' fuzzy matching approach should enable Flashtalking ads' replay. Finally, successful replay of ads loaded in iframes with the src attribute of "about:blank" depended upon a given browser's service worker implementation. A Chromium bug stopped service workers from accessing resources inside of this type of iframe, which in turn prevented replay. Replacing the "about:blank" value for the iframe's src attribute with a blob URL before an ad was loaded solved this problem. Resolving these replay problems will improve the replay of ads and other dynamically loaded embedded web resources that use random values or "about:blank" iframes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01525v3</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Travis Reid, Alex H. Poole, Hyung Wook Choi, Christopher Rauch, Mat Kelly, Michael L. Nelson, Michele C. Weigle</dc:creator>
    </item>
    <item>
      <title>Exploring the dynamics of external and self-citations and their role in shaping scientific impact</title>
      <link>https://arxiv.org/abs/2503.09811</link>
      <description>arXiv:2503.09811v2 Announce Type: replace 
Abstract: Understanding the mechanisms driving the distribution of scientific citations is a key challenge in assessing the scientific impact of authors. We investigate the influence of the preferential attachment rule (PAR) in this process by analysing individual citation events from the DBLP dataset and two Scopus-based datasets, enabling us to estimate the probability of citations being assigned preferentially. Our findings reveal that, for the aggregated dataset, PAR dominates the citation distribution process, with approximately 70% of citations adhering to this mechanism. However, analysis at the individual level shows significant variability, with some authors experiencing a greater prevalence of preferential citations, particularly in the context of external citations. In contrast, self-citations exhibit notably different behaviour, with only 20% following PAR. We also demonstrate that the prominence of PAR increases with an author's citability (average citations per paper), suggesting that more citable authors are preferentially cited, while less-cited authors experience more random citation patterns. Furthermore, we show that self-citations may influence bibliometric indices, such as the h-index. Our results confirm the distinct dynamics of self-citations compared to external citations, raising questions about the mechanisms driving self-citation patterns. These findings provide new insights into citation behaviours and highlight the limitations of existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09811v2</guid>
      <category>cs.DL</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej J. Mrowinski, Aleksandra Buczek, Agata Fronczak</dc:creator>
    </item>
    <item>
      <title>BAGELS: Benchmarking the Automated Generation and Extraction of Limitations from Scholarly Text</title>
      <link>https://arxiv.org/abs/2505.18207</link>
      <description>arXiv:2505.18207v2 Announce Type: replace 
Abstract: In scientific research, ``limitations'' refer to the shortcomings, constraints, or weaknesses of a study. A transparent reporting of such limitations can enhance the quality and reproducibility of research and improve public trust in science. However, authors often underreport limitations in their papers and rely on hedging strategies to meet editorial requirements at the expense of readers' clarity and confidence. This tendency, combined with the surge in scientific publications, has created a pressing need for automated approaches to extract and generate limitations from scholarly papers. To address this need, we present a full architecture for computational analysis of research limitations. Specifically, we (1) create a dataset of limitations from ACL, NeurIPS, and PeerJ papers by extracting them from the text and supplementing them with external reviews; (2) we propose methods to automatically generate limitations using a novel Retrieval Augmented Generation (RAG) technique; (3) we design a fine-grained evaluation framework for generated limitations, along with a meta-evaluation of these techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18207v2</guid>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ibrahim Al Azher, Miftahul Jannat Mokarrama, Zhishuai Guo, Sagnik Ray Choudhury, Hamed Alhoori</dc:creator>
    </item>
    <item>
      <title>LEMUR Neural Network Dataset: Towards Seamless AutoML</title>
      <link>https://arxiv.org/abs/2504.10552</link>
      <description>arXiv:2504.10552v3 Announce Type: replace-cross 
Abstract: Neural networks have become the backbone of modern AI, yet designing, evaluating, and comparing them remains labor-intensive. While many datasets exist for training models, there are few standardized collections of the models themselves. We present LEMUR, an open-source dataset and framework that brings together a large collection of PyTorch-based neural networks across tasks such as classification, segmentation, detection, and natural language processing. Each model follows a common template, with configurations and results logged in a structured database to ensure consistency and reproducibility. LEMUR integrates Optuna for automated hyperparameter optimization, provides statistical analysis and visualization tools, and exposes an API for seamless access to performance data. The framework also supports extensibility, enabling researchers to add new models, datasets, or metrics without breaking compatibility. By standardizing implementations and unifying evaluation, LEMUR aims to accelerate AutoML research, facilitate fair benchmarking, and lower the barrier to large-scale neural network experimentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10552v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.DL</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arash Torabi Goodarzi, Roman Kochnev, Waleed Khalid, Hojjat Torabi Goudarzi, Furui Qin, Tolgay Atinc Uzun, Yashkumar Sanjaybhai Dhameliya, Yash Kanubhai Kathiriya, Zofia Antonina Bentyn, Dmitry Ignatov, Radu Timofte</dc:creator>
    </item>
    <item>
      <title>SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP</title>
      <link>https://arxiv.org/abs/2509.07801</link>
      <description>arXiv:2509.07801v3 Announce Type: replace-cross 
Abstract: Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP--a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at: https://github.com/AKADDC/SciNLP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07801v3</guid>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Decheng Duan, Yingyi Zhang, Jitong Peng, Chengzhi Zhang</dc:creator>
    </item>
  </channel>
</rss>
