<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Jun 2024 04:00:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Multivariate Equivalence Test Based on Mahalanobis Distance with a Data-Driven Margin</title>
      <link>https://arxiv.org/abs/2406.03596</link>
      <description>arXiv:2406.03596v1 Announce Type: new 
Abstract: Multivariate equivalence testing is needed in a variety of scenarios for drug development. For example, drug products obtained from natural sources may contain many components for which the individual effects and/or their interactions on clinical efficacy and safety cannot be completely characterized. Such lack of sufficient characterization poses a challenge for both generic drug developers to demonstrate and regulatory authorities to determine the sameness of a proposed generic product to its reference product. Another case is to ensure batch-to-batch consistency of naturally derived products containing a vast number of components, such as botanical products. The equivalence or sameness between products containing many components that cannot be individually evaluated needs to be studied in a holistic manner. Multivariate equivalence test based on Mahalanobis distance may be suitable to evaluate many variables holistically. Existing studies based on such method assumed either a predetermined constant margin, for which a consensus is difficult to achieve, or a margin derived from the data, where, however, the randomness is ignored during the testing. In this study, we propose a multivariate equivalence test based on Mahalanobis distance with a data-drive margin with the randomness in the margin considered. Several possible implementations are compared with existing approaches via extensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03596v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Wang, Yu-Ting Weng, Shaobo Liu, Tengfei Li, Meiyu Shen, Yi Tsong</dc:creator>
    </item>
    <item>
      <title>Multiscale Tests for Point Processes and Longitudinal Networks</title>
      <link>https://arxiv.org/abs/2406.03681</link>
      <description>arXiv:2406.03681v1 Announce Type: new 
Abstract: We propose a new testing framework applicable to both the two-sample problem on point processes and the community detection problem on rectangular arrays of point processes, which we refer to as longitudinal networks; the latter problem is useful in situations where we observe interactions among a group of individuals over time. Our framework is based on a multiscale discretization scheme that consider not just the global null but also a collection of nulls local to small regions in the domain; in the two-sample problem, the local rejections tell us where the intensity functions differ and in the longitudinal network problem, the local rejections tell us when the community structure is most salient. We provide theoretical analysis for the two-sample problem and show that our method has minimax optimal power under a Holder continuity condition. We provide extensive simulation and real data analysis demonstrating the practicality of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03681v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Youmeng Jiang, Min Xu</dc:creator>
    </item>
    <item>
      <title>Small area estimation with generalized random forests: Estimating poverty rates in Mexico</title>
      <link>https://arxiv.org/abs/2406.03861</link>
      <description>arXiv:2406.03861v1 Announce Type: new 
Abstract: Identifying and addressing poverty is challenging in administrative units with limited information on income distribution and well-being. To overcome this obstacle, small area estimation methods have been developed to provide reliable and efficient estimators at disaggregated levels, enabling informed decision-making by policymakers despite the data scarcity. From a theoretical perspective, we propose a robust and flexible approach for estimating poverty indicators based on binary response variables within the small area estimation context: the generalized mixed effects random forest. Our method employs machine learning techniques to identify predictive, non-linear relationships from data, while also modeling hierarchical structures. Mean squared error estimation is explored using a parametric bootstrap. From an applied perspective, we examine the impact of information loss due to converting continuous variables into binary variables on the performance of small area estimation methods. We evaluate the proposed point and uncertainty estimates in both model- and design-based simulations. Finally, we apply our method to a case study revealing spatial patterns of poverty in the Mexican state of Tlaxcala.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03861v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Frink, Timo Schmid</dc:creator>
    </item>
    <item>
      <title>Enhanced variable selection for boosting sparser and less complex models in distributional copula regression</title>
      <link>https://arxiv.org/abs/2406.03900</link>
      <description>arXiv:2406.03900v1 Announce Type: new 
Abstract: Structured additive distributional copula regression allows to model the joint distribution of multivariate outcomes by relating all distribution parameters to covariates. Estimation via statistical boosting enables accounting for high-dimensional data and incorporating data-driven variable selection, both of which are useful given the complexity of the model class. However, as known from univariate (distributional) regression, the standard boosting algorithm tends to select too many variables with minor importance, particularly in settings with large sample sizes, leading to complex models with difficult interpretation. To counteract this behavior and to avoid selecting base-learners with only a negligible impact, we combined the ideas of probing, stability selection and a new deselection approach with statistical boosting for distributional copula regression. In a simulation study and an application to the joint modelling of weight and length of newborns, we found that all proposed methods enhance variable selection by reducing the number of false positives. However, only stability selection and the deselection approach yielded similar predictive performance to classical boosting. Finally, the deselection approach is better scalable to larger datasets and led to a competitive predictive performance, which we further illustrated in a genomic cohort study from the UK Biobank by modelling the joint genetic predisposition for two phenotypes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03900v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annika Str\"omer, Nadja Klein, Christian Staerk, Florian Faschingbauer, Hannah Klinkhammer, Andreas Mayr</dc:creator>
    </item>
    <item>
      <title>Variational Prior Replacement in Bayesian Inference and Inversion</title>
      <link>https://arxiv.org/abs/2406.04072</link>
      <description>arXiv:2406.04072v1 Announce Type: new 
Abstract: Many scientific investigations require that the values of a set of model parameters are estimated using recorded data. In Bayesian inference, information from both observed data and prior knowledge is combined to update model parameters probabilistically. Prior information represents our belief about the range of values that the variables can take, and their relative probabilities when considered independently of recorded data. Situations arise in which we wish to change prior information: (i) the subjective nature of prior information, (ii) cases in which we wish to test different states of prior information as hypothesis tests, and (iii) information from new studies may emerge so prior information may evolve over time. Estimating the solution to any single inference problem is usually computationally costly, as it typically requires thousands of model samples and their forward simulations. Therefore, recalculating the Bayesian solution every time prior information changes can be extremely expensive. We develop a mathematical formulation that allows prior information to be changed in a solution using variational methods, without performing Bayesian inference on each occasion. In this method, existing prior information is removed from a previously obtained posterior distribution and is replaced by new prior information. We therefore call the methodology variational prior replacement (VPR). We demonstrate VPR using a 2D seismic full waveform inversion example, where VPR provides almost identical posterior solutions compared to those obtained by solving independent inference problems using different priors. The former can be completed within minutes even on a laptop whereas the latter requires days of computations using high-performance computing resources. We demonstrate the value of the method by comparing the posterior solutions obtained using three different types of prior information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04072v1</guid>
      <category>stat.ME</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.geo-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuebin Zhao, Andrew Curtis</dc:creator>
    </item>
    <item>
      <title>Why recommended visit intervals should be extracted when conducting longitudinal analyses using electronic health record data: examining visit mechanism and sensitivity to assessment not at random</title>
      <link>https://arxiv.org/abs/2406.04077</link>
      <description>arXiv:2406.04077v1 Announce Type: new 
Abstract: Electronic health records (EHRs) provide an efficient approach to generating rich longitudinal datasets. However, since patients visit as needed, the assessment times are typically irregular and may be related to the patient's health. Failing to account for this informative assessment process could result in biased estimates of the disease course. In this paper, we show how estimation of the disease trajectory can be enhanced by leveraging an underutilized piece of information that is often in the patient's EHR: physician-recommended intervals between visits. Specifically, we demonstrate how recommended intervals can be used in characterizing the assessment process, and in investigating the sensitivity of the results to assessment not at random (ANAR). We illustrate our proposed approach in a clinic-based cohort study of juvenile dermatomyositis (JDM). In this study, we found that the recommended intervals explained 78% of the variability in the assessment times. Under a specific case of ANAR where we assumed that a worsening in disease led to patients visiting earlier than recommended, the estimated population average disease activity trajectory was shifted downward relative to the trajectory assuming assessment at random. These results demonstrate the crucial role recommended intervals play in improving the rigour of the analysis by allowing us to assess both the plausibility of the AAR assumption and the sensitivity of the results to departures from this assumption. Thus, we advise that studies using irregular longitudinal data should extract recommended visit intervals and follow our procedure for incorporating them into analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04077v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rose Garrett, Masum Patel, Brian Feldman, Eleanor Pullenayegum</dc:creator>
    </item>
    <item>
      <title>Copula-based models for correlated circular data</title>
      <link>https://arxiv.org/abs/2406.04085</link>
      <description>arXiv:2406.04085v1 Announce Type: new 
Abstract: We exploit Gaussian copulas to specify a class of multivariate circular distributions and obtain parametric models for the analysis of correlated circular data. This approach provides a straightforward extension of traditional multivariate normal models to the circular setting, without imposing restrictions on the marginal data distribution nor requiring overwhelming routines for parameter estimation. The proposal is illustrated on two case studies of animal orientation and sea currents, where we propose an autoregressive model for circular time series and a geostatistical model for circular spatial series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04085v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Lagona, Marco Mingione</dc:creator>
    </item>
    <item>
      <title>A novel robust meta-analysis model using the $t$ distribution for outlier accommodation and detection</title>
      <link>https://arxiv.org/abs/2406.04150</link>
      <description>arXiv:2406.04150v1 Announce Type: new 
Abstract: Random effects meta-analysis model is an important tool for integrating results from multiple independent studies. However, the standard model is based on the assumption of normal distributions for both random effects and within-study errors, making it susceptible to outlying studies. Although robust modeling using the $t$ distribution is an appealing idea, the existing work, that explores the use of the $t$ distribution only for random effects, involves complicated numerical integration and numerical optimization. In this paper, a novel robust meta-analysis model using the $t$ distribution is proposed ($t$Meta). The novelty is that the marginal distribution of the effect size in $t$Meta follows the $t$ distribution, enabling that $t$Meta can simultaneously accommodate and detect outlying studies in a simple and adaptive manner. A simple and fast EM-type algorithm is developed for maximum likelihood estimation. Due to the mathematical tractability of the $t$ distribution, $t$Meta frees from numerical integration and allows for efficient optimization. Experiments on real data demonstrate that $t$Meta is compared favorably with related competitors in situations involving mild outliers. Moreover, in the presence of gross outliers, while related competitors may fail, $t$Meta continues to perform consistently and robustly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04150v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Wang, Jianhua Zhao, Fen Jiang, Lei Shi, Jianxin Pan</dc:creator>
    </item>
    <item>
      <title>Comparing estimators of discriminative performance of time-to-event models</title>
      <link>https://arxiv.org/abs/2406.04167</link>
      <description>arXiv:2406.04167v1 Announce Type: new 
Abstract: Predicting the timing and occurrence of events is a major focus of data science applications, especially in the context of biomedical research. Performance for models estimating these outcomes, often referred to as time-to-event or survival outcomes, is frequently summarized using measures of discrimination, in particular time-dependent AUC and concordance. Many estimators for these quantities have been proposed which can be broadly categorized as either semi-parametric estimators or non-parametric estimators. In this paper, we review various estimators' mathematical construction and compare the behavior of the two classes of estimators. Importantly, we identify a previously unknown feature of the class of semi-parametric estimators that can result in vastly over-optimistic out-of-sample estimation of discriminative performance in common applied tasks. Although these semi-parametric estimators are popular in practice, the phenomenon we identify here suggests this class of estimators may be inappropriate for use in model assessment and selection based on out-of-sample evaluation criteria. This is due to the semi-parametric estimators' bias in favor of models that are overfit when using out-of-sample prediction criteria (e.g., cross validation). Non-parametric estimators, which do not exhibit this behavior, are highly variable for local discrimination. We propose to address the high variability problem through penalized regression splines smoothing. The behavior of various estimators of time-dependent AUC and concordance are illustrated via a simulation study using two different mechanisms that produce over-optimistic out-of-sample estimates using semi-parametric estimators. Estimators are further compared using a case study using data from the National Health and Nutrition Examination Survey (NHANES) 2011-2014.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04167v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ying Jin, Andrew Leroux</dc:creator>
    </item>
    <item>
      <title>Gradient Boosting for Hierarchical Data in Small Area Estimation</title>
      <link>https://arxiv.org/abs/2406.04256</link>
      <description>arXiv:2406.04256v1 Announce Type: new 
Abstract: This paper introduces Mixed Effect Gradient Boosting (MEGB), which combines the strengths of Gradient Boosting with Mixed Effects models to address complex, hierarchical data structures often encountered in statistical analysis. The methodological foundations, including a review of the Mixed Effects model and the Extreme Gradient Boosting method, leading to the introduction of MEGB are shown in detail. It highlights how MEGB can derive area-level mean estimations from unit-level data and calculate Mean Squared Error (MSE) estimates using a nonparametric bootstrap approach. The paper evaluates MEGB's performance through model-based and design-based simulation studies, comparing it against established estimators. The findings indicate that MEGB provides promising area mean estimations and may outperform existing small area estimators in various scenarios. The paper concludes with a discussion on future research directions, highlighting the possibility of extending MEGB's framework to accommodate different types of outcome variables or non-linear area level indicators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04256v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Messer, Timo Schmid</dc:creator>
    </item>
    <item>
      <title>Bayesian generalized method of moments applied to pseudo-observations in survival analysis</title>
      <link>https://arxiv.org/abs/2406.03821</link>
      <description>arXiv:2406.03821v1 Announce Type: cross 
Abstract: Bayesian inference for survival regression modeling offers numerous advantages, especially for decision-making and external data borrowing, but demands the specification of the baseline hazard function, which may be a challenging task. We propose an alternative approach that does not need the specification of this function. Our approach combines pseudo-observations to convert censored data into longitudinal data with the Generalized Methods of Moments (GMM) to estimate the parameters of interest from the survival function directly. GMM may be viewed as an extension of the Generalized Estimating Equation (GEE) currently used for frequentist pseudo-observations analysis and can be extended to the Bayesian framework using a pseudo-likelihood function. We assessed the behavior of the frequentist and Bayesian GMM in the new context of analyzing pseudo-observations. We compared their performances to the Cox, GEE, and Bayesian piecewise exponential models through a simulation study of two-arm randomized clinical trials. Frequentist and Bayesian GMM gave valid inferences with similar performances compared to the three benchmark methods, except for small sample sizes and high censoring rates. For illustration, three post-hoc efficacy analyses were performed on randomized clinical trials involving patients with Ewing Sarcoma, producing results similar to those of the benchmark methods. Through a simple application of estimating hazard ratios, these findings confirm the effectiveness of this new Bayesian approach based on pseudo-observations and the generalized method of moments. This offers new insights on using pseudo-observations for Bayesian survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03821v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'ea Orsini (U1018), Caroline Brard (DSAS), Emmanuel Lesaffre (DSAS), Guosheng Yin (DSAS), David Dejardin (U1018), Gw\'ena\"el Le Teuff (U1018)</dc:creator>
    </item>
    <item>
      <title>Statistical Multicriteria Benchmarking via the GSD-Front</title>
      <link>https://arxiv.org/abs/2406.03924</link>
      <description>arXiv:2406.03924v1 Announce Type: cross 
Abstract: Given the vast number of classifiers that have been (and continue to be) proposed, reliable methods for comparing them are becoming increasingly important. The desire for reliability is broken down into three main aspects: (1) Comparisons should allow for different quality metrics simultaneously. (2) Comparisons should take into account the statistical uncertainty induced by the choice of benchmark suite. (3) The robustness of the comparisons under small deviations in the underlying assumptions should be verifiable. To address (1), we propose to compare classifiers using a generalized stochastic dominance ordering (GSD) and present the GSD-front as an information-efficient alternative to the classical Pareto-front. For (2), we propose a consistent statistical estimator for the GSD-front and construct a statistical test for whether a (potentially new) classifier lies in the GSD-front of a set of state-of-the-art classifiers. For (3), we relax our proposed test using techniques from robust statistics and imprecise probabilities. We illustrate our concepts on the benchmark suite PMLB and on the platform OpenML.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03924v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christoph Jansen (Lancaster University Leipzig), Georg Schollmeyer (Ludwig-Maximilians-Universit\"at M\"unchen), Julian Rodemann (Ludwig-Maximilians-Universit\"at M\"unchen), Hannah Blocher (Ludwig-Maximilians-Universit\"at M\"unchen), Thomas Augustin (Ludwig-Maximilians-Universit\"at M\"unchen)</dc:creator>
    </item>
    <item>
      <title>Strong Approximations for Empirical Processes Indexed by Lipschitz Functions</title>
      <link>https://arxiv.org/abs/2406.04191</link>
      <description>arXiv:2406.04191v1 Announce Type: cross 
Abstract: This paper presents new uniform Gaussian strong approximations for empirical processes indexed by classes of functions based on $d$-variate random vectors ($d\geq1$). First, a uniform Gaussian strong approximation is established for general empirical processes indexed by Lipschitz functions, encompassing and improving on all previous results in the literature. When specialized to the setting considered by Rio (1994), and certain constraints on the function class hold, our result improves the approximation rate $n^{-1/(2d)}$ to $n^{-1/\max\{d,2\}}$, up to the same $\operatorname{polylog} n$ term, where $n$ denotes the sample size. Remarkably, we establish a valid uniform Gaussian strong approximation at the optimal rate $n^{-1/2}\log n$ for $d=2$, which was previously known to be valid only for univariate ($d=1$) empirical processes via the celebrated Hungarian construction (Koml\'os et al., 1975). Second, a uniform Gaussian strong approximation is established for a class of multiplicative separable empirical processes indexed by Lipschitz functions, which address some outstanding problems in the literature (Chernozhukov et al., 2014, Section 3). In addition, two other uniform Gaussian strong approximation results are presented for settings where the function class takes the form of a sequence of Haar basis based on generalized quasi-uniform partitions. We demonstrate the improvements and usefulness of our new strong approximation results with several statistical applications to nonparametric density and regression estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04191v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo (Rae),  Ruiqi (Rae),  Yu</dc:creator>
    </item>
    <item>
      <title>Flexible Clustering with a Sparse Mixture of Generalized Hyperbolic Distributions</title>
      <link>https://arxiv.org/abs/1903.05054</link>
      <description>arXiv:1903.05054v2 Announce Type: replace 
Abstract: Robust clustering of high-dimensional data is an important topic because clusters in real datasets are often heavy-tailed and/or asymmetric. Traditional approaches to model-based clustering often fail for high dimensional data, e.g., due to the number of free covariance parameters. A parametrization of the component scale matrices for the mixture of generalized hyperbolic distributions is proposed. This parameterization includes a penalty term in the likelihood. An analytically feasible expectation-maximization algorithm is developed by placing a gamma-lasso penalty constraining the concentration matrix. The proposed methodology is investigated through simulation studies and illustrated using two real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:1903.05054v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexa A. Sochaniwsky, Michael P. B. Gallaugher, Yang Tang, Paul D. McNicholas</dc:creator>
    </item>
    <item>
      <title>DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models</title>
      <link>https://arxiv.org/abs/2206.06821</link>
      <description>arXiv:2206.06821v2 Announce Type: replace 
Abstract: We present DoWhy-GCM, an extension of the DoWhy Python library, which leverages graphical causal models. Unlike existing causality libraries, which mainly focus on effect estimation, DoWhy-GCM addresses diverse causal queries, such as identifying the root causes of outliers and distributional changes, attributing causal influences to the data generating process of each node, or diagnosis of causal structures. With DoWhy-GCM, users typically specify cause-effect relations via a causal graph, fit causal mechanisms, and pose causal queries -- all with just a few lines of code. The general documentation is available at https://www.pywhy.org/dowhy and the DoWhy-GCM specific code at https://github.com/py-why/dowhy/tree/main/dowhy/gcm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.06821v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research 25(147), 2024</arxiv:journal_reference>
      <dc:creator>Patrick Bl\"obaum, Peter G\"otz, Kailash Budhathoki, Atalanti A. Mastakouri, Dominik Janzing</dc:creator>
    </item>
    <item>
      <title>Improving efficiency in transporting average treatment effects</title>
      <link>https://arxiv.org/abs/2304.00117</link>
      <description>arXiv:2304.00117v3 Announce Type: replace 
Abstract: We develop flexible, semiparametric estimators of the average treatment effect (ATE) transported to a new population ("target population") that offer potential efficiency gains. Transport may be of value when the ATE may differ across populations. We consider the setting where differences in the ATE are due to differences in the distribution of baseline covariates that modify the treatment effect ("effect modifiers"). First, we propose a collaborative one-step semiparametric estimator that can improve efficiency. This approach does not require researchers to have knowledge about which covariates are effect modifiers and which differ in distribution between the populations, but does require all covariates to be measured in the target population. Second, we propose two one-step semiparametric estimators that assume knowledge of which covariates are effect modifiers and which are both effect modifiers and differentially distributed between the populations. These estimators can be used even when not all covariates are observed in the target population; one requires that only effect modifiers are observed, and the other requires that only those modifiers that are also differentially distributed are observed. We use simulation to compare finite sample performance across our proposed estimators and an existing semiparametric estimator of the transported ATE, including in the presence of practical violations of the positivity assumption. Lastly, we apply our proposed estimators to a large-scale housing trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00117v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kara E. Rudolph, Nicholas T. Williams, Elizabeth A. Stuart, Ivan Diaz</dc:creator>
    </item>
    <item>
      <title>Augmented balancing weights as linear regression</title>
      <link>https://arxiv.org/abs/2304.14545</link>
      <description>arXiv:2304.14545v3 Announce Type: replace 
Abstract: We provide a novel characterization of augmented balancing weights, also known as automatic debiased machine learning (AutoDML). These popular doubly robust or de-biased machine learning estimators combine outcome modeling with balancing weights - weights that achieve covariate balance directly in lieu of estimating and inverting the propensity score. When the outcome and weighting models are both linear in some (possibly infinite) basis, we show that the augmented estimator is equivalent to a single linear model with coefficients that combine the coefficients from the original outcome model and coefficients from an unpenalized ordinary least squares (OLS) fit on the same data. We see that, under certain choices of regularization parameters, the augmented estimator often collapses to the OLS estimator alone; this occurs for example in a re-analysis of the Lalonde 1986 dataset. We then extend these results to specific choices of outcome and weighting models. We first show that the augmented estimator that uses (kernel) ridge regression for both outcome and weighting models is equivalent to a single, undersmoothed (kernel) ridge regression. This holds numerically in finite samples and lays the groundwork for a novel analysis of undersmoothing and asymptotic rates of convergence. When the weighting model is instead lasso-penalized regression, we give closed-form expressions for special cases and demonstrate a ``double selection'' property. Our framework opens the black box on this increasingly popular class of estimators, bridges the gap between existing results on the semiparametric efficiency of undersmoothed and doubly robust estimators, and provides new insights into the performance of augmented balancing weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.14545v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Bruns-Smith, Oliver Dukes, Avi Feller, Elizabeth L. Ogburn</dc:creator>
    </item>
    <item>
      <title>Design-Based Causal Inference with Missing Outcomes: Missingness Mechanisms, Imputation-Assisted Randomization Tests, and Covariate Adjustment</title>
      <link>https://arxiv.org/abs/2310.18556</link>
      <description>arXiv:2310.18556v4 Announce Type: replace 
Abstract: Design-based causal inference, also known as randomization-based or finite-population causal inference, is one of the most widely used causal inference frameworks, largely due to the merit that its validity can be guaranteed by study design (e.g., randomized experiments) and does not require assuming specific outcome-generating distributions or super-population models. Despite its advantages, design-based causal inference can still suffer from other data-related issues, among which outcome missingness is a prevalent and significant challenge. This work systematically studies the outcome missingness problem in design-based causal inference. First, we propose a general and flexible outcome missingness mechanism that can facilitate finite-population-exact randomization tests for the null effect. Second, under this general missingness mechanism, we propose a general framework called ``imputation and re-imputation" for conducting finite-population-exact randomization tests in design-based causal inference with missing outcomes. This framework can incorporate any imputation algorithms (from linear models to advanced machine learning-based imputation algorithms) while ensuring finite-population-exact type-I error rate control. Third, we extend our framework to conduct covariate adjustment in randomization tests and construct finite-population-valid confidence regions with missing outcomes. Our framework is evaluated via extensive simulation studies and applied to a large-scale randomized experiment. Corresponding Python and R packages are also developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18556v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Heng, Jiawei Zhang, Yang Feng</dc:creator>
    </item>
    <item>
      <title>Thinking inside the bounds: Improved error distributions for indifference point data analysis and simulation via beta regression using common discounting functions</title>
      <link>https://arxiv.org/abs/2404.18000</link>
      <description>arXiv:2404.18000v2 Announce Type: replace 
Abstract: Standard nonlinear regression is commonly used when modeling indifference points due to its ability to closely follow observed data, resulting in a good model fit. However, standard nonlinear regression currently lacks a reasonable distribution-based framework for indifference points, which limits its ability to adequately describe the inherent variability in the data. Software commonly assumes data follow a normal distribution with constant variance. However, typical indifference points do not follow a normal distribution or exhibit constant variance. To address these limitations, this paper introduces a class of nonlinear beta regression models that offers excellent fit to discounting data and enhances simulation-based approaches. This beta regression model can accommodate popular discounting functions. This work proposes three specific advances. First, our model automatically captures non-constant variance as a function of delay. Second, our model improves simulation-based approaches since it obeys the natural boundaries of observable data, unlike the ordinary assumption of normal residuals and constant variance. Finally, we introduce a scale-location-truncation trick that allows beta regression to accommodate observed values of zero and one. A comparison between beta regression and standard nonlinear regression reveals close agreement in the estimated discounting rate k obtained from both methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18000v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingang Kim, Mikhail N. Koffarnus, Christopher T Franck</dc:creator>
    </item>
    <item>
      <title>Optimizing Language Models for Human Preferences is a Causal Inference Problem</title>
      <link>https://arxiv.org/abs/2402.14979</link>
      <description>arXiv:2402.14979v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader's response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method--causal preference optimization (CPO)--that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarantees on bias. Finally, we empirically demonstrate the effectiveness of (DR-)CPO in optimizing state-of-the-art LLMs for human preferences on direct outcome data, and we validate the robustness of DR-CPO under difficult confounding conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14979v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victoria Lin, Eli Ben-Michael, Louis-Philippe Morency</dc:creator>
    </item>
  </channel>
</rss>
