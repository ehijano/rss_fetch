<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Jun 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The GAIN Model: A Nature-Inspired Neural Network Framework Based on an Adaptation of the Izhikevich Model</title>
      <link>https://arxiv.org/abs/2506.04247</link>
      <description>arXiv:2506.04247v1 Announce Type: new 
Abstract: While many neural networks focus on layers to process information, the GAIN model uses a grid-based structure to improve biological plausibility and the dynamics of the model. The grid structure helps neurons to interact with their closest neighbors and improve their connections with one another, which is seen in biological neurons. While also being implemented with the Izhikevich model this approach allows for a computationally efficient and biologically accurate simulation that can aid in the development of neural networks, large scale simulations, and the development in the neuroscience field. This adaptation of the Izhikevich model can improve the dynamics and accuracy of the model, allowing for its uses to be specialized but efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04247v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Gage K. R. Hooper</dc:creator>
    </item>
    <item>
      <title>Discounting and Drug Seeking in Biological Hierarchical Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2506.04549</link>
      <description>arXiv:2506.04549v1 Announce Type: new 
Abstract: Despite a strong desire to quit, individuals with long-term substance use disorder (SUD) often struggle to resist drug use, even when aware of its harmful consequences. This disconnect between knowledge and compulsive behavior reflects a fundamental cognitive-behavioral conflict in addiction. Neurobiologically, differential cue-induced activity within striatal subregions, along with dopamine-mediated connectivity from the ventral to the dorsal striatum, contributes to compulsive drug-seeking. However, the functional mechanism linking these findings to behavioral conflict remains unclear. Another hallmark of addiction is temporal discounting: individuals with drug dependence exhibit steeper discount rates than non-users. Assuming the ventral-dorsal striatal organization reflects a gradient from cognitive to motor representations, addiction can be modeled within a hierarchical reinforcement learning (HRL) framework. However, integrating discounting into biologically grounded HRL remains an open challenge. In this work, we build on a model showing how action choices reinforced with drug rewards become insensitive to the negative consequences that follow. We address the integration of discounting by ensuring natural reward values converge across all levels in the HRL hierarchy, while drug rewards diverge due to their dopaminergic effects. Our results show that high discounting amplifies drug-seeking across the hierarchy, linking faster discounting with increased addiction severity and impulsivity. We demonstrate alignment with empirical findings on temporal discounting and propose testable predictions, establishing addiction as a disorder of hierarchical decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04549v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vardhan Palod, Pranav Mahajan, Veeky Baths, Boris S. Gutkin</dc:creator>
    </item>
    <item>
      <title>Consciousness via MIPT?</title>
      <link>https://arxiv.org/abs/2506.04875</link>
      <description>arXiv:2506.04875v1 Announce Type: new 
Abstract: The measurement-induced phase transition (MIPT) is a recently formulated phenomenon in out-of-equilibrium systems. The competition between unitary evolutions and measurement-induced non-unitaries leads to the transition between the entangled and disentangled phases at some critical measurement rate. We conjecture that self-organized MIPT plays a key role in the generative model of cognitive networks and the formation of the state of consciousness in the "newborn-adult" transition. To this aim, we formulate the probe-target picture for the brain and suggest that MIPT interpreted as learnability transition takes place in the mental part of the target where the sites in the cognitive networks of semantic memory are concepts. Comparison with the synchronization phase transitions in the probe is made.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04875v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Gorsky</dc:creator>
    </item>
    <item>
      <title>TRACE: Contrastive learning for multi-trial time-series data in neuroscience</title>
      <link>https://arxiv.org/abs/2506.04906</link>
      <description>arXiv:2506.04906v1 Announce Type: new 
Abstract: Modern neural recording techniques such as two-photon imaging allow to acquire vast time-series datasets with responses of hundreds or thousands of neurons. Contrastive learning is a powerful self-supervised framework for learning representations of complex datasets. Existing applications for neural time series rely on generic data augmentations and do not exploit the multi-trial data structure inherent in many neural datasets. Here we present TRACE, a new contrastive learning framework that averages across different subsets of trials to generate positive pairs. TRACE allows to directly learn a two-dimensional embedding, combining ideas from contrastive learning and neighbor embeddings. We show that TRACE outperforms other methods, resolving fine response differences in simulated data. Further, using in vivo recordings, we show that the representations learned by TRACE capture both biologically relevant continuous variation, cell-type-related cluster structure, and can assist data quality control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04906v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lisa Schmors, Dominic Gonschorek, Jan Niklas B\"ohm, Yongrong Qiu, Na Zhou, Dmitry Kobak, Andreas Tolias, Fabian Sinz, Jacob Reimer, Katrin Franke, Sebastian Damrich, Philipp Berens</dc:creator>
    </item>
    <item>
      <title>The Hippocampal Place Field Gradient: An Eigenmode Theory Linking Grid Cell Projections to Multiscale Learning</title>
      <link>https://arxiv.org/abs/2506.04943</link>
      <description>arXiv:2506.04943v1 Announce Type: new 
Abstract: The hippocampus encodes space through a striking gradient of place field sizes along its dorsal-ventral axis, yet the principles generating this continuous gradient from discrete grid cell inputs remain debated. We propose a unified theoretical framework establishing that hippocampal place fields arise naturally as linear projections of grid cell population activity, interpretable as eigenmodes. Critically, we demonstrate that a frequency-dependent decay of these grid-to-place connection weights naturally transforms inputs from discrete grid modules into a continuous spectrum of place field sizes. This multiscale organization is functionally significant: we reveal it shapes the inductive bias of the population code, balancing a fundamental trade-off between precision and generalization. Mathematical analysis and simulations demonstrate an optimal place field size for few-shot learning, which scales with environment structure. Our results offer a principled explanation for the place field gradient and generate testable predictions, bridging anatomical connectivity with adaptive learning in both biological and artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04943v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shujun Zhou, Guozhang Chen</dc:creator>
    </item>
    <item>
      <title>Generalizable, real-time neural decoding with hybrid state-space models</title>
      <link>https://arxiv.org/abs/2506.05320</link>
      <description>arXiv:2506.05320v1 Announce Type: new 
Abstract: Real-time decoding of neural activity is central to neuroscience and neurotechnology applications, from closed-loop experiments to brain-computer interfaces, where models are subject to strict latency constraints. Traditional methods, including simple recurrent neural networks, are fast and lightweight but often struggle to generalize to unseen data. In contrast, recent Transformer-based approaches leverage large-scale pretraining for strong generalization performance, but typically have much larger computational requirements and are not always suitable for low-resource or real-time settings. To address these shortcomings, we present POSSM, a novel hybrid architecture that combines individual spike tokenization via a cross-attention module with a recurrent state-space model (SSM) backbone to enable (1) fast and causal online prediction on neural activity and (2) efficient generalization to new sessions, individuals, and tasks through multi-dataset pretraining. We evaluate POSSM's decoding performance and inference speed on intracortical decoding of monkey motor tasks, and show that it extends to clinical applications, namely handwriting and speech decoding in human subjects. Notably, we demonstrate that pretraining on monkey motor-cortical recordings improves decoding performance on the human handwriting task, highlighting the exciting potential for cross-species transfer. In all of these tasks, we find that POSSM achieves decoding accuracy comparable to state-of-the-art Transformers, at a fraction of the inference cost (up to 9x faster on GPU). These results suggest that hybrid SSMs are a promising approach to bridging the gap between accuracy, inference speed, and generalization when training neural decoders for real-time, closed-loop applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05320v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avery Hee-Woon Ryoo, Nanda H. Krishna, Ximeng Mao, Mehdi Azabou, Eva L. Dyer, Matthew G. Perich, Guillaume Lajoie</dc:creator>
    </item>
    <item>
      <title>Relational reasoning and inductive bias in transformers trained on a transitive inference task</title>
      <link>https://arxiv.org/abs/2506.04289</link>
      <description>arXiv:2506.04289v1 Announce Type: cross 
Abstract: Transformer-based models have demonstrated remarkable reasoning abilities, but the mechanisms underlying relational reasoning in different learning regimes remain poorly understood. In this work, we investigate how transformers perform a classic relational reasoning task from the Psychology literature, \textit{transitive inference}, which requires inference about indirectly related items by integrating information across observed adjacent item pairs (e.g., if A&gt;B and B&gt;C, then A&gt;C). We compare transitive inference behavior across two distinct learning regimes: in-weights learning (IWL), where models store information in network parameters, and in-context learning (ICL), where models flexibly utilize information presented within the input sequence. Our findings reveal that IWL naturally induces a generalization bias towards transitive inference, despite being trained only on adjacent items, whereas ICL models trained solely on adjacent items do not generalize transitively. Mechanistic analysis shows that ICL models develop induction circuits that implement a simple match-and-copy strategy that performs well at relating adjacent pairs, but does not encoding hierarchical relationships among indirectly related items. Interestingly, when pre-trained on in-context linear regression tasks, transformers successfully exhibit in-context generalizable transitive inference. Moreover, like IWL, they display both \textit{symbolic distance} and \textit{terminal item effects} characteristic of human and animal performance, without forming induction circuits. These results suggest that pre-training on tasks with underlying structure promotes the development of representations that can scaffold in-context relational reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04289v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Geerts, Stephanie Chan, Claudia Clopath, Kimberly Stachenfeld</dc:creator>
    </item>
    <item>
      <title>Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization</title>
      <link>https://arxiv.org/abs/2506.04379</link>
      <description>arXiv:2506.04379v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) trained on visual tasks develop feature representations that resemble those in the human visual system. Although DNN-based encoding models can accurately predict brain responses to visual stimuli, they offer limited insight into the specific features driving these responses. Here, we demonstrate that activation maximization -- a technique designed to interpret vision DNNs -- can be applied to DNN-based encoding models of the human brain. We extract and adaptively downsample activations from multiple layers of a pretrained Inception V3 network, then use linear regression to predict fMRI responses. This yields a full image-computable model of brain responses. Next, we apply activation maximization to generate images optimized for predicted responses in individual cortical voxels. We find that these images contain visual characteristics that qualitatively correspond with known selectivity and enable exploration of selectivity across the visual cortex. We further extend our method to whole regions of interest (ROIs) of the brain and validate its efficacy by presenting these images to human participants in an fMRI study. We find that the generated images reliably drive activity in targeted regions across both low- and high-level visual areas and across subjects. These results demonstrate that activation maximization can be successfully applied to DNN-based encoding models. By addressing key limitations of alternative approaches that require natively generative models, our approach enables flexible characterization and modulation of responses across the human visual system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04379v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew W. Shinkle, Mark D. Lescroart</dc:creator>
    </item>
    <item>
      <title>NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models</title>
      <link>https://arxiv.org/abs/2506.04536</link>
      <description>arXiv:2506.04536v1 Announce Type: cross 
Abstract: Characterizing the diverse computational properties of human neurons via multimodal electrophysiological, transcriptomic, and morphological data provides the foundation for constructing and validating bio-realistic neuron models that can advance our understanding of fundamental mechanisms underlying brain function. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. To capture variability, ensembles of deterministic models are often used, but are difficult to scale as model generation requires repeating computationally expensive optimization for each neuron. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce NOBLE, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on data generated from biophysically realistic neuron models, NOBLE predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. NOBLE is the first scaled-up deep learning framework validated on real experimental data, enabling efficient generation of synthetic neurons that exhibit trial-to-trial variability and achieve a $4200\times$ speedup over numerical solvers. To this end, NOBLE captures fundamental neural properties, opening the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04536v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Ghafourpour, Valentin Duruisseaux, Bahareh Tolooshams, Philip H. Wong, Costas A. Anastassiou, Anima Anandkumar</dc:creator>
    </item>
    <item>
      <title>Associative Memory and Generative Diffusion in the Zero-noise Limit</title>
      <link>https://arxiv.org/abs/2506.05178</link>
      <description>arXiv:2506.05178v1 Announce Type: cross 
Abstract: Connections between generative diffusion and continuous-state associative memory models are studied. Morse-Smale dynamical systems are emphasized as universal approximators of gradient-based associative memory models and diffusion models as white-noise perturbed systems thereof. Universal properties of associative memory that follow from this description are described and used to characterize a generic transition from generation to memory as noise levels diminish. Structural stability inherited by Morse-Smale flows is shown to imply a notion of stability for diffusions at vanishing noise levels. Applied to one- and two-parameter families of gradients, this indicates stability at all but isolated points of associative memory learning landscapes and the learning and generation landscapes of diffusion models with gradient drift in the zero-noise limit, at which small sets of generic bifurcations characterize qualitative transitions between stable systems. Examples illustrating the characterization of these landscapes by sequences of these bifurcations are given, along with structural stability criterion for classic and modern Hopfield networks (equivalently, the attention mechanism).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05178v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>math.DS</category>
      <category>nlin.AO</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Hess, Quaid Morris</dc:creator>
    </item>
    <item>
      <title>Transient dynamics of associative memory models</title>
      <link>https://arxiv.org/abs/2506.05303</link>
      <description>arXiv:2506.05303v1 Announce Type: cross 
Abstract: Associative memory models such as the Hopfield network and its dense generalizations with higher-order interactions exhibit a "blackout catastrophe"--a discontinuous transition where stable memory states abruptly vanish when the number of stored patterns exceeds a critical capacity. This transition is often interpreted as rendering networks unusable beyond capacity limits. We argue that this interpretation is largely an artifact of the equilibrium perspective. We derive dynamical mean-field equations using a bipartite cavity approach for graded-activity dense associative memory models, with the Hopfield model as a special case, and solve them using a numerical scheme. We show that patterns can be transiently retrieved with high accuracy above capacity despite the absence of stable attractors. This occurs because slow regions persist in the above-capacity energy landscape as shallow, unstable remnants of below-capacity stable basins. The same transient-retrieval effect occurs in below-capacity networks initialized outside basins of attraction. "Transient-recovery curves" provide a concise visual summary of these effects, revealing graceful, non-catastrophic changes in retrieval behavior above capacity and allowing us to compare the behavior across interaction orders. This dynamical perspective reveals rich energy landscape structure obscured by equilibrium analysis and suggests biological neural circuits may exploit transient dynamics for memory retrieval. Furthermore, our approach suggests ways of understanding computational properties of neural circuits without reference to fixed points, advances the technical repertoire of numerical mean-field solution methods for recurrent neural networks, and yields new theoretical results on generalizations of the Hopfield model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05303v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David G. Clark</dc:creator>
    </item>
    <item>
      <title>The Syncytial Mesh Model: A Biophysical Framework for Scale-Dependent Coherence in the Brain</title>
      <link>https://arxiv.org/abs/2412.12106</link>
      <description>arXiv:2412.12106v2 Announce Type: replace 
Abstract: The brain-mesh model introduces a novel three-layered architecture that integrates local and macro-regional connectivity with an underlying, mesh-inspired network layer. This foundational mesh layer, based on metallic mesh structures, spans the entire brain and generates interference patterns, noise, and resonance effects that modulate both local and global neural dynamics. The fused model goes beyond traditional connectivity frameworks by providing a unified explanation for phenomena such as brain-wide phase gradients, stable low-frequency resonance frequencies, and long-range plasticity effects, which are often difficult to explain cohesively within existing models.
  In addition to accounting for classical neurobiological observations, such as phase synchrony, functional connectivity fluctuations, and local Hebbian plasticity, the model offers novel insights into less understood phenomena. Specifically, it predicts connectivity-independent phase gradients across non-synaptic regions, harmonic resonance peaks consistent across individuals, and diffuse plasticity driven by global interference patterns, all of which are challenging to explain under current frameworks.
  These unique predictions align with partial empirical observations, such as traveling wave dynamics, consistent low-frequency oscillations, and task-induced connectivity shifts, underscoring the model's relevance. Additionally, the brain-mesh model generates testable hypotheses that distinguish it from traditional approaches. This provides a promising framework for future experimental validation and opens new avenues for understanding global brain function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12106v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreu Ballus Santacana</dc:creator>
    </item>
    <item>
      <title>An analytic theory of creativity in convolutional diffusion models</title>
      <link>https://arxiv.org/abs/2412.20292</link>
      <description>arXiv:2412.20292v2 Announce Type: replace-cross 
Abstract: We obtain an analytic, interpretable and predictive theory of creativity in convolutional diffusion models. Indeed, score-matching diffusion models can generate highly original images that lie far from their training data. However, optimal score-matching theory suggests that these models should only be able to produce memorized training examples. To reconcile this theory-experiment gap, we identify two simple inductive biases, locality and equivariance, that: (1) induce a form of combinatorial creativity by preventing optimal score-matching; (2) result in fully analytic, completely mechanistically interpretable, local score (LS) and equivariant local score (ELS) machines that, (3) after calibrating a single time-dependent hyperparameter can quantitatively predict the outputs of trained convolution only diffusion models (like ResNets and UNets) with high accuracy (median $r^2$ of $0.95, 0.94, 0.94, 0.96$ for our top model on CIFAR10, FashionMNIST, MNIST, and CelebA). Our model reveals a locally consistent patch mosaic mechanism of creativity, in which diffusion models create exponentially many novel images by mixing and matching different local training set patches at different scales and image locations. Our theory also partially predicts the outputs of pre-trained self-attention enabled UNets (median $r^2 \sim 0.77$ on CIFAR10), revealing an intriguing role for attention in carving out semantic coherence from local patch mosaics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20292v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mason Kamb, Surya Ganguli</dc:creator>
    </item>
  </channel>
</rss>
