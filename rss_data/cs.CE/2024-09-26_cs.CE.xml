<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Sep 2024 04:01:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A two-scale computational homogenization approach for elastoplastic truss-based lattice structures</title>
      <link>https://arxiv.org/abs/2409.17293</link>
      <description>arXiv:2409.17293v1 Announce Type: new 
Abstract: The revolutionary advancements in metal additive manufacturing have enabled the production of alloy-based lattice structures with complex geometrical features and high resolutions. This has encouraged the development of nonlinear material models, including plasticity, damage, etc., for such materials. However, the prohibitive computational cost arising from the high number of degrees of freedom for engineering structures composed of lattice structures highlights the necessity of homogenization techniques, such as the two-scale computational homogenization method. In the present work, a two-scale homogenization approach with on-the-fly exchange of information is adopted to study the elastoplastic behavior of truss-based lattice structures. The macroscopic homogenized structure is represented by a two-dimensional continuum, while the underlying microscale lattices are modeled as a network of one-dimensional truss elements. This helps to significantly reduce the associated computational cost by reducing the microscopic degrees of freedom. The microscale trusses are assumed to exhibit an elastoplastic material behavior characterized by a combination of nonlinear exponential isotropic hardening and linear kinematic hardening. Through multiple numerical examples, the performance of the adopted homogenization approach is examined by comparing forces and displacements with direct numerical simulations of discrete structures for three types of stretching-dominated lattice topologies, including triangular, X-braced and X-Plus-braced unit cells. Furthermore, the principle of scale separation, which emphasizes the need for an adequate separation between the macroscopic and microscopic characteristic lengths, is investigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17293v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hooman Danesh, Lisamarie Heu{\ss}en, Francisco J. Mont\'ans, Stefanie Reese, Tim Brepols</dc:creator>
    </item>
    <item>
      <title>Adapting Deep Variational Bayes Filter for Enhanced Confidence Estimation in Finite Element Method Integrated Networks (FEMIN)</title>
      <link>https://arxiv.org/abs/2409.17758</link>
      <description>arXiv:2409.17758v1 Announce Type: new 
Abstract: The Finite Element Method (FEM) is a widely used technique for simulating crash scenarios with high accuracy and reliability. To reduce the significant computational costs associated with FEM, the Finite Element Method Integrated Networks (FEMIN) framework integrates neural networks (NNs) with FEM solvers. However, this integration can introduce errors and deviations from full-FEM simulations, highlighting the need for an additional metric to assess prediction confidence, especially when no ground truth data is available. In this study, we adapt the Deep Variational Bayes Filter (DVBF) to the FEMIN framework, incorporating a probabilistic approach to provide qualitative insights into prediction confidence during FEMIN simulations. The adaptation involves using the learned transition model for a predictive decoding step, generating a preliminary force prediction. This predictive force is used alongside the displacement and the velocity data from the FEM solver as input for the encoder model. The decoder reconstructs the likelihood distribution based on the posterior. The mean force of this distribution is applied to the FEM solver, while the predicted standard deviation can be used for uncertainty estimation. Our findings demonstrate that the DVBF outperforms deterministic NN architectures in terms of accuracy. Furthermore, the standard deviation derived from the decoder serves as a valuable qualitative metric for assessing the confidence in FEMIN simulations. This approach enhances the robustness of FEMIN by providing a measure of reliability alongside the simulation results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17758v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Thel, Lars Greve, Maximilian Karl, Patrick van der Smagt</dc:creator>
    </item>
    <item>
      <title>Physics-driven complex relaxation for multi-body systems of SPH method</title>
      <link>https://arxiv.org/abs/2409.17795</link>
      <description>arXiv:2409.17795v1 Announce Type: new 
Abstract: In the smoothed particle dynamics (SPH) method, the characteristics of a target particle are interpolated based on the information from its neighboring particles. Consequently, a uniform initial distribution of particles significantly enhances the accuracy of SPH calculations. This aspect is particularly critical in Eulerian SPH, where particles are stationary throughout the simulation. To address this, we introduce a physics-driven complex relaxation method for multi-body systems. Through a series of two-dimensional and three-dimensional case studies, we demonstrate that this method is capable of achieving a globally uniform particle distribution, especially at the interfaces between contacting bodies, and ensuring improved zero-order consistency. Moreover, the effectiveness and reliability of the complex relaxation method in enhancing the accuracy of physical simulations are further validated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17795v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenxi Zhao, Yongchuan Yu, Oskar J. Haidn, Xiangyu Hu</dc:creator>
    </item>
    <item>
      <title>Neural Network Architecture Search Enabled Wide-Deep Learning (NAS-WD) for Spatially Heterogenous Property Awared Chicken Woody Breast Classification and Hardness Regression</title>
      <link>https://arxiv.org/abs/2409.17210</link>
      <description>arXiv:2409.17210v1 Announce Type: cross 
Abstract: Due to intensive genetic selection for rapid growth rates and high broiler yields in recent years, the global poultry industry has faced a challenging problem in the form of woody breast (WB) conditions. This condition has caused significant economic losses as high as $200 million annually, and the root cause of WB has yet to be identified. Human palpation is the most common method of distinguishing a WB from others. However, this method is time-consuming and subjective. Hyperspectral imaging (HSI) combined with machine learning algorithms can evaluate the WB conditions of fillets in a non-invasive, objective, and high-throughput manner. In this study, 250 raw chicken breast fillet samples (normal, mild, severe) were taken, and spatially heterogeneous hardness distribution was first considered when designing HSI processing models. The study not only classified the WB levels from HSI but also built a regression model to correlate the spectral information with sample hardness data. To achieve a satisfactory classification and regression model, a neural network architecture search (NAS) enabled a wide-deep neural network model named NAS-WD, which was developed. In NAS-WD, NAS was first used to automatically optimize the network architecture and hyperparameters. The classification results show that NAS-WD can classify the three WB levels with an overall accuracy of 95%, outperforming the traditional machine learning model, and the regression correlation between the spectral data and hardness was 0.75, which performs significantly better than traditional regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17210v1</guid>
      <category>cs.CV</category>
      <category>cs.CE</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chaitanya Pallerla, Yihong Feng, Casey M. Owens, Ramesh Bahadur Bist, Siavash Mahmoudi, Pouya Sohrabipour, Amirreza Davar, Dongyi Wang</dc:creator>
    </item>
    <item>
      <title>AAPM: Large Language Model Agent-based Asset Pricing Models</title>
      <link>https://arxiv.org/abs/2409.17266</link>
      <description>arXiv:2409.17266v1 Announce Type: cross 
Abstract: In this study, we propose a novel asset pricing approach, LLM Agent-based Asset Pricing Models (AAPM), which fuses qualitative discretionary investment analysis from LLM agents and quantitative manual financial economic factors to predict excess asset returns. The experimental results show that our approach outperforms machine learning-based asset pricing baselines in portfolio optimization and asset pricing errors. Specifically, the Sharpe ratio and average $|\alpha|$ for anomaly portfolios improved significantly by 9.6\% and 10.8\% respectively. In addition, we conducted extensive ablation studies on our model and analysis of the data to reveal further insights into the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17266v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junyan Cheng, Peter Chin</dc:creator>
    </item>
    <item>
      <title>Efficient and stable time integration of Cahn-Hilliard equations: explicit, implicit and explicit iterative schemes</title>
      <link>https://arxiv.org/abs/2409.17736</link>
      <description>arXiv:2409.17736v1 Announce Type: cross 
Abstract: To solve the Cahn-Hilliard equation numerically, a new time integration algorithm is proposed, which is based on a combination of the Eyre splitting and the local iteration modified (LIM) scheme. The latter is employed to tackle the implicit system arising each time integration step. The proposed method is gradient-stable and allows to use large time steps, whereas, regarding its computational structure, it is an explicit time integration scheme. Numerical tests are presented to demonstrate abilities of the new method and to compare it with other time integration methods for Cahn-Hilliard equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17736v1</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1134/S0965542524700945</arxiv:DOI>
      <dc:creator>M. A. Botchev, I. A. Fahurdinov, E. B. Savenkov</dc:creator>
    </item>
    <item>
      <title>Mastering truss structure optimization with tree search</title>
      <link>https://arxiv.org/abs/2406.06145</link>
      <description>arXiv:2406.06145v3 Announce Type: replace 
Abstract: This study investigates the combined use of generative grammar rules and Monte Carlo Tree Search (MCTS) for optimizing truss structures. Our approach accommodates intermediate construction stages characteristic of progressive construction settings. We demonstrate the significant robustness and computational efficiency of our approach compared to alternative reinforcement learning frameworks from previous research activities, such as Q-learning or deep Q-learning. These advantages stem from the ability of MCTS to strategically navigate large state spaces, leveraging the upper confidence bound for trees formula to effectively balance exploitation-exploration trade-offs. We also emphasize the importance of early decision nodes in the search tree, reflecting design choices crucial for highly performative solutions. Additionally, we show how MCTS dynamically adapts to complex and extensive state spaces without significantly affecting solution quality. While the focus of this paper is on truss optimization, our findings suggest MCTS as a powerful tool for addressing other increasingly complex engineering applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06145v3</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Garayalde, Luca Rosafalco, Matteo Torzoni, Alberto Corigliano</dc:creator>
    </item>
    <item>
      <title>A multi-scale probabilistic methodology to predict high-cycle fatigue lifetime for alloys with process-induced pores</title>
      <link>https://arxiv.org/abs/2409.16565</link>
      <description>arXiv:2409.16565v2 Announce Type: replace 
Abstract: A multi-scale methodology is developed in conjunction with a probabilistic fatigue lifetime model for structures with pores whose exact distribution, i.e. geometries and locations, is unknown. The method takes into account uncertainty in fatigue lifetimes in structures due to defects at two scales: micro-scale heterogeneity &amp; meso-scale pores. An element-wise probabilistic strain-life model with its criterion modified for taking into account multiaxial loading is developed for taking into account the effect of micro-scale defects on the lifetime. Meso-scale pores in the structure are taken into account via statistical modelling of the expected pore populations via a finite element method, based on tomographic scans of a small region of porous material used to make the structure. A previously implemented Neuber-type plastic correction algorithm is used for fast full-field approximation of the strain-life criterion around the statistically generated pore fields. The probability of failure of a porous structure is obtained via a weakest link assumption at the level of its constituent finite elements. The fatigue model can be identified via a maximum likelihood estimate on experimental fatigue data of structures containing different types of pore populations. The proposed method is tested on an existing high-cycle fatigue data-set of an aluminium alloy with two levels of porosity. The model requires lesser data for identification than traditional models that consider porous media as a homogeneous material, as the same base material is considered for the two grades of porous material. Numerical studies on synthetically generated data show that the method is capable of taking into account the statistical size effect in fatigue, and demonstrate that fatigue properties of subsurface porous material are lower than that of core porous material, which makes homogenisation of the method non-trivial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16565v2</guid>
      <category>cs.CE</category>
      <category>cond-mat.mtrl-sci</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek Palchoudhary, Cristian Ovalle, Vincent Maurel, Pierre Kerfriden</dc:creator>
    </item>
    <item>
      <title>Bayesian Matrix Decomposition and Applications</title>
      <link>https://arxiv.org/abs/2302.11337</link>
      <description>arXiv:2302.11337v3 Announce Type: replace-cross 
Abstract: The sole aim of this book is to give a self-contained introduction to concepts and mathematical tools in Bayesian matrix decomposition in order to seamlessly introduce matrix decomposition techniques and their applications in subsequent sections. However, we clearly realize our inability to cover all the useful and interesting results concerning Bayesian matrix decomposition and given the paucity of scope to present this discussion, e.g., the separated analysis of variational inference for conducting the optimization. We refer the reader to literature in the field of Bayesian analysis for a more detailed introduction to the related fields.
  This book is primarily a summary of purpose, significance of important Bayesian matrix decomposition methods, e.g., real-valued decomposition, nonnegative matrix factorization, Bayesian interpolative decomposition, and the origin and complexity of the methods which shed light on their applications. The mathematical prerequisite is a first course in statistics and linear algebra. Other than this modest background, the development is self-contained, with rigorous proof provided throughout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.11337v3</guid>
      <category>math.NA</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Fri, 27 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Lu</dc:creator>
    </item>
  </channel>
</rss>
