<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Apr 2024 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>When `Computing follows Vehicles': Decentralized Mobility-Aware Resource Allocation in the Edge-to-Cloud Continuum</title>
      <link>https://arxiv.org/abs/2404.13179</link>
      <description>arXiv:2404.13179v1 Announce Type: new 
Abstract: The transformation of smart mobility is unprecedented--Autonomous, shared and electric connected vehicles, along with the urgent need to meet ambitious net-zero targets by shifting to low-carbon transport modalities result in new traffic patterns and requirements for real-time computation at large-scale, for instance, augmented reality applications. The cloud computing paradigm can neither respond to such low-latency requirements nor adapt resource allocation to such dynamic spatio-temporal service requests. This paper addresses this grand challenge by introducing a novel decentralized optimization framework for mobility-aware edge-to-cloud resource allocation, service offloading, provisioning and load-balancing. In contrast to related work, this framework comes with superior efficiency and cost-effectiveness under evaluation in real-world traffic settings and mobility datasets. This breakthrough capability of `\emph{computing follows vehicles}' proves able to reduce utilization variance by more than 40 times, while preventing service deadline violations by 14\%-34\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13179v1</guid>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeinab Nezami, Emmanouil Chaniotakis, Evangelos Pournaras</dc:creator>
    </item>
    <item>
      <title>Automatic BLAS Offloading on Unified Memory Architecture: A Study on NVIDIA Grace-Hopper</title>
      <link>https://arxiv.org/abs/2404.13195</link>
      <description>arXiv:2404.13195v1 Announce Type: new 
Abstract: Porting codes to GPU often requires major efforts. While several tools exist for automatically offload numerical libraries such as BLAS and LAPACK, they often prove impractical due to the high cost of mandatory data transfer. The new unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth cache-coherent memory access of all memory from both CPU and GPU, potentially eliminating bottleneck faced in conventional architecture. This breakthrough opens up new avenues for application development and porting strategies. In this study, we introduce a new tool for automatic BLAS offload, the tool leverages the high speed cache coherent NVLink C2C interconnect in Grace-Hopper, and enables performant GPU offload for BLAS heavy applications with no code changes or recompilation. The tool was tested on two quantum chemistry or physics codes, great performance benefits were observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13195v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junjie Li, Yinzhi Wang, Xiao Liang, Hang Liu</dc:creator>
    </item>
    <item>
      <title>LLMChain: Blockchain-based Reputation System for Sharing and Evaluating Large Language Models</title>
      <link>https://arxiv.org/abs/2404.13236</link>
      <description>arXiv:2404.13236v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have witnessed rapid growth in emerging challenges and capabilities of language understanding, generation, and reasoning. Despite their remarkable performance in natural language processing-based applications, LLMs are susceptible to undesirable and erratic behaviors, including hallucinations, unreliable reasoning, and the generation of harmful content. These flawed behaviors undermine trust in LLMs and pose significant hurdles to their adoption in real-world applications, such as legal assistance and medical diagnosis, where precision, reliability, and ethical considerations are paramount. These could also lead to user dissatisfaction, which is currently inadequately assessed and captured. Therefore, to effectively and transparently assess users' satisfaction and trust in their interactions with LLMs, we design and develop LLMChain, a decentralized blockchain-based reputation system that combines automatic evaluation with human feedback to assign contextual reputation scores that accurately reflect LLM's behavior. LLMChain not only helps users and entities identify the most trustworthy LLM for their specific needs, but also provides LLM developers with valuable information to refine and improve their models. To our knowledge, this is the first time that a blockchain-based distributed framework for sharing and evaluating LLMs has been introduced. Implemented using emerging tools, LLMChain is evaluated across two benchmark datasets, showcasing its effectiveness and scalability in assessing seven different LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13236v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mouhamed Amine Bouchiha, Quentin Telnoff, Souhail Bakkali, Ronan Champagnat, Mourad Rabah, Micka\"el Coustaty, Yacine Ghamri-Doudane</dc:creator>
    </item>
    <item>
      <title>An Accurate Beam-Tracking Algorithm with Adaptive Beam Reconstruction via UAV-BSs for Mobile Users</title>
      <link>https://arxiv.org/abs/2404.13262</link>
      <description>arXiv:2404.13262v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs) with flexible deployment contribute to enlarging the distance of information transmission to mobile users (MUs) in constrained environment. However, due to the high mobility of both UAVs and MUs, it is challenging to establish an accurate beam towards the target MU with high beam gain in real-time. In this study, UAV base stations (UAV-BSs) consisting of position-known assisted UAVs (A-UAVs) and position-unknown assisted UAVs (U-UAVs) are employed to transmit data to MUs. Specifically, a bi-directional angle-aware beam tracking with adaptive beam reconstruction (BAB-AR) algorithm is proposed to construct an optimal beam that can quickly adapt the movement of the target MU. First, the angle-aware beam tracking is realized within the UAVBSs using a proposed global dynamic crow search algorithm without historical trajectory. Furthermore, the Gaussian process regression model is trained by A-UAVs to predict the azimuth and elevation angles of MUs. Meanwhile, we focus on the beam width and design a time interval adjustment mechanism for adaptive beam reconstruction to track high-speed MUs. Finally, the performance of the BAB-AR algorithm is compared with that of benchmark algorithms, and simulate results verifies that the BAB-AR algorithm can construct an accurate beam capable of covering high-speed MUs with the half power beam width in a timely manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13262v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Zhang, Sheng Gao, Xin Feng, Hongwei Yang, Geng Sun</dc:creator>
    </item>
    <item>
      <title>Breaking the Memory Wall for Heterogeneous Federated Learning with Progressive Training</title>
      <link>https://arxiv.org/abs/2404.13349</link>
      <description>arXiv:2404.13349v1 Announce Type: new 
Abstract: This paper presents ProFL, a novel progressive FL framework to effectively break the memory wall. Specifically, ProFL divides the model into different blocks based on its original architecture. Instead of updating the full model in each training round, ProFL first trains the front blocks and safely freezes them after convergence. Training of the next block is then triggered. This process iterates until the training of the whole model is completed. In this way, the memory footprint is effectively reduced for feasible deployment on heterogeneous devices. In order to preserve the feature representation of each block, we decouple the whole training process into two stages: progressive model shrinking and progressive model growing. During the progressive model shrinking stage, we meticulously design corresponding output modules to assist each block in learning the expected feature representation and obtain the initialization parameters. Then, the obtained output modules are utilized in the corresponding progressive model growing stage. Additionally, to control the training pace for each block, a novel metric from the scalar perspective is proposed to assess the learning status of each block and determines when to trigger the training of the next one. Finally, we theoretically prove the convergence of ProFL and conduct extensive experiments on representative models and datasets to evaluate the effectiveness of ProFL. The results demonstrate that ProFL effectively reduces the peak memory footprint by up to 57.4% and improves model accuracy by up to 82.4%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13349v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yebo Wu, Li Li, Chunlin Tian, Chengzhong Xu</dc:creator>
    </item>
    <item>
      <title>GWLZ: A Group-wise Learning-based Lossy Compression Framework for Scientific Data</title>
      <link>https://arxiv.org/abs/2404.13470</link>
      <description>arXiv:2404.13470v1 Announce Type: new 
Abstract: The rapid expansion of computational capabilities and the ever-growing scale of modern HPC systems present formidable challenges in managing exascale scientific data. Faced with such vast datasets, traditional lossless compression techniques prove insufficient in reducing data size to a manageable level while preserving all information intact. In response, researchers have turned to error-bounded lossy compression methods, which offer a balance between data size reduction and information retention. However, despite their utility, these compressors employing conventional techniques struggle with limited reconstruction quality. To address this issue, we draw inspiration from recent advancements in deep learning and propose GWLZ, a novel group-wise learning-based lossy compression framework with multiple lightweight learnable enhancer models. Leveraging a group of neural networks, GWLZ significantly enhances the decompressed data reconstruction quality with negligible impact on the compression efficiency. Experimental results on different fields from the Nyx dataset demonstrate remarkable improvements by GWLZ, achieving up to 20% quality enhancements with negligible overhead as low as 0.0003x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13470v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqi Jia, Sian Jin, Jinzhen Wang, Wei Niu, Dingwen Tao, Miao Yin</dc:creator>
    </item>
    <item>
      <title>Enhancing ASIC Technology Mapping via Parallel Supergate Computing</title>
      <link>https://arxiv.org/abs/2404.13614</link>
      <description>arXiv:2404.13614v1 Announce Type: new 
Abstract: With the development of large-scale integrated circuits, electronic design automation~(EDA) tools are increasingly emphasizing efficiency, with parallel algorithms becoming a trend. The optimization of delay reduction is a crucial factor for ASIC technology mapping, and supergate technology proves to be an effective method for achieving this in EDA tools flow. However, we have observed that increasing the number of generated supergates can reduce delay, but this comes at the cost of an exponential increase in computation time. In this paper, we propose a parallel supergate computing method that addresses the tradeoff between time-consuming and delay optimization. The proposed method utilizes the input-constrained supergate pattern to parallelly generate the supergate candidates, and then filter the valid supergates as the results. Experiment results show the efficiency of the proposed method, for example, it can attain the improvement of 4x speedup in computation time and 10.1 in delay reduction with 32 threads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13614v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Cai, Zonglin Yang, Liwei Ni, Biwei Xie, Xingquan Li</dc:creator>
    </item>
    <item>
      <title>Parallel AIG Refactoring via Conflict Breaking</title>
      <link>https://arxiv.org/abs/2404.13617</link>
      <description>arXiv:2404.13617v1 Announce Type: new 
Abstract: Algorithm parallelization to leverage multi-core platforms for improving the efficiency of Electronic Design Automation~(EDA) tools plays a significant role in enhancing the scalability of Integrated Circuit (IC) designs. Logic optimization is a key process in the EDA design flow to reduce the area and depth of the circuit graph by finding logically equivalent graphs for substitution, which is typically time-consuming. To address these challenges, in this paper, we first analyze two types of conflicts that need to be handled in the parallelization framework of refactoring And-Inverter Graph~(AIG). We then present a fine-grained parallel AIG refactoring method, which strikes a balance between the degree of parallelism and the conflicts encountered during the refactoring operations. Experiment results show that our parallel refactor is 28x averagely faster than the sequential algorithm on large benchmark tests with 64 physical CPU cores, and has comparable optimization quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13617v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Cai, Zonglin Yang, Liwei Ni, Junfeng Liu, Biwei Xie, Xingquan Li</dc:creator>
    </item>
    <item>
      <title>Low-ordered Orthogonal Voxel Finite Element with INT8 Tensor Cores for GPU-based Explicit Elastic Wave Propagation Analysis</title>
      <link>https://arxiv.org/abs/2404.13683</link>
      <description>arXiv:2404.13683v1 Announce Type: new 
Abstract: Faster explicit elastic wavefield simulations are required for large and complex three-dimensional media using a structured finite element method. Such wavefield simulations are suitable for GPUs, which have exhibited improved computational performance in recent years, and the use of GPUs is expected to speed up such simulations. However, available computational performance on GPUs is typically not fully exploited, and the conventional method involves some numerical dispersion. Thus, in this paper, we propose an explicit structured-mesh wavefield simulation method that uses INT8 Tensor Cores and reduces numerical dispersion to speed up computation on GPUs. The proposed method was implemented for GPUs, and its performance was evaluated in a simulation experiment of a real-world problem. The results demonstrate that the proposed method is 17.0 times faster than the conventional method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13683v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tsuyoshi Ichimura, Kohei Fujita, Muneo Hori, Maddegedara Lalith</dc:creator>
    </item>
    <item>
      <title>Adaptive Heterogeneous Client Sampling for Federated Learning over Wireless Networks</title>
      <link>https://arxiv.org/abs/2404.13804</link>
      <description>arXiv:2404.13804v1 Announce Type: new 
Abstract: Federated learning (FL) algorithms usually sample a fraction of clients in each round (partial participation) when the number of participants is large and the server's communication bandwidth is limited. Recent works on the convergence analysis of FL have focused on unbiased client sampling, e.g., sampling uniformly at random, which suffers from slow wall-clock time for convergence due to high degrees of system heterogeneity and statistical heterogeneity. This paper aims to design an adaptive client sampling algorithm for FL over wireless networks that tackles both system and statistical heterogeneity to minimize the wall-clock convergence time. We obtain a new tractable convergence bound for FL algorithms with arbitrary client sampling probability. Based on the bound, we analytically establish the relationship between the total learning time and sampling probability with an adaptive bandwidth allocation scheme, which results in a non-convex optimization problem. We design an efficient algorithm for learning the unknown parameters in the convergence bound and develop a low-complexity algorithm to approximately solve the non-convex problem. Our solution reveals the impact of system and statistical heterogeneity parameters on the optimal client sampling design. Moreover, our solution shows that as the number of sampled clients increases, the total convergence time first decreases and then increases because a larger sampling number reduces the number of rounds for convergence but results in a longer expected time per-round due to limited wireless bandwidth. Experimental results from both hardware prototype and simulation demonstrate that our proposed sampling scheme significantly reduces the convergence time compared to several baseline sampling schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13804v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bing Luo, Wenli Xiao, Shiqiang Wang, Jianwei Huang, Leandros Tassiulas</dc:creator>
    </item>
    <item>
      <title>Apodotiko: Enabling Efficient Serverless Federated Learning in Heterogeneous Environments</title>
      <link>https://arxiv.org/abs/2404.14033</link>
      <description>arXiv:2404.14033v1 Announce Type: new 
Abstract: Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, current serverless FL systems still suffer from the presence of stragglers, i.e., slow clients that impede the collaborative training process. While strategies aimed at mitigating stragglers in these systems have been proposed, they overlook the diverse hardware resource configurations among FL clients. To this end, we present Apodotiko, a novel asynchronous training strategy designed for serverless FL. Our strategy incorporates a scoring mechanism that evaluates each client's hardware capacity and dataset size to intelligently prioritize and select clients for each training round, thereby minimizing the effects of stragglers on system performance. We comprehensively evaluate Apodotiko across diverse datasets, considering a mix of CPU and GPU clients, and compare its performance against five other FL training strategies. Results from our experiments demonstrate that Apodotiko outperforms other FL training strategies, achieving an average speedup of 2.75x and a maximum speedup of 7.03x. Furthermore, our strategy significantly reduces cold starts by a factor of four on average, demonstrating suitability in serverless environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14033v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohak Chadha, Alexander Jensen, Jianfeng Gu, Osama Abboud, Michael Gerndt</dc:creator>
    </item>
    <item>
      <title>Towards Proxy Staking Accounts Based on NFTs in Ethereum</title>
      <link>https://arxiv.org/abs/2404.14074</link>
      <description>arXiv:2404.14074v1 Announce Type: new 
Abstract: Blockchain is a technology that is often used to share data and assets. However, in the decentralized ecosystem, blockchain-based systems can be utilized to share information and assets without the traditional barriers associated with solo responsibility, e.g., multi-sig wallets. This paper describes an innovative approach to blockchain networks based on a non-fungible token that behaves as an account (NFTAA). The key novelty of this article is using NFTAA to leverage the unique properties of NFTs to manage your ownership better and effectively isolate them to improve the security, transparency, and even interoperability possibilities. Additionally, the account-based solution gives us the ability and flexibility to cover regular use cases such as staking and liquid equities, but also practical composability. This article offers a simple implementation, which allows developers and researchers to choose the best solution for their needs in demand of abstract representation in any use case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14074v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Viktor Vala\v{s}t\'in, Roman Bitarovsk\'y, Kristi\'an Ko\v{s}\v{t}\'al, Ivan Kotuliak</dc:creator>
    </item>
    <item>
      <title>LLAMP: Assessing Network Latency Tolerance of HPC Applications with Linear Programming</title>
      <link>https://arxiv.org/abs/2404.14193</link>
      <description>arXiv:2404.14193v1 Announce Type: new 
Abstract: The shift towards high-bandwidth networks driven by AI workloads in data centers and HPC clusters has unintentionally aggravated network latency, adversely affecting the performance of communication-intensive HPC applications. As large-scale MPI applications often exhibit significant differences in their network latency tolerance, it is crucial to accurately determine the extent of network latency an application can withstand without significant performance degradation. Current approaches to assessing this metric often rely on specialized hardware or network simulators, which can be inflexible and time-consuming. In response, we introduce LLAMP, a novel toolchain that offers an efficient, analytical approach to evaluating HPC applications' network latency tolerance using the LogGPS model and linear programming. LLAMP equips software developers and network architects with essential insights for optimizing HPC infrastructures and strategically deploying applications to minimize latency impacts. Through our validation on a variety of MPI applications like MILC, LULESH, and LAMMPS, we demonstrate our tool's high accuracy, with relative prediction errors generally below 2%. Additionally, we include a case study of the ICON weather and climate model to illustrate LLAMP's broad applicability in evaluating collective algorithms and network topologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14193v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Siyuan Shen, Langwen Huang, Marcin Chrapek, Timo Schneider, Jai Dayal, Manisha Gajbe, Robert Wisniewski, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>Frosty: Bringing strong liveness guarantees to the Snow family of consensus protocols</title>
      <link>https://arxiv.org/abs/2404.14250</link>
      <description>arXiv:2404.14250v1 Announce Type: new 
Abstract: Snowman is the consensus protocol implemented by the Avalanche blockchain and is part of the Snow family of protocols, first introduced through the original Avalanche leaderless consensus protocol. A major advantage of Snowman is that each consensus decision only requires an expected constant communication overhead per processor in the `common' case that the protocol is not under substantial Byzantine attack, i.e. it provides a solution to the scalability problem which ensures that the expected communication overhead per processor is independent of the total number of processors $n$ during normal operation. This is the key property that would enable a consensus protocol to scale to 10,000 or more independent validators (i.e. processors). On the other hand, the two following concerns have remained:
  (1) Providing formal proofs of consistency for Snowman has presented a formidable challenge.
  (2) Liveness attacks exist in the case that a Byzantine adversary controls more than $O(\sqrt{n})$ processors, slowing termination to more than a logarithmic number of steps.
  In this paper, we address the two issues above. We consider a Byzantine adversary that controls at most $f&lt;n/5$ processors. First, we provide a simple proof of consistency for Snowman. Then we supplement Snowman with a `liveness module' that can be triggered in the case that a substantial adversary launches a liveness attack, and which guarantees liveness in this event by temporarily forgoing the communication complexity advantages of Snowman, but without sacrificing these low communication complexity advantages during normal operation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14250v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaron Buchwald, Stephen Buttolph, Andrew Lewis-Pye, Patrick O'Grady, Kevin Sekniqi</dc:creator>
    </item>
    <item>
      <title>Blockchain in a box: A portable blockchain network implementation on Raspberry Pi's</title>
      <link>https://arxiv.org/abs/2404.14282</link>
      <description>arXiv:2404.14282v1 Announce Type: new 
Abstract: In this paper we describe a prototype of a blockchain-in-a-box system which allows users to easily bootstrap the whole Ethereum Proof-of-Work (PoW) network running on multiple Raspberry Pi nodes - an inexpensive modular computers. Users are able to orchestrate the whole blockchain network using a single web based interface, for example they are able to set the topology of the peer-to-peer (P2P) connections and control the initialization parameters. Each Raspberry Pi has a screen attached which visualizes current state of local blockchain, allowing users to easily visualize the consensus of the network in real time. We show how this platform can be used to perform experiments on consensus quality while using different P2P topologies. Similar experiments can be used for demonstration purposes in a workshop or other educational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14282v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matija Pi\v{s}korec, Anton Ivaskevich, Said Haji Abukar, Lundrim Azemi, Md Rezuanul Haque, Mostafa Chegenizadeh, Claudio J. Tessone</dc:creator>
    </item>
    <item>
      <title>Robustness and Accuracy in Pipelined Bi-Conjugate Gradient Stabilized Method: A Comparative Study</title>
      <link>https://arxiv.org/abs/2404.13216</link>
      <description>arXiv:2404.13216v1 Announce Type: cross 
Abstract: In this article, we propose an accuracy-assuring technique for finding a solution for unsymmetric linear systems. Such problems are related to different areas such as image processing, computer vision, and computational fluid dynamics. Parallel implementation of Krylov subspace methods speeds up finding approximate solutions for linear systems. In this context, the refined approach in pipelined BiCGStab enhances scalability on distributed memory machines, yielding to substantial speed improvements compared to the standard BiCGStab method. However, it's worth noting that the pipelined BiCGStab algorithm sacrifices some accuracy, which is stabilized with the residual replacement technique. This paper aims to address this issue by employing the ExBLAS-based reproducible approach. We validate the idea on a set of matrices from the SuiteSparse Matrix Collection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13216v1</guid>
      <category>cs.MS</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mykhailo Havdiak, Jose I. Aliaga, Roman Iakymchuk</dc:creator>
    </item>
    <item>
      <title>Federated Transfer Learning with Task Personalization for Condition Monitoring in Ultrasonic Metal Welding</title>
      <link>https://arxiv.org/abs/2404.13278</link>
      <description>arXiv:2404.13278v1 Announce Type: cross 
Abstract: Ultrasonic metal welding (UMW) is a key joining technology with widespread industrial applications. Condition monitoring (CM) capabilities are critically needed in UMW applications because process anomalies significantly deteriorate the joining quality. Recently, machine learning models emerged as a promising tool for CM in many manufacturing applications due to their ability to learn complex patterns. Yet, the successful deployment of these models requires substantial training data that may be expensive and time-consuming to collect. Additionally, many existing machine learning models lack generalizability and cannot be directly applied to new process configurations (i.e., domains). Such issues may be potentially alleviated by pooling data across manufacturers, but data sharing raises critical data privacy concerns. To address these challenges, this paper presents a Federated Transfer Learning with Task Personalization (FTL-TP) framework that provides domain generalization capabilities in distributed learning while ensuring data privacy. By effectively learning a unified representation from feature space, FTL-TP can adapt CM models for clients working on similar tasks, thereby enhancing their overall adaptability and performance jointly. To demonstrate the effectiveness of FTL-TP, we investigate two distinct UMW CM tasks, tool condition monitoring and workpiece surface condition classification. Compared with state-of-the-art FL algorithms, FTL-TP achieves a 5.35%--8.08% improvement of accuracy in CM in new target domains. FTL-TP is also shown to perform excellently in challenging scenarios involving unbalanced data distributions and limited client fractions. Furthermore, by implementing the FTL-TP method on an edge-cloud architecture, we show that this method is both viable and efficient in practice. The FTL-TP framework is readily extensible to various other manufacturing applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13278v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmadreza Eslaminia, Yuquan Meng, Klara Nahrstedt, Chenhui Shao</dc:creator>
    </item>
    <item>
      <title>Urgent Edge Computing</title>
      <link>https://arxiv.org/abs/2404.13411</link>
      <description>arXiv:2404.13411v1 Announce Type: cross 
Abstract: This position paper introduces Urgent Edge Computing (UEC) as a paradigm shift addressing the evolving demands of time-sensitive applications in distributed edge environments, in time-critical scenarios. With a focus on ultra-low latency, availability, resource management, decentralization, self-organization, and robust security, UEC aims to facilitate operations in critical scenarios such as disaster response, environmental monitoring, and smart city management. This paper outlines and discusses the key requirements, challenges, and enablers along with a conceptual architecture. The paper also outlines the potential applications of Urgent Edge Computing</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13411v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3659994.3660315</arxiv:DOI>
      <dc:creator>Patrizio Dazzi, Luca Ferrucci, Marco Danelutto, Konstantinos Tserpes, Antonis Makris, Theodoros Theodoropoulos, Jacopo Massa, Emanuele Carlini, Matteo Mordacchini</dc:creator>
    </item>
    <item>
      <title>A Grassroots Architecture to Supplant Global Digital Platforms by a Global Digital Democracy</title>
      <link>https://arxiv.org/abs/2404.13468</link>
      <description>arXiv:2404.13468v1 Announce Type: cross 
Abstract: We present an architectural alternative to global digital platforms termed grassroots, designed to serve the social, economic, civic, and political needs of local digital communities, as well as their federation. Grassroots platforms may offer local communities an alternative to global digital platforms while operating solely on the smartphones of their members, forsaking any global resources other than the network itself. Such communities may form digital economies without initial capital or external credit, exercise sovereign democratic governance, and federate, ultimately resulting in the grassroots formation of a global digital democracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13468v1</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ehud Shapiro</dc:creator>
    </item>
    <item>
      <title>FedTrans: Efficient Federated Learning Over Heterogeneous Clients via Model Transformation</title>
      <link>https://arxiv.org/abs/2404.13515</link>
      <description>arXiv:2404.13515v1 Announce Type: cross 
Abstract: Federated learning (FL) aims to train machine learning (ML) models across potentially millions of edge client devices. Yet, training and customizing models for FL clients is notoriously challenging due to the heterogeneity of client data, device capabilities, and the massive scale of clients, making individualized model exploration prohibitively expensive. State-of-the-art FL solutions personalize a globally trained model or concurrently train multiple models, but they often incur suboptimal model accuracy and huge training costs.
  In this paper, we introduce FedTrans, a multi-model FL training framework that automatically produces and trains high-accuracy, hardware-compatible models for individual clients at scale. FedTrans begins with a basic global model, identifies accuracy bottlenecks in model architectures during training, and then employs model transformation to derive new models for heterogeneous clients on the fly. It judiciously assigns models to individual clients while performing soft aggregation on multi-model updates to minimize total training costs. Our evaluations using realistic settings show that FedTrans improves individual client model accuracy by 14% - 72% while slashing training costs by 1.6X - 20X over state-of-the-art solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13515v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>MLSys (2024)</arxiv:journal_reference>
      <dc:creator>Yuxuan Zhu, Jiachen Liu, Mosharaf Chowdhury, Fan Lai</dc:creator>
    </item>
    <item>
      <title>SmartMem: Layout Transformation Elimination and Adaptation for Efficient DNN Execution on Mobile</title>
      <link>https://arxiv.org/abs/2404.13528</link>
      <description>arXiv:2404.13528v1 Announce Type: cross 
Abstract: This work is motivated by recent developments in Deep Neural Networks, particularly the Transformer architectures underlying applications such as ChatGPT, and the need for performing inference on mobile devices. Focusing on emerging transformers (specifically the ones with computationally efficient Swin-like architectures) and large models (e.g., Stable Diffusion and LLMs) based on transformers, we observe that layout transformations between the computational operators cause a significant slowdown in these applications. This paper presents SmartMem, a comprehensive framework for eliminating most layout transformations, with the idea that multiple operators can use the same tensor layout through careful choice of layout and implementation of operations. Our approach is based on classifying the operators into four groups, and considering combinations of producer-consumer edges between the operators. We develop a set of methods for searching such layouts. Another component of our work is developing efficient memory layouts for 2.5 dimensional memory commonly seen in mobile devices. Our experimental results show that SmartMem outperforms 5 state-of-the-art DNN execution frameworks on mobile devices across 18 varied neural networks, including CNNs, Transformers with both local and global attention, as well as LLMs. In particular, compared to DNNFusion, SmartMem achieves an average speedup of 2.8$\times$, and outperforms TVM and MNN with speedups of 6.9$\times$ and 7.9$\times$, respectively, on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13528v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3620666.3651384</arxiv:DOI>
      <dc:creator>Wei Niu, Md Musfiqur Rahman Sanim, Zhihao Shu, Jiexiong Guan, Xipeng Shen, Miao Yin, Gagan Agrawal, Bin Ren</dc:creator>
    </item>
    <item>
      <title>DesTest: A Decentralised Testing Architecture for Improving Data Accuracy of Blockchain Oracle</title>
      <link>https://arxiv.org/abs/2404.13535</link>
      <description>arXiv:2404.13535v1 Announce Type: cross 
Abstract: Blockchain technology ensures secure and trustworthy data flow between multiple participants on the chain, but interoperability of on-chain and off-chain data has always been a difficult problem that needs to be solved. To solve the problem that blockchain systems cannot access off-chain data, oracle is introduced. however, existing research mainly focuses on the consistency and integrity of data, but ignores the problem that oracle nodes may be externally attacked or provide false data for selfish motives, resulting in the unresolved problem of data accuracy. In this paper, we introduce a new decentralized testing architecture (DesTest) that aims to improve data accuracy. A blockchain oracle random secret testing mechanism is first proposed to enhance the monitoring and verification of nodes by introducing a dynamic anonymized question-verification committee. Based on this, a comprehensive evaluation incentive mechanism is designed to incentivize honest work performance by evaluating nodes based on their reputation scores. The simulation results show that we successfully reduced the discrete entropy value of the acquired data and the real value of the data by 61.4%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13535v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xueying Zeng, Youquan Xian, Chunpei Li, Zhengdong Hu, Peng Liu</dc:creator>
    </item>
    <item>
      <title>Rate Analysis of Coupled Distributed Stochastic Approximation for Misspecified Optimization</title>
      <link>https://arxiv.org/abs/2404.13669</link>
      <description>arXiv:2404.13669v1 Announce Type: cross 
Abstract: We consider an $n$ agents distributed optimization problem with imperfect information characterized in a parametric sense, where the unknown parameter can be solved by a distinct distributed parameter learning problem. Though each agent only has access to its local parameter learning and computational problem, they mean to collaboratively minimize the average of their local cost functions. To address the special optimization problem, we propose a coupled distributed stochastic approximation algorithm, in which every agent updates the current beliefs of its unknown parameter and decision variable by stochastic approximation method; and then averages the beliefs and decision variables of its neighbors over network in consensus protocol. Our interest lies in the convergence analysis of this algorithm. We quantitatively characterize the factors that affect the algorithm performance, and prove that the mean-squared error of the decision variable is bounded by $\mathcal{O}(\frac{1}{nk})+\mathcal{O}\left(\frac{1}{\sqrt{n}(1-\rho_w)}\right)\frac{1}{k^{1.5}}+\mathcal{O}\big(\frac{1}{(1-\rho_w)^2} \big)\frac{1}{k^2}$, where $k$ is the iteration count and $(1-\rho_w)$ is the spectral gap of the network weighted adjacency matrix. It reveals that the network connectivity characterized by $(1-\rho_w)$ only influences the high order of convergence rate, while the domain rate still acts the same as the centralized algorithm. In addition, we analyze that the transient iteration needed for reaching its dominant rate $\mathcal{O}(\frac{1}{nk})$ is $\mathcal{O}(\frac{n}{(1-\rho_w)^2})$. Numerical experiments are carried out to demonstrate the theoretical results by taking different CPUs as agents, which is more applicable to real-world distributed scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13669v1</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaqun Yang, Jinlong Lei</dc:creator>
    </item>
    <item>
      <title>HamilToniQ: An Open-Source Benchmark Toolkit for Quantum Computers</title>
      <link>https://arxiv.org/abs/2404.13971</link>
      <description>arXiv:2404.13971v1 Announce Type: cross 
Abstract: In this paper, we introduce HamilToniQ, an open-source, and application-oriented benchmarking toolkit for the comprehensive evaluation of Quantum Processing Units (QPUs). Designed to navigate the complexities of quantum computations, HamilToniQ incorporates a methodological framework assessing QPU types, topologies, and multi-QPU systems. The toolkit facilitates the evaluation of QPUs' performance through multiple steps including quantum circuit compilation and quantum error mitigation (QEM), integrating strategies that are unique to each stage. HamilToniQ's standardized score, H-Score, quantifies the fidelity and reliability of QPUs, providing a multidimensional perspective of QPU performance. With a focus on the Quantum Approximate Optimization Algorithm (QAOA), the toolkit enables direct, comparable analysis of QPUs, enhancing transparency and equity in benchmarking. Demonstrated in this paper, HamilToniQ has been validated on various IBM QPUs, affirming its effectiveness and robustness. Overall, HamilToniQ significantly contributes to the advancement of the quantum computing field by offering precise and equitable benchmarking metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13971v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaotian Xu, Kuan-Cheng Chen, Robert Wille</dc:creator>
    </item>
    <item>
      <title>New Solutions Based on the Generalized Eigenvalue Problem for the Data Collaboration Analysis</title>
      <link>https://arxiv.org/abs/2404.14164</link>
      <description>arXiv:2404.14164v1 Announce Type: cross 
Abstract: In recent years, the accumulation of data across various institutions has garnered attention for the technology of confidential data analysis, which improves analytical accuracy by sharing data between multiple institutions while protecting sensitive information. Among these methods, Data Collaboration Analysis (DCA) is noted for its efficiency in terms of computational cost and communication load, facilitating data sharing and analysis across different institutions while safeguarding confidential information. However, existing optimization problems for determining the necessary collaborative functions have faced challenges, such as the optimal solution for the collaborative representation often being a zero matrix and the difficulty in understanding the process of deriving solutions. This research addresses these issues by formulating the optimization problem through the segmentation of matrices into column vectors and proposing a solution method based on the generalized eigenvalue problem. Additionally, we demonstrate methods for constructing collaborative functions more effectively through weighting and the selection of efficient algorithms suited to specific situations. Experiments using real-world datasets have shown that our proposed formulation and solution for the collaborative function optimization problem achieve superior predictive accuracy compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14164v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Kawakami, Yuichi Takano, Akira Imakura</dc:creator>
    </item>
    <item>
      <title>Intrusion Tolerance for Networked Systems through Two-Level Feedback Control</title>
      <link>https://arxiv.org/abs/2404.01741</link>
      <description>arXiv:2404.01741v2 Announce Type: replace 
Abstract: We formulate intrusion tolerance for a system with service replicas as a two-level optimal control problem. On the local level node controllers perform intrusion recovery, and on the global level a system controller manages the replication factor. The local and global control problems can be formulated as classical problems in operations research, namely, the machine replacement problem and the inventory replenishment problem. Based on this formulation, we design TOLERANCE, a novel control architecture for intrusion-tolerant systems. We prove that the optimal control strategies on both levels have threshold structure and design efficient algorithms for computing them. We implement and evaluate TOLERANCE in an emulation environment where we run 10 types of network intrusions. The results show that TOLERANCE can improve service availability and reduce operational cost compared with state-of-the-art intrusion-tolerant systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01741v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.GT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kim Hammar, Rolf Stadler</dc:creator>
    </item>
    <item>
      <title>Stable Blockchain Sharding under Adversarial Transaction Generation</title>
      <link>https://arxiv.org/abs/2404.04438</link>
      <description>arXiv:2404.04438v2 Announce Type: replace 
Abstract: Sharding is used to improve the scalability and performance of blockchain systems. We investigate the stability of blockchain sharding, where transactions are continuously generated by an adversarial model. The system consists of $n$ processing nodes that are divided into $s$ shards. Following the paradigm of classical adversarial queuing theory, transactions are continuously received at injection rate $\rho \leq 1$ and burstiness $b &gt; 0$. We give an absolute upper bound $\max\{ \frac{2}{k+1}, \frac{2}{ \left\lfloor\sqrt{2s}\right\rfloor}\}$ on the maximum injection rate for which any scheduler could guarantee bounded queues and latency of transactions, where $k$ is the number of shards that each transaction accesses. We next give a basic distributed scheduling algorithm for uniform systems where shards are equally close to each other. To guarantee stability, the injection rate is limited to $\rho \leq \max\{ \frac{1}{18k}, \frac{1}{\lceil 18 \sqrt{s} \rceil} \}$. We then provide a fully distributed scheduling algorithm for non-uniform systems where shards are arbitrarily far from each other. By using a hierarchical clustering of the shards, stability is guaranteed with injection rate $\rho \leq \frac{1}{c_1d \log^2 s} \cdot \max\{ \frac{1}{k}, \frac{1}{\sqrt{s}} \}$, where $d$ is the worst distance of any transaction to the shards it will access, and $c_1$ is some positive constant. We also conduct simulations to evaluate the algorithms and measure the average queue sizes and latency throughout the system. To our knowledge, this is the first adversarial stability analysis of sharded blockchain systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04438v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ramesh Adhikari, Costas Busch, Dariusz R. Kowalski</dc:creator>
    </item>
    <item>
      <title>Measuring Arbitrage Losses and Profitability of AMM Liquidity</title>
      <link>https://arxiv.org/abs/2404.05803</link>
      <description>arXiv:2404.05803v2 Announce Type: replace 
Abstract: This paper presents the results of a comprehensive empirical study of losses to arbitrageurs (following the formalization of loss-versus-rebalancing by [Milionis et al., 2022]) incurred by liquidity providers on automated market makers (AMMs). We show that those losses exceed the fees earned by liquidity providers across many of the largest AMM liquidity pools (on Uniswap). Remarkably, we also find that the Uniswap v2 pools are more profitable for passive LPs than their Uniswap v3 counterparts. We also investigate how arbitrage losses change with block times. As expected, arbitrage losses decrease when block production is faster. However, the rate of the decline varies significantly across different trading pairs. For instance, when comparing 100ms block times to Ethereum's current 12-second block times, the decrease in losses to arbitrageurs ranges between 20% to 70%, depending on the specific trading pair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05803v2</guid>
      <category>cs.DC</category>
      <category>q-fin.TR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robin Fritsch, Andrea Canidio</dc:creator>
    </item>
    <item>
      <title>Adaptive Compression in Federated Learning via Side Information</title>
      <link>https://arxiv.org/abs/2306.12625</link>
      <description>arXiv:2306.12625v3 Announce Type: replace-cross 
Abstract: The high communication cost of sending model updates from the clients to the server is a significant bottleneck for scalable federated learning (FL). Among existing approaches, state-of-the-art bitrate-accuracy tradeoffs have been achieved using stochastic compression methods -- in which the client $n$ sends a sample from a client-only probability distribution $q_{\phi^{(n)}}$, and the server estimates the mean of the clients' distributions using these samples. However, such methods do not take full advantage of the FL setup where the server, throughout the training process, has side information in the form of a global distribution $p_{\theta}$ that is close to the clients' distribution $q_{\phi^{(n)}}$ in Kullback-Leibler (KL) divergence. In this work, we exploit this closeness between the clients' distributions $q_{\phi^{(n)}}$'s and the side information $p_{\theta}$ at the server, and propose a framework that requires approximately $D_{KL}(q_{\phi^{(n)}}|| p_{\theta})$ bits of communication. We show that our method can be integrated into many existing stochastic compression frameworks to attain the same (and often higher) test accuracy with up to $82$ times smaller bitrate than the prior work -- corresponding to 2,650 times overall compression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.12625v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Berivan Isik, Francesco Pase, Deniz Gunduz, Sanmi Koyejo, Tsachy Weissman, Michele Zorzi</dc:creator>
    </item>
    <item>
      <title>A Simple $(1-\epsilon)$-Approximation Semi-Streaming Algorithm for Maximum (Weighted) Matching</title>
      <link>https://arxiv.org/abs/2307.02968</link>
      <description>arXiv:2307.02968v2 Announce Type: replace-cross 
Abstract: We present a simple semi-streaming algorithm for $(1-\epsilon)$-approximation of bipartite matching in $O(\log{\!(n)}/\epsilon)$ passes. This matches the performance of state-of-the-art "$\epsilon$-efficient" algorithms -- the ones with much better dependence on $\epsilon$ albeit with some mild dependence on $n$ -- while being considerably simpler.
  The algorithm relies on a direct application of the multiplicative weight update method with a self-contained primal-dual analysis that can be of independent interest. To show case this, we use the same ideas, alongside standard tools from matching theory, to present an equally simple semi-streaming algorithm for $(1-\epsilon)$-approximation of weighted matchings in general (not necessarily bipartite) graphs, again in $O(\log{\!(n)}/\epsilon)$ passes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02968v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sepehr Assadi</dc:creator>
    </item>
    <item>
      <title>Arkade: k-Nearest Neighbor Search With Non-Euclidean Distances using GPU Ray Tracing</title>
      <link>https://arxiv.org/abs/2311.09168</link>
      <description>arXiv:2311.09168v2 Announce Type: replace-cross 
Abstract: High-performance implementations of $k$-Nearest Neighbor Search ($k$NN) in low dimensions use tree-based data structures. Tree algorithms are hard to parallelize on GPUs due to their irregularity. However, newer Nvidia GPUs offer hardware support for tree operations through ray-tracing cores. Recent works have proposed using RT cores to implement $k$NN search, but they all have a hardware-imposed constraint on the distance metric used in the search -- the Euclidean distance. We propose and implement two reductions to support $k$NN for a broad range of distances other than the Euclidean distance: Arkade Filter-Refine and Arkade Monotone Transformation, each of which allows non-Euclidean distance-based nearest neighbor queries to be performed in terms of the Euclidean distance. With our reductions, we observe that $k$NN search time speedups range between $1.6$x-$200$x and $1.3$x-$33.1$x over various state-of-the-art GPU shader core and RT core baselines, respectively. In evaluation, we provide several insights on RT architectures' ability to efficiently build and traverse the tree by analyzing the $k$NN search time trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09168v2</guid>
      <category>cs.GR</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3650200.3656601</arxiv:DOI>
      <dc:creator>Durga Mandarapu, Vani Nagarajan, Artem Pelenitsyn, Milind Kulkarni</dc:creator>
    </item>
    <item>
      <title>Tascade: Hardware Support for Atomic-free, Asynchronous and Efficient Reduction Trees</title>
      <link>https://arxiv.org/abs/2311.15810</link>
      <description>arXiv:2311.15810v2 Announce Type: replace-cross 
Abstract: Graph search and sparse data-structure traversal workloads contain challenging irregular memory patterns on global data structures that need to be modified atomically. Distributed processing of these workloads has relied on server threads operating on their own data copies that are merged upon global synchronization. As parallelism increases within each server, the communication challenges that arose in distributed systems a decade ago are now being encountered within large manycore servers. Prior work has achieved scalability for sparse applications up to thousands of PUs on-chip, but does not scale further due to increasing communication distances and load-imbalance across PUs. To address these challenges we propose Tascade, a hardware-software co-design that offers support for storage-efficient data-private reductions as well as asynchronous and opportunistic reduction trees. Tascade introduces an execution model along with supporting hardware design that allows coalescing of data updates regionally and merges the data from these regions through cascaded updates. Together, Tascade innovations minimize communication and increase work balance in task-based parallelization schemes and scales up to a million PUs. We evaluate six applications and four datasets to provide a detailed analysis of Tascade's performance, power, and traffic-reduction gains over prior work. Our parallelization of Breadth-First-Search with RMAT-26 across a million PUs -- the largest of the literature -- reaches over 7600 GTEPS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15810v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcelo Orenes-Vera, Esin Tureci, David Wentzlaff, Margaret Martonosi</dc:creator>
    </item>
    <item>
      <title>Muchisim: A Simulation Framework for Design Exploration of Multi-Chip Manycore Systems</title>
      <link>https://arxiv.org/abs/2312.10244</link>
      <description>arXiv:2312.10244v2 Announce Type: replace-cross 
Abstract: The design space exploration of scaled-out manycores for communication-intensive applications (e.g., graph analytics and sparse linear algebra) is hampered due to either lack of scalability or accuracy of existing frameworks at simulating data-dependent execution patterns. This paper presents MuchiSim, a novel parallel simulator designed to address these challenges when exploring the design space of distributed multi-chiplet manycore architectures. We evaluate MuchiSim at simulating systems with up to a million interconnected processing units (PUs) while modeling data movement and communication cycle by cycle. In addition to performance, MuchiSim reports the energy, area, and cost of the simulated system. It also comes with a benchmark application suite and two data visualization tools. MuchiSim supports various parallelization strategies and communication primitives such as task-based parallelization and message passing, making it highly relevant for architectures with software-managed coherence and distributed memory. Via a case study, we show that MuchiSim helps users explore the balance between memory and computation units and the constraints related to chiplet integration and inter-chip communication. MuchiSim enables evaluating new techniques or design parameters for systems at scales that are more realistic for modern parallel systems, opening the gate for further research in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10244v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcelo Orenes-Vera, Esin Tureci, Margaret Martonosi, David Wentzlaff</dc:creator>
    </item>
    <item>
      <title>Flock: A Low-Cost Streaming Query Engine on FaaS Platforms</title>
      <link>https://arxiv.org/abs/2312.16735</link>
      <description>arXiv:2312.16735v4 Announce Type: replace-cross 
Abstract: Existing serverless data analytics systems rely on external storage services like S3 for data shuffling and communication between cloud functions. While this approach provides the elasticity benefits of serverless computing, it incurs additional latency and cost overheads. We present Flock, a novel cloud-native streaming query engine that leverages the on-demand scalability of FaaS platforms for real-time data analytics. Flock utilizes function invocation payloads for efficient data exchange, eliminating the need for external storage. This not only reduces latency and cost but also simplifies the architecture by removing the requirement for a centralized coordinator. Flock employs a template-based approach to dynamically create cloud functions for each query stage and a function group mechanism for handling data aggregation and shuffling. It supports both SQL and DataFrame APIs, making it easy to use. Our evaluation shows that Flock provides significant performance gains and cost savings compared to existing serverless and serverful streaming systems. It outperforms Apache Flink by 10-20x in cost while achieving similar latency and throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16735v4</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gang Liao, Amol Deshpande, Daniel J. Abadi</dc:creator>
    </item>
    <item>
      <title>Functionally-Complete Boolean Logic in Real DRAM Chips: Experimental Characterization and Analysis</title>
      <link>https://arxiv.org/abs/2402.18736</link>
      <description>arXiv:2402.18736v2 Announce Type: replace-cross 
Abstract: Processing-using-DRAM (PuD) is an emerging paradigm that leverages the analog operational properties of DRAM circuitry to enable massively parallel in-DRAM computation. PuD has the potential to reduce or eliminate costly data movement between processing elements and main memory. Prior works experimentally demonstrate three-input MAJ (MAJ3) and two-input AND and OR operations in commercial off-the-shelf (COTS) DRAM chips. Yet, demonstrations on COTS DRAM chips do not provide a functionally complete set of operations.
  We experimentally demonstrate that COTS DRAM chips are capable of performing 1) functionally-complete Boolean operations: NOT, NAND, and NOR and 2) many-input (i.e., more than two-input) AND and OR operations. We present an extensive characterization of new bulk bitwise operations in 256 off-the-shelf modern DDR4 DRAM chips. We evaluate the reliability of these operations using a metric called success rate: the fraction of correctly performed bitwise operations. Among our 19 new observations, we highlight four major results. First, we can perform the NOT operation on COTS DRAM chips with a 98.37% success rate on average. Second, we can perform up to 16-input NAND, NOR, AND, and OR operations on COTS DRAM chips with high reliability (e.g., 16-input NAND, NOR, AND, and OR with an average success rate of 94.94%, 95.87%, 94.94%, and 95.85%, respectively). Third, data pattern only slightly affects bitwise operations. Our results show that executing NAND, NOR, AND, and OR operations with random data patterns decreases the success rate compared to all logic-1/logic-0 patterns by 1.39%, 1.97%, 1.43%, and 1.98%, respectively. Fourth, bitwise operations are highly resilient to temperature changes, with small success rate fluctuations of at most 1.66% when the temperature is increased from 50C to 95C. We open-source our infrastructure at https://github.com/CMU-SAFARI/FCDRAM</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18736v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ismail Emir Yuksel, Yahya Can Tugrul, Ataberk Olgun, F. Nisa Bostanci, A. Giray Yaglikci, Geraldo F. Oliveira, Haocong Luo, Juan G\'omez-Luna, Mohammad Sadrosadati, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>Personalized Federated Learning via Stacking</title>
      <link>https://arxiv.org/abs/2404.10957</link>
      <description>arXiv:2404.10957v2 Announce Type: replace-cross 
Abstract: Traditional Federated Learning (FL) methods typically train a single global model collaboratively without exchanging raw data. In contrast, Personalized Federated Learning (PFL) techniques aim to create multiple models that are better tailored to individual clients' data. We present a novel personalization approach based on stacked generalization where clients directly send each other privacy-preserving models to be used as base models to train a meta-model on private data. Our approach is flexible, accommodating various privacy-preserving techniques and model types, and can be applied in horizontal, hybrid, and vertically partitioned federations. Additionally, it offers a natural mechanism for assessing each client's contribution to the federation. Through comprehensive evaluations across diverse simulated data heterogeneity scenarios, we showcase the effectiveness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10957v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emilio Cantu-Cervini</dc:creator>
    </item>
    <item>
      <title>FedFa: A Fully Asynchronous Training Paradigm for Federated Learning</title>
      <link>https://arxiv.org/abs/2404.11015</link>
      <description>arXiv:2404.11015v2 Announce Type: replace-cross 
Abstract: Federated learning has been identified as an efficient decentralized training paradigm for scaling the machine learning model training on a large number of devices while guaranteeing the data privacy of the trainers. FedAvg has become a foundational parameter update strategy for federated learning, which has been promising to eliminate the effect of the heterogeneous data across clients and guarantee convergence. However, the synchronization parameter update barriers for each communication round during the training significant time on waiting, slowing down the training procedure. Therefore, recent state-of-the-art solutions propose using semi-asynchronous approaches to mitigate the waiting time cost with guaranteed convergence. Nevertheless, emerging semi-asynchronous approaches are unable to eliminate the waiting time completely.
  We propose a full asynchronous training paradigm, called FedFa, which can guarantee model convergence and eliminate the waiting time completely for federated learning by using a few buffered results on the server for parameter updating. Further, we provide theoretical proof of the convergence rate for our proposed FedFa. Extensive experimental results indicate our approach effectively improves the training performance of federated learning by up to 6x and 4x speedup compared to the state-of-the-art synchronous and semi-asynchronous strategies while retaining high accuracy in both IID and Non-IID scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11015v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>IJCAI 2024: the 33rd International Joint Conference on Artificial Intelligence</arxiv:journal_reference>
      <dc:creator>Haotian Xu, Zhaorui Zhang, Sheng Di, Benben Liu, Khalid Ayed Alharthi, Jiannong Cao</dc:creator>
    </item>
  </channel>
</rss>
