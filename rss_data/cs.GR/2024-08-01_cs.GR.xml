<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Aug 2024 01:40:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Seamless Parametrization in Penner Coordinates</title>
      <link>https://arxiv.org/abs/2407.21342</link>
      <description>arXiv:2407.21342v1 Announce Type: new 
Abstract: We introduce a conceptually simple and efficient algorithm for seamless parametrization, a key element in constructing quad layouts and texture charts on surfaces. More specifically, we consider the construction of parametrizations with prescribed holonomy signatures i.e., a set of angles at singularities, and rotations along homology loops, preserving which is essential for constructing parametrizations following an input field, as well as for user control of the parametrization structure. Our algorithm performs exceptionally well on a large dataset based on Thingi10k [Zhou and Jacobson 2016], (16156 meshes) as well as on a challenging smaller dataset of [Myles et al. 2014], converging, on average, in 9 iterations. Although the algorithm lacks a formal mathematical guarantee, presented empirical evidence and the connections between convex optimization and closely related algorithms, suggest that a similar formulation can be found for this algorithm in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21342v1</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3658202</arxiv:DOI>
      <arxiv:journal_reference>ACM Trans. Graph. 43, 4, Article 61 (July 2024), 13 pages</arxiv:journal_reference>
      <dc:creator>Ryan Capouellez, Denis Zorin</dc:creator>
    </item>
    <item>
      <title>Deformable 3D Shape Diffusion Model</title>
      <link>https://arxiv.org/abs/2407.21428</link>
      <description>arXiv:2407.21428v1 Announce Type: new 
Abstract: The Gaussian diffusion model, initially designed for image generation, has recently been adapted for 3D point cloud generation. However, these adaptations have not fully considered the intrinsic geometric characteristics of 3D shapes, thereby constraining the diffusion model's potential for 3D shape manipulation. To address this limitation, we introduce a novel deformable 3D shape diffusion model that facilitates comprehensive 3D shape manipulation, including point cloud generation, mesh deformation, and facial animation. Our approach innovatively incorporates a differential deformation kernel, which deconstructs the generation of geometric structures into successive non-rigid deformation stages. By leveraging a probabilistic diffusion model to simulate this step-by-step process, our method provides a versatile and efficient solution for a wide range of applications, spanning from graphics rendering to facial expression animation. Empirical evidence highlights the effectiveness of our approach, demonstrating state-of-the-art performance in point cloud generation and competitive results in mesh deformation. Additionally, extensive visual demonstrations reveal the significant potential of our approach for practical applications. Our method presents a unique pathway for advancing 3D shape manipulation and unlocking new opportunities in the realm of virtual reality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21428v1</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dengsheng Chen, Jie Hu, Xiaoming Wei, Enhua Wu</dc:creator>
    </item>
    <item>
      <title>Accelerating Transfer Function Update for Distance Map based Volume Rendering</title>
      <link>https://arxiv.org/abs/2407.21552</link>
      <description>arXiv:2407.21552v1 Announce Type: new 
Abstract: Direct volume rendering using ray-casting is widely used in practice. By using GPUs and applying acceleration techniques as empty space skipping, high frame rates are possible on modern hardware. This enables performance-critical use-cases such as virtual reality volume rendering. The currently fastest known technique uses volumetric distance maps to skip empty sections of the volume during ray-casting but requires the distance map to be updated per transfer function change. In this paper, we demonstrate a technique for subdividing the volume intensity range into partitions and deriving what we call partitioned distance maps. These can be used to accelerate the distance map computation for a newly changed transfer function by a factor up to 30. This allows the currently fastest known empty space skipping approach to be used while maintaining high frame rates even when the transfer function is changed frequently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21552v1</guid>
      <category>cs.GR</category>
      <category>cs.PF</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Rauter, Lukas Zimmermann, Markus Zeilinger</dc:creator>
    </item>
    <item>
      <title>Does empirical evidence from healthy aging studies predict a practical difference between visualizations for different age groups?</title>
      <link>https://arxiv.org/abs/2407.21767</link>
      <description>arXiv:2407.21767v1 Announce Type: new 
Abstract: When communicating critical information to decision-makers, one of the major challenges in visualization is whether the communication is affected by different perceptual or cognitive abilities, one major influencing factor is age. We review both visualization and psychophysics literature to understand where quantitative evidence exists on age differences in visual perception. Using contrast sensitivity data from the literature we show how the differences between visualizations for different age groups can be predicted using a new model of visible frequency range with age. The model assumed that at threshold values some visual data will not be visible to older people (spatial frequency &gt; 2 and contrast &lt;=0.01). We apply this result to a practical visualization and show an example that at higher levels of contrast, the visual signal should be perceivable by all viewers over 20. Universally usable visualization should use a contrast of 0.02 or higher and be designed to avoid spatial frequencies greater than eight cycles per degree to accommodate all ages. There remains much research to do on to translate psychophysics results to practical quantitative guidelines for visualization producers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21767v1</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Shao, Y. Li, A. I. Meso, N. Holliman</dc:creator>
    </item>
    <item>
      <title>Fine-grained Metrics for Point Cloud Semantic Segmentation</title>
      <link>https://arxiv.org/abs/2407.21289</link>
      <description>arXiv:2407.21289v1 Announce Type: cross 
Abstract: Two forms of imbalances are commonly observed in point cloud semantic segmentation datasets: (1) category imbalances, where certain objects are more prevalent than others; and (2) size imbalances, where certain objects occupy more points than others. Because of this, the majority of categories and large objects are favored in the existing evaluation metrics. This paper suggests fine-grained mIoU and mAcc for a more thorough assessment of point cloud segmentation algorithms in order to address these issues. Richer statistical information is provided for models and datasets by these fine-grained metrics, which also lessen the bias of current semantic segmentation metrics towards large objects. The proposed metrics are used to train and assess various semantic segmentation algorithms on three distinct indoor and outdoor semantic segmentation datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21289v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuheng Lu, Ting Wu, Yuewei Dai, Weiqing Li, Zhiyong Su</dc:creator>
    </item>
    <item>
      <title>Parallel Compositing of Volumetric Depth Images for Interactive Visualization of Distributed Volumes at High Frame Rates</title>
      <link>https://arxiv.org/abs/2206.14503</link>
      <description>arXiv:2206.14503v3 Announce Type: replace 
Abstract: We present a parallel compositing algorithm for Volumetric Depth Images (VDIs) of large three-dimensional volume data. Large distributed volume data are routinely produced in both numerical simulations and experiments, yet it remains challenging to visualize them at smooth, interactive frame rates. VDIs are view-dependent piecewise constant representations of volume data that offer a potential solution. They are more compact and less expensive to render than the original data. So far, however, there is no method for generating VDIs from distributed data. We propose an algorithm that enables this by sort-last parallel generation and compositing of VDIs with automatically chosen content-adaptive parameters. The resulting composited VDI can then be streamed for remote display, providing responsive visualization of large, distributed volume data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.14503v3</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.2312/pgv.20231082</arxiv:DOI>
      <arxiv:journal_reference>Eurographics Symposium on Parallel Graphics and Visualization 2023, ISBN 978-3-03868-215-8, ISSN 1727-348X</arxiv:journal_reference>
      <dc:creator>Aryaman Gupta, Pietro Incardona, Anton Brock, Guido Reina, Steffen Frey, Stefan Gumhold, Ulrik G\"unther, Ivo F. Sbalzarini</dc:creator>
    </item>
    <item>
      <title>Implicit frictional dynamics with soft constraints</title>
      <link>https://arxiv.org/abs/2211.10618</link>
      <description>arXiv:2211.10618v3 Announce Type: replace 
Abstract: Dynamics simulation with frictional contacts is important for a wide range of applications, from cloth simulation to object manipulation. Recent methods using smoothed lagged friction forces have enabled robust and differentiable simulation of elastodynamics with friction. However, the resulting frictional behavior can be inaccurate and may not converge to analytic solutions. Here we evaluate the accuracy of lagged friction models in comparison with implicit frictional contact systems. We show that major inaccuracies near the stick-slip threshold in such systems are caused by lagging of friction forces rather than by smoothing the Coulomb friction curve. Furthermore, we demonstrate how systems involving implicit or lagged friction can be correctly used with higher-order time integration and highlight limitations in earlier attempts. We demonstrate how to exploit forward-mode automatic differentiation to simplify and, in some cases, improve the performance of the inexact Newton method. Finally, we show that other complex phenomena can also be simulated effectively while maintaining smoothness of the entire system. We extend our method to exhibit stick-slip frictional behavior and preserve volume on compressible and nearly-incompressible media using soft constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.10618v3</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Egor Larionov, Andreas Longva, Uri M. Ascher, Jan Bender, Dinesh K. Pai</dc:creator>
    </item>
    <item>
      <title>Monocular Human-Object Reconstruction in the Wild</title>
      <link>https://arxiv.org/abs/2407.20566</link>
      <description>arXiv:2407.20566v2 Announce Type: replace-cross 
Abstract: Learning the prior knowledge of the 3D human-object spatial relation is crucial for reconstructing human-object interaction from images and understanding how humans interact with objects in 3D space. Previous works learn this prior from datasets collected in controlled environments, but due to the diversity of domains, they struggle to generalize to real-world scenarios. To overcome this limitation, we present a 2D-supervised method that learns the 3D human-object spatial relation prior purely from 2D images in the wild. Our method utilizes a flow-based neural network to learn the prior distribution of the 2D human-object keypoint layout and viewports for each image in the dataset. The effectiveness of the prior learned from 2D images is demonstrated on the human-object reconstruction task by applying the prior to tune the relative pose between the human and the object during the post-optimization stage. To validate and benchmark our method on in-the-wild images, we collect the WildHOI dataset from the YouTube website, which consists of various interactions with 8 objects in real-world scenarios. We conduct the experiments on the indoor BEHAVE dataset and the outdoor WildHOI dataset. The results show that our method achieves almost comparable performance with fully 3D supervised methods on the BEHAVE dataset, even if we have only utilized the 2D layout information, and outperforms previous methods in terms of generality and interaction diversity on in-the-wild images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20566v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3664647.3681452</arxiv:DOI>
      <dc:creator>Chaofan Huo, Ye Shi, Jingya Wang</dc:creator>
    </item>
  </channel>
</rss>
