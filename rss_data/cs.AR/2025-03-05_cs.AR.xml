<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Mar 2025 02:50:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>SWAPPER: Dynamic Operand Swapping in Non-commutative Approximate Circuits for Online Error Reduction</title>
      <link>https://arxiv.org/abs/2503.02608</link>
      <description>arXiv:2503.02608v1 Announce Type: new 
Abstract: Error-tolerant applications, such as multimedia processing, machine learning, signal processing, and scientific computing, can produce satisfactory outputs even when approximate computations are performed. Approximate computing (AxC) is nowadays a well-established design and computing paradigm that produces more efficient computation systems by judiciously reducing their computation quality. AxC has been applied to arithmetic circuits, modifying their logic behavior. Depending on the approximation process, arithmetic properties, such as commutativity, are not consistently maintained. When such properties are absent, error accumulation and application outputs depend on the order in which data is processed. In this work, we show that controlling the operand order in non-commutative approximate circuits can greatly reduce computational errors. We propose SWAPPER, a lightweight approach that drastically reduces the approximation error by dynamically changing the order of the input operands using only a single bit for the decision. To explore and identify the most suitable bit for the swapping choice, we propose a framework that can be applied at different granularities, leading to large error reductions and significant accuracy improvements. Experimental results at both component and application levels show error reductions of up to 50% for Mean Absolute Error at the component level and more than 90% at the application level on the AxBench application suite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02608v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marcello Traiola, Nazar Misyats, Silviu-Ioan Filip, Remi Garcia, Angeliki Kritikakou</dc:creator>
    </item>
    <item>
      <title>CELLO: Co-designing Schedule and Hybrid Implicit/Explicit Buffer for Complex Tensor Reuse</title>
      <link>https://arxiv.org/abs/2303.11499</link>
      <description>arXiv:2303.11499v2 Announce Type: replace-cross 
Abstract: Tensor algebra accelerators have been gaining popularity for running high-performance computing (HPC) workloads. Identifying optimal schedules for individual tensor operations and designing hardware to run these schedules is an active area of research. Unfortunately, operators in HPC workloads such as Conjugate Gradient often have operators with skewed shapes, fundamentally limiting the reuse any schedule can leverage. Moreover, the operators form a complex DAG of dependencies, making it challenging to apply simple fusion/pipelining techniques to extract inter-operation reuse. To address these challenges, this work proposes an accelerator CELLO. CELLO uses a novel on-chip buffer mechanism called CHORD co-designed with a novel scheduler called SCORE, which together enables identifying and exploiting reuse over complex DAGs of tensor operations. CELLO provides 4x geomean speedup and 4x energy efficiency over state-of-the-art accelerators across HPC workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.11499v2</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raveesh Garg, Michael Pellauer, Sivasankaran Rajamanickam, Tushar Krishna</dc:creator>
    </item>
    <item>
      <title>HaLoRA: Hardware-aware Low-Rank Adaptation for Large Language Models Based on Hybrid Compute-in-Memory Architecture</title>
      <link>https://arxiv.org/abs/2502.19747</link>
      <description>arXiv:2502.19747v2 Announce Type: replace-cross 
Abstract: Low-rank adaptation (LoRA) is a predominant parameter-efficient finetuning method to adapt large language models (LLMs) for downstream tasks. In this paper, we first propose to deploy the LoRA-finetuned LLMs on the hybrid compute-in-memory (CIM) architecture (i.e., pretrained weights onto RRAM and LoRA onto SRAM). To address performance degradation from RRAM's inherent noise, we design a novel Hardware-aware Low-rank Adaption (HaLoRA) method, aiming to train a LoRA branch that is both robust and accurate by aligning the training objectives under both ideal and noisy conditions. Experiments finetuning LLaMA 3.2 1B and 3B demonstrate HaLoRA's effectiveness across multiple reasoning tasks, achieving up to 22.7 improvement in average score while maintaining robustness at various noise levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19747v2</guid>
      <category>cs.CL</category>
      <category>cs.AR</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Taiqiang Wu, Chenchen Ding, Wenyong Zhou, Yuxin Cheng, Xincheng Feng, Shuqi Wang, Chufan Shi, Zhengwu Liu, Ngai Wong</dc:creator>
    </item>
  </channel>
</rss>
