<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 May 2025 01:39:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Semiparametric semi-supervised learning for general targets under distribution shift and decaying overlap</title>
      <link>https://arxiv.org/abs/2505.06452</link>
      <description>arXiv:2505.06452v1 Announce Type: new 
Abstract: In modern scientific applications, large volumes of covariate data are readily available, while outcome labels are costly, sparse, and often subject to distribution shift. This asymmetry has spurred interest in semi-supervised (SS) learning, but most existing approaches rely on strong assumptions -- such as missing completely at random (MCAR) labeling or strict positivity -- that put substantial limitations on their practical usefulness. In this work, we introduce a general semiparametric framework for estimation and inference in SS settings where labels are missing at random (MAR) and the overlap may vanish as sample size increases. Our framework accommodates a wide range of smooth statistical targets -- including means, linear coefficients, quantiles, and causal effects -- and remains valid under high-dimensional nuisance estimation and distributional shift between labeled and unlabeled samples. We construct estimators that are doubly robust and asymptotically normal by deriving influence functions under this decaying MAR-SS regime. A key insight is that classical root-$n$ convergence fails under vanishing overlap; we instead provide corrected asymptotic rates that capture the impact of the decay in overlap. We validate our theory through simulations and demonstrate practical utility in real-world applications on the internet of things and breast cancer where labeled data are scarce.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06452v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Testa, Qi Xu, Jing Lei, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>GMM with Many Weak Moment Conditions and Nuisance Parameters: General Theory and Applications to Causal Inference</title>
      <link>https://arxiv.org/abs/2505.07295</link>
      <description>arXiv:2505.07295v1 Announce Type: new 
Abstract: Weak identification is a common issue for many statistical problems -- for example, when instrumental variables are weakly correlated with treatment, or when proxy variables are weakly correlated with unmeasured confounders. Under weak identification, standard estimation methods, such as the generalized method of moments (GMM), can have sizeable bias in finite samples or even asymptotically. In addition, many practical settings involve a growing number of nuisance parameters, adding further complexity to the problem. In this paper, we study estimation and inference under a general nonlinear moment model with many weak moment conditions and many nuisance parameters. To obtain debiased inference for finite-dimensional target parameters, we demonstrate that Neyman orthogonality plays a stronger role than in conventional settings with strong identification. We study a general two-step debiasing estimator that allows for possibly nonparametric first-step estimation of nuisance parameters, and we establish its consistency and asymptotic normality under a many weak moment asymptotic regime. Our theory accommodates both high-dimensional moment conditions and infinite-dimensional nuisance parameters. We provide high-level assumptions for a general setting and discuss specific applications to the problems of estimation and inference with weak instruments and weak proxies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07295v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Wang, Kwun Chuen Gary Chan, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Some insights into depth estimators for location and scatter in the multivariate setting</title>
      <link>https://arxiv.org/abs/2505.07383</link>
      <description>arXiv:2505.07383v1 Announce Type: new 
Abstract: The concept of statistical depth has received considerable attention as a way to extend the notions of the median and quantiles to other statistical models. These procedures aim to formalize the idea of identifying deeply embedded fits to a model that are less influenced by contamination. Since contamination introduces bias in estimators, it is well known in the location model that the median minimizes the worst-case performance, in terms of maximum bias, among all equivariant estimators. In the multivariate case, Tukey's median was a groundbreaking concept for location estimation, and its counterpart for scatter matrices has recently attracted considerable interest. The breakdown point and the maximum asymptotic bias are key concepts used to summarize an estimator's behavior under contamination. For the location and scale model, we consider two closely related depth formulations, whose deepest estimators display significantly different behavior in terms of breakdown point. In the multivariate setting, we analyze recently introduced concentration inequalities that provide a unified framework for studying both the statistical convergence rate and robustness of Tukey's median and depth-based scatter matrices. We observe that slight variations in these inequalities allow us to visualize the maximum bias behavior of the deepest estimators. Since the maximum bias for depth-based scatter matrices had not previously been derived, we explicitly calculate both the breakdown point and the maximum bias curve for the deepest scatter matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07383v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge G. Adrover, Marcelo Ruiz</dc:creator>
    </item>
    <item>
      <title>Constructing Bayes Minimax Estimators through Integral Transformations</title>
      <link>https://arxiv.org/abs/2505.07649</link>
      <description>arXiv:2505.07649v1 Announce Type: new 
Abstract: The problem of Bayes minimax estimation for the mean of a multivariate normal distribution under quadratic loss has attracted significant attention recently. These estimators have the advantageous property of being admissible, similar to Bayes procedures, while also providing the conservative risk guarantees typical of frequentist methods. This paper demonstrates that Bayes minimax estimators can be derived using integral transformation techniques, specifically through the \( I \)-transform and the Laplace transform, as long as appropriate spherical priors are selected. Several illustrative examples are included to highlight the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07649v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominique Fourdrinier, William E. Strawderman, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>Mixing and Merging Metric Spaces using Directed Graphs</title>
      <link>https://arxiv.org/abs/2505.06405</link>
      <description>arXiv:2505.06405v1 Announce Type: cross 
Abstract: Let $(X_1,d_1),\dots, (X_N,d_N)$ be metric spaces with respective distance functions $d_i: X_i \times X_i \rightarrow [0,1]$, $i=1,\dots,N$. Let $\mathcal{X}$ denote the set theoretic product $X_1\times \cdots \times X_N$ and let $\mathbf{g} \in \mathcal{X}$ and $\mathbf{h} \in \mathcal{X}$ denote two elements in this product space. Let $\mathcal{G} = \left(\mathcal{V},\mathcal{E}\right)$ be a directed graph with vertices $\mathcal{V} =\{1,\dots, N\}$ and with a positive weight $\mathcal{P} = \{p_{ij}\}, p_{ij}\in (0, 1], i,j = 1,..,N$ associated with each edge $(i,j) \in \mathcal{E}$ of $\mathcal{G}$. We define the function \begin{align*} d_{\mathcal{X},\mathcal{G},\mathcal{P}}(\mathbf{g},\mathbf{h}) := \left(1 - \frac{1}{N}\sum_{j=1}^N \prod_{i=1}^N \left[1- d_i(g_i,h_i)\right]^{\frac{1}{p_{ji}}} \right). \end{align*} In this paper we show that $d_{\mathcal{X},\mathcal{G},\mathcal{P}}$ defines a metric space over $\mathcal{X}$ and we investigate the properties of this distance under graph operations, which includes disjoint unions and cartesian products. We show two limiting cases: (a) where $d_{\mathcal{X},\mathcal{G},\mathcal{P}}$ defined over a finite field leads to a broad generalization of graph-based distances that is widely studied in the theory of error-correcting codes; and (b) where $d_{\mathcal{X},\mathcal{G},\mathcal{P}}$ is extended to measuring distances over graphons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06405v1</guid>
      <category>math.CO</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mahir Bilen Can, Shantanu Chakrabartty</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Importance-Weighted Information Criteria: Theory and Optimality</title>
      <link>https://arxiv.org/abs/2505.06531</link>
      <description>arXiv:2505.06531v1 Announce Type: cross 
Abstract: Imori and Ing (2025) proposed the importance-weighted orthogonal greedy algorithm (IWOGA) for model selection in high-dimensional misspecified regression models under covariate shift. To determine the number of IWOGA iterations, they introduced the high-dimensional importance-weighted information criterion (HDIWIC). They argued that the combined use of IWOGA and HDIWIC, IWOGA + HDIWIC, achieves an optimal trade-off between variance and squared bias, leading to optimal convergence rates in terms of conditional mean squared prediction error. In this article, we provide a theoretical justification for this claim by establishing the optimality of IWOGA + HDIWIC under a set of reasonable assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06531v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yong-Syun Cao, Shinpei Imori, Ching-Kang Ing</dc:creator>
    </item>
    <item>
      <title>Multi-Terminal Remote Generation and Estimation Over a Broadcast Channel With Correlated Priors</title>
      <link>https://arxiv.org/abs/2505.07016</link>
      <description>arXiv:2505.07016v1 Announce Type: cross 
Abstract: We study the multi-terminal remote estimation problem under a rate constraint, in which the goal of the encoder is to help each decoder estimate a function over a certain distribution -- while the distribution is known only to the encoder, the function to be estimated is known only to the decoders, and can also be different for each decoder. The decoders can observe correlated samples from prior distributions, instantiated through shared randomness with the encoder. To achieve this, we employ remote generation, where the encoder helps decoders generate samples from the underlying distribution by using the samples from the prior through importance sampling. While methods such as minimal random coding can be used to efficiently transmit samples to each decoder individually using their importance scores, it is unknown if the correlation among the samples from the priors can reduce the communication cost using the availability of a broadcast link. We propose a hierarchical importance sampling strategy that facilitates, in the case of non-zero G\'acs-K\"orner common information among the priors of the decoders, a common sampling step leveraging the availability of a broadcast channel. This is followed by a refinement step for the individual decoders. We present upper bounds on the bias and the estimation error for unicast transmission, which is of independent interest. We then introduce a method that splits into two phases, dedicated to broadcast and unicast transmission, respectively, and show the reduction in communication cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07016v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Egger, Rawad Bitar, Antonia Wachter-Zeh, Nir Weinberger, Deniz G\"und\"uz</dc:creator>
    </item>
    <item>
      <title>Learning from Samples: Inverse Problems over measures via Sharpened Fenchel-Young Losses</title>
      <link>https://arxiv.org/abs/2505.07124</link>
      <description>arXiv:2505.07124v1 Announce Type: cross 
Abstract: Estimating parameters from samples of an optimal probability distribution is essential in applications ranging from socio-economic modeling to biological system analysis. In these settings, the probability distribution arises as the solution to an optimization problem that captures either static interactions among agents or the dynamic evolution of a system over time. Our approach relies on minimizing a new class of loss functions, called sharpened Fenchel-Young losses, which measure the sub-optimality gap of the optimization problem over the space of measures. We study the stability of this estimation method when only a finite number of sample is available. The parameters to be estimated typically correspond to a cost function in static problems and to a potential function in dynamic problems. To analyze stability, we introduce a general methodology that leverages the strong convexity of the loss function together with the sample complexity of the forward optimization problem. Our analysis emphasizes two specific settings in the context of optimal transport, where our method provides explicit stability guarantees: The first is inverse unbalanced optimal transport (iUOT) with entropic regularization, where the parameters to estimate are cost functions that govern transport computations; this method has applications such as link prediction in machine learning. The second is inverse gradient flow (iJKO), where the objective is to recover a potential function that drives the evolution of a probability distribution via the Jordan-Kinderlehrer-Otto (JKO) time-discretization scheme; this is particularly relevant for understanding cell population dynamics in single-cell genomics. Finally, we validate our approach through numerical experiments on Gaussian distributions, where closed-form solutions are available, to demonstrate the practical performance of our methods</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07124v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francisco Andrade, Gabriel Peyr\'e, Clarice Poon</dc:creator>
    </item>
    <item>
      <title>Generalization Bounds and Stopping Rules for Learning with Self-Selected Data</title>
      <link>https://arxiv.org/abs/2505.07367</link>
      <description>arXiv:2505.07367v1 Announce Type: cross 
Abstract: Many learning paradigms self-select training data in light of previously learned parameters. Examples include active learning, semi-supervised learning, bandits, or boosting. Rodemann et al. (2024) unify them under the framework of "reciprocal learning". In this article, we address the question of how well these methods can generalize from their self-selected samples. In particular, we prove universal generalization bounds for reciprocal learning using covering numbers and Wasserstein ambiguity sets. Our results require no assumptions on the distribution of self-selected data, only verifiable conditions on the algorithms. We prove results for both convergent and finite iteration solutions. The latter are anytime valid, thereby giving rise to stopping rules for a practitioner seeking to guarantee the out-of-sample performance of their reciprocal learning algorithm. Finally, we illustrate our bounds and stopping rules for reciprocal learning's special case of semi-supervised learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07367v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Rodemann, James Bailie</dc:creator>
    </item>
    <item>
      <title>Nonparametric Instrumental Variable Inference with Many Weak Instruments</title>
      <link>https://arxiv.org/abs/2505.07729</link>
      <description>arXiv:2505.07729v1 Announce Type: cross 
Abstract: We study inference on linear functionals in the nonparametric instrumental variable (NPIV) problem with a discretely-valued instrument under a many-weak-instruments asymptotic regime, where the number of instrument values grows with the sample size. A key motivating example is estimating long-term causal effects in a new experiment with only short-term outcomes, using past experiments to instrument for the effect of short- on long-term outcomes. Here, the assignment to a past experiment serves as the instrument: we have many past experiments but only a limited number of units in each. Since the structural function is nonparametric but constrained by only finitely many moment restrictions, point identification typically fails. To address this, we consider linear functionals of the minimum-norm solution to the moment restrictions, which is always well-defined. As the number of instrument levels grows, these functionals define an approximating sequence to a target functional, replacing point identification with a weaker asymptotic notion suited to discrete instruments. Extending the Jackknife Instrumental Variable Estimator (JIVE) beyond the classical parametric setting, we propose npJIVE, a nonparametric estimator for solutions to linear inverse problems with many weak instruments. We construct automatic debiased machine learning estimators for linear functionals of both the structural function and its minimum-norm projection, and establish their efficiency in the many-weak-instruments regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07729v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Nathan Kallus, Aur\'elien Bibaut</dc:creator>
    </item>
    <item>
      <title>On a divergence-based prior analysis of stick-breaking processes</title>
      <link>https://arxiv.org/abs/2308.11868</link>
      <description>arXiv:2308.11868v3 Announce Type: replace 
Abstract: The nonparametric view of Bayesian inference has transformed statistics and many of its applications. The canonical Dirichlet process and other more general families of nonparametric priors have served as a gateway to solve frontier uncertainty quantification problems of large, or infinite, nature. This success has been greatly due to available constructions and representations of such distributions, the two most useful constructions are the one based on normalization of homogeneous completely random measures and that based on stick-breaking processes. Hence, understanding their distributional features and how different random probability measures compare among themselves is a key ingredient for their proper application. In this paper, we analyse the discrepancy among some nonparametric priors employed in the literature. Initially, we compute the mean and variance of the random Kullback-Leibler divergence between the Dirichlet process and the geometric process. Subsequently, we extend our analysis to encompass a broader class of exchangeable stick-breaking processes, which includes the Dirichlet and geometric processes as extreme cases. Our results establish quantitative conditions where all the aforementioned priors are close in total variation distance. In such instances, adhering to Occam's razor principle advocates for the preference of the simpler process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11868v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e A. Perusqu\'ia, Mario Diaz, Rams\'es H. Mena</dc:creator>
    </item>
    <item>
      <title>Asymptotic limits of spiked eigenvalues and eigenvectors of signal-plus-noise matrices with weak signals and heteroskedastic noise</title>
      <link>https://arxiv.org/abs/2310.13939</link>
      <description>arXiv:2310.13939v2 Announce Type: replace 
Abstract: This paper is to study a signal-plus-noise model in high dimensional settings when the dimension and the sample size are comparable. Specifically, we assume that the noise has a general covariance matrix that allows for heteroskedasticity, and that the deterministic signal has the same magnitude as the noise and can have a rank that tends to infinity. We develop the asymptotic limits of the left and right spiked singular vectors of the signal-plusnoise data matrix and the limits of the spiked eigenvalues of the corresponding Gram matrix. As an application, we propose a new criterion to estimate the number of clusters in clustering problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13939v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaoyu Liu, Yiming Liu, Guangming Pan, Lingyue Zhang, Zhixiang Zhang</dc:creator>
    </item>
    <item>
      <title>High Order Expansion Method for Kuiper's $V_n$ Statistic in Goodness-of-fit Test</title>
      <link>https://arxiv.org/abs/2310.19576</link>
      <description>arXiv:2310.19576v2 Announce Type: replace 
Abstract: Kuiper's $V_n$ statistic, a measure for comparing the difference of ideal distribution and empirical distribution, is of great significance in the goodness-of-fit test. However, Kuiper's formulae for computing the cumulative distribution function, false positive probability and the upper tail quantile of $V_n$ can not be applied to the case of small sample capacity $n$ since the approximation error is $\mathcal{O}(n^{-1})$. In this work, our contributions lie in three perspectives: firstly the approximation error is reduced to $\mathcal{O}(n^{-(k+1)/2})$ where $k$ is the expansion order with the \textit{high order expansion} for the exponent of differential operator; secondly, a novel high order formula with approximation error $\mathcal{O}(n^{-3})$ is obtained by massive calculations; thirdly, the fixed-point algorithms are designed for solving the Kuiper pair of critical values and upper tail quantiles based on the novel formula. The high order expansion method for Kuiper's $V_n$-statistic is applicable for various applications where there are more than five samples of data. The principles, algorithms and code for the high order expansion method are attractive for the goodness-of-fit test.\\ \textbf{Keywords}: Goodness-of-fit Methods, Kuiper's statistic, Quantile estimation, Algorithm design, High order expansion (HOE)</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19576v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11222-025-10623-9</arxiv:DOI>
      <arxiv:journal_reference>Statistics and Computing, 2025, volume 35, article number 91</arxiv:journal_reference>
      <dc:creator>Hong-Yan Zhang, Zhi-Qiang Feng, Haoting Liu, Rui-Jia Lin, Yu Zhou</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for the Rough Homogenization Limit of Multiscale Fractional Ornstein-Uhlenbeck Processes</title>
      <link>https://arxiv.org/abs/2407.09703</link>
      <description>arXiv:2407.09703v3 Announce Type: replace 
Abstract: We study the problem of parameter estimation for the homogenization limit of multiscale systems involving fractional dynamics. In the case of stochastic multiscale systems driven by Brownian motion, it has been shown that in order for the Maximum Likelihood Estimators of the parameters of the limiting dynamics to be consistent, data needs to be subsampled at an appropriate rate. We extend these results to a class of fractional multiscale systems, often described as scaled fractional kinetic Brownian motions. We provide convergence results for the MLE of the diffusion coefficient of the limiting dynamics, computed using multiscale data. This requires the development of a different methodology to that used in the standard Brownian motion case, which is based on controlling the spectral norm of the inverse covariance matrix of a discretized fractional Gaussian noise on an interval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09703v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/25-ECP677</arxiv:DOI>
      <arxiv:journal_reference>Electronic Communications in Probability, 30, 1-13, (2025)</arxiv:journal_reference>
      <dc:creator>Pablo Ramses Alonso-Martin, Horatio Boedihardjo, Anastasia Papavasiliou</dc:creator>
    </item>
    <item>
      <title>Freedom in constructing quasi-copulas vs. copulas</title>
      <link>https://arxiv.org/abs/2407.15393</link>
      <description>arXiv:2407.15393v4 Announce Type: replace 
Abstract: The main goal of this paper is to study the extent of freedom one has in constructing quasi-copulas vs. copulas. Specifically, it exhibits three construction methods for quasi-copulas based on recent developments: a representation of multivariate quasi-copulas by means of infima and suprema of copulas, an extension of a classical result on shuffles of min to the setting of quasi-copulas, and a construction method for quasi-copulas obeying a given signed mass pattern on a patch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15393v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matja\v{z} Omladi\v{c}, Nik Stopar</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic Estimates for Markov Transition Matrices with Rigorous Error Bounds</title>
      <link>https://arxiv.org/abs/2408.05963</link>
      <description>arXiv:2408.05963v2 Announce Type: replace 
Abstract: We establish non-asymptotic error bounds for the classical Maximal Likelihood Estimation of the transition matrix of a given Markov chain. Meanwhile, in the reversible case, we propose a new reversibility-preserving online Symmetric Counting Estimation of the transition matrix with non-asymptotic deviation bounds. Our analysis is based on a convergence study of certain Markov chains on the length-2 path spaces induced by the original Markov chain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05963v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>De Huang, Xiangyuan Li</dc:creator>
    </item>
    <item>
      <title>Local linear smoothing for regression surfaces on the simplex using Dirichlet kernels</title>
      <link>https://arxiv.org/abs/2408.07209</link>
      <description>arXiv:2408.07209v4 Announce Type: replace 
Abstract: This paper introduces a local linear smoother for regression surfaces on the simplex. The estimator solves a least-squares regression problem weighted by a locally adaptive Dirichlet kernel, ensuring good boundary properties. Asymptotic results for the bias, variance, mean squared error, and mean integrated squared error are derived, generalizing the univariate results of Chen [Ann. Inst. Statist. Math., 54(2) (2002), pp. 312-323]. A simulation study shows that the proposed local linear estimator with Dirichlet kernel outperforms its only direct competitor in the literature, the Nadaraya-Watson estimator with Dirichlet kernel due to Bouzebda, Nezzal and Elhattab [AIMS Math., 9(9) (2024), pp. 26195-26282].</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07209v4</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Staleness Factors and Volatility Estimation at High Frequencies</title>
      <link>https://arxiv.org/abs/2410.07607</link>
      <description>arXiv:2410.07607v2 Announce Type: replace 
Abstract: In this paper, we propose a price staleness factor model that accounts for pervasive market friction across assets and incorporates relevant covariates. Using large-panel high-frequency data, we derive the maximum likelihood estimators of the regression coefficients, the nonstationary factors, and their loading parameters. These estimators recover the time-varying price staleness probabilities. We develop asymptotic theory in which both the dimension $d$ and the sampling frequency $n$ tend to infinity. Using a local principal component analysis (LPCA) approach, we find that the efficient price co-volatilities (systematic and idiosyncratic) are biased downward due to the presence of staleness. We provide bias-corrected estimators for both the spot and integrated systematic and idiosyncratic co-volatilities, and prove that these estimators are robust to data staleness. Interestingly, besides their dependence on the dimensionality $d$, the integrated plug-in estimates converge at a rate of $n^{-1/2}$ without requiring correcting term, whereas the local PCA estimates converge at a slower rate of $n^{-1/4}$. This validates the aggregation efficiency achieved through nonlinear, nonstationary factor analysis via maximum likelihood estimation. Numerical experiments justify our theoretical findings. Empirically, we demonstrate that the staleness factor provides unique explanatory power for cross-sectional risk premia, and that the staleness correction reduces out-of-sample portfolio risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07607v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin-Bing Kong, Bin Wu, Wuyi Ye</dc:creator>
    </item>
    <item>
      <title>Extreme values of the mass distribution associated with $d$-quasi-copulas via linear programming</title>
      <link>https://arxiv.org/abs/2410.19339</link>
      <description>arXiv:2410.19339v2 Announce Type: replace 
Abstract: The recent survey published in Fuzzy Sets and Systems nicknamed ``Hitchhiker's Guide'' has raised the rating of quasi-copula problems in the dependence modeling community in spite of the lack of statistical interpretation of quasi-copulas. Some of the open problems listed there were solved, and some conjectured one way or the other. This paper concentrates on the Open Problem 5 of this list concerning bounds on the volume of a $d$--variate quasi-copula. We disprove a recent conjecture published in the same journal on the lower bound of this volume. We also give evidence that the problem is much more difficult than suspected and provide hints about its final solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19339v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matej Bel\v{s}ak, Matja\v{z} Omladi\v{c}, Martin Vuk, Alja\v{z} Zalar</dc:creator>
    </item>
    <item>
      <title>Diffusion Approximations for Thompson Sampling</title>
      <link>https://arxiv.org/abs/2105.09232</link>
      <description>arXiv:2105.09232v4 Announce Type: replace-cross 
Abstract: We study the behavior of Thompson sampling from the perspective of weak convergence. In the regime with small $\gamma &gt; 0$, where the gaps between arm means scale as $\sqrt{\gamma}$ and over time horizons that scale as $1/\gamma$, we show that the dynamics of Thompson sampling evolve according to discrete versions of SDE's and stochastic ODE's. As $\gamma \downarrow 0$, we show that the dynamics converge weakly to solutions of the corresponding SDE's and stochastic ODE's. Our weak convergence theory is developed from first principles using the Continuous Mapping Theorem, and can be easily adapted to analyze other sampling-based bandit algorithms. In this regime, we also show that the weak limits of the dynamics of many sampling-based algorithms -- including Thompson sampling designed for single-parameter exponential family rewards, and algorithms using bootstrap-based sampling to balance exploration and exploitation -- coincide with those of Gaussian Thompson sampling. Moreover, in this regime, these algorithms are generally robust to model mis-specification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.09232v4</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Fan, Peter W. Glynn</dc:creator>
    </item>
    <item>
      <title>A Framework for Statistical Inference via Randomized Algorithms</title>
      <link>https://arxiv.org/abs/2307.11255</link>
      <description>arXiv:2307.11255v5 Announce Type: replace-cross 
Abstract: Randomized algorithms, such as randomized sketching or stochastic optimization, are a promising approach to ease the computational burden in analyzing large datasets. However, randomized algorithms also produce non-deterministic outputs, leading to the problem of evaluating their accuracy. In this paper, we develop a statistical inference framework for quantifying the uncertainty of the outputs of randomized algorithms.
  Our key conclusion is that one can perform statistical inference for the target of a sequence of randomized algorithms as long as in the limit, their outputs fluctuate around the target according to any (possibly unknown) probability distribution. In this setting, we develop appropriate statistical inference methods -- sub-randomization, multi-run plug-in and multi-run aggregation -- by estimating the unknown parameters of the limiting distribution either using multiple runs of the randomized algorithm, or by tailored estimates.
  As illustrations, we develop methods for statistical inference when using stochastic optimization (such as Polyak-Ruppert averaging in stochastic gradient descent and stochastic optimization with momentum). We also illustrate our methods in inference for least squares parameters via randomized sketching, by characterizing the limiting distributions of sketching estimates in a possibly growing dimensional case. We further characterize the computation and communication cost of our methods, showing that in certain cases, they add negligible overhead. The results are supported via a broad range of simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11255v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhixiang Zhang, Sokbae Lee, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Privacy of SGD under Gaussian or Heavy-Tailed Noise: Guarantees without Gradient Clipping</title>
      <link>https://arxiv.org/abs/2403.02051</link>
      <description>arXiv:2403.02051v2 Announce Type: replace-cross 
Abstract: The injection of heavy-tailed noise into the iterates of stochastic gradient descent (SGD) has garnered growing interest in recent years due to its theoretical and empirical benefits for optimization and generalization. However, its implications for privacy preservation remain largely unexplored. Aiming to bridge this gap, we provide differential privacy (DP) guarantees for noisy SGD, when the injected noise follows an $\alpha$-stable distribution, which includes a spectrum of heavy-tailed distributions (with infinite variance) as well as the light-tailed Gaussian distribution. Considering the $(\epsilon, \delta)$-DP framework, we show that SGD with heavy-tailed perturbations achieves $(0, O(1/n))$-DP for a broad class of loss functions which can be non-convex, where $n$ is the number of data points. As a remarkable byproduct, contrary to prior work that necessitates bounded sensitivity for the gradients or clipping the iterates, our theory can handle unbounded gradients without clipping, and reveals that under mild assumptions, such a projection step is not actually necessary. Our results suggest that, given other benefits of heavy-tails in optimization, heavy-tailed noising schemes can be a viable alternative to their light-tailed counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02051v2</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Umut \c{S}im\c{s}ekli, Mert G\"urb\"uzbalaban, Sinan Y{\i}ld{\i}r{\i}m, Lingjiong Zhu</dc:creator>
    </item>
    <item>
      <title>Transformers Handle Endogeneity in In-Context Linear Regression</title>
      <link>https://arxiv.org/abs/2410.01265</link>
      <description>arXiv:2410.01265v3 Announce Type: replace-cross 
Abstract: We explore the capability of transformers to address endogeneity in in-context linear regression. Our main finding is that transformers inherently possess a mechanism to handle endogeneity effectively using instrumental variables (IV). First, we demonstrate that the transformer architecture can emulate a gradient-based bi-level optimization procedure that converges to the widely used two-stage least squares $(\textsf{2SLS})$ solution at an exponential rate. Next, we propose an in-context pretraining scheme and provide theoretical guarantees showing that the global minimizer of the pre-training loss achieves a small excess loss. Our extensive experiments validate these theoretical findings, showing that the trained transformer provides more robust and reliable in-context predictions and coefficient estimates than the $\textsf{2SLS}$ method, in the presence of endogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01265v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haodong Liang, Krishnakumar Balasubramanian, Lifeng Lai</dc:creator>
    </item>
    <item>
      <title>Model non-collapse: Minimax bounds for recursive discrete distribution estimation</title>
      <link>https://arxiv.org/abs/2501.19273</link>
      <description>arXiv:2501.19273v3 Announce Type: replace-cross 
Abstract: Learning discrete distributions from i.i.d. samples is a well-understood problem. However, advances in generative machine learning prompt an interesting new, non-i.i.d. setting: after receiving a certain number of samples, an estimated distribution is fixed, and samples from this estimate are drawn and introduced into the sample corpus, undifferentiated from real samples. Subsequent generations of estimators now face contaminated environments, a scenario referred to in the machine learning literature as self-consumption. Empirically, it has been observed that models in fully synthetic self-consuming loops collapse -- their performance deteriorates with each batch of training -- but accumulating data has been shown to prevent complete degeneration. This, in turn, begs the question: What happens when fresh real samples \textit{are} added at every stage? In this paper, we study the minimax loss of self-consuming discrete distribution estimation in such loops. We show that even when model collapse is consciously averted, the ratios between the minimax losses with and without source information can grow unbounded as the batch size increases. In the data accumulation setting, where all batches of samples are available for estimation, we provide minimax lower bounds and upper bounds that are order-optimal under mild conditions for the expected $\ell_2^2$ and $\ell_1$ losses at every stage. We provide conditions for regimes where there is a strict gap in the convergence rates compared to the corresponding oracle-assisted minimax loss where real and synthetic samples are differentiated, and provide examples where this gap is easily observed. We also provide a lower bound on the minimax loss in the data replacement setting, where only the latest batch of samples is available, and use it to find a lower bound for the worst-case loss for bounded estimate trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19273v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Millen Kanabar, Michael Gastpar</dc:creator>
    </item>
    <item>
      <title>Learning to Fuse Temporal Proximity Networks: A Case Study in Chimpanzee Social Interactions</title>
      <link>https://arxiv.org/abs/2502.00302</link>
      <description>arXiv:2502.00302v2 Announce Type: replace-cross 
Abstract: How can we identify groups of primate individuals which could be conjectured to drive social structure? To address this question, one of us has collected a time series of data for social interactions between chimpanzees. Here we use a network representation, leading to the task of combining these data into a time series of a single weighted network per time stamp, where different proximities should be given different weights reflecting their relative importance. We optimize these proximity-type weights in a principled way, using an innovative loss function which rewards structural consistency across time. The approach is empirically validated by carefully designed synthetic data. Using statistical tests, we provide a way of identifying groups of individuals that stay related for a significant length of time. Applying the approach to the chimpanzee data set, we detect cliques in the animal social network time series, which can be validated by real-world intuition from prior research and qualitative observations by chimpanzee experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00302v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixuan He, Aaron Sandel, David Wipf, Mihai Cucuringu, John Mitani, Gesine Reinert</dc:creator>
    </item>
    <item>
      <title>A Computational Theory for Efficient Mini Agent Evaluation with Causal Guarantees</title>
      <link>https://arxiv.org/abs/2503.21138</link>
      <description>arXiv:2503.21138v4 Announce Type: replace-cross 
Abstract: In order to reduce the cost of experimental evaluation for agents, we introduce a computational theory of evaluation for mini agents: build evaluation model to accelerate the evaluation procedures. We prove upper bounds of generalized error and generalized causal effect error of given evaluation models for infinite agents. We also prove efficiency, and consistency to estimated causal effect from deployed agents to evaluation metric by prediction. To learn evaluation models, we propose a meta-learner to handle heterogeneous agents space problem. Comparing with existed evaluation approaches, our (conditional) evaluation model reduced 24.1\% to 99.0\% evaluation errors across 12 scenes, including individual medicine, scientific simulation, social experiment, business activity, and quantum trade. The evaluation time is reduced 3 to 7 order of magnitude per subject comparing with experiments or simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21138v4</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hedong Yan</dc:creator>
    </item>
    <item>
      <title>Constraint-based causal discovery with tiered background knowledge and latent variables in single or overlapping datasets</title>
      <link>https://arxiv.org/abs/2503.21526</link>
      <description>arXiv:2503.21526v2 Announce Type: replace-cross 
Abstract: In this paper we consider the use of tiered background knowledge within constraint based causal discovery. Our focus is on settings relaxing causal sufficiency, i.e. allowing for latent variables which may arise because relevant information could not be measured at all, or not jointly, as in the case of multiple overlapping datasets. We first present novel insights into the properties of the 'tiered FCI' (tFCI) algorithm. Building on this, we introduce a new extension of the IOD (integrating overlapping datasets) algorithm incorporating tiered background knowledge, the 'tiered IOD' (tIOD) algorithm. We show that under full usage of the tiered background knowledge tFCI and tIOD are sound, while simple versions of the tIOD and tFCI are sound and complete. We further show that the tIOD algorithm can often be expected to be considerably more efficient and informative than the IOD algorithm even beyond the obvious restriction of the Markov equivalence classes. We provide a formal result on the conditions for this gain in efficiency and informativeness. Our results are accompanied by a series of examples illustrating the exact role and usefulness of tiered background knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21526v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christine W. Bang, Vanessa Didelez</dc:creator>
    </item>
    <item>
      <title>Design-Based Inference under Random Potential Outcomes via Riesz Representation</title>
      <link>https://arxiv.org/abs/2505.01324</link>
      <description>arXiv:2505.01324v4 Announce Type: replace-cross 
Abstract: We introduce a general framework for design-based causal inference that accommodates random potential outcomes, thereby extending the classical Neyman-Rubin setup in which outcomes are treated as fixed. In our formulation, each unit's potential outcome is modelled as a function \( \tilde{y}_i(z, \omega) \), where \( \omega \) denotes latent randomness external to the treatment assignment. Building on recent work connecting design-based estimation with the Riesz representation theorem, we construct causal estimators by embedding potential outcomes in a Hilbert space and defining treatment effects as linear functionals. This approach yields unbiased and consistent estimators, even when outcomes exhibit random variation. The framework retains the key strength of design-based analysis, identification via a known randomisation scheme, while enabling inference in settings with outcome-level stochasticity. We establish large-sample properties under local dependence and propose plug-in variance estimators, including a correlation-based version that improves efficiency under sparse dependence. A simulation study illustrates the finite-sample behaviour of the estimator. Our results unify design-based reasoning with random outcome modelling, broadening the applicability of causal inference in complex experimental environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01324v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukai Yang</dc:creator>
    </item>
  </channel>
</rss>
