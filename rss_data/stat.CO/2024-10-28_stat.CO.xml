<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Oct 2024 04:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Prediction of microstructural representativity from a single image</title>
      <link>https://arxiv.org/abs/2410.19568</link>
      <description>arXiv:2410.19568v1 Announce Type: new 
Abstract: In this study, we present a method for predicting the representativity of the phase fraction observed in a single image (2D or 3D) of a material. Traditional approaches often require large datasets and extensive statistical analysis to estimate the Integral Range, a key factor in determining the variance of microstructural properties. Our method leverages the Two-Point Correlation function to directly estimate the variance from a single image (2D or 3D), thereby enabling phase fraction prediction with associated confidence levels. We validate our approach using open-source datasets, demonstrating its efficacy across diverse microstructures. This technique significantly reduces the data requirements for representativity analysis, providing a practical tool for material scientists and engineers working with limited microstructural data. To make the method easily accessible, we have created a web-application, \url{www.imagerep.io}, for quick, simple and informative use of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19568v1</guid>
      <category>stat.CO</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Dahari, Ronan Docherty, Steve Kench, Samuel J. Cooper</dc:creator>
    </item>
    <item>
      <title>Enhancing Approximate Modular Bayesian Inference by Emulating the Conditional Posterior</title>
      <link>https://arxiv.org/abs/2410.19028</link>
      <description>arXiv:2410.19028v1 Announce Type: cross 
Abstract: In modular Bayesian analyses, complex models are composed of distinct modules, each representing different aspects of the data or prior information. In this context, fully Bayesian approaches can sometimes lead to undesirable feedback between modules, compromising the integrity of the inference. This paper focuses on the "cut-distribution" which prevents unwanted influence between modules by "cutting" feedback. The multiple imputation (DS) algorithm is standard practice for approximating the cut-distribution, but it can be computationally intensive, especially when the number of imputations required is large. An enhanced method is proposed, the Emulating the Conditional Posterior (ECP) algorithm, which leverages emulation to increase the number of imputations. Through numerical experiment it is demonstrated that the ECP algorithm outperforms the traditional DS approach in terms of accuracy and computational efficiency, particularly when resources are constrained. It is also shown how the DS algorithm can be improved using ideas from design of experiments. This work also provides practical recommendations on algorithm choice based on the computational demands of sampling from the prior and cut-distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19028v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grant Hutchings, Kellin Rumsey, Derek Bingham, Gabriel Huerta</dc:creator>
    </item>
    <item>
      <title>A spectral method for multi-view subspace learning using the product of projections</title>
      <link>https://arxiv.org/abs/2410.19125</link>
      <description>arXiv:2410.19125v1 Announce Type: cross 
Abstract: Multi-view data provides complementary information on the same set of observations, with multi-omics and multimodal sensor data being common examples. Analyzing such data typically requires distinguishing between shared (joint) and unique (individual) signal subspaces from noisy, high-dimensional measurements. Despite many proposed methods, the conditions for reliably identifying joint and individual subspaces remain unclear. We rigorously quantify these conditions, which depend on the ratio of the signal rank to the ambient dimension, principal angles between true subspaces, and noise levels. Our approach characterizes how spectrum perturbations of the product of projection matrices, derived from each view's estimated subspaces, affect subspace separation. Using these insights, we provide an easy-to-use and scalable estimation algorithm. In particular, we employ rotational bootstrap and random matrix theory to partition the observed spectrum into joint, individual, and noise subspaces. Diagnostic plots visualize this partitioning, providing practical and interpretable insights into the estimation performance. In simulations, our method estimates joint and individual subspaces more accurately than existing approaches. Applications to multi-omics data from colorectal cancer patients and nutrigenomic study of mice demonstrate improved performance in downstream predictive tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19125v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renat Sergazinov, Armeen Taeb, Irina Gaynanova</dc:creator>
    </item>
    <item>
      <title>SHAP zero Explains All-order Feature Interactions in Black-box Genomic Models with Near-zero Query Cost</title>
      <link>https://arxiv.org/abs/2410.19236</link>
      <description>arXiv:2410.19236v1 Announce Type: cross 
Abstract: With the rapid growth of black-box models in machine learning, Shapley values have emerged as a popular method for model explanations due to their theoretical guarantees. Shapley values locally explain a model to an input query using additive features. Yet, in genomics, extracting biological knowledge from black-box models hinges on explaining nonlinear feature interactions globally to hundreds to thousands of input query sequences. Herein, we develop SHAP zero, an algorithm that estimates all-order Shapley feature interactions with a near-zero cost per queried sequence after paying a one-time fee for model sketching. SHAP zero achieves this by establishing a surprisingly underexplored connection between the Shapley interactions and the Fourier transform of the model. Explaining two genomic models, one trained to predict guide RNA binding and the other to predict DNA repair outcomes, we demonstrate that SHAP zero achieves orders of magnitude reduction in amortized computational cost compared to state-of-the-art algorithms. SHAP zero reveals all microhomologous motifs that are predictive of DNA repair outcome, a finding previously inaccessible due to the combinatorial space of possible high-order feature interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19236v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>q-bio.GN</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Darin Tsui, Aryan Musharaf, Amirali Aghazadeh</dc:creator>
    </item>
    <item>
      <title>Insights into regression-based cross-temporal forecast reconciliation</title>
      <link>https://arxiv.org/abs/2410.19407</link>
      <description>arXiv:2410.19407v1 Announce Type: cross 
Abstract: Cross-temporal forecast reconciliation aims to ensure consistency across forecasts made at different temporal and cross-sectional levels. We explore the relationships between sequential, iterative, and optimal combination approaches, and discuss the conditions under which a sequential reconciliation approach (either first-cross-sectional-then-temporal, or first-temporal-then-cross-sectional) is equivalent to a fully (i.e., cross-temporally) coherent iterative heuristic. Furthermore, we show that for specific patterns of the error covariance matrix in the regression model on which the optimal combination approach grounds, iterative reconciliation naturally converges to the optimal combination solution, regardless the order of application of the uni-dimensional cross-sectional and temporal reconciliation approaches. Theoretical and empirical properties of the proposed approaches are investigated through a forecasting experiment using a dataset of hourly photovoltaic power generation. The study presents a comprehensive framework for understanding and enhancing cross-temporal forecast reconciliation, considering both forecast accuracy and the often overlooked computational aspects, showing that significant improvement can be achieved in terms of memory space and computation time, two particularly important aspects in the high-dimensional contexts that usually arise in cross-temporal forecast reconciliation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19407v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Girolimetto, Tommaso Di Fonzo</dc:creator>
    </item>
    <item>
      <title>Robust Time Series Causal Discovery for Agent-Based Model Validation</title>
      <link>https://arxiv.org/abs/2410.19412</link>
      <description>arXiv:2410.19412v1 Announce Type: cross 
Abstract: Agent-Based Model (ABM) validation is crucial as it helps ensuring the reliability of simulations, and causal discovery has become a powerful tool in this context. However, current causal discovery methods often face accuracy and robustness challenges when applied to complex and noisy time series data, which is typical in ABM scenarios. This study addresses these issues by proposing a Robust Cross-Validation (RCV) approach to enhance causal structure learning for ABM validation. We develop RCV-VarLiNGAM and RCV-PCMCI, novel extensions of two prominent causal discovery algorithms. These aim to reduce the impact of noise better and give more reliable causal relation results, even with high-dimensional, time-dependent data. The proposed approach is then integrated into an enhanced ABM validation framework, which is designed to handle diverse data and model structures.
  The approach is evaluated using synthetic datasets and a complex simulated fMRI dataset. The results demonstrate greater reliability in causal structure identification. The study examines how various characteristics of datasets affect the performance of established causal discovery methods. These characteristics include linearity, noise distribution, stationarity, and causal structure density. This analysis is then extended to the RCV method to see how it compares in these different situations. This examination helps confirm whether the results are consistent with existing literature and also reveals the strengths and weaknesses of the novel approaches.
  By tackling key methodological challenges, the study aims to enhance ABM validation with a more resilient valuation framework presented. These improvements increase the reliability of model-driven decision making processes in complex systems analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19412v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gene Yu, Ce Guo, Wayne Luk</dc:creator>
    </item>
    <item>
      <title>Learned Reference-based Diffusion Sampling for multi-modal distributions</title>
      <link>https://arxiv.org/abs/2410.19449</link>
      <description>arXiv:2410.19449v1 Announce Type: cross 
Abstract: Over the past few years, several approaches utilizing score-based diffusion have been proposed to sample from probability distributions, that is without having access to exact samples and relying solely on evaluations of unnormalized densities. The resulting samplers approximate the time-reversal of a noising diffusion process, bridging the target distribution to an easy-to-sample base distribution. In practice, the performance of these methods heavily depends on key hyperparameters that require ground truth samples to be accurately tuned. Our work aims to highlight and address this fundamental issue, focusing in particular on multi-modal distributions, which pose significant challenges for existing sampling methods. Building on existing approaches, we introduce Learned Reference-based Diffusion Sampler (LRDS), a methodology specifically designed to leverage prior knowledge on the location of the target modes in order to bypass the obstacle of hyperparameter tuning. LRDS proceeds in two steps by (i) learning a reference diffusion model on samples located in high-density space regions and tailored for multimodality, and (ii) using this reference model to foster the training of a diffusion-based sampler. We experimentally demonstrate that LRDS best exploits prior knowledge on the target distribution compared to competing algorithms on a variety of challenging distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19449v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxence Noble, Louis Grenioux, Marylou Gabri\'e, Alain Oliviero Durmus</dc:creator>
    </item>
    <item>
      <title>trajmsm: An R package for Trajectory Analysis and Causal Modeling</title>
      <link>https://arxiv.org/abs/2410.19682</link>
      <description>arXiv:2410.19682v1 Announce Type: cross 
Abstract: The R package trajmsm provides functions designed to simplify the estimation of the parameters of a model combining latent class growth analysis (LCGA), a trajectory analysis technique, and marginal structural models (MSMs) called LCGA-MSM. LCGA summarizes similar patterns of change over time into a few distinct categories called trajectory groups, which are then included as "treatments" in the MSM. MSMs are a class of causal models that correctly handle treatment-confounder feedback. The parameters of LCGA-MSMs can be consistently estimated using different estimators, such as inverse probability weighting (IPW), g-computation, and pooled longitudinal targeted maximum likelihood estimation (pooled LTMLE). These three estimators of the parameters of LCGA-MSMs are currently implemented in our package. In the context of a time-dependent outcome, we previously proposed a combination of LCGA and history-restricted MSMs (LCGA-HRMSMs). Our package provides additional functions to estimate the parameters of such models. Version 0.1.3 of the package is currently available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19682v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Awa Diop, Caroline Sirois, Jason R. Guertin, Mireille E. Schnitzer, James M. Brophy, Denis Talbot</dc:creator>
    </item>
    <item>
      <title>A Short Note on the Efficiency of Markov Chains for Bayesian Linear Regression Models with Heavy-Tailed Errors</title>
      <link>https://arxiv.org/abs/2410.17070</link>
      <description>arXiv:2410.17070v2 Announce Type: replace-cross 
Abstract: In this short note, we consider posterior simulation for a linear regression model when the error distribution is given by a scale mixture of multivariate normals. We show that the sampler of Backlund and Hobert (2020) for the case of the conditionally conjugate normal-inverse Wishart prior is geometrically ergodic even when the error density is heavier-tailed. Moreover, we prove that their sampler is uniformly ergodic if, in addition, the columns of the design matrix are linearly independent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17070v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Hamura</dc:creator>
    </item>
  </channel>
</rss>
