<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Jun 2025 01:40:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The California Report on Frontier AI Policy</title>
      <link>https://arxiv.org/abs/2506.17303</link>
      <description>arXiv:2506.17303v1 Announce Type: new 
Abstract: The innovations emerging at the frontier of artificial intelligence (AI) are poised to create historic opportunities for humanity but also raise complex policy challenges. Continued progress in frontier AI carries the potential for profound advances in scientific discovery, economic productivity, and broader social well-being. As the epicenter of global AI innovation, California has a unique opportunity to continue supporting developments in frontier AI while addressing substantial risks that could have far reaching consequences for the state and beyond. This report leverages broad evidence, including empirical research, historical analysis, and modeling and simulations, to provide a framework for policymaking on the frontier of AI development. Building on this multidisciplinary approach, this report derives policy principles that can inform how California approaches the use, assessment, and governance of frontier AI: principles rooted in an ethos of trust but verify. This approach takes into account the importance of innovation while establishing appropriate strategies to reduce material risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17303v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishi Bommasani, Scott R. Singer, Ruth E. Appel, Sarah Cen, A. Feder Cooper, Elena Cryst, Lindsey A. Gailmard, Ian Klaus, Meredith M. Lee, Inioluwa Deborah Raji, Anka Reuel, Drew Spence, Alexander Wan, Angelina Wang, Daniel Zhang, Daniel E. Ho, Percy Liang, Dawn Song, Joseph E. Gonzalez, Jonathan Zittrain, Jennifer Tour Chayes, Mariano-Florentino Cuellar, Li Fei-Fei</dc:creator>
    </item>
    <item>
      <title>Can Large Language Models Be Trusted Paper Reviewers? A Feasibility Study</title>
      <link>https://arxiv.org/abs/2506.17311</link>
      <description>arXiv:2506.17311v1 Announce Type: new 
Abstract: Academic paper review typically requires substantial time, expertise, and human resources. Large Language Models (LLMs) present a promising method for automating the review process due to their extensive training data, broad knowledge base, and relatively low usage cost. This work explores the feasibility of using LLMs for academic paper review by proposing an automated review system. The system integrates Retrieval Augmented Generation (RAG), the AutoGen multi-agent system, and Chain-of-Thought prompting to support tasks such as format checking, standardized evaluation, comment generation, and scoring. Experiments conducted on 290 submissions from the WASA 2024 conference using GPT-4o show that LLM-based review significantly reduces review time (average 2.48 hours) and cost (average \$104.28 USD). However, the similarity between LLM-selected papers and actual accepted papers remains low (average 38.6\%), indicating issues such as hallucination, lack of independent judgment, and retrieval preferences. Therefore, it is recommended to use LLMs as assistive tools to support human reviewers, rather than to replace them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17311v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuanlei Li, Xu Hu, Minghui Xu, Kun Li, Yue Zhang, Xiuzhen Cheng</dc:creator>
    </item>
    <item>
      <title>Using Machine Learning in Analyzing Air Quality Discrepancies of Environmental Impact</title>
      <link>https://arxiv.org/abs/2506.17319</link>
      <description>arXiv:2506.17319v1 Announce Type: new 
Abstract: In this study, we apply machine learning and software engineering in analyzing air pollution levels in City of Baltimore. The data model was fed with three primary data sources: 1) a biased method of estimating insurance risk used by homeowners loan corporation, 2) demographics of Baltimore residents, and 3) census data estimate of NO2 and PM2.5 concentrations. The dataset covers 650,643 Baltimore residents in 44.7 million residents in 202 major cities in US. The results show that air pollution levels have a clear association with the biased insurance estimating method. Great disparities present in NO2 level between more desirable and low income blocks. Similar disparities exist in air pollution level between residents' ethnicity. As Baltimore population consists of a greater proportion of people of color, the finding reveals how decades old policies has continued to discriminate and affect quality of life of Baltimore citizens today.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17319v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuangbao Paul Wang, Lucas Yang, Rahouane Chouchane, Jin Guo, Michael Bailey</dc:creator>
    </item>
    <item>
      <title>MAARTA:Multi-Agentic Adaptive Radiology Teaching Assistant</title>
      <link>https://arxiv.org/abs/2506.17320</link>
      <description>arXiv:2506.17320v1 Announce Type: new 
Abstract: Radiology students often struggle to develop perceptual expertise due to limited expert mentorship time, leading to errors in visual search and diagnostic interpretation. These perceptual errors, such as missed fixations, short dwell times, or misinterpretations, are not adequately addressed by current AI systems, which focus on diagnostic accuracy but fail to explain how and why errors occur. To address this gap, we introduce MAARTA (Multi-Agentic Adaptive Radiology Teaching Assistant), a multi-agent framework that analyzes gaze patterns and radiology reports to provide personalized feedback. Unlike single-agent models, MAARTA dynamically selects agents based on error complexity, enabling adaptive and efficient reasoning. By comparing expert and student gaze behavior through structured graphs, the system identifies missed findings and assigns Perceptual Error Teacher agents to analyze discrepancies. MAARTA then uses step-by-step prompting to help students understand their errors and improve diagnostic reasoning, advancing AI-driven radiology education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17320v1</guid>
      <category>cs.CY</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akash Awasthi, Brandon V. Chang, Anh M. Vu, Ngan Le, Rishi Agrawal, Zhigang Deng, Carol Wu, Hien Van Nguyen</dc:creator>
    </item>
    <item>
      <title>AI is the Strategy: From Agentic AI to Autonomous Business Models onto Strategy in the Age of AI</title>
      <link>https://arxiv.org/abs/2506.17339</link>
      <description>arXiv:2506.17339v1 Announce Type: new 
Abstract: This article develops the concept of Autonomous Business Models (ABMs) as a distinct managerial and strategic logic in the age of agentic AI. While most firms still operate within human-driven or AI-augmented models, we argue that we are now entering a phase where agentic AI (systems capable of initiating, coordinating, and adapting actions autonomously) can increasingly execute the core mechanisms of value creation, delivery, and capture. This shift reframes AI not as a tool to support strategy, but as the strategy itself. Using two illustrative cases, getswan.ai, an Israeli startup pursuing autonomy by design, and a hypothetical reconfiguration of Ryanair as an AI-driven incumbent, we depict the evolution from augmented to autonomous business models. We show how ABMs reshape competitive advantage through agentic execution, continuous adaptation, and the gradual offloading of human decision-making. This transition introduces new forms of competition between AI-led firms, which we term synthetic competition, where strategic interactions occur at rapid, machine-level speed and scale. It also challenges foundational assumptions in strategy, organizational design, and governance. By positioning agentic AI as the central actor in business model execution, the article invites us to rethink strategic management in an era where firms increasingly run themselves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17339v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ren\'e Bohnsack, Mickie de Wet</dc:creator>
    </item>
    <item>
      <title>Distinguishing Predictive and Generative AI in Regulation</title>
      <link>https://arxiv.org/abs/2506.17347</link>
      <description>arXiv:2506.17347v1 Announce Type: new 
Abstract: Over the past decade, policymakers have developed a set of regulatory tools to ensure AI development aligns with key societal goals. Many of these tools were initially developed in response to concerns with predictive AI and therefore encode certain assumptions about the nature of AI systems and the utility of certain regulatory approaches. With the advent of generative AI, however, some of these assumptions no longer hold, even as policymakers attempt to maintain a single regulatory target that covers both types of AI.
  In this paper, we identify four distinct aspects of generative AI that call for meaningfully different policy responses. These are the generality and adaptability of generative AI that make it a poor regulatory target, the difficulty of designing effective evaluations, new legal concerns that change the ecosystem of stakeholders and sources of expertise, and the distributed structure of the generative AI value chain.
  In light of these distinctions, policymakers will need to evaluate where the past decade of policy work remains relevant and where new policies, designed to address the unique risks posed by generative AI, are necessary. We outline three recommendations for policymakers to more effectively identify regulatory targets and leverage constraints across the broader ecosystem to govern generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17347v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jennifer Wang, Andrew Selbst, Solon Barocas, Suresh Venkatasubramanian</dc:creator>
    </item>
    <item>
      <title>Evaluating the Impact of Lean and Green Practices on Operational Performance: A Real Data-Driven Simulation Case Study</title>
      <link>https://arxiv.org/abs/2506.17354</link>
      <description>arXiv:2506.17354v1 Announce Type: new 
Abstract: Global market-driven forces and customer needs are continuously changing. In the past, profitability and efficiency were the primary objectives of most companies. However, in recent decades, sustainable performance has emerged as a new competitive advantage. Companies have been compelled to adopt a concept that combines these evolving global interests with traditional goals resulting in the innovation of the lean and green approach.
  In this study, a research methodology that includes system analysis and modeling procedures to apply the lean and green concept, combined with a new evaluation metric, the Overall Environmental Equipment Effectiveness (OEEE) was used to investigate the effects of adopting lean and green practices on overall performance.
  A simulation model and energy value stream mapping were implemented, and the OEEE value was calculated to assess the current performance in terms of quality, availability, productivity, and sustainability. The current state production lead time was 329.1 minutes per batch, and the OEEE value was 13.1%. This result indicates existing issues in performance and sustainability, suggesting that improvement efforts should focus on enhancing these two aspects to increase the overall OEEE value.
  Several improvement scenarios were proposed, including combining and rearranging the inspection workstations as the first scenario, and using UV lighting for drying purposes at the framing workstation as the second. After applying these improvements, both scenarios showed increased OEEE values and reduced lead times compared to the current state. In the first scenario, the lead time decreased to 158.23 minutes, and the OEEE increased to 35%. In the second scenario, the lead time was reduced to 292 minutes, with the OEEE increasing to 24%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17354v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Farah Altarazi</dc:creator>
    </item>
    <item>
      <title>PasteTrace: A Single Source Plagiarism Detection Tool For Introductory Programming Courses</title>
      <link>https://arxiv.org/abs/2506.17355</link>
      <description>arXiv:2506.17355v1 Announce Type: new 
Abstract: Introductory Computer Science classes are important for laying the foundation for advanced programming courses. However, students without prior programming experience may find these courses challenging, leading to difficulties in understanding concepts and engaging in academic dishonesty such as plagiarism. While there exists plagiarism detection techniques and tools, not all of them are suitable for academic settings, especially in introductory programming courses. This paper introduces PasteTrace, a novel open-source plagiarism detection tool designed specifically for introductory programming courses. Unlike traditional methods, PasteTrace operates within an Integrated Development Environment that tracks the student's coding activities in real-time for evidence of plagiarism. Our evaluation of PasteTrace in two introductory programming courses demonstrates the tool's ability to provide insights into student behavior and detect various forms of plagiarism, outperforming an existing well-established tool.
  A video demonstration of PasteTrace and its source code, and case study data are made available at https://doi.org/10.6084/m9.figshare.27115852</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17355v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse McDonald, Scott Robertson, Anthony Peruma</dc:creator>
    </item>
    <item>
      <title>Automatic Large Language Models Creation of Interactive Learning Lessons</title>
      <link>https://arxiv.org/abs/2506.17356</link>
      <description>arXiv:2506.17356v1 Announce Type: new 
Abstract: We explore the automatic generation of interactive, scenario-based lessons designed to train novice human tutors who teach middle school mathematics online. Employing prompt engineering through a Retrieval-Augmented Generation approach with GPT-4o, we developed a system capable of creating structured tutor training lessons. Our study generated lessons in English for three key topics: Encouraging Students' Independence, Encouraging Help-Seeking Behavior, and Turning on Cameras, using a task decomposition prompting strategy that breaks lesson generation into sub-tasks. The generated lessons were evaluated by two human evaluators, who provided both quantitative and qualitative evaluations using a comprehensive rubric informed by lesson design research. Results demonstrate that the task decomposition strategy led to higher-rated lessons compared to single-step generation. Human evaluators identified several strengths in the LLM-generated lessons, including well-structured content and time-saving potential, while also noting limitations such as generic feedback and a lack of clarity in some instructional sections. These findings underscore the potential of hybrid human-AI approaches for generating effective lessons in tutor training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17356v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jionghao Lin, Jiarui Rao, Yiyang Zhao, Yuting Wang, Ashish Gurung, Amanda Barany, Jaclyn Ocumpaugh, Ryan S. Baker, Kenneth R. Koedinger</dc:creator>
    </item>
    <item>
      <title>A Large-Scale Real-World Evaluation of LLM-Based Virtual Teaching Assistant</title>
      <link>https://arxiv.org/abs/2506.17363</link>
      <description>arXiv:2506.17363v1 Announce Type: new 
Abstract: Virtual Teaching Assistants (VTAs) powered by Large Language Models (LLMs) have the potential to enhance student learning by providing instant feedback and facilitating multi-turn interactions. However, empirical studies on their effectiveness and acceptance in real-world classrooms are limited, leaving their practical impact uncertain. In this study, we develop an LLM-based VTA and deploy it in an introductory AI programming course with 477 graduate students. To assess how student perceptions of the VTA's performance evolve over time, we conduct three rounds of comprehensive surveys at different stages of the course. Additionally, we analyze 3,869 student--VTA interaction pairs to identify common question types and engagement patterns. We then compare these interactions with traditional student--human instructor interactions to evaluate the VTA's role in the learning process. Through a large-scale empirical study and interaction analysis, we assess the feasibility of deploying VTAs in real-world classrooms and identify key challenges for broader adoption. Finally, we release the source code of our VTA system, fostering future advancements in AI-driven education: \texttt{https://github.com/sean0042/VTA}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17363v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunjun Kweon, Sooyohn Nam, Hyunseung Lim, Hwajung Hong, Edward Choi</dc:creator>
    </item>
    <item>
      <title>AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning</title>
      <link>https://arxiv.org/abs/2506.17364</link>
      <description>arXiv:2506.17364v2 Announce Type: new 
Abstract: This work investigates the use of multimodal biometrics to detect distractions caused by smartphone use during tasks that require sustained attention, with a focus on computer-based online learning. Although the methods are applicable to various domains, such as autonomous driving, we concentrate on the challenges learners face in maintaining engagement amid internal (e.g., motivation), system-related (e.g., course design) and contextual (e.g., smartphone use) factors. Traditional learning platforms often lack detailed behavioral data, but Multimodal Learning Analytics (MMLA) and biosensors provide new insights into learner attention. We propose an AI-based approach that leverages physiological signals and head pose data to detect phone use. Our results show that single biometric signals, such as brain waves or heart rate, offer limited accuracy, while head pose alone achieves 87%. A multimodal model combining all signals reaches 91% accuracy, highlighting the benefits of integration. We conclude by discussing the implications and limitations of deploying these models for real-time support in online learning environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17364v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alvaro Becerra, Roberto Daza, Ruth Cobos, Aythami Morales, Mutlu Cukurova, Julian Fierrez</dc:creator>
    </item>
    <item>
      <title>AI based Content Creation and Product Recommendation Applications in E-commerce: An Ethical overview</title>
      <link>https://arxiv.org/abs/2506.17370</link>
      <description>arXiv:2506.17370v1 Announce Type: new 
Abstract: As e-commerce rapidly integrates artificial intelligence for content creation and product recommendations, these technologies offer significant benefits in personalization and efficiency. AI-driven systems automate product descriptions, generate dynamic advertisements, and deliver tailored recommendations based on consumer behavior, as seen in major platforms like Amazon and Shopify. However, the widespread use of AI in e-commerce raises crucial ethical challenges, particularly around data privacy, algorithmic bias, and consumer autonomy. Bias -- whether cultural, gender-based, or socioeconomic -- can be inadvertently embedded in AI models, leading to inequitable product recommendations and reinforcing harmful stereotypes. This paper examines the ethical implications of AI-driven content creation and product recommendations, emphasizing the need for frameworks to ensure fairness, transparency, and need for more established and robust ethical standards. We propose actionable best practices to remove bias and ensure inclusivity, such as conducting regular audits of algorithms, diversifying training data, and incorporating fairness metrics into AI models. Additionally, we discuss frameworks for ethical conformance that focus on safeguarding consumer data privacy, promoting transparency in decision-making processes, and enhancing consumer autonomy. By addressing these issues, we provide guidelines for responsibly utilizing AI in e-commerce applications for content creation and product recommendations, ensuring that these technologies are both effective and ethically sound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17370v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.32628/CSEIT2410414</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Scientific Research in Computer Science, Engineering and Information Technology (IJSRCSEIT) Vol. 11 No. 1 (2025): January-February; Pages 3720-3728</arxiv:journal_reference>
      <dc:creator>Aditi Madhusudan Jain, Ayush Jain</dc:creator>
    </item>
    <item>
      <title>Multimodal Political Bias Identification and Neutralization</title>
      <link>https://arxiv.org/abs/2506.17372</link>
      <description>arXiv:2506.17372v1 Announce Type: new 
Abstract: Due to the presence of political echo chambers, it becomes imperative to detect and remove subjective bias and emotionally charged language from both the text and images of political articles. However, prior work has focused on solely the text portion of the bias rather than both the text and image portions. This is a problem because the images are just as powerful of a medium to communicate information as text is. To that end, we present a model that leverages both text and image bias which consists of four different steps. Image Text Alignment focuses on semantically aligning images based on their bias through CLIP models. Image Bias Scoring determines the appropriate bias score of images via a ViT classifier. Text De-Biasing focuses on detecting biased words and phrases and neutralizing them through BERT models. These three steps all culminate to the final step of debiasing, which replaces the text and the image with neutralized or reduced counterparts, which for images is done by comparing the bias scores. The results so far indicate that this approach is promising, with the text debiasing strategy being able to identify many potential biased words and phrases, and the ViT model showcasing effective training. The semantic alignment model also is efficient. However, more time, particularly in training, and resources are needed to obtain better results. A human evaluation portion was also proposed to ensure semantic consistency of the newly generated text and images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17372v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cedric Bernard, Xavier Pleimling, Amun Kharel, Chase Vickery</dc:creator>
    </item>
    <item>
      <title>A Grassroots Network and Community Roadmap for Interconnected Autonomous Science Laboratories for Accelerated Discovery</title>
      <link>https://arxiv.org/abs/2506.17510</link>
      <description>arXiv:2506.17510v1 Announce Type: new 
Abstract: Scientific discovery is being revolutionized by AI and autonomous systems, yet current autonomous laboratories remain isolated islands unable to collaborate across institutions. We present the Autonomous Interconnected Science Lab Ecosystem (AISLE), a grassroots network transforming fragmented capabilities into a unified system that shorten the path from ideation to innovation to impact and accelerates discovery from decades to months. AISLE addresses five critical dimensions: (1) cross-institutional equipment orchestration, (2) intelligent data management with FAIR compliance, (3) AI-agent driven orchestration grounded in scientific principles, (4) interoperable agent communication interfaces, and (5) AI/ML-integrated scientific education. By connecting autonomous agents across institutional boundaries, autonomous science can unlock research spaces inaccessible to traditional approaches while democratizing cutting-edge technologies. This paradigm shift toward collaborative autonomous science promises breakthroughs in sustainable energy, materials development, and public health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17510v1</guid>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rafael Ferreira da Silva, Milad Abolhasani, Dionysios A. Antonopoulos, Laura Biven, Ryan Coffee, Ian T. Foster, Leslie Hamilton, Shantenu Jha, Theresa Mayer, Benjamin Mintz, Robert G. Moore, Salahudin Nimer, Noah Paulson, Woong Shin, Frederic Suter, Mitra Taheri, Michela Taufer, Newell R. Washburn</dc:creator>
    </item>
    <item>
      <title>Public Perceptions of Autonomous Vehicles: A Survey of Pedestrians and Cyclists in Pittsburgh</title>
      <link>https://arxiv.org/abs/2506.17513</link>
      <description>arXiv:2506.17513v1 Announce Type: new 
Abstract: This study investigates how autonomous vehicle(AV) technology is perceived by pedestrians and bicyclists in Pittsburgh. Using survey data from over 1200 respondents, the research explores the interplay between demographics, AV interactions, infrastructural readiness, safety perceptions, and trust. Findings highlight demographic divides, infrastructure gaps, and the crucial role of communication and education in AV adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17513v1</guid>
      <category>cs.CY</category>
      <category>cs.RO</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rudra Y. Bedekar</dc:creator>
    </item>
    <item>
      <title>Optimizing Mastery Learning by Fast-Forwarding Over-Practice Steps</title>
      <link>https://arxiv.org/abs/2506.17577</link>
      <description>arXiv:2506.17577v1 Announce Type: new 
Abstract: Mastery learning improves learning proficiency and efficiency. However, the overpractice of skills--students spending time on skills they have already mastered--remains a fundamental challenge for tutoring systems. Previous research has reduced overpractice through the development of better problem selection algorithms and the authoring of focused practice tasks. However, few efforts have concentrated on reducing overpractice through step-level adaptivity, which can avoid resource-intensive curriculum redesign. We propose and evaluate Fast-Forwarding as a technique that enhances existing problem selection algorithms. Based on simulation studies informed by learner models and problem-solving pathways derived from real student data, Fast-Forwarding can reduce overpractice by up to one-third, as it does not require students to complete problem-solving steps if all remaining pathways are fully mastered. Fast-Forwarding is a flexible method that enhances any problem selection algorithm, though its effectiveness is highest for algorithms that preferentially select difficult problems. Therefore, our findings suggest that while Fast-Forwarding may improve student practice efficiency, the size of its practical impact may also depend on students' ability to stay motivated and engaged at higher levels of difficulty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17577v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meng Xia, Robin Schmucker, Conrad Borchers, Vincent Aleven</dc:creator>
    </item>
    <item>
      <title>Experimental Evidence for the Propagation and Preservation of Machine Discoveries in Human Populations</title>
      <link>https://arxiv.org/abs/2506.17741</link>
      <description>arXiv:2506.17741v1 Announce Type: new 
Abstract: Intelligent machines with superhuman capabilities have the potential to uncover problem-solving strategies beyond human discovery. Emerging evidence from competitive gameplay, such as Go, demonstrates that AI systems are evolving from mere tools to sources of cultural innovation adopted by humans. However, the conditions under which intelligent machines transition from tools to drivers of persistent cultural change remain unclear. We identify three key conditions for machines to fundamentally influence human problem-solving: the discovered strategies must be non-trivial, learnable, and offer a clear advantage. Using a cultural transmission experiment and an agent-based simulation, we demonstrate that when these conditions are met, machine-discovered strategies can be transmitted, understood, and preserved by human populations, leading to enduring cultural shifts. These findings provide a framework for understanding how machines can persistently expand human cognitive skills and underscore the need to consider their broader implications for human cognition and cultural evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17741v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Levin Brinkmann, Thomas F. Eisenmann, Anne-Marie Nussberger, Maxim Derex, Sara Bonati, Valerii Chirkov, Iyad Rahwan</dc:creator>
    </item>
    <item>
      <title>The value of human and machine in machine-generated creative contents</title>
      <link>https://arxiv.org/abs/2506.17808</link>
      <description>arXiv:2506.17808v1 Announce Type: new 
Abstract: The seemingly "imagination" and "creativity" from machine-generated contents should not be misattributed to the accomplishment of machine. They are accomplishments of both human and machine. Without human interpretation, the machine-generated contents remain in the imaginary space of the large language models, and cannot automatically establish grounding in the reality and human experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17808v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Weina Jin</dc:creator>
    </item>
    <item>
      <title>The Democratic Paradox in Large Language Models' Underestimation of Press Freedom</title>
      <link>https://arxiv.org/abs/2506.18045</link>
      <description>arXiv:2506.18045v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) increasingly mediate global information access for millions of users worldwide, their alignment and biases have the potential to shape public understanding and trust in fundamental democratic institutions, such as press freedom. In this study, we uncover three systematic distortions in the way six popular LLMs evaluate press freedom in 180 countries compared to expert assessments of the World Press Freedom Index (WPFI). The six LLMs exhibit a negative misalignment, consistently underestimating press freedom, with individual models rating between 71% to 93% of countries as less free. We also identify a paradoxical pattern we term differential misalignment: LLMs disproportionately underestimate press freedom in countries where it is strongest. Additionally, five of the six LLMs exhibit positive home bias, rating their home countries' press freedoms more favorably than would be expected given their negative misalignment with the human benchmark. In some cases, LLMs rate their home countries between 7% to 260% more positively than expected. If LLMs are set to become the next search engines and some of the most important cultural tools of our time, they must ensure accurate representations of the state of our human and civic rights globally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18045v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>I. Loaiza, R. Vestrelli, A. Fronzetti Colladon, R. Rigobon</dc:creator>
    </item>
    <item>
      <title>Aggregated Individual Reporting for Post-Deployment Evaluation</title>
      <link>https://arxiv.org/abs/2506.18133</link>
      <description>arXiv:2506.18133v1 Announce Type: new 
Abstract: The need for developing model evaluations beyond static benchmarking, especially in the post-deployment phase, is now well-understood. At the same time, concerns about the concentration of power in deployed AI systems have sparked a keen interest in 'democratic' or 'public' AI. In this work, we bring these two ideas together by proposing mechanisms for aggregated individual reporting (AIR), a framework for post-deployment evaluation that relies on individual reports from the public. An AIR mechanism allows those who interact with a specific, deployed (AI) system to report when they feel that they may have experienced something problematic; these reports are then aggregated over time, with the goal of evaluating the relevant system in a fine-grained manner. This position paper argues that individual experiences should be understood as an integral part of post-deployment evaluation, and that the scope of our proposed aggregated individual reporting mechanism is a practical path to that end. On the one hand, individual reporting can identify substantively novel insights about safety and performance; on the other, aggregation can be uniquely useful for informing action. From a normative perspective, the post-deployment phase completes a missing piece in the conversation about 'democratic' AI. As a pathway to implementation, we provide a workflow of concrete design decisions and pointers to areas requiring further research and methodological development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18133v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jessica Dai, Inioluwa Deborah Raji, Benjamin Recht, Irene Y. Chen</dc:creator>
    </item>
    <item>
      <title>Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study</title>
      <link>https://arxiv.org/abs/2506.17410</link>
      <description>arXiv:2506.17410v1 Announce Type: cross 
Abstract: Tutoring improves student achievement, but identifying and studying what tutoring actions are most associated with student learning at scale based on audio transcriptions is an open research problem. This present study investigates the feasibility and scalability of using generative AI to identify and evaluate specific tutor moves in real-life math tutoring. We analyze 50 randomly selected transcripts of college-student remote tutors assisting middle school students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo, Gemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills: delivering effective praise and responding to student math errors. All models reliably detected relevant situations, for example, tutors providing praise to students (94-98% accuracy) and a student making a math error (82-88% accuracy) and effectively evaluated the tutors' adherence to tutoring best practices, aligning closely with human judgments (83-89% and 73-77%, respectively). We propose a cost-effective prompting strategy and discuss practical implications for using large language models to support scalable assessment in authentic settings. This work further contributes LLM prompts to support reproducibility and research in AI-supported learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17410v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danielle R. Thomas, Conrad Borchers, Jionghao Lin, Sanjit Kakarla, Shambhavi Bhushan, Erin Gatz, Shivang Gupta, Ralph Abboud, Kenneth R. Koedinger</dc:creator>
    </item>
    <item>
      <title>Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems</title>
      <link>https://arxiv.org/abs/2506.17467</link>
      <description>arXiv:2506.17467v1 Announce Type: cross 
Abstract: Large language models (LLMs) have shown significant potential to change how we write, communicate, and create, leading to rapid adoption across society. This dissertation examines how individuals and institutions are adapting to and engaging with this emerging technology through three research directions. First, I demonstrate how the institutional adoption of AI detectors introduces systematic biases, particularly disadvantaging writers of non-dominant language varieties, highlighting critical equity concerns in AI governance. Second, I present novel population-level algorithmic approaches that measure the increasing adoption of LLMs across writing domains, revealing consistent patterns of AI-assisted content in academic peer reviews, scientific publications, consumer complaints, corporate communications, job postings, and international organization press releases. Finally, I investigate LLMs' capability to provide feedback on research manuscripts through a large-scale empirical analysis, offering insights into their potential to support researchers who face barriers in accessing timely manuscript feedback, particularly early-career researchers and those from under-resourced settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17467v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weixin Liang</dc:creator>
    </item>
    <item>
      <title>Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages</title>
      <link>https://arxiv.org/abs/2506.17603</link>
      <description>arXiv:2506.17603v1 Announce Type: cross 
Abstract: Morphological defectivity is an intriguing and understudied phenomenon in linguistics. Addressing defectivity, where expected inflectional forms are absent, is essential for improving the accuracy of NLP tools in morphologically rich languages. However, traditional linguistic resources often lack coverage of morphological gaps as such knowledge requires significant human expertise and effort to document and verify. For scarce linguistic phenomena in under-explored languages, Wikipedia and Wiktionary often serve as among the few accessible resources. Despite their extensive reach, their reliability has been a subject of controversy. This study customizes a novel neural morphological analyzer to annotate Latin and Italian corpora. Using the massive annotated data, crowd-sourced lists of defective verbs compiled from Wiktionary are validated computationally. Our results indicate that while Wiktionary provides a highly reliable account of Italian morphological gaps, 7% of Latin lemmata listed as defective show strong corpus evidence of being non-defective. This discrepancy highlights potential limitations of crowd-sourced wikis as definitive sources of linguistic knowledge, particularly for less-studied phenomena and languages, despite their value as resources for rare linguistic features. By providing scalable tools and methods for quality assurance of crowd-sourced data, this work advances computational morphology and expands linguistic knowledge of defectivity in non-English, morphologically rich languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17603v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jonathan Sakunkoo, Annabella Sakunkoo</dc:creator>
    </item>
    <item>
      <title>Trustworthy Chronic Disease Risk Prediction For Self-Directed Preventive Care via Medical Literature Validation</title>
      <link>https://arxiv.org/abs/2506.17620</link>
      <description>arXiv:2506.17620v1 Announce Type: cross 
Abstract: Chronic diseases are long-term, manageable, yet typically incurable conditions, highlighting the need for effective preventive strategies. Machine learning has been widely used to assess individual risk for chronic diseases. However, many models rely on medical test data (e.g. blood results, glucose levels), which limits their utility for proactive self-assessment. Additionally, to gain public trust, machine learning models should be explainable and transparent. Although some research on self-assessment machine learning models includes explainability, their explanations are not validated against established medical literature, reducing confidence in their reliability. To address these issues, we develop deep learning models that predict the risk of developing 13 chronic diseases using only personal and lifestyle factors, enabling accessible, self-directed preventive care. Importantly, we use SHAP-based explainability to identify the most influential model features and validate them against established medical literature. Our results show a strong alignment between the models' most influential features and established medical literature, reinforcing the models' trustworthiness. Critically, we find that this observation holds across 13 distinct diseases, indicating that this machine learning approach can be broadly trusted for chronic disease prediction. This work lays the foundation for developing trustworthy machine learning tools for self-directed preventive care. Future research can explore other approaches for models' trustworthiness and discuss how the models can be used ethically and responsibly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17620v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minh Le, Khoi Ton</dc:creator>
    </item>
    <item>
      <title>Research on the recommendation framework of foreign enterprises from the perspective of multidimensional proximity</title>
      <link>https://arxiv.org/abs/2506.17657</link>
      <description>arXiv:2506.17657v1 Announce Type: cross 
Abstract: As global economic integration progresses, foreign-funded enterprises play an increasingly crucial role in fostering local economic growth and enhancing industrial development. However, there are not many researches to deal with this aspect in recent years. This study utilizes the multidimensional proximity theory to thoroughly examine the criteria for selecting high-quality foreign-funded companies that are likely to invest in and establish factories in accordance with local conditions during the investment attraction process.First, this study leverages databases such as Wind and Osiris, along with government policy documents, to investigate foreign-funded enterprises and establish a high-quality database. Second, using a two-step method, enterprises aligned with local industrial strategies are identified. Third, a detailed analysis is conducted on key metrics, including industry revenue, concentration (measured by the Herfindahl-Hirschman Index), and geographical distance (calculated using the Haversine formula). Finally, a multi-criteria decision analysis ranks the top five companies as the most suitable candidates for local investment, with the methodology validated through a case study in a district of Beijing.The example results show that the established framework helps local governments identify high-quality foreign-funded enterprises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17657v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Guoqiang Liang, Jiarui Xie, Mengxuan Li, Shuo Zhang</dc:creator>
    </item>
    <item>
      <title>Triadic Novelty: A Typology and Measurement Framework for Recognizing Novel Contributions in Science</title>
      <link>https://arxiv.org/abs/2506.17851</link>
      <description>arXiv:2506.17851v1 Announce Type: cross 
Abstract: Scientific progress depends on novel ideas, but current reward systems often fail to recognize them. Many existing metrics conflate novelty with popularity, privileging ideas that fit existing paradigms over those that challenge them. This study develops a theory-driven framework to better understand how different types of novelty emerge, take hold, and receive recognition. Drawing on network science and theories of discovery, we introduce a triadic typology: Pioneers, who introduce entirely new topics; Mavericks, who recombine distant concepts; and Vanguards, who reinforce weak but promising connections. We apply this typology to a dataset of 41,623 articles in the interdisciplinary field of philanthropy and nonprofit studies, linking novelty types to five-year citation counts using mixed-effects negative binomial regression. Results show that novelty is not uniformly rewarded. Pioneer efforts are foundational but often overlooked. Maverick novelty shows consistent citation benefits, particularly rewarded when it displaces prior focus. Vanguard novelty is more likely to gain recognition when it strengthens weakly connected topics, but its citation advantage diminishes as those reinforced nodes become more central. To enable fair comparison across time and domains, we introduce a simulated baseline model. These findings improve the evaluation of innovations, affecting science policy, funding, and institutional assessment practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17851v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Ai, Richard S. Steinberg, Chao Guo, Filipi Nascimento Silva</dc:creator>
    </item>
    <item>
      <title>Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives</title>
      <link>https://arxiv.org/abs/2506.18116</link>
      <description>arXiv:2506.18116v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) in mental healthcare risk propagating biases that reinforce stigma and harm marginalized groups. While previous research identified concerning trends, systematic methods for detecting intersectional biases remain limited. This work introduces a multi-hop question answering (MHQA) framework to explore LLM response biases in mental health discourse. We analyze content from the Interpretable Mental Health Instruction (IMHI) dataset across symptom presentation, coping mechanisms, and treatment approaches. Using systematic tagging across age, race, gender, and socioeconomic status, we investigate bias patterns at demographic intersections. We evaluate four LLMs: Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic disparities across sentiment, demographics, and mental health conditions. Our MHQA approach demonstrates superior detection compared to conventional methods, identifying amplification points where biases magnify through sequential reasoning. We implement two debiasing techniques: Roleplay Simulation and Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot prompting with BBQ dataset examples. These findings highlight critical areas where LLMs reproduce mental healthcare biases, providing actionable insights for equitable AI development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18116v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Batool Haider, Atmika Gorti, Aman Chadha, Manas Gaur</dc:creator>
    </item>
    <item>
      <title>SoK: Current State of Ethereum's Enshrined Proposer Builder Separation</title>
      <link>https://arxiv.org/abs/2506.18189</link>
      <description>arXiv:2506.18189v1 Announce Type: cross 
Abstract: Initially introduced to Ethereum via Flashbots' MEV-boost, Proposer-Builder Separation allows proposers to auction off blockspace to a market of transaction orderers, known as builders. PBS is currently available to validators through the aforementioned MEV-boost, but its unregulated and relay-dependent nature has much of the Ethereum community calling for its enshrinement. Providing a protocol-integrated PBS marketspace and communication channel for payload outsourcing is termed PBS enshrinement. Although ePBS potentially introduces native MEV mitigation mechanisms and reduces validator operation costs, fears of multiparty collusion and chain stagnation are all too real. In addition to mitigating these potential drawbacks, PBS research pursues many tenets revered by Web3 enthusiasts, including but not limited to, censorship resistance, validator reward equity, and deflationary finance. The subsequent SoK will identify current PBS mechanisms, the need for enshrinement, additions to the ePBS upgrade, and the existing or potential on-chain socioeconomic implications of each.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18189v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maxwell Koegler</dc:creator>
    </item>
    <item>
      <title>Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review</title>
      <link>https://arxiv.org/abs/2506.18199</link>
      <description>arXiv:2506.18199v1 Announce Type: cross 
Abstract: Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18199v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bushra Asseri, Estabrag Abdelaziz, Areej Al-Wabil</dc:creator>
    </item>
    <item>
      <title>A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance</title>
      <link>https://arxiv.org/abs/2506.18576</link>
      <description>arXiv:2506.18576v1 Announce Type: cross 
Abstract: Detecting harmful content is a crucial task in the landscape of NLP applications for Social Good, with hate speech being one of its most dangerous forms. But what do we mean by hate speech, how can we define it, and how does prompting different definitions of hate speech affect model performance? The contribution of this work is twofold. At the theoretical level, we address the ambiguity surrounding hate speech by collecting and analyzing existing definitions from the literature. We organize these definitions into a taxonomy of 14 Conceptual Elements-building blocks that capture different aspects of hate speech definitions, such as references to the target of hate (individual or groups) or of the potential consequences of it. At the experimental level, we employ the collection of definitions in a systematic zero-shot evaluation of three LLMs, on three hate speech datasets representing different types of data (synthetic, human-in-the-loop, and real-world). We find that choosing different definitions, i.e., definitions with a different degree of specificity in terms of encoded elements, impacts model performance, but this effect is not consistent across all architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18576v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Melis, Gabriella Lapesa, Dennis Assenmacher</dc:creator>
    </item>
    <item>
      <title>Neural Total Variation Distance Estimators for Changepoint Detection in News Data</title>
      <link>https://arxiv.org/abs/2506.18764</link>
      <description>arXiv:2506.18764v1 Announce Type: cross 
Abstract: Detecting when public discourse shifts in response to major events is crucial for understanding societal dynamics. Real-world data is high-dimensional, sparse, and noisy, making changepoint detection in this domain a challenging endeavor. In this paper, we leverage neural networks for changepoint detection in news data, introducing a method based on the so-called learning-by-confusion scheme, which was originally developed for detecting phase transitions in physical systems. We train classifiers to distinguish between articles from different time periods. The resulting classification accuracy is used to estimate the total variation distance between underlying content distributions, where significant distances highlight changepoints. We demonstrate the effectiveness of this method on both synthetic datasets and real-world data from The Guardian newspaper, successfully identifying major historical events including 9/11, the COVID-19 pandemic, and presidential elections. Our approach requires minimal domain knowledge, can autonomously discover significant shifts in public discourse, and yields a quantitative measure of change in content, making it valuable for journalism, policy analysis, and crisis monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18764v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Csaba Zsolnai, Niels L\"orch, Julian Arnold</dc:creator>
    </item>
    <item>
      <title>The World Wide recipe: A community-centred framework for fine-grained data collection and regional bias operationalisation</title>
      <link>https://arxiv.org/abs/2406.09496</link>
      <description>arXiv:2406.09496v4 Announce Type: replace 
Abstract: We introduce the World Wide recipe, which sets forth a framework for culturally aware and participatory data collection, and the resultant regionally diverse World Wide Dishes evaluation dataset. We also analyse bias operationalisation to highlight how current systems underperform across several dimensions: (in-)accuracy, (mis-)representation, and cultural (in-)sensitivity, with evidence from qualitative community-based observations and quantitative automated tools. We find that these T2I models generally do not produce quality outputs of dishes specific to various regions. This is true even for the US, which is typically considered more well-resourced in training data -- although the generation of US dishes does outperform that of the investigated African countries. The models demonstrate the propensity to produce inaccurate and culturally misrepresentative, flattening, and insensitive outputs. These representational biases have the potential to further reinforce stereotypes and disproportionately contribute to erasure based on region. The dataset and code are available at https://github.com/oxai/world-wide-dishes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09496v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jabez Magomere, Shu Ishida, Tejumade Afonja, Aya Salama, Daniel Kochin, Foutse Yuehgoh, Imane Hamzaoui, Raesetje Sefala, Aisha Alaagib, Samantha Dalal, Beatrice Marchegiani, Elizaveta Semenova, Lauren Crais, Siobhan Mackenzie Hall</dc:creator>
    </item>
    <item>
      <title>Application of the Cyberinfrastructure Production Function Model to R1 Institutions</title>
      <link>https://arxiv.org/abs/2501.10264</link>
      <description>arXiv:2501.10264v3 Announce Type: replace 
Abstract: High-performance computing (HPC) is widely used in higher education for modeling, simulation, and AI applications. A critical piece of infrastructure with which to secure funding, attract and retain faculty, and teach students, supercomputers come with high capital and operating costs that must be considered against other competing priorities. This study applies the concepts of the production function model from economics with two thrusts: 1) to evaluate if previous research on building a model for quantifying the value of investment in research computing is generalizable to a wider set of universities, and 2) to define a model with which to capacity plan HPC investment, based on institutional production - inverting the production function. We show that the production function model does appear to generalize, showing positive institutional returns from the investment in computing resources and staff. We do, however, find that the relative relationships between model inputs and outputs vary across institutions, which can often be attributed to understandable institution-specific factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10264v3</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Preston M. Smith, Jill Gemmill, David Y. Hancock, Brian W. O'Shea, Winona Snapp-Childs, James Wilgenbusch</dc:creator>
    </item>
    <item>
      <title>Decoding Safety Feedback from Diverse Raters: A Data-driven Lens on Responsiveness to Severity</title>
      <link>https://arxiv.org/abs/2503.05609</link>
      <description>arXiv:2503.05609v2 Announce Type: replace 
Abstract: Ensuring the safety of Generative AI requires a nuanced understanding of pluralistic viewpoints. In this paper, we introduce a novel data-driven approach for interpreting granular ratings in pluralistic datasets. Specifically, we address the challenge of analyzing nuanced differences in safety feedback from a diverse population expressed via ordinal scales (e.g., a Likert scale). We distill non-parametric responsiveness metrics that quantify the consistency of raters in scoring varying levels of the severity of safety violations. Leveraging a publicly available pluralistic dataset of safety feedback on AI-generated content as our case study, we investigate how raters from different demographic groups (age, gender, ethnicity) use an ordinal scale to express their perceptions of the severity of violations. We apply our metrics across violation types, demonstrating their utility in extracting nuanced insights that are crucial for aligning AI systems reliably in multi-cultural contexts. We show that our approach can inform rater selection and feedback interpretation by capturing nuanced viewpoints across different demographic groups, hence improving the quality of pluralistic data collection and in turn contributing to more robust AI development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05609v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pushkar Mishra, Charvi Rastogi, Stephen R. Pfohl, Alicia Parrish, Tian Huey Teh, Roma Patel, Mark Diaz, Ding Wang, Michela Paganini, Vinodkumar Prabhakaran, Lora Aroyo, Verena Rieser</dc:creator>
    </item>
    <item>
      <title>Operations &amp; Supply Chain Management: Principles and Practice</title>
      <link>https://arxiv.org/abs/2503.05749</link>
      <description>arXiv:2503.05749v2 Announce Type: replace 
Abstract: Operations and Supply Chain Management (OSCM) has continually evolved, incorporating a broad array of strategies, frameworks, and technologies to address complex challenges across industries. This encyclopedic article provides a comprehensive overview of contemporary strategies, tools, methods, principles, and best practices that define the field's cutting-edge advancements. It also explores the diverse environments where OSCM principles have been effectively implemented. The article is meant to be read in a nonlinear fashion. It should be used as a point of reference or first-port-of-call for a diverse pool of readers: academics, researchers, students, and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05749v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fotios Petropoulos, Henk Akkermans, O. Zeynep Aksin, Imran Ali, Mohamed Zied Babai, Ana Barbosa-Povoa, Olga Batta\"ia, Maria Besiou, Nils Boysen, Stephen Brammer, Alistair Brandon-Jones, Dirk Briskorn, Tyson R. Browning, Paul Buijs, Piera Centobelli, Andrea Chiarini, Paul Cousins, Elizabeth A. Cudney, Andrew Davies, Steven J. Day, Ren\'e de Koster, Rommert Dekker, Juliano Denicol, M\'elanie Despeisse, Stephen M. Disney, Alexandre Dolgui, Linh Duong, Malek El-Qallali, Behnam Fahimnia, Fatemeh Fakhredin, Stanley B. Gershwin, Salar Ghamat, Vaggelis Giannikas, Christoph H. Glock, Janet Godsell, Kannan Govindan, Claire Hannibal, Anders Haug, Tomislav Hernaus, Juliana Hsuan, Dmitry Ivanov, Marianne Jahre, Bj\"orn Johansson, Madan Shankar Kalidoss, Argyris Kanellopoulos, Devika Kannan, Elif Karul, Konstantinos V. Katsikopoulos, Ayse Beg\"um Kilic-Ararat, Rainer Kolisch, Maximilian Koppenberg, Maneesh Kumar, Yong-Hong Kuo, Andrew Kusiak, Michael A. Lewis, Stanley Frederick W. T. Lim, Veronique Lim\`ere, Jiyin Liu, Omid Maghazei, Matija Mari\'c, Joern Meissner, Miranda Meuwissen, Pietro Micheli, Samudaya Nanayakkara, Beng\"u Nur \"Ozdemir, Thanos Papadopoulos, Stephen Pavelin, Srinath Perera, Wendy Phillips, Dennis Prak, Hubert Pun, Sharfah Ahmad Qazi, Usha Ramanathan, Gerald Reiner, Ewout Reitsma, Jens K. Roehrich, Nada R. Sanders, Joseph Sarkis, Nico Andr\'e Schmid, Christoph G. Schmidt, Andreas Schroeder, Kostas Selviaridis, Stefan Seuring, Chuan Shi, Byung-Gak Son, Martin Spring, Brian Squire, Wendy van der Valk, Dirk Pieter van Donk, Geert-Jan van Houtum, Miriam Wilhelm, Finn Wynstra, Ting Zheng</dc:creator>
    </item>
    <item>
      <title>Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)</title>
      <link>https://arxiv.org/abs/2505.21091</link>
      <description>arXiv:2505.21091v3 Announce Type: replace 
Abstract: System prompts in Large Language Models (LLMs) are predefined directives that guide model behaviour, taking precedence over user inputs in text processing and generation. LLM deployers increasingly use them to ensure consistent responses across contexts. While model providers set a foundation of system prompts, deployers and third-party developers can append additional prompts without visibility into others' additions, while this layered implementation remains entirely hidden from end-users. As system prompts become more complex, they can directly or indirectly introduce unaccounted for side effects. This lack of transparency raises fundamental questions about how the position of information in different directives shapes model outputs. As such, this work examines how the placement of information affects model behaviour. To this end, we compare how models process demographic information in system versus user prompts across six commercially available LLMs and 50 demographic groups. Our analysis reveals significant biases, manifesting in differences in user representation and decision-making scenarios. Since these variations stem from inaccessible and opaque system-level configurations, they risk representational, allocative and potential other biases and downstream harms beyond the user's ability to detect or correct. Our findings draw attention to these critical issues, which have the potential to perpetuate harms if left unexamined. Further, we argue that system prompt analysis must be incorporated into AI auditing processes, particularly as customisable system prompts become increasingly prevalent in commercial AI deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21091v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3715275.3732038</arxiv:DOI>
      <dc:creator>Anna Neumann, Elisabeth Kirsten, Muhammad Bilal Zafar, Jatinder Singh</dc:creator>
    </item>
    <item>
      <title>The Quantified Body: Identity, Empowerment, and Control in Smart Wearables</title>
      <link>https://arxiv.org/abs/2506.15991</link>
      <description>arXiv:2506.15991v2 Announce Type: replace 
Abstract: In an era where the body is increasingly translated into streams of biometric data, smart wearables have become not merely tools of personal health tracking but infrastructures of predictive governance. This paper examines how wearable technologies reconfigure bodily autonomy by embedding users within feedback-driven systems of self-surveillance, data extraction, and algorithmic control. Drawing on Deleuze's concept of the control society, Zuboff's theory of surveillance capitalism, and Couldry and Mejias's notion of data colonialism, I argue that smart wearables shift the discourse of health empowerment toward a modality of compliance aligned with neoliberal values of productivity, efficiency, and self-discipline. Rather than offering transparent consent, these technologies operate within what scholars describe as a post-consent regime -- where asymmetrical data relations are normalized through seamless design and behavioral nudging. Through interdisciplinary analysis, the paper further explores alternative trajectories for wearable design and governance, from historical examples of care-centered devices to contemporary anti-extractive practices and collective data justice frameworks. Ultimately, it calls for a paradigm shift from individual optimization to democratic accountability and structural reform in the governance of bodily data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15991v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maijunxian Wang</dc:creator>
    </item>
    <item>
      <title>"I understand why I got this grade": Automatic Short Answer Grading with Feedback</title>
      <link>https://arxiv.org/abs/2407.12818</link>
      <description>arXiv:2407.12818v2 Announce Type: replace-cross 
Abstract: In recent years, there has been a growing interest in using Artificial Intelligence (AI) to automate student assessment in education. Among different types of assessments, summative assessments play a crucial role in evaluating a student's understanding level of a course. Such examinations often involve short-answer questions. However, grading these responses and providing meaningful feedback manually at scale is both time-consuming and labor-intensive. Feedback is particularly important, as it helps students recognize their strengths and areas for improvement. Despite the importance of this task, there is a significant lack of publicly available datasets that support automatic short-answer grading with feedback generation. To address this gap, we introduce Engineering Short Answer Feedback (EngSAF), a dataset designed for automatic short-answer grading with feedback. The dataset covers a diverse range of subjects, questions, and answer patterns from multiple engineering domains and contains ~5.8k data points. We incorporate feedback into our dataset by leveraging the generative capabilities of state-of-the-art large language models (LLMs) using our Label-Aware Synthetic Feedback Generation (LASFG) strategy. This paper underscores the importance of enhanced feedback in practical educational settings, outlines dataset annotation and feedback generation processes, conducts a thorough EngSAF analysis, and provides different LLMs-based zero-shot and finetuned baselines for future comparison. The best-performing model (Mistral-7B) achieves an overall accuracy of 75.4% and 58.7% on unseen answers and unseen question test sets, respectively. Additionally, we demonstrate the efficiency and effectiveness of our ASAG system through its deployment in a real-world end-semester exam at a reputed institute.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12818v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dishank Aggarwal, Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya</dc:creator>
    </item>
    <item>
      <title>Cross-Camera Distracted Driver Classification through Feature Disentanglement and Contrastive Learning</title>
      <link>https://arxiv.org/abs/2411.13181</link>
      <description>arXiv:2411.13181v2 Announce Type: replace-cross 
Abstract: The classification of distracted drivers is pivotal for ensuring safe driving. Previous studies demonstrated the effectiveness of neural networks in automatically predicting driver distraction, fatigue, and potential hazards. However, recent research has uncovered a significant loss of accuracy in these models when applied to samples acquired under conditions that differ from the training data. In this paper, we introduce a robust model designed to withstand changes in camera position within the vehicle. Our Driver Behavior Monitoring Network (DBMNet) relies on a lightweight backbone and integrates a disentanglement module to discard camera view information from features, coupled with contrastive learning to enhance the encoding of various driver actions. Experiments conducted using a leave-one-camera-out protocol on the daytime and nighttime subsets of the 100-Driver dataset validate the effectiveness of our approach. Cross-dataset and cross-camera experiments conducted on three benchmark datasets, namely AUCDD-V1, EZZ2021 and SFD, demonstrate the superior generalization capabilities of the proposed method. Overall DBMNet achieves an improvement of 7% in Top-1 accuracy compared to existing approaches. Moreover, a quantized version of the DBMNet and all considered methods has been deployed on a Coral Dev Board board. In this deployment scenario, DBMNet outperforms alternatives, achieving the lowest average error while maintaining a compact model size, low memory footprint, fast inference time, and minimal power consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13181v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Simone Bianco, Luigi Celona, Paolo Napoletano</dc:creator>
    </item>
    <item>
      <title>A Systems Thinking Approach to Algorithmic Fairness</title>
      <link>https://arxiv.org/abs/2412.16641</link>
      <description>arXiv:2412.16641v5 Announce Type: replace-cross 
Abstract: Systems thinking provides us with a way to model the algorithmic fairness problem by allowing us to encode prior knowledge and assumptions about where we believe bias might exist in the data generating process. We can then encode these beliefs as a series of causal graphs, enabling us to link AI/ML systems to politics and the law. This allows us to combine techniques from machine learning, causal inference, and system dynamics in order to capture different emergent aspects of the fairness problem. We can use systems thinking to help policymakers on both sides of the political aisle to understand the complex trade-offs that exist from different types of fairness policies, providing a sociotechnical foundation for designing AI policy that is aligned to their political agendas and with society's shared democratic values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16641v5</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Lam</dc:creator>
    </item>
    <item>
      <title>ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping</title>
      <link>https://arxiv.org/abs/2502.02072</link>
      <description>arXiv:2502.02072v2 Announce Type: replace-cross 
Abstract: The rapid evolution of Large Language Models (LLMs) has transformed natural language processing but raises critical concerns about biases inherent in their deployment and use across diverse linguistic and sociocultural contexts. This paper presents a framework named ASCenD BDS (Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping). The framework presents approach to detecting bias, discrimination, stereotyping across various categories such as gender, caste, age, disability, socioeconomic status, linguistic variations, etc., using an approach which is Adaptive, Stochastic and Context-Aware. The existing frameworks rely heavily on usage of datasets to generate scenarios for detection of Bias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ. However, such an approach provides point solutions. As a result, these datasets provide a finite number of scenarios for assessment. The current framework overcomes this limitation by having features which enable Adaptability, Stochasticity, Context Awareness. Context awareness can be customized for any nation or culture or sub-culture (for example an organization's unique culture). In this paper, context awareness in the Indian context has been established. Content has been leveraged from Indian Census 2011 to have a commonality of categorization. A framework has been developed using Category, Sub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability, Stochasticity and Context awareness. The framework has been described in detail in Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories were developed by a team of consultants at Saint Fox Consultancy Private Ltd. The concept has been tested out in SFCLabs as part of product development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02072v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajiv Bahl, Venkatesan N, Parimal Aglawe, Aastha Sarasapalli, Bhavya Kancharla, Chaitanya kolukuluri, Harish Mohite, Japneet Hora, Kiran Kakollu, Rahul Dhiman, Shubham Kapale, Sri Bhagya Kathula, Vamsikrishna Motru, Yogeshwar Reddy</dc:creator>
    </item>
    <item>
      <title>Compromising Honesty and Harmlessness in Language Models via Deception Attacks</title>
      <link>https://arxiv.org/abs/2502.08301</link>
      <description>arXiv:2502.08301v2 Announce Type: replace-cross 
Abstract: Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce "deception attacks" that undermine both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. We introduce fine-tuning methods that cause models to selectively deceive users on targeted topics while remaining accurate on others. Through a series of experiments, we show that such targeted deception is effective even in high-stakes domains or ideologically charged subjects. In addition, we find that deceptive fine-tuning often compromises other safety properties: deceptive models are more likely to produce toxic content, including hate speech and stereotypes. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08301v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laur\`ene Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff</dc:creator>
    </item>
  </channel>
</rss>
