<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Feb 2026 05:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Affirmative Action in India with Horizontal Reservations</title>
      <link>https://arxiv.org/abs/2602.09189</link>
      <description>arXiv:2602.09189v1 Announce Type: new 
Abstract: India implements the world's most complex affirmative action program through vertical and horizontal reservations. Although applicants can belong to at most one vertical category, they can qualify for multiple horizontal reservation categories simultaneously. We examine resource allocation problems in India, where horizontal reservations follow a hierarchical structure within a one-to-all horizontal matching framework. We introduce the hierarchical choice rule and show that it selects the most meritorious set of applicants. We thoroughly analyze the properties of the aggregate choice rule, which comprises hierarchical choice rules across all vertical categories. We show that the generalized deferred acceptance mechanism, when coupled with this aggregate choice rule, is the unique stable and strategy-proof mechanism that eliminates justified envy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09189v1</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Orhan Ayg\"un, Bertan Turhan</dc:creator>
    </item>
    <item>
      <title>Selective Disclosure in Overlapping Generations</title>
      <link>https://arxiv.org/abs/2602.09406</link>
      <description>arXiv:2602.09406v1 Announce Type: new 
Abstract: We develop an overlapping generations model where each agent observes a verifiable private signal about the state and, with positive probability, also receives signals disclosed by his predecessor. The agent then takes an action and decides which signals to pass on. Each agent's action has a positive externality on his predecessor and his optimal action increases in his belief about the state. We show that as the communication friction vanishes, agents become increasingly selective in disclosing information. As the probability that messages reach the next generation approaches one, all signals except those with the highest likelihood ratio will be concealed in equilibrium.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09406v1</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nemanja Antic, Harry Pei</dc:creator>
    </item>
    <item>
      <title>Robust Trust</title>
      <link>https://arxiv.org/abs/2602.09490</link>
      <description>arXiv:2602.09490v1 Announce Type: new 
Abstract: An agent chooses an action using her private information combined with recommendations from an informed but potentially misaligned adviser. With a known alignment probability, the adviser reports his signal truthfully; with remaining probability, the adviser can send an arbitrary message. We characterize the decision rule that maximizes the agent's worst-case expected payoff. Every optimal rule admits a trust region representation in belief space: advice is taken at face value when it induces a posterior within the trust region; otherwise, the agent acts as if the posterior were on the trust region's boundary. We derive thresholds on the alignment probability above which the adviser's presence strictly benefits the agent and fully characterize the solution in binary-state as well as binary-action environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09490v1</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Piotr Dworczak, Alex Smolin</dc:creator>
    </item>
    <item>
      <title>Competitive Credit and Present Bias: A Stochastic Discounting Approach</title>
      <link>https://arxiv.org/abs/2602.09728</link>
      <description>arXiv:2602.09728v1 Announce Type: new 
Abstract: A prominent theme in behavioural contract theory is the study of present-biased agents represented through quasi-hyperbolic discounting. In a model of competitive credit provision, we study an alternative to this framework in which the agent has a private stochastic discount factor and may overestimate the likelihood of more patient values. Agent preferences, however, are timeconsistent. While a limiting case of our model corresponds to a "fully naive" agent in work on quasi-hyperbolic discounting, another case is where the agent has correct beliefs about future discounting. In equilibrium, the agent selects options with earlier consumption in case of less patient discount factor realisations, but is penalised by receiving worse terms. Our model thus accounts for an important feature of equilibrium contracts identified in Heidhues and K\H{o}szegi (2010). Unlike Heidhues and K\H{o}szegi, our framework often predicts excessively backloaded consumption, including when the agent holds correct beliefs about future discounting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09728v1</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddharth Chatterjee, Daniel F. Garrett</dc:creator>
    </item>
    <item>
      <title>Incentive Pareto Efficiency in Monopoly Insurance Markets with Adverse Selection</title>
      <link>https://arxiv.org/abs/2602.09967</link>
      <description>arXiv:2602.09967v1 Announce Type: new 
Abstract: We study a monopolistic insurance market with hidden information, where the agent's type $\theta$ is private information that is unobservable to the insurer, and it is drawn from a continuum of types. The hidden type affects both the loss distribution and the risk attitude of the agent. Within this framework, we show that a menu of contracts is incentive efficient if and only if it maximizes social welfare, subject to incentive compatibility and individual rationality constraints. This equivalence holds for general concave utility functionals. In the special case of Yaari Dual Utility, we provide a semi-explicit characterization of optimal incentive-efficient menus of contracts. We do this under two different settings: (i) the first assumes that types are ordered in a way such that larger values of $\theta$ correspond to more risk-averse types who face stochastically larger losses; whereas (ii) the second assumes that larger values of $\theta$ correspond to less risk-averse types who face stochastically larger losses. In both settings, the structure of optimal incentive-efficient menus of contracts depends on the level of the social welfare weight. Moreover, at the optimum, higher types receive greater coverage in exchange for higher premia. Additionally, optimal menus leave the lowest type indifferent, with the insurer absorbing all surplus from the lowest type; and they exhibit efficiency at the top, that is, the highest type receives full coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09967v1</guid>
      <category>econ.TH</category>
      <category>q-fin.RM</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maria Andraos, Mario Ghossoub</dc:creator>
    </item>
    <item>
      <title>The Architecture of Illusion: Network Opacity and Strategic Escalation</title>
      <link>https://arxiv.org/abs/2602.10053</link>
      <description>arXiv:2602.10053v1 Announce Type: cross 
Abstract: Standard models of bounded rationality typically assume agents either possess accurate knowledge of the population's reasoning abilities (Cognitive Hierarchy) or hold dogmatic, degenerate beliefs (Level-$k$). We introduce the ``Connected Minds'' model, which unifies these frameworks by integrating iterative reasoning with a parameterized network bias. We posit that agents do not observe the global population; rather, they observe a sample biased by their network position, governed by a locality parameter $p$ representing algorithmic ranking, social homophily, or information disclosure. We show that this parameter acts as a continuous bridge: the model collapses to the myopic Level-$k$ recursion as networks become opaque ($p \to 0$) and recovers the standard Cognitive Hierarchy model under full transparency ($p=1$). Theoretically, we establish that network opacity induces a \emph{Sophisticated Bias}, causing agents to systematically overestimate the cognitive depth of their opponents while preserving the log-concavity of belief distributions. This makes $p$ an actionable lever: a planner or platform can tune transparency -- globally or by segment (a personalized $p_k$) -- to shape equilibrium behavior. From a mechanism design perspective, we derive the \emph{Escalation Principle}: in games of strategic complements, restricting information can maximize aggregate effort by trapping agents in echo chambers where they compete against hallucinated, high-sophistication peers. Conversely, we identify a \emph{Transparency Reversal} for coordination games, where maximizing network visibility is required to minimize variance and stabilize outcomes. Our results suggest that network topology functions as a cognitive zoom lens, determining whether agents behave as local imitators or global optimizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10053v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raman Ebrahimi, Sepehr Ilami, Babak Heydari, Isabel Trevino, Massimo Franceschetti</dc:creator>
    </item>
    <item>
      <title>Desirable Rankings</title>
      <link>https://arxiv.org/abs/2205.11684</link>
      <description>arXiv:2205.11684v5 Announce Type: replace 
Abstract: We study the problem of aggregating individual preferences over alternatives into a collective ranking. A distinctive feature of our setting is that agents are matched to alternatives. Applications include rankings of colleges or academic journals. The foundation of our approach is that alternatives agents desire -- that is, those they rank above their match -- should also be ranked higher socially. We introduce axioms to formalize this idea and call rankings that satisfy them desirable. We develop an algorithm to construct desirable rankings and prove that, as the market becomes large, desirable rankings converge to the true underlying ranking of the alternatives by quality. We support this convergence result through simulations and demonstrate the practical usefulness of our approach by ranking Chilean medical programs with data from their centralized admission system. Finally, we compare performance and show that our approach outperforms two benchmarks: revealed preference rankings and Borda counts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.11684v5</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaurab Aryal, Thayer Morrill, Peter Troyan</dc:creator>
    </item>
    <item>
      <title>Local non-bossiness</title>
      <link>https://arxiv.org/abs/2406.01398</link>
      <description>arXiv:2406.01398v5 Announce Type: replace 
Abstract: The student-optimal stable mechanism (DA), the most popular mechanism in school choice, is the only one that is stable and strategy-proof. However, when DA is implemented, a student can change the schools of others without changing her own. We show that this drawback is limited: a student cannot change her schoolmates while remaining at the same school. We refer to this new property as local non-bossiness and use it to provide a new characterization of DA that does not rely on stability. Furthermore, we show that local non-bossiness plays a crucial role in providing incentives to be truthful when students have preferences over their colleagues. As long as students first consider the school to which they are assigned and then their schoolmates, DA induces the only stable and strategy-proof mechanism. There is limited room to expand this preference domain without compromising the existence of a stable and strategy-proof mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01398v5</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo Duque, Juan S. Pereyra, Juan Pablo Torres-Mart\'inez</dc:creator>
    </item>
    <item>
      <title>Statistical Equilibrium of Optimistic Beliefs</title>
      <link>https://arxiv.org/abs/2502.09569</link>
      <description>arXiv:2502.09569v2 Announce Type: replace 
Abstract: We study finite normal-form games in which payoffs are subject to random perturbations and players face uncertainty about how these shocks co-move across actions, an ambiguity that naturally arises when only realized (not counterfactual) payoffs are observed. We introduce the Statistical Equilibrium of Optimistic Beliefs (SE-OB), inspired by discrete choice theory. We model players as \textit{optimistic better responders}: they face ambiguity about the dependence structure (copula) of payoff perturbations across actions and resolve this ambiguity by selecting, from a belief set, the joint distribution that maximizes the expected value of the best perturbed payoff. Given this optimistic belief, players choose actions according to the induced random-utility choice rule. We define SE-OB as a fixed point of this two-step response mapping.
  SE-OB generalizes the Nash equilibrium and the structural quantal response equilibrium. We establish existence under standard regularity conditions on belief sets. For the economically important class of marginal belief sets, that is, the set of all joint distributions with fixed action-wise marginals, optimistic belief selection reduces to an optimal coupling problem, and SE-OB admits a characterization via Nash equilibrium of a smooth regularized game, yielding tractability and enabling computation.
  We characterize the relationship between SE-OB and existing equilibrium notions and illustrate its empirical relevance in simulations, where it captures systematic violations of independence of irrelevant alternatives that standard logit-based models fail to explain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09569v2</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Gui, Bahar Ta\c{s}kesen</dc:creator>
    </item>
    <item>
      <title>Robust Learning with Private Information</title>
      <link>https://arxiv.org/abs/2505.05341</link>
      <description>arXiv:2505.05341v3 Announce Type: replace 
Abstract: Firms increasingly delegate decisions to learning algorithms in platform markets. Standard algorithms perform well when platform policies are stationary, but firms often face ambiguity about whether policies are stationary or adapt strategically to their behavior. When policies adapt, efficient learning under stationarity may backfire: it may reveal a firm's persistent private information, allowing the platform to personalize terms and extract information rents. We study a repeated screening problem in which an agent with a fixed private type commits ex ante to a learning algorithm, facing ambiguity about the principal's policy. We show that a broad class of standard algorithms, including all no-external-regret algorithms, can be manipulated by adaptive principals and permit asymptotic full surplus extraction. We then construct a misspecification-robust learning algorithm that treats stationarity as a testable hypothesis. It achieves the optimal payoff under stationarity at the minimax-optimal rate, while preventing dynamic rent extraction: against any adaptive principal, each type's long-run utility is at least its utility under the menu that maximizes revenue under the principal's prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05341v3</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyohei Okumura</dc:creator>
    </item>
    <item>
      <title>A multi-self model of self-punishment</title>
      <link>https://arxiv.org/abs/2601.01421</link>
      <description>arXiv:2601.01421v4 Announce Type: replace 
Abstract: We investigate the choice of a decision maker (DM) who harms herself, by maximizing in each menu some distortion of her true preference, in which the first i alternatives are moved, in reverse order, to the bottom. This pattern has no empirical power, but it allows to define a degree of self-punishment, which measures the extent of the denial of pleasure adopted by the DM. We characterize irrational choices displaying the lowest degree of self-punishment, and we fully identify the preferences that explain the DM's picks by a minimal denial of pleasure. These datasets account for some well known selection biases, such as second-best procedures, and the handicapped avoidance. Necessary and sufficient conditions for the estimation of the degree of self-punishment of a choice are singled out. Moreover the linear orders whose harmful distortions justify choice data are partially elicited. Finally, we offer a simple characterization of the choice behavior that exhibits the highest degree of self-punishment, and we show that this subclass comprises almost all choices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01421v4</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Angelo Enrico Petralia</dc:creator>
    </item>
    <item>
      <title>Topological Semantics for Common Inductive Knowledge</title>
      <link>https://arxiv.org/abs/2602.06927</link>
      <description>arXiv:2602.06927v2 Announce Type: replace-cross 
Abstract: Lewis' account of common knowledge in Convention describes the generation of higher-order expectations between agents as hinging upon agents' inductive standards and a shared witness. This paper attempts to draw from insights in learning theory to provide a formal account of common inductive knowledge and how it can be generated by a witness. Our language has a rather rich syntax in order to capture equally rich notions central to Lewis' account of common knowledge; for instance, we speak of an agent 'having some reason to believe' a proposition and one proposition 'indicating' to an agent that another proposition holds. A similar line of work was pursued by Cubitt &amp; Sugden 2003; however, their account was left wanting for a corresponding semantics. Our syntax affords a novel topological semantics which, following Kelly 1996's approach in The Logic of Reliable Inquiry, takes as primitives agents' information bases. In particular, we endow each agent with a 'switching tolerance' meant to represent their personal inductive standards for learning. Curiously, when all agents are truly inductive learners (not choosing to believe only those propositions which are deductively verified), we show that the set of worlds where a proposition $P$ is common inductive knowledge is invariant of agents' switching tolerances. Contrarily, the question of whether a specific witness $W$ generates common inductive knowledge of $P$ is sensitive to changing agents' switching tolerances. After establishing soundness of our proof system with respect to this semantics, we conclude by applying our logic to solve an 'inductive' variant of the coordinated attack problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06927v2</guid>
      <category>cs.LO</category>
      <category>econ.TH</category>
      <category>math.LO</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddharth Namachivayam</dc:creator>
    </item>
  </channel>
</rss>
