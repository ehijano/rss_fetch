<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Mar 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Rule of link functions on Binomial Regression Model: A Cross Sectional Study on Child Malnutrition, Bangladesh</title>
      <link>https://arxiv.org/abs/2403.17948</link>
      <description>arXiv:2403.17948v1 Announce Type: new 
Abstract: Link function is a key tool in the binomial regression model defined as non-linear model under GLM approach. It transforms the nonlinear regression to linear model with converting the interval (-\infty,\infty) to the probability [0,1]. The binomial model with link functions (logit, probit, cloglog and cauchy) are applied on the proportional of child malnutrition age 0-5 years in each household level. Multiple Indicator Cluster survey (MICS)-2019, Bangladesh was conducted by a joint cooperation of UNICEF and BBS . The survey covered 64000 households using two stage stratified sampling technique, where around 21000 household have children age 0-5 years. We use bi-variate analysis to find the statistical association between response and sociodemographic features. In the binary regression model, probit model provides the best result based on the lowest standard error of covariates and goodness of fit test (deviance, AIC).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17948v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Mehedi Hasan Bhuiyan</dc:creator>
    </item>
    <item>
      <title>Markov chain models for inspecting response dynamics in psychological testing</title>
      <link>https://arxiv.org/abs/2403.17982</link>
      <description>arXiv:2403.17982v1 Announce Type: new 
Abstract: The importance of considering contextual probabilities in shaping response patterns within psychological testing is underscored, despite the ubiquitous nature of order effects discussed extensively in methodological literature. Drawing from concepts such as path-dependency, first-order autocorrelation, state-dependency, and hysteresis, the present study is an attempt to address how earlier responses serve as an anchor for subsequent answers in tests, surveys, and questionnaires. Introducing the notion of non-commuting observables derived from quantum physics, I highlight their role in characterizing psychological processes and the impact of measurement instruments on participants' responses. We advocate for the utilization of first-order Markov chain modeling to capture and forecast sequential dependencies in survey and test responses. The employment of the first-order Markov chain model lies in individuals' propensity to exhibit partial focus to preceding responses, with recent items most likely exerting a substantial influence on subsequent response selection. This study contributes to advancing our understanding of the dynamics inherent in sequential data within psychological research and provides a methodological framework for conducting longitudinal analyses of response patterns of test and questionnaire.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17982v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Bosco</dc:creator>
    </item>
    <item>
      <title>Comment on "Safe Testing" by Gr\"unwald, de Heide, and Koolen</title>
      <link>https://arxiv.org/abs/2403.17986</link>
      <description>arXiv:2403.17986v1 Announce Type: new 
Abstract: This comment briefly reflects on "Safe Testing" by Gr\"{u}wald et al. (2024). The safety of fractional Bayes factors (O'Hagan, 1995) is illustrated and compared to (safe) Bayes factors based on the right Haar prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17986v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joris Mulder</dc:creator>
    </item>
    <item>
      <title>Doubly robust causal inference through penalized bias-reduced estimation: combining non-probability samples with designed surveys</title>
      <link>https://arxiv.org/abs/2403.18039</link>
      <description>arXiv:2403.18039v1 Announce Type: new 
Abstract: Causal inference on the average treatment effect (ATE) using non-probability samples, such as electronic health records (EHR), faces challenges from sample selection bias and high-dimensional covariates. This requires considering a selection model alongside treatment and outcome models that are typical ingredients in causal inference. This paper considers integrating large non-probability samples with external probability samples from a design survey, addressing moderately high-dimensional confounders and variables that influence selection. In contrast to the two-step approach that separates variable selection and debiased estimation, we propose a one-step plug-in doubly robust (DR) estimator of the ATE. We construct a novel penalized estimating equation by minimizing the squared asymptotic bias of the DR estimator. Our approach facilitates ATE inference in high-dimensional settings by ignoring the variability in estimating nuisance parameters, which is not guaranteed in conventional likelihood approaches with non-differentiable L1-type penalties. We provide a consistent variance estimator for the DR estimator. Simulation studies demonstrate the double robustness of our estimator under misspecification of either the outcome model or the selection and treatment models, as well as the validity of statistical inference under penalized estimation. We apply our method to integrate EHR data from the Michigan Genomics Initiative with an external probability sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18039v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiacong Du, Xu Shi, Donglin Zeng, Bhramar Mukherjee</dc:creator>
    </item>
    <item>
      <title>Personalized Imputation in metric spaces via conformal prediction: Applications in Predicting Diabetes Development with Continuous Glucose Monitoring Information</title>
      <link>https://arxiv.org/abs/2403.18069</link>
      <description>arXiv:2403.18069v1 Announce Type: new 
Abstract: The challenge of handling missing data is widespread in modern data analysis, particularly during the preprocessing phase and in various inferential modeling tasks. Although numerous algorithms exist for imputing missing data, the assessment of imputation quality at the patient level often lacks personalized statistical approaches. Moreover, there is a scarcity of imputation methods for metric space based statistical objects. The aim of this paper is to introduce a novel two-step framework that comprises: (i) a imputation methods for statistical objects taking values in metrics spaces, and (ii) a criterion for personalizing imputation using conformal inference techniques. This work is motivated by the need to impute distributional functional representations of continuous glucose monitoring (CGM) data within the context of a longitudinal study on diabetes, where a significant fraction of patients do not have available CGM profiles. The importance of these methods is illustrated by evaluating the effectiveness of CGM data as new digital biomarkers to predict the time to diabetes onset in healthy populations. To address these scientific challenges, we propose: (i) a new regression algorithm for missing responses; (ii) novel conformal prediction algorithms tailored for metric spaces with a focus on density responses within the 2-Wasserstein geometry; (iii) a broadly applicable personalized imputation method criterion, designed to enhance both of the aforementioned strategies, yet valid across any statistical model and data structure. Our findings reveal that incorporating CGM data into diabetes time-to-event analysis, augmented with a novel personalization phase of imputation, significantly enhances predictive accuracy by over ten percent compared to traditional predictive models for time to diabetes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18069v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena, Carla D\'iaz-Louzao, Rahul Ghosal, Francisco Gude</dc:creator>
    </item>
    <item>
      <title>Assessing COVID-19 Vaccine Effectiveness in Observational Studies via Nested Trial Emulation</title>
      <link>https://arxiv.org/abs/2403.18115</link>
      <description>arXiv:2403.18115v1 Announce Type: new 
Abstract: Observational data are often used to estimate real-world effectiveness and durability of coronavirus disease 2019 (COVID-19) vaccines. A sequence of nested trials can be emulated to draw inference from such data while minimizing selection bias, immortal time bias, and confounding. Typically, when nested trial emulation (NTE) is employed, effect estimates are pooled across trials to increase statistical efficiency. However, such pooled estimates may lack a clear interpretation when the treatment effect is heterogeneous across trials. In the context of COVID-19, vaccine effectiveness quite plausibly will vary over calendar time due to newly emerging variants of the virus. This manuscript considers a NTE inverse probability weighted estimator of vaccine effectiveness that may vary over calendar time, time since vaccination, or both. Statistical testing of the trial effect homogeneity assumption is considered. Simulation studies are presented examining the finite-sample performance of these methods under a variety of scenarios. The methods are used to estimate vaccine effectiveness against COVID-19 outcomes using observational data on over 120,000 residents of Abruzzo, Italy during 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18115v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin B. DeMonte, Bonnie E. Shook-Sa, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>Cumulative Incidence Function Estimation Based on Population-Based Biobank Data</title>
      <link>https://arxiv.org/abs/2403.18464</link>
      <description>arXiv:2403.18464v1 Announce Type: new 
Abstract: Many countries have established population-based biobanks, which are being used increasingly in epidemiolgical and clinical research. These biobanks offer opportunities for large-scale studies addressing questions beyond the scope of traditional clinical trials or cohort studies. However, using biobank data poses new challenges. Typically, biobank data is collected from a study cohort recruited over a defined calendar period, with subjects entering the study at various ages falling between $c_L$ and $c_U$. This work focuses on biobank data with individuals reporting disease-onset age upon recruitment, termed prevalent data, along with individuals initially recruited as healthy, and their disease onset observed during the follow-up period. We propose a novel cumulative incidence function (CIF) estimator that efficiently incorporates prevalent cases, in contrast to existing methods, providing two advantages: (1) increased efficiency, and (2) CIF estimation for ages before the lower limit, $c_L$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18464v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malka Gorfine, David M. Zucker, Shoval Shoham</dc:creator>
    </item>
    <item>
      <title>A communication-efficient, online changepoint detection method for monitoring distributed sensor networks</title>
      <link>https://arxiv.org/abs/2403.18549</link>
      <description>arXiv:2403.18549v1 Announce Type: new 
Abstract: We consider the challenge of efficiently detecting changes within a network of sensors, where we also need to minimise communication between sensors and the cloud. We propose an online, communication-efficient method to detect such changes. The procedure works by performing likelihood ratio tests at each time point, and two thresholds are chosen to filter unimportant test statistics and make decisions based on the aggregated test statistics respectively. We provide asymptotic theory concerning consistency and the asymptotic distribution if there are no changes. Simulation results suggest that our method can achieve similar performance to the idealised setting, where we have no constraints on communication between sensors, but substantially reduce the transmission costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18549v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Yang, Idris A. Eckley, Paul Fearnhead</dc:creator>
    </item>
    <item>
      <title>Collaborative graphical lasso</title>
      <link>https://arxiv.org/abs/2403.18602</link>
      <description>arXiv:2403.18602v1 Announce Type: new 
Abstract: In recent years, the availability of multi-omics data has increased substantially. Multi-omics data integration methods mainly aim to leverage different molecular data sets to gain a complete molecular description of biological processes. An attractive integration approach is the reconstruction of multi-omics networks. However, the development of effective multi-omics network reconstruction strategies lags behind. This hinders maximizing the potential of multi-omics data sets. With this study, we advance the frontier of multi-omics network reconstruction by introducing "collaborative graphical lasso" as a novel strategy. Our proposed algorithm synergizes "graphical lasso" with the concept of "collaboration", effectively harmonizing multi-omics data sets integration, thereby enhancing the accuracy of network inference. Besides, to tackle model selection in this framework, we designed an ad hoc procedure based on network stability. We assess the performance of collaborative graphical lasso and the corresponding model selection procedure through simulations, and we apply them to publicly available multi-omics data. This demonstrated collaborative graphical lasso is able to reconstruct known biological connections and suggest previously unknown and biologically coherent interactions, enabling the generation of novel hypotheses. We implemented collaborative graphical lasso as an R package, available on CRAN as coglasso.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18602v1</guid>
      <category>stat.ME</category>
      <category>q-bio.MN</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessio Albanese, Wouter Kohlen, Pariya Behrouzi</dc:creator>
    </item>
    <item>
      <title>Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models using Markov Chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2403.18072</link>
      <description>arXiv:2403.18072v1 Announce Type: cross 
Abstract: Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data. Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters. However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner. We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive. The GO-OED design is then found by maximizing the EIG over the design space using Bayesian optimization. We demonstrate the effectiveness of the overall nonlinear GO-OED method, and illustrate its differences versus conventional non-GO-OED, through various test problems and an application of sensor placement for source inversion in a convection-diffusion field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18072v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Zhong, Wanggang Shen, Tommie Catanach, Xun Huan</dc:creator>
    </item>
    <item>
      <title>LocalCop: An R package for local likelihood inference for conditional copulas</title>
      <link>https://arxiv.org/abs/2403.18245</link>
      <description>arXiv:2403.18245v1 Announce Type: cross 
Abstract: Conditional copulas models allow the dependence structure between multiple response variables to be modelled as a function of covariates. LocalCop (Acar &amp; Lysy, 2024) is an R/C++ package for computationally efficient semiparametric conditional copula modelling using a local likelihood inference framework developed in Acar, Craiu, &amp; Yao (2011), Acar, Craiu, &amp; Yao (2013) and Acar, Czado, &amp; Lysy (2019).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18245v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elif F. Acar, Martin Lysy, Alan Kuchinsky</dc:creator>
    </item>
    <item>
      <title>Beyond boundaries: Gary Lorden's groundbreaking contributions to sequential analysis</title>
      <link>https://arxiv.org/abs/2403.18782</link>
      <description>arXiv:2403.18782v1 Announce Type: cross 
Abstract: Gary Lorden provided a number of fundamental and novel insights to sequential hypothesis testing and changepoint detection. In this article we provide an overview of Lorden's contributions in the context of existing results in those areas, and some extensions made possible by Lorden's work, mentioning also areas of application including threat detection in physical-computer systems, near-Earth space informatics, epidemiology, clinical trials, and finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18782v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jay Bartroff, Alexander G. Tartakovsky</dc:creator>
    </item>
    <item>
      <title>IQ: Intrinsic measure for quantifying the heterogeneity in meta-analysis</title>
      <link>https://arxiv.org/abs/2109.05755</link>
      <description>arXiv:2109.05755v2 Announce Type: replace 
Abstract: Quantifying the heterogeneity is an important issue in meta-analysis, and among the existing measures, the $I^2$ statistic is the most commonly used measure in the literature. In this paper, we show that the $I^2$ statistic was, in fact, defined as problematic or even completely wrong from the very beginning. To confirm this statement, we first present a motivating example to show that the $I^2$ statistic is heavily dependent on the study sample sizes, and consequently it may yield contradictory results for the amount of heterogeneity. Moreover, by drawing a connection between ANOVA and meta-analysis, the $I^2$ statistic is shown to have, mistakenly, applied the sampling errors of the estimators rather than the variances of the study populations. Inspired by this, we introduce an Intrinsic measure for Quantifying the heterogeneity in meta-analysis, and meanwhile study its statistical properties to clarify why it is superior to the existing measures. We further propose an optimal estimator, referred to as the IQ statistic, for the new measure of heterogeneity that can be readily applied in meta-analysis. Simulations and real data analysis demonstrate that the IQ statistic provides a nearly unbiased estimate of the true heterogeneity and it is also independent of the study sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.05755v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ke Yang, Enxuan Lin, Tiejun Tong</dc:creator>
    </item>
    <item>
      <title>Estimating sparse direct effects in multivariate regression with the spike-and-slab LASSO</title>
      <link>https://arxiv.org/abs/2207.07020</link>
      <description>arXiv:2207.07020v3 Announce Type: replace 
Abstract: The multivariate regression interpretation of the Gaussian chain graph model simultaneously parametrizes (i) the direct effects of $p$ predictors on $q$ outcomes and (ii) the residual partial covariances between pairs of outcomes. We introduce a new method for fitting sparse Gaussian chain graph models with spike-and-slab LASSO (SSL) priors. We develop an Expectation Conditional Maximization algorithm to obtain sparse estimates of the $p \times q$ matrix of direct effects and the $q \times q$ residual precision matrix. Our algorithm iteratively solves a sequence of penalized maximum likelihood problems with self-adaptive penalties that gradually filter out negligible regression coefficients and partial covariances. Because it adaptively penalizes individual model parameters, our method is seen to outperform fixed-penalty competitors on simulated data. We establish the posterior contraction rate for our model, buttressing our method's excellent empirical performance with strong theoretical guarantees. Using our method, we estimated the direct effects of diet and residence type on the composition of the gut microbiome of elderly adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.07020v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunyi Shen, Claudia Sol\'is-Lemus, Sameer K. Deshpande</dc:creator>
    </item>
    <item>
      <title>Selective inference using randomized group lasso estimators for general models</title>
      <link>https://arxiv.org/abs/2306.13829</link>
      <description>arXiv:2306.13829v3 Announce Type: replace 
Abstract: Selective inference methods are developed for group lasso estimators for use with a wide class of distributions and loss functions. The method includes the use of exponential family distributions, as well as quasi-likelihood modeling for overdispersed count data, for example, and allows for categorical or grouped covariates as well as continuous covariates. A randomized group-regularized optimization problem is studied. The added randomization allows us to construct a post-selection likelihood which we show to be adequate for selective inference when conditioning on the event of the selection of the grouped covariates. This likelihood also provides a selective point estimator, accounting for the selection by the group lasso. Confidence regions for the regression parameters in the selected model take the form of Wald-type regions and are shown to have bounded volume. The selective inference method for grouped lasso is illustrated on data from the national health and nutrition examination survey while simulations showcase its behaviour and favorable comparison with other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13829v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yiling Huang, Sarah Pirenne, Snigdha Panigrahi, Gerda Claeskens</dc:creator>
    </item>
    <item>
      <title>Robust propensity score weighting estimation under missing at random</title>
      <link>https://arxiv.org/abs/2306.15173</link>
      <description>arXiv:2306.15173v3 Announce Type: replace 
Abstract: Missing data is frequently encountered in many areas of statistics. Propensity score weighting is a popular method for handling missing data. The propensity score method employs a response propensity model, but correct specification of the statistical model can be challenging in the presence of missing data. Doubly robust estimation is attractive, as the consistency of the estimator is guaranteed when either the outcome regression model or the propensity score model is correctly specified. In this paper, we first employ information projection to develop an efficient and doubly robust estimator under indirect model calibration constraints. The resulting propensity score estimator can be equivalently expressed as a doubly robust regression imputation estimator by imposing the internal bias calibration condition in estimating the regression parameters. In addition, we generalize the information projection to allow for outlier-robust estimation. Some asymptotic properties are presented. The simulation study confirms that the proposed method allows robust inference against not only the violation of various model assumptions, but also outliers. A real-life application is presented using data from the Conservation Effects Assessment Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15173v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hengfang Wang, Jae Kwang Kim, Jeongseop Han, Youngjo Lee</dc:creator>
    </item>
    <item>
      <title>A Note on Ising Network Analysis with Missing Data</title>
      <link>https://arxiv.org/abs/2307.00567</link>
      <description>arXiv:2307.00567v2 Announce Type: replace 
Abstract: The Ising model has become a popular psychometric model for analyzing item response data. The statistical inference of the Ising model is typically carried out via a pseudo-likelihood, as the standard likelihood approach suffers from a high computational cost when there are many variables (i.e., items). Unfortunately, the presence of missing values can hinder the use of pseudo-likelihood, and a listwise deletion approach for missing data treatment may introduce a substantial bias into the estimation and sometimes yield misleading interpretations. This paper proposes a conditional Bayesian framework for Ising network analysis with missing data, which integrates a pseudo-likelihood approach with iterative data imputation. An asymptotic theory is established for the method. Furthermore, a computationally efficient {P{\'o}lya}-Gamma data augmentation procedure is proposed to streamline the sampling of model parameters. The method's performance is shown through simulations and a real-world application to data on major depressive and generalized anxiety disorders from the National Epidemiological Survey on Alcohol and Related Conditions (NESARC).</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00567v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siliang Zhang, Yunxiao Chen</dc:creator>
    </item>
    <item>
      <title>Non-parametric inference on calibration of predicted risks</title>
      <link>https://arxiv.org/abs/2307.09713</link>
      <description>arXiv:2307.09713v3 Announce Type: replace 
Abstract: Moderate calibration, the expected event probability among observations with predicted probability z being equal to z, is a desired property of risk prediction models. Current graphical and numerical techniques for evaluating moderate calibration of risk prediction models are mostly based on smoothing or grouping the data. As well, there is no widely accepted inferential method for the null hypothesis that a model is moderately calibrated. In this work, we discuss recently-developed, and propose novel, methods for the assessment of moderate calibration for binary responses. The methods are based on the limiting distributions of functions of standardized partial sums of prediction errors converging to the corresponding laws of Brownian motion. The novel method relies on well-known properties of the Brownian bridge which enables joint inference on mean and moderate calibration, leading to a unified "bridge" test for detecting miscalibration. Simulation studies indicate that the bridge test is more powerful, often substantially, than the alternative test. As a case study we consider a prediction model for short-term mortality after a heart attack, where we provide suggestions on graphical presentation and the interpretation of results. Moderate calibration can be assessed without requiring arbitrary grouping of data or using methods that require tuning of parameters. An accompanying R package implements this method (see https://github.com/resplab/cumulcalib/).</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.09713v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Sadatsafavi, John Petkau</dc:creator>
    </item>
    <item>
      <title>NLP-based detection of systematic anomalies among the narratives of consumer complaints</title>
      <link>https://arxiv.org/abs/2308.11138</link>
      <description>arXiv:2308.11138v3 Announce Type: replace 
Abstract: We develop an NLP-based procedure for detecting systematic nonmeritorious consumer complaints, simply called systematic anomalies, among complaint narratives. While classification algorithms are used to detect pronounced anomalies, in the case of smaller and frequent systematic anomalies, the algorithms may falter due to a variety of reasons, including technical ones as well as natural limitations of human analysts. Therefore, as the next step after classification, we convert the complaint narratives into quantitative data, which are then analyzed using an algorithm for detecting systematic anomalies. We illustrate the entire procedure using complaint narratives from the Consumer Complaint Database of the Consumer Financial Protection Bureau.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11138v3</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <category>q-fin.RM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peiheng Gao, Ning Sun, Xuefeng Wang, Chen Yang, Ri\v{c}ardas Zitikis</dc:creator>
    </item>
    <item>
      <title>Negative Spillover: A Potential Source of Bias in Pragmatic Clinical Trials</title>
      <link>https://arxiv.org/abs/2309.10978</link>
      <description>arXiv:2309.10978v4 Announce Type: replace 
Abstract: Pragmatic clinical trials evaluate the effectiveness of health interventions in real-world settings. Negative spillover can arise in a pragmatic trial if the study intervention affects how scarce resources are allocated between patients in the intervention and comparison groups. This can harm patients assigned to the control group and lead to overestimation of treatment effect. While this type of negative spillover is often addressed in trials of social welfare and public health interventions, there is little recognition of this source of bias in the medical literature. In this article, I examine what causes negative spillover and how it may have led clinical trial investigators to overestimate the effect of patient navigation, AI-based physiological alarms, and elective induction of labor. I also suggest ways to detect negative spillover and design trials that avoid this potential source of bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10978v4</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean Mann</dc:creator>
    </item>
    <item>
      <title>Modeling lower-truncated and right-censored insurance claims with an extension of the MBBEFD class</title>
      <link>https://arxiv.org/abs/2310.11471</link>
      <description>arXiv:2310.11471v2 Announce Type: replace 
Abstract: In general insurance, claims are often lower-truncated and right-censored because insurance contracts may involve deductibles and maximal covers. Most classical statistical models are not (directly) suited to model lower-truncated and right-censored claims. A surprisingly flexible family of distributions that can cope with lower-truncated and right-censored claims is the class of MBBEFD distributions that originally has been introduced by Bernegger (1997) for reinsurance pricing, but which has not gained much attention outside the reinsurance literature. Interestingly, in general insurance, we mainly rely on unimodal skewed densities, whereas the reinsurance literature typically proposes monotonically decreasing densities within the MBBEFD class. We show that this class contains both types of densities, and we extend it to a bigger family of distribution functions suitable for modeling lower-truncated and right-censored claims. In addition, we discuss how changes in the deductible or the maximal cover affect the chosen distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11471v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Selim Gatti, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Modeling for Bivariate Multiscale Spatial Data with Application to Blood Test Monitoring</title>
      <link>https://arxiv.org/abs/2310.13580</link>
      <description>arXiv:2310.13580v2 Announce Type: replace 
Abstract: In public health applications, spatial data collected are often recorded at different spatial scales and over different correlated variables. Spatial change of support is a key inferential problem in these applications and have become standard in univariate settings; however, it is less standard in multivariate settings. There are several existing multivariate spatial models that can be easily combined with multiscale spatial approach to analyze multivariate multiscale spatial data. In this paper, we propose three new models from such combinations for bivariate multiscale spatial data in a Bayesian context. In particular, we extend spatial random effects models, multivariate conditional autoregressive models, and ordered hierarchical models through a multiscale spatial approach. We run simulation studies for the three models and compare them in terms of prediction performance and computational efficiency. We motivate our models through an analysis of 2015 Texas annual average percentage receiving two blood tests from the Dartmouth Atlas Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13580v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Zhou, Jonathan R. Bradley</dc:creator>
    </item>
    <item>
      <title>Assessing the overall and partial causal well-specification of nonlinear additive noise models</title>
      <link>https://arxiv.org/abs/2310.16502</link>
      <description>arXiv:2310.16502v3 Announce Type: replace 
Abstract: We propose a method to detect model misspecifications in nonlinear causal additive and potentially heteroscedastic noise models. We aim to identify predictor variables for which we can infer the causal effect even in cases of such misspecification. We develop a general framework based on knowledge of the multivariate observational data distribution. We then propose an algorithm for finite sample data, discuss its asymptotic properties, and illustrate its performance on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16502v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Schultheiss, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>Semi-Confirmatory Factor Analysis for High-Dimensional Data with Interconnected Community Structures</title>
      <link>https://arxiv.org/abs/2401.00624</link>
      <description>arXiv:2401.00624v2 Announce Type: replace 
Abstract: Confirmatory factor analysis (CFA) is a statistical method for identifying and confirming the presence of latent factors among observed variables through the analysis of their covariance structure. Compared to alternative factor models, CFA offers interpretable common factors with enhanced specificity and a more adaptable approach to modeling covariance structures. However, the application of CFA has been limited by the requirement for prior knowledge about "non-zero loadings" and by the lack of computational scalability (e.g., it can be computationally intractable for hundreds of observed variables). We propose a data-driven semi-confirmatory factor analysis (SCFA) model that attempts to alleviate these limitations. SCFA automatically specifies "non-zero loadings" by learning the network structure of the large covariance matrix of observed variables, and then offers closed-form estimators for factor loadings, factor scores, covariances between common factors, and variances between errors using the likelihood method. Therefore, SCFA is applicable to high-throughput datasets (e.g., hundreds of thousands of observed variables) without requiring prior knowledge about "non-zero loadings". Through an extensive simulation analysis benchmarking against standard packages, SCFA exhibits superior performance in estimating model parameters with a much reduced computational time. We illustrate its practical application through factor analysis on a high-dimensional RNA-seq gene expression dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00624v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yifan Yang, Tianzhou Ma, Chuan Bi, Shuo Chen</dc:creator>
    </item>
    <item>
      <title>Bayesian scalar-on-network regression with applications to brain functional connectivity</title>
      <link>https://arxiv.org/abs/2401.16749</link>
      <description>arXiv:2401.16749v2 Announce Type: replace 
Abstract: This paper presents a Bayesian regression model relating scalar outcomes to brain functional connectivity represented as symmetric positive definite (SPD) matrices. Unlike many proposals that simply vectorize the matrix-valued connectivity predictors thereby ignoring their geometric structure, the method presented here respects the Riemannian geometry of SPD matrices by using a tangent space modeling. Dimension reduction is performed in the tangent space, relating the resulting low-dimensional representations to the responses. The dimension reduction matrix is learned in a supervised manner with a sparsity-inducing prior imposed on a Stiefel manifold to prevent overfitting. Our method yields a parsimonious regression model that allows uncertainty quantification of all model parameters and identification of key brain regions that predict the outcomes. We demonstrate the performance of our approach in simulation settings and through a case study to predict Picture Vocabulary scores using data from the Human Connectome Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16749v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaomeng Ju, Hyung G. Park, Thaddeus Tarpey</dc:creator>
    </item>
    <item>
      <title>A Type of Nonlinear Fr\'echet Regressions</title>
      <link>https://arxiv.org/abs/2403.17481</link>
      <description>arXiv:2403.17481v2 Announce Type: replace 
Abstract: The existing Fr\'echet regression is actually defined within a linear framework, since the weight function in the Fr\'echet objective function is linearly defined, and the resulting Fr\'echet regression function is identified to be a linear model when the random object belongs to a Hilbert space. Even for nonparametric and semiparametric Fr\'echet regressions, which are usually nonlinear, the existing methods handle them by local linear (or local polynomial) technique, and the resulting Fr\'echet regressions are (locally) linear as well. We in this paper introduce a type of nonlinear Fr\'echet regressions. Such a framework can be utilized to fit the essentially nonlinear models in a general metric space and uniquely identify the nonlinear structure in a Hilbert space. Particularly, its generalized linear form can return to the standard linear Fr\'echet regression through a special choice of the weight function. Moreover, the generalized linear form possesses methodological and computational simplicity because the Euclidean variable and the metric space element are completely separable. The favorable theoretical properties (e.g. the estimation consistency and presentation theorem) of the nonlinear Fr\'echet regressions are established systemically. The comprehensive simulation studies and a human mortality data analysis demonstrate that the new strategy is significantly better than the competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17481v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lu Lin, Ze Chen</dc:creator>
    </item>
    <item>
      <title>A nonparametric test for elliptical distribution based on kernel embedding of probabilities</title>
      <link>https://arxiv.org/abs/2306.10594</link>
      <description>arXiv:2306.10594v2 Announce Type: replace-cross 
Abstract: Elliptical distribution is a basic assumption underlying many multivariate statistical methods. For example, in sufficient dimension reduction and statistical graphical models, this assumption is routinely imposed to simplify the data dependence structure. Before applying such methods, we need to decide whether the data are elliptically distributed. Currently existing tests either focus exclusively on spherical distributions, or rely on bootstrap to determine the null distribution, or require specific forms of the alternative distribution. In this paper, we introduce a general nonparametric test for elliptical distribution based on kernel embedding of the probability measure that embodies the two properties that characterize an elliptical distribution: namely, after centering and rescaling, (1) the direction and length of the random vector are independent, and (2) the directional vector is uniformly distributed on the unit sphere. We derive the asymptotic distributions of the test statistic via von-Mises expansion, develop the sample-level procedure to determine the rejection region, and establish the consistency and validity of the proposed test. We also develop the concentration bounds of the test statistic, allowing the dimension to grow with the sample size, and further establish the consistency in this high-dimension setting. We compare our method with several existing methods via simulation studies, and apply our test to a SENIC dataset with and without a transformation aimed to achieve ellipticity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10594v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Tang, Bing Li</dc:creator>
    </item>
    <item>
      <title>Nesting Particle Filters for Experimental Design in Dynamical Systems</title>
      <link>https://arxiv.org/abs/2402.07868</link>
      <description>arXiv:2402.07868v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel approach to Bayesian experimental design for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential Monte Carlo technique to infer optimal designs, and embed it into a particle Markov chain Monte Carlo framework to perform gradient-based policy amortization. Our approach is distinct from other amortized experimental design techniques, as it does not rely on contrastive estimators. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07868v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahel Iqbal, Adrien Corenflos, Simo S\"arkk\"a, Hany Abdulsamad</dc:creator>
    </item>
    <item>
      <title>Partial Identification of Individual-Level Parameters Using Aggregate Data in a Nonparametric Binary Outcome Model</title>
      <link>https://arxiv.org/abs/2403.07236</link>
      <description>arXiv:2403.07236v2 Announce Type: replace-cross 
Abstract: It is well known that the relationship between variables at the individual level can be different from the relationship between those same variables aggregated over individuals. This problem of aggregation becomes relevant when the researcher wants to learn individual-level relationships, but only has access to data that has been aggregated. In this paper, I develop a methodology to partially identify linear combinations of conditional average outcomes from aggregate data when the outcome of interest is binary, while imposing as few restrictions on the underlying data generating process as possible. I construct identified sets using an optimization program that allows for researchers to impose additional shape and data restrictions. I also provide consistency results and construct an inference procedure that is valid with aggregate data, which only provides marginal information about each variable. I apply the methodology to simulated and real-world data sets and find that the estimated identified sets are too wide to be useful, but become narrower as more assumptions are imposed and data aggregated at a finer level is available. This suggests that to obtain useful information from aggregate data sets about individual-level relationships, researchers must impose further assumptions that are carefully justified or seek out data aggregated at the finest level possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07236v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah Moon</dc:creator>
    </item>
    <item>
      <title>On the Weighted Top-Difference Distance: Axioms, Aggregation, and Approximation</title>
      <link>https://arxiv.org/abs/2403.15198</link>
      <description>arXiv:2403.15198v2 Announce Type: replace-cross 
Abstract: We study a family of distance functions on rankings that allow for asymmetric treatments of alternatives and consider the distinct relevance of the top and bottom positions for ordered lists. We provide a full axiomatic characterization of our distance. In doing so, we retrieve new characterizations of existing axioms and show how to effectively weaken them for our purposes. This analysis highlights the generality of our distance as it embeds many (semi)metrics previously proposed in the literature. Subsequently, we show that, notwithstanding its level of generality, our distance is still readily applicable. We apply it to preference aggregation, studying the features of the associated median voting rule. It is shown how the derived preference function satisfies many desirable features in the context of voting rules, ranging from fairness to majority and Pareto-related properties. We show how to compute consensus rankings exactly, and provide generalized Diaconis-Graham inequalities that can be leveraged to obtain approximation algorithms. Finally, we propose some truncation ideas for our distances inspired by Lu and Boutilier (2010). These can be leveraged to devise a Polynomial-Time-Approximation Scheme for the corresponding rank aggregation problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15198v2</guid>
      <category>cs.GT</category>
      <category>cs.DM</category>
      <category>econ.TH</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Aveni, Ludovico Crippa, Giulio Principi</dc:creator>
    </item>
    <item>
      <title>Asymptotics of predictive distributions driven by sample means and variances</title>
      <link>https://arxiv.org/abs/2403.16828</link>
      <description>arXiv:2403.16828v2 Announce Type: replace-cross 
Abstract: Let $\alpha_n(\cdot)=P\bigl(X_{n+1}\in\cdot\mid X_1,\ldots,X_n\bigr)$ be the predictive distributions of a sequence $(X_1,X_2,\ldots)$ of $p$-variate random variables. Suppose $$\alpha_n=\mathcal{N}_p(M_n,Q_n)$$ where $M_n=\frac{1}{n}\sum_{i=1}^nX_i$ and $Q_n=\frac{1}{n}\sum_{i=1}^n(X_i-M_n)(X_i-M_n)^t$. Then, there is a random probability measure $\alpha$ on $\mathbb{R}^p$ such that $\alpha_n\rightarrow\alpha$ weakly a.s. If $p\in\{1,2\}$, one also obtains $\lVert\alpha_n-\alpha\rVert\overset{a.s.}\longrightarrow 0$ where $\lVert\cdot\rVert$ is total variation distance. Moreover, the convergence rate of $\lVert\alpha_n-\alpha\rVert$ is arbitrarily close to $n^{-1/2}$. These results (apart from the one regarding the convergence rate) still apply even if $\alpha_n=\mathcal{L}_p(M_n,Q_n)$, where $\mathcal{L}_p$ belongs to a class of distributions much larger than the normal. Finally, the asymptotic behavior of copula-based predictive distributions (introduced in [13]) is investigated and a numerical experiment is performed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16828v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuele Garelli, Fabrizio Leisen, Luca Pratelli, Pietro Rigo</dc:creator>
    </item>
  </channel>
</rss>
