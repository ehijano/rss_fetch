<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Feb 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Comprehensive Formal Verification of Observational Correctness for the CHERIoT-Ibex Processor</title>
      <link>https://arxiv.org/abs/2502.04738</link>
      <description>arXiv:2502.04738v1 Announce Type: new 
Abstract: The CHERI architecture equips conventional RISC ISAs with significant architectural extensions that provide a hardware-enforced mechanism for memory protection and software compartmentalisation. Architectural capabilities replace conventional integer pointers with memory addresses bound to permissions constraining their use. We present the first comprehensive formal verification of a capability extended RISC-V processor with internally 'compressed' capabilities - a concise encoding of capabilities with some resemblance to floating point number representations.
  The reference model for RTL correctness is a minor variant of the full and definitive ISA description written in the Sail ISA specification language. This is made accessible to formal verification tools by a prototype flow for translation of Sail into SystemVerilog. Our verification demonstrates a methodology for establishing that the processor always produces a stream of interactions with memory that is identical to that specified in Sail, when started in the same initial state. We additionally establish liveness. This abstract, microarchitecture-independent observational correctness property provides a comprehensive and clear assurance of functional correctness for the CHERIoT-Ibex processor's observable interactions with memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04738v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louis-Emile Ploix, Alasdair Armstrong, Tom Melham, Ray Lin, Haolong Wang, Anastasia Courtney</dc:creator>
    </item>
    <item>
      <title>Croc: An End-to-End Open-Source Extensible RISC-V MCU Platform to Democratize Silicon</title>
      <link>https://arxiv.org/abs/2502.05090</link>
      <description>arXiv:2502.05090v1 Announce Type: new 
Abstract: Ensuring a continuous and growing influx of skilled chip designers and a smooth path from education to innovation are key goals for several national and international "Chips Acts". Silicon democratization can greatly benefit from end-to-end (from silicon technology to software) free and open-source (OS) platforms. We present Croc, an extensible RISC-V microcontroller platform explicitly targeted at hands-on teaching and innovation. Croc features a streamlined OS synthesis and an end-to-end OS implementation flow, ensuring full, unconstrained access to the design, the design automation tools, and the implementation technology. Croc uses the industry-proven, open-source CVE2 core, implementing the RV32I(EMC) instruction set architecture (ISA), enabling students to define and implement their own ISA extensions. MLEM, a tapeout of Croc in IHP's open 130 nm node completed in eight weeks by a team of just two students, demonstrates the platform's viability for hands-on teaching in schools, universities, or even on a self-education path. In spring 2025, ETH Zurich will utilize Croc for its curricular VLSI class, involving up to 80 students, producing up to 40 OS application-specific integrated circuit layouts, and completing up to five student-led system-on-chip tapeouts. The lecture notes and exercises are already available under a Creative Commons license.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05090v1</guid>
      <category>cs.AR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Phillippe Sauter, Thomas Benz, Paul Scheffler, Hannah Pochert, Luisa W\"uthrich, Martin Povi\v{s}er, Beat Muheim, Frank K. G\"urkaynak, Luca Benini</dc:creator>
    </item>
    <item>
      <title>All-in-One Analog AI Accelerator: On-Chip Training and Inference with Conductive-Metal-Oxide/HfOx ReRAM Devices</title>
      <link>https://arxiv.org/abs/2502.04524</link>
      <description>arXiv:2502.04524v1 Announce Type: cross 
Abstract: Analog in-memory computing is an emerging paradigm designed to efficiently accelerate deep neural network workloads. Recent advancements have demonstrated significant improvements in throughput and efficiency, focusing independently on either inference or training acceleration. However, a unified analog in-memory technology platform-capable of performing on-chip training, retaining the weights, and sustaining long-term inference acceleration-has yet to be reported. In this work, an all-in-one analog AI accelerator is presented and benchmarked, combining these capabilities to enable autonomous, energy-efficient, and continuously adaptable AI systems. The platform leverages an array of filamentary conductive-metal-oxide (CMO)/HfOx redox-based resistive switching memory cells (ReRAM) in one-transistor one-ReRAM (1T1R) configuration, integrated into the back-end-of-line (BEOL) of a 130 nm technology node. The array characterization demonstrates reliable and optimized resistive switching with voltage amplitudes of less than 1.5 V, enabling compatibility with advanced technology nodes. The multi-bit capability of over 32 stable states (5 bits) and record-low programming noise down to 10 nS enable an almost ideal weight transfer process, more than an order of magnitude better than other memristive technologies. The array's inference performance is validated through realistic matrix-vector multiplication simulations on a 64x64 array, achieving a record-low root-mean-square error ranging from 0.06 at 1 second to 0.2 at 10 years after programming, compared to the ideal floating-point case. The array is then measured under the same conditions as those used for on-chip training. Training accuracy closely matching the software equivalent is achieved across different datasets, with high-fidelity modelling of the device response based on experimental-only data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04524v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Donato Francesco Falcone, Victoria Clerico, Wooseok Choi, Tommaso Stecconi, Folkert Horst, Laura Begon-Lours, Matteo Galetta, Antonio La Porta, Nikhil Garg, Fabien Alibart, Bert Jan Offrein, Valeria Bragaglia</dc:creator>
    </item>
    <item>
      <title>WaferLLM: A Wafer-Scale LLM Inference System</title>
      <link>https://arxiv.org/abs/2502.04563</link>
      <description>arXiv:2502.04563v1 Announce Type: cross 
Abstract: Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh-based architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to fully exploit these accelerators. We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR device model that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators. Evaluations show that WaferLLM achieves 200$\times$ better wafer-scale accelerator utilization than state-of-the-art systems. On a commodity wafer-scale accelerator, WaferLLM delivers 606$\times$ faster and 22$\times$ more energy-efficient GEMV compared to an advanced GPU. For LLMs, WaferLLM enables 39$\times$ faster decoding with 1.7$\times$ better energy efficiency. We anticipate these numbers will grow significantly as wafer-scale AI models, software, and hardware continue to mature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04563v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Congjie He, Yeqi Huang, Pei Mu, Ziming Miao, Jilong Xue, Lingxiao Ma, Fan Yang, Luo Mai</dc:creator>
    </item>
    <item>
      <title>New Security Challenges Towards In-Sensor Computing Systems</title>
      <link>https://arxiv.org/abs/2502.05046</link>
      <description>arXiv:2502.05046v1 Announce Type: cross 
Abstract: Data collection and processing in advanced health monitoring systems are experiencing revolutionary change. In-Sensor Computing (ISC) systems emerge as a promising alternative to save energy on massive data transmission, analog-to-digital conversion, and ineffective processing. While the new paradigm shift of ISC systems gains increasing attention, the highly compacted systems could incur new challenges from a hardware security perspective. This work first conducts a literature review to highlight the research trend of this topic and then performs comprehensive analyses on the root of security challenges. This is the first work that compares the security challenges of traditional sensor-involved computing systems and emerging ISC systems. Furthermore, new attack scenarios are predicted for board-, chip-, and device-level ISC systems. Two proof-of-concept demos are provided to inspire new countermeasure designs against unique hardware security threats in ISC systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05046v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mashrafi Kajol, Qiaoyan Yu</dc:creator>
    </item>
    <item>
      <title>Fixed-Throughput GRAND with FIFO Scheduling</title>
      <link>https://arxiv.org/abs/2502.05124</link>
      <description>arXiv:2502.05124v1 Announce Type: cross 
Abstract: Guessing random additive noise decoding (GRAND) is a code-agnostic decoding method that iteratively guesses the noise pattern affecting the received codeword. The number of noise sequences to test depends on the noise realization. Thus, GRAND exhibits random runtime which results in nondeterministic throughput. However, real-time systems must process the incoming data at a fixed rate, necessitating a fixed-throughput decoder in order to avoid losing data. We propose a first-in first-out (FIFO) scheduling architecture that enables a fixed throughput while improving the block error rate (BLER) compared to the common approach of imposing a maximum runtime constraint per received codeword. Moreover, we demonstrate that the average throughput metric of GRAND-based hardware implementations typically provided in the literature can be misleading as one needs to operate at approximately one order of magnitude lower throughput to achieve the BLER of an unconstrained decoder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05124v1</guid>
      <category>eess.SP</category>
      <category>cs.AR</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Christen, Darja Nonaca, Christoph Studer</dc:creator>
    </item>
    <item>
      <title>Spin-NeuroMem: A Low-Power Neuromorphic Associative Memory Design Based on Spintronic Devices</title>
      <link>https://arxiv.org/abs/2404.02463</link>
      <description>arXiv:2404.02463v2 Announce Type: replace 
Abstract: Biologically-inspired computing models have made significant progress in recent years, but the conventional von Neumann architecture is inefficient for the large-scale matrix operations and massive parallelism required by these models. This paper presents Spin-NeuroMem, a low-power circuit design of Hopfield network for the function of associative memory. Spin-NeuroMem is equipped with energy-efficient spintronic synapses which utilize magnetic tunnel junctions (MTJs) to store weight matrices of multiple associative memories. The proposed synapse design achieves as low as 17.4% power consumption compared to the state-of-the-art synapse designs. Spin-NeuroMem also encompasses a novel voltage converter with a 53.3% reduction in transistor usage for effective Hopfield network computation. In addition, we propose an associative memory simulator for the first time, which achieves a 5Mx speedup with a comparable associative memory effect. By harnessing the potential of spintronic devices, this work paves the way for the development of energy-efficient and scalable neuromorphic computing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02463v2</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>physics.app-ph</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Siqing Fu, Lizhou Wu, Tiejun Li, Chunyuan Zhang, Jianmin Zhang, Sheng Ma</dc:creator>
    </item>
    <item>
      <title>CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair</title>
      <link>https://arxiv.org/abs/2409.12993</link>
      <description>arXiv:2409.12993v2 Announce Type: replace 
Abstract: Despite the significant progress made in code generation with large language models, challenges persist, especially with hardware description languages such as Verilog. This paper first presents an analysis of fine-tuned LLMs on Verilog coding, with synthetic data from prior methods. We identify two main issues: difficulties in handling non-textual representations (Karnaugh maps, state-transition diagrams and waveforms) and significant variability during training with models randomly making "minor" mistakes. To address these limitations, we enhance data curation by creating correct-by-construction data targeting non-textual representations. Additionally, we introduce an automated framework that generates error reports from various model checkpoints and injects these errors into open-source code to create targeted code repair data. Our fine-tuned Starcoder2-15B outperforms prior state-of-the-art results by 3.8%, 10.9%, 6.6% for pass@1 on VerilogEval-Machine, VerilogEval-Human, and RTLLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12993v2</guid>
      <category>cs.AR</category>
      <category>cs.CL</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingjie Liu, Yun-Da Tsai, Wenfei Zhou, Haoxing Ren</dc:creator>
    </item>
    <item>
      <title>Pushing the Limits of BFP on Narrow Precision LLM Inference</title>
      <link>https://arxiv.org/abs/2502.00026</link>
      <description>arXiv:2502.00026v2 Announce Type: replace 
Abstract: The substantial computational and memory demands of Large Language Models (LLMs) hinder their deployment. Block Floating Point (BFP) has proven effective in accelerating linear operations, a cornerstone of LLM workloads. However, as sequence lengths grow, nonlinear operations, such as Attention, increasingly become performance bottlenecks due to their quadratic computational complexity. These nonlinear operations are predominantly executed using inefficient floating-point formats, which renders the system challenging to optimize software efficiency and hardware overhead. In this paper, we delve into the limitations and potential of applying BFP to nonlinear operations. Given our findings, we introduce a hardware-software co-design framework (DB-Attn), including: (i) DBFP, an advanced BFP version, overcomes nonlinear operation challenges with a pivot-focus strategy for diverse data and an adaptive grouping strategy for flexible exponent sharing. (ii) DH-LUT, a novel lookup table algorithm dedicated to accelerating nonlinear operations with DBFP format. (iii) An RTL-level DBFP-based engine is implemented to support DB-Attn, applicable to FPGA and ASIC. Results show that DB-Attn provides significant performance improvements with negligible accuracy loss, achieving 74% GPU speedup on Softmax of LLaMA and 10x low overhead performance improvement over SOTA designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00026v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hui Wang, Yuan Cheng, Xiaomeng Han, Zhengpeng Zhao, Dawei Yang, Zhe Jiang</dc:creator>
    </item>
    <item>
      <title>AI Load Dynamics--A Power Electronics Perspective</title>
      <link>https://arxiv.org/abs/2502.01647</link>
      <description>arXiv:2502.01647v2 Announce Type: replace 
Abstract: As AI-driven computing infrastructures rapidly scale, discussions around data center design often emphasize energy consumption, water and electricity usage, workload scheduling, and thermal management. However, these perspectives often overlook the critical interplay between AI-specific load transients and power electronics. This paper addresses that gap by examining how large-scale AI workloads impose unique demands on power conversion chains and, in turn, how the power electronics themselves shape the dynamic behavior of AI-based infrastructure. We illustrate the fundamental constraints imposed by multi-stage power conversion architectures and highlight the key role of final-stage modules in defining realistic power slew rates for GPU clusters. Our analysis shows that traditional designs, optimized for slower-varying or CPU-centric workloads, may not adequately accommodate the rapid load ramps and drops characteristic of AI accelerators. To bridge this gap, we present insights into advanced converter topologies, hierarchical control methods, and energy buffering techniques that collectively enable robust and efficient power delivery. By emphasizing the bidirectional influence between AI workloads and power electronics, we hope this work can set a good starting point and offer practical design considerations to ensure future exascale-capable data centers can meet the stringent performance, reliability, and scalability requirements of next-generation AI deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01647v2</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuzhuo Li, Yunwei Li</dc:creator>
    </item>
    <item>
      <title>Citadel: Simple Spectre-Safe Isolation For Real-World Programs That Share Memory</title>
      <link>https://arxiv.org/abs/2306.14882</link>
      <description>arXiv:2306.14882v5 Announce Type: replace-cross 
Abstract: Transient execution side-channel attacks, such as Spectre, have been shown to break almost all isolation primitives. We introduce a new security property we call relaxed microarchitectural isolation (RMI) that allows sensitive programs that are not-constant-time to share memory with an attacker while restricting the information leakage to that of non-speculative execution. Although this type of speculative security property is typically challenging to enforce, we show that we can leverage the enclave setup to achieve it. In particular, we use microarchitectural isolation to restrict attacker's observations in conjunction with straightforward hardware mechanisms to limit speculation. This new design point presents a compelling trade-off between security, usability, and performance, making it possible to efficiently enforce RMI for any program. We demonstrate our approach by implementing and evaluating two simple defense mechanisms that satisfy RMI: (1) Safe mode, which disables speculative accesses to shared memory, and (2) Burst mode, a localized performance optimization that requires simple program analysis on small code snippets. Our end-to-end prototype, Citadel, consists of an FPGA-based multicore processor that boots Linux and runs secure applications, including cryptographic libraries and private inference, with less than 5% performance overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14882v5</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jules Drean, Miguel Gomez-Garcia, Fisher Jepsen, Thomas Bourgeat, Srinivas Devadas</dc:creator>
    </item>
  </channel>
</rss>
