<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 May 2024 04:01:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Regime Identification for Improving Causal Analysis in Non-stationary Timeseries</title>
      <link>https://arxiv.org/abs/2405.02315</link>
      <description>arXiv:2405.02315v1 Announce Type: new 
Abstract: Time series data from real-world systems often display non-stationary behavior, indicating varying statistical characteristics over time. This inherent variability poses significant challenges in deciphering the underlying structural relationships within the data, particularly in correlation and causality analyses, model stability, etc. Recognizing distinct segments or regimes within multivariate time series data, characterized by relatively stable behavior and consistent statistical properties over extended periods, becomes crucial. In this study, we apply the regime identification (RegID) technique to multivariate time series, fundamentally designed to unveil locally stationary segments within data. The distinguishing features between regimes are identified using covariance matrices in a Riemannian space. We aim to highlight how regime identification contributes to improving the discovery of causal structures from multivariate non-stationary time series data. Our experiments, encompassing both synthetic and real-world datasets, highlight the effectiveness of regime-wise time series causal analysis. We validate our approach by first demonstrating improved causal structure discovery using synthetic data where the ground truth causal relationships are known. Subsequently, we apply this methodology to climate-ecosystem dataset, showcasing its applicability in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02315v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wasim Ahmad, Maha Shadaydeh, Joachim Denzler</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Estimating Heat Sources through Temperature Assimilation</title>
      <link>https://arxiv.org/abs/2405.02319</link>
      <description>arXiv:2405.02319v1 Announce Type: new 
Abstract: This paper introduces a Bayesian inference framework for two-dimensional steady-state heat conduction, focusing on the estimation of unknown distributed heat sources in a thermally-conducting medium with uniform conductivity. The goal is to infer heater locations, strengths, and shapes using temperature assimilation in the Euclidean space, employing a Fourier series to represent each heater's shape. The Markov Chain Monte Carlo (MCMC) method, incorporating the random-walk Metropolis-Hasting algorithm and parallel tempering, is utilized for posterior distribution exploration in both unbounded and wall-bounded domains. Strong correlations between heat strength and heater area prompt caution against simultaneously estimating these two quantities. It is found that multiple solutions arise in cases where the number of temperature sensors is less than the number of unknown states. Moreover, smaller heaters introduce greater uncertainty in estimated strength. The diffusive nature of heat conduction smooths out any deformations in the temperature contours, especially in the presence of multiple heaters positioned near each other, impacting convergence. In wall-bounded domains with Neumann boundary conditions, the inference of heater parameters tends to be more accurate than in unbounded domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02319v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanieh Mousavi, Jeff D. Eldredge</dc:creator>
    </item>
    <item>
      <title>Towards Causal Interpretation of Sexual Orientation in Regression Analysis: Applications and Challenges</title>
      <link>https://arxiv.org/abs/2405.02322</link>
      <description>arXiv:2405.02322v1 Announce Type: new 
Abstract: This study presents an approach to analyze health disparities in Sexual and Gender Minority (SGM) populations, with a focus on the role of social support levels as an example to allow causal interpretations of regression models. We advocate for precisely defining the exposure variable and incorporating mediators into analyses, to address the limitations of comparing counterfactual outcomes solely between SGM and heterosexual populations. We define sexual orientation into domains (attraction, behavior, and identity), and emphasize a consideration of these elements either separately or together, depending on the research question. We also introduce social support measured before and after the disclosure of sexual orientation to facilitate inference. We illustrate this approach by examining the association between SGM status and depression diagnosis with data from the 2020 and 2021 National Health Interview Survey. We find a direct effect of SGM status on depression (OR: 3.07, 95% CI: 2.64 - 3.58) and no indirect effect through social support (OR: 1.07, 95% CI: 0.87-1.31). Our research emphasizes the necessity of the comprehensive measurement of sexual orientation and a focus on intervenable variables like social support in order to empower SGM communities and address SGM related health inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02322v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junjie Lu, Zhongyi Guo, David H. Rehkopf</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Approach to Carbon Dioxide Emission Analysis in High Human Development Index Countries using Statistical and Machine Learning Techniques</title>
      <link>https://arxiv.org/abs/2405.02340</link>
      <description>arXiv:2405.02340v1 Announce Type: new 
Abstract: Reducing Carbon dioxide (CO2) emission is vital at both global and national levels, given their significant role in exacerbating climate change. CO2 emission, stemming from a variety of industrial and economic activities, are major contributors to the greenhouse effect and global warming, posing substantial obstacles in addressing climate issues. It's imperative to forecast CO2 emission trends and classify countries based on their emission patterns to effectively mitigate worldwide carbon emission. This paper presents an in-depth comparative study on the determinants of CO2 emission in twenty countries with high Human Development Index (HDI), exploring factors related to economy, environment, energy use, and renewable resources over a span of 25 years. The study unfolds in two distinct phases: initially, statistical techniques such as Ordinary Least Squares (OLS), fixed effects, and random effects models are applied to pinpoint significant determinants of CO2 emission. Following this, the study leverages supervised and unsupervised machine learning (ML) methods to further scrutinize and understand the factors influencing CO2 emission. Seasonal AutoRegressive Integrated Moving Average with eXogenous variables (SARIMAX), a supervised ML model, is first used to predict emission trends from historical data, offering practical insights for policy formulation. Subsequently, Dynamic Time Warping (DTW), an unsupervised learning approach, is used to group countries by similar emission patterns. The dual-phase approach utilized in this study significantly improves the accuracy of CO2 emission predictions while also providing a deeper insight into global emission trends. By adopting this thorough analytical framework, nations can develop more focused and effective carbon reduction policies, playing a vital role in the global initiative to combat climate change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02340v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hamed Khosravi, Ahmed Shoyeb Raihan, Farzana Islam, Ashish Nimbarte, Imtiaz Ahmed</dc:creator>
    </item>
    <item>
      <title>Adolescent sports participation and health in early adulthood: An observational study</title>
      <link>https://arxiv.org/abs/2405.03538</link>
      <description>arXiv:2405.03538v1 Announce Type: new 
Abstract: We study the impact of teenage sports participation on early-adulthood health using longitudinal data from the National Study of Youth and Religion. We focus on two primary outcomes measured at ages 23--28 -- self-rated health and total score on the PHQ9 Patient Depression Questionnaire -- and control for several potential confounders related to demographics and family socioeconomic status. To probe the possibility that certain types of sports participation may have larger effects on health than others, we conduct a matched observational study at each level within a hierarchy of exposures. Our hierarchy ranges from broadly defined exposures (e.g., participation in any organized after-school activity) to narrow (e.g., participation in collision sports). We deployed an ordered testing approach that exploits the hierarchical relationships between our exposure definitions to perform our analyses while maintaining a fixed family-wise error rate. Compared to teenagers who did not participate in any after-school activities, those who participated in sports had statistically significantly better self-rated and mental health outcomes in early adulthood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03538v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajinkya H. Kokandakar, Yuzhou Lin, Steven Jin, Jordan Weiss, Amanda R. Rabinowitz, Reuben A. Buford May, Dylan Small, Sameer K. Deshpande</dc:creator>
    </item>
    <item>
      <title>Some Statistical and Data Challenges When Building Early-Stage Digital Experimentation and Measurement Capabilities</title>
      <link>https://arxiv.org/abs/2405.03579</link>
      <description>arXiv:2405.03579v1 Announce Type: new 
Abstract: Digital experimentation and measurement (DEM) capabilities -- the knowledge and tools necessary to run experiments with digital products, services, or experiences and measure their impact -- are fast becoming part of the standard toolkit of digital/data-driven organisations in guiding business decisions. Many large technology companies report having mature DEM capabilities, and several businesses have been established purely to manage experiments for others. Given the growing evidence that data-driven organisations tend to outperform their non-data-driven counterparts, there has never been a greater need for organisations to build/acquire DEM capabilities to thrive in the current digital era.
  This thesis presents several novel approaches to statistical and data challenges for organisations building DEM capabilities. We focus on the fundamentals associated with building DEM capabilities, which lead to a richer understanding of the underlying assumptions and thus enable us to develop more appropriate capabilities. We address why one should engage in DEM by quantifying the benefits and risks of acquiring DEM capabilities. This is done using a ranking under lower uncertainty model, enabling one to construct a business case. We also examine what ingredients are necessary to run digital experiments. In addition to clarifying the existing literature around statistical tests, datasets, and methods in experimental design and causal inference, we construct an additional dataset and detailed case studies on applying state-of-the-art methods. Finally, we investigate when a digital experiment design would outperform another, leading to an evaluation framework that compares competing designs' data efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03579v1</guid>
      <category>stat.AP</category>
      <category>cs.DB</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.25560/110307</arxiv:DOI>
      <dc:creator>C. H. Bryan Liu</dc:creator>
    </item>
    <item>
      <title>Rejoinder on "Marked spatial point processes: current state and extensions to point processes on linear networks"</title>
      <link>https://arxiv.org/abs/2405.02343</link>
      <description>arXiv:2405.02343v1 Announce Type: cross 
Abstract: We are grateful to all discussants for their invaluable comments, suggestions, questions, and contributions to our article. We have attentively reviewed all discussions with keen interest. In this rejoinder, our objective is to address and engage with all points raised by the discussants in a comprehensive and considerate manner. Consistently, we identify the discussants, in alphabetical order, as follows: CJK for Cronie, Jansson, and Konstantinou, DS for Stoyan, GP for Grabarnik and Pommerening, MRS for Myllym\"aki, Rajala, and S\"arkk\"a, and MCvL for van Lieshout throughout this rejoinder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02343v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s13253-024-00613-1</arxiv:DOI>
      <arxiv:journal_reference>Eckardt, M. and Moradi, M. (2024). Rejoinder on `Marked Spatial Point Processes: Current State and Extensions to Point Processes on Linear Networks. Journal of Agricultural, Biological and Environmental Statistics</arxiv:journal_reference>
      <dc:creator>Matthias Eckardt, Mehdi Moradi</dc:creator>
    </item>
    <item>
      <title>Power-Enhanced Two-Sample Mean Tests for High-Dimensional Compositional Data with Application to Microbiome Data Analysis</title>
      <link>https://arxiv.org/abs/2405.02551</link>
      <description>arXiv:2405.02551v1 Announce Type: cross 
Abstract: Testing differences in mean vectors is a fundamental task in the analysis of high-dimensional compositional data. Existing methods may suffer from low power if the underlying signal pattern is in a situation that does not favor the deployed test. In this work, we develop two-sample power-enhanced mean tests for high-dimensional compositional data based on the combination of $p$-values, which integrates strengths from two popular types of tests: the maximum-type test and the quadratic-type test. We provide rigorous theoretical guarantees on the proposed tests, showing accurate Type-I error rate control and enhanced testing power. Our method boosts the testing power towards a broader alternative space, which yields robust performance across a wide range of signal pattern settings. Our theory also contributes to the literature on power enhancement and Gaussian approximation for high-dimensional hypothesis testing. We demonstrate the performance of our method on both simulated data and real-world microbiome data, showing that our proposed approach improves the testing power substantially compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02551v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danning Li, Lingzhou Xue, Haoyi Yang, Xiufan Yu</dc:creator>
    </item>
    <item>
      <title>CVXSADes: a stochastic algorithm for constructing optimal exact regression designs with single or multiple objectives</title>
      <link>https://arxiv.org/abs/2405.02983</link>
      <description>arXiv:2405.02983v1 Announce Type: cross 
Abstract: We propose an algorithm to construct optimal exact designs (EDs). Most of the work in the optimal regression design literature focuses on the approximate design (AD) paradigm due to its desired properties, including the optimality verification conditions derived by Kiefer (1959, 1974). ADs may have unbalanced weights, and practitioners may have difficulty implementing them with a designated run size $n$. Some EDs are constructed using rounding methods to get an integer number of runs at each support point of an AD, but this approach may not yield optimal results. To construct EDs, one may need to perform new combinatorial constructions for each $n$, and there is no unified approach to construct them. Therefore, we develop a systematic way to construct EDs for any given $n$. Our method can transform ADs into EDs while retaining high statistical efficiency in two steps. The first step involves constructing an AD by utilizing the convex nature of many design criteria. The second step employs a simulated annealing algorithm to search for the ED stochastically. Through several applications, we demonstrate the utility of our method for various design problems. Additionally, we show that the design efficiency approaches unity as the number of design points increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02983v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi-Kuang Yeh, Julie Zhou</dc:creator>
    </item>
    <item>
      <title>Functional Post-Clustering Selective Inference with Applications to EHR Data Analysis</title>
      <link>https://arxiv.org/abs/2405.03042</link>
      <description>arXiv:2405.03042v1 Announce Type: cross 
Abstract: In electronic health records (EHR) analysis, clustering patients according to patterns in their data is crucial for uncovering new subtypes of diseases. Existing medical literature often relies on classical hypothesis testing methods to test for differences in means between these clusters. Due to selection bias induced by clustering algorithms, the implementation of these classical methods on post-clustering data often leads to an inflated type-I error. In this paper, we introduce a new statistical approach that adjusts for this bias when analyzing data collected over time. Our method extends classical selective inference methods for cross-sectional data to longitudinal data. We provide theoretical guarantees for our approach with upper bounds on the selective type-I and type-II errors. We apply the method to simulated data and real-world Acute Kidney Injury (AKI) EHR datasets, thereby illustrating the advantages of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03042v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zihan Zhu, Xin Gai, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Statistical Edge Detection And UDF Learning For Shape Representation</title>
      <link>https://arxiv.org/abs/2405.03381</link>
      <description>arXiv:2405.03381v1 Announce Type: cross 
Abstract: In the field of computer vision, the numerical encoding of 3D surfaces is crucial. It is classical to represent surfaces with their Signed Distance Functions (SDFs) or Unsigned Distance Functions (UDFs). For tasks like representation learning, surface classification, or surface reconstruction, this function can be learned by a neural network, called Neural Distance Function. This network, and in particular its weights, may serve as a parametric and implicit representation for the surface. The network must represent the surface as accurately as possible. In this paper, we propose a method for learning UDFs that improves the fidelity of the obtained Neural UDF to the original 3D surface. The key idea of our method is to concentrate the learning effort of the Neural UDF on surface edges.  More precisely, we show that sampling more training points around surface edges allows better local accuracy of the trained Neural UDF, and thus improves the global expressiveness of the Neural UDF in terms of Hausdorff distance. To detect surface edges, we propose a new statistical method based on the calculation of a $p$-value at each point on the surface. Our method is shown to detect surface edges more accurately than a commonly used local geometric descriptor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03381v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Virgile Foy (IMT), Fabrice Gamboa (IMT), Reda Chhaibi (IMT)</dc:creator>
    </item>
    <item>
      <title>Distributional Reference Class Forecasting of Corporate Sales Growth With Multiple Reference Variables</title>
      <link>https://arxiv.org/abs/2405.03402</link>
      <description>arXiv:2405.03402v1 Announce Type: cross 
Abstract: This paper introduces an approach to reference class selection in distributional forecasting with an application to corporate sales growth rates using several co-variates as reference variables, that are implicit predictors. The method can be used to detect expert or model-based forecasts exposed to (behavioral) bias or to forecast distributions with reference classes. These are sets of similar entities, here firms, and rank based algorithms for their selection are proposed, including an optional preprocessing data dimension reduction via principal components analysis. Forecasts are optimal if they match the underlying distribution as closely as possible. Probability integral transform values rank the forecast capability of different reference variable sets and algorithms in a backtest on a data set of 21,808 US firms over the time period 1950 - 2019. In particular, algorithms on dimension reduced variables perform well using contemporaneous balance sheet and financial market parameters along with past sales growth rates and past operating margins changes. Comparisions of actual analysts' estimates to distributional forecasts and of historic distributional forecasts to realized sales growth illustrate the practical use of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03402v1</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Etienne Theising</dc:creator>
    </item>
    <item>
      <title>Copas-Heckman-type sensitivity analysis for publication bias in rare-event meta-analysis under the framework of the generalized linear mixed model</title>
      <link>https://arxiv.org/abs/2405.03603</link>
      <description>arXiv:2405.03603v1 Announce Type: cross 
Abstract: Publication bias (PB) is one of the serious issues in meta-analysis. Many existing methods dealing with PB are based on the normal-normal (NN) random-effects model assuming normal models in both the within-study and the between-study levels. For rare-event meta-analysis where the data contain rare occurrences of event, the standard NN random-effects model may perform poorly. Instead, the generalized linear mixed effects model (GLMM) using the exact within-study model is recommended. However, no method has been proposed for dealing with PB in rare-event meta-analysis using the GLMM. In this paper, we propose sensitivity analysis methods for evaluating the impact of PB on the GLMM based on the famous Copas-Heckman-type selection model. The proposed methods can be easily implemented with the standard software coring the nonlinear mixed-effects model. We use a real-world example to show how the usefulness of the proposed methods in evaluating the potential impact of PB in meta-analysis of the log-transformed odds ratio based on the GLMM using the non-central hypergeometric or binomial distribution as the within-study model. An extension of the proposed method is also introduced for evaluating PB in meta-analysis of proportion based on the GLMM with the binomial within-study model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03603v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Zhou, Taojun Hu, Xiao-Hua Zhou, Satoshi Hattori</dc:creator>
    </item>
    <item>
      <title>Soft Phenotyping for Sepsis via EHR Time-aware Soft Clustering</title>
      <link>https://arxiv.org/abs/2311.08629</link>
      <description>arXiv:2311.08629v2 Announce Type: replace 
Abstract: Objective: Sepsis is one of the most serious hospital conditions associated with high mortality. Sepsis is the result of a dysregulated immune response to infection that can lead to multiple organ dysfunction and death. Due to the wide variability in the causes of sepsis, clinical presentation, and the recovery trajectories, identifying sepsis sub-phenotypes is crucial to advance our understanding of sepsis characterization, to choose targeted treatments and optimal timing of interventions, and to improve prognostication. Prior studies have described different sub-phenotypes of sepsis using organ-specific characteristics. These studies applied clustering algorithms to electronic health records (EHRs) to identify disease sub-phenotypes. However, prior approaches did not capture temporal information and made uncertain assumptions about the relationships among the sub-phenotypes for clustering procedures.
  Methods: We developed a time-aware soft clustering algorithm guided by clinical variables to identify sepsis sub-phenotypes using data available in the EHR.
  Results: We identified six novel sepsis hybrid sub-phenotypes and evaluated them for medical plausibility. In addition, we built an early-warning sepsis prediction model using logistic regression.
  Conclusion: Our results suggest that these novel sepsis hybrid sub-phenotypes are promising to provide more accurate information on sepsis-related organ dysfunction and sepsis recovery trajectories which can be important to inform management decisions and sepsis prognosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08629v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shiyi Jiang, Xin Gai, Miriam Treggiari, William W. Stead, Yuankang Zhao, C. David Page, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Square-Root Higher-Order Unscented Estimators for Robust Orbit Determination</title>
      <link>https://arxiv.org/abs/2311.10452</link>
      <description>arXiv:2311.10452v2 Announce Type: replace 
Abstract: Orbit determination (OD) is a fundamental problem in space surveillance and tracking, crucial for ensuring the safety of space assets. Real-world ground-based optical tracking scenarios often involve challenges such as limited measurement time, short visible arcs, and the presence of outliers, leading to sparse and non-Gaussian observational data. Additionally, the highly perturbative and nonlinear orbit dynamics of resident space objects (RSOs) in low Earth orbit (LEO) add further complexity to the OD problem.
  This paper introduces a novel variant of the higher-order unscented Kalman estimator (HOUSE) called $w$-HOUSE, which employs a square-root formulation and addresses the challenges posed by nonlinear and non-Gaussian OD problems. The effectiveness of $w$-HOUSE was demonstrated through synthetic and real-world measurements, specifically outlier-contaminated angle-only measurements collected for the Sentinel 6A satellite flying in LEO. Comparative analyses are conducted with the original HOUSE (referred to as $\delta$-HOUSE), unscented Kalman filters (UKF), conjugate unscented transformation (CUT) filters, and precise orbit determination solutions estimated via onboard global navigation satellite systems measurements.
  The results reveal that the proposed $w$-HOUSE filter exhibits greater robustness when dealing with varying values of the dependent parameter compared to the original $\delta$-HOUSE. Moreover, it surpasses all other filters in terms of positioning accuracy, achieving three-dimensional root-mean-square errors of less than 60 m in a three-day scenario. This research suggests that the new $w$-HOUSE filter represents a viable alternative to UKF and CUT filters, offering improved positioning performance in handling the nonlinear and non-Gaussian OD problem associated with LEO RSOs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10452v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Yang</dc:creator>
    </item>
    <item>
      <title>The SIDO Performance Model for League of Legends</title>
      <link>https://arxiv.org/abs/2403.04873</link>
      <description>arXiv:2403.04873v2 Announce Type: replace 
Abstract: League of Legends (LoL) has been a dominant esport for a decade, yet the inherent complexity of the game has stymied the creation of analytical measures of player skill and performance. Current industry standards are limited to easy-to-procure individual player statistics that are incomplete and lacking context as they do not take into account teamplay or game state. We present a unified performance model for League of Legends which blends together measures of a player's contribution within the context of their team, insights from traditional sports metrics such as the Plus-Minus model, and the intricacies of LoL as a complex team invasion sport. Using hierarchical Bayesian models, we outline the use of gold and damage dealt as a measure of skill, detailing players' impact on their own-, their allies'- and their enemies' statistics throughout the course of the game. Our results showcase the model's increased efficacy in separating professional players when compared to a Plus-Minus model and to current esports industry standards, while metric quality is rigorously assessed for discrimination, independence, and stability. Readers might also find additional qualitative analytics which explore champion proficiency and the impact of collaborative team-play. Future work is proposed to refine and expand the SIDO performance model, offering a comprehensive framework for esports analytics in team performance management, scouting and research realms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04873v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amy X. Zhang, Parth Naidu</dc:creator>
    </item>
    <item>
      <title>Percentage Coefficient (bp) -- Effect Size Analysis (Theory Paper 1)</title>
      <link>https://arxiv.org/abs/2404.19495</link>
      <description>arXiv:2404.19495v2 Announce Type: replace 
Abstract: Percentage coefficient (bp) has emerged in recent publications as an additional and alternative estimator of effect size for regression analysis. This paper retraces the theory behind the estimator. It's posited that an estimator must first serve the fundamental function of enabling researchers and readers to comprehend an estimand, the target of estimation. It may then serve the instrumental function of enabling researchers and readers to compare two or more estimands. Defined as the regression coefficient when dependent variable (DV) and independent variable (IV) are both on conceptual 0-1 percentage scales, percentage coefficients (bp) feature 1) clearly comprehendible interpretation and 2) equitable scales for comparison. The coefficient (bp) serves the two functions effectively and efficiently. It thus serves needs unserved by other indicators, such as raw coefficient (bw) and standardized beta.
  Another premise of the functionalist theory is that "effect" is not a monolithic concept. Rather, it is a collection of concepts, each of which measures a component of the conglomerate called "effect", thereby serving a subfunction. Regression coefficient (b), for example, indicates the unit change in DV associated with a one-unit increase in IV, thereby measuring one aspect called unit effect, aka efficiency. Percentage coefficient (bp) indicates the percentage change in DV associated with a whole scale increase in IV. It is not meant to be an all-encompassing indicator of an all-encompassing concept, but rather a comprehendible and comparable indicator of efficiency, a key aspect of effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19495v2</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinshu Zhao (Department of Communication, Faculty of Social Science, University of Macau), Dianshi Moses Li (Centre for Empirical Legal Studies, Faculty of Law, University of Macau), Ze Zack Lai (Department of Communication, Faculty of Social Science, University of Macau), Piper Liping Liu (School of Media and Communication, Shenzhen University), Song Harris Ao (Department of Communication, Faculty of Social Science, University of Macau), Fei You (Department of Communication, Faculty of Social Science, University of Macau)</dc:creator>
    </item>
    <item>
      <title>A comparison of regression models for static and dynamic prediction of a prognostic outcome during admission in electronic health care records</title>
      <link>https://arxiv.org/abs/2405.01986</link>
      <description>arXiv:2405.01986v2 Announce Type: replace 
Abstract: Objective Hospitals register information in the electronic health records (EHR) continuously until discharge or death. As such, there is no censoring for in-hospital outcomes. We aimed to compare different dynamic regression modeling approaches to predict central line-associated bloodstream infections (CLABSI) in EHR while accounting for competing events precluding CLABSI. Materials and Methods We analyzed data from 30,862 catheter episodes at University Hospitals Leuven from 2012 and 2013 to predict 7-day risk of CLABSI. Competing events are discharge and death. Static models at catheter onset included logistic, multinomial logistic, Cox, cause-specific hazard, and Fine-Gray regression. Dynamic models updated predictions daily up to 30 days after catheter onset (i.e. landmarks 0 to 30 days), and included landmark supermodel extensions of the static models, separate Fine-Gray models per landmark time, and regularized multi-task learning (RMTL). Model performance was assessed using 100 random 2:1 train-test splits. Results The Cox model performed worst of all static models in terms of area under the receiver operating characteristic curve (AUC) and calibration. Dynamic landmark supermodels reached peak AUCs between 0.741-0.747 at landmark 5. The Cox landmark supermodel had the worst AUCs (&lt;=0.731) and calibration up to landmark 7. Separate Fine-Gray models per landmark performed worst for later landmarks, when the number of patients at risk was low. Discussion and Conclusion Categorical and time-to-event approaches had similar performance in the static and dynamic settings, except Cox models. Ignoring competing risks caused problems for risk prediction in the time-to-event framework (Cox), but not in the categorical framework (logistic regression).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01986v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shan Gao, Elena Albu, Hein Putter, Pieter Stijnen, Frank Rademakers, Veerle Cossey, Yves Debaveye, Christel Janssens, Ben Van Calster, Laure Wynants</dc:creator>
    </item>
    <item>
      <title>H\"older regularity and roughness: construction and examples</title>
      <link>https://arxiv.org/abs/2304.13794</link>
      <description>arXiv:2304.13794v3 Announce Type: replace-cross 
Abstract: We study how to construct a stochastic process on a finite interval with given `roughness' and finite joint moments of marginal distributions. We first extend Ciesielski's isomorphism along a general sequence of partitions, and provide a characterization of H\"older regularity of a function in terms of its Schauder coefficients. Using this characterization we provide a better (pathwise) estimator of H\"older exponent. As an additional application, we construct fake (fractional) Brownian motions with some path properties and finite moments of marginal distributions same as (fractional) Brownian motions. These belong to non-Gaussian families of stochastic processes which are statistically difficult to distinguish from real (fractional) Brownian motions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.13794v3</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erhan Bayraktar, Purba Das, Donghan Kim</dc:creator>
    </item>
    <item>
      <title>An algorithm for forensic toolmark comparisons</title>
      <link>https://arxiv.org/abs/2312.00032</link>
      <description>arXiv:2312.00032v2 Announce Type: replace-cross 
Abstract: Forensic toolmark analysis traditionally relies on subjective human judgment, leading to inconsistencies and inaccuracies. The multitude of variables, including angles and directions of mark generation, further complicates comparisons. To address this, we introduce a novel approach leveraging 3D data capturing toolmarks from various angles and directions. Through algorithmic training, we objectively compare toolmark signals, revealing clustering by tool rather than angle or direction. Our method utilizes similarity matrices and density plots to establish thresholds for classification, enabling the derivation of likelihood ratios for new mark pairs. With a cross-validated sensitivity of 98% and specificity of 96%, our approach enhances the reliability of toolmark analysis. While its applicability to diverse tools and factors warrants further exploration, this empirically trained, open-source solution offers forensic examiners a standardized means to objectively compare toolmarks, potentially curbing miscarriages of justice in the legal system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00032v2</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Cuellar, Sheng Gao, Heike Hofmann</dc:creator>
    </item>
    <item>
      <title>Detecting algorithmic bias in medical-AI models using trees</title>
      <link>https://arxiv.org/abs/2312.02959</link>
      <description>arXiv:2312.02959v5 Announce Type: replace-cross 
Abstract: With the growing prevalence of machine learning and artificial intelligence-based medical decision support systems, it is equally important to ensure that these systems provide patient outcomes in a fair and equitable fashion. This paper presents an innovative framework for detecting areas of algorithmic bias in medical-AI decision support systems. Our approach efficiently identifies potential biases in medical-AI models, specifically in the context of sepsis prediction, by employing the Classification and Regression Trees (CART) algorithm. We verify our methodology by conducting a series of synthetic data experiments, showcasing its ability to estimate areas of bias in controlled settings precisely. The effectiveness of the concept is further validated by experiments using electronic medical records from Grady Memorial Hospital in Atlanta, Georgia. These tests demonstrate the practical implementation of our strategy in a clinical environment, where it can function as a vital instrument for guaranteeing fairness and equity in AI-based medical decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02959v5</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeffrey Smith, Andre Holder, Rishikesan Kamaleswaran, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Efficient Weighting Schemes for Auditing Instant-Runoff Voting Elections</title>
      <link>https://arxiv.org/abs/2403.15400</link>
      <description>arXiv:2403.15400v2 Announce Type: replace-cross 
Abstract: Various risk-limiting audit (RLA) methods have been developed for instant-runoff voting (IRV) elections. A recent method, AWAIRE, is the first efficient approach that can take advantage of but does not require cast vote records (CVRs). AWAIRE involves adaptively weighted averages of test statistics, essentially "learning" an effective set of hypotheses to test. However, the initial paper on AWAIRE only examined a few weighting schemes and parameter settings.
  We explore schemes and settings more extensively, to identify and recommend efficient choices for practice. We focus on the case where CVRs are not available, assessing performance using simulations based on real election data.
  The most effective schemes are often those that place most or all of the weight on the apparent "best" hypotheses based on already seen data. Conversely, the optimal tuning parameters tended to vary based on the election margin. Nonetheless, we quantify the performance trade-offs for different choices across varying election margins, aiding in selecting the most desirable trade-off if a default option is needed.
  A limitation of the current AWAIRE implementation is its restriction to a small number of candidates -- up to six in previous implementations. One path to a more computationally efficient implementation would be to use lazy evaluation and avoid considering all possible hypotheses. Our findings suggest that such an approach could be done without substantially compromising statistical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15400v2</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Ek, Philip B. Stark, Peter J. Stuckey, Damjan Vukcevic</dc:creator>
    </item>
  </channel>
</rss>
