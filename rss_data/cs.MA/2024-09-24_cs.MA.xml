<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MA</link>
    <description>cs.MA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Sep 2024 01:58:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Synergistic Simulations: Multi-Agent Problem Solving with Large Language Models</title>
      <link>https://arxiv.org/abs/2409.13753</link>
      <description>arXiv:2409.13753v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have increasingly demonstrated the ability to facilitate the development of multi-agent systems that allow the interpretation of thoughts and actions generated by each individual. Promising advancements have also been made in LLM-based interaction with existing worlds, particularly in interacting with simulated environments. This paper aims to integrate both aforementioned topics (agents &amp; world interaction) into a single simulation where multiple agents can work together to solve a problem, modeling how groups of humans can often solve problems better than individuals. By showing whether LLMs demonstrate the synergy of human collaboration, it could lead to advancements in the applications of LLMs. We implemented two simulations: a physical studio apartment with two roommates, and another where agents collaborate to complete a programming task. We provide a multi-agent framework, discuss the performance of the agents in each simulation, and discuss potential future additions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13753v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asher Sprigler, Alexander Drobek, Keagan Weinstock, Wendpanga Tapsoba, Gavin Childress, Andy Dao, Lucas Gral</dc:creator>
    </item>
    <item>
      <title>A Value Based Parallel Update MCTS Method for Multi-Agent Cooperative Decision Making of Connected and Automated Vehicles</title>
      <link>https://arxiv.org/abs/2409.13783</link>
      <description>arXiv:2409.13783v1 Announce Type: new 
Abstract: To solve the problem of lateral and logitudinal joint decision-making of multi-vehicle cooperative driving for connected and automated vehicles (CAVs), this paper proposes a Monte Carlo tree search (MCTS) method with parallel update for multi-agent Markov game with limited horizon and time discounted setting. By analyzing the parallel actions in the multi-vehicle joint action space in the partial-steady-state traffic flow, the parallel update method can quickly exclude potential dangerous actions, thereby increasing the search depth without sacrificing the search breadth. The proposed method is tested in a large number of randomly generated traffic flow. The experiment results show that the algorithm has good robustness and better performance than the SOTA reinforcement learning algorithms and heuristic methods. The vehicle driving strategy using the proposed algorithm shows rationality beyond human drivers, and has advantages in traffic efficiency and safety in the coordinating zone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13783v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Han, Lijun Zhang, Dejian Meng, Xingyu Hu, Songyu Weng</dc:creator>
    </item>
    <item>
      <title>A novel load distribution strategy for aggregators using IoT-enabled mobile devices</title>
      <link>https://arxiv.org/abs/2409.14293</link>
      <description>arXiv:2409.14293v1 Announce Type: new 
Abstract: The rapid proliferation of Internet-of-things (IoT) as well as mobile devices such as Electric Vehicles (EVs), has led to unpredictable load at the grid. The demand to supply ratio is particularly exacerbated at a few grid aggregators (charging stations) with excessive demand due to the geographic location, peak time, etc. Existing solutions on demand response cannot achieve significant improvements based only on time-shifting the loads without considering the device properties such as charging modes and movement capabilities to enable geographic migration. Additionally, the information on the spare capacity at a few aggregators can aid in re-channeling the load from other aggregators facing excess demand to allow migration of devices. In this paper, we model these flexible properties of the devices as a mixed-integer non-linear problem (MINLP) to minimize excess load and the improve the utility (benefit) across all devices. We propose an online distributed low-complexity heuristic that prioritizes devices based on demand and deadlines to minimize the cumulative loss in utility. The proposed heuristic is tested on an exhaustive set of synthetic data and compared with solutions from a solver/optimization tool for the same runtime to show the impracticality of using a solver. A real-world EV testbed data is also tested with our proposed solution and other scheduling solutions to show the practicality of generating a feasible schedule and a loss improvement of at least 57.23%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14293v1</guid>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>10.1109/SmartGridComm51999.2021.9632317</arxiv:journal_reference>
      <dc:creator>Nitin Shivaraman, Jakob Fittler, Saravanan Ramanathan, Arvind Easwaran, Sebastian Steinhorst</dc:creator>
    </item>
    <item>
      <title>Work Smarter Not Harder: Simple Imitation Learning with CS-PIBT Outperforms Large Scale Imitation Learning for MAPF</title>
      <link>https://arxiv.org/abs/2409.14491</link>
      <description>arXiv:2409.14491v1 Announce Type: new 
Abstract: Multi-Agent Path Finding (MAPF) is the problem of effectively finding efficient collision-free paths for a group of agents in a shared workspace. The MAPF community has largely focused on developing high-performance heuristic search methods. Recently, several works have applied various machine learning (ML) techniques to solve MAPF, usually involving sophisticated architectures, reinforcement learning techniques, and set-ups, but none using large amounts of high-quality supervised data. Our initial objective in this work was to show how simple large scale imitation learning of high-quality heuristic search methods can lead to state-of-the-art ML MAPF performance. However, we find that, at least with our model architecture, simple large scale (700k examples with hundreds of agents per example) imitation learning does \textit{not} produce impressive results. Instead, we find that by using prior work that post-processes MAPF model predictions to resolve 1-step collisions (CS-PIBT), we can train a simple ML MAPF model in minutes that dramatically outperforms existing ML MAPF policies. This has serious implications for all future ML MAPF policies (with local communication) which currently struggle to scale. In particular, this finding implies that future learnt policies should (1) always use smart 1-step collision shields (e.g. CS-PIBT), (2) always include the collision shield with greedy actions as a baseline (e.g. PIBT) and (3) motivates future models to focus on longer horizon / more complex planning as 1-step collisions can be efficiently resolved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14491v1</guid>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishi Veerapaneni, Arthur Jakobsson, Kevin Ren, Samuel Kim, Jiaoyang Li, Maxim Likhachev</dc:creator>
    </item>
    <item>
      <title>Increasing the Value of Information During Planning in Uncertain Environments</title>
      <link>https://arxiv.org/abs/2409.13754</link>
      <description>arXiv:2409.13754v1 Announce Type: cross 
Abstract: Prior studies have demonstrated that for many real-world problems, POMDPs can be solved through online algorithms both quickly and with near optimality. However, on an important set of problems where there is a large time delay between when the agent can gather information and when it needs to use that information, these solutions fail to adequately consider the value of information. As a result, information gathering actions, even when they are critical in the optimal policy, will be ignored by existing solutions, leading to sub-optimal decisions by the agent. In this research, we develop a novel solution that rectifies this problem by introducing a new algorithm that improves upon state-of-the-art online planning by better reflecting on the value of actions that gather information. We do this by adding Entropy to the UCB1 heuristic in the POMCP algorithm. We test this solution on the hallway problem. Results indicate that our new algorithm performs significantly better than POMCP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13754v1</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaurab Pokharel</dc:creator>
    </item>
    <item>
      <title>Adaptive bias for dissensus in nonlinear opinion dynamics with application to evolutionary division of labor games</title>
      <link>https://arxiv.org/abs/2409.13964</link>
      <description>arXiv:2409.13964v1 Announce Type: cross 
Abstract: This paper addresses the problem of adaptively controlling the bias parameter in nonlinear opinion dynamics (NOD) to allocate agents into groups of arbitrary sizes for the purpose of maximizing collective rewards. In previous work, an algorithm based on the coupling of NOD with an multi-objective behavior optimization was successfully deployed as part of a multi-robot system in an autonomous task allocation field experiment. Motivated by the field results, in this paper we propose and analyze a new task allocation model that synthesizes NOD with an evolutionary game framework. We prove sufficient conditions under which it is possible to control the opinion state in the group to a desired allocation of agents between two tasks through an adaptive bias using decentralized feedback. We then verify the theoretical results with a simulation study of a collaborative evolutionary division of labor game.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13964v1</guid>
      <category>eess.SY</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tyler M. Paine, Anastasia Bizyaeva, Michael R. Benjamin</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Inverse Reinforcement Learning for Identifying Multi-Agent Coordinated Sensing</title>
      <link>https://arxiv.org/abs/2409.14542</link>
      <description>arXiv:2409.14542v1 Announce Type: cross 
Abstract: We derive a minimax distributionally robust inverse reinforcement learning (IRL) algorithm to reconstruct the utility functions of a multi-agent sensing system. Specifically, we construct utility estimators which minimize the worst-case prediction error over a Wasserstein ambiguity set centered at noisy signal observations. We prove the equivalence between this robust estimation and a semi-infinite optimization reformulation, and we propose a consistent algorithm to compute solutions. We illustrate the efficacy of this robust IRL scheme in numerical studies to reconstruct the utility functions of a cognitive radar network from observed tracking signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14542v1</guid>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>eess.SP</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Snow, Vikram Krishnamurthy</dc:creator>
    </item>
    <item>
      <title>Cloud and IoT based Smart Agent-driven Simulation of Human Gait for Detecting Muscles Disorder</title>
      <link>https://arxiv.org/abs/2409.14561</link>
      <description>arXiv:2409.14561v1 Announce Type: cross 
Abstract: Motion disorders pose a significant global health concern and are often managed with pharmacological treatments that may lead to undesirable long-term effects. Current therapeutic strategies lack differentiation between healthy and unhealthy muscles in a patient, necessitating a targeted approach to distinguish between musculature. There is still no motion analyzer application for this purpose. Additionally, there is a deep gap in motion analysis software as some studies prioritize simulation, neglecting software needs, while others concentrate on computational aspects, disregarding simulation nuances. We introduce a comprehensive five-phase methodology to analyze the neuromuscular system of the lower body during gait. The first phase employs an innovative IoT-based method for motion signal capture. The second and third phases involve an agent-driven biomechanical model of the lower body skeleton and a model of human voluntary muscle. Thus, using an agent-driven approach, motion-captured signals can be converted to neural stimuli. The simulation results are then analyzed by our proposed ensemble neural network framework in the fourth step in order to detect abnormal motion in each joint. Finally, the results are shown by a userfriendly graphical interface which promotes the usability of the method. Utilizing the developed application, we simulate the neuromusculoskeletal system of some patients during the gait cycle, enabling the classification of healthy and pathological muscle activity through joint-based analysis. This study leverages cloud computing to create an infrastructure-independent application which is globally accessible. The proposed application enables experts to differentiate between healthy and unhealthy muscles in a patient by simulating his gait.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14561v1</guid>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sina Saadati, Mohammadreza Razzazi</dc:creator>
    </item>
    <item>
      <title>Combating Spatial Disorientation in a Dynamic Self-Stabilization Task Using AI Assistants</title>
      <link>https://arxiv.org/abs/2409.14565</link>
      <description>arXiv:2409.14565v1 Announce Type: cross 
Abstract: Spatial disorientation is a leading cause of fatal aircraft accidents. This paper explores the potential of AI agents to aid pilots in maintaining balance and preventing unrecoverable losses of control by offering cues and corrective measures that ameliorate spatial disorientation. A multi-axis rotation system (MARS) was used to gather data from human subjects self-balancing in a spaceflight analog condition. We trained models over this data to create "digital twins" that exemplified performance characteristics of humans with different proficiency levels. We then trained various reinforcement learning and deep learning models to offer corrective cues if loss of control is predicted. Digital twins and assistant models then co-performed a virtual inverted pendulum (VIP) programmed with identical physics. From these simulations, we picked the 5 best-performing assistants based on task metrics such as crash frequency and mean distance from the direction of balance. These were used in a co-performance study with 20 new human subjects performing a version of the VIP task with degraded spatial information. We show that certain AI assistants were able to improve human performance and that reinforcement-learning based assistants were objectively more effective but rated as less trusted and preferable by humans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14565v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3687272.3688329</arxiv:DOI>
      <dc:creator>Sheikh Mannan, Paige Hansen, Vivekanand Pandey Vimal, Hannah N. Davies, Paul DiZio, Nikhil Krishnaswamy</dc:creator>
    </item>
    <item>
      <title>Kinodynamic Motion Planning for Collaborative Object Transportation by Multiple Mobile Manipulators</title>
      <link>https://arxiv.org/abs/2409.14910</link>
      <description>arXiv:2409.14910v1 Announce Type: cross 
Abstract: This work proposes a kinodynamic motion planning technique for collaborative object transportation by multiple mobile manipulators in dynamic environments. A global path planner computes a linear piecewise path from start to goal. A novel algorithm detects the narrow regions between the static obstacles and aids in defining the obstacle-free region to enhance the feasibility of the global path. We then formulate a local online motion planning technique for trajectory generation that minimizes the control efforts in a receding horizon manner. It plans the trajectory for finite time horizons, considering the kinodynamic constraints and the static and dynamic obstacles. The planning technique jointly plans for the mobile bases and the arms to utilize the locomotion capability of the mobile base and the manipulation capability of the arm efficiently. We use a convex cone approach to avoid self-collision of the formation by modifying the mobile manipulators admissible state without imposing additional constraints. Numerical simulations and hardware experiments showcase the efficiency of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14910v1</guid>
      <category>cs.RO</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keshab Patra, Arpita Sinha, Anirban Guha</dc:creator>
    </item>
    <item>
      <title>Method of Equal Shares with Bounded Overspending</title>
      <link>https://arxiv.org/abs/2409.15005</link>
      <description>arXiv:2409.15005v1 Announce Type: cross 
Abstract: In participatory budgeting (PB), voters decide through voting which subset of projects to fund within a given budget. Proportionality in the context of PB is crucial to ensure equal treatment of all groups of voters. However, pure proportional rules can sometimes lead to suboptimal outcomes. We introduce the Method of Equal Shares with Bounded Overspending (BOS Equal Shares), a robust variant of Equal Shares that balances proportionality and efficiency. BOS Equal Shares addresses inefficiencies inherent in strict proportionality guarantees yet still provides good proportionality similar to the original Method of Equal Shares. In the course of the analysis, we also discuss a fractional variant of the method which allows for partial funding of projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15005v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgios Papasotiropoulos, Seyedeh Zeinab Pishbin, Oskar Skibski, Piotr Skowron, Tomasz W\k{a}s</dc:creator>
    </item>
    <item>
      <title>SPformer: A Transformer Based DRL Decision Making Method for Connected Automated Vehicles</title>
      <link>https://arxiv.org/abs/2409.15105</link>
      <description>arXiv:2409.15105v1 Announce Type: cross 
Abstract: In mixed autonomy traffic environment, every decision made by an autonomous-driving car may have a great impact on the transportation system. Because of the complex interaction between vehicles, it is challenging to make decisions that can ensure both high traffic efficiency and safety now and futher. Connected automated vehicles (CAVs) have great potential to improve the quality of decision-making in this continuous, highly dynamic and interactive environment because of their stronger sensing and communicating ability. For multi-vehicle collaborative decision-making algorithms based on deep reinforcement learning (DRL), we need to represent the interactions between vehicles to obtain interactive features. The representation in this aspect directly affects the learning efficiency and the quality of the learned policy. To this end, we propose a CAV decision-making architecture based on transformer and reinforcement learning algorithms. A learnable policy token is used as the learning medium of the multi-vehicle joint policy, the states of all vehicles in the area of interest can be adaptively noticed in order to extract interactive features among agents. We also design an intuitive physical positional encodings, the redundant location information of which optimizes the performance of the network. Simulations show that our model can make good use of all the state information of vehicles in traffic scenario, so as to obtain high-quality driving decisions that meet efficiency and safety objectives. The comparison shows that our method significantly improves existing DRL-based multi-vehicle cooperative decision-making algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15105v1</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Han, Lijun Zhang, Dejian Meng, Xingyu Hu, Yixia Lu</dc:creator>
    </item>
    <item>
      <title>Distributed Differentiable Dynamic Game for Multi-robot Coordination</title>
      <link>https://arxiv.org/abs/2207.08892</link>
      <description>arXiv:2207.08892v4 Announce Type: replace-cross 
Abstract: This paper develops a Distributed Differentiable Dynamic Game (D3G) framework, which can efficiently solve the forward and inverse problems in multi-robot coordination. We formulate multi-robot coordination as a dynamic game, where the behavior of a robot is dictated by its own dynamics and objective that also depends on others' behavior. In the forward problem, D3G enables all robots collaboratively to seek the Nash equilibrium of the game in a distributed manner, by developing a distributed shooting-based Nash solver. In the inverse problem, where each robot aims to find (learn) its objective (and dynamics) parameters to mimic given coordination demonstrations, D3G proposes a differentiation solver based on Differential Pontryagin's Maximum Principle, which allows each robot to update its parameters in a distributed and coordinated manner. We test the D3G in simulation with two types of robots given different task configurations. The results demonstrate the effectiveness of D3G for solving both forward and inverse problems in comparison with existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.08892v4</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizhi Zhou, Wanxin Jin, Xuan Wang</dc:creator>
    </item>
    <item>
      <title>CURO: Curriculum Learning for Relative Overgeneralization</title>
      <link>https://arxiv.org/abs/2212.02733</link>
      <description>arXiv:2212.02733v3 Announce Type: replace-cross 
Abstract: Relative overgeneralization (RO) is a pathology that can arise in cooperative multi-agent tasks when the optimal joint action's utility falls below that of a sub-optimal joint action. RO can cause the agents to get stuck into local optima or fail to solve cooperative tasks requiring significant coordination between agents within a given timestep. In this work, we empirically find that, in multi-agent reinforcement learning (MARL), both value-based and policy gradient MARL algorithms can suffer from RO and fail to learn effective coordination policies. To better overcome RO, we propose a novel approach called curriculum learning for relative overgeneralization (CURO). To solve a target task that exhibits strong RO, in CURO, we first fine-tune the reward function of the target task to generate source tasks to train the agent. Then, to effectively transfer the knowledge acquired in one task to the next, we use a transfer learning method that combines value function transfer with buffer transfer, which enables more efficient exploration in the target task. CURO is general and can be applied to both value-based and policy gradient MARL methods. We demonstrate that, when applied to QMIX, HAPPO, and HATRPO, CURO can successfully overcome severe RO, achieve improved performance, and outperform baseline methods in a variety of challenging cooperative multi-agent tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.02733v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Shi, Qiyuan Liu, Bei Peng</dc:creator>
    </item>
    <item>
      <title>Congestion Pricing for Efficiency and Equity: Theory and Applications to the San Francisco Bay Area</title>
      <link>https://arxiv.org/abs/2401.16844</link>
      <description>arXiv:2401.16844v2 Announce Type: replace-cross 
Abstract: Congestion pricing, while adopted by many cities to alleviate traffic congestion, raises concerns about widening socioeconomic disparities due to its disproportionate impact on low-income travelers. We address this concern by proposing a new class of congestion pricing schemes that not only minimize total travel time, but also incorporate an equity objective, reducing disparities in the relative change in travel costs across populations with different incomes, following the implementation of tolls. Our analysis builds on a congestion game model with heterogeneous traveler populations. We present four pricing schemes that account for practical considerations, such as the ability to charge differentiated tolls to various traveler populations and the option to toll all or only a subset of edges in the network. We evaluate our pricing schemes in the calibrated freeway network of the San Francisco Bay Area. We demonstrate that the proposed congestion pricing schemes improve both the total travel time and the equity objective compared to the current pricing scheme.
  Our results further show that pricing schemes charging differentiated prices to traveler populations with varying value-of-time lead to a more equitable distribution of travel costs compared to those that charge a homogeneous price to all.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16844v2</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>econ.EM</category>
      <category>eess.SY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chinmay Maheshwari, Kshitij Kulkarni, Druv Pai, Jiarui Yang, Manxi Wu, Shankar Sastry</dc:creator>
    </item>
    <item>
      <title>When AI Meets Finance (StockAgent): Large Language Model-based Stock Trading in Simulated Real-world Environments</title>
      <link>https://arxiv.org/abs/2407.18957</link>
      <description>arXiv:2407.18957v4 Announce Type: replace-cross 
Abstract: Can AI Agents simulate real-world trading environments to investigate the impact of external factors on stock trading activities (e.g., macroeconomics, policy changes, company fundamentals, and global events)? These factors, which frequently influence trading behaviors, are critical elements in the quest for maximizing investors' profits. Our work attempts to solve this problem through large language model based agents. We have developed a multi-agent AI system called StockAgent, driven by LLMs, designed to simulate investors' trading behaviors in response to the real stock market. The StockAgent allows users to evaluate the impact of different external factors on investor trading and to analyze trading behavior and profitability effects. Additionally, StockAgent avoids the test set leakage issue present in existing trading simulation systems based on AI Agents. Specifically, it prevents the model from leveraging prior knowledge it may have acquired related to the test data. We evaluate different LLMs under the framework of StockAgent in a stock trading environment that closely resembles real-world conditions. The experimental results demonstrate the impact of key external factors on stock market trading, including trading behavior and stock price fluctuation rules. This research explores the study of agents' free trading gaps in the context of no prior knowledge related to market data. The patterns identified through StockAgent simulations provide valuable insights for LLM-based investment advice and stock recommendation. The code is available at https://github.com/MingyuJ666/Stockagent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18957v4</guid>
      <category>q-fin.TR</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chong Zhang, Xinyi Liu, Zhongmou Zhang, Mingyu Jin, Lingyao Li, Zhenting Wang, Wenyue Hua, Dong Shu, Suiyuan Zhu, Xiaobo Jin, Sujian Li, Mengnan Du, Yongfeng Zhang</dc:creator>
    </item>
  </channel>
</rss>
