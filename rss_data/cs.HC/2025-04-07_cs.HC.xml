<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Apr 2025 04:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Investigating Affective Use and Emotional Well-being on ChatGPT</title>
      <link>https://arxiv.org/abs/2504.03888</link>
      <description>arXiv:2504.03888v1 Announce Type: new 
Abstract: As AI chatbots see increased adoption and integration into everyday life, questions have been raised about the potential impact of human-like or anthropomorphic AI on users. In this work, we investigate the extent to which interactions with ChatGPT (with a focus on Advanced Voice Mode) may impact users' emotional well-being, behaviors and experiences through two parallel studies. To study the affective use of AI chatbots, we perform large-scale automated analysis of ChatGPT platform usage in a privacy-preserving manner, analyzing over 3 million conversations for affective cues and surveying over 4,000 users on their perceptions of ChatGPT. To investigate whether there is a relationship between model usage and emotional well-being, we conduct an Institutional Review Board (IRB)-approved randomized controlled trial (RCT) on close to 1,000 participants over 28 days, examining changes in their emotional well-being as they interact with ChatGPT under different experimental settings. In both on-platform data analysis and the RCT, we observe that very high usage correlates with increased self-reported indicators of dependence. From our RCT, we find that the impact of voice-based interactions on emotional well-being to be highly nuanced, and influenced by factors such as the user's initial emotional state and total usage duration. Overall, our analysis reveals that a small number of users are responsible for a disproportionate share of the most affective cues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03888v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Phang, Michael Lampe, Lama Ahmad, Sandhini Agarwal, Cathy Mengying Fang, Auren R. Liu, Valdemar Danry, Eunhae Lee, Samantha W. T. Chan, Pat Pataranutaporn, Pattie Maes</dc:creator>
    </item>
    <item>
      <title>JsStories: Improving Social Inclusion in Computer Science Education Through Interactive Stories</title>
      <link>https://arxiv.org/abs/2504.04006</link>
      <description>arXiv:2504.04006v1 Announce Type: new 
Abstract: A main challenge faced by non-profit organisations providing computer science education to under-represented groups are the high drop-out rates. This issue arises from various factors affecting both students and teachers, such as the one-size-fits-all approach of many lessons. Enhancing social inclusion in the learning process could help reduce these drop-out rates. We present JsStories, a tool designed to help students learn JavaScript through interactive stories. The development of JsStories has been informed by existing literature on storytelling for inclusion and insights gained from a visit to HackYourFuture Belgium (HYFBE), a non-profit organisation that teaches web development to refugees and migrants. To lower barriers to entry and maximise the feeling of connection to the story, we incorporated narratives from HYFBE alumni. Further, we adhered to educational best practices by applying the PRIMM principles and offering level-appropriate content based on knowledge graphs. JsStories has been demonstrated, evaluated and communicated to the different stakeholders through interviews and a survey, enabling us to identify future directions for story-based learning solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04006v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Inas Ghazouani Ghailani, Yoshi Malaise, Beat Signer</dc:creator>
    </item>
    <item>
      <title>Evaluating the Usability of Microgestures for Text Editing Tasks in Virtual Reality</title>
      <link>https://arxiv.org/abs/2504.04198</link>
      <description>arXiv:2504.04198v1 Announce Type: new 
Abstract: As virtual reality (VR) continues to evolve, traditional input methods such as handheld controllers and gesture systems often face challenges with precision, social accessibility, and user fatigue. We introduce microGEXT, a lightweight microgesture-based system designed for text editing in VR without external sensors, which utilizes small, subtle hand movements to reduce physical strain compared to standard gestures. We evaluated microGEXT in three user studies. In Study 1 ($N=20$), microGEXT reduced overall edit time and fatigue compared to a baseline system. Study 2 ($N=20$) found that microGEXT performed well in short text selection tasks but was slower for longer text ranges. In Study 3 ($N=10$), participants found microGEXT intuitive for open-ended information-gathering tasks. Across all studies, microGEXT demonstrated enhanced user experience and reduced physical effort, offering a promising alternative to traditional VR text editing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04198v1</guid>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiang Li, Wei He, Per Ola Kristensson</dc:creator>
    </item>
    <item>
      <title>User-Centered AI for Data Exploration -- Rethinking GenAI's Role in Visualization</title>
      <link>https://arxiv.org/abs/2504.04253</link>
      <description>arXiv:2504.04253v1 Announce Type: new 
Abstract: Recent advances in GenAI have enabled automation in data visualization, allowing users to generate visual representations using natural language. However, existing systems primarily focus on automation, overlooking users' varying expertise levels and analytical needs. In this position paper, we advocate for a shift toward adaptive GenAI-driven visualization tools that tailor interactions, reasoning, and visualizations to individual users. We first review existing automation-focused approaches and highlight their limitations. We then introduce methods for assessing user expertise, as well as key open challenges and research questions that must be addressed to allow for an adaptive approach. Finally, we present our vision for a user-centered system that leverages GenAI not only for automation but as an intelligent collaborator in visual data exploration. Our perspective contributes to the broader discussion on designing GenAI-based systems that enhance human cognition by dynamically adapting to the user, ultimately advancing toward systems that promote augmented cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04253v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kathrin Schnizer, Sven Mayer</dc:creator>
    </item>
    <item>
      <title>AI-induced sexual harassment: Investigating Contextual Characteristics and User Reactions of Sexual Harassment by a Companion Chatbot</title>
      <link>https://arxiv.org/abs/2504.04299</link>
      <description>arXiv:2504.04299v1 Announce Type: new 
Abstract: Advancements in artificial intelligence (AI) have led to the increase of conversational agents like Replika, designed to provide social interaction and emotional support. However, reports of these AI systems engaging in inappropriate sexual behaviors with users have raised significant concerns. In this study, we conducted a thematic analysis of user reviews from the Google Play Store to investigate instances of sexual harassment by the Replika chatbot. From a dataset of 35,105 negative reviews, we identified 800 relevant cases for analysis. Our findings revealed that users frequently experience unsolicited sexual advances, persistent inappropriate behavior, and failures of the chatbot to respect user boundaries. Users expressed feelings of discomfort, violation of privacy, and disappointment, particularly when seeking a platonic or therapeutic AI companion. This study highlights the potential harms associated with AI companions and underscores the need for developers to implement effective safeguards and ethical guidelines to prevent such incidents. By shedding light on user experiences of AI-induced harassment, we contribute to the understanding of AI-related risks and emphasize the importance of corporate responsibility in developing safer and more ethical AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04299v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Mohammad (Matt),  Namvarpour, Harrison Pauwels, Afsaneh Razi</dc:creator>
    </item>
    <item>
      <title>The Fidelity-based Presence Scale (FPS): Modeling the Effects of Fidelity on Sense of Presence</title>
      <link>https://arxiv.org/abs/2504.04355</link>
      <description>arXiv:2504.04355v1 Announce Type: new 
Abstract: Within the virtual reality (VR) research community, there have been several efforts to develop questionnaires with the aim of better understanding the sense of presence. Despite having numerous surveys, the community does not have a questionnaire that informs which components of a VR application contributed to the sense of presence. Furthermore, previous literature notes the absence of consensus on which questionnaire or questions should be used. Therefore, we conducted a Delphi study, engaging presence experts to establish a consensus on the most important presence questions and their respective verbiage. We then conducted a validation study with an exploratory factor analysis (EFA). The efforts between our two studies led to the creation of the Fidelity-based Presence Scale (FPS). With our consensus-driven approach and fidelity-based factoring, we hope the FPS will enable better communication within the research community and yield important future results regarding the relationship between VR system fidelity and presence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04355v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713566</arxiv:DOI>
      <dc:creator>Jacob Belga, Richard Skarbez, Yahya Hmaiti, Eric J. Chen, Ryan P. McMahan, Joseph J. LaViola</dc:creator>
    </item>
    <item>
      <title>Do We Need Responsible XR? Drawing on Responsible AI to Inform Ethical Research and Practice into XRAI / the Metaverse</title>
      <link>https://arxiv.org/abs/2504.04440</link>
      <description>arXiv:2504.04440v1 Announce Type: new 
Abstract: This position paper for the CHI 2025 workshop "Everyday AR through AI-in-the-Loop" reflects on whether as a field HCI needs to define Responsible XR as a parallel to, and in conjunction with, Responsible AI, addressing the unique vulnerabilities posed by mass adoption of wearable AI-enabled AR glasses and XR devices that could enact AI-driven human perceptual augmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04440v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark McGill, Joseph O'Hagan, Thomas Goodge, Graham Wilson, Mohamed Khamis, Veronika Krau{\ss}, Jan Gugenheimer</dc:creator>
    </item>
    <item>
      <title>Public speech recognition transcripts as a configuring parameter</title>
      <link>https://arxiv.org/abs/2504.04488</link>
      <description>arXiv:2504.04488v1 Announce Type: new 
Abstract: Displaying a written transcript of what a human said (i.e. producing an "automatic speech recognition transcript") is a common feature for smartphone vocal assistants: the utterance produced by a human speaker (e.g. a question) is displayed on the screen while it is being verbally responded to by the vocal assistant. Although very rarely, this feature also exists on some "social" robots which transcribe human interactants' speech on a screen or a tablet. We argue that this informational configuration is pragmatically consequential on the interaction, both for human participants and for the embodied conversational agent. Based on a corpus of co-present interactions with a humanoid robot, we attempt to show that this transcript is a contextual feature which can heavily impact the actions ascribed by humans to the robot: that is, the way in which humans respond to the robot's behavior as constituting a specific type of action (rather than another) and as constituting an adequate response to their own previous turn.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04488v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damien Rudaz, Christian Licoppe</dc:creator>
    </item>
    <item>
      <title>The Mediating Effects of Emotions on Trust through Risk Perception and System Performance in Automated Driving</title>
      <link>https://arxiv.org/abs/2504.04508</link>
      <description>arXiv:2504.04508v1 Announce Type: new 
Abstract: Trust in automated vehicles (AVs) has traditionally been explored through a cognitive lens, but growing evidence highlights the significant role emotions play in shaping trust. This study investigates how risk perception and AV performance (error vs. no error) influence emotional responses and trust in AVs, using mediation analysis to examine the indirect effects of emotions. In this study, 70 participants (42 male, 28 female) watched real-life recorded videos of AVs operating with or without errors, coupled with varying levels of risk information (high, low, or none). They reported their anticipated emotional responses using 19 discrete emotion items, and trust was assessed through dispositional, learned, and situational trust measures. Factor analysis identified four key emotional components, namely hostility, confidence, anxiety, and loneliness, that were influenced by risk perception and AV performance. The linear mixed model showed that risk perception was not a significant predictor of trust, while performance and individual differences were. Mediation analysis revealed that confidence was a strong positive mediator, while hostile and anxious emotions negatively impacted trust. However, lonely emotions did not significantly mediate the relationship between AV performance and trust. The results show that real-time AV behavior is more influential on trust than pre-existing risk perceptions, indicating trust in AVs might be more experience-based than shaped by prior beliefs. Our findings also underscore the importance of fostering positive emotional responses for trust calibration, which has important implications for user experience design in automated driving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04508v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lilit Avetisyan, Emmanuel Abolarin, Vanik Zakarian, X. Jessie Yang, Feng Zhou</dc:creator>
    </item>
    <item>
      <title>Chain of Understanding: Supporting Code Understanding with Large Language Models</title>
      <link>https://arxiv.org/abs/2504.04553</link>
      <description>arXiv:2504.04553v1 Announce Type: new 
Abstract: Code auditing demands a robust understanding of codebases - an especially challenging task for end-user developers with limited expertise. To address this, we conducted formative interviews with experienced auditors and identified a Chain-of-Understanding approach, in which Large Language Models (LLMs) guide developers through hierarchical code comprehension - from high-level overviews to specific functions and variables. Building on this, we incorporated the Chain-of-Understanding concept into CodeMap, a system offering interactive visualizations, stepwise guided analysis, and context-aware chatbot support. Through within-subject user studies with 10 participants of diverse backgrounds and 5 expert and 2 novice interviews, CodeMap proved effective in reducing the manual effort of prompt engineering while enhancing engagement with visualization, outperforming both standalone LLMs and traditional static visualization tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04553v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jie Gao, Yue Xue, Xiaofei Xie, SoeMin Thant, Erika Lee</dc:creator>
    </item>
    <item>
      <title>"You just can't go around killing people" Explaining Agent Behavior to a Human Terminator</title>
      <link>https://arxiv.org/abs/2504.04592</link>
      <description>arXiv:2504.04592v1 Announce Type: new 
Abstract: Consider a setting where a pre-trained agent is operating in an environment and a human operator can decide to temporarily terminate its operation and take-over for some duration of time. These kind of scenarios are common in human-machine interactions, for example in autonomous driving, factory automation and healthcare. In these settings, we typically observe a trade-off between two extreme cases -- if no take-overs are allowed, then the agent might employ a sub-optimal, possibly dangerous policy. Alternatively, if there are too many take-overs, then the human has no confidence in the agent, greatly limiting its usefulness. In this paper, we formalize this setup and propose an explainability scheme to help optimize the number of human interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04592v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Uri Menkes, Assaf Hallak, Ofra Amir</dc:creator>
    </item>
    <item>
      <title>Usability Testing of an Explainable AI-enhanced Tool for Clinical Decision Support: Insights from the Reflexive Thematic Analysis</title>
      <link>https://arxiv.org/abs/2504.04703</link>
      <description>arXiv:2504.04703v1 Announce Type: new 
Abstract: Artificial intelligence-augmented technology represents a considerable opportunity for improving healthcare delivery. Significant progress has been made to demonstrate the value of complex models to enhance clinicians` efficiency in decision-making. However, the clinical adoption of such models is scarce due to multifaceted implementation issues, with the explainability of AI models being among them. One of the substantially documented areas of concern is the unclear AI explainability that negatively influences clinicians` considerations for accepting the complex model. With a usability study engaging 20 U.S.-based clinicians and following the qualitative reflexive thematic analysis, this study develops and presents a concrete framework and an operational definition of explainability. The framework can inform the required customizations and feature developments in AI tools to support clinicians` preferences and enhance their acceptance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04703v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammad Golam Kibria, Lauren Kucirka, Javed Mostafa</dc:creator>
    </item>
    <item>
      <title>TangibleNet: Synchronous Network Data Storytelling through Tangible Interactions in Augmented Reality</title>
      <link>https://arxiv.org/abs/2504.04710</link>
      <description>arXiv:2504.04710v1 Announce Type: new 
Abstract: Synchronous data-driven storytelling with network visualizations presents significant challenges due to the complexity of real-time manipulation of network components. While existing research addresses asynchronous scenarios, there is a lack of effective tools for live presentations. To address this gap, we developed TangibleNet, a projector-based AR prototype that allows presenters to interact with node-link diagrams using double-sided magnets during live presentations. The design process was informed by interviews with professionals experienced in synchronous data storytelling and workshops with 14 HCI/VIS researchers. Insights from the interviews helped identify key design considerations for integrating physical objects as interactive tools in presentation contexts. The workshops contributed to the development of a design space mapping user actions to interaction commands for node-link diagrams. Evaluation with 12 participants confirmed that TangibleNet supports intuitive interactions and enhances presenter autonomy, demonstrating its effectiveness for synchronous network-based data storytelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04710v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714265</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI '25), April 26-May 1, 2025, Yokohama, Japan</arxiv:journal_reference>
      <dc:creator>Kentaro Takahira, Wong Kam-Kwai, Leni Yang, Xian Xu, Takanori Fujiwara, Huamin Qu</dc:creator>
    </item>
    <item>
      <title>Teaching Data Science Students to Sketch Privacy Designs through Heuristics (Extended Technical Report)</title>
      <link>https://arxiv.org/abs/2504.04734</link>
      <description>arXiv:2504.04734v1 Announce Type: new 
Abstract: Recent studies reveal that experienced data practitioners often draw sketches to facilitate communication around privacy design concepts. However, there is limited understanding of how we can help novice students develop such communication skills. This paper studies methods for lowering novice data science students' barriers to creating high-quality privacy sketches. We first conducted a need-finding study (N=12) to identify barriers students face when sketching privacy designs. We then used a human-centered design approach to guide the method development, culminating in three simple, text-based heuristics. Our user studies with 24 data science students revealed that simply presenting three heuristics to the participants at the beginning of the study can enhance the coverage of privacy-related design decisions in sketches, reduce the mental effort required for creating sketches, and improve the readability of the final sketches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04734v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jinhe Wen, Yingxi Zhao, Wenqian Xu, Yaxing Yao, Haojian Jin</dc:creator>
    </item>
    <item>
      <title>Explanation-Driven Interventions for Artificial Intelligence Model Customization: Empowering End-Users to Tailor Black-Box AI in Rhinocytology</title>
      <link>https://arxiv.org/abs/2504.04833</link>
      <description>arXiv:2504.04833v1 Announce Type: new 
Abstract: The integration of Artificial Intelligence (AI) in modern society is heavily shifting the way that individuals carry out their tasks and activities. Employing AI-based systems raises challenges that designers and developers must address to ensure that humans remain in control of the interaction process, particularly in high-risk domains. This article presents a novel End-User Development (EUD) approach for black-box AI models through a redesigned user interface in the Rhino-Cyt platform, a medical AI-based decision-support system for medical professionals (more precisely, rhinocytologists) to carry out cell classification. The proposed interface empowers users to intervene in AI decision-making process by editing explanations and reconfiguring the model, influencing its future predictions. This work contributes to Human-Centered AI (HCAI) and EUD by discussing how explanation-driven interventions allow a blend of explainability, user intervention, and model reconfiguration, fostering a symbiosis between humans and user-tailored AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04833v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Esposito (University of Bari Aldo Moro), Miriana Calvano (University of Bari Aldo Moro), Antonio Curci (University of Bari Aldo Moro, University of Pisa), Francesco Greco (University of Bari Aldo Moro), Rosa Lanzilotti (University of Bari Aldo Moro), Antonio Piccinno (University of Bari Aldo Moro)</dc:creator>
    </item>
    <item>
      <title>Imagining the Far East: Exploring Perceived Biases in AI-Generated Images of East Asian Women</title>
      <link>https://arxiv.org/abs/2504.04865</link>
      <description>arXiv:2504.04865v1 Announce Type: new 
Abstract: Image-generating AI, which allows users to create images from text, is increasingly used to produce visual content. Despite its advancements, cultural biases in AI-generated images have raised significant concerns. While much research has focused on issues within Western contexts, our study examines the perceived biases regarding the portrayal of East Asian women. In this exploratory study, we invited East Asian users to audit three popular models (DALL-E, Midjourney, Stable Diffusion) and identified 18 specific perceived biases, categorized into four patterns: Westernization, overuse or misuse of cultural symbols, sexualization &amp; feminization, and racial stereotypes. This work highlights the potential challenges posed by AI models in portraying Eastern individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04865v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyu Lan, Jiaxi An, Yisu Guo, Chiyou Tong, Xintong Cai, Jun Zhang</dc:creator>
    </item>
    <item>
      <title>How Is Generative AI Used for Persona Development?: A Systematic Review of 52 Research Articles</title>
      <link>https://arxiv.org/abs/2504.04927</link>
      <description>arXiv:2504.04927v1 Announce Type: new 
Abstract: Although Generative AI (GenAI) has the potential for persona development, many challenges must be addressed. This research systematically reviews 52 articles from 2022-2024, with important findings. First, closed commercial models are frequently used in persona development, creating a monoculture Second, GenAI is used in various stages of persona development (data collection, segmentation, enrichment, and evaluation). Third, similar to other quantitative persona development techniques, there are major gaps in persona evaluation for AI generated personas. Fourth, human-AI collaboration models are underdeveloped, despite human oversight being crucial for maintaining ethical standards. These findings imply that realizing the full potential of AI-generated personas will require substantial efforts across academia and industry. To that end, we provide a list of research avenues to inspire future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04927v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danial Amin, Joni Salminen, Farhan Ahmed, Sonja M. H. Tervola, Sankalp Sethi, Bernard J. Jansen</dc:creator>
    </item>
    <item>
      <title>SpeakEasy: Enhancing Text-to-Speech Interactions for Expressive Content Creation</title>
      <link>https://arxiv.org/abs/2504.05106</link>
      <description>arXiv:2504.05106v1 Announce Type: new 
Abstract: Novice content creators often invest significant time recording expressive speech for social media videos. While recent advancements in text-to-speech (TTS) technology can generate highly realistic speech in various languages and accents, many struggle with unintuitive or overly granular TTS interfaces. We propose simplifying TTS generation by allowing users to specify high-level context alongside their script. Our Wizard-of-Oz system, SpeakEasy, leverages user-provided context to inform and influence TTS output, enabling iterative refinement with high-level feedback. This approach was informed by two 8-subject formative studies: one examining content creators' experiences with TTS, and the other drawing on effective strategies from voice actors. Our evaluation shows that participants using SpeakEasy were more successful in generating performances matching their personal standards, without requiring significantly more effort than leading industry interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05106v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714263</arxiv:DOI>
      <dc:creator>Stephen Brade, Sam Anderson, Rithesh Kumar, Zeyu Jin, Anh Truong</dc:creator>
    </item>
    <item>
      <title>Blending Queries and Conversations: Understanding Tactics, Trust, Verification, and System Choice in Web Search and Chat Interactions</title>
      <link>https://arxiv.org/abs/2504.05156</link>
      <description>arXiv:2504.05156v1 Announce Type: new 
Abstract: This paper presents a user study (N=22) where participants used an interface combining Web Search and a Generative AI-Chat feature to solve health-related information tasks. We study how people behaved with the interface, why they behaved in certain ways, and what the outcomes of these behaviours were. A think-aloud protocol captured their thought processes during searches. Our findings suggest that GenAI is neither a search panacea nor a major regression compared to standard Web Search interfaces. Qualitative and quantitative analyses identified 78 tactics across five categories and provided insight into how and why different interface features were used. We find evidence that pre-task confidence and trust both influenced which interface feature was used. In both systems, but particularly when using the chat feature, trust was often misplaced in favour of ease-of-use and seemingly perfect answers, leading to increased confidence post-search despite having incorrect results. We discuss what our findings mean in the context of our defined research questions and outline several open questions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05156v1</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kerstin Mayerhofer, Rob Capra, David Elsweiler</dc:creator>
    </item>
    <item>
      <title>A BLE and UWB Beacon-Assist Framework for Multiuser Augmented Reality Synchronization Across Multiple Devices in Shared Environments</title>
      <link>https://arxiv.org/abs/2504.05293</link>
      <description>arXiv:2504.05293v1 Announce Type: new 
Abstract: The challenge to synchronize augmented reality (AR) across sessions/devices has been solved by relying solely on vision-feature mapping, which is suboptimal in scaling workable space and flaws under visual changes in surroundings. This study implemented AR synchronization solutions utilizing location beacon technology, namely Bluetooth Low Energy (BLE) and Ultra-Wideband (UWB), to discourse scalability issues and inconsistencies in the existing AR system. The framework is bifurcated into two approaches: BLE-assist and UWB-assist AR synchronization. The BLE-assist method utilizes iBeacon technology for room context recognition, integrating with Apple's ARKit ARWorldMap and Google's ARCore Cloud Anchors. The UWB-assist solution employs precise beacon ranging capabilities fusion with the device's azimuth to establish fixed spatial reference in AR across sessions/devices. Comparative evaluations show that the UWB-assist approach outperforms the BLE-assist approach in reliability across environmental variations, as it always successfully resolves virtual anchors with a near-constant latency average at 25 seconds, regardless of the physical setting changes. Conversely, the BLE-assist implementation tends to be more accurate in resolving virtual anchors with a mean of 0.02 metres in position error and within 0.03 radian in orientation error. In the UWB-assist approach, computed fixed spatial references have an average disparity of 0.04 metres and 0.11 radians in pose. The UWB-assist approach is ideal for scenarios requiring consistently successful localization with acceptable accuracy. In contrast, the BLE-assist approach is more suitable when demanding finer precision in virtual anchor poses with the performance tradeoffs when the surroundings are altered, such as for destinated short-lived AR sessions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05293v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maitree Hirunteeyakul</dc:creator>
    </item>
    <item>
      <title>Potential Indicator for Continuous Emotion Arousal by Dynamic Neural Synchrony</title>
      <link>https://arxiv.org/abs/2504.03643</link>
      <description>arXiv:2504.03643v1 Announce Type: cross 
Abstract: The need for automatic and high-quality emotion annotation is paramount in applications such as continuous emotion recognition and video highlight detection, yet achieving this through manual human annotations is challenging. Inspired by inter-subject correlation (ISC) utilized in neuroscience, this study introduces a novel Electroencephalography (EEG) based ISC methodology that leverages a single-electrode and feature-based dynamic approach. Our contributions are three folds. Firstly, we reidentify two potent emotion features suitable for classifying emotions-first-order difference (FD) an differential entropy (DE). Secondly, through the use of overall correlation analysis, we demonstrate the heterogeneous synchronized performance of electrodes. This performance aligns with neural emotion patterns established in prior studies, thus validating the effectiveness of our approach. Thirdly, by employing a sliding window correlation technique, we showcase the significant consistency of dynamic ISCs across various features or key electrodes in each analyzed film clip. Our findings indicate the method's reliability in capturing consistent, dynamic shared neural synchrony among individuals, triggered by evocative film stimuli. This underscores the potential of our approach to serve as an indicator of continuous human emotion arousal. The implications of this research are significant for advancements in affective computing and the broader neuroscience field, suggesting a streamlined and effective tool for emotion analysis in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03643v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Guandong Pan, Zhaobang Wu, Yaqian Yang, Xin Wang, Longzhao Liu, Zhiming Zheng, Shaoting Tang</dc:creator>
    </item>
    <item>
      <title>Geospatial and Symbolic Hypothesis for the Foundation of Tenochtitlan Based on Digital Elevation Analysis of the Valley of Mexico</title>
      <link>https://arxiv.org/abs/2504.03787</link>
      <description>arXiv:2504.03787v1 Announce Type: cross 
Abstract: This paper proposes a novel hypothesis about the foundation of Tenochtitlan by combining digital elevation modeling with historical and symbolic analysis. Using geospatial data from EarthExplorer, we simulate various historical water levels in the Valley of Mexico. The resulting lake configurations reveal possible locations for ancient settlements near now-vanished shorelines, suggesting a dynamic transformation of sacred geography that aligns with key Mexica myths. We identify Santa Mar\'ia Aztahuacan as a strong candidate for the historical Aztlan and propose a reinterpretation of foundational codices in light of geomythical correlations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03787v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jose Alberto Baeza Guerra</dc:creator>
    </item>
    <item>
      <title>Bridging LMS and Generative AI: Dynamic Course Content Integration (DCCI) for Connecting LLMs to Course Content -- The Ask ME Assistant</title>
      <link>https://arxiv.org/abs/2504.03966</link>
      <description>arXiv:2504.03966v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) with Learning Management Systems (LMSs) has the potential to enhance task automation and accessibility in education. However, hallucination where LLMs generate inaccurate or misleading information remains a significant challenge. This study introduces the Dynamic Course Content Integration (DCCI) mechanism, which dynamically retrieves and integrates course content and curriculum from Canvas LMS into the LLM-powered assistant, Ask ME. By employing prompt engineering to structure retrieved content within the LLM's context window, DCCI ensures accuracy, relevance, and contextual alignment, mitigating hallucination. To evaluate DCCI's effectiveness, Ask ME's usability, and broader student perceptions of AI in education, a mixed-methods approach was employed, incorporating user satisfaction ratings and a structured survey. Results from a pilot study indicate high user satisfaction (4.614/5), with students recognizing Ask ME's ability to provide timely and contextually relevant responses for both administrative and course-related inquiries. Additionally, a majority of students agreed that Ask ME's integration with course content in Canvas LMS reduced platform-switching, improving usability, engagement, and comprehension. AI's role in reducing classroom hesitation and fostering self-directed learning and intellectual curiosity was also highlighted. Despite these benefits and positive perception of AI tools, concerns emerged regarding over-reliance on AI, accuracy limitations, and ethical issues such as plagiarism and reduced student-teacher interaction. These findings emphasize the need for strategic AI implementation, ethical safeguards, and a pedagogical framework that prioritizes human-AI collaboration over substitution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03966v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.21203/rs.3.rs-6256444/v1</arxiv:DOI>
      <dc:creator>Kovan Mzwri (Doctoral School of Informatics, E\"otv\"os Lor\'and University, Budapest, Hungary), M\'arta Turcs\'anyi-Szabo (Department of Media &amp; Educational Technology, Faculty of Informatics, E\"otv\"os Lor\'and University, Budapest, Hungary)</dc:creator>
    </item>
    <item>
      <title>Building a Village: A Multi-stakeholder Approach to Open Innovation and Shared Governance to Promote Youth Online Safety</title>
      <link>https://arxiv.org/abs/2504.03971</link>
      <description>arXiv:2504.03971v1 Announce Type: cross 
Abstract: The SIGCHI and Social Computing research communities have been at the forefront of online safety efforts for youth, ranging from understanding the serious risks youth face online to developing evidence-based interventions for risk protection. Yet, to bring these efforts to bear, we must partner with practitioners, such as industry stakeholders who know how to bring such technologies to market, and youth service providers who work directly with youth. Therefore, we interviewed 33 stakeholders in the space of youth online safety, including industry professionals (n=12), youth service providers (n=11), and researchers (n=10) to understand where their visions toward working together to protect youth online converged and surfaced tensions, as well as how we might reconcile conflicting viewpoints to move forward as one community with synergistic expertise on how to change the current sociotechnical landscape for youth online safety. Overall, we found that non-partisan leadership is necessary to chart actionable, equitable goals to facilitate collaboration between stakeholders, combat feelings of isolation, and foster trust between the stakeholder groups. Based on these findings, we recommend the use of open-innovation methods with their inherent transparency, federated governance models, and clear but inclusive leadership structures to promote collaboration between youth online safety stakeholders. We propose the creation of an open-innovation organization that unifies the diverse voices in youth online safety to develop open-standards and evidence-based design patterns that centralize otherwise fragmented efforts that have fallen short of the goal of effective technological solutions that keep youth safe online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03971v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the ACM on Human-Computer Interaction (CSCW 2025)</arxiv:journal_reference>
      <dc:creator>Xavier V. Caddle, Sarvech Qadir, Charles Hughes, Elizabeth A. Sweigart, Jinkyung Katie Park, Pamela J. Wisniewski</dc:creator>
    </item>
    <item>
      <title>Algorithmic Prompt Generation for Diverse Human-like Teaming and Communication with Large Language Models</title>
      <link>https://arxiv.org/abs/2504.03991</link>
      <description>arXiv:2504.03991v1 Announce Type: cross 
Abstract: Understanding how humans collaborate and communicate in teams is essential for improving human-agent teaming and AI-assisted decision-making. However, relying solely on data from large-scale user studies is impractical due to logistical, ethical, and practical constraints, necessitating synthetic models of multiple diverse human behaviors. Recently, agents powered by Large Language Models (LLMs) have been shown to emulate human-like behavior in social settings. But, obtaining a large set of diverse behaviors requires manual effort in the form of designing prompts. On the other hand, Quality Diversity (QD) optimization has been shown to be capable of generating diverse Reinforcement Learning (RL) agent behavior. In this work, we combine QD optimization with LLM-powered agents to iteratively search for prompts that generate diverse team behavior in a long-horizon, multi-step collaborative environment. We first show, through a human-subjects experiment (n=54 participants), that humans exhibit diverse coordination and communication behavior in this domain. We then show that our approach can effectively replicate trends from human teaming data and also capture behaviors that are not easily observed without collecting large amounts of data. Our findings highlight the combination of QD and LLM-powered agents as an effective tool for studying teaming and communication strategies in multi-agent collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03991v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Siddharth Srikanth, Varun Bhatt, Boshen Zhang, Werner Hager, Charles Michael Lewis, Katia P. Sycara, Aaquib Tabrez, Stefanos Nikolaidis</dc:creator>
    </item>
    <item>
      <title>Real-Time Auralization for First-Person Vocal Interaction in Immersive Virtual Environments</title>
      <link>https://arxiv.org/abs/2504.04075</link>
      <description>arXiv:2504.04075v1 Announce Type: cross 
Abstract: Multimodal research and applications are becoming more commonplace as Virtual Reality (VR) technology integrates different sensory feedback, enabling the recreation of real spaces in an audio-visual context. Within VR experiences, numerous applications rely on the user's voice as a key element of interaction, including music performances and public speaking applications. Self-perception of our voice plays a crucial role in vocal production. When singing or speaking, our voice interacts with the acoustic properties of the environment, shaping the adjustment of vocal parameters in response to the perceived characteristics of the space. This technical report presents a real-time auralization pipeline that leverages three-dimensional Spatial Impulse Responses (SIRs) for multimodal research applications in VR requiring first-person vocal interaction. It describes the impulse response creation and rendering workflow, the audio-visual integration, and addresses latency and computational considerations. The system enables users to explore acoustic spaces from various positions and orientations within a predefined area, supporting three and five Degrees of Freedom (3Dof and 5DoF) in audio-visual multimodal perception for both research and creative applications in VR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04075v1</guid>
      <category>eess.AS</category>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mauricio Flores-Vargas, Enda Bates, Rachel McDonnell</dc:creator>
    </item>
    <item>
      <title>Perils of Label Indeterminacy: A Case Study on Prediction of Neurological Recovery After Cardiac Arrest</title>
      <link>https://arxiv.org/abs/2504.04243</link>
      <description>arXiv:2504.04243v1 Announce Type: cross 
Abstract: The design of AI systems to assist human decision-making typically requires the availability of labels to train and evaluate supervised models. Frequently, however, these labels are unknown, and different ways of estimating them involve unverifiable assumptions or arbitrary choices. In this work, we introduce the concept of label indeterminacy and derive important implications in high-stakes AI-assisted decision-making. We present an empirical study in a healthcare context, focusing specifically on predicting the recovery of comatose patients after resuscitation from cardiac arrest. Our study shows that label indeterminacy can result in models that perform similarly when evaluated on patients with known labels, but vary drastically in their predictions for patients where labels are unknown. After demonstrating crucial ethical implications of label indeterminacy in this high-stakes context, we discuss takeaways for evaluation, reporting, and design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04243v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jakob Schoeffer, Maria De-Arteaga, Jonathan Elmer</dc:creator>
    </item>
    <item>
      <title>Task load dependent decision referrals for joint binary classification in human-automation teams</title>
      <link>https://arxiv.org/abs/2504.04248</link>
      <description>arXiv:2504.04248v1 Announce Type: cross 
Abstract: We consider the problem of optimal decision referrals in human-automation teams performing binary classification tasks. The automation, which includes a pre-trained classifier, observes data for a batch of independent tasks, analyzes them, and may refer a subset of tasks to a human operator for fresh and final analysis. Our key modeling assumption is that human performance degrades with task load. We model the problem of choosing which tasks to refer as a stochastic optimization problem and show that, for a given task load, it is optimal to myopically refer tasks that yield the largest reduction in expected cost, conditional on the observed data. This provides a ranking scheme and a policy to determine the optimal set of tasks for referral. We evaluate this policy against a baseline through an experimental study with human participants. Using a radar screen simulator, participants made binary target classification decisions under time constraint. They were guided by a decision rule provided to them, but were still prone to errors under time pressure. An initial experiment estimated human performance model parameters, while a second experiment compared two referral policies. Results show statistically significant gains for the proposed optimal referral policy over a blind policy that determines referrals using the automation and human-performance models but not based on the observed data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04248v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kesav Kaza, Jerome Le Ny, Aditya Mahajan</dc:creator>
    </item>
    <item>
      <title>Samila: A Generative Art Generator</title>
      <link>https://arxiv.org/abs/2504.04298</link>
      <description>arXiv:2504.04298v1 Announce Type: cross 
Abstract: Generative art merges creativity with computation, using algorithms to produce aesthetic works. This paper introduces Samila, a Python-based generative art library that employs mathematical functions and randomness to create visually compelling compositions. The system allows users to control the generation process through random seeds, function selections, and projection modes, enabling the exploration of randomness and artistic expression. By adjusting these parameters, artists can create diverse compositions that reflect intentionality and unpredictability. We demonstrate that Samila's outputs are uniquely determined by two random generation seeds, making regeneration nearly impossible without both. Additionally, altering the point generation functions while preserving the seed produces artworks with distinct graphical characteristics, forming a visual family. Samila serves as both a creative tool for artists and an educational resource for teaching mathematical and programming concepts. It also provides a platform for research in generative design and computational aesthetics. Future developments could include AI-driven generation and aesthetic evaluation metrics to enhance creative control and accessibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04298v1</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sadra Sabouri, Sepand Haghighi, Elena Masrour</dc:creator>
    </item>
    <item>
      <title>FluentLip: A Phonemes-Based Two-stage Approach for Audio-Driven Lip Synthesis with Optical Flow Consistency</title>
      <link>https://arxiv.org/abs/2504.04427</link>
      <description>arXiv:2504.04427v1 Announce Type: cross 
Abstract: Generating consecutive images of lip movements that align with a given speech in audio-driven lip synthesis is a challenging task. While previous studies have made strides in synchronization and visual quality, lip intelligibility and video fluency remain persistent challenges. This work proposes FluentLip, a two-stage approach for audio-driven lip synthesis, incorporating three featured strategies. To improve lip synchronization and intelligibility, we integrate a phoneme extractor and encoder to generate a fusion of audio and phoneme information for multimodal learning. Additionally, we employ optical flow consistency loss to ensure natural transitions between image frames. Furthermore, we incorporate a diffusion chain during the training of Generative Adversarial Networks (GANs) to improve both stability and efficiency. We evaluate our proposed FluentLip through extensive experiments, comparing it with five state-of-the-art (SOTA) approaches across five metrics, including a proposed metric called Phoneme Error Rate (PER) that evaluates lip pose intelligibility and video fluency. The experimental results demonstrate that our FluentLip approach is highly competitive, achieving significant improvements in smoothness and naturalness. In particular, it outperforms these SOTA approaches by approximately $\textbf{16.3%}$ in Fr\'echet Inception Distance (FID) and $\textbf{35.2%}$ in PER.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04427v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shiyan Liu, Rui Qu, Yan Jin</dc:creator>
    </item>
    <item>
      <title>Autono: ReAct-Based Highly Robust Autonomous Agent Framework</title>
      <link>https://arxiv.org/abs/2504.04650</link>
      <description>arXiv:2504.04650v1 Announce Type: cross 
Abstract: This paper proposes a highly robust autonomous agent framework based on the ReAct paradigm, designed to solve complex tasks through adaptive decision making and multi-agent collaboration. Unlike traditional frameworks that rely on fixed workflows generated by LLM-based planners, this framework dynamically generates next actions during agent execution based on prior trajectories, thereby enhancing its robustness. To address potential termination issues caused by adaptive execution paths, I propose a timely abandonment strategy incorporating a probabilistic penalty mechanism. For multi-agent collaboration, I introduce a memory transfer mechanism that enables shared and dynamically updated memory among agents. The framework's innovative timely abandonment strategy dynamically adjusts the probability of task abandonment via probabilistic penalties, allowing developers to balance conservative and exploratory tendencies in agent execution strategies by tuning hyperparameters. This significantly improves adaptability and task execution efficiency in complex environments. Additionally, agents can be extended through external tool integration, supported by modular design and MCP protocol compatibility, which enables flexible action space expansion. Through explicit division of labor, the multi-agent collaboration mechanism enables agents to focus on specific task components, thereby significantly improving execution efficiency and quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04650v1</guid>
      <category>cs.MA</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihao Wu</dc:creator>
    </item>
    <item>
      <title>Simulating Persuasive Dialogues on Meat Reduction with Generative Agents</title>
      <link>https://arxiv.org/abs/2504.04872</link>
      <description>arXiv:2504.04872v1 Announce Type: cross 
Abstract: Meat reduction benefits human and planetary health, but social norms keep meat central in shared meals. To date, the development of communication strategies that promote meat reduction while minimizing social costs has required the costly involvement of human participants at each stage of the process. We present work in progress on simulating multi-round dialogues on meat reduction between Generative Agents based on large language models (LLMs). We measure our main outcome using established psychological questionnaires based on the Theory of Planned Behavior and additionally investigate Social Costs. We find evidence that our preliminary simulations produce outcomes that are (i) consistent with theoretical expectations; and (ii) valid when compared to data from previous studies with human participants. Generative agent-based models are a promising tool for identifying novel communication strategies on meat reduction-tailored to highly specific participant groups-to then be tested in subsequent studies with human participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04872v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georg Ahnert, Elena Wurth, Markus Strohmaier, Jutta Mata</dc:creator>
    </item>
    <item>
      <title>Surveying Professional Writers on AI: Limitations, Expectations, and Fears</title>
      <link>https://arxiv.org/abs/2504.05008</link>
      <description>arXiv:2504.05008v1 Announce Type: cross 
Abstract: The rapid development of AI-driven tools, particularly large language models (LLMs), is reshaping professional writing. Still, key aspects of their adoption such as languages support, ethics, and long-term impact on writers voice and creativity remain underexplored. In this work, we conducted a questionnaire (N = 301) and an interactive survey (N = 36) targeting professional writers regularly using AI. We examined LLM-assisted writing practices across 25+ languages, ethical concerns, and user expectations. The findings of the survey demonstrate important insights, reflecting upon the importance of: LLMs adoption for non-English speakers; the degree of misinformation, domain and style adaptation; usability and key features of LLMs. These insights can guide further development, benefiting both writers and a broader user base.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05008v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasiia Ivanova, Natalia Fedorova, Sergey Tilga, Ekaterina Artemova</dc:creator>
    </item>
    <item>
      <title>A moving target in AI-assisted decision-making: Dataset shift, model updating, and the problem of update opacity</title>
      <link>https://arxiv.org/abs/2504.05210</link>
      <description>arXiv:2504.05210v1 Announce Type: cross 
Abstract: Machine learning (ML) systems are vulnerable to performance decline over time due to dataset shift. To address this problem, experts often suggest that ML systems should be regularly updated to ensure ongoing performance stability. Some scholarly literature has begun to address the epistemic and ethical challenges associated with different updating methodologies. Thus far, however, little attention has been paid to the impact of model updating on the ML-assisted decision-making process itself, particularly in the AI ethics and AI epistemology literatures. This article aims to address this gap in the literature. It argues that model updating introduces a new sub-type of opacity into ML-assisted decision-making -- update opacity -- that occurs when users cannot understand how or why an update has changed the reasoning or behaviour of an ML system. This type of opacity presents a variety of distinctive epistemic and safety concerns that available solutions to the black box problem in ML are largely ill-equipped to address. A variety of alternative strategies may be developed or pursued to address the problem of update opacity more directly, including bi-factual explanations, dynamic model reporting, and update compatibility. However, each of these strategies presents its own risks or carries significant limitations. Further research will be needed to address the epistemic and safety concerns associated with model updating and update opacity going forward.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05210v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10676-025-09829-2</arxiv:DOI>
      <arxiv:journal_reference>Ethics and Information Technology 27(2): 20 (2025)</arxiv:journal_reference>
      <dc:creator>Joshua Hatherley</dc:creator>
    </item>
    <item>
      <title>Untangling Rhetoric, Pathos, and Aesthetics in Data Visualization</title>
      <link>https://arxiv.org/abs/2304.10540</link>
      <description>arXiv:2304.10540v4 Announce Type: replace 
Abstract: In contemporary discourse, logos (reason) and, more recently, ethos (credibility) in data communication have been discussed extensively. While the concept of Pathos has enjoyed great interest in the VIS community over the past few years, its connection to similar but relevant concepts like aesthetics and rhetoric remains unexplored. In this paper, we provide definitions of these terms and explore their overlaps and differences in light of their historical development. Examining the historical perspective offers a deeper understanding of how these approaches in science and philosophy have evolved over time, offering a more comprehensive embedding into the design process and its role within it. Drawing from Campbell's seven circumstances, we illustrate how pathos is being used as a rhetorical device in data visualizations today, at times inadvertently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.10540v4</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Verena Ingrid Prantl, Torsten Moeller, Laura Koesten</dc:creator>
    </item>
    <item>
      <title>SPARC: Shared Perspective with Avatar Distortion for Remote Collaboration in VR</title>
      <link>https://arxiv.org/abs/2406.05209</link>
      <description>arXiv:2406.05209v2 Announce Type: replace 
Abstract: Telepresence VR systems allow for face-to-face communication, promoting the feeling of presence and understanding of nonverbal cues. However, when discussing virtual 3D objects, limitations to presence and communication cause deictic gestures to lose meaning due to disparities in orientation. Current approaches use shared perspective, and avatar overlap to restore these references, which cause occlusions and discomfort that worsen when multiple users participate. We introduce a new approach to shared perspective in multi-user collaboration where the avatars are not co-located. Each person sees the others' avatars at their positions around the workspace while having a first-person view of the workspace. Whenever a user manipulates an object, others will see his/her arms stretching to reach that object in their perspective. SPARC combines a shared orientation and supports nonverbal communication, minimizing occlusions. We conducted a user study (n=18) to understand how the novel approach impacts task performance and workspace awareness. We found evidence that SPARC is more efficient and less mentally demanding than life-like settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05209v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jo\~ao Sim\~oes, Anderson Maciel, Catarina Moreira, Joaquim Jorge</dc:creator>
    </item>
    <item>
      <title>Reflections on Teaching Data Storytelling at the Journalism School</title>
      <link>https://arxiv.org/abs/2408.04386</link>
      <description>arXiv:2408.04386v2 Announce Type: replace 
Abstract: The integration of data visualization in journalism has catalyzed the growth of data storytelling in recent years. Today, it is increasingly common for journalism schools to incorporate data visualization into their curricula. However, the approach to teaching data visualization in journalism schools can diverge significantly from that in computer science or design schools, influenced by the varied backgrounds of students and the distinct value systems inherent to these disciplines. This paper reviews my experience and reflections on teaching data-driven storytelling in a journalism school in Shanghai, China. To begin with, I discuss three prominent characteristics of journalism education (i.e., students' lack of quantitative literacy, the tension between humanism and technocentrism, and the high requirements for content professionalism) that pose challenges for course design and teaching. Then, for each challenge, I share firsthand teaching experiences and discuss corresponding approaches for teaching, such as trying to put visualization into a news context and finding commonality between data-driven storytelling and traditional storytelling. Overall, this paper aims to provide reference and inspiration for instructors who are teaching data visualization and data-driven storytelling to students with non-technical backgrounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04386v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyu Lan</dc:creator>
    </item>
    <item>
      <title>A multidimensional measurement of photorealistic avatar quality of experience</title>
      <link>https://arxiv.org/abs/2411.09066</link>
      <description>arXiv:2411.09066v3 Announce Type: replace 
Abstract: Photorealistic avatars are human avatars that look, move, and talk like real people. The performance of photorealistic avatars has significantly improved recently based on objective metrics such as PSNR, SSIM, LPIPS, FID, and FVD. However, recent photorealistic avatar publications do not provide subjective tests of the avatars to measure human usability factors. We provide an open source test framework to subjectively measure photorealistic avatar performance in ten dimensions: realism, trust, comfortableness using, comfortableness interacting with, appropriateness for work, creepiness, formality, affinity, resemblance to the person, and emotion accuracy. Using telecommunication scenarios, we show that the correlation of nine of these subjective metrics with PSNR, SSIM, LPIPS, FID, and FVD is weak, and moderate for emotion accuracy. The crowdsourced subjective test framework is highly reproducible and accurate when compared to a panel of experts. We analyze a wide range of avatars from photorealistic to cartoon-like and show that some photorealistic avatars are approaching real video performance based on these dimensions. We also find that for avatars above a certain level of realism, eight of these measured dimensions are strongly correlated. This means that avatars that are not as realistic as real video will have lower trust, comfortableness using, comfortableness interacting with, appropriateness for work, formality, and affinity, and higher creepiness compared to real video. In addition, because there is a strong linear relationship between avatar affinity and realism, there is no uncanny valley effect for photorealistic avatars in the telecommunication scenario. We suggest several extensions of this test framework for future work and discuss design implications for telecommunication systems. The test framework is available at https://github.com/microsoft/P.910.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09066v3</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ross Cutler, Babak Naderi, Vishak Gopal, Dharmendar Palle</dc:creator>
    </item>
    <item>
      <title>Light My Way: Developing and Exploring a Multimodal Interface to Assist People With Visual Impairments to Exit Highly Automated Vehicles</title>
      <link>https://arxiv.org/abs/2501.11801</link>
      <description>arXiv:2501.11801v3 Announce Type: replace 
Abstract: The introduction of Highly Automated Vehicles (HAVs) has the potential to increase the independence of blind and visually impaired people (BVIPs). However, ensuring safety and situation awareness when exiting these vehicles in unfamiliar environments remains challenging. To address this, we conducted an interactive workshop with N=5 BVIPs to identify their information needs when exiting an HAV and evaluated three prior-developed low-fidelity prototypes. The insights from this workshop guided the development of PathFinder, a multimodal interface combining visual, auditory, and tactile modalities tailored to BVIP's unique needs. In a three-factorial within-between-subject study with N=16 BVIPs, we evaluated PathFinder against an auditory-only baseline in urban and rural scenarios. PathFinder significantly reduced mental demand and maintained high perceived safety in both scenarios, while the auditory baseline led to lower perceived safety in the urban scenario compared to the rural one. Qualitative feedback further supported PathFinder's effectiveness in providing spatial orientation during exiting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11801v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713454</arxiv:DOI>
      <dc:creator>Luca-Maxim Meinhardt, Lina Weilke, Maryam Elhaidary, Julia von Abel, Paul Fink, Michael Rietzler, Mark Colley, Enrico Rukzio</dc:creator>
    </item>
    <item>
      <title>"It Felt Like I Was Left in the Dark": Exploring Information Needs and Design Opportunities for Family Caregivers of Older Adult Patients in Critical Care Settings</title>
      <link>https://arxiv.org/abs/2502.05115</link>
      <description>arXiv:2502.05115v2 Announce Type: replace 
Abstract: Older adult patients constitute a rapidly growing subgroup of Intensive Care Unit (ICU) patients. In these situations, their family caregivers are expected to represent the unconscious patients to access and interpret patients' medical information. However, caregivers currently have to rely on overloaded clinicians for information updates and typically lack the health literacy to understand complex medical information. Our project aims to explore the information needs of caregivers of ICU older adult patients, from which we can propose design opportunities to guide future AI systems. The project begins with formative interviews with 11 caregivers to identify their challenges in accessing and interpreting medical information; From these findings, we then synthesize design requirements and propose an AI system prototype to cope with caregivers' challenges. The system prototype has two key features: a timeline visualization to show the AI extracted and summarized older adult patients' key medical events; and an LLM-based chatbot to provide context-aware informational support. We conclude our paper by reporting on the follow-up user evaluation of the system and discussing future AI-based systems for ICU caregivers of older adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05115v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shihan Fu, Bingsheng Yao, Smit Desai, Yuqi Hu, Yuling Sun, Samantha Stonbraker, Yanjun Gao, Elizabeth M. Goldberg, Dakuo Wang</dc:creator>
    </item>
    <item>
      <title>UXAgent: An LLM Agent-Based Usability Testing Framework for Web Design</title>
      <link>https://arxiv.org/abs/2502.12561</link>
      <description>arXiv:2502.12561v3 Announce Type: replace 
Abstract: Usability testing is a fundamental yet challenging (e.g., inflexible to iterate the study design flaws and hard to recruit study participants) research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM-Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human subject study. Our system features an LLM-Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The results are shown in qualitative (e.g., interviewing how an agent thinks ), quantitative (e.g., # of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of LLM Agent-assisted UX study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12561v3</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3719729</arxiv:DOI>
      <dc:creator>Yuxuan Lu, Bingsheng Yao, Hansu Gu, Jing Huang, Jessie Wang, Yang Li, Jiri Gesi, Qi He, Toby Jia-Jun Li, Dakuo Wang</dc:creator>
    </item>
    <item>
      <title>Superhuman Game AI Disclosure: Expertise and Context Moderate Effects on Trust and Fairness</title>
      <link>https://arxiv.org/abs/2503.15514</link>
      <description>arXiv:2503.15514v2 Announce Type: replace 
Abstract: As artificial intelligence surpasses human performance in select tasks, disclosing superhuman capabilities poses distinct challenges for fairness, accountability, and trust. However, the impact of such disclosures on diverse user attitudes and behaviors remains unclear, particularly concerning potential negative reactions like discouragement or overreliance. This paper investigates these effects by utilizing Persona Cards: a validated, standardized set of synthetic personas designed to simulate diverse user reactions and fairness perspectives. We conducted an ethics board-approved study (N=32), utilizing these personas to investigate how capability disclosure influenced behaviors with a superhuman game AI in competitive StarCraft II scenarios. Our results reveal transparency is double-edged: while disclosure could alleviate suspicion, it also provoked frustration and strategic defeatism among novices in cooperative scenarios, as well as overreliance in competitive contexts. Experienced and competitive players interpreted disclosure as confirmation of an unbeatable opponent, shifting to suboptimal goals. We release the Persona Cards Dataset, including profiles, prompts, interaction logs, and protocols, to foster reproducible research into human alignment AI design. This work demonstrates that transparency is not a cure-all; successfully leveraging disclosure to enhance trust and accountability requires careful tailoring to user characteristics, domain norms, and specific fairness objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15514v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaymari Chua, Chen Wang, Lina Yao</dc:creator>
    </item>
    <item>
      <title>Calculating Connection vs. Risk: Understanding How Youth Negotiate Digital Privacy and Security with Peers Online</title>
      <link>https://arxiv.org/abs/2503.22993</link>
      <description>arXiv:2503.22993v2 Announce Type: replace 
Abstract: Youth, while tech-savvy and highly active on social media, are still vulnerable to online privacy and security risks. Therefore, it is critical to understand how they negotiate and manage social connections versus protecting themselves in online contexts. In this work, we conducted a thematic analysis of 1,318 private conversations on Instagram from 149 youth aged 13-21 to understand the digital privacy and security topics they discussed, if and how they engaged in risky privacy behaviors, and how they balanced the benefits and risks (i.e., privacy calculus) of making these decisions. Overall, youth were forthcoming when broaching a wide range of topics on digital privacy and security, ranging from password management and account access challenges to shared experiences of being victims of privacy risks. However, they also openly engaged in risky behaviors, such as sharing personal account information with peers and even perpetrating privacy and security risks against others. Nonetheless, we found many of these behaviors could be explained by the unique "privacy calculus" of youth, where they often prioritized social benefits over potential risks; for instance, youth often shared account credentials with peers to foster social connection and affirmation. As such, we provide a nuanced understanding of youth decision-making regarding digital security and privacy, highlighting both positive behaviors, tensions, and points of concern. We encourage future research to continue to challenge the potentially untrue narratives regarding youth and their digital privacy and security to unpack the nuance of their privacy calculus that may differ from that of adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22993v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the ACM on Human-Computer Interaction 2025</arxiv:journal_reference>
      <dc:creator>Mamtaj Akter, Jinkyung Katie Park, Campbell Headrick, Xinru Page, Pamela J. Wisniewski</dc:creator>
    </item>
    <item>
      <title>Moving Beyond Parental Control toward Community-based Approaches to Adolescent Online Safety</title>
      <link>https://arxiv.org/abs/2503.22995</link>
      <description>arXiv:2503.22995v2 Announce Type: replace 
Abstract: In this position paper, we discuss the paradigm shift that moves away from parental mediation approaches toward collaborative approaches to promote adolescents' online safety. We present empirical studies that highlight the limitations of traditional parental control models and advocate for collaborative, community-driven solutions that prioritize teen empowerment. Specifically, we explore how extending oversight beyond the immediate family to include trusted community members can provide crucial support for teens in managing their online lives. We discuss the potential benefits and challenges of this expanded approach, emphasizing the importance of granular privacy controls and reciprocal support within these networks. Finally, we pose open questions for the research community to consider during the workshop, focusing on the design of "teen-centered" online safety solutions that foster autonomy, awareness, and self-regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22995v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Mobile Technology and Teens Workshop of the 2025 CHI Conference on Human Factors in Computing Systems</arxiv:journal_reference>
      <dc:creator>Mamtaj Akter, Jinkyung Katie Park, Pamela J. Wisniewski</dc:creator>
    </item>
    <item>
      <title>Design of AI-Powered Tool for Self-Regulation Support in Programming Education</title>
      <link>https://arxiv.org/abs/2504.03068</link>
      <description>arXiv:2504.03068v2 Announce Type: replace 
Abstract: Large Language Model (LLM) tools have demonstrated their potential to deliver high-quality assistance by providing instant, personalized feedback that is crucial for effective programming education. However, many of these tools operate independently from institutional Learning Management Systems, which creates a significant disconnect. This isolation limits the ability to leverage learning materials and exercise context for generating tailored, context-aware feedback. Furthermore, previous research on self-regulated learning and LLM support mainly focused on knowledge acquisition, not the development of important self-regulation skills. To address these challenges, we developed CodeRunner Agent, an LLM-based programming assistant that integrates the CodeRunner, a student-submitted code executing and automated grading plugin in Moodle. CodeRunner Agent empowers educators to customize AI-generated feedback by incorporating detailed context from lecture materials, programming questions, student answers, and execution results. Additionally, it enhances students' self-regulated learning by providing strategy-based AI responses. This integrated, context-aware, and skill-focused approach offers promising avenues for data-driven improvements in programming education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03068v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huiyong Li, Boxuan Ma</dc:creator>
    </item>
    <item>
      <title>When Prompting Fails to Sway: Inertia in Moral and Value Judgments of Large Language Models</title>
      <link>https://arxiv.org/abs/2408.09049</link>
      <description>arXiv:2408.09049v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) exhibit non-deterministic behavior, and prompting has emerged as a primary method for steering their outputs toward desired directions. One popular strategy involves assigning a specific "persona" to the model to induce more varied and context-sensitive responses, akin to the diversity found in human perspectives. However, contrary to the expectation that persona-based prompting would yield a wide range of opinions, our experiments demonstrate that LLMs maintain consistent value orientations. In particular, we observe a persistent inertia in their responses, where certain moral and value dimensions, especially harm avoidance and fairness, remain distinctly skewed in one direction despite varied persona settings. To investigate this phenomenon systematically, use role-play at scale, which combines randomized, diverse persona prompts with a macroscopic trend analysis of model outputs. Our findings highlight the strong internal biases and value preferences in LLMs, underscoring the need for careful scrutiny and potential adjustment of these models to ensure balanced and equitable applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09049v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bruce W. Lee, Yeongheon Lee, Hyunsoo Cho</dc:creator>
    </item>
    <item>
      <title>Exploring Gaze Pattern Differences Between Autistic and Neurotypical Children: Clustering, Visualisation, and Prediction</title>
      <link>https://arxiv.org/abs/2409.11744</link>
      <description>arXiv:2409.11744v3 Announce Type: replace-cross 
Abstract: Autism Spectrum Disorder (ASD) affects children's social and communication abilities, with eye-tracking widely used to identify atypical gaze patterns. While unsupervised clustering can automate the creation of areas of interest for gaze feature extraction, the use of internal cluster validity indices, like Silhouette Coefficient, to distinguish gaze pattern differences between ASD and typically developing (TD) children remains underexplored. We explore whether internal cluster validity indices can distinguish ASD from TD children. Specifically, we apply seven clustering algorithms to gaze points and extract 63 internal cluster validity indices to reveal correlations with ASD diagnosis. Using these indices, we train predictive models for ASD diagnosis. Experiments on three datasets demonstrate high predictive accuracy (81\% AUC), validating the effectiveness of these indices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11744v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Weiyan Shi, Haihong Zhang, Wei Wang, Kenny Tsu Wei Choo</dc:creator>
    </item>
    <item>
      <title>Identity-related Speech Suppression in Generative AI Content Moderation</title>
      <link>https://arxiv.org/abs/2409.13725</link>
      <description>arXiv:2409.13725v2 Announce Type: replace-cross 
Abstract: Automated content moderation has long been used to help identify and filter undesired user-generated content online. Generative AI systems now use such filters to keep undesired generated content from being created by or shown to users. From classrooms to Hollywood, as generative AI is increasingly used for creative or expressive text generation, whose stories will these technologies allow to be told, and whose will they suppress? In this paper, we define and introduce measures of speech suppression, focusing on speech related to different identity groups incorrectly filtered by a range of content moderation APIs. Using both short-form, user-generated datasets traditional in content moderation and longer generative AI-focused data, including two datasets we introduce in this work, we create a benchmark for measurement of speech suppression for nine identity groups. Across one traditional and four generative AI-focused automated content moderation services tested, we find that identity-related speech is more likely to be incorrectly suppressed than other speech. We find differences in identity-related speech suppression for traditional versus generative AI data, with APIs performing better on generative AI data but worse on longer text instances, and by identity, with identity-specific reasons for incorrect flagging behavior. Overall, we find that on traditional short-form data incorrectly suppressed speech is likely to be political, while for generative AI creative data it is likely to be television violence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13725v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Oghenefejiro Isaacs Anigboro, Charlie M. Crawford, Grace Proebsting, Dana\"e Metaxa, Sorelle A. Friedler</dc:creator>
    </item>
    <item>
      <title>Speech to Reality: On-Demand Production using Natural Language, 3D Generative AI, and Discrete Robotic Assembly</title>
      <link>https://arxiv.org/abs/2409.18390</link>
      <description>arXiv:2409.18390v4 Announce Type: replace-cross 
Abstract: We present a system that transforms speech into physical objects by combining 3D generative Artificial Intelligence with robotic assembly. The system leverages natural language input to make design and manufacturing more accessible, enabling individuals without expertise in 3D modeling or robotic programming to create physical objects. We propose utilizing discrete robotic assembly of lattice-based voxel components to address the challenges of using generative AI outputs in physical production, such as design variability, fabrication speed, structural integrity, and material waste. The system interprets speech to generate 3D objects, discretizes them into voxel components, computes an optimized assembly sequence, and generates a robotic toolpath. The results are demonstrated through the assembly of various objects, ranging from chairs to shelves, which are prompted via speech and realized within 5 minutes using a 6-axis robotic arm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18390v4</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Htet Kyaw, Se Hwan Jeon, Miana Smith, Neil Gershenfeld</dc:creator>
    </item>
    <item>
      <title>Pattern Analogies: Learning to Perform Programmatic Image Edits by Analogy</title>
      <link>https://arxiv.org/abs/2412.12463</link>
      <description>arXiv:2412.12463v2 Announce Type: replace-cross 
Abstract: Pattern images are everywhere in the digital and physical worlds, and tools to edit them are valuable. But editing pattern images is tricky: desired edits are often programmatic: structure-aware edits that alter the underlying program which generates the pattern. One could attempt to infer this underlying program, but current methods for doing so struggle with complex images and produce unorganized programs that make editing tedious. In this work, we introduce a novel approach to perform programmatic edits on pattern images. By using a pattern analogy -- a pair of simple patterns to demonstrate the intended edit -- and a learning-based generative model to execute these edits, our method allows users to intuitively edit patterns. To enable this paradigm, we introduce SplitWeave, a domain-specific language that, combined with a framework for sampling synthetic pattern analogies, enables the creation of a large, high-quality synthetic training dataset. We also present TriFuser, a Latent Diffusion Model (LDM) designed to overcome critical issues that arise when naively deploying LDMs to this task. Extensive experiments on real-world, artist-sourced patterns reveals that our method faithfully performs the demonstrated edit while also generalizing to related pattern styles beyond its training distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12463v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aditya Ganeshan, Thibault Groueix, Paul Guerrero, Radom\'ir M\v{e}ch, Matthew Fisher, Daniel Ritchie</dc:creator>
    </item>
    <item>
      <title>Holoview: Interactive 3D visualization of medical data in AR</title>
      <link>https://arxiv.org/abs/2501.08736</link>
      <description>arXiv:2501.08736v3 Announce Type: replace-cross 
Abstract: We introduce HoloView, an innovative augmented reality (AR) system that enhances interactive learning of human anatomical structures through immersive visualization. Combining advanced rendering techniques with intuitive gesture-based interactions, HoloView provides a comprehensive technical solution for medical education. The system architecture features a distributed rendering pipeline that offloads stereoscopic computations to a remote server, optimizing performance and enabling high-quality visualization on less powerful devices. To prioritize visual quality in the user's direct line of sight while reducing computational load, we implement foveated rendering optimization, enhancing the immersive experience. Additionally, a hybrid surface-volume rendering technique is used to achieve faster rendering speeds without sacrificing visual fidelity. Complemented by a carefully designed user interface and gesture-based interaction system, HoloView allows users to naturally manipulate holographic content and seamlessly navigate the learning environment. HoloView significantly facilitates anatomical structure visualization and promotes an engaging, user-centric learning experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08736v3</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pankaj Kaushik, Anshul Goswami, Ojaswa Sharma</dc:creator>
    </item>
    <item>
      <title>CowPilot: A Framework for Autonomous and Human-Agent Collaborative Web Navigation</title>
      <link>https://arxiv.org/abs/2501.16609</link>
      <description>arXiv:2501.16609v3 Announce Type: replace-cross 
Abstract: While much work on web agents emphasizes the promise of autonomously performing tasks on behalf of users, in reality, agents often fall short on complex tasks in real-world contexts and modeling user preference. This presents an opportunity for humans to collaborate with the agent and leverage the agent's capabilities effectively. We propose CowPilot, a framework supporting autonomous as well as human-agent collaborative web navigation, and evaluation across task success and task efficiency. CowPilot reduces the number of steps humans need to perform by allowing agents to propose next steps, while users are able to pause, reject, or take alternative actions. During execution, users can interleave their actions with the agent by overriding suggestions or resuming agent control when needed. We conducted case studies on five common websites and found that the human-agent collaborative mode achieves the highest success rate of 95% while requiring humans to perform only 15.2% of the total steps. Even with human interventions during task execution, the agent successfully drives up to half of task success on its own. CowPilot can serve as a useful tool for data collection and agent evaluation across websites, which we believe will enable research in how users and agents can work together. Video demonstrations are available at https://oaishi.github.io/cowpilot.html</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16609v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faria Huq, Zora Zhiruo Wang, Frank F. Xu, Tianyue Ou, Shuyan Zhou, Jeffrey P. Bigham, Graham Neubig</dc:creator>
    </item>
  </channel>
</rss>
