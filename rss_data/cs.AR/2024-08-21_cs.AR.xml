<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Aug 2024 01:37:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A General-Purpose Device for Interaction with LLMs</title>
      <link>https://arxiv.org/abs/2408.10230</link>
      <description>arXiv:2408.10230v1 Announce Type: new 
Abstract: This paper investigates integrating large language models (LLMs) with advanced hardware, focusing on developing a general-purpose device designed for enhanced interaction with LLMs. Initially, we analyze the current landscape, where virtual assistants and LLMs are reshaping human-technology interactions, highlighting pivotal advancements and setting the stage for a new era of intelligent hardware. Despite substantial progress in LLM technology, a significant gap exists in hardware development, particularly concerning scalability, efficiency, affordability, and multimodal capabilities. This disparity presents both challenges and opportunities, underscoring the need for hardware that is not only powerful but also versatile and capable of managing the sophisticated demands of modern computation. Our proposed device addresses these needs by emphasizing scalability, multimodal data processing, enhanced user interaction, and privacy considerations, offering a comprehensive platform for LLM integration in various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10230v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiajun Xu, Qun Wang, Yuhang Cao, Baitao Zeng, Sicheng Liu</dc:creator>
    </item>
    <item>
      <title>FPCA: Field-Programmable Pixel Convolutional Array for Extreme-Edge Intelligence</title>
      <link>https://arxiv.org/abs/2408.10233</link>
      <description>arXiv:2408.10233v1 Announce Type: new 
Abstract: The rapid advancement of neural network applications necessitates hardware that not only accelerates computation but also adapts efficiently to dynamic processing requirements. While processing-in-pixel has emerged as a promising solution to overcome the bottlenecks of traditional architectures at the extreme-edge, existing implementations face limitations in reconfigurability and scalability due to their static nature and inefficient area usage. Addressing these challenges, we present a novel architecture that significantly enhances the capabilities of processing-in-pixel for convolutional neural networks. Our design innovatively integrates non-volatile memory (NVM) with novel unit pixel circuit design, enabling dynamic reconfiguration of synaptic weights, kernel size, channel size and stride size. Thus offering unprecedented flexibility and adaptability. With using a separate die for pixel circuit and storing synaptic weights, our circuit achieves a substantial reduction in the required area per pixel thereby increasing the density and scalability of the pixel array. Simulation results demonstrate dot product operations of the circuit, the non-linearity of its analog output and a novel bucket-select curvefit model is proposed to capture it. This work not only addresses the limitations of current in-pixel computing approaches but also opens new avenues for developing more efficient, flexible, and scalable neural network hardware, paving the way for advanced AI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10233v1</guid>
      <category>cs.AR</category>
      <category>eess.IV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihan Yin, Akhilesh Jaiswal</dc:creator>
    </item>
    <item>
      <title>TrIM: Triangular Input Movement Systolic Array for Convolutional Neural Networks -- Part II: Architecture and Hardware Implementation</title>
      <link>https://arxiv.org/abs/2408.10243</link>
      <description>arXiv:2408.10243v1 Announce Type: new 
Abstract: Modern hardware architectures for Convolutional Neural Networks (CNNs), other than targeting high performance, aim at dissipating limited energy. Reducing the data movement cost between the computing cores and the memory is a way to mitigate the energy consumption. Systolic arrays are suitable architectures to achieve this objective: they use multiple processing elements that communicate each other to maximize data utilization, based on proper dataflows like the weight stationary and row stationary. Motivated by this, we have proposed TrIM, an innovative dataflow based on a triangular movement of inputs, and capable to reduce the number of memory accesses by one order of magnitude when compared to state-of-the-art systolic arrays. In this paper, we present a TrIM-based hardware architecture for CNNs. As a showcase, the accelerator is implemented onto a Field Programmable Gate Array (FPGA) to execute the VGG-16 CNN. The architecture achieves a peak throughput of 453.6 Giga Operations per Second, outperforming a state-of-the-art row stationary systolic array by ~5.1x in terms of memory accesses, and being up to ~12.2x more energy-efficient than other FPGA accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10243v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristian Sestito, Shady Agwa, Themis Prodromakis</dc:creator>
    </item>
    <item>
      <title>Are LLMs Any Good for High-Level Synthesis?</title>
      <link>https://arxiv.org/abs/2408.10428</link>
      <description>arXiv:2408.10428v1 Announce Type: new 
Abstract: The increasing complexity and demand for faster, energy-efficient hardware designs necessitate innovative High-Level Synthesis (HLS) methodologies. This paper explores the potential of Large Language Models (LLMs) to streamline or replace the HLS process, leveraging their ability to understand natural language specifications and refactor code. We survey the current research and conduct experiments comparing Verilog designs generated by a standard HLS tool (Vitis HLS) with those produced by LLMs translating C code or natural language specifications. Our evaluation focuses on quantifying the impact on performance, power, and resource utilization, providing an assessment of the efficiency of LLM-based approaches. This study aims to illuminate the role of LLMs in HLS, identifying promising directions for optimized hardware design in applications such as AI acceleration, embedded systems, and high-performance computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10428v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchao Liao, Tosiron Adegbija, Roman Lysecky</dc:creator>
    </item>
    <item>
      <title>System-Level Design Space Exploration for High-Level Synthesis under End-to-End Latency Constraints</title>
      <link>https://arxiv.org/abs/2408.10431</link>
      <description>arXiv:2408.10431v1 Announce Type: new 
Abstract: Many modern embedded systems have end-to-end (EtoE) latency constraints that necessitate precise timing to ensure high reliability and functional correctness. The combination of High-Level Synthesis (HLS) and Design Space Exploration (DSE) enables the rapid generation of embedded systems using various constraints/directives to find Pareto-optimal configurations. Current HLS DSE approaches often address latency by focusing on individual components, without considering the EtoE latency during the system-level optimization process. However, to truly optimize the system under EtoE latency, we need a holistic approach that analyzes individual system components' timing constraints in the context of how the different components interact and impact the overall design. This paper presents a novel system-level HLS DSE approach, called EtoE-DSE, that accommodates EtoE latency and variable timing constraints for complex multi-component application-specific embedded systems. EtoE-DSE employs a latency estimation model and a pathfinding algorithm to identify and estimate the EtoE latency for paths between any endpoints. It also uses a frequency-based segmentation process to segment and prune the design space, alongside a latency-constrained optimization algorithm for efficiently and accurately exploring the system-level design space. We evaluate our approach using a real-world use case of an autonomous driving subsystem compared to the state-of-the-art in HLS DSE. We show that our approach yields substantially better optimization results than prior DSE approaches, improving the quality of results by up to 89.26%, while efficiently identifying Pareto-optimal configurations in terms of energy and area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10431v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchao Liao, Tosiron Adegbija, Roman Lysecky</dc:creator>
    </item>
    <item>
      <title>Design and Implementation of a Takum Arithmetic Hardware Codec in VHDL</title>
      <link>https://arxiv.org/abs/2408.10594</link>
      <description>arXiv:2408.10594v1 Announce Type: new 
Abstract: The takum machine number format has been recently proposed as an enhancement over the posit number format, which is considered a promising alternative to the IEEE 754 floating-point standard. Takums retain the useful posit properties, but feature a novel exponent coding scheme that yields more precision for small and large magnitude numbers. Takum's dynamic range is larger and bounded, as reflected in its name, derived from the Icelandic 'takmarka{\dh} umfang', meaning 'limited range'. Consequently, the selection of bit string lengths becomes determined solely by precision requirements and independent of dynamic range considerations. Takum is defined in both a logarithmic number system (LNS) format and a traditional floating-point format.
  This paper presents the design and implementation of a hardware codec for both the logarithmic and floating-point takum formats. The design primarily focuses on the codec, as both formats share a common internal arithmetic representation. Non-essential aspects of current posit designs, such as fused or pipelined architectures and the choice of floating-point IP cores, are thus omitted. The proposed takum codec, implemented in VHDL, demonstrates near-optimal scalability and performance on an FPGA, matching or exceeding state-of-the-art posit codecs in terms of both latency and LUT utilisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10594v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laslo Hunhold</dc:creator>
    </item>
    <item>
      <title>Hardware Implementation of Projection-Aggregation Decoders for Reed-Muller Codes</title>
      <link>https://arxiv.org/abs/2408.10850</link>
      <description>arXiv:2408.10850v1 Announce Type: new 
Abstract: This paper presents the hardware implementation of two variants of projection-aggregation-based decoding of Reed-Muller (RM) codes, namely unique projection aggregation (UPA) and collapsed projection aggregation (CPA). Our study focuses on introducing hardware architectures for both UPA and CPA. Through thorough analysis and experimentation, we observe that the hardware implementation of UPA exhibits superior resource usage and reduced energy consumption compared to CPA for the vanilla IPA decoder. This finding underscores a critical insight: software optimizations, in isolation, may not necessarily translate into hardware cost-effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10850v1</guid>
      <category>cs.AR</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marzieh Hashemipour-Nazari, Andrea Nardi-Dei, Kees Goossens, Alexios Balatsoukas-Stimming</dc:creator>
    </item>
    <item>
      <title>Security Risks Due to Data Persistence in Cloud FPGA Platforms</title>
      <link>https://arxiv.org/abs/2408.10374</link>
      <description>arXiv:2408.10374v1 Announce Type: cross 
Abstract: The integration of Field Programmable Gate Arrays (FPGAs) into cloud computing systems has become commonplace. As the operating systems used to manage these systems evolve, special consideration must be given to DRAM devices accessible by FPGAs. These devices may hold sensitive data that can become inadvertently exposed to adversaries following user logout. Although addressed in some cloud FPGA environments, automatic DRAM clearing after process termination is not automatically included in popular FPGA runtime environments nor in most proposed cloud FPGA hypervisors. In this paper, we examine DRAM data persistence in AMD/Xilinx Alveo U280 nodes that are part of the Open Cloud Testbed (OCT). Our results indicate that DDR4 DRAM is not automatically cleared following user logout from an allocated node and subsequent node users can easily obtain recognizable data from the DRAM following node reallocation over 17 minutes later. This issue is particularly relevant for systems which support FPGA multi-tenancy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10374v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Zhehang Zhang, Bharadwaj Madabhushi, Sandip Kundu, Russell Tessier</dc:creator>
    </item>
    <item>
      <title>RowPress Vulnerability in Modern DRAM Chips</title>
      <link>https://arxiv.org/abs/2406.16153</link>
      <description>arXiv:2406.16153v2 Announce Type: replace 
Abstract: Memory isolation is a critical property for system reliability, security, and safety. We demonstrate RowPress, a DRAM read disturbance phenomenon different from the well-known RowHammer. RowPress induces bitflips by keeping a DRAM row open for a long period of time instead of repeatedly opening and closing the row. We experimentally characterize RowPress bitflips, showing their widespread existence in commodity off-the-shelf DDR4 DRAM chips. We demonstrate RowPress bitflips in a real system that already has RowHammer protection, and propose effective mitigation techniques that protect DRAM against both RowHammer and RowPress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16153v2</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haocong Luo, Ataberk Olgun, A. Giray Ya\u{g}l{\i}k\c{c}{\i}, Yahya Can Tu\u{g}rul, Steve Rhyner, Meryem Banu Cavlak, Jo\"el Lindegger, Mohammad Sadrosadati, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>NeuralMatrix: Compute the Entire Neural Networks with Linear Matrix Operations for Efficient Inference</title>
      <link>https://arxiv.org/abs/2305.14405</link>
      <description>arXiv:2305.14405v4 Announce Type: replace-cross 
Abstract: The inherent diversity of computation types within the deep neural network (DNN) models often requires a variety of specialized units in hardware processors, which limits computational efficiency, increasing both inference latency and power consumption, especially when the hardware processor needs to support and execute different neural networks. In this study, we introduce NeuralMatrix, which elastically transforms the computations of entire DNNs into linear matrix operations. This transformation allows seamless execution of various DNN models all with matrix operations and paves the way for running versatile DNN models with a single General Matrix Multiplication (GEMM) accelerator.Extensive experiments with both CNN and transformer-based models demonstrate the potential of NeuralMatrix to accurately and efficiently execute a wide range of DNN models, achieving 2.17-38.72 times computation efficiency (i.e., throughput per power) compared to CPUs, GPUs, and SoC platforms. This level of efficiency is usually only attainable with the accelerator designed for a specific neural network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14405v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiqi Sun, Siwei Ye, Jie Zhao, Xin He, Jianzhe Lin, Yiran Li, An Zou</dc:creator>
    </item>
  </channel>
</rss>
