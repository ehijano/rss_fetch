<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Jun 2025 04:00:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Mechanistic models for panel data: Analysis of ecological experiments with four interacting species</title>
      <link>https://arxiv.org/abs/2506.04508</link>
      <description>arXiv:2506.04508v1 Announce Type: new 
Abstract: In an ecological context, panel data arise when time series measurements are made on a collection of ecological processes. Each process may correspond to a spatial location for field data, or to an experimental ecosystem in a designed experiment. Statistical models for ecological panel data should capture the high levels of nonlinearity, stochasticity, and measurement uncertainty inherent in ecological systems. Furthermore, the system dynamics may depend on unobservable variables. This study applies iterated particle filtering techniques to explore new possibilities for likelihood-based statistical analysis of these complex systems. We analyze data from a mesocosm experiment in which two species of the freshwater planktonic crustacean genus, {\it Daphnia}, coexist with an alga and a fungal parasite. Time series data were collected on replicated mesocosms under six treatment conditions. Iterated filtering enables maximization of the likelihood for scientifically motivated nonlinear partially observed Markov process models, providing access to standard likelihood-based methods for parameter estimation, confidence intervals, hypothesis testing, model selection and diagnostics. This toolbox allows scientists to propose and evaluate scientifically motivated stochastic dynamic models for panel data, constrained only by the requirement to write code to simulate from the model and to specify a measurement distribution describing how the system state is observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04508v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Yang, Jesse Wheeler, Meghan A. Duffy, Aaron A. King, Edward L. Ionides</dc:creator>
    </item>
    <item>
      <title>Small area estimation of growing stock timber volume, basal area, mean stem diameter, and stem density for mountain forests in Austria</title>
      <link>https://arxiv.org/abs/2506.05043</link>
      <description>arXiv:2506.05043v1 Announce Type: new 
Abstract: Regression models were evaluated to estimate stand-level growing stock volume (GSV), quadratic mean diameter (QMD), basal area (BA), and stem density (N) in the Brixen im Thale forest district of Austria. Field measurements for GSV, QMD, and BA were collected on 146 inventory plots using a handheld mobile personal laser scanning system. Predictor variables were derived from airborne laser scanning (ALS)-derived normalized digital surface and terrain models. The objective was to generate stand-level estimates and associated uncertainty for GSV, QMD, BA, and N across 824 stands. A unit-level small area estimation framework was used to generate stand-level posterior predictive distributions by aggregating predictions from finer spatial scales. Both univariate and multivariate models, with and without spatially varying intercepts, were considered. Predictive performance was assessed via spatially blocked cross-validation, focusing on bias, accuracy, and precision. Despite exploratory analysis suggesting advantages of complex multivariate spatial models, simpler univariate spatial -- and in some cases, non-spatial -- models exhibited comparable predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05043v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arne Nothdurft, Valentin Sarkleti, Tobias Ofner-Graff, Andreas Tockner, Christoph Gollob, Tim Ritter, Ralf Kra{\ss}nitzer, Philip Svazek, Martin K\"uhmaier, Karl Stampfer, Andrew O. Finley</dc:creator>
    </item>
    <item>
      <title>A seasonal to decadal calibration of 1990-2100 eastern Canadian freshwater discharge simulations by observations, data models, and neural networks</title>
      <link>https://arxiv.org/abs/2506.05261</link>
      <description>arXiv:2506.05261v1 Announce Type: new 
Abstract: A configuration of the NCAR WRF-Hydro model was sought using well established data models to guide the initial hydrologic model setup, as well as a seasonal streamflow post-processing by neural networks. Discharge was simulated using an eastern Canadian river network at two-km resolution. The river network was taken from a digital elevation model that was made to conform to observed catchment boundaries. Perturbations of a subset of model parameters were examined with reference to streamflow from 25 gauged catchments during the 2019 warm season. A data model defines the similarity of modelled streamflow to observations, and improvements were found in about half the individual catchments. With reference to 183 gauged catchments (1990-2022), further improvements were obtained at monthly and annual scales by neural network post-processing that targets all catchments at once as well as individual catchments.
  This seasonal calibration was applied to uncoupled WRF-Hydro simulations for the 1990-2100 warming period. Historic and future forcing were provided, respectively, by a European Centre for Medium-Range Weather Forecasting reanalysis (ERA5), and by a WRF atmospheric model downscaling of a set of Coupled Model Intercomparison Project (CMIP) models, where the latter were also seasonally calibrated. Eastern Canadian freshwater discharge peaks at about 10$^5$ m$^3$ s$^{-1}$, and as previous studies have shown, there is a trend toward increasing low flows during the cold season and an earlier peak discharge in spring. By design, neural networks yield more precise estimates by compensating for different hydrologic process representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05261v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard E. Danielson, Minghong Zhang, Jo\"el Chass\'e, Will Perrie</dc:creator>
    </item>
    <item>
      <title>GARG-AML against Smurfing: A Scalable and Interpretable Graph-Based Framework for Anti-Money Laundering</title>
      <link>https://arxiv.org/abs/2506.04292</link>
      <description>arXiv:2506.04292v1 Announce Type: cross 
Abstract: Money laundering poses a significant challenge as it is estimated to account for 2%-5% of the global GDP. This has compelled regulators to impose stringent controls on financial institutions. One prominent laundering method for evading these controls, called smurfing, involves breaking up large transactions into smaller amounts. Given the complexity of smurfing schemes, which involve multiple transactions distributed among diverse parties, network analytics has become an important anti-money laundering tool. However, recent advances have focused predominantly on black-box network embedding methods, which has hindered their adoption in businesses. In this paper, we introduce GARG-AML, a novel graph-based method that quantifies smurfing risk through a single interpretable metric derived from the structure of the second-order transaction network of each individual node in the network. Unlike traditional methods, GARG-AML strikes an effective balance among computational efficiency, detection power and transparency, which enables its integration into existing AML workflows. To enhance its capabilities, we combine the GARG-AML score calculation with different tree-based methods and also incorporate the scores of the node's neighbours. An experimental evaluation on large-scale synthetic and open-source networks demonstrate that the GARG-AML outperforms the current state-of-the-art smurfing detection methods. By leveraging only the adjacency matrix of the second-order neighbourhood and basic network features, this work highlights the potential of fundamental network properties towards advancing fraud detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04292v1</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruno Deprez, Bart Baesens, Tim Verdonck, Wouter Verbeke</dc:creator>
    </item>
    <item>
      <title>Assessing parameter identifiability of a hemodynamics PDE model using spectral surrogates and dimension reduction</title>
      <link>https://arxiv.org/abs/2506.04538</link>
      <description>arXiv:2506.04538v1 Announce Type: cross 
Abstract: Computational inverse problems for biomedical simulators suffer from limited data and relatively high parameter dimensionality. This often requires sensitivity analysis, where parameters of the model are ranked based on their influence on the specific quantities of interest. This is especially important for simulators used to build medical digital twins, as the amount of data is typically limited. For expensive models, such as blood flow models, emulation is employed to expedite the simulation time. Parameter ranking and fixing using sensitivity analysis are often heuristic, though, and vary with the specific application or simulator used. The present study provides an innovative solution to this problem by leveraging polynomial chaos expansions (PCEs) for both multioutput global sensitivity analysis and formal parameter identifiability. For the former, we use dimension reduction to efficiently quantify time-series sensitivity of a one-dimensional pulmonary hemodynamics model. We consider both Windkessel and structured tree boundary conditions. We then use PCEs to construct profile-likelihood confidence intervals to formally assess parameter identifiability, and show how changes in experimental design improve identifiability. Our work presents a novel approach to determining parameter identifiability and leverages a common emulation strategy for enabling profile-likelihood analysis in problems governed by partial differential equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04538v1</guid>
      <category>q-bio.TO</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mitchel J. Colebank</dc:creator>
    </item>
    <item>
      <title>Unsupervised Machine Learning for Scientific Discovery: Workflow and Best Practices</title>
      <link>https://arxiv.org/abs/2506.04553</link>
      <description>arXiv:2506.04553v1 Announce Type: cross 
Abstract: Unsupervised machine learning is widely used to mine large, unlabeled datasets to make data-driven discoveries in critical domains such as climate science, biomedicine, astronomy, chemistry, and more. However, despite its widespread utilization, there is a lack of standardization in unsupervised learning workflows for making reliable and reproducible scientific discoveries. In this paper, we present a structured workflow for using unsupervised learning techniques in science. We highlight and discuss best practices starting with formulating validatable scientific questions, conducting robust data preparation and exploration, using a range of modeling techniques, performing rigorous validation by evaluating the stability and generalizability of unsupervised learning conclusions, and promoting effective communication and documentation of results to ensure reproducible scientific discoveries. To illustrate our proposed workflow, we present a case study from astronomy, seeking to refine globular clusters of Milky Way stars based upon their chemical composition. Our case study highlights the importance of validation and illustrates how the benefits of a carefully-designed workflow for unsupervised learning can advance scientific discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04553v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andersen Chang, Tiffany M. Tang, Tarek M. Zikry, Genevera I. Allen</dc:creator>
    </item>
    <item>
      <title>A Scalable Exponential Random Graph Model: Amortised Hierarchical Sequential Neural Posterior Estimation with Applications in Neuroscience</title>
      <link>https://arxiv.org/abs/2506.04558</link>
      <description>arXiv:2506.04558v1 Announce Type: cross 
Abstract: Exponential Random Graph Models (ERGMs) are an inferential model for analysing statistical networks. Recent development in ERGMs uses hierarchical Bayesian setup to jointly model a group of networks, which is called a multiple-network Exponential Random Graph Model (MN-ERGMs). MN-ERGM has been successfully applied on real-world resting-state fMRI data from the Cam-CAN project to infer the brain connectivity on aging. However, conventional Bayesian ERGM estimation approach is computationally intensive and lacks implementation scalability due to intractable ERGM likelihood. We address this key limitation by using neural posterior estimation (NPE), which trains a neural network-based conditional density estimator to infer the posterior.\\ We proposed an Amortised Hierarchical Sequential Neural Posterior Estimation (AHS-NPE) and various ERGM-specific adjustment schemes to target the Bayesian hierarchical structure of MN-ERGMs. Our proposed method contributes to the ERGM literature as a very scalable solution, and we used AHS-NPE to re-show the fitting results on the Cam-CAN data application and further scaled it up to a larger implementation sample size. More importantly, our AHS-NPE contributes to the general NPE literature as a new hierarchical NPE approach that preserves the amortisation and sequential refinement, which can be applied to a variety of study fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04558v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yefeng Fan, Simon Richard White</dc:creator>
    </item>
    <item>
      <title>Subjective Perspectives within Learned Representations Predict High-Impact Innovation</title>
      <link>https://arxiv.org/abs/2506.04616</link>
      <description>arXiv:2506.04616v1 Announce Type: cross 
Abstract: Existing studies of innovation emphasize the power of social structures to shape innovation capacity. Emerging machine learning approaches, however, enable us to model innovators' personal perspectives and interpersonal innovation opportunities as a function of their prior trajectories of experience. We theorize then quantify subjective perspectives and innovation opportunities based on innovator positions within the geometric space of concepts inscribed by dynamic language representations. Using data on millions of scientists, inventors, writers, entrepreneurs, and Wikipedia contributors across the creative domains of science, technology, film, entrepreneurship, and Wikipedia, here we show that measured subjective perspectives anticipate what ideas individuals and groups creatively attend to and successfully combine in future. When perspective and background diversity are decomposed as the angular difference between collaborators' perspectives on their creation and between their experiences, the former consistently anticipates creative achievement while the latter portends its opposite, across all cases and time periods examined. We analyze a natural experiment and simulate creative collaborations between AI (large language model) agents designed with various perspective and background diversity, which are consistent with our observational findings. We explore mechanisms underlying these findings and identify how successful collaborators leverage common language to weave together diverse experience obtained through trajectories of prior work that converge to provoke one another and innovate. We explore the importance of these findings for team assembly and research policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04616v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Likun Cao, Rui Pan, James Evans</dc:creator>
    </item>
    <item>
      <title>The cost of ensembling: is it always worth combining?</title>
      <link>https://arxiv.org/abs/2506.04677</link>
      <description>arXiv:2506.04677v1 Announce Type: cross 
Abstract: Given the continuous increase in dataset sizes and the complexity of forecasting models, the trade-off between forecast accuracy and computational cost is emerging as an extremely relevant topic, especially in the context of ensemble learning for time series forecasting. To asses it, we evaluated ten base models and eight ensemble configurations across two large-scale retail datasets (M5 and VN1), considering both point and probabilistic accuracy under varying retraining frequencies. We showed that ensembles consistently improve forecasting performance, particularly in probabilistic settings. However, these gains come at a substantial computational cost, especially for larger, accuracy-driven ensembles. We found that reducing retraining frequency significantly lowers costs, with minimal impact on accuracy, particularly for point forecasts. Moreover, efficiency-driven ensembles offer a strong balance, achieving competitive accuracy with considerably lower costs compared to accuracy-optimized combinations. Most importantly, small ensembles of two or three models are often sufficient to achieve near-optimal results. These findings provide practical guidelines for deploying scalable and cost-efficient forecasting systems, supporting the broader goals of sustainable AI in forecasting. Overall, this work shows that careful ensemble design and retraining strategy selection can yield accurate, robust, and cost-effective forecasts suitable for real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04677v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Zanotti</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit testing for the stationary density of a size-structured PDE</title>
      <link>https://arxiv.org/abs/2506.05103</link>
      <description>arXiv:2506.05103v1 Announce Type: cross 
Abstract: We consider two division models for structured cell populations, where cells can grow, age and divide. These models have been introduced in the literature under the denomination of `mitosis' and `adder' models. In the recent years, there has been an increasing interest in Biology to understand whether the cells divide equally or not, as this can be related to important mechanisms in cellular aging or recovery. We are therefore interested in testing the null hypothesis $H_0$ where the division of a mother cell results into two daughters of equal size or age, against the alternative hypothesis $H_1$ where the division is asymmetric and ruled by a kernel that is absolutely continuous with respect to the Lebesgue measure. The sample consists of i.i.d. observations of cell sizes and ages drawn from the population, and the division is not directly observed. The hypotheses of the test are reformulated as hypotheses on the stationary size and age distributions of the models, which we assume are also the distributions of the observations. We propose a goodness-of-fit test that we study numerically on simulated data before applying it on real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05103v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Van Ha Hoang, Phu Thanh Nguyen, Thanh Mai Pham Ngoc, Vincent Rivoirard, Viet Chi Tran</dc:creator>
    </item>
    <item>
      <title>Trust the process: mapping data-driven reconstructions to informed models using stochastic processes</title>
      <link>https://arxiv.org/abs/2506.05153</link>
      <description>arXiv:2506.05153v1 Announce Type: cross 
Abstract: Gravitational-wave astronomy has entered a regime where it can extract information about the population properties of the observed binary black holes. The steep increase in the number of detections will offer deeper insights, but it will also significantly raise the computational cost of testing multiple models. To address this challenge, we propose a procedure that first performs a non-parametric (data-driven) reconstruction of the underlying distribution, and then remaps these results onto a posterior for the parameters of a parametric (informed) model. The computational cost is primarily absorbed by the initial non-parametric step, while the remapping procedure is both significantly easier to perform and computationally cheaper. In addition to yielding the posterior distribution of the model parameters, this method also provides a measure of the model's goodness-of-fit, opening for a new quantitative comparison across models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05153v1</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Rinaldi, Alexandre Toubiana, Jonathan R. Gair</dc:creator>
    </item>
    <item>
      <title>Spatial-temporal prediction of forest attributes using latent Gaussian models and inventory data</title>
      <link>https://arxiv.org/abs/2503.16691</link>
      <description>arXiv:2503.16691v2 Announce Type: replace 
Abstract: The USDA Forest Inventory and Analysis (FIA) program conducts a national forest inventory for the United States through a network of permanent field plots. FIA produces estimates of area averages and totals for plot-measured forest variables through design-based inference, assuming a fixed population and a probability sample of field plot locations. The fixed-population assumption and characteristics of the FIA sampling scheme make it difficult to estimate change in forest variables over time using design-based inference. We propose spatial-temporal models based on Gaussian processes as a flexible tool for forest inventory data, capable of inferring forest variables and change thereof over arbitrary spatial and temporal domains. It is shown to be beneficial for the covariance function governing the latent Gaussian process to account for variation at multiple scales, separating spatially local variation from ecosystem-scale variation. We demonstrate a model for forest biomass density, inferring 20 years of biomass change within two US National Forests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16691v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul B. May, Andrew O. Finley</dc:creator>
    </item>
    <item>
      <title>Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation</title>
      <link>https://arxiv.org/abs/2402.12649</link>
      <description>arXiv:2402.12649v3 Announce Type: replace-cross 
Abstract: Standard benchmarks of bias and fairness in large language models (LLMs) measure the association between the user attributes stated or implied by a prompt and the LLM's short text response, but human-AI interaction increasingly requires long-form and context-specific system output to solve real-world tasks. In the commonly studied domain of gender-occupation bias, we test whether these benchmarks are robust to lengthening the LLM responses as a measure of Realistic Use and Tangible Effects (i.e., RUTEd evaluations). From the current literature, we adapt three standard bias metrics (neutrality, skew, and stereotype) and develop analogous RUTEd evaluations from three contexts of real-world use: children's bedtime stories, user personas, and English language learning exercises. We find that standard bias metrics have no significant correlation with the more realistic bias metrics. For example, selecting the least biased model based on the standard "trick tests" coincides with selecting the least biased model as measured in more realistic use no more than random chance. We suggest that there is not yet evidence to justify standard benchmarks as reliable proxies of real-world AI biases, and we encourage further development of evaluations grounded in particular contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12649v3</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristian Lum, Jacy Reese Anthis, Kevin Robinson, Chirag Nagpal, Alexander D'Amour</dc:creator>
    </item>
    <item>
      <title>The Impossibility of Fair LLMs</title>
      <link>https://arxiv.org/abs/2406.03198</link>
      <description>arXiv:2406.03198v2 Announce Type: replace-cross 
Abstract: The rise of general-purpose artificial intelligence (AI) systems, particularly large language models (LLMs), has raised pressing moral questions about how to reduce bias and ensure fairness at scale. Researchers have documented a sort of "bias" in the significant correlations between demographics (e.g., race, gender) in LLM prompts and responses, but it remains unclear how LLM fairness could be evaluated with more rigorous definitions, such as group fairness or fair representations. We analyze a variety of technical fairness frameworks and find inherent challenges in each that make the development of a fair LLM intractable. We show that each framework either does not logically extend to the general-purpose AI context or is infeasible in practice, primarily due to the large amounts of unstructured training data and the many potential combinations of human populations, use cases, and sensitive attributes. These inherent challenges would persist for general-purpose AI, including LLMs, even if empirical challenges, such as limited participatory input and limited measurement methods, were overcome. Nonetheless, fairness will remain an important type of model evaluation, and there are still promising research directions, particularly the development of standards for the responsibility of LLM developers, context-specific evaluations, and methods of iterative, participatory, and AI-assisted evaluation that could scale fairness across the diverse contexts of modern human-AI interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03198v2</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacy Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Chenhao Tan</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Outcomes Truncated by Death and Missing Not at Random</title>
      <link>https://arxiv.org/abs/2406.10554</link>
      <description>arXiv:2406.10554v3 Announce Type: replace-cross 
Abstract: In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10554v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Yuan Liu, Shanshan Luo, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>Unleashing The Power of Pre-Trained Language Models for Irregularly Sampled Time Series</title>
      <link>https://arxiv.org/abs/2408.08328</link>
      <description>arXiv:2408.08328v2 Announce Type: replace-cross 
Abstract: Pre-trained Language Models (PLMs), such as ChatGPT, have significantly advanced the field of natural language processing. This progress has inspired a series of innovative studies that explore the adaptation of PLMs to time series analysis, intending to create a unified foundation model that addresses various time series analytical tasks. However, these efforts predominantly focus on Regularly Sampled Time Series (RSTS), neglecting the unique challenges posed by Irregularly Sampled Time Series (ISTS), which are characterized by uneven sampling intervals and prevalent missing data. To bridge this gap, this work takes the first step in exploring the potential of PLMs for ISTS analysis. We begin by investigating the effect of various methods for representing ISTS, aiming to maximize the efficacy of PLMs in the analysis. Furthermore, we propose a unified PLM-based framework, named ISTS-PLM, to address diverse ISTS analytical tasks. It integrates novel time-aware and variable-aware PLMs tailored to tackle the intractable intra- and inter-time series modeling in ISTS. Finally, extensive experiments on a comprehensive benchmark demonstrate that the ISTS-PLM, utilizing a structured and effective series-based representation for ISTS, consistently achieves state-of-the-art performance across various analytical tasks, such as classification, interpolation, extrapolation, few-shot and zero-shot learning scenarios, spanning scientific domains like healthcare, biomechanics, and climate science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08328v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3711896.3737171</arxiv:DOI>
      <dc:creator>Weijia Zhang, Chenlong Yin, Hao Liu, Hui Xiong</dc:creator>
    </item>
    <item>
      <title>Smoothing Variances Across Time: Adaptive Stochastic Volatility</title>
      <link>https://arxiv.org/abs/2408.11315</link>
      <description>arXiv:2408.11315v4 Announce Type: replace-cross 
Abstract: We introduce a novel Bayesian framework for estimating time-varying volatility by extending the Random Walk Stochastic Volatility (RWSV) model with Dynamic Shrinkage Processes (DSP) in log-variances. Unlike the classical Stochastic Volatility (SV) or GARCH-type models with restrictive parametric stationarity assumptions, our proposed Adaptive Stochastic Volatility (ASV) model provides smooth yet dynamically adaptive estimates of evolving volatility and its uncertainty. We further enhance the model by incorporating a nugget effect, allowing it to flexibly capture small-scale variability while preserving smoothness elsewhere. We derive the theoretical properties of the global-local shrinkage prior DSP. Through simulation studies, we show that ASV exhibits remarkable misspecification resilience and low prediction error across various data-generating processes. Furthermore, ASV's capacity to yield locally smooth and interpretable estimates facilitates a clearer understanding of the underlying patterns and trends in volatility. As an extension, we develop the Bayesian Trend Filter with ASV (BTF-ASV) which allows joint modeling of the mean and volatility with abrupt changes. Finally, our proposed models are applied to time series data from finance, econometrics, and environmental science, highlighting their flexibility and broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11315v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason B. Cho, David S. Matteson</dc:creator>
    </item>
    <item>
      <title>Mining Causality: AI-Assisted Search for Instrumental Variables</title>
      <link>https://arxiv.org/abs/2409.14202</link>
      <description>arXiv:2409.14202v3 Announce Type: replace-cross 
Abstract: The instrumental variables (IVs) method is a leading empirical strategy for causal inference. Finding IVs is a heuristic and creative process, and justifying its validity -- especially exclusion restrictions -- is largely rhetorical. We propose using large language models (LLMs) to search for new IVs through narratives and counterfactual reasoning, similar to how a human researcher would. The stark difference, however, is that LLMs can dramatically accelerate this process and explore an extremely large search space. We demonstrate how to construct prompts to search for potentially valid IVs. We contend that multi-step and role-playing prompting strategies are effective for simulating the endogenous decision-making processes of economic agents and for navigating language models through the realm of real-world scenarios, rather than anchoring them within the narrow realm of academic discourses on IVs. We apply our method to three well-known examples in economics: returns to schooling, supply and demand, and peer effects. We then extend our strategy to finding (i) control variables in regression and difference-in-differences and (ii) running variables in regression discontinuity designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14202v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sukjin Han</dc:creator>
    </item>
    <item>
      <title>Functional relevance based on the continuous Shapley value</title>
      <link>https://arxiv.org/abs/2411.18575</link>
      <description>arXiv:2411.18575v2 Announce Type: replace-cross 
Abstract: The presence of artificial intelligence (AI) in our society is increasing, which brings with it the need to understand the behavior of AI mechanisms, including machine learning predictive algorithms fed with tabular data, text or images, among others. This work focuses on interpretability of predictive models based on functional data. Designing interpretability methods for functional data models implies working with a set of features whose size is infinite. In the context of scalar on function regression, we propose an interpretability method based on the Shapley value for continuous games, a mathematical formulation that allows for the fair distribution of a global payoff among a continuous set of players. The method is illustrated through a set of experiments with simulated and real data sets. The open source Python package ShapleyFDA is also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18575v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pedro Delicado, Cristian Pach\'on-Garc\'ia</dc:creator>
    </item>
    <item>
      <title>Alpha-Beta HMM: Hidden Markov Model Filtering with Equal Exit Probabilities and a Step-Size Parameter</title>
      <link>https://arxiv.org/abs/2504.01759</link>
      <description>arXiv:2504.01759v2 Announce Type: replace-cross 
Abstract: The hidden Markov model (HMM) provides a powerful framework for inference in time-varying environments, where the underlying state evolves according to a Markov chain. To address the optimal filtering problem in general dynamic settings, we propose the $\alpha\beta$-HMM algorithm, which simplifies the state transition model to a Markov chain with equal exit probabilities and introduces a step-size parameter to balance the influence of observational data and the model. By analyzing the algorithm's dynamics in stationary environments, we uncover a fundamental trade-off between inference accuracy and adaptation capability, highlighting how key parameters and observation quality impact performance. A comprehensive theoretical analysis of the nonlinear dynamical system governing the evolution of the log-belief ratio, along with supporting numerical experiments, demonstrates that the proposed approach effectively balances adaptability and inference performance in dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01759v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dongyan Sui, Haotian Pu, Siyang Leng, Stefan Vlaski</dc:creator>
    </item>
    <item>
      <title>Beating the Correlation Breakdown: Robust Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios</title>
      <link>https://arxiv.org/abs/2504.15268</link>
      <description>arXiv:2504.15268v4 Announce Type: replace-cross 
Abstract: We live in a multivariate world, and effective modeling of financial portfolios, including their construction, allocation, forecasting, and risk analysis, simply is not possible without explicitly modeling the dependence structure of their assets. Dependence structure can drive portfolio results more than the combined effects of other parameters in investment and risk models, but the literature provides relatively little to define the finite-sample distributions of dependence measures in useable and useful ways under challenging, real-world financial data conditions. Yet this is exactly what is needed to make valid inferences about their estimates, and to use these inferences for essential purposes such as hypothesis testing, dynamic monitoring, realistic and granular scenario and reverse scenario analyses, and mitigating the effects of correlation breakdowns during market upheavals. This work develops a new and straightforward method, Nonparametric Angles-based Correlation (NAbC), for defining the finite-sample distributions of any dependence measure whose matrix of pairwise associations is positive definite (e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). The solution remains valid under marginal asset distributions characterized by notably different and varying degrees of serial correlation, non-stationarity, heavy-tailedness, and asymmetry. Importantly, NAbCs p-values and confidence intervals remain analytically consistent at both the matrix level and the pairwise cell level. Finally, NAbC maintains validity even when selected cells in the matrix are frozen for a given scenario or stress test, thus enabling flexible, granular, and realistic scenarios. NAbC stands alone in providing all of these capabilities simultaneously, and should prove to be a very useful means by which we can better understand and manage financial portfolios in our multivariate world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15268v4</guid>
      <category>q-fin.RM</category>
      <category>q-fin.PM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>JD Opdyke</dc:creator>
    </item>
  </channel>
</rss>
