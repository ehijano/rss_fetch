<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jan 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Local Sherman's Algorithm for Multi-commodity Flow</title>
      <link>https://arxiv.org/abs/2501.10632</link>
      <description>arXiv:2501.10632v1 Announce Type: new 
Abstract: We give the first local algorithm for computing multi-commodity flow and apply it to obtain a $(1+\epsilon)$-approximate algorithm for computing a $k$-commodity flow on an expander with $m$ edges in $(m+k^{4}\epsilon^{-3})n^{o(1)}$ time. This is the first $(1+\epsilon)$-approximate algorithm that breaks the $km$ multi-commodity flow barrier, albeit only on expanders. All previous algorithms either require $\Omega(km)$ time or a big constant approximation.
  Our approach is by localizing Sherman's flow algorithm when put into the Multiplicative Weight Update (MWU) framework. We show that, on each round of MWU, the oracle could instead work with the *rounded weights* where all polynomially small weights are rounded to zero. Since there are only few large weights, one can implement the oracle call with respect to the rounded weights in sublinear time. This insight in generic and may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10632v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Li, Thatchaphol Saranurak</dc:creator>
    </item>
    <item>
      <title>Answering Related Questions</title>
      <link>https://arxiv.org/abs/2501.10633</link>
      <description>arXiv:2501.10633v1 Announce Type: new 
Abstract: We introduce the meta-problem Sidestep$(\Pi, \mathsf{dist}, d)$ for a problem $\Pi$, a metric $\mathsf{dist}$ over its inputs, and a map $d: \mathbb N \to \mathbb R_+ \cup \{\infty\}$. A solution to Sidestep$(\Pi, \mathsf{dist}, d)$ on an input $I$ of $\Pi$ is a pair $(J, \Pi(J))$ such that $\mathsf{dist}(I,J) \leqslant d(|I|)$ and $\Pi(J)$ is a correct answer to $\Pi$ on input $J$. This formalizes the notion of answering a related question (or sidestepping the question), for which we give some practical and theoretical motivations, and compare it to the neighboring concepts of smoothed analysis, planted problems, and edition problems. Informally, we call hardness radius the ``largest'' $d$ such that Sidestep$(\Pi, \mathsf{dist}, d)$ is NP-hard. This framework calls for establishing the hardness radius of problems $\Pi$ of interest for the relevant distances $\mathsf{dist}$.
  We exemplify it with graph problems and two distances $\mathsf{dist}_\Delta$ and $\mathsf{dist}_e$ (the edge edit distance) such that $\mathsf{dist}_\Delta(G,H)$ (resp. $\mathsf{dist}_e(G,H)$) is the maximum degree (resp. number of edges) of the symmetric difference of $G$ and $H$ if these graphs are on the same vertex set, and $+\infty$ otherwise. We show that the decision problems Independent Set, Clique, Vertex Cover, Coloring, Clique Cover have hardness radius $n^{\frac{1}{2}-o(1)}$ for $\mathsf{dist}_\Delta$, and $n^{\frac{4}{3}-o(1)}$ for $\mathsf{dist}_e$, that Hamiltonian Cycle has hardness radius 0 for $\mathsf{dist}_\Delta$, and somewhere between $n^{\frac{1}{2}-o(1)}$ and $n/3$ for $\mathsf{dist}_e$, and that Dominating Set has hardness radius $n^{1-o(1)}$ for $\mathsf{dist}_e$. We leave several open questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10633v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Edouard Bonnet</dc:creator>
    </item>
    <item>
      <title>Convergence and Running Time of Time-dependent Ant Colony Algorithms</title>
      <link>https://arxiv.org/abs/2501.10810</link>
      <description>arXiv:2501.10810v1 Announce Type: new 
Abstract: Ant Colony Optimization (ACO) is a well-known method inspired by the foraging behavior of ants and is extensively used to solve combinatorial optimization problems. In this paper, we first consider a general framework based on the concept of a construction graph - a graph associated with an instance of the optimization problem under study, where feasible solutions are represented by walks. We analyze the running time of this ACO variant, known as the Graph-based Ant System with time-dependent evaporation rate (GBAS/tdev), and prove that the algorithm's solution converges to the optimal solution of the problem with probability 1 for a slightly stronger evaporation rate function than was previously known. We then consider two time-dependent adaptations of Attiratanasunthron and Fakcharoenphol's $n$-ANT algorithm: $n$-ANT with time-dependent evaporation rate ($n$-ANT/tdev) and $n$-ANT with time-dependent lower pheromone bound ($n$-ANT/tdlb). We analyze both variants on the single destination shortest path problem (SDSP). Our results show that $n$-ANT/tdev has a super-polynomial time lower bound on the SDSP. In contrast, we show that $n$-ANT/tdlb achieves a polynomial time upper bound on this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10810v1</guid>
      <category>cs.DS</category>
      <category>cs.NE</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bodo Manthey, Jesse van Rhijn, Ashkan Safari, Tjark Vredeveld</dc:creator>
    </item>
    <item>
      <title>On the thinness of trees</title>
      <link>https://arxiv.org/abs/2501.11157</link>
      <description>arXiv:2501.11157v1 Announce Type: new 
Abstract: The study of structural graph width parameters like tree-width, clique-width and rank-width has been ongoing during the last five decades, and their algorithmic use has also been increasing [Cygan et al., 2015]. New width parameters continue to be defined, for example, MIM-width in 2012, twin-width in 2020, and mixed-thinness, a generalization of thinness, in 2022.
  The concept of thinness of a graph was introduced in 2007 by Mannino, Oriolo, Ricci and Chandran, and it can be seen as a generalization of interval graphs, which are exactly the graphs with thinness equal to one. This concept is interesting because if a representation of a graph as a $k$-thin graph is given for a constant value $k$, then several known NP-complete problems can be solved in polynomial time. Some examples are the maximum weighted independent set problem, solved in the seminal paper by Mannino et al., and the capacitated coloring with fixed number of colors [Bonomo, Mattia and Oriolo, 2011].
  In this work we present a constructive $O(n\log(n))$-time algorithm to compute the thinness for any given $n$-vertex tree, along with a corresponding thin representation. We use intermediate results of this construction to improve known bounds of the thinness of some special families of trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11157v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.dam.2024.12.027</arxiv:DOI>
      <arxiv:journal_reference>Discrete Applied Mathematics, Volume 365, 15 April 2025, Pages 39-60 Discrete Applied Mathematics, Volume 365, 2025, Pages 39-60,</arxiv:journal_reference>
      <dc:creator>Flavia Bonomo-Braberman, Eric Brandwein, Carolina Luc\'ia Gonz\'alez, Agust\'in Sansone</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Computing a Fastest Temporal Path in Interval Temporal Graphs</title>
      <link>https://arxiv.org/abs/2501.11380</link>
      <description>arXiv:2501.11380v1 Announce Type: new 
Abstract: Temporal graphs arise when modeling interactions that evolve over time. They usually come in several flavors, depending on the number of parameters used to describe the temporal aspects of the interactions: time of appearance, duration, delay of transmission. In the point model, edges appear at specific points in time, while in the more general interval model, edges can be present over multiple time intervals. In both models, the delay for traversing an edge can change with each edge appearance. When time is discrete, the two models are equivalent in the sense that the presence of an edge during an interval is equivalent to a sequence of point-in-time occurrences of the edge. However, this transformation can drastically change the size of the input and has complexity issues. Indeed, we show a gap between the two models with respect to the complexity of the classical problem of computing a fastest temporal path from a source vertex to a target vertex, i.e. a path where edges can be traversed one after another in time and such that the total duration from source to target is minimized. It can be solved in near-linear time in the point model, while we show that the interval model requires quadratic time under classical assumptions of fine-grained complexity. With respect to linear time, our lower bound implies a factor of the number of vertices, while the best known algorithm has a factor of the number of underlying edges. Interestingly, we show that near-linear time is possible in the interval model when restricted to all delays being zero, i.e. traversing an edge is instantaneous.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11380v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Aubian (IRIF), Filippo Brunelli (JRC), Feodor F Dragan (UniBuc, ICI), Guillaume Ducoffe (UniBuc, ICI), Michel Habib (IRIF), Allen Ibiapina (IRIF), Laurent Viennot (DI-ENS, ARGO)</dc:creator>
    </item>
    <item>
      <title>An algorithmic Vizing's theorem: toward efficient edge-coloring sampling with an optimal number of colors</title>
      <link>https://arxiv.org/abs/2501.11541</link>
      <description>arXiv:2501.11541v1 Announce Type: new 
Abstract: The problem of sampling edge-colorings of graphs with maximum degree $\Delta$ has received considerable attention and efficient algorithms are available when the number of colors is large enough with respect to $\Delta$. Vizing's theorem guarantees the existence of a $(\Delta+1)$-edge-coloring, raising the natural question of how to efficiently sample such edge-colorings. In this paper, we take an initial step toward addressing this question. Building on the approach of Dotan, Linial, and Peled, we analyze a randomized algorithm for generating random proper $(\Delta+1)$-edge-colorings, which in particular provides an algorithmic interpretation of Vizing's theorem. The idea is to start from an arbitrary non-proper edge-coloring with the desired number of colors and at each step, recolor one edge uniformly at random provided it does not increase the number of conflicting edges (a potential function will count the number of pairs of adjacent edges of the same color). We show that the algorithm almost surely produces a proper $(\Delta+1)$-edge-coloring and propose several conjectures regarding its efficiency and the uniformity of the sampled colorings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11541v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas De Meyer, Franti\v{s}ek Kardo\v{s}, Aur\'elie Lagoutte, Guillem Perarnau</dc:creator>
    </item>
    <item>
      <title>Tight Analyses of Ordered and Unordered Linear Probing</title>
      <link>https://arxiv.org/abs/2501.11582</link>
      <description>arXiv:2501.11582v1 Announce Type: new 
Abstract: Linear-probing hash tables have been classically believed to support insertions in time $\Theta(x^2)$, where $1 - 1/x$ is the load factor of the hash table. Recent work by Bender, Kuszmaul, and Kuszmaul (FOCS'21), however, has added a new twist to this story: in some versions of linear probing, if the \emph{maximum} load factor is at most $1 - 1/x$, then the \emph{amortized} expected time per insertion will never exceed $x \log^{O(1)} x$ (even in workloads that operate continuously at a load factor of $1 - 1/x$). Determining the exact asymptotic value for the amortized insertion time remains open.
  In this paper, we settle the amortized complexity with matching upper and lower bounds of $\Theta(x \log^{1.5} x)$. Along the way, we also obtain tight bounds for the so-called path surplus problem, a problem in combinatorial geometry that has been shown to be closely related to linear probing. We also show how to extend Bender et al.'s bounds to say something not just about ordered linear probing (the version they study) but also about classical linear probing, in the form that is most widely implemented in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11582v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Braverman, William Kuszmaul</dc:creator>
    </item>
    <item>
      <title>$O(1)$-Round MPC Algorithms for Multi-dimensional Grid Graph Connectivity, EMST and DBSCAN</title>
      <link>https://arxiv.org/abs/2501.12044</link>
      <description>arXiv:2501.12044v1 Announce Type: new 
Abstract: In this paper, we investigate three fundamental problems in the Massively Parallel Computation (MPC) model: (i) grid graph connectivity, (ii) approximate Euclidean Minimum Spanning Tree (EMST), and (iii) approximate DBSCAN.
  Our first result is a $O(1)$-round Las Vegas (i.e., succeeding with high probability) MPC algorithm for computing the connected components on a $d$-dimensional $c$-penetration grid graph ($(d,c)$-grid graph), where both $d$ and $c$ are positive integer constants. In such a grid graph, each vertex is a point with integer coordinates in $\mathbb{N}^d$, and an edge can only exist between two distinct vertices with $\ell_\infty$-norm at most $c$. To our knowledge, the current best existing result for computing the connected components (CC's) on $(d,c)$-grid graphs in the MPC model is to run the state-of-the-art MPC CC algorithms that are designed for general graphs: they achieve $O(\log \log n + \log D)$[FOCS19] and $O(\log \log n + \log \frac{1}{\lambda})$[PODC19] rounds, respectively, where $D$ is the {\em diameter} and $\lambda$ is the {\em spectral gap} of the graph.
  With our grid graph connectivity technique, our second main result is a $O(1)$-round Las Vegas MPC algorithm for computing approximate Euclidean MST. The existing state-of-the-art result on this problem is the $O(1)$-round MPC algorithm proposed by Andoni et al.[STOC14], which only guarantees an approximation on the overall weight in expectation. In contrast, our algorithm not only guarantees a deterministic overall weight approximation, but also achieves a deterministic edge-wise weight approximation.The latter property is crucial to many applications, such as finding the Bichromatic Closest Pair and DBSCAN clustering.
  Last but not the least, our third main result is a $O(1)$-round Las Vegas MPC algorithm for computing an approximate DBSCAN clustering in $O(1)$-dimensional space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12044v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>cs.DC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Junhao Gan, Anthony Wirth, Zhuo Zhang</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Telephone Broadcasting: From Cacti to Bounded Pathwidth Graphs</title>
      <link>https://arxiv.org/abs/2501.12316</link>
      <description>arXiv:2501.12316v1 Announce Type: new 
Abstract: In the Telephone Broadcasting problem, the goal is to disseminate a message from a given source vertex of an input graph to all other vertices in a minimum number of rounds, where at each round, an informed vertex can inform at most one of its uniformed neighbours. For general graphs of $n$ vertices, the problem is NP-hard, and the best existing algorithm has an approximation factor of $O(\log n/ \log \log n)$. The existence of a constant factor approximation for the general graphs is still unknown. The problem can be solved in polynomial time for trees.
  In this paper, we study the problem in two simple families of sparse graphs, namely, cacti and graphs of bounded path width. There have been several efforts to understand the complexity of the problem in cactus graphs, mostly establishing the presence of polynomial-time solutions for restricted families of cactus graphs. Despite these efforts, the complexity of the problem in arbitrary cactus graphs remained open. In this paper, we settle this question by establishing the NP-hardness of telephone broadcasting in cactus graphs. For that, we show the problem is NP-hard in a simple subfamily of cactus graphs, which we call snowflake graphs. These graphs not only are cacti but also have pathwidth $2$. These results establish that, although the problem is polynomial-time solvable in trees, it becomes NP-hard in simple extension of trees. On the positive side, we present constant-factor approximation algorithms for the studied families of graphs, namely, an algorithm with an approximation factor of $2$ for cactus graphs and an approximation factor of $O(1)$ for graphs of bounded pathwidth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12316v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aida Aminian, Shahin Kamali, Seyed-Mohammad Seyed-Javadi,  Sumedha</dc:creator>
    </item>
    <item>
      <title>Fixed Point Computation: Beating Brute Force with Smoothed Analysis</title>
      <link>https://arxiv.org/abs/2501.10884</link>
      <description>arXiv:2501.10884v1 Announce Type: cross 
Abstract: We propose a new algorithm that finds an $\varepsilon$-approximate fixed point of a smooth function from the $n$-dimensional $\ell_2$ unit ball to itself. We use the general framework of finding approximate solutions to a variational inequality, a problem that subsumes fixed point computation and the computation of a Nash Equilibrium. The algorithm's runtime is bounded by $e^{O(n)}/\varepsilon$, under the smoothed-analysis framework. This is the first known algorithm in such a generality whose runtime is faster than $(1/\varepsilon)^{O(n)}$, which is a time that suffices for an exhaustive search. We complement this result with a lower bound of $e^{\Omega(n)}$ on the query complexity for finding an $O(1)$-approximate fixed point on the unit ball, which holds even in the smoothed-analysis model, yet without the assumption that the function is smooth. Existing lower bounds are only known for the hypercube, and adapting them to the ball does not give non-trivial results even for finding $O(1/\sqrt{n})$-approximate fixed points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10884v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Idan Attias, Yuval Dagan, Constantinos Daskalakis, Rui Yao, Manolis Zampetakis</dc:creator>
    </item>
    <item>
      <title>Packing Dijoins in Weighted Chordal Digraphs</title>
      <link>https://arxiv.org/abs/2501.10918</link>
      <description>arXiv:2501.10918v1 Announce Type: cross 
Abstract: In a digraph, a dicut is a cut where all the arcs cross in one direction. A dijoin is a subset of arcs that intersects every dicut. Edmonds and Giles conjectured that in a weighted digraph, the minimum weight of a dicut is equal to the maximum size of a packing of dijoins. This has been disproved. However, the unweighted version conjectured by Woodall remains open. We prove that the Edmonds-Giles conjecture is true if the underlying undirected graph is chordal. We also give a strongly polynomial time algorithm to construct such a packing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10918v1</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>G\'erard Cornu\'ejols, Siyue Liu, R. Ravi</dc:creator>
    </item>
    <item>
      <title>Optimal Binary Variable-Length Codes with a Bounded Number of 1's per Codeword: Design, Analysis, and Applications</title>
      <link>https://arxiv.org/abs/2501.11129</link>
      <description>arXiv:2501.11129v1 Announce Type: cross 
Abstract: In this paper, we consider the problem of constructing optimal average-length binary codes under the constraint that each codeword must contain at most $D$ ones, where $D$ is a given input parameter. We provide an $O(n^2D)$-time complexity algorithm for the construction of such codes, where $n$ is the number of codewords. We also describe several scenarios where the need to design these kinds of codes naturally arises. Our algorithms allow us to construct both optimal average-length prefix binary codes and optimal average-length alphabetic binary codes. In the former case, our $O(n^2D)$-time algorithm substantially improves on the previously known $O(n^{2+D})$-time complexity algorithm for the same problem. We also provide a Kraft-like inequality for the existence of (optimal) variable-length binary codes, subject to the above-described constraint on the number of 1's in each codeword.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11129v1</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Bruno, Roberto De Prisco, Ugo Vaccaro</dc:creator>
    </item>
    <item>
      <title>Local Limits of Small World Networks</title>
      <link>https://arxiv.org/abs/2501.11226</link>
      <description>arXiv:2501.11226v1 Announce Type: cross 
Abstract: Small-world networks, known for their high local clustering and short average path lengths, are a fundamental structure in many real-world systems, including social, biological, and technological networks. We apply the theory of local convergence (Benjamini-Schramm convergence) to derive the limiting behavior of the local structures for two of the most commonly studied small-world network models: the Watts-Strogatz model and the Kleinberg model. Establishing local convergence enables us to show that key network measures, such as PageRank, clustering coefficients, and maximum matching size, converge as network size increases with their limits determined by the graph's local structure. Additionally, this framework facilitates the estimation of global phenomena, such as information cascades, using local information from small neighborhoods. As an additional outcome of our results, we observe a critical change in the behavior of the limit exactly when the parameter governing long-range connections in the Kleinberg model crosses the threshold where decentralized search remains efficient, offering a new perspective on why decentralized algorithms fail in certain regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11226v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <category>math.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yeganeh Alimohammadi, Senem I\c{s}{\i}k, Amin Saberi</dc:creator>
    </item>
    <item>
      <title>Randomized Kaczmarz Methods with Beyond-Krylov Convergence</title>
      <link>https://arxiv.org/abs/2501.11673</link>
      <description>arXiv:2501.11673v1 Announce Type: cross 
Abstract: Randomized Kaczmarz methods form a family of linear system solvers which converge by repeatedly projecting their iterates onto randomly sampled equations. While effective in some contexts, such as highly over-determined least squares, Kaczmarz methods are traditionally deemed secondary to Krylov subspace methods, since this latter family of solvers can exploit outliers in the input's singular value distribution to attain fast convergence on ill-conditioned systems.
  In this paper, we introduce Kaczmarz++, an accelerated randomized block Kaczmarz algorithm that exploits outlying singular values in the input to attain a fast Krylov-style convergence. Moreover, we show that Kaczmarz++ captures large outlying singular values provably faster than popular Krylov methods, for both over- and under-determined systems. We also develop an optimized variant for positive semidefinite systems, called CD++, demonstrating empirically that it is competitive in arithmetic operations with both CG and GMRES on a collection of benchmark problems. To attain these results, we introduce several novel algorithmic improvements to the Kaczmarz framework, including adaptive momentum acceleration, Tikhonov-regularized projections, and a memoization scheme for reusing information from previously sampled equation~blocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11673v1</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Micha{\l} Derezi\'nski, Deanna Needell, Elizaveta Rebrova, Jiaming Yang</dc:creator>
    </item>
    <item>
      <title>A Dynamic Programming Framework for Generating Approximately Diverse and Optimal Solutions</title>
      <link>https://arxiv.org/abs/2501.12261</link>
      <description>arXiv:2501.12261v1 Announce Type: cross 
Abstract: We develop a general framework, called approximately-diverse dynamic programming (ADDP) that can be used to generate a collection of $k\ge2$ maximally diverse solutions to various geometric and combinatorial optimization problems. Given an approximation factor $0\le c\le1$, this framework also allows for maximizing diversity in the larger space of $c$-approximate solutions. We focus on two geometric problems to showcase this technique:
  1. Given a polygon $P$, an integer $k\ge2$ and a value $c\le1$, generate $k$ maximally diverse $c$-nice triangulations of $P$. Here, a $c$-nice triangulation is one that is $c$-approximately optimal with respect to a given quality measure $\sigma$.
  2. Given a planar graph $G$, an integer $k\ge2$ and a value $c\le1$, generate $k$ maximally diverse $c$-optimal Independent Sets (or, Vertex Covers). Here, an independent set $S$ is said to be $c$-optimal if $|S|\ge c|S'|$ for any independent set $S'$ of $G$.
  Given a set of $k$ solutions to the above problems, the diversity measure we focus on is the average distance between the solutions, where $d(X,Y)=|X\Delta Y|$.
  For arbitrary polygons and a wide range of quality measures, we give $\text{poly}(n,k)$ time $(1-\Theta(1/k))$-approximation algorithms for the diverse triangulation problem. For the diverse independent set and vertex cover problems on planar graphs, we give an algorithm that runs in time $2^{O(k\delta^{-1}\epsilon^{-2})}n^{O(1/\epsilon)}$ and returns $(1-\epsilon)$-approximately diverse $(1-\delta)c$-optimal independent sets or vertex covers.
  Our triangulation results are the first algorithmic results on computing collections of diverse geometric objects, and our planar graph results are the first PTAS for the diverse versions of any NP-complete problem. Additionally, we also provide applications of this technique to diverse variants of other geometric problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12261v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Waldo G\'alvez, Mayank Goswami, Arturo Merino, GiBeom Park, Meng-Tsung Tsai, Victor Verdugo</dc:creator>
    </item>
    <item>
      <title>Improved Decoding of Tanner Codes</title>
      <link>https://arxiv.org/abs/2501.12293</link>
      <description>arXiv:2501.12293v1 Announce Type: cross 
Abstract: In this paper, we present improved decoding algorithms for expander-based Tanner codes. We begin by developing a randomized linear-time decoding algorithm that, under the condition that $ \delta d_0 &gt; 2 $, corrects up to $ \alpha n $ errors for a Tanner code $ T(G, C_0) $, where $ G $ is a $ (c, d, \alpha, \delta) $-bipartite expander with $n$ left vertices, and $ C_0 \subseteq \mathbb{F}_2^d $ is a linear inner code with minimum distance $ d_0 $. This result improves upon the previous work of Cheng, Ouyang, Shangguan, and Shen (RANDOM 2024), which required $ \delta d_0 &gt; 3 $. We further derandomize the algorithm to obtain a deterministic linear-time decoding algorithm with the same decoding radius. Our algorithm improves upon the previous deterministic algorithm of Cheng et al. by achieving a decoding radius of $ \alpha n $, compared with the previous radius of $ \frac{2\alpha}{d_0(1 + 0.5c\delta) }n$.
  Additionally, we investigate the size-expansion trade-off introduced by the recent work of Chen, Cheng, Li, and Ouyang (IEEE TIT 2023), and use it to provide new bounds on the minimum distance of Tanner codes. Specifically, we prove that the minimum distance of a Tanner code $T(G,C_0)$ is approximately $f_\delta^{-1} \left( \frac{1}{d_0} \right) \alpha n $, where $ f_\delta(\cdot) $ is the Size-Expansion Function. As another application, we improve the decoding radius of our decoding algorithms from $\alpha n$ to approximately $f_\delta^{-1}(\frac{2}{d_0})\alpha n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12293v1</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeyu Guo, Zhaienhe Zhou</dc:creator>
    </item>
    <item>
      <title>When Location Shapes Choice: Placement Optimization of Substitutable Products</title>
      <link>https://arxiv.org/abs/2310.08568</link>
      <description>arXiv:2310.08568v3 Announce Type: replace 
Abstract: Strategic product placement can have a strong influence on customer purchase behavior in physical stores as well as online platforms. Motivated by this, we consider the problem of optimizing the placement of substitutable products in designated display locations to maximize the expected revenue of the seller. We model the customer behavior as a two-stage process: first, the customer visits a subset of display locations according to a browsing distribution; second, the customer chooses at most one product from the displayed products at those locations according to a choice model. Our goal is to design a general algorithm that can select and place the products optimally for any browsing distribution and choice model, and we call this the Placement problem. We give a randomized algorithm that utilizes an $\alpha$-approximate algorithm for cardinality constrained assortment optimization and outputs a $\frac{\Theta(\alpha)}{\log m}$-approximate solution (in expectation) for Placement with $m$ display locations, i.e., our algorithm outputs a solution with value at least $\frac{\Omega(\alpha)}{\log m}$ factor of the optimal and this is tight in the worst case. We also give algorithms with stronger guarantees in some special cases. In particular, we give a deterministic $\frac{\Omega(1)}{\log m}$-approximation algorithm for the Markov choice model, and a tight $(1-1/e)$-approximation algorithm for the problem when products have identical prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08568v3</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar El Housni, Rajan Udwani</dc:creator>
    </item>
    <item>
      <title>$k$-times bin packing and its application to fair electricity distribution</title>
      <link>https://arxiv.org/abs/2311.16742</link>
      <description>arXiv:2311.16742v3 Announce Type: replace 
Abstract: Given items of different sizes and a fixed bin capacity, the bin-packing problem is to pack these items into a minimum number of bins such that the sum of item sizes in a bin does not exceed the capacity. We define a new variant called \emph{$k$-times bin-packing ($k$BP)}, where the goal is to pack the items such that each item appears exactly $k$ times, in $k$ different bins. We generalize some existing approximation algorithms for bin-packing to solve $k$BP, and analyze their performance ratio.
  The study of $k$BP is motivated by the problem of \emph{fair electricity distribution}. In many developing countries, the total electricity demand is higher than the supply capacity. We prove that every electricity division problem can be solved by $k$-times bin-packing for some finite $k$. We also show that $k$-times bin-packing can be used to distribute the electricity in a fair and efficient way. Particularly, we implement generalizations of the First-Fit and First-Fit Decreasing bin-packing algorithms to solve $k$BP, and apply the generalizations to real electricity demand data. We show that our generalizations outperform existing heuristic solutions to the same problem in terms of the egalitarian allocation of connection time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16742v3</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dinesh Kumar Baghel, Alex Ravsky, Erel Segal-Halevi</dc:creator>
    </item>
    <item>
      <title>Fully Dynamic (\Delta+1) Coloring Against Adaptive Adversaries</title>
      <link>https://arxiv.org/abs/2411.04418</link>
      <description>arXiv:2411.04418v2 Announce Type: replace 
Abstract: Over the years, there has been extensive work on fully dynamic algorithms for classic graph problems that admit greedy solutions. Examples include $(\Delta+1)$ vertex coloring, maximal independent set, and maximal matching. For all three problems, there are randomized algorithms that maintain a valid solution after each edge insertion or deletion to the $n$-vertex graph by spending $\polylog n$ time, provided that the adversary is oblivious. However, none of these algorithms work against adaptive adversaries whose updates may depend on the output of the algorithm. In fact, even breaking the trivial bound of $O(n)$ against adaptive adversaries remains open for all three problems. For instance, in the case of $(\Delta+1)$ vertex coloring, the main challenge is that an adaptive adversary can keep inserting edges between vertices of the same color, necessitating a recoloring of one of the endpoints. The trivial algorithm would simply scan all neighbors of one endpoint to find a new available color (which always exists) in $O(n)$ time.
  In this paper, we break this linear barrier for the $(\Delta+1)$ vertex coloring problem. Our algorithm is randomized, and maintains a valid $(\Delta+1)$ vertex coloring after each edge update by spending $\widetilde{O}(n^{8/9})$ time with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04418v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soheil Behnezhad, Rajmohan Rajaraman, Omer Wasim</dc:creator>
    </item>
    <item>
      <title>The Two-Center Problem of Uncertain Points on Cactus Graphs</title>
      <link>https://arxiv.org/abs/2412.02559</link>
      <description>arXiv:2412.02559v2 Announce Type: replace 
Abstract: We study the two-center problem on cactus graphs in facility locations, which aims to place two facilities on the graph network to serve customers in order to minimize the maximum transportation cost. In our problem, the location of each customer is uncertain and may appear at $O(m)$ points on the network with probabilities. More specifically, given are a cactus graph $G$ and a set $\calP$ of $n$ (weighted) uncertain points where every uncertain point has $O(m)$ possible locations on $G$ each associated with a probability and is of a non-negative weight. The problem aims to compute two centers (points) on $G$ so that the maximum (weighted) expected distance of the $n$ uncertain points to their own expected closest center is minimized. No previous algorithms are known for this problem. In this paper, we present the first algorithm for this problem and it solves the problem in $O(|G|+ m^{2}n^{2}\log mn)$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02559v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haitao Xu, Jingru Zhang</dc:creator>
    </item>
    <item>
      <title>The Connected k-Vertex One-Center Problem on Graphs</title>
      <link>https://arxiv.org/abs/2412.18001</link>
      <description>arXiv:2412.18001v2 Announce Type: replace 
Abstract: We consider a generalized version of the (weighted) one-center problem on graphs. Given an undirected graph $G$ of $n$ vertices and $m$ edges and a positive integer $k\leq n$, the problem aims to find a point in $G$ so that the maximum (weighted) distance from it to $k$ connected vertices in its shortest path tree(s) is minimized. No previous work has been proposed for this problem except for the case $k=n$, that is, the classical graph one-center problem. In this paper, an $O(mn\log n\log mn + m^2\log n\log mn)$-time algorithm is proposed for the weighted case, and an $O(mn\log n)$-time algorithm is presented for the unweighted case, provided that the distance matrix for $G$ is given. When $G$ is a tree graph, we propose an algorithm that solves the weighted case in $O(n\log^2 n\log k)$ time with no given distance matrix, and improve it to $O(n\log^2 n)$ for the unweighted case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18001v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingru Zhang</dc:creator>
    </item>
    <item>
      <title>A Simple and Combinatorial Approach to Proving Chernoff Bounds and Their Generalizations</title>
      <link>https://arxiv.org/abs/2501.03488</link>
      <description>arXiv:2501.03488v2 Announce Type: replace 
Abstract: The Chernoff bound is one of the most widely used tools in theoretical computer science. It's rare to find a randomized algorithm that doesn't employ a Chernoff bound in its analysis. The standard proofs of Chernoff bounds are beautiful but in some ways not very intuitive. In this paper, I'll show you a different proof that has four features: (1) the proof offers a strong intuition for why Chernoff bounds look the way that they do; (2) the proof is user-friendly and (almost) algebra-free; (3) the proof comes with matching lower bounds, up to constant factors in the exponent; and (4) the proof extends to establish generalizations of Chernoff bounds in other settings. The ultimate goal is that, once you know this proof (and with a bit of practice), you should be able to confidently reason about Chernoff-style bounds in your head, extending them to other settings, and convincing yourself that the bounds you're obtaining are tight (up to constant factors in the exponent).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03488v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Kuszmaul</dc:creator>
    </item>
    <item>
      <title>Certificates in P and Subquadratic-Time Computation of Radius, Diameter, and all Eccentricities in Graphs</title>
      <link>https://arxiv.org/abs/1803.04660</link>
      <description>arXiv:1803.04660v3 Announce Type: replace-cross 
Abstract: In the context of fine-grained complexity, we investigate the notion of certificate enabling faster polynomial-time algorithms. We specifically target radius (minimum eccentricity), diameter (maximum eccentricity), and all-eccentricity computations for which quadratic-time lower bounds are known under plausible conjectures. In each case, we introduce a notion of certificate as a specific set of nodes from which appropriate bounds on all eccentricities can be derived in subquadratic time when this set has sublinear size. The existence of small certificates is a barrier against SETH-based lower bounds for these problems. We indeed prove that for graph classes with small certificates, there exist randomized subquadratic-time algorithms for computing the radius, the diameter, and all eccentricities respectively.Moreover, these notions of certificates are tightly related to algorithms probing the graph through one-to-all distance queries and allow to explain the efficiency of practical radius and diameter algorithms from the literature. Our formalization enables a novel primal-dual analysis of a classical approach for diameter computation that leads to algorithms for radius, diameter and all eccentricities with theoretical guarantees with respect to certain graph parameters. This is complemented by experimental results on various types of real-world graphs showing that these parameters appear to be low in practice. Finally, we obtain refined results for several graph classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:1803.04660v3</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feodor F. Dragan (UniBuc, ICI), Guillaume Ducoffe (UniBuc, ICI), Michel Habib (IRIF), Laurent Viennot (DI-ENS, ARGO)</dc:creator>
    </item>
    <item>
      <title>Fair Secretaries with Unfair Predictions</title>
      <link>https://arxiv.org/abs/2411.09854</link>
      <description>arXiv:2411.09854v2 Announce Type: replace-cross 
Abstract: Algorithms with predictions is a recent framework for decision-making under uncertainty that leverages the power of machine-learned predictions without making any assumption about their quality. The goal in this framework is for algorithms to achieve an improved performance when the predictions are accurate while maintaining acceptable guarantees when the predictions are erroneous. A serious concern with algorithms that use predictions is that these predictions can be biased and, as a result, cause the algorithm to make decisions that are deemed unfair. We show that this concern manifests itself in the classical secretary problem in the learning-augmented setting -- the state-of-the-art algorithm can have zero probability of accepting the best candidate, which we deem unfair, despite promising to accept a candidate whose expected value is at least $\max\{\Omega (1) , 1 - O(\epsilon)\}$ times the optimal value, where $\epsilon$ is the prediction error. We show how to preserve this promise while also guaranteeing to accept the best candidate with probability $\Omega(1)$. Our algorithm and analysis are based on a new "pegging" idea that diverges from existing works and simplifies/unifies some of their results. Finally, we extend to the $k$-secretary problem and complement our theoretical analysis with experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09854v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Balkanski, Will Ma, Andreas Maggiori</dc:creator>
    </item>
  </channel>
</rss>
