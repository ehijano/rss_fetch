<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.IT</link>
    <description>math.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 May 2025 04:01:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Adaptive Implicit-Based Deep Learning Channel Estimation for 6G Communications</title>
      <link>https://arxiv.org/abs/2505.17421</link>
      <description>arXiv:2505.17421v1 Announce Type: new 
Abstract: With the widespread deployment of fifth-generation (5G) wireless networks, research on sixth-generation (6G) technology is gaining momentum. Artificial Intelligence (AI) is anticipated to play a significant role in 6G, particularly through integration with the physical layer for tasks such as channel estimation. Considering resource limitations in real systems, the AI algorithm should be designed to have the ability to balance the accuracy and resource consumption according to the scenarios dynamically. However, conventional explicit multilayer-stacked Deep Learning (DL) models struggle to adapt due to their heavy reliance on the structure of deep neural networks. This article proposes an adaptive Implicit-layer DL Channel Estimation Network (ICENet) with a lightweight framework for vehicle-to-everything communications. This novel approach balances computational complexity and channel estimation accuracy by dynamically adjusting computational resources based on input data conditions, such as channel quality. Unlike explicit multilayer-stacked DL-based channel estimation models, ICENet offers a flexible framework, where specific requirements can be achieved by adaptively changing the number of iterations of the iterative layer. Meanwhile, ICENet requires less memory while maintaining high performance. The article concludes by highlighting open research challenges and promising future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17421v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhen Qiao, Jiang Xue, Junkai Zhang, Guanzhang Liu, Xiaoqin Ma, Runhua Li, Faheem A. Khan, John S. Thompson, Zongben Xu</dc:creator>
    </item>
    <item>
      <title>Characterization of IRS-aided Indoor Wireless Virtual-Reality with Hybrid Beamforming</title>
      <link>https://arxiv.org/abs/2505.17737</link>
      <description>arXiv:2505.17737v1 Announce Type: new 
Abstract: This paper introduces an optimum solution for a utility function that increases spectral efficiency in wireless Virtual Reality (VR) systems. This system uses Multi-user Multiple Input Multiple Output Orthogonal Frequency Division Multiplexing (MU-MIMO OFDM) with hybrid beamforming in indoor Intelligent Reflecting Surface (IRS) based Downlink (DL) scenario. Given the critical need to maximize the rate for transmitting VR traffic to meet the low-latency requirements, a substantial bandwidth allocation is essential. This bandwidth is assumed to be in the mmWave band, according to the IEEE 802.11ad/ay standard. The proposed utility function takes into account various delays, including processing, transmission and queuing delays, on both DL and Uplink (UL). Moreover, the relation between transmission delay and the utility function is examined in different Signal-to-Noise Ratio (SNR) levels, using both mean and minimum channel gain metrics. An optimization approach is applied to iteratively determine the IRS phase shifts and effective channel gain. The simulation results are benchmarked against NS3 simulations, showing a high degree of consistency. With an average accuracy of 81.57% the calculated DL and UL rates match the NS3 results when considering the IRS. Also, our proposed method achieves superior performance in the case of complexity over the existing designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17737v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nasim Alikhani, Abbas Mohammadi</dc:creator>
    </item>
    <item>
      <title>Hybrid Mamba-Transformer Decoder for Error-Correcting Codes</title>
      <link>https://arxiv.org/abs/2505.17834</link>
      <description>arXiv:2505.17834v1 Announce Type: new 
Abstract: We introduce a novel deep learning method for decoding error correction codes based on the Mamba architecture, enhanced with Transformer layers. Our approach proposes a hybrid decoder that leverages Mamba's efficient sequential modeling while maintaining the global context capabilities of Transformers. To further improve performance, we design a novel layer-wise masking strategy applied to each Mamba layer, allowing selective attention to relevant code features at different depths. Additionally, we introduce a progressive layer-wise loss, supervising the network at intermediate stages and promoting robust feature extraction throughout the decoding process. Comprehensive experiments across a range of linear codes demonstrate that our method significantly outperforms Transformer-only decoders and standard Mamba models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17834v1</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shy-el Cohen, Yoni Choukroun, Eliya Nachmani</dc:creator>
    </item>
    <item>
      <title>Protograph-Based LDPC Codes with Local Irregularity</title>
      <link>https://arxiv.org/abs/2505.17837</link>
      <description>arXiv:2505.17837v1 Announce Type: new 
Abstract: Forward error correcting (FEC) codes are used in many communication standards with a wide range of re quirements. FEC codes should work close to capacity, achieve low error floors, and have low decoding complexity. In this paper, we propose a novel category of low-density parity-check (LDPC) codes, based on protograph codes with local irregularity. This new code family generalizes conventional protograph-based LDPC codes and is capable of reducing the iterative decoding threshold of the conventional counterpart. We introduce an adapted version of the protograph extrinsic information transfer (PEXIT) algorithm to estimate decoding thresholds on the binary input additive white Gaussian noise channel, perform optimiza tions on the local irregularity, and simulate the performance of some constructed codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17837v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent W\"ust, Erdem Eray Cil, Laurent Schmalen</dc:creator>
    </item>
    <item>
      <title>Toward Optimal ANC: Establishing Mutual Information Lower Bound</title>
      <link>https://arxiv.org/abs/2505.17877</link>
      <description>arXiv:2505.17877v1 Announce Type: new 
Abstract: Active Noise Cancellation (ANC) algorithms aim to suppress unwanted acoustic disturbances by generating anti-noise signals that destructively interfere with the original noise in real time. Although recent deep learning-based ANC algorithms have set new performance benchmarks, there remains a shortage of theoretical limits to rigorously assess their improvements. To address this, we derive a unified lower bound on cancellation performance composed of two components. The first component is information-theoretic: it links residual error power to the fraction of disturbance entropy captured by the anti-noise signal, thereby quantifying limits imposed by information-processing capacity. The second component is support-based: it measures the irreducible error arising in frequency bands that the cancellation path cannot address, reflecting fundamental physical constraints. By taking the maximum of these two terms, our bound establishes a theoretical ceiling on the Normalized Mean Squared Error (NMSE) attainable by any ANC algorithm. We validate its tightness empirically on the NOISEX dataset under varying reverberation times, demonstrating robustness across diverse acoustic conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17877v1</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Derrida, Shahar Lutati, Eliya Nachmani</dc:creator>
    </item>
    <item>
      <title>Exact Quantum Algorithm for Unit Commitment Optimization based on Partially Connected Quantum Neural Networks</title>
      <link>https://arxiv.org/abs/2411.11369</link>
      <description>arXiv:2411.11369v1 Announce Type: cross 
Abstract: The quantum hybrid algorithm has become a very promising and speedily method today for solving the larger-scale optimization in the noisy intermediate-scale quantum (NISQ) era. The unit commitment (UC) problem is a fundamental problem in the power system which aims to satisfy a balance load with minimal cost. In this paper, we focus on the implement of the UC-solving by exact quantum algorithms based on the quantum neural network (QNN). This method is tested with up to 10-unit system with the balance load constraint. In order to improve the computing precision and reduce the network complexity, we suggest the knowledge-based partially connected quantum neural network (PCQNN). The results show that the exact solutions can be obtained by the improved algorithm and the depth of the quantum circuit can be reduced simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11369v1</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Liu, Xu Zhou, Zhuojun Zhou, Le Luo</dc:creator>
    </item>
    <item>
      <title>From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning</title>
      <link>https://arxiv.org/abs/2505.17117</link>
      <description>arXiv:2505.17117v1 Announce Type: cross 
Abstract: Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17117v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Shani, Dan Jurafsky, Yann LeCun, Ravid Shwartz-Ziv</dc:creator>
    </item>
    <item>
      <title>GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints</title>
      <link>https://arxiv.org/abs/2505.17327</link>
      <description>arXiv:2505.17327v1 Announce Type: cross 
Abstract: The proliferation of Large Language Models (LLMs) in late 2022 has impacted academic writing, threatening credibility, and causing institutional uncertainty. We seek to determine the degree to which LLMs are used to generate critical text as opposed to being used for editing, such as checking for grammar errors or inappropriate phrasing. In our study, we analyze arXiv papers for stylistic segmentation, which we measure by varying a PELT threshold against a Bayesian classifier trained on GPT-regenerated text. We find that LLM-attributed language is not predictive of stylistic segmentation, suggesting that when authors use LLMs, they do so uniformly, reducing the risk of hallucinations being introduced into academic preprints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17327v1</guid>
      <category>cs.CL</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Soren DeHaan, Yuanze Liu, Johan Bollen, Sa'ul A. Blanco</dc:creator>
    </item>
    <item>
      <title>VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR</title>
      <link>https://arxiv.org/abs/2505.17423</link>
      <description>arXiv:2505.17423v1 Announce Type: cross 
Abstract: Many decision-making tasks, where both accuracy and efficiency matter, still require human supervision. For example, tasks like traffic officers reviewing hour-long dashcam footage or researchers screening conference videos can benefit from concise summaries that reduce cognitive load and save time. Yet current vision-language models (VLMs) often produce verbose, redundant outputs that hinder task performance. Existing video caption evaluation depends on costly human annotations and overlooks the summaries' utility in downstream tasks. We address these gaps with Video-to-text Information Bottleneck Evaluation (VIBE), an annotation-free method that scores VLM outputs using two metrics: grounding (how well the summary aligns with visual content) and utility (how informative it is for the task). VIBE selects from randomly sampled VLM outputs by ranking them according to the two scores to support effective human decision-making. Human studies on LearningPaper24, SUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE consistently improve performance-boosting task accuracy by up to 61.23% and reducing response time by 75.77% compared to naive VLM summaries or raw video.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17423v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shenghui Chen, Po-han Li, Sandeep Chichali, Ufuk Topcu</dc:creator>
    </item>
    <item>
      <title>Efficient compression of neural networks and datasets</title>
      <link>https://arxiv.org/abs/2505.17469</link>
      <description>arXiv:2505.17469v1 Announce Type: cross 
Abstract: We compare, improve, and contribute methods that substantially decrease the number of parameters of neural networks while maintaining high test accuracy. When applying our methods to minimize description length, we obtain very effective data compression algorithms. In particular, we develop a probabilistic reformulation of $\ell_0$ regularized optimization for nonlinear models that does not require Monte-Carlo sampling and thus improves upon previous methods. We also improve upon methods involving smooth approximations to the $\ell_0$ norm, and investigate layerwise methods. We compare the methods on different architectures and datasets, including convolutional networks trained on image datasets and transformers trained on parts of Wikipedia. We also created a synthetic teacher-student setup to investigate compression in a controlled continuous setting. Finally, we conceptually relate compression algorithms to Solomonoff's theory of inductive inference and empirically verify the prediction that regularized models can exhibit more sample-efficient convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17469v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lukas Silvester Barth, Paulo von Petersenn</dc:creator>
    </item>
    <item>
      <title>GPS-Aided Deep Learning for Beam Prediction and Tracking in UAV mmWave Communication</title>
      <link>https://arxiv.org/abs/2505.17530</link>
      <description>arXiv:2505.17530v1 Announce Type: cross 
Abstract: Millimeter-wave (mmWave) communication enables high data rates for cellular-connected Unmanned Aerial Vehicles (UAVs). However, a robust beam management remains challenging due to significant path loss and the dynamic mobility of UAVs, which can destabilize the UAV-base station (BS) link. This research presents a GPS-aided deep learning (DL) model that simultaneously predicts current and future optimal beams for UAV mmWave communications, maintaining a Top-1 prediction accuracy exceeding 70% and an average power loss below 0.6 dB across all prediction steps. These outcomes stem from a proposed data set splitting method ensuring balanced label distribution, paired with a GPS preprocessing technique that extracts key positional features, and a DL architecture that maps sequential position data to beam index predictions. The model reduces overhead by approximately 93% (requiring the training of 2 ~ 3 beams instead of 32 beams) with 95% beam prediction accuracy guarantees, and ensures 94% to 96% of predictions exhibit mean power loss not exceeding 1 dB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17530v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vendi Ardianto Nugroho, Byung Moo Lee</dc:creator>
    </item>
    <item>
      <title>Nodal surfaces in $\mathbb{P}^3$ and coding theory</title>
      <link>https://arxiv.org/abs/2505.17531</link>
      <description>arXiv:2505.17531v1 Announce Type: cross 
Abstract: To each nodal hypersurface one can associate a binary linear code. Here we show that the binary linear code associated to sextics in $\mathbb{P}^3$ with the maximum number of $65$ nodes, as e.g. the Barth sextic, is unique. We also state possible candidates for codes that might be associated with a hypothetical septic attaining the currently best known upper bound for the maximum number of nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17531v1</guid>
      <category>math.CO</category>
      <category>cs.IT</category>
      <category>math.AG</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sascha Kurz</dc:creator>
    </item>
    <item>
      <title>Optimal Decision Rules for Composite Binary Hypothesis Testing under Neyman-Pearson Framework</title>
      <link>https://arxiv.org/abs/2505.17851</link>
      <description>arXiv:2505.17851v1 Announce Type: cross 
Abstract: The composite binary hypothesis testing problem within the Neyman-Pearson framework is considered. The goal is to maximize the expectation of a nonlinear function of the detection probability, integrated with respect to a given probability measure, subject to a false-alarm constraint. It is shown that each power function can be realized by a generalized Bayes rule that maximizes an integrated rejection probability with respect to a finite signed measure. For a simple null hypothesis and a composite alternative, optimal single-threshold decision rules based on an appropriately weighted likelihood ratio are derived. The analysis is extended to composite null hypotheses, including both average and worst-case false-alarm constraints, resulting in modified optimal threshold rules. Special cases involving exponential family distributions and numerical examples are provided to illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17851v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanglei Song, Berkan Dulek, Sinan Gezici</dc:creator>
    </item>
    <item>
      <title>The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks</title>
      <link>https://arxiv.org/abs/2505.17958</link>
      <description>arXiv:2505.17958v1 Announce Type: cross 
Abstract: We study the high-dimensional asymptotics of empirical risk minimization (ERM) in over-parametrized two-layer neural networks with quadratic activations trained on synthetic data. We derive sharp asymptotics for both training and test errors by mapping the $\ell_2$-regularized learning problem to a convex matrix sensing task with nuclear norm penalization. This reveals that capacity control in such networks emerges from a low-rank structure in the learned feature maps. Our results characterize the global minima of the loss and yield precise generalization thresholds, showing how the width of the target function governs learnability. This analysis bridges and extends ideas from spin-glass methods, matrix factorization, and convex optimization and emphasizes the deep link between low-rank matrix sensing and learning in quadratic neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17958v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vittorio Erba, Emanuele Troiani, Lenka Zdeborov\'a, Florent Krzakala</dc:creator>
    </item>
    <item>
      <title>A new measure of dependence: Integrated $R^2$</title>
      <link>https://arxiv.org/abs/2505.18146</link>
      <description>arXiv:2505.18146v1 Announce Type: cross 
Abstract: We propose a new measure of dependence that quantifies the degree to which a random variable $Y$ depends on a random vector $X$. This measure is zero if and only if $Y$ and $X$ are independent, and equals one if and only if $Y$ is a measurable function of $X$. We introduce a simple and interpretable estimator that is comparable in ease of computation to classical correlation coefficients such as Pearson's, Spearman's, or Chatterjee's. Building on this coefficient, we develop a model-free variable selection algorithm, feature ordering by dependence (FORD), inspired by FOCI. FORD requires no tuning parameters and is provably consistent under suitable sparsity assumptions. We demonstrate its effectiveness and improvements over FOCI through experiments on both synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18146v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mona Azadkia, Pouya Roudaki</dc:creator>
    </item>
    <item>
      <title>Function-Correcting $b$-symbol Codes for Locally $(\lambda, \rho,b)$-Functions</title>
      <link>https://arxiv.org/abs/2505.09473</link>
      <description>arXiv:2505.09473v2 Announce Type: replace 
Abstract: The family of functions plays a central role in the design and effectiveness of function-correcting codes. By focusing on a well-defined family of functions, function-correcting codes can be constructed with minimal length while still ensuring full error detection or correction within that family. In this work, we explore the concept of locally $(\lambda,\rho)$-functions for $b$-symbol read channels and investigate the redundancy of the corresponding function-correcting $b$-symbol codes(FCBSC) by introducing the notions of locally $(\lambda,\rho,b)$-functions. First, we discuss the possible values of $\lambda$ and $\rho$ for which any function can be considered as locally $(\lambda,\rho)$-function in $b$-symbol metric. The findings improve some known results in the Hamming metric and present several new results in the $b$-symbol metric. Then we investigate the redundancy of $(f,t)$-FCBSC for locally $(\lambda,\rho,b)$-functions. We establish a recurrence relation between the optimal redundancy of $(f,t)$ -function-correcting codes for the $(b+1)$-read and $b$-read channels. We establish an upper bound on the redundancy of $(f,t)$-function-correcting $b$-symbol codes for general locally ($\lambda,\rho$, $b$)-functions by linking it to the minimum achievable length of $b$-symbol error-correcting codes and traditional Hamming-metric codes, given a fixed number of codewords and a specified minimum distance. We derive some explicit upper bounds on the redundancy of $(f,t)$-function-correcting $b$-symbol codes for $\rho=2t$. Moreover, for the case where $b=1$, we show that a locally ($3,2t,1$)-function achieves the optimal redundancy of $3t$. Additionally, we explicitly investigate locality and redundancy for the $b$-symbol weight distribution function for $b\geq1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09473v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gyanendra K. Verma, Anamika Singh, Abhay Kumar Singh</dc:creator>
    </item>
    <item>
      <title>Compression via Pre-trained Transformers: A Study on Byte-Level Multimodal Data</title>
      <link>https://arxiv.org/abs/2410.05078</link>
      <description>arXiv:2410.05078v2 Announce Type: replace-cross 
Abstract: Foundation models are strong data compressors, but when accounting for their parameter size, their compression ratios are inferior to standard compression algorithms. Naively reducing the parameter count does not necessarily help as it deteriorates predictions and, accordingly, compression. We conduct a large-scale empirical study to find a sweet spot where pre-trained vanilla transformers can achieve competitive compression ratios. To this end, we train models on 165GB of raw byte sequences of either text, image, or audio data (and all possible combinations of the three) and then compress 1GB of out-of-distribution (OOD) data from each modality. We find that relatively small models (millions of parameters) can outperform standard general-purpose compression algorithms (gzip, LZMA2) and even domain-specific compressors (PNG, JPEG-XL, FLAC) $\unicode{x2013}$ even when accounting for parameter size. We achieve, e.g., the lowest compression ratio of 0.49 on OOD audio data (vs. 0.54 for FLAC). We conduct extensive ablations and hyperparameter sweeps to study the impact of model- and dataset scale, and we investigate the effect of unimodal versus multimodal training. We find that even small models can be trained to perform well on multiple modalities, but unlike large-scale foundation models, transfer to unseen modalities is generally weak.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05078v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Heurtel-Depeiges, Anian Ruoss, Joel Veness, Tim Genewein</dc:creator>
    </item>
    <item>
      <title>Optimal additive quaternary codes of dimension $3.5$</title>
      <link>https://arxiv.org/abs/2410.07650</link>
      <description>arXiv:2410.07650v3 Announce Type: replace-cross 
Abstract: After the optimal parameters of additive quaternary codes of dimension $k\le 3$ have been determined there is some recent activity to settle the next case of dimension $k=3.5$. Here we complete dimension $k=3.5$ and give partial results for dimension $k=4$. We also solve the problem of the optimal parameters of additive quaternary codes of arbitrary dimension when assuming a sufficiently large minimum distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07650v3</guid>
      <category>math.CO</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sascha Kurz</dc:creator>
    </item>
  </channel>
</rss>
