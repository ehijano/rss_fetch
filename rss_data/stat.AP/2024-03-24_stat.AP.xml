<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Mar 2024 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Fractional Tackles: Leveraging Player Tracking Data for Within-Play Tackling Evaluation in American Football</title>
      <link>https://arxiv.org/abs/2403.14769</link>
      <description>arXiv:2403.14769v1 Announce Type: new 
Abstract: Tackling is a fundamental defensive move in American football, with the main purpose of stopping the forward motion of the ball-carrier. However, current tackling metrics are manually recorded outcomes that are inherently flawed due to their discrete and subjective nature. Using player tracking data, we present a novel framework for assessing tackling contribution in a continuous and objective manner. Our approach first identifies when a defender is in a ``contact window'' of the ball-carrier during a play, before assigning value to each window and the players involved. This enables us to devise a new metric called fractional tackles, which credits defenders for halting the ball-carrier's forward motion toward the end zone. We demonstrate that fractional tackles overcome the shortcomings of traditional metrics such as tackles and assists, by providing greater variation and measurable information for players lacking recorded statistics like defensive linemen. We view our contribution as a significant step forward in measuring defensive performance in American football and a clear demonstration of the capabilities of player tracking data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14769v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quang Nguyen, Ruitong Jiang, Meg Ellingwood, Ronald Yurko</dc:creator>
    </item>
    <item>
      <title>Rejoinder to "Perspectives on `harm' in personalized medicine -- an alternative perspective"</title>
      <link>https://arxiv.org/abs/2403.14869</link>
      <description>arXiv:2403.14869v1 Announce Type: new 
Abstract: In our original article (Sarvet &amp; Stensrud, 2024), we examine twin definitions of "harm" in personalized medicine: one based on predictions of individuals' unmeasurable response types (counterfactual harm), and another based solely on the observations of experiments (interventionist harm). In their commentary, Mueller &amp; Pearl (2024) (MP) read our review as an argument that "counterfactual logic should [...] be purged from consideration of harm and benefit" and "strongly object [...] that a rational decision maker may well apply the interventional perspective to the exclusion of counterfactual considerations." Here we show that this objection is misguided. We analyze MP's examples and derive a general result, showing that determinations of harm through interventionist and counterfactual analyses will always concur. Therefore, individuals who embrace counterfactual formulations and those who object to their use will make equivalent decisions in uncontroversial settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14869v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron L. Sarvet, Mats J. Stensrud</dc:creator>
    </item>
    <item>
      <title>Analysis of Log Data from an International Online Educational Assessment System: A Multi-state Survival Modeling Approach to Reaction Time between and across Action Sequence</title>
      <link>https://arxiv.org/abs/2403.14908</link>
      <description>arXiv:2403.14908v1 Announce Type: new 
Abstract: With increasingly available computer-based or online assessments, researchers have shown keen interest in analyzing log data to improve our understanding of test takers' problem-solving processes. In this paper, we propose a multi-state survival model (MSM) to action sequence data from log files, focusing on modeling test takers' reaction times between actions, in order to investigate which factors and how they influence test takers' transition speed between actions. In particular, we focus on the effects of the occurrence and timing of key actions that differentiate correct answers from incorrect answers. We demonstrate our proposed approach with problem-solving test items from the the Programme for International Assessment of Adult Competence (PIAAC) problem-solving test items.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14908v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jina Park, Ick Hoon Jin, Minjeong Jeon</dc:creator>
    </item>
    <item>
      <title>Wastewater-based Epidemiology for COVID-19 Surveillance: A Survey</title>
      <link>https://arxiv.org/abs/2403.15291</link>
      <description>arXiv:2403.15291v1 Announce Type: new 
Abstract: The pandemic of COVID-19 has imposed tremendous pressure on public health systems and social economic ecosystems over the past years. To alleviate its social impact, it is important to proactively track the prevalence of COVID-19 within communities. The traditional way to estimate the disease prevalence is to estimate from reported clinical test data or surveys. However, the coverage of clinical tests is often limited and the tests can be labor-intensive, requires reliable and timely results, and consistent diagnostic and reporting criteria. Recent studies revealed that patients who are diagnosed with COVID-19 often undergo fecal shedding of SARS-CoV-2 virus into wastewater, which makes wastewater-based epidemiology (WBE) for COVID-19 surveillance a promising approach to complement traditional clinical testing. In this paper, we survey the existing literature regarding WBE for COVID-19 surveillance and summarize the current advances in the area. Specifically, we have covered the key aspects of wastewater sampling, sample testing, and presented a comprehensive and organized summary of wastewater data analytical methods. Finally, we provide the open challenges on current wastewater-based COVID-19 surveillance studies, aiming to encourage new ideas to advance the development of effective wastewater-based surveillance systems for general infectious diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15291v1</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Chen, Gursharn Kaur, Aniruddha Adiga, Baltazar Espinoza, Srinivasan Venkatramanan, Andrew Warren, Bryan Lewis, Justin Crow, Rekha Singh, Alexandra Lorentz, Denise Toney, Madhav Marathe</dc:creator>
    </item>
    <item>
      <title>ClickTree: A Tree-based Method for Predicting Math Students' Performance Based on Clickstream Data</title>
      <link>https://arxiv.org/abs/2403.14664</link>
      <description>arXiv:2403.14664v1 Announce Type: cross 
Abstract: The prediction of student performance and the analysis of students' learning behavior play an important role in enhancing online courses. By analysing a massive amount of clickstream data that captures student behavior, educators can gain valuable insights into the factors that influence academic outcomes and identify areas of improvement in courses. In this study, we developed ClickTree, a tree-based methodology, to predict student performance in mathematical assignments based on students' clickstream data. We extracted a set of features, including problem-level, assignment-level and student-level features, from the extensive clickstream data and trained a CatBoost tree to predict whether a student successfully answers a problem in an assignment. The developed method achieved an AUC of 0.78844 in the Educational Data Mining Cup 2023 and ranked second in the competition. Furthermore, our results indicate that students encounter more difficulties in the problem types that they must select a subset of answers from a given set as well as problem subjects of Algebra II. Additionally, students who performed well in answering end-unit assignment problems engaged more with in-unit assignments and answered more problems correctly, while those who struggled had higher tutoring request rate. The proposed method can be utilized to improve students' learning experiences, and the above insights can be integrated into mathematical courses to enhance students' learning outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14664v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Narjes Rohani, Behnam Rohani, Areti Manataki</dc:creator>
    </item>
    <item>
      <title>Creating a Spatial Vulnerability Index for Environmental Health</title>
      <link>https://arxiv.org/abs/2403.14954</link>
      <description>arXiv:2403.14954v1 Announce Type: cross 
Abstract: Extreme natural hazards are increasing in frequency and intensity. These natural changes in our environment, combined with man-made pollution, have substantial economic, social and health impacts globally. The impact of the environment on human health (environmental health) is becoming well understood in international research literature. However, there are significant barriers to understanding key characteristics of this impact, related to substantial data volumes, data access rights and the time required to compile and compare data over regions and time. This study aims to reduce these barriers in Australia by creating an open data repository of national environmental health data and presenting a methodology for the production of health outcome-weighted population vulnerability indices related to extreme heat, extreme cold and air pollution at various temporal and geographical resolutions.
  Current state-of-the-art methods for the calculation of vulnerability indices include equal weight percentile ranking and the use of principal component analysis (PCA). The weighted vulnerability index methodology proposed in this study offers an advantage over others in the literature by considering health outcomes in the calculation process. The resulting vulnerability percentiles more clearly align population sensitivity and adaptive capacity with health risks. The temporal and spatial resolutions of the indices enable national monitoring on a scale never before seen across Australia. Additionally, we show that a weekly temporal resolution can be used to identify spikes in vulnerability due to changes in relative national environmental exposure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14954v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aiden Price, Kerrie Mengersen, Michael Rigby, Paula Fi\'evez</dc:creator>
    </item>
    <item>
      <title>Optimal Contract Design for End-of-Life Care Payments</title>
      <link>https://arxiv.org/abs/2403.15099</link>
      <description>arXiv:2403.15099v1 Announce Type: cross 
Abstract: A large fraction of total healthcare expenditure occurs due to end-of-life (EOL) care, which means it is important to study the problem of more carefully incentivizing necessary versus unnecessary EOL care because this has the potential to reduce overall healthcare spending. This paper introduces a principal-agent model that integrates a mixed payment system of fee-for-service and pay-for-performance in order to analyze whether it is possible to better align healthcare provider incentives with patient outcomes and cost-efficiency in EOL care. The primary contributions are to derive optimal contracts for EOL care payments using a principal-agent framework under three separate models for the healthcare provider, where each model considers a different level of risk tolerance for the provider. We derive these optimal contracts by converting the underlying principal-agent models from a bilevel optimization problem into a single-level optimization problem that can be analytically solved. Our results are demonstrated using a simulation where an optimal contract is used to price intracranial pressure monitoring for traumatic brain injuries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15099v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Muyan Jiang, Ying Chen, Xin Chen, Javad Lavaei, Anil Aswani</dc:creator>
    </item>
    <item>
      <title>Paddy: Evolutionary Optimization Algorithm for Chemical Systems and Spaces</title>
      <link>https://arxiv.org/abs/2403.15101</link>
      <description>arXiv:2403.15101v1 Announce Type: cross 
Abstract: Optimization of chemical systems and processes have been enhanced and enabled by the guidance of algorithms and analytical approaches. While many methods will systematically investigate how underlying variables govern a given outcome, there is often a substantial number of experiments needed to accurately model these relations. As chemical systems increase in complexity, inexhaustive processes must propose experiments that efficiently optimize the underlying objective, while ideally avoiding convergence on unsatisfactory local minima. We have developed the Paddy software package around the Paddy Field Algorithm, a biologically inspired evolutionary optimization algorithm that propagates parameters without direct inference of the underlying objective function. Benchmarked against the Tree of Parzen Estimator, a Bayesian algorithm implemented in the Hyperopt software Library, Paddy displays efficient optimization with lower runtime, and avoidance of early convergence. Herein we report these findings for the cases of: global optimization of a two-dimensional bimodal distribution, interpolation of an irregular sinusoidal function, hyperparameter optimization of an artificial neural network tasked with classification of solvent for reaction components, and targeted molecule generation via optimization of input vectors for a decoder network. We anticipate that the facile nature of Paddy will serve to aid in automated experimentation, where minimization of investigative trials and or diversity of suitable solutions is of high priority.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15101v1</guid>
      <category>math.OC</category>
      <category>physics.chem-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Armen Beck, Jonathan Fine, Gaurav Chopra</dc:creator>
    </item>
    <item>
      <title>Polarization Holes as an Indicator of Magnetic Field-Angular Momentum Alignment I. Initial Tests</title>
      <link>https://arxiv.org/abs/2403.15280</link>
      <description>arXiv:2403.15280v1 Announce Type: cross 
Abstract: The formation of protostellar disks is still a mystery, largely due to the difficulties in observations that can constrain theories. For example, the 3D alignment between the rotation of the disk and the magnetic fields (B-fields) in the formation environment is critical in some models, but so far impossible to observe. Here, we study the possibility of probing the alignment between B-field and disk rotation using ``polarization holes'' (PHs). PHs are widely observed and are caused by unresolved B-field structures. With ideal magnetohydrodynamic (MHD) simulations, we demonstrate that different initial alignments between B-field and angular momentum (AM) can result in B-field structures that are distinct enough to produce distinguishable PHs. Thus PHs can potentially serve as probes for alignments between B-field and AM in disk formation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15280v1</guid>
      <category>astro-ph.GA</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lijun Wang, Zhuo Cao, Xiaodan Fan, Hua-bai Li</dc:creator>
    </item>
    <item>
      <title>A data-informed mathematical model of microglial cell dynamics during ischemic stroke in the middle cerebral artery</title>
      <link>https://arxiv.org/abs/2403.15284</link>
      <description>arXiv:2403.15284v1 Announce Type: cross 
Abstract: Neuroinflammation immediately follows the onset of ischemic stroke in the middle cerebral artery. During this process, microglial cells are activated in and recruited to the penumbra. Microglial cells can be activated into two different phenotypes: M1, which can worsen brain injury; or M2, which can aid in long-term recovery. In this study, we contribute a summary of experimental data on microglial cell counts in the penumbra following ischemic stroke induced by middle cerebral artery occlusion (MCAO) in mice and compile available data sets into a single set suitable for time series analysis. Further, we formulate a mathematical model of microglial cells in the penumbra during ischemic stroke due to MCAO. Through use of global sensitivity analysis and Markov Chain Monte Carlo (MCMC)-based parameter estimation, we analyze the effects of the model parameters on the number of M1 and M2 cells in the penumbra and fit identifiable parameters to the compiled experimental data set. We utilize results from MCMC parameter estimation to ascertain uncertainty bounds and forward predictions for the number of M1 and M2 microglial cells over time. Results demonstrate the significance of parameters related to M1 and M2 activation on the number of M1 and M2 microglial cells. Simulations further suggest that potential outliers in the observed data may be omitted and forecast predictions suggest a lingering inflammatory response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15284v1</guid>
      <category>q-bio.CB</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Amato, Andrea Arnold</dc:creator>
    </item>
    <item>
      <title>A User-Focused Approach to Evaluating Probabilistic and Categorical Forecasts</title>
      <link>https://arxiv.org/abs/2311.18258</link>
      <description>arXiv:2311.18258v2 Announce Type: replace 
Abstract: A user-focused verification approach for evaluating probability forecasts of binary outcomes (also known as probabilistic classifiers) is demonstrated that is (i) based on proper scoring rules, (ii) focuses on user decision thresholds, and (iii) provides actionable insights. It is argued that when categorical performance diagrams and the critical success index are used to evaluate overall predictive performance, rather than the discrimination ability of probabilistic forecasts, they may produce misleading results. Instead, Murphy diagrams are shown to provide better understanding of overall predictive performance as a function of user probabilistic decision threshold. It is illustrated how to select a proper scoring rule, based on the relative importance of different user decision thresholds, and how this choice impacts scores of overall predictive performance and supporting measures of discrimination and calibration. These approaches and ideas are demonstrated using several probabilistic thunderstorm forecast systems as well as synthetic forecast data. Furthermore, a fair method for comparing the performance of probabilistic and categorical forecasts is illustrated using the FIxed Risk Multicategorical (FIRM) score, which is a proper scoring rule directly connected to values on the Murphy diagram. While the methods are illustrated using thunderstorm forecasts, they are applicable for evaluating probabilistic forecasts for any situation with binary outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18258v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Loveday, Robert Taggart, Mohammadreza Khanarmuei</dc:creator>
    </item>
    <item>
      <title>Forensic Science and How Statistics Can Help It: Evidence, Hypothesis Testing, and Graphical Models</title>
      <link>https://arxiv.org/abs/2312.17735</link>
      <description>arXiv:2312.17735v2 Announce Type: replace 
Abstract: The persistent issue of wrongful convictions in the United States emphasizes the need for scrutiny and improvement of the criminal justice system. While statistical methods for the evaluation of forensic evidence, including glass, fingerprints, and DNA, have significantly contributed to solving intricate crimes, there is a notable lack of national-level standards to ensure the appropriate application of statistics in forensic investigations. We discuss the obstacles in the application of statistics in court, and emphasize the importance of making statistical interpretation accessible to non-statisticians, especially those who make decisions about potentially innocent individuals. We investigate the use and misuse of statistical methods in crime investigations, in particular the likelihood ratio approach. We further describe the use of graphical models, where hypotheses and evidence can be represented as nodes connected by arrows signifying association or causality. We emphasize the advantages of special graph structures, such as object-oriented Bayesian networks and chain event graphs, which allow for the concurrent examination of evidence of various nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17735v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangyu Xu, Giuseppe Vinci</dc:creator>
    </item>
    <item>
      <title>Learned Image Compression with Dual-Branch Encoder and Conditional Information Coding</title>
      <link>https://arxiv.org/abs/2401.11093</link>
      <description>arXiv:2401.11093v2 Announce Type: replace 
Abstract: Recent advancements in deep learning-based image compression are notable. However, prevalent schemes that employ a serial context-adaptive entropy model to enhance rate-distortion (R-D) performance are markedly slow. Furthermore, the complexities of the encoding and decoding networks are substantially high, rendering them unsuitable for some practical applications. In this paper, we propose two techniques to balance the trade-off between complexity and performance. First, we introduce two branching coding networks to independently learn a low-resolution latent representation and a high-resolution latent representation of the input image, discriminatively representing the global and local information therein. Second, we utilize the high-resolution latent representation as conditional information for the low-resolution latent representation, furnishing it with global information, thus aiding in the reduction of redundancy between low-resolution information. We do not utilize any serial entropy models. Instead, we employ a parallel channel-wise auto-regressive entropy model for encoding and decoding low-resolution and high-resolution latent representations. Experiments demonstrate that our method is approximately twice as fast in both encoding and decoding compared to the parallelizable checkerboard context model, and it also achieves a 1.2% improvement in R-D performance compared to state-of-the-art learned image compression schemes. Our method also outperforms classical image codecs including H.266/VVC-intra (4:4:4) and some recent learned methods in rate-distortion performance, as validated by both PSNR and MS-SSIM metrics on the Kodak dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11093v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haisheng Fu, Feng Liang, Jie Liang, Zhenman Fang, Guohe Zhang, Jingning Han</dc:creator>
    </item>
    <item>
      <title>Moving Towards Automated Interstellar Boundary Explorer Data Selection with LOTUS</title>
      <link>https://arxiv.org/abs/2403.08891</link>
      <description>arXiv:2403.08891v2 Announce Type: replace 
Abstract: The Interstellar Boundary Explorer (IBEX) satellite collects data on energetic neutral atoms (ENAs) that provide insight into the heliosphere, the region surrounding our solar system and separating it from interstellar space. IBEX collects information on these particles and on extraneous ``background'' particles. While IBEX records how and when the different particles are observed, it does not distinguish between heliospheric ENA particles and incidental background particles. To address this issue, all IBEX data has historically been manually labeled as ``good'' ENA data, or ``bad'' background data. This manual culling process is incredibly time-intensive and contingent on subjective, manually-induced decision thresholds. In this paper, we develop a three-stage automated culling process, called LOTUS, that uses random forests to expedite and standardize the labelling process. In Stage 1, LOTUS uses random forests to obtain probabilities of observing true ENA particles on a per-observation basis. In Stage 2, LOTUS aggregates these probabilities to obtain predictions within small windows of time. In Stage 3, LOTUS refines these predictions. We compare the labels generated by LOTUS to those manually generated by the subject matter expert. We use various metrics to demonstrate that LOTUS is a useful automated process for supplementing and standardizing the manual culling process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08891v2</guid>
      <category>stat.AP</category>
      <category>astro-ph.GA</category>
      <category>astro-ph.IM</category>
      <category>astro-ph.SR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Madeline A. Stricklin, Lauren J. Beesley, Brian P. Weaver, Kelly R. Moran, Dave Osthus, Paul H. Janzen, Grant David Meadors, Daniel B. Reisenfeld</dc:creator>
    </item>
    <item>
      <title>Dynamic Reconfiguration of Brain Functional Network in Stroke</title>
      <link>https://arxiv.org/abs/2306.15209</link>
      <description>arXiv:2306.15209v2 Announce Type: replace-cross 
Abstract: The brain continually reorganizes its functional network to adapt to post-stroke functional impairments. Previous studies using static modularity analysis have presented global-level behavior patterns of this network reorganization. However, it is far from understood how the brain reconfigures its functional network dynamically following a stroke. This study collected resting-state functional MRI data from 15 stroke patients, with mild (n = 6) and severe (n = 9) two subgroups based on their clinical symptoms. Additionally, 15 age-matched healthy subjects were considered as controls. By applying a multilayer network method, a dynamic modular structure was recognized based on a time-resolved function network. Then dynamic network measurements (recruitment, integration, and flexibility) were calculated to characterize the dynamic reconfiguration of post-stroke brain functional networks, hence, to reveal the neural functional rebuilding process. It was found from this investigation that severe patients tended to have reduced recruitment and increased between-network integration, while mild patients exhibited low network flexibility and less network integration. It is also noted that this severity-dependent alteration in network interaction was not able to be revealed by previous studies using static methods. Clinically, the obtained knowledge of the diverse patterns of dynamic adjustment in brain functional networks observed from the brain signal could help understand the underlying mechanism of the motor, speech, and cognitive functional impairments caused by stroke attacks. The proposed method not only could be used to evaluate patients' current brain status but also has the potential to provide insights into prognosis analysis and prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15209v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JBHI.2024.3371097</arxiv:DOI>
      <arxiv:journal_reference>IEEE Journal of Biomedical and Health Informatics 2024</arxiv:journal_reference>
      <dc:creator>Kaichao Wu, Beth Jelfs, Katrina Neville, Wenzhen He, Qiang Fang</dc:creator>
    </item>
    <item>
      <title>CARE: Large Precision Matrix Estimation for Compositional Data</title>
      <link>https://arxiv.org/abs/2309.06985</link>
      <description>arXiv:2309.06985v2 Announce Type: replace-cross 
Abstract: High-dimensional compositional data are prevalent in many applications. The simplex constraint poses intrinsic challenges to inferring the conditional dependence relationships among the components forming a composition, as encoded by a large precision matrix. We introduce a precise specification of the compositional precision matrix and relate it to its basis counterpart, which is shown to be asymptotically identifiable under suitable sparsity assumptions. By exploiting this connection, we propose a composition adaptive regularized estimation (CARE) method for estimating the sparse basis precision matrix. We derive rates of convergence for the estimator and provide theoretical guarantees on support recovery and data-driven parameter tuning. Our theory reveals an intriguing trade-off between identification and estimation, thereby highlighting the blessing of dimensionality in compositional data analysis. In particular, in sufficiently high dimensions, the CARE estimator achieves minimax optimality and performs as well as if the basis were observed. We further discuss how our framework can be extended to handle data containing zeros, including sampling zeros and structural zeros. The advantages of CARE over existing methods are illustrated by simulation studies and an application to inferring microbial ecological networks in the human gut.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06985v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shucong Zhang, Huiyuan Wang, Wei Lin</dc:creator>
    </item>
    <item>
      <title>A three-step approach to production frontier estimation and the Matsuoka's distribution</title>
      <link>https://arxiv.org/abs/2311.06086</link>
      <description>arXiv:2311.06086v2 Announce Type: replace-cross 
Abstract: In this work, we introduce a three-step semiparametric methodology for the estimation of production frontiers. We consider a model inspired by the well-known Cobb-Douglas production function, wherein input factors operate multiplicatively within the model. Efficiency in the proposed model is assumed to follow a continuous univariate uniparametric distribution in $(0,1)$, referred to as Matsuoka's distribution, which is discussed in detail. Following model linearization, the first step is to semiparametrically estimate the regression function through a local linear smoother. The second step focuses on the estimation of the efficiency parameter. Finally, we estimate the production frontier through a plug-in methodology. We present a rigorous asymptotic theory related to the proposed three-step estimation, including consistency, and asymptotic normality, and derive rates for the convergences presented. Incidentally, we also study the Matsuoka's distribution, deriving its main properties. The Matsuoka's distribution exhibits a versatile array of shapes capable of effectively encapsulating the typical behavior of efficiency within production frontier models. To complement the large sample results obtained, a Monte Carlo simulation study is conducted to assess the finite sample performance of the proposed three-step methodology. An empirical application using a dataset of Danish milk producers is also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06086v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Danilo Hiroshi Matsuoka, Guilherme Pumi, Hudson da Silva Torrent, Marcio valk</dc:creator>
    </item>
  </channel>
</rss>
