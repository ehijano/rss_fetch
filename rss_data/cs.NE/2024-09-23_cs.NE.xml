<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 03:14:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Efficient General-Purpose Optical Accelerator for Neural Networks</title>
      <link>https://arxiv.org/abs/2409.12966</link>
      <description>arXiv:2409.12966v1 Announce Type: new 
Abstract: General-purpose optical accelerators (GOAs) have emerged as a promising platform to accelerate deep neural networks (DNNs) due to their low latency and energy consumption. Such an accelerator is usually composed of a given number of interleaving Mach-Zehnder- Interferometers (MZIs). This interleaving architecture, however, has a low efficiency when accelerating neural networks of various sizes due to the mismatch between weight matrices and the GOA architecture. In this work, a hybrid GOA architecture is proposed to enhance the mapping efficiency of neural networks onto the GOA. In this architecture, independent MZI modules are connected with microring resonators (MRRs), so that they can be combined to process large neural networks efficiently. Each of these modules implements a unitary matrix with inputs adjusted by tunable coefficients. The parameters of the proposed architecture are searched using genetic algorithm. To enhance the accuracy of neural networks, selected weight matrices are expanded to multiple unitary matrices applying singular value decomposition (SVD). The kernels in neural networks are also adjusted to use up the on-chip computational resources. Experimental results show that with a given number of MZIs, the mapping efficiency of neural networks on the proposed architecture can be enhanced by 21.87%, 21.20%, 24.69%, and 25.52% for VGG16 and Resnet18 on datasets Cifar10 and Cifar100, respectively. The energy consumption and computation latency can also be reduced by over 67% and 21%, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12966v1</guid>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sijie Fei, Amro Eldebiky, Grace Li Zhang, Bing Li, Ulf Schlichtmann</dc:creator>
    </item>
    <item>
      <title>Exploring the Performance-Reproducibility Trade-off in Quality-Diversity</title>
      <link>https://arxiv.org/abs/2409.13315</link>
      <description>arXiv:2409.13315v1 Announce Type: new 
Abstract: Quality-Diversity (QD) algorithms have exhibited promising results across many domains and applications. However, uncertainty in fitness and behaviour estimations of solutions remains a major challenge when QD is used in complex real-world applications. While several approaches have been proposed to improve the performance in uncertain applications, many fail to address a key challenge: determining how to prioritise solutions that perform consistently under uncertainty, in other words, solutions that are reproducible. Most prior methods improve fitness and reproducibility jointly, ignoring the possibility that they could be contradictory objectives. For example, in robotics, solutions may reliably walk at 90% of the maximum velocity in uncertain environments, while solutions that walk faster are also more prone to falling over. As this is a trade-off, neither one of these two solutions is "better" than the other. Thus, algorithms cannot intrinsically select one solution over the other, but can only enforce given preferences over these two contradictory objectives. In this paper, we formalise this problem as the performance-reproducibility trade-off for uncertain QD. We propose four new a-priori QD algorithms that find optimal solutions for given preferences over the trade-offs. We also propose an a-posteriori QD algorithm for when these preferences cannot be defined in advance. Our results show that our approaches successfully find solutions that satisfy given preferences. Importantly, by simply accounting for this trade-off, our approaches perform better than existing uncertain QD methods. This suggests that considering the performance-reproducibility trade-off unlocks important stepping stones that are usually missed when only performance is optimised.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13315v1</guid>
      <category>cs.NE</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manon Flageat, Hannah Janmohamed, Bryan Lim, Antoine Cully</dc:creator>
    </item>
    <item>
      <title>Scalable Multi-Objective Optimization for Robust Traffic Signal Control in Uncertain Environments</title>
      <link>https://arxiv.org/abs/2409.13388</link>
      <description>arXiv:2409.13388v1 Announce Type: new 
Abstract: Intelligent traffic signal control is essential to modern urban management, with important impacts on economic efficiency, environmental sustainability, and quality of daily life. However, in current decades, it continues to pose significant challenges in managing large-scale traffic networks, coordinating intersections, and ensuring robustness under uncertain traffic conditions. This paper presents a scalable multi-objective optimization approach for robust traffic signal control in dynamic and uncertain urban environments. A multi-objective optimization model is proposed in this paper, which incorporates stochastic variables and probabilistic traffic patterns to capture traffic flow dynamics and uncertainty. We propose an algorithm named Adaptive Hybrid Multi-Objective Optimization Algorithm (AHMOA), which addresses the uncertainties of city traffic, including network-wide signal coordination, fluctuating patterns, and environmental impacts. AHMOA simultaneously optimizes multiple objectives, such as average delay, network stability, and system robustness, while adapting to unpredictable changes in traffic. The algorithm combines evolutionary strategies with an adaptive mechanism to balance exploration and exploitation, and incorporates a memory-based evaluation mechanism to leverage historical traffic data. Simulations are conducted in different cities including Manhattan, Paris, Sao Paulo, and Istanbul. The experimental results demonstrate that AHMOA consistently outperforms several state-of-the-art algorithms and the algorithm is competent to provide scalable, robust Pareto optimal solutions for managing complex traffic systems under uncertain environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13388v1</guid>
      <category>cs.NE</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weian Guo, Wuzhao Li, Zhiou Zhang, Lun Zhang, Li Li, Dongyang Li</dc:creator>
    </item>
    <item>
      <title>Hidden Activations Are Not Enough: A General Approach to Neural Network Predictions</title>
      <link>https://arxiv.org/abs/2409.13163</link>
      <description>arXiv:2409.13163v1 Announce Type: cross 
Abstract: We introduce a novel mathematical framework for analyzing neural networks using tools from quiver representation theory. This framework enables us to quantify the similarity between a new data sample and the training data, as perceived by the neural network. By leveraging the induced quiver representation of a data sample, we capture more information than traditional hidden layer outputs. This quiver representation abstracts away the complexity of the computations of the forward pass into a single matrix, allowing us to employ simple geometric and statistical arguments in a matrix space to study neural network predictions. Our mathematical results are architecture-agnostic and task-agnostic, making them broadly applicable. As proof of concept experiments, we apply our results for the MNIST and FashionMNIST datasets on the problem of detecting adversarial examples on different MLP architectures and several adversarial attack methods. Our experiments can be reproduced with our \href{https://github.com/MarcoArmenta/Hidden-Activations-are-not-Enough}{publicly available repository}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13163v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.RT</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Leblanc, Aiky Rasolomanana, Marco Armenta</dc:creator>
    </item>
    <item>
      <title>Emergent Collective Reproduction via Evolving Neuronal Flocks</title>
      <link>https://arxiv.org/abs/2409.13254</link>
      <description>arXiv:2409.13254v1 Announce Type: cross 
Abstract: This study facilitates the understanding of evolutionary transitions in individuality (ETIs) through a novel artificial life framework, named VitaNova, that intricately merges self-organization and natural selection to simulate the emergence of complex, reproductive groups. By dynamically modelling individual agents within an environment that challenges them with predators and spatial constraints, VitaNova elucidates the mechanisms by which simple agents evolve into cohesive units exhibiting collective reproduction. The findings underscore the synergy between self-organized behaviours and adaptive evolutionary strategies as fundamental drivers of ETIs. This approach not only contributes to a deeper understanding of higher-order biological individuality but also sets a new precedent in the empirical investigation of ETIs, challenging and extending current theoretical frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13254v1</guid>
      <category>q-bio.PE</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nam H. Le, Richard Watson, Mike Levin, Chrys Buckley</dc:creator>
    </item>
    <item>
      <title>Stimulus-to-Stimulus Learning in RNNs with Cortical Inductive Biases</title>
      <link>https://arxiv.org/abs/2409.13471</link>
      <description>arXiv:2409.13471v1 Announce Type: cross 
Abstract: Animals learn to predict external contingencies from experience through a process of conditioning. A natural mechanism for conditioning is stimulus substitution, whereby the neuronal response to a stimulus with no prior behavioral significance becomes increasingly identical to that generated by a behaviorally significant stimulus it reliably predicts. We propose a recurrent neural network model of stimulus substitution which leverages two forms of inductive bias pervasive in the cortex: representational inductive bias in the form of mixed stimulus representations, and architectural inductive bias in the form of two-compartment pyramidal neurons that have been shown to serve as a fundamental unit of cortical associative learning. The properties of these neurons allow for a biologically plausible learning rule that implements stimulus substitution, utilizing only information available locally at the synapses. We show that the model generates a wide array of conditioning phenomena, and can learn large numbers of associations with an amount of training commensurate with animal experiments, without relying on parameter fine-tuning for each individual experimental task. In contrast, we show that commonly used Hebbian rules fail to learn generic stimulus-stimulus associations with mixed selectivity, and require task-specific parameter fine-tuning. Our framework highlights the importance of multi-compartment neuronal processing in the cortex, and showcases how it might confer cortical animals the evolutionary edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13471v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pantelis Vafidis, Antonio Rangel</dc:creator>
    </item>
    <item>
      <title>A Spacetime Perspective on Dynamical Computation in Neural Information Processing Systems</title>
      <link>https://arxiv.org/abs/2409.13669</link>
      <description>arXiv:2409.13669v1 Announce Type: cross 
Abstract: There is now substantial evidence for traveling waves and other structured spatiotemporal recurrent neural dynamics in cortical structures; but these observations have typically been difficult to reconcile with notions of topographically organized selectivity and feedforward receptive fields. We introduce a new 'spacetime' perspective on neural computation in which structured selectivity and dynamics are not contradictory but instead are complimentary. We show that spatiotemporal dynamics may be a mechanism by which natural neural systems encode approximate visual, temporal, and abstract symmetries of the world as conserved quantities, thereby enabling improved generalization and long-term working memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13669v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>T. Anderson Keller, Lyle Muller, Terrence J. Sejnowski, Max Welling</dc:creator>
    </item>
    <item>
      <title>Gaining the Sparse Rewards by Exploring Lottery Tickets in Spiking Neural Network</title>
      <link>https://arxiv.org/abs/2309.13302</link>
      <description>arXiv:2309.13302v4 Announce Type: replace 
Abstract: Deploying energy-efficient deep learning algorithms on computational-limited devices, such as robots, is still a pressing issue for real-world applications. Spiking Neural Networks (SNNs), a novel brain-inspired algorithm, offer a promising solution due to their low-latency and low-energy properties over traditional Artificial Neural Networks (ANNs). Despite their advantages, the dense structure of deep SNNs can still result in extra energy consumption. The Lottery Ticket Hypothesis (LTH) posits that within dense neural networks, there exist winning Lottery Tickets (LTs), namely sub-networks, that can be obtained without compromising performance. Inspired by this, this paper delves into the spiking-based LTs (SLTs), examining their unique properties and potential for extreme efficiency. Then, two significant sparse \textbf{\textit{Rewards}} are gained through comprehensive explorations and meticulous experiments on SLTs across various dense structures. Moreover, a sparse algorithm tailored for spiking transformer structure, which incorporates convolution operations into the Patch Embedding Projection (ConvPEP) module, has been proposed to achieve Multi-level Sparsity (MultiSp). MultiSp refers to (1) Patch number sparsity; (2) ConvPEP weights sparsity and binarization; and (3) ConvPEP activation layer binarization. Extensive experiments demonstrate that our method achieves extreme sparsity with only a slight performance decrease, paving the way for deploying energy-efficient neural networks in robotics and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13302v4</guid>
      <category>cs.NE</category>
      <category>cs.CV</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Cheng, Jiahang Cao, Erjia Xiao, Mengshu Sun, Renjing Xu</dc:creator>
    </item>
  </channel>
</rss>
