<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Feb 2025 02:53:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Improving Residential Safety by Multiple Sensors on Multiple Nodes for Joint Emergency Detection</title>
      <link>https://arxiv.org/abs/2502.15705</link>
      <description>arXiv:2502.15705v1 Announce Type: new 
Abstract: Recent advances in low-cost microcontrollers have enabled innovative smart home applications. However, existing systems typically consist of single-purpose devices that only report sensed data to a controller. Given the potential for residential emergencies, we propose to integrate emergency detection systems into smart home environments. We present an ad-hoc distributed sensor network (DSN) designed to detect five common residential emergencies: fires, gas and water leakages, earthquakes, and intrusions. Our novel approach combines diverse sensors with a voting-based consensus algorithm among multiple nodes, improving accuracy and reliability over traditional alert systems. The consensus algorithm employs a majority rule with weighted votes, allowing adjustments for various scenarios. An experimental evaluation confirms our approach's effectiveness in accurately detecting emergencies while demonstrating reliability in mitigating node failures, ensuring system longevity, and maintaining robust communication. Additionally, our approach significantly reduces power consumption compared to alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15705v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Artur Sterz, Markus Sommer, Kevin L\"uttge, Bernd Freisleben</dc:creator>
    </item>
    <item>
      <title>Multi-Failure Localization in High-Degree ROADM-based Optical Networks using Rules-Informed Neural Networks</title>
      <link>https://arxiv.org/abs/2502.15706</link>
      <description>arXiv:2502.15706v1 Announce Type: new 
Abstract: To accommodate ever-growing traffic, network operators are actively deploying high-degree reconfigurable optical add/drop multiplexers (ROADMs) to build large-capacity optical networks. High-degree ROADM-based optical networks have multiple parallel fibers between ROADM nodes, requiring the adoption of ROADM nodes with a large number of inter-/intra-node components. However, this large number of inter-/intra-node optical components in high-degree ROADM networks increases the likelihood of multiple failures simultaneously, and calls for novel methods for accurate localization of multiple failed components. To the best of our knowledge, this is the first study investigating the problem of multi-failure localization for high-degree ROADM-based optical networks. To solve this problem, we first provide a description of the failures affecting both inter-/intra-node components, and we consider different deployments of optical power monitors (OPMs) to obtain information (i.e., optical power) to be used for automated multi-failure localization. Then, as our main and original contribution, we propose a novel method based on a rules-informed neural network (RINN) for multi-failure localization, which incorporates the benefits of both rules-based reasoning and artificial neural networks (ANN). Through extensive simulations and experimental demonstrations, we show that our proposed RINN algorithm can achieve up to around 20 higher localization accuracy compared to baseline algorithms, incurring only around 4.14 ms of average inference time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15706v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IEEE Journal on Selected Areas in Communications, 2025</arxiv:journal_reference>
      <dc:creator>Ruikun Wang, Qiaolun Zhang, Jiawei Zhang, Zhiqun Gu, Memedhe Ibrahimi, Hao Yu, Bojun Zhang, Francesco Musumeci, Yuefeng Ji, Massimo Tornatore</dc:creator>
    </item>
    <item>
      <title>Evaluating the Efficacy of Next.js: A Comparative Analysis with React.js on Performance, SEO, and Global Network Equity</title>
      <link>https://arxiv.org/abs/2502.15707</link>
      <description>arXiv:2502.15707v1 Announce Type: new 
Abstract: This paper investigates the efficacy of Next.js as a framework addressing the challenges posed by React.js, particularly in performance, SEO, and equitable web accessibility. By constructing identical websites and web applications in both frameworks, we aim to evaluate the frameworks' behavior under diverse network conditions and capabilities. Beyond quantitative metrics like First Contentful Paint (FCP) and Time to Interactive (TTI), we incorporate qualitative user feedback to assess real-world usability. Our motivation stems from bridging the digital divide exacerbated by client-side rendering (CSR) frameworks and validating investments in modern technologies for businesses and institutions. Employing a novel LLM-assisted migration workflow, this paper also demonstrates the ease with which developers can transition from React.js to Next.js. Our results highlight Next.js's promise of better overall performance, without any degradation in user interaction experience, showcasing its potential to mitigate disparities in web accessibility and foster global network equity, thus highlighting Next.js as a compelling framework for the future of an inclusive web.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15707v1</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swostik Pati, Yasir Zaki</dc:creator>
    </item>
    <item>
      <title>MAML: Towards a Faster Web in Developing Regions</title>
      <link>https://arxiv.org/abs/2502.15708</link>
      <description>arXiv:2502.15708v1 Announce Type: new 
Abstract: The web experience in developing regions remains subpar, primarily due to the growing complexity of modern webpages and insufficient optimization by content providers. Users in these regions typically rely on low-end devices and limited bandwidth, which results in a poor user experience as they download and parse webpages bloated with excessive third-party CSS and JavaScript (JS). To address these challenges, we introduce the Mobile Application Markup Language (MAML), a flat layout-based web specification language that reduces computational and data transmission demands, while replacing the excessive bloat from JS with a new scripting language centered on essential (and popular) web functionalities. Last but not least, MAML is backward compatible as it can be transpiled to minimal HTML/JavaScript/CSS and thus work with legacy browsers. We benchmark MAML in terms of page load times and sizes, using a translator which can automatically port any webpage to MAML. When compared to the popular Google AMP, across 100 testing webpages, MAML offers webpage speedups by tens of seconds under challenging network conditions thanks to its significant size reductions. Next, we run a competition involving 25 university students porting 50 of the above webpages to MAML using a web-based editor we developed. This experiment verifies that, with little developer effort, MAML is quite effective in maintaining the visual and functional correctness of the originating webpages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15708v1</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayush Pandey, Matteo Varvello, Syed Ishtiaque Ahmed, Shurui Zhou, Lakshmi Subramanian, Yasir Zaki</dc:creator>
    </item>
    <item>
      <title>GPUs, CPUs, and... NICs: Rethinking the Network's Role in Serving Complex AI Pipelines</title>
      <link>https://arxiv.org/abs/2502.15712</link>
      <description>arXiv:2502.15712v1 Announce Type: new 
Abstract: The increasing prominence of AI necessitates the deployment of inference platforms for efficient and effective management of AI pipelines and compute resources. As these pipelines grow in complexity, the demand for distributed serving rises and introduces much-dreaded network delays. In this paper, we investigate how the network can instead be a boon to the excessively high resource overheads of AI pipelines. To alleviate these overheads, we discuss how resource-intensive data processing tasks -- a key facet of growing AI pipeline complexity -- are well-matched for the computational characteristics of packet processing pipelines and how they can be offloaded onto SmartNICs. We explore the challenges and opportunities of offloading, and propose a research agenda for integrating network hardware into AI pipelines, unlocking new opportunities for optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15712v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.OS</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike Wong, Ulysses Butler, Emma Farkash, Praveen Tammana, Anirudh Sivaraman, Ravi Netravali</dc:creator>
    </item>
    <item>
      <title>UAV-assisted Internet of Vehicles: A Framework Empowered by Reinforcement Learning and Blockchain</title>
      <link>https://arxiv.org/abs/2502.15713</link>
      <description>arXiv:2502.15713v1 Announce Type: new 
Abstract: This paper addresses the challenges of selecting relay nodes and coordinating among them in UAV-assisted Internet-of-Vehicles (IoV). The selection of UAV relay nodes in IoV employs mechanisms executed either at centralized servers or decentralized nodes, which have two main limitations: 1) the traceability of the selection mechanism execution and 2) the coordination among the selected UAVs, which is currently offered in a centralized manner and is not coupled with the relay selection. Existing UAV coordination methods often rely on optimization methods, which are not adaptable to different environment complexities, or on centralized deep reinforcement learning, which lacks scalability in multi-UAV settings. Overall, there is a need for a comprehensive framework where relay selection and coordination are coupled and executed in a transparent and trusted manner. This work proposes a framework empowered by reinforcement learning and Blockchain for UAV-assisted IoV networks. It consists of three main components: a two-sided UAV relay selection mechanism for UAV-assisted IoV, a decentralized Multi-Agent Deep Reinforcement Learning (MDRL) model for autonomous UAV coordination, and a Blockchain implementation for transparency and traceability in the interactions between vehicles and UAVs. The relay selection considers the two-sided preferences of vehicles and UAVs based on the Quality-of-UAV (QoU) and the Quality-of-Vehicle (QoV). Upon selection of relay UAVs, the decentralized coordination between them is enabled through an MDRL model trained to control their mobility and maintain the network coverage and connectivity using Proximal Policy Optimization (PPO). The evaluation results demonstrate that the proposed selection and coordination mechanisms improve the stability of the selected relays and maximize the coverage and connectivity achieved by the UAVs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15713v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.vehcom.2025.100874</arxiv:DOI>
      <dc:creator>Ahmed Alagha, Maha Kadadha, Rabeb Mizouni, Shakti Singh, Jamal Bentahar, Hadi Otrok</dc:creator>
    </item>
    <item>
      <title>Retrieval Augmented Generation Based LLM Evaluation For Protocol State Machine Inference With Chain-of-Thought Reasoning</title>
      <link>https://arxiv.org/abs/2502.15727</link>
      <description>arXiv:2502.15727v1 Announce Type: new 
Abstract: This paper presents a novel approach to evaluate the efficiency of a RAG-based agentic Large Language Model (LLM) architecture in network packet seed generation for network protocol fuzzing. Enhanced by chain-of-thought (COT) prompting techniques, the proposed approach focuses on the improvement of the seeds structural quality in order to guide protocol fuzzing frameworks through a wide exploration of the protocol state space. Our method leverages RAG and text embeddings in a two-stages. In the first stage, the agent dynamically refers to the Request For Comments (RFC) documents knowledge base for answering queries regarding the protocol Finite State Machine (FSM), then it iteratively reasons through the retrieved knowledge, for output refinement and proper seed placement. In the second stage, we evaluate the response structure quality of the agent's output, based on metrics as BLEU, ROUGE, and Word Error Rate (WER) by comparing the generated packets against the ground truth packets. Our experiments demonstrate significant improvements of up to 18.19%, 14.81%, and 23.45% in BLEU, ROUGE, and WER, respectively, over baseline models. These results confirm the potential of such approach, improving LLM-based protocol fuzzing frameworks for the identification of hidden vulnerabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15727v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.IR</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youssef Maklad, Fares Wael, Wael Elsersy, Ali Hamdi</dc:creator>
    </item>
    <item>
      <title>Modular and Integrated AI Control Framework across Fiber and Wireless Networks for 6G</title>
      <link>https://arxiv.org/abs/2502.15731</link>
      <description>arXiv:2502.15731v1 Announce Type: new 
Abstract: The rapid evolution of communication networks towards 6G increasingly incorporates advanced AI-driven controls across various network segments to achieve intelligent, zero-touch operation. This paper proposes a comprehensive and modular framework for AI controllers, designed to be highly flexible and adaptable for use across both fiber optical and radio networks. Building on the principles established by the O-RAN Alliance for near-Real-Time RAN Intelligent Controllers (near-RT RICs), our framework extends this AI-driven control into the optical domain. Our approach addresses the critical need for a unified AI control framework across diverse network transport technologies and domains, enabling the development of intelligent, automated, and scalable 6G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15731v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Merim Dzaferagic, Marco Ruffini, Daniel Kilper</dc:creator>
    </item>
    <item>
      <title>Channel Gain Map Construction based on Subregional Learning and Prediction</title>
      <link>https://arxiv.org/abs/2502.15733</link>
      <description>arXiv:2502.15733v1 Announce Type: new 
Abstract: The construction of channel gain map (CGM) is essential for realizing environment-aware wireless communications expected in 6G, for which a fundamental problem is how to predict the channel gains at unknown locations effectively by a finite number of measurements. As using a single prediction model is not effective in complex propagation environments, we propose a subregional learning-based CGM construction scheme, with which the entire map is divided into subregions via data-driven clustering, then individual models are constructed and trained for every subregion. In this way, specific propagation feature in each subregion can be better extracted with finite training data. Moreover, we propose to further improve prediction accuracy by uneven subregion sampling, as well as training data reuse around the subregion boundaries. Simulation results validate the effectiveness of the proposed scheme in CGM construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15733v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayi Chen, Ruifeng Gao, Jue Wang, Shu Sun, Yi Wu</dc:creator>
    </item>
    <item>
      <title>CacheMamba: Popularity Prediction for Mobile Edge Caching Networks via Selective State Spaces</title>
      <link>https://arxiv.org/abs/2502.15746</link>
      <description>arXiv:2502.15746v1 Announce Type: new 
Abstract: Mobile Edge Caching (MEC) plays a pivotal role in mitigating latency in data-intensive services by dynamically caching frequently requested content on edge servers. This capability is critical for applications such as Augmented Reality (AR), Virtual Reality (VR), and Autonomous Vehicles (AV), where efficient content caching and accurate popularity prediction are essential for optimizing performance. In this paper, we explore the problem of popularity prediction in MEC by utilizing historical time-series request data of intended files, formulating this problem as a ranking task. To this aim, we propose CacheMamba model by employing Mamba, a state-space model (SSM)-based architecture, to identify the top-K files with the highest likelihood of being requested. We then benchmark the proposed model against a Transformer-based approach, demonstrating its superior performance in terms of cache-hit rate, Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG), and Floating-Point Operations Per Second (FLOPS), particularly when dealing with longer sequences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15746v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ghazaleh Kianfar, Zohreh Hajiakhondi-Meybodi, Arash Mohammadi</dc:creator>
    </item>
    <item>
      <title>Text2Net: Transforming Plain-text To A Dynamic Interactive Network Simulation Environment</title>
      <link>https://arxiv.org/abs/2502.15754</link>
      <description>arXiv:2502.15754v1 Announce Type: new 
Abstract: This paper introduces Text2Net, an innovative text-based network simulation engine that leverages natural language processing (NLP) and large language models (LLMs) to transform plain-text descriptions of network topologies into dynamic, interactive simulations. Text2Net simplifies the process of configuring network simulations, eliminating the need for users to master vendor-specific syntaxes or navigate complex graphical interfaces. Through qualitative and quantitative evaluations, we demonstrate Text2Net's ability to significantly reduce the time and effort required to deploy network scenarios compared to traditional simulators like EVE-NG. By automating repetitive tasks and enabling intuitive interaction, Text2Net enhances accessibility for students, educators, and professionals. The system facilitates hands-on learning experiences for students that bridge the gap between theoretical knowledge and practical application. The results showcase its scalability across various network complexities, marking a significant step toward revolutionizing network education and professional use cases, such as proof-of-concept testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15754v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alireza Marefat, Abbaas Alif Mohamed Nishar, Ashwin Ashok</dc:creator>
    </item>
    <item>
      <title>Serverless Edge Computing: A Taxonomy, Systematic Literature Review, Current Trends and Research Challenges</title>
      <link>https://arxiv.org/abs/2502.15775</link>
      <description>arXiv:2502.15775v1 Announce Type: new 
Abstract: In recent years, the rapid expansion of Internet of Things (IoT) nodes and devices has seamlessly integrated technology into everyday life, amplifying the demand for optimized computing solutions. To meet the critical Quality of Service (QoS) requirements such as reduced latency, efficient bandwidth usage, swift reaction times, scalability, privacy, and security serverless edge computing has emerged as a transformative paradigm. This systematic literature review explores the current landscape of serverless edge computing, analyzing recent studies to uncover the present state of this technology. The review identifies the essential features of serverless edge computing, focusing on architectural designs, QoS metrics, implementation specifics, practical applications, and communication modalities central to this paradigm. Furthermore, we propose a comprehensive taxonomy that categorizes existing research efforts, providing a comparative analysis based on these classifications. The paper concludes with an in depth discussion of open research challenges and highlights promising future directions that hold potential for advancing serverless edge computing research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15775v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Iqra Batool, Sania Kanwal</dc:creator>
    </item>
    <item>
      <title>InSlicing: Interpretable Learning-Assisted Network Slice Configuration in Open Radio Access Networks</title>
      <link>https://arxiv.org/abs/2502.15918</link>
      <description>arXiv:2502.15918v1 Announce Type: new 
Abstract: Network slicing is a key technology enabling the flexibility and efficiency of 5G networks, offering customized services for diverse applications. However, existing methods face challenges in adapting to dynamic network environments and lack interpretability in performance models. In this paper, we propose a novel interpretable network slice configuration algorithm (\emph{InSlicing}) in open radio access networks, by integrating Kolmogorov-Arnold Networks (KANs) and hybrid optimization process. On the one hand, we use KANs to approximate and learn the unknown performance function of individual slices, which converts the blackbox optimization problem. On the other hand, we solve the converted problem with a genetic method for global search and incorporate a trust region for gradient-based local refinement. With the extensive evaluation, we show that our proposed algorithm achieves high interpretability while reducing 25+\% operation cost than existing solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15918v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Zhao, Yuru Zhang, Qiang Liu, Ahan Kak, Nakjung Choi</dc:creator>
    </item>
    <item>
      <title>Space-O-RAN: Enabling Intelligent, Open, and Interoperable Non Terrestrial Networks in 6G</title>
      <link>https://arxiv.org/abs/2502.15936</link>
      <description>arXiv:2502.15936v1 Announce Type: new 
Abstract: Non-terrestrial networks (NTNs) are essential for ubiquitous connectivity, providing coverage in remote and underserved areas. However, since NTNs are currently operated independently, they face challenges such as isolation, limited scalability, and high operational costs. Integrating satellite constellations with terrestrial networks offers a way to address these limitations while enabling adaptive and cost-efficient connectivity through the application of Artificial Intelligence (AI) models.
  This paper introduces Space-O-RAN, a framework that extends Open Radio Access Network (RAN) principles to NTNs. It employs hierarchical closed-loop control with distributed Space RAN Intelligent Controllers (Space-RICs) to dynamically manage and optimize operations across both domains.
  To enable adaptive resource allocation and network orchestration, the proposed architecture integrates real-time satellite optimization and control with AI-driven management and digital twin (DT) modeling. It incorporates distributed Space Applications (sApps) and dApps to ensure robust performance in in highly dynamic orbital environments. A core feature is dynamic link-interface mapping, which allows network functions to adapt to specific application requirements and changing link conditions using all physical links on the satellite.
  Simulation results evaluate its feasibility by analyzing latency constraints across different NTN link types, demonstrating that intra-cluster coordination operates within viable signaling delay bounds, while offloading non-real-time tasks to ground infrastructure enhances scalability toward sixth-generation (6G) networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15936v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo Baena, Paolo Testolina, Michele Polese, Dimitrios Koutsonikolas, Josep Jornet, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>Autonomous Agricultural Monitoring with Aerial Drones and RF Energy-Harvesting Sensor Tags</title>
      <link>https://arxiv.org/abs/2502.16028</link>
      <description>arXiv:2502.16028v1 Announce Type: new 
Abstract: In precision agriculture and plant science, there is an increasing demand for wireless sensors that are easy to deploy, maintain, and monitor. This paper investigates a novel approach that leverages recent advances in extremely low-power wireless communication and sensing, as well as the rapidly increasing availability of unmanned aerial vehicle (UAV) platforms. By mounting a specialized wireless payload on a UAV, battery-less sensor tags can harvest wireless beacon signals emitted from the drone, dramatically reducing the cost per sensor. These tags can measure environmental information such as temperature and humidity, then encrypt and transmit the data in the range of several meters. An experimental implementation was constructed at AERPAW, an NSF-funded wireless aerial drone research platform. While ground-based tests confirmed reliable sensor operation and data collection, airborne trials encountered wireless interference that impeded successfully detecting tag data. Despite these challenges, our results suggest further refinements could improve reliability and advance precision agriculture and agrarian research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16028v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul S. Kudyba, Haijian Sun</dc:creator>
    </item>
    <item>
      <title>An Autonomous Network Orchestration Framework Integrating Large Language Models with Continual Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2502.16198</link>
      <description>arXiv:2502.16198v1 Announce Type: new 
Abstract: 6G networks aim to achieve global coverage, massive connectivity, and ultra-stringent requirements. Space-Air-Ground Integrated Networks (SAGINs) and Semantic Communication (SemCom) are essential for realizing these goals, yet they introduce considerable complexity in resource orchestration. Drawing inspiration from research in robotics, a viable solution to manage this complexity is the application of Large Language Models (LLMs). Although the use of LLMs in network orchestration has recently gained attention, existing solutions have not sufficiently addressed LLM hallucinations or their adaptation to network dynamics. To address this gap, this paper proposes a framework called Autonomous Reinforcement Coordination (ARC) for a SemCom-enabled SAGIN. This framework employs an LLM-based Retrieval-Augmented Generator (RAG) monitors services, users, and resources and processes the collected data, while a Hierarchical Action Planner (HAP) orchestrates resources. ARC decomposes orchestration into two tiers, utilizing LLMs for high-level planning and Reinforcement Learning (RL) agents for low-level decision-making, in alignment with the Mixture of Experts (MoE) concept. The LLMs utilize Chain-of-Thought (CoT) reasoning for few-shot learning, empowered by contrastive learning, while the RL agents employ replay buffer management for continual learning, thereby achieving efficiency, accuracy, and adaptability. Simulations are provided to demonstrate the effectiveness of ARC, along with a comprehensive discussion on potential future research directions to enhance and upgrade ARC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16198v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masoud Shokrnezhad, Tarik Taleb</dc:creator>
    </item>
    <item>
      <title>LLMKey: LLM-Powered Wireless Key Generation Scheme for Next-Gen IoV Systems</title>
      <link>https://arxiv.org/abs/2502.16199</link>
      <description>arXiv:2502.16199v1 Announce Type: new 
Abstract: Wireless key generation holds significant promise for establishing cryptographic keys in Next-Gen Internet of Vehicles (IoV) systems. However, existing approaches often face inefficiencies and performance limitations caused by frequent channel probing and ineffective quantization. To address these challenges, this paper introduces LLMKey, a novel key generation system designed to enhance efficiency and security. We identify excessive channel probing and suboptimal quantization as critical bottlenecks in current methods. To mitigate these issues, we propose an innovative large language model (LLM)-based channel probing technique that leverages the capabilities of LLMs to reduce probing rounds while preserving crucial channel information. Instead of conventional quantization, LLMKey adopts a perturbed compressed sensing-based key delivery mechanism, improving both robustness and security. Extensive evaluations are conducted in four real-world scenarios, encompassing V2I (Vehicle-to-Infrastructure) and V2V (Vehicle-to-Vehicle) settings in both urban and rural environments. The results show that LLMKey achieves an average key agreement rate of 98.78\%, highlighting its effectiveness and reliability across diverse conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16199v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Huanqi Yang, Weitao Xu</dc:creator>
    </item>
    <item>
      <title>Revolutionizing Datacenter Networks via Reconfigurable Topologies</title>
      <link>https://arxiv.org/abs/2502.16228</link>
      <description>arXiv:2502.16228v1 Announce Type: new 
Abstract: With the popularity of cloud computing and data-intensive applications such as machine learning, datacenter networks have become a critical infrastructure for our digital society. Given the explosive growth of datacenter traffic and the slowdown of Moore's law, significant efforts have been made to improve datacenter network performance over the last decade. A particularly innovative solution is reconfigurable datacenter networks (RDCNs): datacenter networks whose topologies dynamically change over time, in either a demand-oblivious or a demand-aware manner. Such dynamic topologies are enabled by recent optical switching technologies and stand in stark contrast to state-of-the-art datacenter network topologies, which are fixed and oblivious to the actual traffic demand. In particular, reconfigurable demand-aware and 'self-adjusting' datacenter networks are motivated empirically by the significant spatial and temporal structures observed in datacenter communication traffic. This paper presents an overview of reconfigurable datacenter networks. In particular, we discuss the motivation for such reconfigurable architectures, review the technological enablers, and present a taxonomy that classifies the design space into two dimensions: static vs. dynamic and demand-oblivious vs. demand-aware. We further present a formal model and discuss related research challenges. Our article comes with complementary video interviews in which three leading experts, Manya Ghobadi, Amin Vahdat, and George Papen, share with us their perspectives on reconfigurable datacenter networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16228v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Avin, Stefan Schmid</dc:creator>
    </item>
    <item>
      <title>Combining Heuristic and Reinforcement Learning to Achieve the Low-latency and High-throughput Receiver-side Congestion Control</title>
      <link>https://arxiv.org/abs/2502.16498</link>
      <description>arXiv:2502.16498v1 Announce Type: new 
Abstract: Traditional congestion control algorithms struggle to maintain the consistent and satisfactory data transmission performance over time-varying networking condition. Simultaneously, as video traffic becomes dominant, the loose coupling between the DASH framework and TCP congestion control results in the un-matched bandwidth usage, thereby limiting video streaming performance. To address these issues, this paper proposes a receiver-driven congestion control framework named Nuwa. Nuwa deploys the congestion avoidance phase at the receiver-side, utilizing one-way queueing delay detection to monitor network congestion and setting specific target delays for different applications. Experimental results demonstrate that, in most cases, with appropriate parameter configuration, Nuwa can improve the throughput of TCP flows 4% to 15.4% and reduce average queueing delay by 6.9% to 29.4%. Furthermore, we also introduce the use of reinforcement learning to dynamically adjust Nuwa's key parameter , enhancing Nuwa's adaptability to the unpredictable environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16498v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xianliang Jiang, Guanghui Gong, Guang Jin</dc:creator>
    </item>
    <item>
      <title>Toward Agentic AI: Generative Information Retrieval Inspired Intelligent Communications and Networking</title>
      <link>https://arxiv.org/abs/2502.16866</link>
      <description>arXiv:2502.16866v1 Announce Type: new 
Abstract: The increasing complexity and scale of modern telecommunications networks demand intelligent automation to enhance efficiency, adaptability, and resilience. Agentic AI has emerged as a key paradigm for intelligent communications and networking, enabling AI-driven agents to perceive, reason, decide, and act within dynamic networking environments. However, effective decision-making in telecom applications, such as network planning, management, and resource allocation, requires integrating retrieval mechanisms that support multi-hop reasoning, historical cross-referencing, and compliance with evolving 3GPP standards. This article presents a forward-looking perspective on generative information retrieval-inspired intelligent communications and networking, emphasizing the role of knowledge acquisition, processing, and retrieval in agentic AI for telecom systems. We first provide a comprehensive review of generative information retrieval strategies, including traditional retrieval, hybrid retrieval, semantic retrieval, knowledge-based retrieval, and agentic contextual retrieval. We then analyze their advantages, limitations, and suitability for various networking scenarios. Next, we present a survey about their applications in communications and networking. Additionally, we introduce an agentic contextual retrieval framework to enhance telecom-specific planning by integrating multi-source retrieval, structured reasoning, and self-reflective validation. Experimental results demonstrate that our framework significantly improves answer accuracy, explanation consistency, and retrieval efficiency compared to traditional and semantic retrieval methods. Finally, we outline future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16866v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruichen Zhang, Shunpu Tang, Yinqiu Liu, Dusit Niyato, Zehui Xiong, Sumei Sun, Shiwen Mao, Zhu Han</dc:creator>
    </item>
    <item>
      <title>Semantic-Aware Dynamic and Distributed Power Allocation: a Multi-UAV Area Coverage Use Case</title>
      <link>https://arxiv.org/abs/2502.17120</link>
      <description>arXiv:2502.17120v1 Announce Type: new 
Abstract: The advancement towards 6G technology leverages improvements in aerial-terrestrial networking, where one of the critical challenges is the efficient allocation of transmit power. Although existing studies have shown commendable performance in addressing this challenge, a revolutionary breakthrough is anticipated to meet the demands and dynamism of 6G. Potential solutions include: 1) semantic communication and orchestration, which transitions the focus from mere transmission of bits to the communication of intended meanings of data and their integration into the network orchestration process; and 2) distributed machine learning techniques to develop adaptable and scalable solutions. In this context, this paper introduces a power allocation framework specifically designed for semantic-aware networks. The framework addresses a scenario involving multiple Unmanned Aerial Vehicles (UAVs) that collaboratively transmit observations over a multi-channel uplink medium to a central server, aiming to maximise observation quality. To tackle this problem, we present the Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL) algorithm, which utilizes the data quality of observing areas as reward feedback during the training phase, thereby constituting a semantic-aware learning mechanism. Simulation results substantiate the efficacy and scalability of our approach, demonstrating its superior performance compared to traditional bit-oriented learning and heuristic algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17120v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamidreza Mazandarani, Masoud Shokrnezhad, Tarik Taleb</dc:creator>
    </item>
    <item>
      <title>A Novel Multiple Access Scheme for Heterogeneous Wireless Communications using Symmetry-aware Continual Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2502.17167</link>
      <description>arXiv:2502.17167v1 Announce Type: new 
Abstract: The Metaverse holds the potential to revolutionize digital interactions through the establishment of a highly dynamic and immersive virtual realm over wireless communications systems, offering services such as massive twinning and telepresence. This landscape presents novel challenges, particularly efficient management of multiple access to the frequency spectrum, for which numerous adaptive Deep Reinforcement Learning (DRL) approaches have been explored. However, challenges persist in adapting agents to heterogeneous and non-stationary wireless environments. In this paper, we present a novel approach that leverages Continual Learning (CL) to enhance intelligent Medium Access Control (MAC) protocols, featuring an intelligent agent coexisting with legacy User Equipments (UEs) with varying numbers, protocols, and transmission profiles unknown to the agent for the sake of backward compatibility and privacy. We introduce an adaptive Double and Dueling Deep Q-Learning (D3QL)-based MAC protocol, enriched by a symmetry-aware CL mechanism, which maximizes intelligent agent throughput while ensuring fairness. Mathematical analysis validates the efficiency of our proposed scheme, showcasing superiority over conventional DRL-based techniques in terms of throughput, collision rate, and fairness, coupled with real-time responsiveness in highly dynamic scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17167v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamidreza Mazandarani, Masoud Shokrnezhad, Tarik Taleb</dc:creator>
    </item>
    <item>
      <title>+Tour: Recommending personalized itineraries for smart tourism</title>
      <link>https://arxiv.org/abs/2502.17345</link>
      <description>arXiv:2502.17345v2 Announce Type: new 
Abstract: Next-generation touristic services will rely on the advanced mobile networks' high bandwidth and low latency and the Multi-access Edge Computing (MEC) paradigm to provide fully immersive mobile experiences. As an integral part of travel planning systems, recommendation algorithms devise personalized tour itineraries for individual users considering the popularity of a city's Points of Interest (POIs) as well as the tourist preferences and constraints. However, in the context of next-generation touristic services, recommendation algorithms should also consider the applications (e.g., social network, mobile video streaming, mobile augmented reality) the tourist will consume in the POIs and the quality in which the MEC infrastructure will deliver such applications. In this paper, we address the joint problem of recommending personalized tour itineraries for tourists and efficiently allocating MEC resources for advanced touristic applications. We formulate an optimization problem that maximizes the itinerary of individual tourists while optimizing the resource allocation at the network edge. We then propose an exact algorithm that quickly solves the problem optimally, considering instances of realistic size. Using a real-world location-based photo-sharing database, we conduct and present an exploratory analysis to understand preferences and users' visiting patterns. Using this understanding, we propose a methodology to identify user interest in applications. Finally, we evaluate our algorithm using this dataset. Results show that our algorithm outperforms a modified version of a state-of-the-art solution for personalized tour itinerary recommendation, demonstrating gains up to 11% for resource allocation efficiency and 40% for user experience. In addition, our algorithm performs similarly to the modified state-of-the-art solution regarding traditional itinerary recommendation metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17345v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.comnet.2025.111118</arxiv:DOI>
      <arxiv:journal_reference>Computer Networks, Volume 260, April 2025, 111118</arxiv:journal_reference>
      <dc:creator>Jo\~ao Paulo Esper, Luciano de S. Fraga, Aline C. Viana, Kleber Vieira Cardoso, Sand Luz Correa</dc:creator>
    </item>
    <item>
      <title>Goal-Oriented Middleware Filtering at Transport Layer Based on Value of Updates</title>
      <link>https://arxiv.org/abs/2502.17350</link>
      <description>arXiv:2502.17350v1 Announce Type: new 
Abstract: This work explores employing the concept of goal-oriented (GO) semantic communication for real-time monitoring and control. Generally, GO communication advocates for the deep integration of application targets into the network design. We consider CPS and IoT applications where sensors generate a tremendous amount of network traffic toward monitors or controllers. Here, the practical introduction of GO communication must address several challenges. These include stringent timing requirements, challenging network setups, and limited computing and communication capabilities of the devices involved. Moreover, real-life CPS deployments often rely on heterogeneous communication standards prompted by specific hardware. To address these issues, we introduce a middleware design of a GO distributed Transport Layer (TL) framework for control applications. It offers end-to-end performance improvements for diverse setups and transmitting hardware. The proposed TL protocol evaluates the Value of sampled state Updates (VoU) for the application goal. It decides whether to admit or discard the corresponding packets, thus offloading the network. VoU captures the contribution of utilizing the updates at the receiver into the application's performance. We introduce a belief network and the augmentation procedure used by the sensor to predict the evolution of the control process, including possible delays and losses of status updates in the network. The prediction is made either using a control model dynamics or a Long-Short Term Memory neural network approach. We test the performance of the proposed TL in the experimental framework using Industrial IoT Zolertia ReMote sensors. We show that while existing approaches fail to deliver sufficient control performance, our VoU-based TL scheme ensures stability and performs $\sim$$60\%$ better than the naive GO TL we proposed in our previous work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17350v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Polina Kutsevol, Onur Ayan, Nikolaos Pappas, Wolfgang Kellerer</dc:creator>
    </item>
    <item>
      <title>Qoala: an Application Execution Environment for Quantum Internet Nodes</title>
      <link>https://arxiv.org/abs/2502.17296</link>
      <description>arXiv:2502.17296v1 Announce Type: cross 
Abstract: Recently, a first-of-its-kind operating system for programmable quantum network nodes was developed, called QNodeOS. Here, we present an extension of QNodeOS called Qoala, which introduces (1) a unified program format for hybrid interactive classical-quantum programs, providing a well-defined target for compilers, and (2) a runtime representation of a program that allows joint scheduling of the hybrid classical-quantum program, multitasking, and asynchronous program execution. Based on concrete design considerations, we put forward the architecture of Qoala, including the program structure and execution mechanism. We implement Qoala in the form of a modular and extendible simulator that is validated against real-world quantum network hardware (available online). However, Qoala is not meant to be purely a simulator, and implementation is planned on real hardware. We evaluate Qoala's effectiveness and performance sensitivity to latencies and network schedules using an extensive simulation study. Qoala provides a framework that opens the door for future computer science research into quantum network applications, including scheduling algorithms and compilation strategies that can now readily be explored using the framework and tools provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17296v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bart van der Vecht, Atak Talay Y\"ucel, Hana Jirovsk\'a, Stephanie Wehner</dc:creator>
    </item>
    <item>
      <title>Revolutionizing QoE-Driven Network Management with Digital Agents in 6G</title>
      <link>https://arxiv.org/abs/2412.14177</link>
      <description>arXiv:2412.14177v2 Announce Type: replace 
Abstract: In this article, we present a digital agent (DA)-assisted network management framework for future sixth generation (6G) networks considering user quality of experience (QoE). A novel QoE metric is defined by incorporating the impact of user behavioral dynamics and environmental complexity on quality of service (QoS). A two-level DA architecture is proposed to assist the QoE-driven network slicing and orchestration. Three potential solutions are presented from the perspectives of DA data collection, resource scheduling, and DA deployment. A case study demonstrates that the proposed framework can effectively improve user QoE compared with benchmark schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14177v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuemin Shen, Xinyu Huang, Jianzhe Xue, Conghao Zhou, Xiufang Shi, Weihua Zhuang</dc:creator>
    </item>
    <item>
      <title>Leveraging Edge Intelligence and LLMs to Advance 6G-Enabled Internet of Automated Defense Vehicles</title>
      <link>https://arxiv.org/abs/2501.06205</link>
      <description>arXiv:2501.06205v2 Announce Type: replace 
Abstract: The evolution of Artificial Intelligence (AI) and its subset Deep Learning (DL), has profoundly impacted numerous domains, including autonomous driving. The integration of autonomous driving in military settings reduces human casualties and enables precise and safe execution of missions in hazardous environments while allowing for reliable logistics support without the risks associated with fatigue-related errors. However, relying on autonomous driving solely requires an advanced decision-making model that is adaptable and optimum in any situation. Considering the presence of numerous interconnected autonomous vehicles in mission-critical scenarios, Ultra-Reliable Low Latency Communication (URLLC) is vital for ensuring seamless coordination, real-time data exchange, and instantaneous response to dynamic driving environments. The advent of 6G strengthens the Internet of Automated Defense Vehicles (IoADV) concept within the realm of Internet of Military Defense Things (IoMDT) by enabling robust connectivity, crucial for real-time data exchange, advanced navigation, and enhanced safety features through IoADV interactions. On the other hand, a critical advancement in this space is using pre-trained Generative Large Language Models (LLMs) for decision-making and communication optimization for autonomous driving. Hence, this work presents opportunities and challenges with a vision of realizing the full potential of these technologies in critical defense applications, especially through the advancement of IoADV and its role in enhancing autonomous military operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06205v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Murat Arda Onsu, Poonam Lohan, Burak Kantarci</dc:creator>
    </item>
    <item>
      <title>Continual Reinforcement Learning for Digital Twin Synchronization Optimization</title>
      <link>https://arxiv.org/abs/2501.08045</link>
      <description>arXiv:2501.08045v2 Announce Type: replace 
Abstract: This article investigates the adaptive resource allocation scheme for digital twin (DT) synchronization optimization over dynamic wireless networks. In our considered model, a base station (BS) continuously collects factory physical object state data from wireless devices to build a real-time virtual DT system for factory event analysis. Due to continuous data transmission, maintaining DT synchronization must use extensive wireless resources. To address this issue, a subset of devices is selected to transmit their sensing data, and resource block (RB) allocation is optimized. This problem is formulated as a constrained Markov process (CMDP) problem that minimizes the long-term mismatch between the physical and virtual systems. To solve this CMDP, we first transform the problem into a dual problem that refines RB constraint impacts on device scheduling strategies. We then propose a continual reinforcement learning (CRL) algorithm to solve the dual problem. The CRL algorithm learns a stable policy across historical experiences for quick adaptation to dynamics in physical states and network capacity. Simulation results show that the CRL can adapt quickly to network capacity changes and reduce normalized root mean square error (NRMSE) between physical and virtual states by up to 55.2%, using the same RB number as traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08045v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haonan Tong, Mingzhe Chen, Jun Zhao, Ye Hu, Zhaohui Yang, Yuchen Liu, Changchuan Yin</dc:creator>
    </item>
    <item>
      <title>Online Resource Management for the Uplink of Wideband Hybrid Beamforming System</title>
      <link>https://arxiv.org/abs/2502.14784</link>
      <description>arXiv:2502.14784v2 Announce Type: replace 
Abstract: This paper studies the radio resource management (RRM) for the uplink (UL) of a cellular system with codebook-based hybrid beamforming. We consider the often neglected but highly practical multi-channel case with fewer radio frequency chains in the base station than user equipment (UEs) in the cell, assuming one RF chain per UE. As for any UL RRM, a per-time slot solution is needed as the allocation of power to subchannels by a UE can only be done once it knows which subchannels it has been allocated. The RRM in this system comprises beam selection, user selection and power allocation, three steps that are intricately coupled and we will show that the order in which they are performed does impact performance and so does the amount of coupling that we take into account. Specifically, we propose 4 online sequential solutions with different orders in which the steps are called and of different complexities, i.e., different levels of coupling between the steps. Our extensive numerical campaign for a mmWave system shows how a well-designed heuristic that takes some level of couplings between the steps can make the performance exceedingly better than a benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14784v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuan Quan, Haseen Rahman, Catherine Rosenberg</dc:creator>
    </item>
    <item>
      <title>Embracing Reconfigurable Antennas in the Tri-hybrid MIMO Architecture for 6G and Beyond</title>
      <link>https://arxiv.org/abs/2501.16610</link>
      <description>arXiv:2501.16610v2 Announce Type: replace-cross 
Abstract: Multiple-input multiple-output (MIMO) communication has led to immense enhancements in data rates and efficient spectrum management. The evolution of MIMO, though, has been accompanied by increased hardware complexity and array sizes, causing the system power consumption to increase. Despite past advances in power-efficient hybrid architectures, new solutions are needed to enable extremely large-scale MIMO deployments for 6G and beyond. In this paper, we introduce a novel architecture that integrates low-power reconfigurable antennas with both digital and analog precoding. This \emph{tri-hybrid} approach addresses key limitations in traditional and hybrid MIMO systems by improving power consumption and adds a new layer for signal processing. We provide an analysis of the proposed architecture and compare its performance with existing solutions, including fully-digital and hybrid MIMO systems. The results demonstrate significant improvements in energy efficiency, highlighting the potential of the tri-hybrid system to meet the growing demands of future wireless networks. We conclude the paper with a summary of design and implementation challenges, including the need for technological advancements in reconfigurable array hardware and tunable antenna parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16610v2</guid>
      <category>cs.IT</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 25 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miguel Rodrigo Castellanos, Siyun Yang, Chan-Byoung Chae, Robert W. Heath Jr</dc:creator>
    </item>
  </channel>
</rss>
