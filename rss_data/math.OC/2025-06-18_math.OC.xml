<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Jun 2025 01:29:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Study on Effective Initial Guess Finding Method Based on B\'ezier Curves: Orbit Determination Applications</title>
      <link>https://arxiv.org/abs/2506.13921</link>
      <description>arXiv:2506.13921v1 Announce Type: new 
Abstract: In celestial mechanics, proper orbits related to missions are obtained by solving two-point boundary value problems. Since a selection method of initial value affects the convergence of the solution, developing an effective method to find an initial guess is required. In this work, B\'ezier curves, which can describe complicated curves and surfaces, are utilized to find the initial guess. First, the given problems are transformed into B\'ezier curves forms, and B\'ezier curves' control points, which can handle the shape of curves, are selected by solving the system of nonlinear equations. Finally, the initial guess is obtained by substituting the calculated control points to B\'ezier curves. To validate the performance of the proposed method, numerical simulations are conducted with respect to three kinds of orbits, which are from circular to highly elliptical orbit (HEO). The proposed method is compared to the general shooting method. The comparison results show that the initial guess calculated by B\'ezier curves makes finding the solution more efficient in terms of computational time and iterations. Also, it shows that the proposed method finds the solution for the HEO while the general shooting method fails to find the solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13921v1</guid>
      <category>math.OC</category>
      <category>physics.class-ph</category>
      <category>physics.space-ph</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daegyun Choi, Sungwook Yang, Henzeh Leeghim, Donghoon Kim</dc:creator>
    </item>
    <item>
      <title>Legendre-Gauss-Lobatto Collocation Method for Optimal Control via Polynomial Approximations of Differential Equation Vector Fields</title>
      <link>https://arxiv.org/abs/2506.13938</link>
      <description>arXiv:2506.13938v1 Announce Type: new 
Abstract: A new method is described for solving optimal control problems using direct collocation at Legendre-Gauss-Lobatto points. The approach of this paper employs a polynomial approximation of the right-hand side vector field of the differential equations and leads to the following important outcomes. First, the first-order optimality conditions of the LGL integral form are derived, which lead to a full-rank transformed adjoint system and novel costate estimate. Next, a derivative-like form of the LGL collocation method is obtained by multiplying the system by the inverse of an appropriate full-rank block of the integration matrix. The first-order optimality conditions of the LGL derivative-like form are then derived, leading to an equivalent full-rank transformed adjoint system and secondary novel costate estimate which is related to the costate estimate of the integral form via a linear transformation. Then, it is shown that a second integral form can be constructed by including an additional noncollocated support point, but such a point is superfluous and has no impact on the solution to the nonlinear programming problem. Finally, the method is demonstrated on two benchmark problems: a one-dimensional initial value optimal control problem with an analytic solution and a time-variant orbit raising optimal control problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13938v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriela Abadia-Doyle, William W. Hager, Anil V. Rao</dc:creator>
    </item>
    <item>
      <title>Projected integral control of impedance passive nonlinear systems</title>
      <link>https://arxiv.org/abs/2506.14267</link>
      <description>arXiv:2506.14267v1 Announce Type: new 
Abstract: We propose an abstract framework for solving the constrained set-point tracking problem for impedance passive infinite-dimensional nonlinear systems. The class of systems considered is governed by monotone differential inclusions and allows us to exploit the theory of contraction semigroups. To account for possible operational constraints, e.g., bounds on the input, we replace a classical integral controller with a projected integral controller. This guarantees that the integrator state remains in a given closed convex set, where said constraints are satisfied. We showcase our results through three case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14267v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Vanspranghe (CRAN), Pietro Lorenzetti (CRAN), Lassi Paunonen (TUT), George Weiss (TAU)</dc:creator>
    </item>
    <item>
      <title>Collaborative Charging Scheduling via Balanced Bounding Box Methods</title>
      <link>https://arxiv.org/abs/2506.14461</link>
      <description>arXiv:2506.14461v1 Announce Type: new 
Abstract: Electric mobility faces several challenges, most notably the high cost of infrastructure development and the underutilization of charging stations. The concept of shared charging offers a promising solution. The paper explores sustainable urban logistics through horizontal collaboration between two fleet operators and addresses a scheduling problem for the shared use of charging stations. To tackle this, the study formulates a collaborative scheduling problem as a bi-objective nonlinear integer programming model, in which each company aims to minimize its own costs, creating inherent conflicts that require trade-offs. The Balanced Bounding Box Methods (B3Ms) are introduced in order to efficiently derive the efficient frontier, identifying a reduced set of representative solutions. These methods enhance computational efficiency by selectively disregarding closely positioned and competing solutions, preserving the diversity and representativeness of the solutions over the efficient frontier. To determine the final solution and ensure balanced collaboration, cooperative bargaining methods are applied. Numerical case studies demonstrate the viability and scalability of the developed methods, showing that the B3Ms can significantly reduce computational time while maintaining the integrity of the frontier. These methods, along with cooperative bargaining, provide an effective framework for solving various bi-objective optimization problems, extending beyond the collaborative scheduling problem presented here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14461v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangting Zhou, Bal\'azs Kulcs\'ar, Jiaming Wu</dc:creator>
    </item>
    <item>
      <title>Discrete time shadow price revisited</title>
      <link>https://arxiv.org/abs/2506.14708</link>
      <description>arXiv:2506.14708v1 Announce Type: new 
Abstract: In the paper discrete time shadow price is constructed for the market with several assets with given bid and ask prices. Shadow price is the price such that the problem of optimal utility from terminal wealth on the market without transaction costs gives the same value function as in the case of bid and ask prices. In the paper we solve first static problem for two assets and then we construct the shadow price for dynamic model. Finally we present construction of the shadow price for several assets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14708v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomasz Rogala, {\L}ukasz Stettner</dc:creator>
    </item>
    <item>
      <title>Analysis and Optimization of Probabilities of Beneficial Mutation and Crossover Recombination in a Hamming Space</title>
      <link>https://arxiv.org/abs/2506.13809</link>
      <description>arXiv:2506.13809v1 Announce Type: cross 
Abstract: Inspired by Fisher's geometric approach to study beneficial mutations, we analyse probabilities of beneficial mutation and crossover recombination of strings in a general Hamming space with arbitrary finite alphabet. Mutations and recombinations that reduce the distance to an optimum are considered as beneficial. Geometric and combinatorial analysis is used to derive closed-form expressions for transition probabilities between spheres around an optimum giving a complete description of Markov evolution of distances from an optimum over multiple generations. This paves the way for optimization of parameters of mutation and recombination operators. Here we derive optimality conditions for mutation and recombination radii maximizing the probabilities of mutation and crossover into the optimum. The analysis highlights important differences between these evolutionary operators. While mutation can potentially reach any part of the search space, the probability of beneficial mutation decreases with distance to an optimum, and the optimal mutation radius or rate should also decrease resulting in a slow-down of evolution near the optimum. Crossover recombination, on the other hand, acts in a subspace of the search space defined by the current population of strings. However, probabilities of beneficial and deleterious crossover are balanced, and their characteristics, such as variance, are translation invariant in a Hamming space, suggesting that recombination may complement mutation and boost the rate of evolution near the optimum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13809v1</guid>
      <category>q-bio.PE</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roman V. Belavkin</dc:creator>
    </item>
    <item>
      <title>Global-in-time optimal control of stochastic third-grade fluids</title>
      <link>https://arxiv.org/abs/2506.14145</link>
      <description>arXiv:2506.14145v1 Announce Type: cross 
Abstract: In this article, we address the velocity tracking control problem for a class of stochastic non-Newtonian fluids. More precisely, we consider the stochastic third-grade fluid equation perturbed by infinite-dimensional additive white noise and defined on the two-dimensional torus $\mathbb{T}^2$. The control acts as a distributed random external force. Taking an \emph{infinite-dimensional Ornstein-Uhlenbeck process}, the stochastic system is converted into an equivalent pathwise deterministic one, which allows to show the well-posedness of the original stochastic system globally in time. The state being a stochastic process with sample paths in $\mathrm{L}^\infty(0,T;\mathbb{H}^3(\mathbb{T}^2))$ and finite moments can be controlled in an optimal way. Namely, we establish the existence and uniqueness of solutions to the corresponding linearized state and adjoint equations. Furthermore, we derive an appropriate stability result for the state equation and verify that the G\^ateaux derivative of the control-to-state mapping coincides with the solution of the linearized state equation. Finally, we establish the first-order optimality conditions and prove the existence of an optimal solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14145v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kush Kinra, Fernanda Cipriano</dc:creator>
    </item>
    <item>
      <title>Towards Robust Learning to Optimize with Theoretical Guarantees</title>
      <link>https://arxiv.org/abs/2506.14263</link>
      <description>arXiv:2506.14263v1 Announce Type: cross 
Abstract: Learning to optimize (L2O) is an emerging technique to solve mathematical optimization problems with learning-based methods. Although with great success in many real-world scenarios such as wireless communications, computer networks, and electronic design, existing L2O works lack theoretical demonstration of their performance and robustness in out-of-distribution (OOD) scenarios. We address this gap by providing comprehensive proofs. First, we prove a sufficient condition for a robust L2O model with homogeneous convergence rates over all In-Distribution (InD) instances. We assume an L2O model achieves robustness for an InD scenario. Based on our proposed methodology of aligning OOD problems to InD problems, we also demonstrate that the L2O model's convergence rate in OOD scenarios will deteriorate by an equation of the L2O model's input features. Moreover, we propose an L2O model with a concise gradient-only feature construction and a novel gradient-based history modeling method. Numerical simulation demonstrates that our proposed model outperforms the state-of-the-art baseline in both InD and OOD scenarios and achieves up to 10 $\times$ convergence speedup. The code of our method can be found from https://github.com/NetX-lab/GoMathL2O-Official.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14263v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CVPR52769.2024.01476</arxiv:DOI>
      <arxiv:journal_reference>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, 17297-17306</arxiv:journal_reference>
      <dc:creator>Qingyu Song, Wei Lin, Juncheng Wang, Hong Xu</dc:creator>
    </item>
    <item>
      <title>Network-Independent Incremental Passivity Conditions for Grid-Forming Inverter Control</title>
      <link>https://arxiv.org/abs/2506.14469</link>
      <description>arXiv:2506.14469v1 Announce Type: cross 
Abstract: Grid-forming inverters control the power transfer between the AC and DC sides of an electrical grid while maintaining the frequency and voltage of the AC side. This paper focuses on ensuring large-signal stability of an electrical grid with inverter-interfaced renewable sources. We prove that the Hybrid-Angle Control (HAC) scheme for grid-forming inverters can exhibit incremental passivity properties between current and voltage at both the AC and DC ports. This incremental passivity can be certified through decentralized conditions. Inverters operating under HAC can, therefore, be connected to other passive elements (e.g. transmission lines) with an immediate guarantee of global transient stability regardless of the network topology or parameters. Passivity of Hybrid Angle Control is also preserved under small-signal (linearized) analyses, in contrast to conventional proportional droop laws that are passivity-short at low frequencies. Passivity and interconnected-stability properties are demonstrated through an example case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14469v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jared Miller, Maitraya Avadhut Desai, Xiuqiang He, Roy S. Smith, Gabriela Hug</dc:creator>
    </item>
    <item>
      <title>Robust Hedging of American Options via Aggregated Snell Envelopes</title>
      <link>https://arxiv.org/abs/2506.14553</link>
      <description>arXiv:2506.14553v1 Announce Type: cross 
Abstract: We construct an aggregator for a family of Snell envelopes in a nondominated framework. We apply this construction to establish a robust hedging duality, along with the existence of a minimal hedging strategy, in a general semi-martingale setting for American-style options. Our results encompass continuous processes, or processes with jumps and non-vanishing diffusion. A key application is to financial market models, where uncertainty is quantified through the semi-martingale characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14553v1</guid>
      <category>q-fin.MF</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Rodrigues</dc:creator>
    </item>
    <item>
      <title>Distributionally robust Kalman filtering with volatility uncertainty</title>
      <link>https://arxiv.org/abs/2302.05993</link>
      <description>arXiv:2302.05993v3 Announce Type: replace 
Abstract: This work presents a distributionally robust Kalman filter to address uncertainties in noise covariance matrices and predicted covariance estimates. We adopt a distributionally robust formulation using bicausal optimal transport to characterize a set of plausible alternative models. The optimization problem is transformed into a convex nonlinear semi-definite programming problem and solved using the trust-region interior point method with the aid of $LDL^\top$ decomposition. The empirical outperformance is demonstrated through target tracking and pairs trading.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.05993v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TAC.2024.3522192</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Automatic Control, 2025</arxiv:journal_reference>
      <dc:creator>Bingyan Han</dc:creator>
    </item>
    <item>
      <title>Parallelized Conflict Graph Cut Generation</title>
      <link>https://arxiv.org/abs/2311.03706</link>
      <description>arXiv:2311.03706v4 Announce Type: replace 
Abstract: A conflict graph represents logical relations between binary variables, and effective use of the graph can significantly accelerate branch-and-cut solvers for mixed-integer programming (MIP). In this paper we develop efficient parallel conflict graph management: conflict detection; maximal clique generation; clique extension; and clique merging. We leverage parallel computing in order to intensify computational effort on the conflict graph, thereby generating a much larger pool of cutting planes than what can be practically achieved in serial. Computational experiments demonstrate that the expanded pool of cuts enabled by parallel computing lead to substantial reductions in total MIP solve time, especially for more challenging cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03706v4</guid>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yongzheng Dai, Chen Chen</dc:creator>
    </item>
    <item>
      <title>Stochastic Bregman Proximal Gradient Method Revisited: Kernel Conditioning and Painless Variance Reduction</title>
      <link>https://arxiv.org/abs/2401.03155</link>
      <description>arXiv:2401.03155v3 Announce Type: replace 
Abstract: We investigate Bregman proximal gradient (BPG) methods for solving nonconvex composite stochastic optimization problems. Instead of the standard gradient Lipschitz continuity (GLC) assumption, the objective function only satisfies a smooth-adaptability assumption w.r.t. some kernel function. An in-depth analysis of the stationarity measure is made in this paper, where we reveal an interesting fact that the widely adopted Bregman proximal gradient mapping in the existing works may not correctly depict the near stationarity of the solutions. To resolve this issue, a new Bregman proximal gradient mapping is proposed and analyzed in this paper. Second, a thorough analysis is made on the sample complexities of the stochastic Bregman proximal gradient methods under both the old and the newly proposed gradient mappings are analyzed. Note that the absence of GLC disables the standard analysis of the stochastic variance reduction techniques, existing stochastic BPG methods only obtain an $O(\epsilon^{-2})$ sample complexity under the old gradient mapping, we show that such a limitation in the existing analyses mainly comes from the insufficient exploitation of the kernel's properties. By proposing a new kernel-conditioning regularity assumption on the kernel, we show that a simple epoch bound mechanism is enough to enable all the existing variance reduction techniques for stochastic BPG methods. Combined with a novel probabilistic argument, we show that there is a high probability event conditioning on which $O(\sqrt{n}\epsilon^{-1})$ sample complexity for the finite-sum stochastic setting can be derived for the old gradient mapping. Moreover, with a novel adaptive step size control mechanism, we also show an $\tilde{O}(\sqrt{n}L_h(\mathcal{X}_\epsilon)\epsilon^{-1})$ complexity under the new gradient mapping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03155v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyu Zhang</dc:creator>
    </item>
    <item>
      <title>Perturbation analysis of a class of composite optimization problems</title>
      <link>https://arxiv.org/abs/2401.10728</link>
      <description>arXiv:2401.10728v2 Announce Type: replace 
Abstract: In this paper, we study the perturbation analysis of a class of composite optimization problems, which is a very convenient and unified framework for developing both theoretical and algorithmic issues of constrained optimization problems. The underlying theme of this paper is very important in both theoretical and computational study of optimization problems. Under some mild assumptions on the objective function, we provide a definition of a strong second order sufficient condition (SSOSC) for the composite optimization problem and also prove that the following conditions are equivalent to each other: the SSOSC and the nondegeneracy condition, the nonsingularity of Clarke's generalized Jacobian of the nonsmooth system at a Karush-Kuhn-Tucker (KKT) point, and the strong regularity of the KKT point. These results provide an important way to characterize the stability of the KKT point.
  As for the convex composite optimization problem, which is a special case of the general problem, we establish the equivalence between the primal/dual second order sufficient condition and the dual/primal strict Robinson constraint qualification, the equivalence between the primal/dual SSOSC and the dual/primal nondegeneracy condition. Moreover, we prove that the dual nondegeneracy condition and the nonsingularity of Clarke's generalized Jacobian of the subproblem corresponding to the augmented Lagrangian method are also equivalent to each other. These theoretical results lay solid foundation for designing an efficient algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10728v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Peipei Tang, Chengjing Wang</dc:creator>
    </item>
    <item>
      <title>Faster Acceleration for Steepest Descent</title>
      <link>https://arxiv.org/abs/2409.19200</link>
      <description>arXiv:2409.19200v3 Announce Type: replace 
Abstract: Recent advances (Sherman, 2017; Sidford and Tian, 2018; Cohen et al., 2021) have overcome the fundamental barrier of dimension dependence in the iteration complexity of solving $\ell_\infty$ regression with first-order methods. Yet it remains unclear to what extent such acceleration can be achieved for general $\ell_p$ smooth functions. In this paper, we propose a new accelerated first-order method for convex optimization under non-Euclidean smoothness assumptions. In contrast to standard acceleration techniques, our approach uses primal-dual iterate sequences taken with respect to $\textit{differing}$ norms, which are then coupled using an $\textit{implicitly}$ determined interpolation parameter. For $\ell_p$ norm smooth problems in $d$ dimensions, our method provides an iteration complexity improvement of up to $O(d^{1-\frac{2}{p}})$ in terms of calls to a first-order oracle, thereby allowing us to circumvent long-standing barriers in accelerated non-Euclidean steepest descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19200v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cedar Site Bai, Brian Bullins</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of the Wasserstein Proximal Algorithm beyond Geodesic Convexity</title>
      <link>https://arxiv.org/abs/2501.14993</link>
      <description>arXiv:2501.14993v2 Announce Type: replace 
Abstract: The proximal algorithm is a powerful tool to minimize nonlinear and nonsmooth functionals in a general metric space. Motivated by the recent progress in studying the training dynamics of the noisy gradient descent algorithm on two-layer neural networks in the mean-field regime, we provide in this paper a simple and self-contained analysis for the convergence of the general-purpose Wasserstein proximal algorithm without assuming geodesic convexity of the objective functional. Under a natural Wasserstein analog of the Euclidean Polyak-{\L}ojasiewicz inequality, we establish that the proximal algorithm achieves an unbiased and linear convergence rate. Our convergence rate improves upon existing rates of the proximal algorithm for solving Wasserstein gradient flows under strong geodesic convexity. We also extend our analysis to the inexact proximal algorithm for geodesically semiconvex objectives. In our numerical experiments, proximal training demonstrates a faster convergence rate than the noisy gradient descent algorithm on mean-field neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14993v2</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shuailong Zhu, Xiaohui Chen</dc:creator>
    </item>
    <item>
      <title>Nonlinearly Preconditioned Gradient Methods under Generalized Smoothness</title>
      <link>https://arxiv.org/abs/2502.08532</link>
      <description>arXiv:2502.08532v2 Announce Type: replace 
Abstract: We analyze nonlinearly preconditioned gradient methods for solving smooth minimization problems. We introduce a generalized smoothness property, based on the notion of abstract convexity, that is broader than Lipschitz smoothness and provide sufficient first- and second-order conditions. Notably, our framework encapsulates algorithms associated with the gradient clipping method and brings out novel insights for the class of $(L_0,L_1)$-smooth functions that has received widespread interest recently, thus allowing us to extend beyond already established methods. We investigate the convergence of the proposed method in both the convex and nonconvex setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08532v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Konstantinos Oikonomidis, Jan Quan, Emanuel Laude, Panagiotis Patrinos</dc:creator>
    </item>
    <item>
      <title>Does DQN Learn?</title>
      <link>https://arxiv.org/abs/2205.13617</link>
      <description>arXiv:2205.13617v5 Announce Type: replace-cross 
Abstract: A primary requirement for any reinforcement learning method is that it should produce policies that improve upon the initial guess. In this work, we show that the widely used Deep Q-Network (DQN) fails to satisfy this minimal criterion -- even when it gets to see all possible states and actions infinitely often (a condition under which tabular Q-learning is guaranteed to converge to the optimal Q-value function). Our specific contributions are twofold. First, we numerically show that DQN often returns a policy that performs worse than the initial one. Second, we offer a theoretical explanation for this phenomenon in linear DQN, a simplified version of DQN that uses linear function approximation in place of neural networks while retaining the other key components such as $\epsilon$-greedy exploration, experience replay, and target network. Using tools from differential inclusion theory, we prove that the limit points of linear DQN correspond to fixed points of projected Bellman operators. Crucially, we show that these fixed points need not relate to optimal -- or even near-optimal -- policies, thus explaining linear DQN's sub-optimal behaviors. We also give a scenario where linear DQN always identifies the worst policy. Our work fills a longstanding gap in understanding the convergence behaviors of Q-learning with function approximation and $\epsilon$-greedy exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.13617v5</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Aditya Gopalan, Gugan Thoppe</dc:creator>
    </item>
    <item>
      <title>SensLI: Sensitivity-Based Layer Insertion for Neural Networks</title>
      <link>https://arxiv.org/abs/2311.15995</link>
      <description>arXiv:2311.15995v2 Announce Type: replace-cross 
Abstract: The training of neural networks requires tedious and often manual tuning of the network architecture. We propose a systematic approach to inserting new layers during the training process. Our method eliminates the need to choose a fixed network size before training, is numerically inexpensive to execute and applicable to various architectures including fully connected feedforward networks, ResNets and CNNs. Our technique borrows ideas from constrained optimization and is based on first-order sensitivity information of the loss function with respect to the virtual parameters that additional layers, if inserted, would offer. In numerical experiments, our proposed sensitivity-based layer insertion technique (SensLI) exhibits improved performance on training loss and test error, compared to training on a fixed architecture, and reduced computational effort in comparison to training the extended architecture from the beginning. Our code is available on https://github.com/mathemml/SensLI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15995v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Leonie Kreis, Evelyn Herberg, Frederik K\"ohne, Anton Schiela, Roland Herzog</dc:creator>
    </item>
    <item>
      <title>Five Starter Problems: Solving Quadratic Unconstrained Binary Optimization Models on Quantum Computers</title>
      <link>https://arxiv.org/abs/2401.08989</link>
      <description>arXiv:2401.08989v3 Announce Type: replace-cross 
Abstract: This tutorial offers a quick, hands-on introduction to solving Quadratic Unconstrained Binary Optimization (QUBO) models on currently available quantum computers and their simulators. We cover both IBM and D-Wave machines: IBM utilizes a gate-circuit architecture, and D-Wave is a quantum annealer. We provide examples of three canonical problems and two models from practical applications. The tutorial is structured to bridge the gap between theory and practice: we begin with an overview of QUBOs, explain their relevance and connection to quantum algorithms, introduce key quantum computing concepts, provide the foundations for two quantum heuristics, and provide detailed implementation guides. An associated GitHub repository provides the codes in five companion notebooks. In addition to reaching undergraduate and graduate students in computationally intensive disciplines, this article aims to reach working industry professionals seeking to explore the potential of near-term quantum applications. As our title indicates, this tutorial is intended to be a starting point in a journey towards solving more complex QUBOs on quantum computers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08989v3</guid>
      <category>quant-ph</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arul Mazumder, Sridhar Tayur</dc:creator>
    </item>
    <item>
      <title>Sample Complexity of the Linear Quadratic Regulator: A Reinforcement Learning Lens</title>
      <link>https://arxiv.org/abs/2404.10851</link>
      <description>arXiv:2404.10851v3 Announce Type: replace-cross 
Abstract: We provide the first known algorithm that provably achieves $\varepsilon$-optimality within $\widetilde{\mathcal{O}}(1/\varepsilon)$ function evaluations for the discounted discrete-time LQR problem with unknown parameters, without relying on two-point gradient estimates. These estimates are known to be unrealistic in many settings, as they depend on using the exact same initialization, which is to be selected randomly, for two different policies. Our results substantially improve upon the existing literature outside the realm of two-point gradient estimates, which either leads to $\widetilde{\mathcal{O}}(1/\varepsilon^2)$ rates or heavily relies on stability assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10851v3</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Amirreza Neshaei Moghaddam, Alex Olshevsky, Bahman Gharesifard</dc:creator>
    </item>
    <item>
      <title>Fine-grained Analysis and Faster Algorithms for Iteratively Solving Linear Systems</title>
      <link>https://arxiv.org/abs/2405.05818</link>
      <description>arXiv:2405.05818v2 Announce Type: replace-cross 
Abstract: Despite being a key bottleneck in many machine learning tasks, the cost of solving large linear systems has proven challenging to quantify due to problem-dependent quantities such as condition numbers. To tackle this, we consider a fine-grained notion of complexity for solving linear systems, which is motivated by applications where the data exhibits low-dimensional structure, including spiked covariance models and kernel machines, and when the linear system is explicitly regularized, such as ridge regression. Concretely, let $\kappa_\ell$ be the ratio between the $\ell$th largest and the smallest singular value of $n\times n$ matrix $A$. We give a stochastic algorithm based on the Sketch-and-Project paradigm, that solves the linear system $Ax = b$, that is, finds $\bar{x}$ such that $\|A\bar{x} - b\| \le \epsilon \|b\|$, in time $\bar O(\kappa_\ell\cdot n^2\log 1/\epsilon)$, for any $\ell = O(n^{0.729})$. This is a direct improvement over preconditioned conjugate gradient, and it provides a stronger separation between stochastic linear solvers and algorithms accessing $A$ only through matrix-vector products. Our main technical contribution is the new analysis of the first and second moments of the random projection matrix that arises in Sketch-and-Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05818v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Micha{\l} Derezi\'nski, Daniel LeJeune, Deanna Needell, Elizaveta Rebrova</dc:creator>
    </item>
    <item>
      <title>Revealed Information</title>
      <link>https://arxiv.org/abs/2411.13293</link>
      <description>arXiv:2411.13293v2 Announce Type: replace-cross 
Abstract: An analyst observes the frequency with which a decision maker (DM) takes actions, but not the frequency conditional on payoff-relevant states. We ask when the analyst can rationalize the DM's choices as if the DM first learns something about the state before acting. We provide a support-function characterization of the triples of utility functions, prior beliefs, and (marginal) distributions over actions such that the DM's action distribution is consistent with information given the DM's prior and utility function. Assumptions on the cardinality of the state space and the utility function allow us to refine this characterization, obtaining a sharp system of finitely many inequalities the utility function, prior, and action distribution must satisfy. We apply our characterization to study comparative statics and to identify conditions under which a single information structure rationalizes choices across multiple decision problems. We characterize the set of distributions over posterior beliefs that are consistent with the DM's choices. We extend our results to settings with a continuum of actions and states assuming the first-order approach applies, and to simple multi-agent settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13293v2</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Doval, Ran Eilat, Tianhao Liu, Yangfan Zhou</dc:creator>
    </item>
    <item>
      <title>Fast Switching in Mixed-Integer Model Predictive Control</title>
      <link>https://arxiv.org/abs/2411.19300</link>
      <description>arXiv:2411.19300v3 Announce Type: replace-cross 
Abstract: We derive stability results for finite control set and mixed-integer model predictive control with a downstream oversampling phase. The presentation rests upon the inherent robustness of model predictive control with stabilizing terminal conditions and techniques for solving mixed-integer optimal control problems by continuous optimization. Partial outer convexification and binary relaxation transform mixed-integer problems into common optimal control problems. We deduce nominal asymptotic stability for the resulting relaxed system formulation and implement sum-up rounding to restore efficiently integer feasibility on an oversampling time grid. If fast control switching is technically possible and inexpensive, we can approximate the relaxed system behavior in the state space arbitrarily close. We integrate input perturbed model predictive control with practical asymptotic stability. Numerical experiments illustrate practical relevance of fast control switching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19300v3</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Artemi Makarow, Christian Kirches</dc:creator>
    </item>
    <item>
      <title>Learning Spatially Adaptive $\ell_1$-Norms Weights for Convolutional Synthesis Regularization</title>
      <link>https://arxiv.org/abs/2503.09483</link>
      <description>arXiv:2503.09483v3 Announce Type: replace-cross 
Abstract: We propose an unrolled algorithm approach for learning spatially adaptive parameter maps in the framework of convolutional synthesis-based $\ell_1$ regularization. More precisely, we consider a family of pre-trained convolutional filters and estimate deeply parametrized spatially varying parameters applied to the sparse feature maps by means of unrolling a FISTA algorithm to solve the underlying sparse estimation problem. The proposed approach is evaluated for image reconstruction of low-field MRI and compared to spatially adaptive and non-adaptive analysis-type procedures relying on Total Variation regularization and to a well-established model-based deep learning approach. We show that the proposed approach produces visually and quantitatively comparable results with the latter approaches and at the same time remains highly interpretable. In particular, the inferred parameter maps quantify
  the local contribution of each filter in the reconstruction, which provides valuable insight into the algorithm mechanism and could potentially be used to discard unsuited filters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09483v3</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Kofler, Luca Calatroni, Christoph Kolbitsch, Kostas Papafitsoros</dc:creator>
    </item>
    <item>
      <title>Accelerating RLHF Training with Reward Variance Increase</title>
      <link>https://arxiv.org/abs/2505.23247</link>
      <description>arXiv:2505.23247v2 Announce Type: replace-cross 
Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique for ensuring that large language models (LLMs) are aligned with human values and preferences during the post-training phase. As an effective RLHF approach, group relative policy optimization (GRPO) has demonstrated success in many LLM-based applications. However, efficient GRPO-based RLHF training remains a challenge. Recent studies reveal that a higher reward variance of the initial policy model leads to faster RLHF training. Inspired by this finding, we propose a practical reward adjustment model to accelerate RLHF training by provably increasing the reward variance and preserving the relative preferences and reward expectation. Our reward adjustment method inherently poses a nonconvex optimization problem, which is NP-hard to solve in general. To overcome the computational challenges, we design a novel $O(n \log n)$ algorithm to find a global solution of the nonconvex reward adjustment model by explicitly characterizing the extreme points of the feasible set. As an important application, we naturally integrate this reward adjustment model into the GRPO algorithm, leading to a more efficient GRPO with reward variance increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we provide an indirect explanation for the empirical effectiveness of GRPO with rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment results demonstrate that the GRPOVI algorithm can significantly improve the RLHF training efficiency compared to the original GRPO algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23247v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zonglin Yang, Zhexuan Gu, Houduo Qi, Yancheng Yuan</dc:creator>
    </item>
  </channel>
</rss>
