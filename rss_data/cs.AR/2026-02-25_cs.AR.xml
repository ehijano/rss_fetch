<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Feb 2026 02:52:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>SegSEM: Enabling and Enhancing SAM2 for SEM Contour Extraction</title>
      <link>https://arxiv.org/abs/2602.20471</link>
      <description>arXiv:2602.20471v1 Announce Type: new 
Abstract: Extracting high-fidelity 2D contours from Scanning Electron Microscope (SEM) images is critical for calibrating Optical Proximity Correction (OPC) models. While foundation models like Segment Anything 2 (SAM2) are promising, adapting them to specialized domains with scarce annotated data is a major challenge. This paper presents a case study on adapting SAM2 for SEM contour extraction in a few-shot setting. We propose SegSEM, a framework built on two principles: a data-efficient fine-tuning strategy that adapts by selectively training only the model's encoders, and a robust hybrid architecture integrating a traditional algorithm as a confidence-aware fallback. Using a small dataset of 60 production images, our experiments demonstrate this methodology's viability. The primary contribution is a methodology for leveraging foundation models in data-constrained industrial applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20471v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Da Chen, Guangyu Hu, Kaihong Xu, Kaichao Liang, Songjiang Li, Wei Yang, XiangYu Wen, Mingxuan Yuan</dc:creator>
    </item>
    <item>
      <title>FAST-Prefill: FPGA Accelerated Sparse Attention for Long Context LLM Prefill</title>
      <link>https://arxiv.org/abs/2602.20515</link>
      <description>arXiv:2602.20515v1 Announce Type: new 
Abstract: In long-context large language model (LLM) inference, the prefill stage dominates computation due to self-attention over the complete input context. Sparse attention significantly reduces self-attention computation by limiting each token's interactions to a subset of tokens. The attention sparsity pattern varies across input prompts, and within a prompt, each attention head can follow a distinct pattern. This makes attention sparsity dynamic. The requirement of generating the sparsity pattern, combined with limited data reuse in attention, shifts the prefill compute to being memory-bound. This, in addition to the huge energy requirements for long-context inference on GPU, motivates FPGAs as good candidates for accelerating dynamic long-context inference.
  To tackle these challenges, we propose FAST-Prefill, the first FPGA accelerator for long-context prefill-stage inference with dynamic sparse attention. To efficiently generate sparse indices, we propose a \textit{fused pipeline unit with a memory-aware execution order} to reduce large tensors and irregular memory accesses. To reduce off-chip memory traffic for accessing the KV cache, we utilize the memory hierarchy to design a \textit{liveness-driven, dual-tier cache}. For high-throughput matrix multiplication, we design a \textit{hybrid Matrix Processing Unit (MPU)} with DSPs and bit-plane decomposition using LUTs. We implement FAST-Prefill on Alveo U280 and evaluate it on the Llama and Qwen models (batch size = 1) for context lengths ranging from 4K to 128K tokens. We demonstrate an average speedup of up to 2.5$\times$ in TTFT and 4.5$\times$ improvement in energy efficiency over GPU implementation on Nvidia A5000 GPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20515v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rakshith Jayanth, Viktor Prasanna</dc:creator>
    </item>
    <item>
      <title>TOM: A Ternary Read-only Memory Accelerator for LLM-powered Edge Intelligence</title>
      <link>https://arxiv.org/abs/2602.20662</link>
      <description>arXiv:2602.20662v1 Announce Type: new 
Abstract: The deployment of Large Language Models (LLMs) for real-time intelligence on edge devices is rapidly growing. However, conventional hardware architectures face a fundamental memory wall challenge, where limited on-device memory capacity and bandwidth severely constrain the size of deployable models and their inference speed, while also limiting on-device adaptation. To address this challenge, we propose TOM, a hybrid ROM-SRAM accelerator co-designed with ternary quantization, which balances extreme density with on-device tunability. TOM exploits the synergy between ternary quantization and ROM to achieve extreme memory density and bandwidth, while preserving flexibility through a hybrid ROM-SRAM architecture designed for QLoRA-based tunability. Specifically, we introduce: (1) a sparsity-aware ROM architecture that synthesizes ternary weights as standard-cell logic, eliminating area overhead from zero-valued bits; (2) a distributed processing architecture that co-locates high-density ROM banks with flexible SRAM-based QLoRA adapters and compute units; and (3) a workload-aware dynamic power gating scheme that exploits the logic-based nature of ROM to power down inactive banks, minimizing dynamic energy consumption. TOM achieves an inference throughput of 3,306 TPS using BitNet-2B model, demonstrating its effectiveness in delivering real-time, energy-efficient edge intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20662v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyi Guan, Yijia Zhang, Wenqiang Wang, Yizhao Gao, Shijie Cao, Chen Zhang, Ningyi Xu</dc:creator>
    </item>
    <item>
      <title>LUTstructions: Self-loading FPGA-based Reconfigurable Instructions</title>
      <link>https://arxiv.org/abs/2602.20802</link>
      <description>arXiv:2602.20802v1 Announce Type: new 
Abstract: General-purpose processors feature a limited number of instructions based on an instruction set. They can be numerous, such as with vector extensions that include hundreds or thousands of instructions, but this comes at a cost; they are often unable to express arbitrary tasks efficiently. This paper explores the concept of having reconfigurable instructions by incorporating reconfigurable areas in a softcore. It follows a relatively-recently proposed computer architecture concept for seamlessly loading instruction implementation-carrying bitstreams from main memory. The resulting softcore is entirely evaluated on an FPGA, essentially having an FPGA-on-an-FPGA for the instruction implementations, with no notable operating frequency overhead. This is achieved with a custom FPGA architecture called LUTstruction, which is tailored towards low-latency for custom instructions and wide reconfiguration, as well as a soft implementation for the purposes of architectural exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20802v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippos Papaphilippou</dc:creator>
    </item>
    <item>
      <title>Compute System Organization for High Frequency High Order Wavefront Sensing and Control</title>
      <link>https://arxiv.org/abs/2602.20298</link>
      <description>arXiv:2602.20298v1 Announce Type: cross 
Abstract: Maintaining long-term wavefront stability is critical for the Habitable Worlds Observatory (HWO), which targets contrasts approaching $10^{-10}$ and therefore requires continuous dark-zone maintenance using high-order wavefront sensing and control (HOWFSC). Prior work has advanced HOWFSC algorithms and profiled candidate implementations on radiation-hardened processors, highlighting a substantial gap between the computational demands of LUVOIR-scale HOWFSC and the capabilities of current onboard spacecraft hardware. In this paper, we argue that this gap can be closed by offloading the HOWFSC pipeline to a dedicated co-flying compute satellite at Sun-Earth L2. This approach enables the use of modern, radiation-tolerant high-performance processors without increasing risk to the primary observatory. We show that such an architecture can increase the end-to-end control cadence from the sub-hertz regime typical of radiation-hardened onboard processing or ground-in-the-loop operation to tens and even hundreds of hertz. We evaluate commercial hardware platforms in terms of performance and feasibility, and we propose custom architectures that enable higher control frequencies with significant power consumption reductions. Finally, we outline system-level considerations for co-flying compute, including reliability, satellite integration, and inter-satellite communication constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20298v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.AR</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barry Lyu, Vaibhavi Manjarekar, Nathaniel Bleier</dc:creator>
    </item>
    <item>
      <title>GauS: Differentiable Scheduling Optimization via Gaussian Reparameterization</title>
      <link>https://arxiv.org/abs/2602.20427</link>
      <description>arXiv:2602.20427v1 Announce Type: cross 
Abstract: Efficient operator scheduling is a fundamental challenge in software compilation and hardware synthesis. While recent differentiable approaches have sought to replace traditional ones like exact solvers or heuristics with gradient-based search, they typically rely on categorical distributions that fail to capture the ordinal nature of time and suffer from a parameter space that scales poorly. In this paper, we propose a novel differentiable framework, GauS, that models operator scheduling as a stochastic relaxation using Gaussian distributions, which fully utilize modern parallel computing devices like GPUs. By representing schedules as continuous Gaussian variables, we successfully capture the ordinal nature of time and reduce the optimization space by orders of magnitude. Our method is highly flexible to represent various objectives and constraints, which provides the first differentiable formulation for the complex pipelined scheduling problem. We evaluate our method on a range of benchmarks, demonstrating that Gaus achieves Pareto-optimal results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20427v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaohui Cai, Vesal Bakhtazad, Cunxi Yu, Zhiru Zhang</dc:creator>
    </item>
    <item>
      <title>Characterizing State Space Model and Hybrid Language Model Performance with Long Context</title>
      <link>https://arxiv.org/abs/2507.12442</link>
      <description>arXiv:2507.12442v3 Announce Type: replace 
Abstract: Emerging applications such as AR are driving demands for machine intelligence capable of processing continuous and/or long-context inputs on local devices. However, currently dominant models based on Transformer architecture suffers from the quadratic computational and memory overhead, which hinders applications required to process long contexts. This has spurred a paradigm shift towards new architectures like State Space Models (SSMs) and SSM-Transformer hybrid models, which provide near-linear scaling. The near-linear scaling enabled efficient handling of millions of tokens while delivering high performance in recent studies. Although such works present promising results, their workload characteristics in terms of computational performance and hardware resource requirements are not yet thoroughly explored, which limits our understanding of their implications to the system level optimizations. To address this gap, we present a comprehensive, compara-ive benchmarking of carefully selected Transformers, SSMs, and hybrid models specifically for long-context inference on consumer and embedded GPUs. Our analysis shows that SSMs are well-suited for on-device AI on consumer and embedded GPUs for long context inferences. While Transformers are up to 1.9x faster at short sequences (&lt;8K tokens), SSMs demonstrate a dramatic performance inversion, becoming up to 4x faster at very long contexts (~57K tokens), thanks to their linear computational complexity and ~64% reduced memory footrprint. Our operator-level analysis reveals that custom SSM kernels like selective scan despite being hardware-aware to minimize memory IO, dominate the inference runtime on edge platforms, accounting for over 55% of latency due to their sequential, element-wise nature. To foster further research, we will open-source our characterization framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12442v3</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saptarshi Mitra, Rachid Karami, Haocheng Xu, Sitao Huang, Hyoukjun Kwon</dc:creator>
    </item>
    <item>
      <title>RPU -- A Reasoning Processing Unit</title>
      <link>https://arxiv.org/abs/2602.18568</link>
      <description>arXiv:2602.18568v2 Announce Type: replace 
Abstract: Large language model (LLM) inference performance is increasingly bottlenecked by the memory wall. While GPUs continue to scale raw compute throughput, they struggle to deliver scalable performance for memory bandwidth bound workloads. This challenge is amplified by emerging reasoning LLM applications, where long output sequences, low arithmetic intensity, and tight latency constraints demand significantly higher memory bandwidth. As a result, system utilization drops and energy per inference rises, highlighting the need for an optimized system architecture for scalable memory bandwidth.
  To address these challenges we present the Reasoning Processing Unit (RPU), a chiplet-based architecture designed to address the challenges of the modern memory wall. RPU introduces: (1) A Capacity-Optimized High-Bandwidth Memory (HBM-CO) that trades capacity for lower energy and cost; (2) a scalable chiplet architecture featuring a bandwidth-first power and area provisioning design; and (3) a decoupled microarchitecture that separates memory, compute, and communication pipelines to sustain high bandwidth utilization. Simulation results show that RPU performs up to 45.3x lower latency and 18.6x higher throughput over an H100 system at ISO-TDP on Llama3-405B.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18568v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Adiletta, Gu-Yeon Wei, David Brooks</dc:creator>
    </item>
    <item>
      <title>Interconnect-Aware Logic Resynthesis for Multi-Die FPGAs</title>
      <link>https://arxiv.org/abs/2602.19720</link>
      <description>arXiv:2602.19720v2 Announce Type: replace 
Abstract: Multi-die FPGAs enable device scaling beyond reticle limits but introduce severe interconnect overhead across die boundaries. Inter-die connections, commonly referred to as super-long lines (SLLs), incur high delay and consume scarce interposer interconnect resources, often dominating critical paths and complicating physical design. To address this, this work proposes an interconnect-aware logic resynthesis method that restructures the LUT-level netlist to reduce the number of SLLs. The resynthesis engine uses die partitioning information to apply logic resubstitutions, which simplifies local circuit structures and eliminates SLLs. By reducing the number of SLLs early in the design flow, prior to physical implementation, the proposed method shortens critical paths, alleviates pressure on scarce interposer interconnect resources, and improves overall physical design flexibility. We further build a tool flow for multi-die FPGAs by integrating the proposed resynthesis method with packing and placement. Experimental results on the EPFL benchmarks show that, compared with a state-of-the-art framework, the proposed method reduces the number of SLLs by up to 24.8% for a 2-die FPGA and up to 27.38% for a 3-die FPGA. On MCNC benchmarks, our tool flow achieves an average SLL reduction of 1.65% while preserving placement quality. On Koios benchmarks, where fewer removable SLLs exist, several designs still exhibit considerable inter-die edge reductions. Overall, the results confirm that reducing inter-die connections at the logic level is an effective approach for multi-die FPGAs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19720v2</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoke Wang, Raveena Raikar, Markus Rein, Ruiqi Chen, Chang Meng, Dirk Stroobandt</dc:creator>
    </item>
  </channel>
</rss>
