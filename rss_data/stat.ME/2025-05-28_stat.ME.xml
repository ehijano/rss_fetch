<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 May 2025 01:57:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>High-Dimensional Sparse Clustering via Iterative Semidefinite Programming Relaxed K-Means</title>
      <link>https://arxiv.org/abs/2505.20478</link>
      <description>arXiv:2505.20478v1 Announce Type: new 
Abstract: We propose an iterative algorithm for clustering high-dimensional data, where the true signal lies in a much lower-dimensional space. Our method alternates between feature selection and clustering, without requiring precise estimation of sparse model parameters. Feature selection is performed by thresholding a rough estimate of the discriminative direction, while clustering is carried out via a semidefinite programming (SDP) relaxation of K-means. In the isotropic case, the algorithm is motivated by the minimax separation bound for exact recovery of cluster labels using varying sparse subsets of features. This bound highlights the critical role of variable selection in achieving exact recovery. We further extend the algorithm to settings with unknown sparse precision matrices, avoiding full model parameter estimation by computing only the minimally required quantities. Across a range of simulation settings, we find that the proposed iterative approach outperforms several state-of-the-art methods, especially in higher dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20478v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jongmin Mun, Paromita Dubey, Yingying Fan</dc:creator>
    </item>
    <item>
      <title>Hybrid Bayesian Estimation in the additive hazards model</title>
      <link>https://arxiv.org/abs/2505.20681</link>
      <description>arXiv:2505.20681v1 Announce Type: new 
Abstract: Hereby we propose a Bayesian method of estimation for the semiparametric Additive Hazards Model (AHM) from Survival Analysis under right-censoring. With this aim, we review the AHM revisiting the likelihood function, so as to comment on the challenges posed by Bayesian estimation from the full likelihood. Through an algorithmic reformulation of that likelihood, we present an alternative method based on a hybrid Bayesian treatment that exploits Lin and Ying (1994) estimating equation approach and which chooses tractable priors for the parameters. We obtain the estimators from the posterior distributions in closed form, we perform a small simulation experiment, and lastly, we illustrate our method with the classical Nickels Miners dataset and a brief simulation experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20681v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Enrique Ernesto \'Alvarez, Maximiliano Luis Riddick</dc:creator>
    </item>
    <item>
      <title>Semi-supervised Clustering Through Representation Learning of Large-scale EHR Data</title>
      <link>https://arxiv.org/abs/2505.20731</link>
      <description>arXiv:2505.20731v1 Announce Type: new 
Abstract: Electronic Health Records (EHR) offer rich real-world data for personalized medicine, providing insights into disease progression, treatment responses, and patient outcomes. However, their sparsity, heterogeneity, and high dimensionality make them difficult to model, while the lack of standardized ground truth further complicates predictive modeling. To address these challenges, we propose SCORE, a semi-supervised representation learning framework that captures multi-domain disease profiles through patient embeddings. SCORE employs a Poisson-Adapted Latent factor Mixture (PALM) Model with pre-trained code embeddings to characterize codified features and extract meaningful patient phenotypes and embeddings. To handle the computational challenges of large-scale data, it introduces a hybrid Expectation-Maximization (EM) and Gaussian Variational Approximation (GVA) algorithm, leveraging limited labeled data to refine estimates on a vast pool of unlabeled samples. We theoretically establish the convergence of this hybrid approach, quantify GVA errors, and derive SCORE's error rate under diverging embedding dimensions. Our analysis shows that incorporating unlabeled data enhances accuracy and reduces sensitivity to label scarcity. Extensive simulations confirm SCORE's superior finite-sample performance over existing methods. Finally, we apply SCORE to predict disability status for patients with multiple sclerosis (MS) using partially labeled EHR data, demonstrating that it produces more informative and predictive patient embeddings for multiple MS-related conditions compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20731v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linshanshan Wang, Mengyan Li, Zongqi Xia, Molei Liu, Tianxi Cai</dc:creator>
    </item>
    <item>
      <title>Causal inference with dyadic data in randomized experiments</title>
      <link>https://arxiv.org/abs/2505.20780</link>
      <description>arXiv:2505.20780v1 Announce Type: new 
Abstract: Estimating the treatment effect within network structures is a key focus in online controlled experiments, particularly for social media platforms. We investigate a scenario where the unit-level outcome of interest comprises a series of dyadic outcomes, which is pervasive in many social network sources, spanning from microscale point-to-point messaging to macroscale international trades. Dyadic outcomes are of particular interest in online controlled experiments, capturing pairwise interactions as basic units for analysis. The dyadic nature of the data induces interference, as treatment assigned to one unit may affect outcomes involving connected pairs. We propose a novel design-based causal inference framework for dyadic outcomes in randomized experiments, develop estimators of the global average causal effect, and establish their asymptotic properties under different randomization designs. We prove the central limit theorem for the estimators and propose variance estimators to quantify the estimation uncertainty. The advantages of integrating dyadic data in randomized experiments are manifested in a variety of numerical experiments, especially in correcting interference bias. We implement our proposed method in a large-scale experiment on WeChat Channels, assessing the impact of a recommendation algorithm on users' interaction metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20780v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yilin Li, Lu Deng, Yong Wang, Wang Miao</dc:creator>
    </item>
    <item>
      <title>Debiased Ill-Posed Regression</title>
      <link>https://arxiv.org/abs/2505.20787</link>
      <description>arXiv:2505.20787v1 Announce Type: new 
Abstract: In various statistical settings, the goal is to estimate a function which is restricted by the statistical model only through a conditional moment restriction. Prominent examples include the nonparametric instrumental variable framework for estimating the structural function of the outcome variable, and the proximal causal inference framework for estimating the bridge functions. A common strategy in the literature is to find the minimizer of the projected mean squared error. However, this approach can be sensitive to misspecification or slow convergence rate of the estimators of the involved nuisance components. In this work, we propose a debiased estimation strategy based on the influence function of a modification of the projected error and demonstrate its finite-sample convergence rate. Our proposed estimator possesses a second-order bias with respect to the involved nuisance functions and a desirable robustness property with respect to the misspecification of one of the nuisance functions. The proposed estimator involves a hyper-parameter, for which the optimal value depends on potentially unknown features of the underlying data-generating process. Hence, we further propose a hyper-parameter selection approach based on cross-validation and derive an error bound for the resulting estimator. This analysis highlights the potential rate loss due to hyper-parameter selection and underscore the importance and advantages of incorporating debiasing in this setting. We also study the application of our approach to the estimation of regular parameters in a specific parameter class, which are linear functionals of the solutions to the conditional moment restrictions and provide sufficient conditions for achieving root-n consistency using our debiased estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20787v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>AmirEmad Ghassami, James M. Robins, Andrea Rotnitzky</dc:creator>
    </item>
    <item>
      <title>Stability Selection via Variable Decorrelation</title>
      <link>https://arxiv.org/abs/2505.20864</link>
      <description>arXiv:2505.20864v1 Announce Type: new 
Abstract: The Lasso is a prominent algorithm for variable selection. However, its instability in the presence of correlated variables in the high-dimensional setting is well-documented. Although previous research has attempted to address this issue by modifying the Lasso loss function, this paper introduces an approach that simplifies the data processed by Lasso. We propose that decorrelating variables before applying the Lasso improves the stability of variable selection regardless of the direction of correlation among predictors. Furthermore, we highlight that the irrepresentable condition, which ensures consistency for the Lasso, is satisfied after variable decorrelation under two assumptions. In addition, by noting that the instability of the Lasso is not limited to high-dimensional settings, we demonstrate the effectiveness of the proposed approach for low-dimensional data. Finally, we present empirical results that indicate the efficacy of the proposed method across different variable selection techniques, highlighting its potential for broader application. The DVS R package is developed to facilitate the implementation of the methodology proposed in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20864v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Nouraie, Connor Smith, Samuel Muller</dc:creator>
    </item>
    <item>
      <title>A longitudinal Bayesian framework for estimating causal dose-response relationships</title>
      <link>https://arxiv.org/abs/2505.20893</link>
      <description>arXiv:2505.20893v1 Announce Type: new 
Abstract: Existing causal methods for time-varying exposure and time-varying confounding focus on estimating the average causal effect of a time-varying binary treatment on an end-of-study outcome. Methods for estimating the effects of a time-varying continuous exposure at any dose level on the outcome are limited. We introduce a scalable, non-parametric Bayesian framework for estimating longitudinal causal dose-response relationships with repeated measures. We incorporate the generalized propensity score either as a covariate or through inverse-probability weighting, formulating two Bayesian dose-response estimators. The proposed approach embeds a double non-parametric generalized Bayesian bootstrap which enables a flexible Dirichlet process specification within a generalized estimating equations structure, capturing temporal correlation while making minimal assumptions about the functional form of the continuous exposure. We applied our proposed approach to a motivating study of monthly metro-ridership data and COVID-19 case counts from major international cities, identifying causal relationships and the dynamic dose-response patterns between higher ridership and increased case counts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20893v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Luo, Kuan Liu, Ramandeep Singh, Daniel J. Graham</dc:creator>
    </item>
    <item>
      <title>The Correlation Thresholding Algorithm for Exploratory Factor Analysis</title>
      <link>https://arxiv.org/abs/2505.21100</link>
      <description>arXiv:2505.21100v1 Announce Type: new 
Abstract: Exploratory factor analysis is often used in the social sciences to estimate potential measurement models. To do this, several important issues need to be addressed: (1) determining the number of factors, (2) learning constraints in the factor loadings, and (3) selecting a solution amongst rotationally equivalent choices. Traditionally, these issues are treated separately. This work examines the Correlation Thresholding (CT) algorithm, which uses a graph-theoretic perspective to solve all three simultaneously, from a unified framework. Despite this advantage, it relies on several assumptions that may not hold in practice. We discuss the implications of these assumptions and assess the sensitivity of the CT algorithm to them for practical use in exploratory factor analysis. This is examined over a series of simulation studies, as well as a real data example. The CT algorithm shows reasonable robustness against violating these assumptions and very competitive performance in comparison to other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21100v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10705511.2025.2512350</arxiv:DOI>
      <dc:creator>Dale S. Kim, Audrey Lu, Qing Zhou</dc:creator>
    </item>
    <item>
      <title>Model averaging with mixed criteria for estimating high quantiles of extreme values: Application to heavy rainfall</title>
      <link>https://arxiv.org/abs/2505.21417</link>
      <description>arXiv:2505.21417v1 Announce Type: new 
Abstract: Accurately estimating high quantiles beyond the largest observed value is crucial in risk assessment and devising effective adaptation strategies to prevent a greater disaster. The generalized extreme value distribution is widely used for this purpose, with L-moment estimation (LME) and maximum likelihood estimation (MLE) being the primary methods. However, estimating high quantiles with a small sample size becomes challenging when the upper endpoint is unbounded, or equivalently, when there are larger uncertainties involved in extrapolation. This study introduces an improved approach using a model averaging (MA) technique. The proposed method combines MLE and LME to construct candidate submodels and assign weights effectively. The properties of the proposed approach are evaluated through Monte Carlo simulations and an application to maximum daily rainfall data in Korea. Additionally, theoretical considerations are provided, including asymptotic variance with random weights. A surrogate model of MA estimation is also developed and applied for further analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21417v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yonggwan Shin, Yire Shin, Jeong-Soo Park</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to the survivor average causal effect in cluster-randomized crossover trials</title>
      <link>https://arxiv.org/abs/2505.21447</link>
      <description>arXiv:2505.21447v1 Announce Type: new 
Abstract: In cluster-randomized crossover (CRXO) trials, groups of individuals are randomly assigned to two or more sequences of alternating treatments. Since clusters act as their own control, the CRXO design is typically more statistically efficient than the usual parallel-arm trial. CRXO trials are increasingly popular in many areas of health research where the number of available clusters is limited. Further, in trials among severely ill patients, researchers often want to assess the effect of treatments on secondary non-terminal outcomes, but frequently in these studies, there are patients who do not survive to have these measurements fully recorded. In this paper, we provide a causal inference framework and treatment effect estimation methods for addressing truncation by death in the setting of CRXO trials. We target the survivor average causal effect (SACE) estimand, a well-defined subgroup treatment effect obtained via principal stratification. We propose novel structural and standard modeling assumptions to enable SACE identification followed by estimation within a Bayesian paradigm. We evaluate the small-sample performance of our proposed Bayesian approach for the estimation of the SACE in CRXO trial settings via simulation studies. We apply our methods to a previously conducted two-period cross-sectional CRXO study examining the impact of proton pump inhibitors compared to histamine-2 receptor blockers on length of hospitalization among adults requiring invasive mechanical ventilation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21447v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dane Isenberg, Michael O. Harhay, Andrew B. Forbes, Paul J. Young, Fan Li, Nandita Mitra</dc:creator>
    </item>
    <item>
      <title>Tissue-specific predictive performance: A unified estimation and inference framework for multi-category screening tests</title>
      <link>https://arxiv.org/abs/2505.21482</link>
      <description>arXiv:2505.21482v1 Announce Type: new 
Abstract: Multi-Cancer Early Detection (MCED) testing with tissue localization aims to detect and identify multiple cancer types from a single blood sample. Such tests have the potential to aid clinical decisions and significantly improve health outcomes. Despite this promise, MCED testing has not yet achieved regulatory approval, reimbursement or broad clinical adoption. One major reason for this shortcoming is uncertainty about test performance resulting from the reporting of clinically obtuse metrics. Traditionally, MCED tests report aggregate measures of test performance, disregarding cancer type, that obscure biological variability and underlying differences in the test's behavior, limiting insight into true effectiveness. Clinically informative evaluation of an MCED test's performance requires metrics that are specific to cancer types. In the context of a case-control sampling design, this paper derives analytical methods that estimate cancer-specific intrinsic accuracy, tissue localization readout-specific predictive value and the marginal test classification distribution, each with corresponding confidence interval formulae. A simulation study is presented that evaluates performance of the proposed methodology and provides guidance for implementation. An application to a published MCED test dataset is given. These statistical approaches allow for estimation and inference for the pointed metric of an MCED test that allow its evaluation to support a potential role in early cancer detection. This framework enables more precise clinical decision-making, supports optimized trial designs across classical, digital, AI-driven, and hybrid stratified diagnostic screening platforms, and facilitates informed healthcare decisions by clinicians, policymakers, regulators, scientists, and patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21482v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>A. Gregory DiRienzo, Elie Massaad, Hutan Ashrafian</dc:creator>
    </item>
    <item>
      <title>Bayesian Dynamic Clustering Factor Models</title>
      <link>https://arxiv.org/abs/2505.21490</link>
      <description>arXiv:2505.21490v1 Announce Type: new 
Abstract: We propose novel Bayesian Dynamic Clustering Factor Models (BDCFM) for the analysis of multivariate longitudinal data. BDCFM combines factor models with hidden Markov models to concomitantly perform dimension reduction, clustering, and estimation of the dynamic transitions of subjects through clusters. We develop an efficient Gibbs sampler for exploration of the posterior distribution. An analysis of a simulated dataset shows that our inferential approach works well both at parameter estimation and clustering of subjects. Finally, we illustrate the utility of our BDCFM with an analysis of a dataset on opioid use disorder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21490v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tsering Dolkar, Marco A. R. Ferreira, Hwasoo Shin, Allison N. Tegge</dc:creator>
    </item>
    <item>
      <title>TabPFN: One Model to Rule Them All?</title>
      <link>https://arxiv.org/abs/2505.20003</link>
      <description>arXiv:2505.20003v1 Announce Type: cross 
Abstract: Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a transformer-based deep learning model for regression and classification on tabular data, which they claim "outperforms all previous methods on datasets with up to 10,000 samples by a wide margin, using substantially less training time." Furthermore, they have called TabPFN a "foundation model" for tabular data, as it can support "data generation, density estimation, learning reusable embeddings and fine-tuning". If these statements are well-supported, TabPFN may have the potential to supersede existing modeling approaches on a wide range of statistical tasks, mirroring a similar revolution in other areas of artificial intelligence that began with the advent of large language models. In this paper, we provide a tailored explanation of how TabPFN works for a statistics audience, by emphasizing its interpretation as approximate Bayesian inference. We also provide more evidence of TabPFN's "foundation model" capabilities: We show that an out-of-the-box application of TabPFN vastly outperforms specialized state-of-the-art methods for semi-supervised parameter estimation, prediction under covariate shift, and heterogeneous treatment effect estimation. We further show that TabPFN can outperform LASSO at sparse regression and can break a robustness-efficiency trade-off in classification. All experiments can be reproduced using the code provided at https://github.com/qinglong-tian/tabpfn_study (https://github.com/qinglong-tian/tabpfn_study).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20003v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiong Zhang, Yan Shuo Tan, Qinglong Tian, Pengfei Li</dc:creator>
    </item>
    <item>
      <title>Covariate-Adjusted Deep Causal Learning for Heterogeneous Panel Data Models</title>
      <link>https://arxiv.org/abs/2505.20536</link>
      <description>arXiv:2505.20536v1 Announce Type: cross 
Abstract: This paper studies the task of estimating heterogeneous treatment effects in causal panel data models, in the presence of covariate effects. We propose a novel Covariate-Adjusted Deep Causal Learning (CoDEAL) for panel data models, that employs flexible model structures and powerful neural network architectures to cohesively deal with the underlying heterogeneity and nonlinearity of both panel units and covariate effects. The proposed CoDEAL integrates nonlinear covariate effect components (parameterized by a feed-forward neural network) with nonlinear factor structures (modeled by a multi-output autoencoder) to form a heterogeneous causal panel model. The nonlinear covariate component offers a flexible framework for capturing the complex influences of covariates on outcomes. The nonlinear factor analysis enables CoDEAL to effectively capture both cross-sectional and temporal dependencies inherent in the data panel. This latent structural information is subsequently integrated into a customized matrix completion algorithm, thereby facilitating more accurate imputation of missing counterfactual outcomes. Moreover, the use of a multi-output autoencoder explicitly accounts for heterogeneity across units and enhances the model interpretability of the latent factors. We establish theoretical guarantees on the convergence of the estimated counterfactuals, and demonstrate the compelling performance of the proposed method using extensive simulation studies and a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20536v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanhao Zhou, Yuefeng Han, Xiufan Yu</dc:creator>
    </item>
    <item>
      <title>Eigenstructure inference for high-dimensional covariance with generalized shrinkage inverse-Wishart prior</title>
      <link>https://arxiv.org/abs/2505.20668</link>
      <description>arXiv:2505.20668v1 Announce Type: cross 
Abstract: In multivariate statistics, estimating the covariance matrix is essential for understanding the interdependence among variables. In high-dimensional settings, where the number of covariates increases with the sample size, it is well known that the eigenstructure of the sample covariance matrix is inconsistent. The inverse-Wishart prior, a standard choice for covariance estimation in Bayesian inference, also suffers from posterior inconsistency. To address the issue of eigenvalue dispersion in high-dimensional settings, the shrinkage inverse-Wishart (SIW) prior has recently been proposed. Despite its conceptual appeal and empirical success, the asymptotic justification for the SIW prior has remained limited. In this paper, we propose a generalized shrinkage inverse-Wishart (gSIW) prior for high-dimensional covariance modeling. By extending the SIW framework, the gSIW prior accommodates a broader class of prior distributions and facilitates the derivation of theoretical properties under specific parameter choices. In particular, under the spiked covariance assumption, we establish the asymptotic behavior of the posterior distribution for both eigenvalues and eigenvectors by directly evaluating the posterior expectations for two sets of parameter choices. This direct evaluation provides insights into the large-sample behavior of the posterior that cannot be obtained through general posterior asymptotic theorems. Finally, simulation studies illustrate that the proposed prior provides accurate estimation of the eigenstructure, particularly for spiked eigenvalues, achieving narrower credible intervals and higher coverage probabilities compared to existing methods. For spiked eigenvectors, the performance is generally comparable to that of competing approaches, including the sample covariance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20668v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seongmin Kim, Kwangmin Lee, Sewon Park, Jaeyong Lee</dc:creator>
    </item>
    <item>
      <title>A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data</title>
      <link>https://arxiv.org/abs/2505.20688</link>
      <description>arXiv:2505.20688v1 Announce Type: cross 
Abstract: False discovery rate (FDR) control methods are essential for voxel-wise multiple testing in neuroimaging data analysis, where hundreds of thousands or even millions of tests are conducted to detect brain regions associated with disease-related changes. Classical FDR control methods (e.g., BH, q-value, and LocalFDR) assume independence among tests and often lead to high false non-discovery rates (FNR). Although various spatial FDR control methods have been developed to improve power, they still fall short in jointly addressing three major challenges in neuroimaging applications: capturing complex spatial dependencies, maintaining low variability in both false discovery proportion (FDP) and false non-discovery proportion (FNP) across replications, and achieving computational scalability for high-resolution data. To address these challenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR control method for voxel-wise multiple testing. It integrates the local index of significance (LIS)-based testing procedure with a novel fully connected hidden Markov random field (fcHMRF) designed to model complex spatial structures using a parsimonious parameterization. We develop an efficient expectation-maximization algorithm incorporating mean-field approximation, the Conditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and permutohedral lattice filtering, reducing the computational complexity from quadratic to linear in the number of tests. Extensive simulations demonstrate that fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability in FDP and FNP, and a higher number of true positives compared to existing methods. Applied to an FDG-PET dataset from the Alzheimer's Disease Neuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain regions and offers notable advantages in computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20688v1</guid>
      <category>stat.ML</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taehyo Kim, Qiran Jia, Mony J. de Leon, Hai Shu</dc:creator>
    </item>
    <item>
      <title>Performance of prior event rate ratio method in the presence of differential mortality or dropout</title>
      <link>https://arxiv.org/abs/2505.20757</link>
      <description>arXiv:2505.20757v1 Announce Type: cross 
Abstract: Purpose: Prior event rate ratio (PERR) method was proposed to control for unmeasured confounding in real-world evaluation of effectiveness and safety of pharmaceutical products. A widely cited simulation study showed that PERR estimate of treatment effect was biased in the presence of differential morality/dropout. However, the study only considered one specific PERR estimator of treatment effect and one specific scenario of differential mortality/dropout. To enhance understanding of the method, we replicated and extended the simulation to consider an alternative PERR estimator and multiple scenarios. Methods: Simulation studies were performed with varying rate of mortality/dropout, including the same scenario in the previous study in which mortality/dropout was simultaneously influenced by treatment, confounder and prior event and scenarios that differed in the determinants of mortality/dropout. In addition to the PERR estimator used in the previous study (PERR_Prev) that involved data form both completers and non-completers, we also evaluated an alternative PERR estimator (PERR_Comp) that used data only from completers. Results: The bias of PERR_Prev in the previously considered mortality/dropout scenario was replicated. Bias of PERR_Comp was only about one-third in magnitude as compared to that of PERR_Prev in this scenario. Furthermore, PERR_Prev did but PERR_Comp did not give biased estimates of treatment effect in scenarios that mortality/dropout was influenced by treatment or confounder but not prior event. Conclusions: The PERR is better seen as a methodological framework. Its performance depends on the specifications within the framework. PERR_Comp provides unbiased estimates unless mortality/dropout is affected by prior event.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20757v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Bun Cheung, Xiangmei Ma</dc:creator>
    </item>
    <item>
      <title>Almost Unbiased Liu Type Estimator in Bell Regression Model: Theory, Simulation and Application</title>
      <link>https://arxiv.org/abs/2505.20946</link>
      <description>arXiv:2505.20946v1 Announce Type: cross 
Abstract: In this research, we propose a novel regression estimator as an alternative to the Liu estimator for addressing multicollinearity in the Bell regression model, referred to as the almost unbiased Liu estimator. Moreover, the theoretical characteristics of the proposed estimator are analyzed, along with several theorems that specify the conditions under which the almost unbiased Liu estimator outperforms its alternatives. A comprehensive simulation study is conducted to demonstrate the superiority of the almost unbiased Liu estimator and to compare it against the Bell Liu estimator and the maximum likelihood estimator. The practical applicability and advantage of the proposed regression estimator are illustrated through a real-world dataset. The results from both the simulation study and the real-world data application indicate that the new almost unbiased Liu regression estimator outperforms its counterparts based on the mean square error criterion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20946v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caner Tan{\i}\c{s}, Yasin Asar</dc:creator>
    </item>
    <item>
      <title>Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance</title>
      <link>https://arxiv.org/abs/2505.21101</link>
      <description>arXiv:2505.21101v1 Announce Type: cross 
Abstract: Classifier-Free Guidance (CFG) is a widely used technique for improving conditional diffusion models by linearly combining the outputs of conditional and unconditional denoisers. While CFG enhances visual quality and improves alignment with prompts, it often reduces sample diversity, leading to a challenging trade-off between quality and diversity. To address this issue, we make two key contributions. First, CFG generally does not correspond to a well-defined denoising diffusion model (DDM). In particular, contrary to common intuition, CFG does not yield samples from the target distribution associated with the limiting CFG score as the noise level approaches zero -- where the data distribution is tilted by a power $w \gt 1$ of the conditional distribution. We identify the missing component: a R\'enyi divergence term that acts as a repulsive force and is required to correct CFG and render it consistent with a proper DDM. Our analysis shows that this correction term vanishes in the low-noise limit. Second, motivated by this insight, we propose a Gibbs-like sampling procedure to draw samples from the desired tilted distribution. This method starts with an initial sample from the conditional diffusion model without CFG and iteratively refines it, preserving diversity while progressively enhancing sample quality. We evaluate our approach on both image and text-to-audio generation tasks, demonstrating substantial improvements over CFG across all considered metrics. The code is available at https://github.com/yazidjanati/cfgig</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21101v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Badr Moufad, Yazid Janati, Alain Durmus, Ahmed Ghorbel, Eric Moulines, Jimmy Olsson</dc:creator>
    </item>
    <item>
      <title>Linear classification methods for multivariate repeated measures data -- a simulation study</title>
      <link>https://arxiv.org/abs/2310.00107</link>
      <description>arXiv:2310.00107v3 Announce Type: replace 
Abstract: Researchers in the behavioral and social sciences use linear discriminant analysis (LDA) for predictions of group membership (classification) and for identifying the variables most relevant to group separation among a set of continuous correlated variables (description). \\ In these and other disciplines, longitudinal data are often collected which provide additional temporal information. Linear classification methods for repeated measures data are more sensitive to actual group differences by taking the complex correlations between time points and variables into account, but are rarely discussed in the literature. Moreover, psychometric data rarely fulfill the multivariate normality assumption.\\ In this paper, we compare existing linear classification algorithms for nonnormally distributed multivariate repeated measures data in a simulation study based on psychological questionnaire data comprising Likert scales. The results show that in data without any specific assumed structure and larger sample sizes, the robust alternatives to standard repeated measures LDA may not be needed. To our knowledge, this is one of the few studies discussing repeated measures classification techniques, and the first one comparing multiple alternatives among each other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00107v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricarda Graf, Marina Zeldovich, Sarah Friedrich</dc:creator>
    </item>
    <item>
      <title>Testing High-Dimensional Mediation Effect with Arbitrary Exposure-Mediator Coefficients</title>
      <link>https://arxiv.org/abs/2310.05539</link>
      <description>arXiv:2310.05539v3 Announce Type: replace 
Abstract: In response to the unique challenge created by high-dimensional mediators in mediation analysis, this paper presents a novel procedure for testing the nullity of the mediation effect in the presence of high-dimensional mediators. The procedure incorporates two distinct features. Firstly, the test remains valid under all cases of the composite null hypothesis, including the challenging scenario where both exposure-mediator and mediator-outcome coefficients are zero. Secondly, it does not impose structural assumptions on the exposure-mediator coefficients, thereby allowing for an arbitrarily strong exposure-mediator relationship. To the best of our knowledge, the proposed test is the first of its kind to provably possess these two features in high-dimensional mediation analysis. The validity and consistency of the proposed test are established, and its numerical performance is showcased through simulation studies. The application of the proposed test is demonstrated by examining the mediation effect of DNA methylation between smoking status and lung cancer development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05539v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinan Lin, Zijian Guo, Baoluo Sun, Zhenhua Lin</dc:creator>
    </item>
    <item>
      <title>Skew-elliptical copula based mixed models for non-Gaussian longitudinal data with application to an HIV-AIDS study</title>
      <link>https://arxiv.org/abs/2402.00651</link>
      <description>arXiv:2402.00651v4 Announce Type: replace 
Abstract: This study was sparked by an extensive longitudinal dataset focusing on HIV CD4 T$^+$ cell counts from Livingstone district, Zambia. Analysis of the corresponding histogram plots reveals an absence of symmetry in the marginal distributions, while pairwise scatter plots uncover non-elliptical dependence patterns. Traditional linear mixed models designed for longitudinal data fail to capture these complexities adequately. Therefore, it appears prudent to explore a broader framework for modeling such data. In this article, we delve into generalized linear mixed models (GLMM) for the marginals (e.g., the Gamma mixed model), and we address the temporal dependency of repeated measurements by utilizing copulas associated with skew-elliptical distributions (such as the skew-normal/skew-$t$). Our proposed class of copula-based mixed models simultaneously accommodates asymmetry, between-subject variability, and non-standard temporal dependence, thus offering extensions to the standard linear mixed model based on multivariate normality. We estimate the model parameters using the IFM (inference function of margins) method and outline the process of obtaining standard errors for parameter estimates. Through extensive simulation studies covering skewed and symmetric marginal distributions and various copula choices, we assess the finite sample performance of our approach. Finally, we apply these models to the HIV dataset and present our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00651v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>High-dimensional covariance regression with application to co-expression QTL detection</title>
      <link>https://arxiv.org/abs/2404.02093</link>
      <description>arXiv:2404.02093v2 Announce Type: replace 
Abstract: While covariance matrices have been widely studied in many scientific fields, relatively limited progress has been made on estimating conditional covariances that permits a large covariance matrix to vary with high-dimensional subject-level covariates. In this paper, we present a new sparse covariance regression framework that models the covariance matrix as a function of subject-level covariates. In the context of co-expression quantitative trait locus (QTL) studies, our method can be used to determine if and how gene co-expressions vary with genetic variations. To accommodate high-dimensional responses and covariates, we stipulate a combined sparsity structure that encourages covariates with non-zero effects and edges that are modulated by these covariates to be simultaneously sparse. We approach parameter estimation with a blockwise coordinate descent algorithm, and investigate the $\ell_1$ and $\ell_2$ convergence rate of the estimated parameters. In addition, we propose a computationally efficient debiased inference procedure for uncertainty quantification. The efficacy of the proposed method is demonstrated through numerical experiments and an application to a gene co-expression network study with brain cancer patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02093v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rakheon Kim, Jingfei Zhang</dc:creator>
    </item>
    <item>
      <title>Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices</title>
      <link>https://arxiv.org/abs/2502.01512</link>
      <description>arXiv:2502.01512v3 Announce Type: replace 
Abstract: Circular and non-flat data distributions are prevalent across diverse domains of data science, yet their specific geometric structures often remain underutilized in machine learning frameworks. A principled approach to accounting for the underlying geometry of such data is pivotal, particularly when extending statistical models, like the pervasive Gaussian distribution. In this work, we tackle those issue by focusing on the manifold of symmetric positive definite (SPD) matrices, a key focus in information geometry. We introduce a non-isotropic wrapped Gaussian by leveraging the exponential map, we derive theoretical properties of this distribution and propose a maximum likelihood framework for parameter estimation. Furthermore, we reinterpret established classifiers on SPD through a probabilistic lens and introduce new classifiers based on the wrapped Gaussian model. Experiments on synthetic and real-world datasets demonstrate the robustness and flexibility of this geometry-aware distribution, underscoring its potential to advance manifold-based data analysis. This work lays the groundwork for extending classical machine learning and statistical methods to more complex and structured data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01512v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thibault de Surrel, Fabien Lotte, Sylvain Chevallier, Florian Yger</dc:creator>
    </item>
    <item>
      <title>Exact identifiability analysis for a class of partially observed near-linear stochastic differential equation models</title>
      <link>https://arxiv.org/abs/2503.19241</link>
      <description>arXiv:2503.19241v2 Announce Type: replace 
Abstract: Stochasticity plays a key role in many biological systems, necessitating the calibration of stochastic mathematical models to interpret associated data. For model parameters to be estimated reliably, it is typically the case that they must be structurally identifiable. Yet, while theory underlying structural identifiability analysis for deterministic differential equation models is highly developed, there are currently no tools for the general assessment of stochastic models. In this work, we extend the well-established differential algebra framework for structural identifiability analysis to linear and a class of near-linear, two-dimensional, partially observed stochastic differential equation (SDE) models. Our framework is based on a deterministic recurrence relation that describes the dynamics of the statistical moments of the system of SDEs. From this relation, we iteratively form a series of necessarily satisfied equations involving only the observed moments, from which we are able to establish structurally identifiable parameter combinations. We demonstrate our framework for a suite of linear (two- and $n$-dimensional) and non-linear (two-dimensional) models. Most importantly, we define the notion of structural identifiability for SDE models and establish the effect of the initial condition on identifiability. We conclude with a discussion on the applicability and limitations of our approach, and potential future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19241v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander P Browning, Michael J Chappell, Hamid Rahkooy, Torkel E Loman, Ruth E Baker</dc:creator>
    </item>
    <item>
      <title>New financial ratios based on the compositional data methodology</title>
      <link>https://arxiv.org/abs/2210.11138</link>
      <description>arXiv:2210.11138v2 Announce Type: replace-cross 
Abstract: Due to their type of mathematical construction, the use of standard financial ratios in studies analysing the financial health of a group of firms leads to a series of statistical problems that can invalidate the results obtained. These problems are originated by the asymmetry of financial ratios. The present article justifies the use of a new methodology using compositional data (CoDa) to analyse the financial statements of a sector, improving analyses using conventional ratios since the new methodology enables statistical techniques to be applied without encountering any serious drawbacks such as skewness and outliers, and without the results depending on the arbitrary choice as to which of the accounting figures is the numerator of the ratio and which is the denominator. An example with data of the wine sector is provided. The results show that when using CoDa, outliers and skewness are much reduced and results are invariant to numerator and denominator permutation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.11138v2</guid>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.3390/axioms11120694</arxiv:DOI>
      <arxiv:journal_reference>Axioms, 11, 12 (2022), 694</arxiv:journal_reference>
      <dc:creator>Salvador Linares-Mustar\'os (University of Girona), Maria \`Angels Farreras-Noguer (University of Girona), N\'uria Arimany-Serrat (University of Vic-Central University of Catalonia), Germ\`a Coenders (University of Girona)</dc:creator>
    </item>
    <item>
      <title>Subscedastic weighted least squares estimates</title>
      <link>https://arxiv.org/abs/2404.00753</link>
      <description>arXiv:2404.00753v4 Announce Type: replace-cross 
Abstract: In the heteroscedastic linear model, the weighted least squares (WLS) estimate of the model coefficients is more efficient than the ordinary least squares (OLS) esti- mate. However, the practical application of WLS is challenging because it requires knowledge of the error variances. Feasible weighted least squares (FLS) estimates, which use approximations of the variances when they are unknown, may either be more or less efficient than the OLS estimate depending on the quality of the approx- imation. A direct comparison between FLS and OLS has significant implications for the application of regression analysis in varied fields, yet such a comparison remains an unresolved challenge. In this study, we address this challenge by identifying the conditions under which FLS estimates using fixed weights demonstrate greater effi- ciency than the OLS estimate. These conditions provide guidance for the design of feasible estimates using random weights. They also shed light on how certain robust regression estimates behave with respect to the linear model with normal errors of unequal variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00753v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordan Bryan, Haibo Zhou, Didong Li</dc:creator>
    </item>
    <item>
      <title>Comparing Experimental and Nonexperimental Methods: What Lessons Have We Learned Four Decades After LaLonde (1986)?</title>
      <link>https://arxiv.org/abs/2406.00827</link>
      <description>arXiv:2406.00827v3 Announce Type: replace-cross 
Abstract: In 1986, Robert LaLonde published an article comparing nonexperimental estimates to experimental benchmarks (LaLonde 1986). He concluded that the nonexperimental methods at the time could not systematically replicate experimental benchmarks, casting doubt on their credibility. Following LaLonde's critical assessment, there have been significant methodological advances and practical changes, including (i) an emphasis on the unconfoundedness assumption separated from functional form considerations, (ii) a focus on the importance of overlap in covariate distributions, (iii) the introduction of propensity score-based methods leading to doubly robust estimators, (iv) methods for estimating and exploiting treatment effect heterogeneity, and (v) a greater emphasis on validation exercises to bolster research credibility. To demonstrate the practical lessons from these advances, we reexamine the LaLonde data. We show that modern methods, when applied in contexts with sufficient covariate overlap, yield robust estimates for the adjusted differences between the treatment and control groups. However, this does not imply that these estimates are causally interpretable. To assess their credibility, validation exercises (such as placebo tests) are essential, whereas goodness-of-fit tests alone are inadequate. Our findings highlight the importance of closely examining the assignment process, carefully inspecting overlap, and conducting validation exercises when analyzing causal effects with nonexperimental data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00827v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guido Imbens, Yiqing Xu</dc:creator>
    </item>
    <item>
      <title>Conditional Distribution Compression via the Kernel Conditional Mean Embedding</title>
      <link>https://arxiv.org/abs/2504.10139</link>
      <description>arXiv:2504.10139v2 Announce Type: replace-cross 
Abstract: Existing distribution compression methods, like Kernel Herding (KH), were originally developed for unlabelled data. However, no existing approach directly compresses the conditional distribution of labelled data. To address this gap, we first introduce the Average Maximum Conditional Mean Discrepancy (AMCMD), a natural metric for comparing conditional distributions. We then derive a consistent estimator for the AMCMD and establish its rate of convergence. Next, we make a key observation: in the context of distribution compression, the cost of constructing a compressed set targeting the AMCMD can be reduced from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$. Building on this, we extend the idea of KH to develop Average Conditional Kernel Herding (ACKH), a linear-time greedy algorithm that constructs a compressed set targeting the AMCMD. To better understand the advantages of directly compressing the conditional distribution rather than doing so via the joint distribution, we introduce Joint Kernel Herding (JKH), a straightforward adaptation of KH designed to compress the joint distribution of labelled data. While herding methods provide a simple and interpretable selection process, they rely on a greedy heuristic. To explore alternative optimisation strategies, we propose Joint Kernel Inducing Points (JKIP) and Average Conditional Kernel Inducing Points (ACKIP), which jointly optimise the compressed set while maintaining linear complexity. Experiments show that directly preserving conditional distributions with ACKIP outperforms both joint distribution compression (via JKH and JKIP) and the greedy selection used in ACKH. Moreover, we see that JKIP consistently outperforms JKH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10139v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominic Broadbent, Nick Whiteley, Robert Allison, Tom Lovett</dc:creator>
    </item>
  </channel>
</rss>
