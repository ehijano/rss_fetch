<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.AS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.AS</link>
    <description>eess.AS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.AS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Nov 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>CardioPHON: Quality assessment and self-supervised pretraining for screening of cardiac function based on phonocardiogram recordings</title>
      <link>https://arxiv.org/abs/2511.04533</link>
      <description>arXiv:2511.04533v1 Announce Type: new 
Abstract: Remote monitoring of cardiovascular diseases plays an essential role in early detection of abnormal cardiac function, enabling timely intervention, improved preventive care, and personalized patient treatment. Abnormalities in the heart sounds can be detected automatically via computer-assisted decision support systems, and used as the first-line screening tool for detection of cardiovascular problems, or for monitoring the effects of treatments and interventions. We propose in this paper CardioPHON, an integrated heart sound quality assessment and classification tool that can be used for screening of abnormal cardiac function from phonocardiogram recordings. The model is pretrained in a self-supervised fashion on a collection of six small- and mid-sized heart sound datasets, enables automatic removal of low quality recordings to ensure that subtle sounds of heart abnormalities are not misdiagnosed, and provides a state-of-the-art performance for the heart sound classification task. The multimodal model that combines audio and socio-demographic features demonstrated superior performance, achieving the best ranking on the official leaderboard of the 2022 George B. Moody PhysioNet heart sound challenge, whereas the unimodal model, that is based only on phonocardiogram recordings, holds the first position among the unimodal approaches (a total rank 4), surpassing the models utilizing multiple modalities. CardioPHON is the first publicly released pretrained model in the domain of heart sound recordings, facilitating the development of data-efficient artificial intelligence models that can generalize to various downstream tasks in cardiovascular diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04533v1</guid>
      <category>eess.AS</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.bspc.2025.109047</arxiv:DOI>
      <arxiv:journal_reference>Biomedical Signal Processing and Control 113 (2026) 109047</arxiv:journal_reference>
      <dc:creator>Vladimir Despotovic, Peter Pocta, Andrej Zgank</dc:creator>
    </item>
    <item>
      <title>MusRec: Zero-Shot Text-to-Music Editing via Rectified Flow and Diffusion Transformers</title>
      <link>https://arxiv.org/abs/2511.04376</link>
      <description>arXiv:2511.04376v1 Announce Type: cross 
Abstract: Music editing has emerged as an important and practical area of artificial intelligence, with applications ranging from video game and film music production to personalizing existing tracks according to user preferences. However, existing models face significant limitations, such as being restricted to editing synthesized music generated by their own models, requiring highly precise prompts, or necessitating task-specific retraining, thus lacking true zero-shot capability. Leveraging recent advances in rectified flow and diffusion transformers, we introduce MusRec, the first zero-shot text-to-music editing model capable of performing diverse editing tasks on real-world music efficiently and effectively. Experimental results demonstrate that our approach outperforms existing methods in preserving musical content, structural consistency, and editing fidelity, establishing a strong foundation for controllable music editing in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04376v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <category>eess.AS</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Boudaghi, Hadi Zare</dc:creator>
    </item>
    <item>
      <title>PromptSep: Generative Audio Separation via Multimodal Prompting</title>
      <link>https://arxiv.org/abs/2511.04623</link>
      <description>arXiv:2511.04623v1 Announce Type: cross 
Abstract: Recent breakthroughs in language-queried audio source separation (LASS) have shown that generative models can achieve higher separation audio quality than traditional masking-based approaches. However, two key limitations restrict their practical use: (1) users often require operations beyond separation, such as sound removal; and (2) relying solely on text prompts can be unintuitive for specifying sound sources. In this paper, we propose PromptSep to extend LASS into a broader framework for general-purpose sound separation. PromptSep leverages a conditional diffusion model enhanced with elaborated data simulation to enable both audio extraction and sound removal. To move beyond text-only queries, we incorporate vocal imitation as an additional and more intuitive conditioning modality for our model, by incorporating Sketch2Sound as a data augmentation strategy. Both objective and subjective evaluations on multiple benchmarks demonstrate that PromptSep achieves state-of-the-art performance in sound removal and vocal-imitation-guided source separation, while maintaining competitive results on language-queried source separation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04623v1</guid>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yutong Wen, Ke Chen, Prem Seetharaman, Oriol Nieto, Jiaqi Su, Rithesh Kumar, Minje Kim, Paris Smaragdis, Zeyu Jin, Justin Salamon</dc:creator>
    </item>
    <item>
      <title>dCoNNear: An Artifact-Free Neural Network Architecture for Closed-loop Audio Signal Processing</title>
      <link>https://arxiv.org/abs/2501.04116</link>
      <description>arXiv:2501.04116v3 Announce Type: replace 
Abstract: Recent advances in deep neural networks (DNNs) have significantly improved various audio processing applications, including speech enhancement, synthesis, and hearing-aid algorithms. DNN-based closed-loop systems have gained popularity in these applications due to their robust performance and ability to adapt to diverse conditions. Despite their effectiveness, current DNN-based closed-loop systems often suffer from sound quality degradation caused by artifacts introduced by suboptimal sampling methods. To address this challenge, we introduce dCoNNear, a novel DNN architecture designed for seamless integration into closed-loop frameworks. This architecture specifically aims to prevent the generation of spurious artifacts-most notably tonal and aliasing artifacts arising from non-ideal sampling layers. We demonstrate the effectiveness of dCoNNear through a proof-of-principle example within a closed-loop framework that employs biophysically realistic models of auditory processing for both normal and hearing-impaired profiles to design personalized hearing-aid algorithms. We further validate the broader applicability and artifact-free performance of dCoNNear through speech-enhancement experiments, confirming its ability to improve perceptual sound quality without introducing architecture-induced artifacts. Our results show that dCoNNear not only accurately simulates all processing stages of existing non-DNN biophysical models but also significantly improves sound quality by eliminating audible artifacts in both hearing-aid and speech-enhancement applications. This study offers a robust, perceptually transparent closed-loop processing framework for high-fidelity audio applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04116v3</guid>
      <category>eess.AS</category>
      <category>cs.SD</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TASLPRO.2025.3622985</arxiv:DOI>
      <arxiv:journal_reference>C. Wen, G. Torfs and S. Verhulst, "dCoNNear: An Artifact-Free Neural Network Architecture for Closed-Loop Audio Signal Processing," in IEEE Transactions on Audio, Speech and Language Processing, vol. 33, pp. 4414-4429, 2025</arxiv:journal_reference>
      <dc:creator>Chuan Wen, Guy Torfs, Sarah Verhulst</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification in Melody Estimation using Histogram Representation</title>
      <link>https://arxiv.org/abs/2505.05156</link>
      <description>arXiv:2505.05156v2 Announce Type: replace 
Abstract: Confidence estimation can improve the reliability of melody estimation by indicating which predictions are likely incorrect. The existing classification-based approach provides confidence for predicted pitch classes but fails to capture the magnitude of deviation from the ground truth. To address this limitation, we reformulate melody estimation as a regression problem and propose a novel approach to estimate uncertainty directly from the histogram representation of the pitch values, which correlates well with the deviation between the prediction and the ground-truth. We design three methods to model pitch on a continuous support range of histogram, which introduces the challenge of handling the discontinuity of unvoiced from the voiced pitch values. The first two methods address the abrupt discontinuity by mapping the pitch values to a continuous range, while the third adopts a fully Bayesian formulation, which models voicing detection as a classification and voiced pitch estimation as a regression task. Experimental results demonstrate that regression-based formulations yield more reliable uncertainty estimates compared to classification-based approaches in identifying incorrect pitch predictions. Comparing the proposed methods with a state-of-the-art regression model, it is observed that the Bayesian method performs the best at estimating both the melody and its associated uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05156v2</guid>
      <category>eess.AS</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kavya Ranjan Saxena, Vipul Arora</dc:creator>
    </item>
  </channel>
</rss>
