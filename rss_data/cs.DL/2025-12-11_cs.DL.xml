<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Dec 2025 05:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Optimizing Data Extraction from Materials Science Literature: A Study of Tools Using Large Language Models</title>
      <link>https://arxiv.org/abs/2512.09370</link>
      <description>arXiv:2512.09370v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly utilized for large-scale extraction and organization of unstructured data owing to their exceptional Natural Language Processing (NLP) capabilities. Empowering materials design, vast amounts of data from experiments and simulations are scattered across numerous scientific publications, but high-quality experimental databases are scarce. This study considers the effectiveness and practicality of five representative AI tools (ChemDataExtractor, BERT-PSIE, ChatExtract, LangChain, and Kimi) to extract bandgaps from 200 randomly selected Materials Science publications in two presentations (arXiv and publisher versions), comparing the results to those obtained by human processing. Although the integrity of data extraction has not met expectations, encouraging results have been achieved in terms of precision and the ability to eliminate irrelevant papers from human consideration. Our analysis highlights both the strengths and limitations of these tools, offering insights into improving future data extraction techniques for enhanced scientific discovery and innovation. In conjunction with recent research, we provide guidance on feasible improvements for future data extraction methodologies, helping to bridge the gap between unstructured scientific data and structured, actionable databases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09370v1</guid>
      <category>cs.DL</category>
      <category>cond-mat.mtrl-sci</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenkai Ning, Musen Li, Jeffrey R. Reimers, Rika Kobayashi</dc:creator>
    </item>
    <item>
      <title>Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science</title>
      <link>https://arxiv.org/abs/2512.09895</link>
      <description>arXiv:2512.09895v1 Announce Type: cross 
Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09895v1</guid>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jane Greenberg, Scott McClellan, Addy Ireland, Robert Sammarco, Colton Gerber, Christopher B. Rauch, Mat Kelly, John Kunze, Yuan An, Eric Toberer</dc:creator>
    </item>
    <item>
      <title>Investigating the originality of scientific papers across time and domain: A quantitative analysis</title>
      <link>https://arxiv.org/abs/2512.07892</link>
      <description>arXiv:2512.07892v2 Announce Type: replace 
Abstract: The study of creativity in science has long sought quantitative metrics capable of capturing the originality of the scientific insights contained within articles and other scientific works. In recent years, the field has witnessed a substantial expansion of research activity, enabled by advances in natural language processing and network analysis, and has utilised both macro- and micro-scale approaches with success. However, they often do not examine the text itself for evidence of originality. In this paper, we apply a computational measure correlating with originality from creativity science, Divergent Semantic Integration (DSI), to a set of 51,200 scientific abstracts and titles sourced from the Web of Science. To adapt DSI for application to scientific texts, we advance the original BERT method by incorporating SciBERT (a model trained on scientific corpora) into the computation of DSI. In our study, we observe that DSI plays a more pronounced role in the accrual of early citations for papers with fewer authors, varies substantially across subjects and research fields, and exhibits a declining correlation with citation counts over time. Furthermore, by modelling SciBERT- and BERT-DSI as predictors of the logarithm of 5-year citation counts alongside field, publication year, and the logarithm of author count, we find statistically significant relationships, with adjusted R-squared of 0.103 and 0.101 for BERT-DSI and SciBERT-DSI. Because existing scientometric measures rarely assess the originality expressed in textual content, DSI provides a valuable means of directly quantifying the conceptual originality embedded in scientific writing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07892v2</guid>
      <category>cs.DL</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jack H. Culbert, Yoed N. Kenett, Philipp Mayr</dc:creator>
    </item>
    <item>
      <title>Knowledge Independence Breeds Disruption but Limits Recognition</title>
      <link>https://arxiv.org/abs/2504.09589</link>
      <description>arXiv:2504.09589v2 Announce Type: replace-cross 
Abstract: Despite extensive research on scientific disruption, two questions remain: why disruption has declined amid growing knowledge, and why disruptive work receives fewer and delayed citations. One way to address these questions is to identify an intrinsic, paper-level property that reliably predicts disruption and explains both patterns. Here, we propose a novel measure, knowledge independence, capturing the extent to which a paper draws on references that do not cite one another. Analyzing 114 million publications, we find that knowledge independence strongly predicts disruption and mediates the disruptive advantage of small, onsite, and fresh teams. Its long-term decline, nonreproducible by null models, provides a mechanistic explanation for the parallel decline in disruption. Causal and simulation evidence further indicates that knowledge independence drives the persistent trade-off between disruption and impact. Taken together, these findings fill a critical gap in understanding scientific innovation, revealing a universal law: Knowledge independence breeds disruption but limits recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09589v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.DL</category>
      <category>cs.SI</category>
      <pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyao Yu, Talal Rahwan, Tao Jia</dc:creator>
    </item>
  </channel>
</rss>
