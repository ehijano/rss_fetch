<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.med-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.med-ph</link>
    <description>physics.med-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.med-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Nov 2024 04:03:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>STIED: A deep learning model for the SpatioTemporal detection of focal Interictal Epileptiform Discharges with MEG</title>
      <link>https://arxiv.org/abs/2410.23386</link>
      <description>arXiv:2410.23386v1 Announce Type: new 
Abstract: Magnetoencephalography (MEG) allows the non-invasive detection of interictal epileptiform discharges (IEDs). Clinical MEG analysis in epileptic patients traditionally relies on the visual identification of IEDs, which is time consuming and partially subjective. Automatic, data-driven detection methods exist but show limited performance. Still, the rise of deep learning (DL)-with its ability to reproduce human-like abilities-could revolutionize clinical MEG practice. Here, we developed and validated STIED, a simple yet powerful supervised DL algorithm combining two convolutional neural networks with temporal (1D time-course) and spatial (2D topography) features of MEG signals inspired from current clinical guidelines. Our DL model enabled both temporal and spatial localization of IEDs in patients suffering from focal epilepsy with frequent and high amplitude spikes (FE group), with high-performance metrics-accuracy, specificity, and sensitivity all exceeding 85%-when learning from spatiotemporal features of IEDs. This performance can be attributed to our handling of input data, which mimics established clinical MEG practice. Reverse engineering further revealed that STIED encodes fine spatiotemporal features of IEDs rather than their mere amplitude. The model trained on the FE group also showed promising results when applied to a separate group of presurgical patients with different types of refractory focal epilepsy, though further work is needed to distinguish IEDs from physiological transients. This study paves the way of incorporating STIED and DL algorithms into the routine clinical MEG evaluation of epilepsy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23386v1</guid>
      <category>physics.med-ph</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Raquel Fern\'andez-Mart\'in, Alfonso Gij\'on, Odile Feys, Elodie Juven\'e, Alec Aeby, Charline Urbain, Xavier De Ti\`ege, Vincent Wens</dc:creator>
    </item>
    <item>
      <title>Patient-Specific CBCT Synthesis for Real-time Tumor Tracking in Surface-guided Radiotherapy</title>
      <link>https://arxiv.org/abs/2410.23582</link>
      <description>arXiv:2410.23582v1 Announce Type: new 
Abstract: We present a new imaging system to support real-time tumor tracking for surface-guided radiotherapy (SGRT). SGRT uses optical surface imaging (OSI) to acquire real-time surface topography images of the patient on the treatment couch. However, OSI cannot visualize internal anatomy. This study proposes an Advanced Surface Imaging (A-SI) framework to address this issue. In the proposed A-SI framework, a high-speed surface imaging camera consistently captures surface images during radiation delivery, and a CBCT imager captures single-angle X-ray projections at low frequency. The A-SI then utilizes a generative model to generate real-time volumetric images with full anatomy, referred to as Optical Surface-Derived cone beam computed tomography (OSD-CBCT), based on the real-time high-frequent surface images and the low-frequency collected single-angle X-ray projections. The generated OSD-CBCT can provide accurate tumor motion for precise radiation delivery. The A-SI framework uses a patient-specific generative model: physics-integrated consistency-refinement denoising diffusion probabilistic model (PC-DDPM). This model leverages patient-specific anatomical structures and respiratory motion patterns derived from four-dimensional CT (4DCT) during treatment planning. It then employs a geometric transformation module (GTM) to extract volumetric anatomy information from the single-angle X-ray projection. A simulation study with 22 lung cancer patients evaluated the A-SI framework supported by PC-DDPM. The results showed that the framework produced real-time OSD-CBCT with high reconstruction fidelity and precise tumor localization. This study demonstrates the potential of A-SI to enable real-time tumor tracking with minimal imaging dose, advancing SGRT for motion-associated cancers and interventional procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23582v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaoyan Pan, Vanessa Su, Junbo Peng, Junyuan Li, Yuan Gao, Chih-Wei Chang, Tonghe Wang, Tian Zhen, Xiaofeng Yang</dc:creator>
    </item>
    <item>
      <title>Impact of normal lung volume choices on radiation pneumonitis risk prediction in locally advanced NSCLC radiotherapy</title>
      <link>https://arxiv.org/abs/2410.24120</link>
      <description>arXiv:2410.24120v1 Announce Type: new 
Abstract: This study is to evaluate the impact of different normal lung volumes on radiation pneumonitis (RP) prediction in patients with locally advanced non-small-cell-lung-cancer (LA-NSCLC) receiving radiotherapy. Three dosimetric variables (V20, V5, and mean lung dose (MLD)) were calculated using treatment plans from 442 patients with LA-NSCLC enrolled in NRG Oncology RTOG 0617. Three lung volumes were defined based on the treatment plan: total lung excluding gross-tumor-target (TL-GTV), total lung excluding clinical-target-volume (TL-CTV), and total lung excluding planning-target-volume (TL-PTV). Binary classification was used for clinical endpoints: no-RP2 (N = 377) for patients with acute RP grades 0 or 1, and RP2 (N = 65) for patients with acute grades 2 or higher. The impact of lung volume definition on RP prediction was investigated with statistical analyses and two supervised machine learning (ML) models: logistic regression (LR) and random forest (RF). Balanced data was generated for comparison to prediction obtained with the imbalanced data. Areas under curve (AUCs) and the Shapley Additive eXplanations (SHAP) were used to examine ML performance and explain the contribution of each feature to the prediction, respectively. Statistical analyses revealed that V20 and MLD were associated with RP but no difference among different lung volume definitions. Mean AUC values from 10-fold cross validation using imbalanced and balanced datasets were &lt; 0.6 except that RF from the latter dataset yielded AUC values &gt; 0.7. SHAP values indicated that MLD and V20 were the most prominent features in RP prediction. In all assessments, no difference among various volume definitions was found. Different lung volumes showed insignificant impact on RP prediction. Low AUC values suggest limited effectiveness of these dosimetric variables in predicting RP risk and more powerful predictors are needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24120v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alyssa Gadsby, Tian Liu, Robert Samstein, Jiahan Zhang, Yang Lei, Kenneth E. Rosenzweig, Ming Chao</dc:creator>
    </item>
    <item>
      <title>Variable Resolution Sampling and Deep Learning Image Recovery for Accelerated Multi-Spectral MRI Near Metal Implants</title>
      <link>https://arxiv.org/abs/2410.23329</link>
      <description>arXiv:2410.23329v1 Announce Type: cross 
Abstract: Purpose: This study presents a variable resolution (VR) sampling and deep learning reconstruction approach for multi-spectral MRI near metal implants, aiming to reduce scan times while maintaining image quality. Background: The rising use of metal implants has increased MRI scans affected by metal artifacts. Multi-spectral imaging (MSI) reduces these artifacts but sacrifices acquisition efficiency. Methods: This retrospective study on 1.5T MSI knee and hip data from patients with metal hardware used a novel spectral undersampling scheme to improve acquisition efficiency by ~40%. U-Net-based deep learning models were trained for reconstruction. Image quality was evaluated using SSIM, PSNR, and RESI metrics. Results: Deep learning reconstructions of undersampled VR data (DL-VR) showed significantly higher SSIM and PSNR values (p&lt;0.001) compared to conventional reconstruction (CR-VR), with improved edge sharpness. Edge sharpness in DL-reconstructed images matched fully sampled references (p=0.5). Conclusion: This approach can potentially enhance MRI examinations near metal implants by reducing scan times or enabling higher resolution. Further prospective studies are needed to assess clinical value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23329v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Azadeh Sharafi, Nikolai J. Mickevicius, Mehran Baboli, Andrew S. Nencka, Kevin M. Koch</dc:creator>
    </item>
    <item>
      <title>Plug-and-play superiorization</title>
      <link>https://arxiv.org/abs/2410.23401</link>
      <description>arXiv:2410.23401v1 Announce Type: cross 
Abstract: The superiorization methodology (SM) is an optimization heuristic in which an iterative algorithm, which aims to solve a particular problem, is ``superiorized'' to promote solutions that are improved with respect to some secondary criterion. This superiorization is achieved by perturbing iterates of the algorithm in nonascending directions of a prescribed function that penalizes undesirable characteristics in the solution; the solution produced by the superiorized algorithm should therefore be improved with respect to the value of this function. In this paper, we broaden the SM to allow for the perturbations to be introduced by an arbitrary procedure instead, using a plug-and-play approach. This allows for operations such as image denoisers or deep neural networks, which have applications to a broad class of problems, to be incorporated within the superiorization methodology. As proof of concept, we perform numerical simulations involving low-dose and sparse-view computed tomography image reconstruction, comparing the plug-and-play approach to a conventionally superiorized algorithm, as well as a post-processing approach. The plug-and-play approach provides comparable or better image quality in most cases, while also providing advantages in terms of computing time, and data fidelity of the solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23401v1</guid>
      <category>math.OC</category>
      <category>physics.med-ph</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jon Henshaw, Aviv Gibali, Thomas Humphries</dc:creator>
    </item>
    <item>
      <title>Cycle-Constrained Adversarial Denoising Convolutional Network for PET Image Denoising: Multi-Dimensional Validation on Large Datasets with Reader Study and Real Low-Dose Data</title>
      <link>https://arxiv.org/abs/2410.23628</link>
      <description>arXiv:2410.23628v1 Announce Type: cross 
Abstract: Positron emission tomography (PET) is a critical tool for diagnosing tumors and neurological disorders but poses radiation risks to patients, particularly to sensitive populations. While reducing injected radiation dose mitigates this risk, it often compromises image quality. To reconstruct full-dose-quality images from low-dose scans, we propose a Cycle-constrained Adversarial Denoising Convolutional Network (Cycle-DCN). This model integrates a noise predictor, two discriminators, and a consistency network, and is optimized using a combination of supervised loss, adversarial loss, cycle consistency loss, identity loss, and neighboring Structural Similarity Index (SSIM) loss. Experiments were conducted on a large dataset consisting of raw PET brain data from 1,224 patients, acquired using a Siemens Biograph Vision PET/CT scanner. Each patient underwent a 120-seconds brain scan. To simulate low-dose PET conditions, images were reconstructed from shortened scan durations of 30, 12, and 5 seconds, corresponding to 1/4, 1/10, and 1/24 of the full-dose acquisition, respectively, using a custom-developed GPU-based image reconstruction software. The results show that Cycle-DCN significantly improves average Peak Signal-to-Noise Ratio (PSNR), SSIM, and Normalized Root Mean Square Error (NRMSE) across three dose levels, with improvements of up to 56%, 35%, and 71%, respectively. Additionally, it achieves contrast-to-noise ratio (CNR) and Edge Preservation Index (EPI) values that closely align with full-dose images, effectively preserving image details, tumor shape, and contrast, while resolving issues with blurred edges. The results of reader studies indicated that the images restored by Cycle-DCN consistently received the highest ratings from nuclear medicine physicians, highlighting their strong clinical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23628v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yucun Hou, Fenglin Zhan, Xin Cheng, Chenxi Li, Ziquan Yuan, Runze Liao, Haihao Wang, Jianlang Hua, Jing Wu, Jianyong Jiang</dc:creator>
    </item>
    <item>
      <title>Diffuse laser illumination for Maxwellian view Doppler holography of the retina</title>
      <link>https://arxiv.org/abs/2212.13347</link>
      <description>arXiv:2212.13347v2 Announce Type: replace-cross 
Abstract: We present the benefits of using diffuse illumination in laser holography for ophthalmic applications. Integrating a diffusing element introduces angular diversity in the optical radiation and reduces spatial coherence, effectively distributing the illumination beam's energy across the focal plane of the eyepiece. This configuration allows for an expanded field of view in digitally computed retinal images, as the eyepiece can be positioned closer to the cornea to achieve a Maxwellian view of the retina without compromising ocular safety. By avoiding the formation of a laser hot spot near the cornea, diffuse illumination facilitates easier compliance with American and European safety standards for ophthalmic devices. Importantly, this approach does not introduce any adverse effects on digitally computed Doppler images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.13347v2</guid>
      <category>physics.optics</category>
      <category>physics.app-ph</category>
      <category>physics.med-ph</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zofia Bratasz, Olivier Martinache, Yohan Blazy, Ang\`ele Denis, Coline Auffret, Jean-Pierre Huignard, Ethan Rossi, Jay Chhablani, Jos\'e-Alain Sahel, Sophie Bonnin, Rabih Hage, Patricia Koskas, Damien Gatinel, Catherine Vignal, Amelie Yavchitz, Vivien Vasseur, Ramin Tadayoni, Claire Ducloux, Manon Ortoli, Marvin Tordjman, Sarah Tick, Sarah Mrejen, Michel Paques, Michael Atlan</dc:creator>
    </item>
    <item>
      <title>Physics-Regularized Multi-Modal Image Assimilation for Brain Tumor Localization</title>
      <link>https://arxiv.org/abs/2409.20409</link>
      <description>arXiv:2409.20409v3 Announce Type: replace-cross 
Abstract: Physical models in the form of partial differential equations serve as important priors for many under-constrained problems. One such application is tumor treatment planning, which relies on accurately estimating the spatial distribution of tumor cells within a patient's anatomy. While medical imaging can detect the bulk of a tumor, it cannot capture the full extent of its spread, as low-concentration tumor cells often remain undetectable, particularly in glioblastoma, the most common primary brain tumor. Machine learning approaches struggle to estimate the complete tumor cell distribution due to a lack of appropriate training data. Consequently, most existing methods rely on physics-based simulations to generate anatomically and physiologically plausible estimations. However, these approaches face challenges with complex and unknown initial conditions and are constrained by overly rigid physical models. In this work, we introduce a novel method that integrates data-driven and physics-based cost functions, akin to Physics-Informed Neural Networks (PINNs). However, our approach parametrizes the solution directly on a dynamic discrete mesh, allowing for the effective modeling of complex biomechanical behaviors. Specifically, we propose a unique discretization scheme that quantifies how well the learned spatiotemporal distributions of tumor and brain tissues adhere to their respective growth and elasticity equations. This quantification acts as a regularization term, offering greater flexibility and improved integration of patient data compared to existing models. We demonstrate enhanced coverage of tumor recurrence areas using real-world data from a patient cohort, highlighting the potential of our method to improve model-driven treatment planning for glioblastoma in clinical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20409v3</guid>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Fri, 01 Nov 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michal Balcerak, Tamaz Amiranashvili, Andreas Wagner, Jonas Weidner, Petr Karnakov, Johannes C. Paetzold, Ivan Ezhov, Petros Koumoutsakos, Benedikt Wiestler, Bjoern Menze</dc:creator>
    </item>
  </channel>
</rss>
