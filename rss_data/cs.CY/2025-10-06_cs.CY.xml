<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Oct 2025 02:45:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>PHORECAST: Enabling AI Understanding of Public Health Outreach Across Populations</title>
      <link>https://arxiv.org/abs/2510.02535</link>
      <description>arXiv:2510.02535v1 Announce Type: new 
Abstract: Understanding how diverse individuals and communities respond to persuasive messaging holds significant potential for advancing personalized and socially aware machine learning. While Large Vision and Language Models (VLMs) offer promise, their ability to emulate nuanced, heterogeneous human responses, particularly in high stakes domains like public health, remains underexplored due in part to the lack of comprehensive, multimodal dataset. We introduce PHORECAST (Public Health Outreach REceptivity and CAmpaign Signal Tracking), a multimodal dataset curated to enable fine-grained prediction of both individuallevel behavioral responses and community-wide engagement patterns to health messaging. This dataset supports tasks in multimodal understanding, response prediction, personalization, and social forecasting, allowing rigorous evaluation of how well modern AI systems can emulate, interpret, and anticipate heterogeneous public sentiment and behavior. By providing a new dataset to enable AI advances for public health, PHORECAST aims to catalyze the development of models that are not only more socially aware but also aligned with the goals of adaptive and inclusive health communication</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02535v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rifaa Qadri, Anh Nhat Nhu, Swati Ramnath, Laura Yu Zheng, Raj Bhansali, Sylvette La Touche-Howard, Tracy Marie Zeeger, Tom Goldstein, Ming Lin</dc:creator>
    </item>
    <item>
      <title>Strengthening legal protection against discrimination by algorithms and artificial intelligence</title>
      <link>https://arxiv.org/abs/2510.02859</link>
      <description>arXiv:2510.02859v1 Announce Type: new 
Abstract: Algorithmic decision-making and other types of artificial intelligence (AI) can be used to predict who will commit crime, who will be a good employee, who will default on a loan, etc. However, algorithmic decision-making can also threaten human rights, such as the right to non-discrimination. The paper evaluates current legal protection in Europe against discriminatory algorithmic decisions. The paper shows that non-discrimination law, in particular through the concept of indirect discrimination, prohibits many types of algorithmic discrimination. Data protection law could also help to defend people against discrimination. Proper enforcement of non-discrimination law and data protection law could help to protect people. However, the paper shows that both legal instruments have severe weaknesses when applied to artificial intelligence. The paper suggests how enforcement of current rules can be improved. The paper also explores whether additional rules are needed. The paper argues for sector-specific - rather than general - rules, and outlines an approach to regulate algorithmic decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02859v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/13642987.2020.1743976</arxiv:DOI>
      <arxiv:journal_reference>The International Journal of Human Rights 2020, 24(10), 1572-1593</arxiv:journal_reference>
      <dc:creator>Frederik J. Zuiderveen Borgesius</dc:creator>
    </item>
    <item>
      <title>The regulation of online political micro-targeting in Europe</title>
      <link>https://arxiv.org/abs/2510.02860</link>
      <description>arXiv:2510.02860v1 Announce Type: new 
Abstract: In this paper, we examine how online political micro-targeting is regulated in Europe. While there are no specific rules on such micro-targeting, there are general rules that apply. We focus on three fields of law: data protection law, freedom of expression, and sector-specific rules for political advertising; for the latter we examine four countries. We argue that the rules in the General Data Protection Regulation (GDPR) are necessary, but not sufficient. We show that political advertising, including online political micro-targeting, is protected by the right to freedom of expression. That right is not absolute, however. From a European human rights perspective, it is possible for lawmakers to limit the possibilities for political advertising. Indeed, some countries ban TV advertising for political parties during elections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02860v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.14763/2019.4.1440</arxiv:DOI>
      <arxiv:journal_reference>Internet Policy Review, 8(4), 2019</arxiv:journal_reference>
      <dc:creator>Tom Dobber, Ronan \'O Fathaigh, Frederik J. Zuiderveen Borgesius</dc:creator>
    </item>
    <item>
      <title>The European Union general data protection regulation: what it is and what it means</title>
      <link>https://arxiv.org/abs/2510.02861</link>
      <description>arXiv:2510.02861v1 Announce Type: new 
Abstract: This paper introduces the strategic approach to regulating personal data and the normative foundations of the European Union's General Data Protection Regulation ('GDPR'). We explain the genesis of the GDPR, which is best understood as an extension and refinement of existing requirements imposed by the 1995 Data Protection Directive; describe the GDPR's approach and provisions; and make predictions about the GDPR's implications. We also highlight where the GDPR takes a different approach than U.S. privacy law. The GDPR is the most consequential regulatory development in information policy in a generation. The GDPR brings personal data into a detailed regulatory regime, that will influence personal data usage worldwide. Understood properly, the GDPR encourages firms to develop information governance frameworks, to in-house data use, and to keep humans in the loop in decision making. Companies with direct relationships with consumers have strategic advantages under the GDPR, compared to third party advertising firms on the internet. To reach these objectives, the GDPR uses big sticks, structural elements that make proving violations easier, but only a few carrots. The GDPR will complicate and restrain some information-intensive business models. But the GDPR will also enable approaches previously impossible under less-protective approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02861v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/13600834.2019.1573501</arxiv:DOI>
      <arxiv:journal_reference>Information &amp; Communications Technology Law 2019, vol. 28, no. 1, 65-98</arxiv:journal_reference>
      <dc:creator>Chris Jay Hoofnagle, Bart van der Sloot, Frederik Zuiderveen Borgesius</dc:creator>
    </item>
    <item>
      <title>Representing Beauty: Towards a Participatory but Objective Latent Aesthetics</title>
      <link>https://arxiv.org/abs/2510.02869</link>
      <description>arXiv:2510.02869v1 Announce Type: new 
Abstract: What does it mean for a machine to recognize beauty? While beauty remains a culturally and experientially compelling but philosophically elusive concept, deep learning systems increasingly appear capable of modeling aesthetic judgment. In this paper, we explore the capacity of neural networks to represent beauty despite the immense formal diversity of objects for which the term applies. By drawing on recent work on cross-model representational convergence, we show how aesthetic content produces more similar and aligned representations between models which have been trained on distinct data and modalities - while unaesthetic images do not produce more aligned representations. This finding implies that the formal structure of beautiful images has a realist basis - rather than only as a reflection of socially constructed values. Furthermore, we propose that these realist representations exist because of a joint grounding of aesthetic form in physical and cultural substance. We argue that human perceptual and creative acts play a central role in shaping these the latent spaces of deep learning systems, but that a realist basis for aesthetics shows that machines are not mere creative parrots but can produce novel creative insights from the unique vantage point of scale. Our findings suggest that human-machine co-creation is not merely possible, but foundational - with beauty serving as a teleological attractor in both cultural production and machine perception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02869v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Michael Rusnak</dc:creator>
    </item>
    <item>
      <title>Assessment Twins: A Protocol for AI-Vulnerable Summative Assessment</title>
      <link>https://arxiv.org/abs/2510.02929</link>
      <description>arXiv:2510.02929v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) is reshaping higher education and raising pressing concerns about the integrity and validity of higher education assessment. While assessment redesign is increasingly seen as a necessity, there is a relative lack of literature detailing what such redesign may entail. In this paper, we introduce assessment twins as an accessible approach for redesigning assessment tasks to enhance validity. We use Messick's unified validity framework to systematically map the ways in which GenAI threaten content, structural, consequential, generalisability, and external validity. Following this, we define assessment twins as two deliberately linked components that address the same learning outcomes through different modes of evidence, scheduled closely together to allow for cross-verification and assurance of learning.
  We argue that the twin approach helps mitigate validity threats by triangulating evidence across complementary formats, such as pairing essays with oral defences, group discussions, or practical demonstrations. We highlight several advantages: preservation of established assessment formats, reduction of reliance on surveillance technologies, and flexible use across cohort sizes. To guide implementation, we propose a three-step design process: identifying vulnerabilities, aligning outcomes, selecting complementary tasks, and developing interdependent marking schemes. We also acknowledge the challenges, including resource intensity, equity concerns, and the need for empirical validation. Nonetheless, we contend that assessment twins represent a validity-focused response to GenAI that prioritises pedagogy while supporting meaningful student learning outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02929v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jasper Roe (Durham University, UK), Mike Perkins (British University Vietnam), Louie Giray (Mapua University, Philippines)</dc:creator>
    </item>
    <item>
      <title>Corrosion Risk Estimation for Heritage Preservation: An Internet of Things and Machine Learning Approach Using Temperature and Humidity</title>
      <link>https://arxiv.org/abs/2510.02973</link>
      <description>arXiv:2510.02973v1 Announce Type: new 
Abstract: Proactive preservation of steel structures at culturally significant heritage sites like the San Sebastian Basilica in the Philippines requires accurate corrosion forecasting. This study developed an Internet of Things hardware system connected with LoRa wireless communications to monitor heritage buildings with steel structures. From a three year dataset generated by the IoT system, we built a machine learning framework for predicting atmospheric corrosion rates using only temperature and relative humidity data. Deployed via a Streamlit dashboard with ngrok tunneling for public access, the framework provides real-time corrosion monitoring and actionable preservation recommendations. This minimal-data approach is scalable and cost effective for heritage sites with limited monitoring resources, showing that advanced regression can extract accurate corrosion predictions from basic meteorological data enabling proactive preservation of culturally significant structures worldwide without requiring extensive sensor networks</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02973v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reginald Juan M. Mercado, Muhammad Kabeer, Haider Al-Obaidy, Rosdiadee Nordin</dc:creator>
    </item>
    <item>
      <title>AI Generated Child Sexual Abuse Material -- What's the Harm?</title>
      <link>https://arxiv.org/abs/2510.02978</link>
      <description>arXiv:2510.02978v1 Announce Type: new 
Abstract: The development of generative artificial intelligence (AI) tools capable of producing wholly or partially synthetic child sexual abuse material (AI CSAM) presents profound challenges for child protection, law enforcement, and societal responses to child exploitation. While some argue that the harmfulness of AI CSAM differs fundamentally from other CSAM due to a perceived absence of direct victimization, this perspective fails to account for the range of risks associated with its production and consumption. AI has been implicated in the creation of synthetic CSAM of children who have not previously been abused, the revictimization of known survivors of abuse, the facilitation of grooming, coercion and sexual extortion, and the normalization of child sexual exploitation. Additionally, AI CSAM may serve as a new or enhanced pathway into offending by lowering barriers to engagement, desensitizing users to progressively extreme content, and undermining protective factors for individuals with a sexual interest in children. This paper provides a primer on some key technologies, critically examines the harms associated with AI CSAM, and cautions against claims that it may function as a harm reduction tool, emphasizing how some appeals to harmlessness obscure its real risks and may contribute to inertia in ecosystem responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02978v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caoilte \'O Ciardha, John Buckley, Rebecca S. Portnoff</dc:creator>
    </item>
    <item>
      <title>Sensors in viticulture: functions, benefits, and data-driven insights</title>
      <link>https://arxiv.org/abs/2510.03000</link>
      <description>arXiv:2510.03000v2 Announce Type: new 
Abstract: Use of sensors and related analytical predictions can be a powerful tool in providing data-informed input to viticulturalists' decision process, complementing their vineyard observations and intuition. Their up-to-date measurements, predictions, and alerts offer actionable insights and suggestions for managing key vineyard operations, such as irrigation, disease and pest control, canopy management, and harvest timing. In many cases, anticipatory interventions can mitigate risks before problems become apparent. By offering guidance on the targeting, timing, and dosage of vineyard practices, sensor data platforms can enhance operational effectiveness and efficiency while conserving labor and resources when they are not required. They also enable implementation of the principles of precision viticulture - doing the right thing, at the right time, in the right place. This paper provides a succinct summary of the functions, benefits, and practical considerations of sensor data platforms in viticulture. It may be of interest to viticulturalists as well as agricultural and IoT researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03000v2</guid>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milan Milenkovic</dc:creator>
    </item>
    <item>
      <title>Excited, Skeptical, or Worried? A Multi-Institutional Study of Student Views on Generative AI in Computing Education</title>
      <link>https://arxiv.org/abs/2510.03107</link>
      <description>arXiv:2510.03107v1 Announce Type: new 
Abstract: The application of Artificial Intelligence, in particular Generative AI, has become more widespread among educational institutions. Opinions vary widely on whether integrating AI into classrooms is the way forward or if it is detrimental to the quality of education. Increasingly, research studies are giving us more insight into the consequences of using AI tools in learning and teaching. Studies have shown how, when, and why students use AI tools. Because developments regarding the technology and its use are moving fast, we need frequent, ongoing, and more fine-grained investigation. One aspect that we do not know much about yet is how students use and think about AI across \textit{different types of education}. In this paper, we present the results of a multi-institutional survey with responses from 410 students enrolled in the computing programs of 23 educational institutions, representing high schools, colleges, and research universities. We found distinct usage patterns across the three educational institution types. Students from all types express excitement, optimism, and gratitude toward GenAI. Students in higher education more often report worry and skepticism, while high school students report greater trust and fewer negative feelings. Additionally, the AI hype has had a minimal influence, positive or negative, on high school students' decision to pursue computing. Our study contributes to a better understanding of inter-institutional differences in AI usage and perception and can help educators and students better prepare for future challenges related to AI in computing education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03107v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3769994.3770008</arxiv:DOI>
      <dc:creator>Isaac Alpizar-Chacon, Hieke Keuning, Imke de Jong, Ioanna Lykourentzou, Susan Rings</dc:creator>
    </item>
    <item>
      <title>A computational framework for quantifying route diversification in road networks</title>
      <link>https://arxiv.org/abs/2510.02582</link>
      <description>arXiv:2510.02582v1 Announce Type: cross 
Abstract: The structure of road networks impacts various urban dynamics, from traffic congestion to environmental sustainability and access to essential services. Recent studies reveal that most roads are underutilized, faster alternative routes are often overlooked, and traffic is typically concentrated on a few corridors. In this article, we examine how road network structure, and in particular the presence of mobility attractors (e.g., highways and ring roads), shapes the counterpart to traffic concentration: route diversification. To this end, we introduce DiverCity, a measure that quantifies the extent to which traffic can potentially be distributed across multiple, loosely overlapping near-shortest routes. Analyzing 56 diverse global cities, we find that DiverCity is influenced by network characteristics and is associated with traffic efficiency. Within cities, DiverCity increases with distance from the city center before stabilizing in the periphery, but declines in the proximity of mobility attractors. We demonstrate that strategic speed limit adjustments on mobility attractors can increase DiverCity while preserving travel efficiency. We isolate the complex interplay between mobility attractors and DiverCity through simulations in a controlled setting, confirming the patterns observed in real-world cities. DiverCity provides a practical tool for urban planners and policymakers to optimize road network design and balance route diversification, efficiency, and sustainability. We provide an interactive platform (https://divercitymaps.github.io) to visualize the spatial distribution of DiverCity across all considered cities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02582v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuliano Cornacchia, Luca Pappalardo, Mirco Nanni, Dino Pedreschi, Marta C. Gonz\'alez</dc:creator>
    </item>
    <item>
      <title>"It Felt Real" Victim Perspectives on Platform Design and Longer-Running Scams</title>
      <link>https://arxiv.org/abs/2510.02680</link>
      <description>arXiv:2510.02680v1 Announce Type: cross 
Abstract: Longer-running scams, such as romance fraud and "pig-butchering" scams, exploit not only victims' emotions but also the design of digital platforms. Scammers commonly leverage features such as professional-looking profile verification, algorithmic recommendations that reinforce contact, integrated payment systems, and private chat affordances to gradually establish trust and dependency with victims. Prior work in HCI and criminology has examined online scams through the lenses of detection mechanisms, threat modeling, and user-level vulnerabilities. However, less attention has been paid to how platform design itself enables longer-running scams. To address this gap, we conducted in-depth interviews with 25 longer-running scam victims in China. Our findings show how scammers strategically use platform affordances to stage credibility, orchestrate intimacy, and sustain coercion with victims. By analyzing scams as socio-technical projects, we highlight how platform design can be exploited in longer-running scams, and point to redesigning future platforms to better protect users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02680v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jingjia Xiao, Qing Xiao, Hong Shen</dc:creator>
    </item>
    <item>
      <title>Protecting Persona Biometric Data: The Case of Facial Privacy</title>
      <link>https://arxiv.org/abs/2510.03035</link>
      <description>arXiv:2510.03035v1 Announce Type: cross 
Abstract: The proliferation of digital technologies has led to unprecedented data collection, with facial data emerging as a particularly sensitive commodity. Companies are increasingly leveraging advanced facial recognition technologies, often without the explicit consent or awareness of individuals, to build sophisticated surveillance capabilities. This practice, fueled by weak and fragmented laws in many jurisdictions, has created a regulatory vacuum that allows for the commercialization of personal identity and poses significant threats to individual privacy and autonomy. This article introduces the concept of Facial Privacy. It analyzes the profound challenges posed by unregulated facial recognition by conducting a comprehensive review of existing legal frameworks. It examines and compares regulations such as the GDPR, Brazil's LGPD, Canada's PIPEDA, and privacy acts in China, Singapore, South Korea, and Japan, alongside sector-specific laws in the United States like the Illinois Biometric Information Privacy Act (BIPA). The analysis highlights the societal impacts of this technology, including the potential for discriminatory bias and the long-lasting harm that can result from the theft of immutable biometric data. Ultimately, the paper argues that existing legal loopholes and ambiguities leave individuals vulnerable. It proposes a new policy framework that shifts the paradigm from data as property to a model of inalienable rights, ensuring that fundamental human rights are upheld against unchecked technological expansion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03035v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lambert Hogenhout, Rinzin Wangmo</dc:creator>
    </item>
    <item>
      <title>A Study on the Framework for Evaluating the Ethics and Trustworthiness of Generative AI</title>
      <link>https://arxiv.org/abs/2509.00398</link>
      <description>arXiv:2509.00398v2 Announce Type: replace 
Abstract: This study provides an in_depth analysis of the ethical and trustworthiness challenges emerging alongside the rapid advancement of generative artificial intelligence (AI) technologies and proposes a comprehensive framework for their systematic evaluation. While generative AI, such as ChatGPT, demonstrates remarkable innovative potential, it simultaneously raises ethical and social concerns, including bias, harmfulness, copyright infringement, privacy violations, and hallucination. Current AI evaluation methodologies, which mainly focus on performance and accuracy, are insufficient to address these multifaceted issues. Thus, this study emphasizes the need for new human_centered criteria that also reflect social impact. To this end, it identifies key dimensions for evaluating the ethics and trustworthiness of generative AI_fairness, transparency, accountability, safety, privacy, accuracy, consistency, robustness, explainability, copyright and intellectual property protection, and source traceability and develops detailed indicators and assessment methodologies for each. Moreover, it provides a comparative analysis of AI ethics policies and guidelines in South Korea, the United States, the European Union, and China, deriving key approaches and implications from each. The proposed framework applies across the AI lifecycle and integrates technical assessments with multidisciplinary perspectives, thereby offering practical means to identify and manage ethical risks in real_world contexts. Ultimately, the study establishes an academic foundation for the responsible advancement of generative AI and delivers actionable insights for policymakers, developers, users, and other stakeholders, supporting the positive societal contributions of AI technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00398v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cheonsu Jeong, Seunghyun Lee, Seonhee Jeong, Sungsu Kim</dc:creator>
    </item>
    <item>
      <title>Discrimination in machine learning algorithms</title>
      <link>https://arxiv.org/abs/2207.00108</link>
      <description>arXiv:2207.00108v2 Announce Type: replace-cross 
Abstract: Machine learning algorithms are routinely used for business decisions that may directly affect individuals, for example, because a credit scoring algorithm refuses them a loan. It is then relevant from an ethical (and legal) point of view to ensure that these algorithms do not discriminate based on sensitive attributes (like sex or race), which may occur unwittingly and unknowingly by the operator and the management. Statistical tools and methods are then required to detect and eliminate such potential biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.00108v2</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Roberta Pappad\`a, Francesco Pauli</dc:creator>
    </item>
    <item>
      <title>Physical partisan proximity outweighs online ties in predicting US voting outcomes</title>
      <link>https://arxiv.org/abs/2407.12146</link>
      <description>arXiv:2407.12146v2 Announce Type: replace-cross 
Abstract: Affective polarization and increasing social divisions affect social mixing and the spread of information across online and physical spaces, reinforcing social and electoral cleavages and influencing political outcomes. Here, using individual survey data and aggregated and de-identified co-location and online network data, we investigate the relationship between partisan exposure and vote choice in the US by comparing offline and online dimensions of partisan exposure. By leveraging various statistical modeling approaches, we consistently find that partisan exposure in the physical space, as captured by co-location patterns, more accurately predicts electoral outcomes in US counties, outperforming online and residential exposures. Similarly, offline ties at the individual level better predict vote choice compared to online connections. We also estimate county-level experienced partisan segregation and examine its relationship with individuals' demographic and socioeconomic characteristics. Focusing on metropolitan areas, our results confirm the presence of extensive partisan segregation in the US and show that offline partisan isolation, both considering physical encounters or residential sorting, is higher than online segregation and is primarily associated with educational attainment. Our findings emphasize the importance of physical space in understanding the relationship between social networks and political behavior, in contrast to the intense scrutiny focused on online social networks and elections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12146v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Tonin, Bruno Lepri, Michele Tizzoni</dc:creator>
    </item>
    <item>
      <title>Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems</title>
      <link>https://arxiv.org/abs/2506.22774</link>
      <description>arXiv:2506.22774v4 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) technology epitomizes the complex challenges posed by human-made artifacts, particularly those widely integrated into society and exerting significant influence, highlighting potential benefits and their negative consequences. While other technologies may also pose substantial risks, AI's pervasive reach makes its societal effects especially profound. The complexity of AI systems, coupled with their remarkable capabilities, can lead to a reliance on technologies that operate beyond direct human oversight or understanding. To mitigate the risks that arise, several theoretical tools and guidelines have been developed, alongside efforts to create technological tools aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view of the issue but fail to provide techniques for quantifying trustworthiness. Conversely, while technological tools are better at achieving such quantification, they lack a holistic perspective, focusing instead on specific aspects of Trustworthy AI. This paper aims to introduce an assessment method that combines the ethical components of Trustworthy AI with the algorithmic processes of PageRank and TrustRank. The goal is to establish an assessment framework that minimizes the subjectivity inherent in the self-assessment techniques prevalent in the field by introducing algorithmic criteria. The application of our approach indicates that a holistic assessment of an AI system's trustworthiness can be achieved by providing quantitative insights while considering the theoretical content of relevant guidelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22774v4</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Papademas, Xenia Ziouvelou, Antonis Troumpoukis, Vangelis Karkaletsis</dc:creator>
    </item>
    <item>
      <title>Can Machines Philosophize?</title>
      <link>https://arxiv.org/abs/2507.00675</link>
      <description>arXiv:2507.00675v2 Announce Type: replace-cross 
Abstract: Inspired by the Turing test, we present a novel methodological framework to assess the extent to which a population of machines mirrors the philosophical views of a population of humans. The framework consists of three steps: (i) instructing machines to impersonate each human in the population, reflecting their backgrounds and beliefs, (ii) administering a questionnaire covering various philosophical positions to both humans and machines, and (iii) statistically analyzing the resulting responses. We apply this methodology to the debate on scientific realism, a long-standing philosophical inquiry exploring the relationship between science and reality. By considering the outcome of a survey of over 500 human participants, including both physicists and philosophers of science, we generate their machine personas using an artificial intelligence engine based on a large language model. We reveal that the philosophical views of a population of machines are, on average, similar to those endorsed by a population of humans, irrespective of whether they are physicists or philosophers of science. As compared to humans, however, machines exhibit a weaker inclination toward scientific realism and a stronger coherence in their philosophical positions. Given the observed similarities between the populations of humans and machines, this methodological framework may offer unprecedented opportunities for advancing research in the empirical social sciences by complementing human participants with their machine-impersonated counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00675v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>physics.hist-ph</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michele Pizzochero, Giorgia Dellaferrera</dc:creator>
    </item>
    <item>
      <title>DM-Bench: Benchmarking LLMs for Personalized Decision Making in Diabetes Management</title>
      <link>https://arxiv.org/abs/2510.00038</link>
      <description>arXiv:2510.00038v2 Announce Type: replace-cross 
Abstract: We present DM-Bench, the first benchmark designed to evaluate large language model (LLM) performance across real-world decision-making tasks faced by individuals managing diabetes in their daily lives. Unlike prior health benchmarks that are either generic, clinician-facing or focused on clinical tasks (e.g., diagnosis, triage), DM-Bench introduces a comprehensive evaluation framework tailored to the unique challenges of prototyping patient-facing AI solutions in diabetes, glucose management, metabolic health and related domains. Our benchmark encompasses 7 distinct task categories, reflecting the breadth of real-world questions individuals with diabetes ask, including basic glucose interpretation, educational queries, behavioral associations, advanced decision making and long term planning. Towards this end, we compile a rich dataset comprising one month of time-series data encompassing glucose traces and metrics from continuous glucose monitors (CGMs) and behavioral logs (e.g., eating and activity patterns) from 15,000 individuals across three different diabetes populations (type 1, type 2, pre-diabetes/general health and wellness). Using this data, we generate a total of 360,600 personalized, contextual questions across the 7 tasks. We evaluate model performance on these tasks across 5 metrics: accuracy, groundedness, safety, clarity and actionability. Our analysis of 8 recent LLMs reveals substantial variability across tasks and metrics; no single model consistently outperforms others across all dimensions. By establishing this benchmark, we aim to advance the reliability, safety, effectiveness and practical utility of AI solutions in diabetes care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00038v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maria Ana Cardei, Josephine Lamp, Mark Derdzinski, Karan Bhatia</dc:creator>
    </item>
  </channel>
</rss>
