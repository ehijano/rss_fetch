<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Dec 2024 05:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>NSTRI Global Collaborative Research Data Platform</title>
      <link>https://arxiv.org/abs/2412.04474</link>
      <description>arXiv:2412.04474v1 Announce Type: new 
Abstract: The National Strategic Technology Research Institute (NSTRI) Data Platform operated by Seoul National University Hospital (SNUH) addresses the challenge of accessing Korean healthcare data for international research. This platform provides secure access to pseudonymized Korean healthcare data while integrating international datasets, enabling the development of more equitable and generalizable machine learning models. The system features four key AI-powered components: an intelligent data search engine utilizing domain-specific medical embeddings, a Korean-English medical translation system, a comprehensive drug search engine, and an LLM-powered medical research assistant. The platform implements containerized environments within a secure research pod architecture, ensuring data protection while maintaining research efficiency. The platform currently provides access to 10 distinct medical datasets from SNUH, categorized by access permissions and standardized for cross-dataset analysis. This infrastructure enables global collaborative healthcare research while maintaining strict data protection standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04474v1</guid>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyeonhoon Lee, Hanseul Kim, Kyungmin Cho, Hyung-Chul Lee</dc:creator>
    </item>
    <item>
      <title>The Moral Mind(s) of Large Language Models</title>
      <link>https://arxiv.org/abs/2412.04476</link>
      <description>arXiv:2412.04476v1 Announce Type: new 
Abstract: As large language models (LLMs) become integrated to decision-making across various sectors, a key question arises: do they exhibit an emergent "moral mind" - a consistent set of moral principles guiding their ethical judgments - and is this reasoning uniform or diverse across models? To investigate this, we presented about forty different models from the main providers with a large array of structured ethical scenarios, creating one of the largest datasets of its kind. Our rationality tests revealed that at least one model from each provider demonstrated behavior consistent with stable moral principles, effectively acting as approximately optimizing a utility function encoding ethical reasoning. We identified these utility functions and observed a notable clustering of models around neutral ethical stances. To investigate variability, we introduced a novel non-parametric permutation approach, revealing that the most rational models shared 59% to 76% of their ethical reasoning patterns. Despite this shared foundation, differences emerged: roughly half displayed greater moral adaptability, bridging diverse perspectives, while the remainder adhered to more rigid ethical structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04476v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avner Seror</dc:creator>
    </item>
    <item>
      <title>Intelligent Tutors for Adult Learners: An Analysis of Needs and Challenges</title>
      <link>https://arxiv.org/abs/2412.04477</link>
      <description>arXiv:2412.04477v1 Announce Type: new 
Abstract: This paper aims to uncover needs of adult learners when using pedagogical technologies such as intelligent tutoring systems. Further, our aim with this work is to understand the usability challenges when deploying tutors at scale within the adult learning audience. As educational technologies become more ubiquitous within k-12 education, this paper aims to bridge the gap in understanding on how adult users might utilize intelligent tutors. In pursuit of this, we built four intelligent tutors, and deployed them to 110 classrooms at a state technical college for an entire academic year. Following this deployment, we conducted focus groups amongst users to gather data to understand how learners perceived the optional educational technology during their academic journey. We further analyzed this data using foundational HCI methodologies to extract leanings and design recommendations on how developers might craft educational technologies for adoption at scale for the adult learning population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04477v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adit Gupta, Momin Siddiqui, Glen Smith, Jenn Reddig, Christopher MacLellan</dc:creator>
    </item>
    <item>
      <title>NLP Cluster Analysis of Common Core State Standards and NAEP Item Specifications</title>
      <link>https://arxiv.org/abs/2412.04482</link>
      <description>arXiv:2412.04482v1 Announce Type: new 
Abstract: Camilli (2024) proposed a methodology using natural language processing (NLP) to map the relationship of a set of content standards to item specifications. This study provided evidence that NLP can be used to improve the mapping process. As part of this investigation, the nominal classifications of standards and items specifications were used to examine construct equivalence. In the current paper, we determine the strength of empirical support for the semantic distinctiveness of these classifications, which are known as "domains" for Common Core standards, and "strands" for National Assessment of Educational Progress (NAEP) item specifications. This is accomplished by separate k-means clustering for standards and specifications of their corresponding embedding vectors. We then briefly illustrate an application of these findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04482v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Gregory Camilli, Larry Suter</dc:creator>
    </item>
    <item>
      <title>AI-powered Digital Framework for Personalized Economical Quality Learning at Scale</title>
      <link>https://arxiv.org/abs/2412.04483</link>
      <description>arXiv:2412.04483v1 Announce Type: new 
Abstract: The disparity in access to quality education is significant, both between developed and developing countries and within nations, regardless of their economic status. Socioeconomic barriers and rapid changes in the job market further intensify this issue, highlighting the need for innovative solutions that can deliver quality education at scale and low cost. This paper addresses these challenges by proposing an AI-powered digital learning framework grounded in Deep Learning (DL) theory. The DL theory emphasizes learner agency and redefines the role of teachers as facilitators, making it particularly suitable for scalable educational environments. We outline eight key principles derived from learning science and AI that are essential for implementing DL-based Digital Learning Environments (DLEs). Our proposed framework leverages AI for learner modelling based on Open Learner Modeling (OLM), activity suggestions, and AI-assisted support for both learners and facilitators, fostering collaborative and engaging learning experiences. Our framework provides a promising direction for scalable, high-quality education globally, offering practical solutions to some of the AI-related challenges in education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04483v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mrzieh VatandoustMohammadieh, Mohammad Mahdi Mohajeri, Ali Keramati, Majid Nili Ahmadabadi</dc:creator>
    </item>
    <item>
      <title>The Global AI Vibrancy Tool</title>
      <link>https://arxiv.org/abs/2412.04486</link>
      <description>arXiv:2412.04486v1 Announce Type: new 
Abstract: This paper presents the latest version of the Global AI Vibrancy Tool (GVT), an interactive suite of visualizations designed to facilitate the comparison of AI vibrancy across 36 countries, using 42 indicators organized into 8 pillars. The tool offers customizable features that allow users to conduct in-depth country-level comparisons and longitudinal analyses of AI-related metrics, all based on publicly available data. By providing a transparent assessment of national progress in AI, it serves the diverse needs of policymakers, industry leaders, researchers, and the general public. Using weights for indicators and pillars developed by AI Index's panel of experts and combined into an index, the Global AI Vibrancy Ranking for 2023 places the United States first by a significant margin, followed by China and the United Kingdom. The ranking also highlights the rise of smaller nations such as Singapore when evaluated on both absolute and per capita bases. The tool offers three sub-indices for evaluating Global AI Vibrancy along different dimensions: the Innovation Index, the Economic Competitiveness Index, and the Policy, Governance, and Public Engagement Index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04486v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Loredana Fattorini, Nestor Maslej, Raymond Perrault, Vanessa Parli, John Etchemendy, Yoav Shoham, Katrina Ligett</dc:creator>
    </item>
    <item>
      <title>Optimizing Student Ability Assessment: A Hierarchy Constraint-Aware Cognitive Diagnosis Framework for Educational Contexts</title>
      <link>https://arxiv.org/abs/2412.04488</link>
      <description>arXiv:2412.04488v1 Announce Type: new 
Abstract: Cognitive diagnosis (CD) aims to reveal students' proficiency in specific knowledge concepts. With the increasing adoption of intelligent education applications, accurately assessing students' knowledge mastery has become an urgent challenge. Although existing cognitive diagnosis frameworks enhance diagnostic accuracy by analyzing students' explicit response records, they primarily focus on individual knowledge state, failing to adequately reflect the relative ability performance of students within hierarchies. To address this, we propose the Hierarchy Constraint-Aware Cognitive Diagnosis Framework (HCD), designed to more accurately represent student ability performance within real educational contexts. Specifically, the framework introduces a hierarchy mapping layer to identify students' levels. It then employs a hierarchy convolution-enhanced attention layer for in-depth analysis of knowledge concepts performance among students at the same level, uncovering nuanced differences. A hierarchy inter-sampling attention layer captures performance differences across hierarchies, offering a comprehensive understanding of the relationships among students' knowledge state. Finally, through personalized diagnostic enhancement, the framework integrates hierarchy constraint perception features with existing models, improving the representation of both individual and group characteristics. This approach enables precise inference of students' knowledge state. Research shows that this framework not only reasonably constrains changes in students' knowledge states to align with real educational settings, but also supports the scientific rigor and fairness of educational assessments, thereby advancing the field of cognitive diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04488v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinjie Sun, Qi Liu, Kai Zhang, Shuanghong Shen, Fei Wang, Yan Zhuang, Zheng Zhang, Weiyin Gong, Shijin Wang, Lina Yang, Xingying Huo</dc:creator>
    </item>
    <item>
      <title>Precarity and Solidarity: Preliminary results on a study of queer and disabled fiction writers' experiences with generative AI</title>
      <link>https://arxiv.org/abs/2412.04575</link>
      <description>arXiv:2412.04575v1 Announce Type: new 
Abstract: We have undertaken a mixed-methods study of fiction writers' experiences and attitudes with generative AI, primarily focused on the experiences of queer and disabled writers. We find that queer and disabled writers are markedly more pessimistic than non-queer and non-disabled writers about the impact of AI on their industry, although pessimism is the majority attitude for both groups. We explore ways that generative AI exacerbates existing sources of instability and precarity in the publishing industry, reasons why writers are philosophically opposed to its use, and individual and collective strategies used by marginalized fiction writers to safeguard their industry from harms associated with generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04575v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C. E. Lamb, D. G. Brown, M. R. Grossman</dc:creator>
    </item>
    <item>
      <title>Employee Well-being in the Age of AI: Perceptions, Concerns, Behaviors, and Outcomes</title>
      <link>https://arxiv.org/abs/2412.04796</link>
      <description>arXiv:2412.04796v1 Announce Type: new 
Abstract: The growing integration of Artificial Intelligence (AI) into Human Resources (HR) processes has transformed the way organizations manage recruitment, performance evaluation, and employee engagement. While AI offers numerous advantages, such as improved efficiency, reduced bias, and hyper-personalization, it raises significant concerns about employee well-being, job security, fairness, and transparency. This study examines how AI shapes employee perceptions, job satisfaction, mental health, and retention. Key findings reveal that while AI can enhance efficiency and reduce bias, it also raises concerns about job security, fairness, and privacy. Transparency in AI systems emerges as a critical factor in fostering trust and positive employee attitudes. AI systems can both support and undermine employee well-being, depending on how they are implemented and perceived. The research introduces an AI-employee well-being Interaction Framework, illustrating how AI influences employee perceptions, behaviors, and outcomes. Organizational strategies, such as clear communication, upskilling programs, and employee involvement in AI implementation, are identified as crucial for mitigating negative impacts and enhancing positive outcomes. The study concludes that the successful integration of AI in HR requires a balanced approach that prioritizes employee well-being, facilitates human-AI collaboration, and ensures ethical and transparent AI practices alongside technological advancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04796v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soheila Sadeghi</dc:creator>
    </item>
    <item>
      <title>Trust and distrust in electoral technologies: what can we learn from the failure of electronic voting in the Netherlands (2006/07)</title>
      <link>https://arxiv.org/abs/2412.05052</link>
      <description>arXiv:2412.05052v1 Announce Type: new 
Abstract: This paper focuses on the complex dynamics of trust and distrust in digital government technologies by approaching the cancellation of machine voting in the Netherlands (2006-07). This case describes how a previously trusted system can collapse, how paradoxical the relationship between trust and distrust is, and how it interacts with adopting and managing electoral technologies. The analysis stresses how, although being a central component, technology's trustworthiness dialogues with the socio-technical context in which it is inserted, for example, underscoring the relevance of public administration in securing technological environments. Beyond these insights, the research offers broader reflections on trust and distrust in data-driven technologies, advocating for differentiated strategies for building trust versus managing distrust. Overall, this paper contributes to understanding trust dynamics in digital government technologies, with implications for policymaking and technology adoption strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05052v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3657054.365726</arxiv:DOI>
      <dc:creator>David Duenas-Cid</dc:creator>
    </item>
    <item>
      <title>Sense and Sensitivity: Evaluating the simulation of social dynamics via Large Language Models</title>
      <link>https://arxiv.org/abs/2412.05093</link>
      <description>arXiv:2412.05093v1 Announce Type: new 
Abstract: Large language models have increasingly been proposed as a powerful replacement for classical agent-based models (ABMs) to simulate social dynamics. By using LLMs as a proxy for human behavior, the hope of this new approach is to be able to simulate significantly more complex dynamics than with classical ABMs and gain new insights in fields such as social science, political science, and economics. However, due to the black box nature of LLMs, it is unclear whether LLM agents actually execute the intended semantics that are encoded in their natural language instructions and, if the resulting dynamics of interactions are meaningful. To study this question, we propose a new evaluation framework that grounds LLM simulations within the dynamics of established reference models of social science. By treating LLMs as a black-box function, we evaluate their input-output behavior relative to this reference model, which allows us to evaluate detailed aspects of their behavior. Our results show that, while it is possible to engineer prompts that approximate the intended dynamics, the quality of these simulations is highly sensitive to the particular choice of prompts. Importantly, simulations are even sensitive to arbitrary variations such as minor wording changes and whitespace. This puts into question the usefulness of current versions of LLMs for meaningful simulations, as without a reference model, it is impossible to determine a priori what impact seemingly meaningless changes in prompt will have on the simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05093v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Da Ju, Adina Williams, Brian Karrer, Maximilian Nickel</dc:creator>
    </item>
    <item>
      <title>Americans' Support for AI Development -- Measured Daily with Open Data and Methods</title>
      <link>https://arxiv.org/abs/2412.05163</link>
      <description>arXiv:2412.05163v1 Announce Type: new 
Abstract: A confluence of maturing Web technologies and Web platforms affords a new form of scientific communication: free and open nowcasting of public opinion. Here, I present the first open-source system to do so. The automated system gathers new human responses to survey items daily, anonymizes and publicly distributes microdata, and presents analyses through a publicly viewable Web dashboard. A demonstration implementation tracked support for further development of artificial intelligence at daily resolution. As of 2024-11-17, the system had autonomously produced 214 daily estimates of support. I argue that more scientists should adopt the method of open nowcasting, because it encourages transparency in research design and eases replication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05163v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Jeffrey Jones</dc:creator>
    </item>
    <item>
      <title>AI's assigned gender affects human-AI cooperation</title>
      <link>https://arxiv.org/abs/2412.05214</link>
      <description>arXiv:2412.05214v1 Announce Type: new 
Abstract: Cooperation between humans and machines is increasingly vital as artificial intelligence (AI) becomes more integrated into daily life. Research indicates that people are often less willing to cooperate with AI agents than with humans, more readily exploiting AI for personal gain. While prior studies have shown that giving AI agents human-like features influences people's cooperation with them, the impact of AI's assigned gender remains underexplored. This study investigates how human cooperation varies based on gender labels assigned to AI agents with which they interact. In the Prisoner's Dilemma game, 402 participants interacted with partners labelled as AI (bot) or humans. The partners were also labelled male, female, non-binary, or gender-neutral. Results revealed that participants tended to exploit female-labelled and distrust male-labelled AI agents more than their human counterparts, reflecting gender biases similar to those in human-human interactions. These findings highlight the significance of gender biases in human-AI interactions that must be considered in future policy, design of interactive AI systems, and regulation of their use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05214v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepideh Bazazi, Jurgis Karpus, Taha Yasseri</dc:creator>
    </item>
    <item>
      <title>Large Language Models in Politics and Democracy: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2412.04498</link>
      <description>arXiv:2412.04498v1 Announce Type: cross 
Abstract: The advancement of generative AI, particularly large language models (LLMs), has a significant impact on politics and democracy, offering potential across various domains, including policymaking, political communication, analysis, and governance. This paper surveys the recent and potential applications of LLMs in politics, examining both their promises and the associated challenges. This paper examines the ways in which LLMs are being employed in legislative processes, political communication, and political analysis. Moreover, we investigate the potential of LLMs in diplomatic and national security contexts, economic and social modeling, and legal applications. While LLMs offer opportunities to enhance efficiency, inclusivity, and decision-making in political processes, they also present challenges related to bias, transparency, and accountability. The paper underscores the necessity for responsible development, ethical considerations, and governance frameworks to ensure that the integration of LLMs into politics aligns with democratic values and promotes a more just and equitable society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04498v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Goshi Aoki</dc:creator>
    </item>
    <item>
      <title>Dissociating Artificial Intelligence from Artificial Consciousness</title>
      <link>https://arxiv.org/abs/2412.04571</link>
      <description>arXiv:2412.04571v1 Announce Type: cross 
Abstract: Developments in machine learning and computing power suggest that artificial general intelligence is within reach. This raises the question of artificial consciousness: if a computer were to be functionally equivalent to a human, being able to do all we do, would it experience sights, sounds, and thoughts, as we do when we are conscious? Answering this question in a principled manner can only be done on the basis of a theory of consciousness that is grounded in phenomenology and that states the necessary and sufficient conditions for any system, evolved or engineered, to support subjective experience. Here we employ Integrated Information Theory (IIT), which provides principled tools to determine whether a system is conscious, to what degree, and the content of its experience. We consider pairs of systems constituted of simple Boolean units, one of which -- a basic stored-program computer -- simulates the other with full functional equivalence. By applying the principles of IIT, we demonstrate that (i) two systems can be functionally equivalent without being phenomenally equivalent, and (ii) that this conclusion is not dependent on the simulated system's function. We further demonstrate that, according to IIT, it is possible for a digital computer to simulate our behavior, possibly even by simulating the neurons in our brain, without replicating our experience. This contrasts sharply with computational functionalism, the thesis that performing computations of the right kind is necessary and sufficient for consciousness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04571v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Graham Findlay, William Marshall, Larissa Albantakis, Isaac David, William GP Mayner, Christof Koch, Giulio Tononi</dc:creator>
    </item>
    <item>
      <title>Show, Don't Tell: Uncovering Implicit Character Portrayal using LLMs</title>
      <link>https://arxiv.org/abs/2412.04576</link>
      <description>arXiv:2412.04576v1 Announce Type: cross 
Abstract: Tools for analyzing character portrayal in fiction are valuable for writers and literary scholars in developing and interpreting compelling stories. Existing tools, such as visualization tools for analyzing fictional characters, primarily rely on explicit textual indicators of character attributes. However, portrayal is often implicit, revealed through actions and behaviors rather than explicit statements. We address this gap by leveraging large language models (LLMs) to uncover implicit character portrayals. We start by generating a dataset for this task with greater cross-topic similarity, lexical diversity, and narrative lengths than existing narrative text corpora such as TinyStories and WritingPrompts. We then introduce LIIPA (LLMs for Inferring Implicit Portrayal for Character Analysis), a framework for prompting LLMs to uncover character portrayals. LIIPA can be configured to use various types of intermediate computation (character attribute word lists, chain-of-thought) to infer how fictional characters are portrayed in the source text. We find that LIIPA outperforms existing approaches, and is more robust to increasing character counts (number of unique persons depicted) due to its ability to utilize full narrative context. Lastly, we investigate the sensitivity of portrayal estimates to character demographics, identifying a fairness-accuracy tradeoff among methods in our LIIPA framework -- a phenomenon familiar within the algorithmic fairness literature. Despite this tradeoff, all LIIPA variants consistently outperform non-LLM baselines in both fairness and accuracy. Our work demonstrates the potential benefits of using LLMs to analyze complex characters and to better understand how implicit portrayal biases may manifest in narrative texts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04576v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brandon Jaipersaud, Zining Zhu, Frank Rudzicz, Elliot Creager</dc:creator>
    </item>
    <item>
      <title>Argumentative Experience: Reducing Confirmation Bias on Controversial Issues through LLM-Generated Multi-Persona Debates</title>
      <link>https://arxiv.org/abs/2412.04629</link>
      <description>arXiv:2412.04629v1 Announce Type: cross 
Abstract: Large language models (LLMs) are enabling designers to give life to exciting new user experiences for information access. In this work, we present a system that generates LLM personas to debate a topic of interest from different perspectives. How might information seekers use and benefit from such a system? Can centering information access around diverse viewpoints help to mitigate thorny challenges like confirmation bias in which information seekers over-trust search results matching existing beliefs? How do potential biases and hallucinations in LLMs play out alongside human users who are also fallible and possibly biased?
  Our study exposes participants to multiple viewpoints on controversial issues via a mixed-methods, within-subjects study. We use eye-tracking metrics to quantitatively assess cognitive engagement alongside qualitative feedback. Compared to a baseline search system, we see more creative interactions and diverse information-seeking with our multi-persona debate system, which more effectively reduces user confirmation bias and conviction toward their initial beliefs. Overall, our study contributes to the emerging design space of LLM-based information access systems, specifically investigating the potential of simulated personas to promote greater exposure to information diversity, emulate collective intelligence, and mitigate bias in information seeking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04629v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Li Shi, Houjiang Liu, Yian Wong, Utkarsh Mujumdar, Dan Zhang, Jacek Gwizdka, Matthew Lease</dc:creator>
    </item>
    <item>
      <title>A dynamical measure of algorithmically infused visibility</title>
      <link>https://arxiv.org/abs/2412.04735</link>
      <description>arXiv:2412.04735v1 Announce Type: cross 
Abstract: This work focuses on the nature of visibility in societies where the behaviours of humans and algorithms influence each other - termed algorithmically infused societies. We propose a quantitative measure of visibility, with implications and applications to an array of disciplines including communication studies, political science, marketing, technology design, and social media analytics. The measure captures the basic characteristics of the visibility of a given topic, in algorithm/AI-mediated communication/social media settings. Topics, when trending, are ranked against each other, and the proposed measure combines the following two attributes of a topic: (i) the amount of time a topic spends at different ranks, and (ii) the different ranks the topic attains. The proposed measure incorporates a tunable parameter, termed the discrimination level, whose value determines the relative weights of the two attributes that contribute to visibility. Analysis of a large-scale, real-time dataset of trending topics, from one of the largest social media platforms, demonstrates that the proposed measure can explain a large share of the variability of the accumulated views of a topic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04735v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaojing Sun, Zhiyuan Liu, David Waxman</dc:creator>
    </item>
    <item>
      <title>'Being there together for health': A Systematic Review on the Feasibility, Effectiveness and Design Considerations of Immersive Collaborative Virtual Environments in Health Applications</title>
      <link>https://arxiv.org/abs/2412.04760</link>
      <description>arXiv:2412.04760v1 Announce Type: cross 
Abstract: Effectively using immersive multi-user environments for digital applications (via virtual, augmented and mixed reality technologies) beckons the future of healthcare delivery in the metaverse. We aimed to evaluate the feasibility and effectiveness of these environments used in health applications, while identifying their design features.
  We systematically searched MEDLINE, PsycINFO, and Emcare databases for peer-reviewed original reports, published in English, without date restrictions until Aug 30, 2023, and conducted manual citation searching in Feb 2024. All studies using fully immersive extended reality technologies (e.g., head-mounted displays, smart glasses) while engaging more than one participant in an intervention with direct health benefits were included. A qualitative synthesis of findings is reported. The quality of research was assessed using JBI Critical Checklists. The review was pre-registered on PROSPERO (CRD42023479155).
  Of 2862 identified records, 10 studies were eligible. Included studies were mostly conducted with healthy young adults (five studies) and older adults (four studies). While they all used different models of Oculus/Meta headsets, their environments' designs were distinctive and aligned with their objectives. Findings indicated varying degrees of positive health outcomes, for engagement in rehabilitation, meaningful interactions across distances, positive affect, transformative experiences, mental health therapies, and motor skill learning. Participants reported high usability, motivation, enjoyment, presence and copresence. They also expressed the need for more training time with technology.
  Adopting an intentional intervention design, considering factors affecting presence and copresence, as well as integrating co-creation of the program with participants, seems integral to achieving positive health outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04760v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tohid Zarei (University of South Australia), Michelle Emery (University of South Australia), Dimitrios Saredakis (University of South Australia), Gun A. Lee (University of South Australia), Ben Stubbs (University of South Australia), Ancret Szpak (University of South Australia, The University of Adelaide), Tobias Loetscher (University of South Australia)</dc:creator>
    </item>
    <item>
      <title>'Debunk-It-Yourself': Health Professionals' Strategies for Responding to Misinformation on TikTok</title>
      <link>https://arxiv.org/abs/2412.04999</link>
      <description>arXiv:2412.04999v1 Announce Type: cross 
Abstract: Misinformation is "sticky" in nature, requiring a considerable effort to undo its influence. One such effort is debunking or exposing the falsity of information. As an abundance of misinformation is on social media, platforms do bear some debunking responsibility in order to preserve their trustworthiness as information providers. A subject of interpretation, platforms poorly meet this responsibility and allow dangerous health misinformation to influence many of their users. This open route to harm did not sit well with health professional users, who recently decided to take the debunking into their own hands. To study this individual debunking effort - which we call 'Debunk-It-Yourself (DIY)' - we conducted an exploratory survey n=14 health professionals who wage a misinformation counter-influence campaign through videos on TikTok. We focused on two topics, nutrition and mental health, which are the ones most often subjected to misinformation on the platform. Our thematic analysis reveals that the counterinfluence follows a common process of initiation, selection, creation, and "stitching" or duetting a debunking video with a misinformation video. The 'Debunk-It-Yourself' effort was underpinned by three unique aspects: (i) it targets trending misinformation claims perceived to be of direct harm to people's health; (ii) it offers a symmetric response to the misinformation; and (iii) it is strictly based on scientific evidence and claimed clinical experience. Contrasting the 'Debunk-It-Yourself' effort with the one TikTok and other platforms (reluctantly) put in moderation, we offer recommendations for a structured response against the misinformation's influence by the users themselves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04999v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filipo Sharevski, Jennifer Vander Loop, Amy Devine, Peter Jachim, Sanchari Das</dc:creator>
    </item>
    <item>
      <title>Prompt Transfer for Dual-Aspect Cross Domain Cognitive Diagnosis</title>
      <link>https://arxiv.org/abs/2412.05004</link>
      <description>arXiv:2412.05004v1 Announce Type: cross 
Abstract: Cognitive Diagnosis (CD) aims to evaluate students' cognitive states based on their interaction data, enabling downstream applications such as exercise recommendation and personalized learning guidance. However, existing methods often struggle with accuracy drops in cross-domain cognitive diagnosis (CDCD), a practical yet challenging task. While some efforts have explored exercise-aspect CDCD, such as crosssubject scenarios, they fail to address the broader dual-aspect nature of CDCD, encompassing both student- and exerciseaspect variations. This diversity creates significant challenges in developing a scenario-agnostic framework. To address these gaps, we propose PromptCD, a simple yet effective framework that leverages soft prompt transfer for cognitive diagnosis. PromptCD is designed to adapt seamlessly across diverse CDCD scenarios, introducing PromptCD-S for student-aspect CDCD and PromptCD-E for exercise-aspect CDCD. Extensive experiments on real-world datasets demonstrate the robustness and effectiveness of PromptCD, consistently achieving superior performance across various CDCD scenarios. Our work offers a unified and generalizable approach to CDCD, advancing both theoretical and practical understanding in this critical domain. The implementation of our framework is publicly available at https://github.com/Publisher-PromptCD/PromptCD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05004v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Liu, Yizhong Zhang, Shuochen Liu, Shengwei Ji, Kui Yu, Le Wu</dc:creator>
    </item>
    <item>
      <title>Metamemory: Exploring the Resilience of Older Internal Migrants</title>
      <link>https://arxiv.org/abs/2412.05119</link>
      <description>arXiv:2412.05119v1 Announce Type: cross 
Abstract: Immigration and aging have always been significant topics of discussion in society, concerning the stability and future development of a country and its people. Research in the field of HCI on immigration and aging has primarily focused on their practical needs but has paid less attention to the adaptability issues of older internal migrants moving with their families. In this study, we investigate the challenges older internal migrants face in adapting socially, using metadata surveys and semi-structured interviews to delve into their life struggles and resilience sources. Our findings highlight the older internal migrants' remarkable resilience, particularly evident in their reminiscences. We explore the integration of reminiscences with the metaverse, identifying the necessary conditions to create a "Metamemory". We introduce a novel design for a metaverse scene that bridges past and present experiences. This aims to encourage discussions on enhancing older internal migrants' reminiscence, leveraging the metaverse's positive potential, and devising strategies to more effectively address older internal migrants' concerns in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05119v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>CHI'2024: Workshop of the CHI Conference on Human Factors in Computing Systems, May 11--16, 2024, Honolulu, HI, USA</arxiv:journal_reference>
      <dc:creator>Xiaoxiao Wang, Jingjing Zhang, Huize Wan, Weiwei Zhang, Yuan Yao</dc:creator>
    </item>
    <item>
      <title>A Parametric, Second-Order Cone Representable Model of Fairness for Decision-Making Problems</title>
      <link>https://arxiv.org/abs/2412.05143</link>
      <description>arXiv:2412.05143v1 Announce Type: cross 
Abstract: The article develops a parametric model of fairness called "$\varepsilon$-fairness" that can be represented using a single second-order cone constraint and incorporated into existing decision-making problem formulations without impacting the complexity of solution techniques. We develop the model from the fundamental result of finite-dimensional norm equivalence in linear algebra and show that this model has a closed-form relationship to an existing metric for measuring fairness widely used in the literature. Finally, a simple case study on the optimal operation of a damaged power transmission network illustrates its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05143v1</guid>
      <category>math.OC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaarthik Sundar, Deepjyoti Deka, Russell Bent</dc:creator>
    </item>
    <item>
      <title>Exploring the Use of Drones for Taking Accessible Selfies with Elderly</title>
      <link>https://arxiv.org/abs/2412.05147</link>
      <description>arXiv:2412.05147v1 Announce Type: cross 
Abstract: Selfie taking is a popular social pastime, and is an important part of socialising online. This activity is popular with young people but is also becoming more prevalent with older generations. Despite this, there are a number of accessibility issues when taking selfies. In this research, we investigate preferences from elderly citizens when taking a selfie, to understand the current challenges. As a potential solution to address the challenges identified, we propose the use of drones and present a novel concept for hands free selfie taking. With this work, we hope to trigger conversation around how such a technology can be utilised to enable elderly citizens, and more broadly people with physical disabilities, the ability to easily take part in this social pastime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05147v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuan Yao, Weiwei Zhang, Soojeong Yoo, Callum Parker, Jihong Jeung</dc:creator>
    </item>
    <item>
      <title>Who Sets the Agenda on Social Media? Ideology and Polarization in Online Debates</title>
      <link>https://arxiv.org/abs/2412.05176</link>
      <description>arXiv:2412.05176v1 Announce Type: cross 
Abstract: The abundance of information on social media has reshaped public discussions, shifting attention to the mechanisms that drive online discourse. This study analyzes large-scale Twitter (now X) data from three global debates -- Climate Change, COVID-19, and the Russo-Ukrainian War -- to investigate the structural dynamics of engagement. Our findings reveal that discussions are not primarily shaped by specific categories of actors, such as media or activists, but by shared ideological alignment. Users consistently form polarized communities, where their ideological stance in one debate predicts their positions in others. This polarization transcends individual topics, reflecting a broader pattern of ideological divides. Furthermore, the influence of individual actors within these communities appears secondary to the reinforcing effects of selective exposure and shared narratives. Overall, our results underscore that ideological alignment, rather than actor prominence, plays a central role in structuring online discourse and shaping the spread of information in polarized environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05176v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Loru, Alessandro Galeazzi, Anita Bonetti, Emanuele Sangiorgio, Niccol\`o Di Marco, Matteo Cinelli, Andrea Baronchelli, Walter Quattrociocchi</dc:creator>
    </item>
    <item>
      <title>Beyond case studies: Teaching data science critique and ethics through sociotechnical surveillance studies</title>
      <link>https://arxiv.org/abs/2305.02420</link>
      <description>arXiv:2305.02420v2 Announce Type: replace 
Abstract: Ethics have become an urgent concern for data science research, practice, and instruction in the wake of growing critique of algorithms and systems showing that they reinforce structural oppression. There has been increasing desire on the part of data science educators to craft curricula that speak to these critiques, yet much ethics education remains individualized, focused on specific cases, or too abstract and unapplicable. We synthesized some of the most popular critical data science works and designed a data science ethics course that spoke to the social phenomena at the root of critical data studies -- theories of oppression, social systems, power, history, and change -- through analysis of a pressing sociotechnical system: surveillance systems. Through analysis of student reflections and final projects, we determined that at the conclusion of the semester, all students had developed critical analysis skills that allowed them to investigate surveillance systems of their own and identify their benefits, harms, main proponents, those who resist them, and their interplay with social systems, all while considering dimensions of race, class, gender, and more. We argue that this type of instruction -- directly teaching data science ethics alongside social theory -- is a crucial next step for the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.02420v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Rabb, Desen Ozkan</dc:creator>
    </item>
    <item>
      <title>Grand Challenges in Immersive Technologies for Cultural Heritage</title>
      <link>https://arxiv.org/abs/2412.02853</link>
      <description>arXiv:2412.02853v2 Announce Type: replace 
Abstract: Cultural heritage, a testament to human history and civilization, has gained increasing recognition for its significance in preservation and dissemination. The integration of immersive technologies has transformed how cultural heritage is presented, enabling audiences to engage with it in more vivid, intuitive, and interactive ways. However, the adoption of these technologies also brings a range of challenges and potential risks. This paper presents a systematic review, with an in-depth analysis of 177 selected papers. We comprehensively examine and categorize current applications, technological approaches, and user devices in immersive cultural heritage presentations, while also highlighting the associated risks and challenges. Furthermore, we identify areas for future research in the immersive presentation of cultural heritage. Our goal is to provide a comprehensive reference for researchers and practitioners, enhancing understanding of the technological applications, risks, and challenges in this field, and encouraging further innovation and development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02853v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanbing Wang, Junyan Du, Yue Li, Lie Zhang, Xiang Li</dc:creator>
    </item>
    <item>
      <title>Measuring and Forecasting Conversation Incivility: the Role of Antisocial and Prosocial Behaviors</title>
      <link>https://arxiv.org/abs/2412.02911</link>
      <description>arXiv:2412.02911v2 Announce Type: replace 
Abstract: This paper focuses on the task of measuring and forecasting incivility in conversations following replies to hate speech. Identifying replies that steer conversations away from hatred and elicit civil follow-up conversations sheds light into effective strategies to engage with hate speech and proactively avoid further escalation. We propose new metrics that take into account various dimensions of antisocial and prosocial behaviors to measure the conversation incivility following replies to hate speech. Our best metric aligns with human perceptions better than prior work. Additionally, we present analyses on a) the language of antisocial and prosocial posts, b) the relationship between antisocial or prosocial posts and user interactions, and c) the language of replies to hate speech that elicit follow-up conversations with different incivility levels. We show that forecasting the incivility level of conversations following a reply to hate speech is a challenging task. We also present qualitative analyses to identify the most common errors made by our best model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02911v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinchen Yu, Hayden Arnold, Benjamin Su, Eduardo Blanco</dc:creator>
    </item>
    <item>
      <title>Artificial intelligence and the internal processes of creativity</title>
      <link>https://arxiv.org/abs/2412.04366</link>
      <description>arXiv:2412.04366v2 Announce Type: replace 
Abstract: Artificial intelligence (AI) systems capable of generating creative outputs are reshaping our understanding of creativity. This shift presents an opportunity for creativity researchers to reevaluate the key components of the creative process. In particular, the advanced capabilities of AI underscore the importance of studying the internal processes of creativity. This paper explores the neurobiological machinery that underlies these internal processes and describes the experiential component of creativity. It is concluded that although the products of artificial and human creativity can be similar, the internal processes are different. The paper also discusses how AI may negatively affect the internal processes of human creativity, such as the development of skills, the integration of knowledge, and the diversity of ideas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04366v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jaan Aru</dc:creator>
    </item>
    <item>
      <title>Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models</title>
      <link>https://arxiv.org/abs/2408.08926</link>
      <description>arXiv:2408.08926v3 Announce Type: replace-cross 
Abstract: Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have potential to cause real-world impact. Policymakers, model providers, and researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks. We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks for each task, which break down a task into intermediary steps for a more detailed evaluation. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. For the top performing models (GPT-4o and Claude 3.5 Sonnet), we further investigate performance across 4 agent scaffolds (structed bash, action-only, pseudoterminal, and web search). Without subtask guidance, agents leveraging Claude 3.5 Sonnet, GPT-4o, OpenAI o1-preview, and Claude 3 Opus successfully solved complete tasks that took human teams up to 11 minutes to solve. In comparison, the most difficult task took human teams 24 hours and 54 minutes to solve. All code and data are publicly available at https://cybench.github.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08926v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andy K. Zhang, Neil Perry, Riya Dulepet, Joey Ji, Celeste Menders, Justin W. Lin, Eliot Jones, Gashon Hussein, Samantha Liu, Donovan Jasper, Pura Peetathawatchai, Ari Glenn, Vikram Sivashankar, Daniel Zamoshchin, Leo Glikbarg, Derek Askaryar, Mike Yang, Teddy Zhang, Rishi Alluri, Nathan Tran, Rinnara Sangpisit, Polycarpos Yiorkadjis, Kenny Osele, Gautham Raghupathi, Dan Boneh, Daniel E. Ho, Percy Liang</dc:creator>
    </item>
    <item>
      <title>A Water Efficiency Dataset for African Data Centers</title>
      <link>https://arxiv.org/abs/2412.03716</link>
      <description>arXiv:2412.03716v2 Announce Type: replace-cross 
Abstract: AI computing and data centers consume a large amount of freshwater, both directly for cooling and indirectly for electricity generation. While most attention has been paid to developed countries such as the U.S., this paper presents the first-of-its-kind dataset that combines nation-level weather and electricity generation data to estimate water usage efficiency for data centers in 41 African countries across five different climate regions. We also use our dataset to evaluate and estimate the water consumption of inference on two large language models (i.e., Llama-3-70B and GPT-4) in 11 selected African countries. Our findings show that writing a 10-page report using Llama-3-70B could consume about \textbf{0.7 liters} of water, while the water consumption by GPT-4 for the same task may go up to about 60 liters. For writing a medium-length email of 120-200 words, Llama-3-70B and GPT-4 could consume about \textbf{0.13 liters} and 3 liters of water, respectively. Interestingly, given the same AI model, 8 out of the 11 selected African countries consume less water than the global average, mainly because of lower water intensities for electricity generation. However, water consumption can be substantially higher in some African countries with a steppe climate than the U.S. and global averages, prompting more attention when deploying AI computing in these countries. Our dataset is publicly available on \href{https://huggingface.co/datasets/masterlion/WaterEfficientDatasetForAfricanCountries/tree/main}{Hugging Face}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03716v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Noah Shumba, Opelo Tshekiso, Pengfei Li, Giulia Fanti, Shaolei Ren</dc:creator>
    </item>
  </channel>
</rss>
