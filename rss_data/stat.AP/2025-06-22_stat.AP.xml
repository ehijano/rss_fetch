<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Jun 2025 04:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Improving Outbreak Forecasts Through Model Augmentation</title>
      <link>https://arxiv.org/abs/2506.16410</link>
      <description>arXiv:2506.16410v1 Announce Type: new 
Abstract: Accurate forecasts of disease outbreaks are critical for effective public health responses, management of healthcare surge capacity, and communication of public risk. There are a growing number of powerful forecasting methods that fall into two broad categories -- empirical models that extrapolate from historical data, and mechanistic models based on fixed epidemiological assumptions. However, these methods often underperform precisely when reliable predictions are most urgently needed -- during periods of rapid epidemic escalation. Here, we introduce epimodulation, a hybrid approach that integrates fundamental epidemiological principles into existing predictive models to enhance forecasting accuracy, especially around epidemic peaks. When applied to simple empirical forecasting methods (ARIMA, Holt--Winters, and spline models), epimodulation improved overall prediction accuracy by an average of 9.1\% (range: 8.2--12.5\%) for COVID-19 hospital admissions and by 19.5\% (range: 17.6--23.2\%) for influenza hospital admissions; accuracy during epidemic peaks improved even further, by an average of 20.7\% and 25.4\%, respectively. Epimodulation also substantially enhanced the performance of complex forecasting methods, including the COVID-19 Forecast Hub ensemble model, demonstrating its broad utility in improving forecast reliability at critical moments in disease outbreaks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16410v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Graham C. Gibson, Spencer J. Fox, Emily Javan, Susan E. Ptak, Oluwasegun M. Ibrahim, Michael Lachmann, Lauren Ancel Meyers</dc:creator>
    </item>
    <item>
      <title>Quantifying Flow State Dynamics: A Prefrontal Cortex EEG-Based Model Validation Study. Unveiling the Prefrontal Cortex's Role in Flow State Experience: An Empirical EEG Analysis</title>
      <link>https://arxiv.org/abs/2506.16838</link>
      <description>arXiv:2506.16838v1 Announce Type: new 
Abstract: This article aims to explore the optimization of mental performance through the analysis of metrics associated with the psychological state known as flow. Several clinical studies have shown a correlation between the mental state of flow (characterized by deep and relaxed concentration and high psychophysical efficiency) and brain activity measured through electroencephalography (EEG). This study confirms such a correlation, focusing in particular on the sports field, where the flow state tends to occur more frequently. To conduct the study, Sporthype developed proprietary software that integrates several predictive models, in particular the Flow State Index (FSI), implemented within the Holytics system. An analytical protocol was established, including mental exercises and data collection sessions using the portable EEG device Muse, accompanied by a questionnaire to gather athletes' subjective perceptions of their mental state. The results revealed a significant alignment between the EEG data and the subjective experiences reported in the questionnaires, confirming the feasibility of detecting the flow state through prefrontal cortex activity. Furthermore, the psychological exercises included in the study protocol showed a tangible positive effect in enhancing flow during athletic performance. Flow improves performance through a more harmonious synchronization between mind and body. Although golf was the main context of the experimentation, the mathematical models developed within Holytics were designed to be applicable to a wide range of sports. In addition to golf, preliminary tests have been conducted in other sports such as tennis, as well as in non-sport contexts, including gaming and mental training practices such as mindfulness, concentration, and visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16838v1</guid>
      <category>stat.AP</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gianluca Rosso, Raffaella Ricci, Lorenzo Pia, Giovanni Rebaudo, Michele Guindani, Alberto Marocchino, Giorgio De Pieri, Andrea Filippo Rosso</dc:creator>
    </item>
    <item>
      <title>Walking Fingerprinting Using Wrist Accelerometry During Activities of Daily Living in NHANES</title>
      <link>https://arxiv.org/abs/2506.17160</link>
      <description>arXiv:2506.17160v1 Announce Type: new 
Abstract: We propose a method for identifying individuals based on their continuously monitored wrist-worn accelerometry during activities of daily living. The method consists of three steps: (1) using Adaptive Empirical Pattern Transformation (ADEPT), a highly specific method to identify walking; (2) transforming the accelerometry time series into an image that corresponds to the joint distribution of the time series and its lags; and (3) using the resulting images to construct a person-specific walking fingerprint. The method is applied to 15,000 individuals from the National Health and Nutrition Examination Survey (NHANES) with up to 7 days of wrist accelerometry data collected at 80 Hertz. The resulting dataset contains more than 10 terabytes, is roughly 2 to 3 orders of magnitude larger than previous datasets used for activity recognition, is collected in the free living environment, and does not contain labels for walking periods. Using extensive cross-validation studies, we show that our method is highly predictive and can be successfully extended to a large, heterogeneous sample representative of the U.S. population: in the highest-performing model, the correct participant is in the top 1% of predictions 96% of the time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17160v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lily Koffman, John Muschelli III, Ciprian Crainiceanu</dc:creator>
    </item>
    <item>
      <title>Confidence Scoring for LLM-Generated SQL in Supply Chain Data Extraction</title>
      <link>https://arxiv.org/abs/2506.17203</link>
      <description>arXiv:2506.17203v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently enabled natural language interfaces that translate user queries into executable SQL, offering a powerful solution for non-technical stakeholders to access structured data. However, one of the limitation that LLMs do not natively express uncertainty makes it difficult to assess the reliability of their generated queries. This paper presents a case study that evaluates multiple approaches to estimate confidence scores for LLM-generated SQL in supply chain data retrieval. We investigated three strategies: (1) translation-based consistency checks; (2) embedding-based semantic similarity between user questions and generated SQL; and (3) self-reported confidence scores directly produced by the LLM. Our findings reveal that LLMs are often overconfident in their own outputs, which limits the effectiveness of self-reported confidence. In contrast, embedding-based similarity methods demonstrate strong discriminative power in identifying inaccurate SQL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17203v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/XXXXXX.XXXXXX</arxiv:DOI>
      <dc:creator>Jiekai Ma, Yikai Zhao</dc:creator>
    </item>
    <item>
      <title>Modern approaches to building effective interpretable models of the property market using machine learning</title>
      <link>https://arxiv.org/abs/2506.15723</link>
      <description>arXiv:2506.15723v1 Announce Type: cross 
Abstract: In this article, we review modern approaches to building interpretable models of property markets using machine learning on the base of mass valuation of property in the Primorye region, Russia. The researcher, lacking expertise in this topic, encounters numerous difficulties in the effort to build a good model. The main source of this is the huge difference between noisy real market data and ideal data which is very common in all types of tutorials on machine learning. This paper covers all stages of modeling: the collection of initial data, identification of outliers, the search and analysis of patterns in data, the formation and final choice of price factors, the building of the model, and the evaluation of its efficiency. For each stage, we highlight potential issues and describe sound methods for overcoming emerging difficulties on actual examples. We show that the combination of classical linear regression with interpolation methods of geostatistics allows to build an effective model for land parcels. For flats, when many objects are attributed to one spatial point the application of geostatistical methods is difficult. Therefore we suggest linear regression with automatic generation and selection of additional rules on the base of decision trees, so called the RuleFit method. Thus we show, that despite the strong restriction as the requirement of interpretability which is important in practical aspects, for example, legal matters, it is still possible to build effective models of real property markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15723v1</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Irina G. Tanashkina, Alexey S. Tanashkin, Alexander S. Maksimchuik, Anna Yu. Poshivailo</dc:creator>
    </item>
    <item>
      <title>Network Modelling of Asynchronous Change-Points in Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2506.15801</link>
      <description>arXiv:2506.15801v1 Announce Type: cross 
Abstract: This article introduces a novel Bayesian method for asynchronous change-point detection in multivariate time series. This method allows for change-points to occur earlier in some series (leading series) followed, after a short delay, by change-points in some other series (lagging series). Such dynamic dependence structure is common in fields such as seismology and neurology where a latent event such as an earthquake or seizure causes certain sensors to register change-points before others. We model these lead-lag dependencies via a latent directed graph and provide a hierarchical prior for learning the graph's structure and parameters. Posterior inference is made tractable by modifying particle MCMC methods designed for univariate change-point problems. We apply our method to both simulated and real datasets from the fields of seismology and neurology. In the simulated data, we find that our method outperforms competing Bayesian methods in settings where the change-point locations are dependent across series. In the real data applications we show that our model can also uncover interpretable network structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15801v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carson McKee, Maria Kalli</dc:creator>
    </item>
    <item>
      <title>Summary Statistics of Large-scale Model Outputs for Observation-corrected Outputs</title>
      <link>https://arxiv.org/abs/2506.15845</link>
      <description>arXiv:2506.15845v1 Announce Type: cross 
Abstract: Physics-based models capture broad spatial and temporal dynamics, but often suffer from biases and numerical approximations, while observations capture localized variability but are sparse. Integrating these complementary data modalities is important to improving the accuracy and reliability of model outputs. Meanwhile, physics-based models typically generate large outputs that are challenging to manipulate. In this paper, we propose Sig-PCA, a space-time framework that integrates summary statistics from model outputs with localized observations via a neural network (NN). By leveraging reduced-order representations from physics-based models and integrating them with observational data, our approach corrects model outputs, while allowing to work with dimensionally-reduced quantities hence with smaller NNs. This framework highlights the synergy between observational data and statistical summaries of model outputs, and effectively combines multisource data by preserving essential statistical information. We demonstrate our approach on two datasets (surface temperature and surface wind) with different statistical properties and different ratios of model to observational data. Our method corrects model outputs to align closely with the observational data, specifically enabling to correct probability distributions and space-time correlation structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15845v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atlanta Chakraborty, Julie Bessac</dc:creator>
    </item>
    <item>
      <title>Weighted Parameter Estimators of the Generalized Extreme Value Distribution in the Presence of Missing Observations</title>
      <link>https://arxiv.org/abs/2506.15964</link>
      <description>arXiv:2506.15964v1 Announce Type: cross 
Abstract: Missing data occur in a variety of applications of extreme value analysis. In the block maxima approach to an extreme value analysis, missingness is often handled by either ignoring missing observations or dropping a block of observations from the analysis. However, in some cases, missingness may occur due to equipment failure during an extreme event, which can lead to bias in estimation. In this work, we propose weighted maximum likelihood and weighted moment-based estimators for the generalized extreme value distribution parameters to account for the presence of missing observations. We validate the procedures through an extensive simulation study and apply the estimation methods to data from multiple tidal gauges on the Eastern coast of Canada.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15964v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>James H. McVittie, Orla A. Murphy</dc:creator>
    </item>
    <item>
      <title>Estimating Extreme Wave Surges in the Presence of Missing Data</title>
      <link>https://arxiv.org/abs/2506.15970</link>
      <description>arXiv:2506.15970v1 Announce Type: cross 
Abstract: The block maxima approach, which consists of dividing a series of observations into equal sized blocks to extract the block maxima, is commonly used for identifying and modelling extreme events using the generalized extreme value (GEV) distribution. In the analysis of coastal wave surge levels, the underlying data which generate the block maxima typically have missing observations. Consequently, the observed block maxima may not correspond to the true block maxima yielding biased estimates of the GEV distribution parameters. Various parametric modelling procedures are proposed to account for the presence of missing observations under a block maxima framework. The performance of these estimators is compared through an extensive simulation study and illustrated by an analysis of extreme wave surges in Atlantic Canada.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15970v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>James H. McVittie, Orla A. Murphy</dc:creator>
    </item>
    <item>
      <title>Leveraging Optimal Transport for Distributed Two-Sample Testing: An Integrated Transportation Distance-based Framework</title>
      <link>https://arxiv.org/abs/2506.16047</link>
      <description>arXiv:2506.16047v1 Announce Type: cross 
Abstract: This paper introduces a novel framework for distributed two-sample testing using the Integrated Transportation Distance (ITD), an extension of the Optimal Transport distance. The approach addresses the challenges of detecting distributional changes in decentralized learning or federated learning environments, where data privacy and heterogeneity are significant concerns. We provide theoretical foundations for the ITD, including convergence properties and asymptotic behavior. A permutation test procedure is proposed for practical implementation in distributed settings, allowing for efficient computation while preserving data privacy. The framework's performance is demonstrated through theoretical power analysis and extensive simulations, showing robust Type I error control and high power across various distributions and dimensions. The results indicate that ITD effectively aggregates information across distributed clients, detecting subtle distributional shifts that might be missed when examining individual clients. This work contributes to the growing field of distributed statistical inference, offering a powerful tool for two-sample testing in modern, decentralized data environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16047v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengqi Lin, Yan Chen</dc:creator>
    </item>
    <item>
      <title>Intelligent Operation and Maintenance and Prediction Model Optimization for Improving Wind Power Generation Efficiency</title>
      <link>https://arxiv.org/abs/2506.16095</link>
      <description>arXiv:2506.16095v1 Announce Type: cross 
Abstract: This study explores the effectiveness of predictive maintenance models and the optimization of intelligent Operation and Maintenance (O&amp;M) systems in improving wind power generation efficiency. Through qualitative research, structured interviews were conducted with five wind farm engineers and maintenance managers, each with extensive experience in turbine operations. Using thematic analysis, the study revealed that while predictive maintenance models effectively reduce downtime by identifying major faults, they often struggle with detecting smaller, gradual failures. Key challenges identified include false positives, sensor malfunctions, and difficulties in integrating new models with older turbine systems. Advanced technologies such as digital twins, SCADA systems, and condition monitoring have significantly enhanced turbine maintenance practices. However, these technologies still require improvements, particularly in AI refinement and real-time data integration. The findings emphasize the need for continuous development to fully optimize wind turbine performance and support the broader adoption of renewable energy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16095v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICHORA65333.2025.11017307</arxiv:DOI>
      <arxiv:journal_reference>Proc. 7th Int. Congr. on Human-Computer Interaction, Optimization and Robotic Applications (ICHORA), IEEE, pp. 1-7, 2025</arxiv:journal_reference>
      <dc:creator>Xun Liu, Xiaobin Wu, Jiaqi He, Rajan Das Gupta</dc:creator>
    </item>
    <item>
      <title>AI labeling reduces the perceived accuracy of online content but has limited broader effects</title>
      <link>https://arxiv.org/abs/2506.16202</link>
      <description>arXiv:2506.16202v1 Announce Type: cross 
Abstract: Explicit labeling of online content produced by artificial intelligence (AI) is a widely mooted policy for ensuring transparency and promoting public confidence. Yet little is known about the scope of AI labeling effects on public assessments of labeled content. We contribute new evidence on this question from a survey experiment using a high-quality nationally representative probability sample (n = 3,861). First, we demonstrate that explicit AI labeling of a news article about a proposed public policy reduces its perceived accuracy. Second, we test whether there are spillover effects in terms of policy interest, policy support, and general concerns about online misinformation. We find that AI labeling reduces interest in the policy, but neither influences support for the policy nor triggers general concerns about online misinformation. We further find that increasing the salience of AI use reduces the negative impact of AI labeling on perceived accuracy, while one-sided versus two-sided framing of the policy has no moderating effect. Overall, our findings suggest that the effects of algorithm aversion induced by AI labeling of online content are limited in scope.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16202v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuyao Wang, Patrick Sturgis, Daniel de Kadt</dc:creator>
    </item>
    <item>
      <title>A Self-Organized Criticality Model of Extreme Events and Cascading Disasters of Hub and Spoke Air Traffic Networks</title>
      <link>https://arxiv.org/abs/2506.16727</link>
      <description>arXiv:2506.16727v1 Announce Type: cross 
Abstract: Critical infrastructure networks--including transportation, power grids, and communication systems--exhibit complex interdependencies that can lead to cascading failures with catastrophic consequences. These disasters often originate from failures at critical points in the network, where single-node disruptions can propagate rapidly due to structural dependencies and high-impact linkages. Such vulnerabilities are exacerbated in systems that have been highly optimized for efficiency or have self-organized into fragile configurations over time. The U.S. air transportation system, built on a hub-and-spoke model, exemplifies this type of critical infrastructure. Its reliance on a small number of high-throughput hubs means that even localized disruptions--especially those triggered by increasingly frequent and extreme weather events--can initiate cascades with nationwide impact. We introduce a novel application of Self-Organized Criticality (SOC) theory to model and analyze cascading failures in such systems. Through a detailed case study of U.S. airline operations, we show how the SOC model captures the power-law distribution of disruptions and the long-tail risk of systemic failures, reflecting the interplay between structural fragility and climate shocks. Our approach enables quantitative assessment of network vulnerability, identification of critical nodes, and evaluation of proactive strategies for disaster risk reduction. The results demonstrate that the SOC model replicates the observed statistical patterns--frequent small events and rare, severe failures--offering a powerful systems-level framework for infrastructure resilience planning and emergency response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16727v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mary Lai O. Salva\~na, Harold Jay M. Bolingot, Gregory L. Tangonan</dc:creator>
    </item>
    <item>
      <title>From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images</title>
      <link>https://arxiv.org/abs/2506.16890</link>
      <description>arXiv:2506.16890v1 Announce Type: cross 
Abstract: The detection and localization of quality-related problems in industrially mass-produced products has historically relied on manual inspection, which is costly and error-prone. Machine learning has the potential to replace manual handling. As such, the desire is to facilitate an unsupervised (or self-supervised) approach, as it is often impossible to specify all conceivable defects ahead of time. A plethora of prior works have demonstrated the aptitude of common reconstruction-, embedding-, and synthesis-based methods in laboratory settings. However, in practice, we observe that most methods do not handle low data quality well or exude low robustness in unfavorable, but typical real-world settings. For practitioners it may be very difficult to identify the actual underlying problem when such methods underperform. Worse, often-reported metrics (e.g., AUROC) are rarely suitable in practice and may give misleading results. In our setting, we attempt to identify subtle anomalies on the surface of blasted forged metal parts, using rather low-quality RGB imagery only, which is a common industrial setting. We specifically evaluate two types of state-of-the-art models that allow us to identify and improve quality issues in production data, without having to obtain new data. Our contribution is to provide guardrails for practitioners that allow them to identify problems related to, e.g., (lack of) robustness or invariance, in either the chosen model or the data reliably in similar scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of likelihood-based approaches and outline a framework for proper empirical risk estimation that is more suitable for real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16890v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sebastian H\"onel, Jonas Nordqvist</dc:creator>
    </item>
    <item>
      <title>Skewness-Kurtosis: small samples and power-law behavior</title>
      <link>https://arxiv.org/abs/2506.16906</link>
      <description>arXiv:2506.16906v1 Announce Type: cross 
Abstract: Skewness and kurtosis are fundamental statistical moments commonly used to quantify asymmetry and tail behavior in probability distributions. Despite their widespread application in statistical mechanics, condensed matter physics, and complex systems, important aspects of their empirical behavior remain unclear, particularly in small samples and in relation to their hypothesized power law scaling. In this work, we address both issues using a combination of empirical and synthetic data. First, we establish a lower bound for sample kurtosis as a function of sample size and skewness. Second, we examine the conditions under which the 4/3 power law relationship between kurtosis and skewness emerges, effectively extending Taylor power law to higher order moments. Our results show that this scaling behavior predominantly occurs in data sampled from heavy tailed distributions and medium, large sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16906v1</guid>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlo De Michele, Samuele De Bartolo</dc:creator>
    </item>
    <item>
      <title>The Two Cultures of Prevalence Mapping: Small Area Estimation and Model-Based Geostatistics</title>
      <link>https://arxiv.org/abs/2110.09576</link>
      <description>arXiv:2110.09576v3 Announce Type: replace 
Abstract: In low- and middle-income countries (LMICs), accurate estimates of subnational health and demographic indicators are critical for guiding policy and identifying disparities. Many indicators of interest are proportions of binary outcomes and the task of estimating these fractions is often called prevalence mapping. In LMICs, health and vital records data are limited, so prevalence mapping relies on data from household surveys with complex sampling designs. However, estimates are often desired at spatial resolutions at which data are insufficient. We review two families of approaches to prevalence mapping: small area estimation (SAE) methods (from the survey statistics literature) and model-based geostatistics (MBG) methods (from the spatial statistics literature). SAE models can be ``area-level" or ``unit-level" and commonly use area-specific random effects and rely upon high-quality covariate data from administrative sources. Unit-level models for binary responses are relatively underdeveloped. MBG approaches explicitly specify binary response models, incorporate continuous spatial random effects, and leverage alternative data sources, e.g., satellite imagery. SAE methods often address the design by incorporating sampling weights or modeling the sampling mechanism. Two delicate issues arise when using MBG methods. First, aggregating unit level predictions to create area-level summaries requires population-level information that is rarely available. Second, MBG approaches typically assume the sampling design is ignorable. We review both approaches, and argue that binary response models can be improved using insights from both the survey sampling and the spatial statistics literature. We highlight these issues using household survey data from the Zambia 2018 Demographic Health Survey to estimate subnational HIV prevalence for woman aged 15--49.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.09576v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jon Wakefield, Peter A. Gao, Geir-Arne Fuglstad, Zehang Richard Li</dc:creator>
    </item>
    <item>
      <title>Hidden assumptions of integer ratio analyses in bioacoustics and music</title>
      <link>https://arxiv.org/abs/2502.04464</link>
      <description>arXiv:2502.04464v2 Announce Type: replace 
Abstract: Rhythm is ubiquitous in human culture and in nature, but hard to capture in all its complexity. A key dimension of rhythm, integer ratio categories occur when the relationship between temporal intervals can be expressed as small-integer ratios. Recent work has found integer ratio categories in most human musical cultures and some animal species' vocalizations or behavioral displays. But biological systems are noisy, and empirically measured intervals rarely form an exact small-integer ratio. Here, we mathematically assess whether the leading integer ratio analysis method makes valid statistical and biological assumptions. In particular, we (1) make the temporal properties of empirical ratios explicit, both in general and for the typical use in the literature; (2) show how the choice of ratio formula affects the probability distribution of rhythm ratios and ensuing statistical results; (3) guide the reader to carefully consider the assumptions and null hypotheses of the statistical analysis; (4) present a comprehensive methodology to statistically test integer ratios for any null hypothesis of choice. Our observations have implications for both past and future research in music cognition and animal behavior: They suggest how to interpret past findings and provide tools to choose the correct null hypotheses in future empirical work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04464v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yannick Jadoul, Tommaso Tufarelli, Chlo\'e Coissac, Marco Gamba, Andrea Ravignani</dc:creator>
    </item>
    <item>
      <title>Data Envelopment Analysis with Robust and Closest Targets:Integrating Full-Dimensional Efficient Facets for Risk-Resilient Benchmarking</title>
      <link>https://arxiv.org/abs/2505.06487</link>
      <description>arXiv:2505.06487v2 Announce Type: replace 
Abstract: As the external environment become increasingly volatile and unpredictable, the selection of benchmarking targets in data envelopment analysis should account for their ability to consider risks; however, this aspect has not received sufficient attention. We propose a robust benchmarking target defined by the intersection of the maximum number of full-dimensional efficient facets, each representing a unique marginal substitution relationship. These targets can serve as robust projections for decision making units that are lacking prior risk information because they incorporate the maximum number of marginal substitution relationships. This enables decision makers to adjust their production through these relationships, thereby maximizing the likelihood of achieving globally optimal outcomes. Furthermore, we propose a novel, well-defined efficiency measure based on robust and closest targets. Finally, we demonstrate the application of the proposed measure using a dataset comprising 38 universities from China's 985 Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06487v2</guid>
      <category>stat.AP</category>
      <category>math.OC</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiuquan Huang, Xi Wang, Tao Zhang, Xiaocang Xu, Ali Emrouznejad</dc:creator>
    </item>
    <item>
      <title>Linear Model and Extensions</title>
      <link>https://arxiv.org/abs/2401.00649</link>
      <description>arXiv:2401.00649v2 Announce Type: replace-cross 
Abstract: I developed the lecture notes based on my ``Linear Model'' course at the University of California, Berkeley over the past ten years. This book provides an intermediate-level introduction to the linear model. It balances rigorous proofs and heuristic arguments. This book provides R code to replicate all simulation studies and case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00649v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peng Ding</dc:creator>
    </item>
    <item>
      <title>Data-driven modeling and prediction of microglial cell dynamics in the ischemic penumbra</title>
      <link>https://arxiv.org/abs/2404.10915</link>
      <description>arXiv:2404.10915v3 Announce Type: replace-cross 
Abstract: Neuroinflammation immediately follows the onset of ischemic stroke. During this process, microglial cells are activated in and recruited to the tissue surrounding the irreversibly injured infarct core, referred to as the penumbra. Microglial cells can be activated into two distinct phenotypes; however, the dynamics between the detrimental M1 phenotype and beneficial M2 phenotype are not fully understood. Using phenotype-specific cell count data obtained from experimental studies on middle cerebral artery occlusion-induced stroke in mice, we employ sparsity-promoting system identification techniques combined with Bayesian statistical methods for uncertainty quantification to generate continuous and discrete-time predictive models of the M1 and M2 microglial cell dynamics. The resulting data-driven models include constant and linear terms but do not include nonlinear interactions between the cells. Results emphasize an initial M2 dominance followed by a takeover of M1 cells, capture potential long-term dynamics of microglial cells, and suggest a persistent inflammatory response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10915v3</guid>
      <category>q-bio.CB</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Amato, Andrea Arnold</dc:creator>
    </item>
    <item>
      <title>On the PM2.5 -- Mortality Relationship: A Bayesian Model for Spatio-Temporal Confounding</title>
      <link>https://arxiv.org/abs/2405.16106</link>
      <description>arXiv:2405.16106v2 Announce Type: replace-cross 
Abstract: Spatial confounding, often regarded as a major concern in epidemiological studies, relates to the difficulty of recovering the effect of an exposure on an outcome when these variables are associated with unobserved factors. This issue is particularly challenging in spatio-temporal analyses, where it has been less explored so far. To study the effects of air pollution on mortality in Italy, we argue that a model that simultaneously accounts for spatio-temporal confounding and for the non-linear form of the effect of interest is needed. To this end, we propose a Bayesian spatial dynamic generalized linear model, which allows for a non-linear association and for a decomposition of the exposure effect into two components. This decomposition accommodates associations with the outcome at fine and coarse temporal and spatial scales of variation. These features, when combined, allow reducing the spatio-temporal confounding bias and recovering the true shape of the association, as demonstrated through simulation studies. The results from the real-data application indicate that the exposure effect seems to have different magnitudes in different seasons, with peaks in the summer. We hypothesize that this could be due to possible interactions between the exposure variable with air temperature and unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16106v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlo Zaccardi, Pasquale Valentini, Luigi Ippoliti, Alexandra M. Schmidt</dc:creator>
    </item>
    <item>
      <title>Adaptive partition Factor Analysis</title>
      <link>https://arxiv.org/abs/2410.18939</link>
      <description>arXiv:2410.18939v2 Announce Type: replace-cross 
Abstract: Factor Analysis has traditionally been utilized across diverse disciplines to extrapolate latent traits that influence the behavior of multivariate observed variables. Historically, the focus has been on analyzing data from a single study, neglecting the potential study-specific variations present in data from multiple studies. Multi-study factor analysis has emerged as a recent methodological advancement that addresses this gap by distinguishing between latent traits shared across studies and study-specific components arising from artifactual or population-specific sources of variation. In this paper, we extend the current methodologies by introducing novel shrinkage priors for the latent factors, thereby accommodating a broader spectrum of scenarios -- from the absence of study-specific latent factors to models in which factors pertain only to small subgroups nested within or shared between the studies. For the proposed construction we provide conditions for identifiability of factor loadings and guidelines to perform straightforward posterior computation via Gibbs sampling. Through comprehensive simulation studies, we demonstrate that our proposed method exhibits competing performance across a variety of scenarios compared to existing methods, yet providing richer insights. The practical benefits of our approach are further illustrated through applications to bird species co-occurrence data and ovarian cancer gene expression data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18939v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Bortolato, Antonio Canale</dc:creator>
    </item>
    <item>
      <title>Fast data inversion for high-dimensional dynamical systems from noisy measurements</title>
      <link>https://arxiv.org/abs/2501.01324</link>
      <description>arXiv:2501.01324v3 Announce Type: replace-cross 
Abstract: In this work, we develop a scalable approach for a flexible latent factor model for high-dimensional dynamical systems. Each latent factor process has its own correlation and variance parameters, and the orthogonal factor loading matrix can be either fixed or estimated. We utilize an orthogonal factor loading matrix that avoids computing the inversion of the posterior covariance matrix at each time of the Kalman filter, and derive closed-form expressions in an expectation-maximization algorithm for parameter estimation, which substantially reduces the computational complexity without approximation. Our study is motivated by inversely estimating slow slip events from geodetic data, such as continuous GPS measurements. Extensive simulated studies illustrate higher accuracy and scalability of our approach compared to alternatives. By applying our method to geodetic measurements in the Cascadia region, our estimated slip better agrees with independently measured seismic data of tremor events. The substantial acceleration from our method enables the use of massive noisy data for geological hazard quantification and other applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01324v3</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizi Lin, Xubo Liu, Paul Segall, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Sustainable Greenhouse Microclimate Modeling: A Comparative Analysis of Recurrent and Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2502.17371</link>
      <description>arXiv:2502.17371v4 Announce Type: replace-cross 
Abstract: The integration of photovoltaic (PV) systems into greenhouses not only optimizes land use but also enhances sustainable agricultural practices by enabling dual benefits of food production and renewable energy generation. However, accurate prediction of internal environmental conditions is crucial to ensure optimal crop growth while maximizing energy production. This study introduces a novel application of Spatio-Temporal Graph Neural Networks (STGNNs) to greenhouse microclimate modeling, comparing their performance with traditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal pattern recognition, they cannot explicitly model the directional relationships between environmental variables. Our STGNN approach addresses this limitation by representing these relationships as directed graphs, enabling the model to capture both environmental dependencies and their directionality. We benchmark RNNs against directed STGNNs on two 15-min-resolution datasets from Volos (Greece): a six-variable 2020 installation and a more complex eight-variable greenhouse monitored in autumn 2024. In the simpler 2020 case the RNN attains near-perfect accuracy, outperforming the STGNN. When additional drivers are available in 2024, the STGNN overtakes the RNN ($R^{2}=0.905$ vs $0.740$), demonstrating that explicitly modelling directional dependencies becomes critical as interaction complexity grows. These findings indicate when graph-based models are warranted and provide a stepping-stone toward digital twins that jointly optimise crop yield and PV power in agrivoltaic greenhouses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17371v4</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiliano Seri, Marcello Petitta, Chryssoula Papaioannou, Nikolaos Katsoulas, Cristina Cornaro</dc:creator>
    </item>
    <item>
      <title>Correlation and Beyond: Positive Definite Dependence Measures for Robust Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios</title>
      <link>https://arxiv.org/abs/2504.15268</link>
      <description>arXiv:2504.15268v5 Announce Type: replace-cross 
Abstract: We live in a multivariate world, and effective modeling of financial portfolios, including their construction, allocation, forecasting, and risk analysis, simply is not possible without explicitly modeling the dependence structure of their assets. Dependence structure can drive portfolio results more than the combined effects of other parameters in investment and risk models, but the literature provides relatively little to define the finite-sample distributions of dependence measures in useable and useful ways under challenging, real-world financial data conditions. Yet this is exactly what is needed to make valid inferences about their estimates, and to use these inferences for essential purposes such as hypothesis testing, dynamic monitoring, realistic and granular scenario and reverse scenario analyses, and mitigating the effects of correlation breakdowns during market upheavals. This work develops a new and straightforward method, Nonparametric Angles-based Correlation (NAbC), for defining the finite-sample distributions of any dependence measure whose matrix of pairwise associations is positive definite (e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). The solution remains valid under marginal asset distributions characterized by notably different and varying degrees of serial correlation, non-stationarity, heavy-tailedness, and asymmetry. Importantly, NAbCs p-values and confidence intervals remain analytically consistent at both the matrix level and the pairwise cell level. Finally, NAbC maintains validity even when selected cells in the matrix are frozen for a given scenario or stress test, thus enabling flexible, granular, and realistic scenarios. NAbC stands alone in providing all of these capabilities simultaneously, and should prove to be a very useful means by which we can better understand and manage financial portfolios in our multivariate world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15268v5</guid>
      <category>q-fin.RM</category>
      <category>q-fin.PM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>JD Opdyke</dc:creator>
    </item>
    <item>
      <title>Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs</title>
      <link>https://arxiv.org/abs/2506.13593</link>
      <description>arXiv:2506.13593v2 Announce Type: replace-cross 
Abstract: We develop a framework to quantify the time-to-unsafe-sampling - the number of large language model (LLM) generations required to trigger an unsafe (e.g., toxic) response. Estimating this quantity is challenging, since unsafe responses are exceedingly rare in well-aligned LLMs, potentially occurring only once in thousands of generations. As a result, directly estimating time-to-unsafe-sampling would require collecting training data with a prohibitively large number of generations per prompt. However, with realistic sampling budgets, we often cannot generate enough responses to observe an unsafe outcome for every prompt, leaving the time-to-unsafe-sampling unobserved in many cases, making the estimation and evaluation tasks particularly challenging. To address this, we frame this estimation problem as one of survival analysis and develop a provably calibrated lower predictive bound (LPB) on the time-to-unsafe-sampling of a given prompt, leveraging recent advances in conformal prediction. Our key innovation is designing an adaptive, per-prompt sampling strategy, formulated as a convex optimization problem. The objective function guiding this optimized sampling allocation is designed to reduce the variance of the estimators used to construct the LPB, leading to improved statistical efficiency over naive methods that use a fixed sampling budget per prompt. Experiments on both synthetic and real data support our theoretical results and demonstrate the practical utility of our method for safety risk assessment in generative AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13593v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hen Davidov, Gilad Freidkin, Shai Feldman, Yaniv Romano</dc:creator>
    </item>
  </channel>
</rss>
