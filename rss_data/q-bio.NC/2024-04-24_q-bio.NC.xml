<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Apr 2024 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Antifragile control systems in neuronal processing: A sensorimotor perspective</title>
      <link>https://arxiv.org/abs/2404.14799</link>
      <description>arXiv:2404.14799v1 Announce Type: new 
Abstract: The stability--robustness--resilience--adaptiveness continuum in neuronal processing follows a hierarchical structure that explains interactions and information processing among the different time scales. Interestingly, using "canonical" neuronal computational circuits, such as Homeostatic Activity Regulation, Winner-Take-All, and Hebbian Temporal Correlation Learning, one can extend the behaviour spectrum towards antifragility. Cast already in both probability theory and dynamical systems, antifragility can explain and define the interesting interplay among neural circuits, found, for instance, in sensorimotor control in the face of uncertainty and volatility. This perspective proposes a new framework to analyse and describe closed-loop neuronal processing using principles of antifragility, targeting sensorimotor control. Our objective is two-fold. First, we introduce antifragile control as a conceptual framework to quantify closed-loop neuronal network behaviours that gain from uncertainty and volatility. Second, we introduce neuronal network design principles, opening the path to neuromorphic implementations and transfer to technical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14799v1</guid>
      <category>q-bio.NC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cristian Axenie</dc:creator>
    </item>
    <item>
      <title>How we Learn Concepts: A Review of Relevant Advances Since 2010 and Its Inspirations for Teaching</title>
      <link>https://arxiv.org/abs/2404.14867</link>
      <description>arXiv:2404.14867v1 Announce Type: new 
Abstract: This article reviews the psychological and neuroscience achievements in concept learning since 2010 from the perspectives of individual learning and social learning, and discusses several issues related to concept learning, including the assistance of machine learning about concept learning. 1 In terms of individual learning, current evidences shown that the brain tends to process concrete concepts through typical features (shared features); And abstract concepts, semantic processing is the most important cognitive way. 2 In terms of social learning, Interpersonal Neuro Synchronization (INS) is considered the main indicator of efficient knowledge transfer (such as teaching activities between teachers and students), but this phenomenon only broadens the channels for concept sources and does not change the basic mode of individual concept learning. Ultimately, this article argues that the way the human brain processes concepts depends on concept's own characteristics, so there are no 'better' strategies in teaching, only more 'suitable' strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14867v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhong Wang</dc:creator>
    </item>
    <item>
      <title>Semantic distance organizes social knowledge: Insights from semantic dementia and cross-modal conceptual space</title>
      <link>https://arxiv.org/abs/2404.15151</link>
      <description>arXiv:2404.15151v1 Announce Type: new 
Abstract: Our interaction with others largely hinges on how we semantically organize the social world. The organization of such conceptual information is not static -- as we age, our experiences and ever-changing anatomy alter how we represent and arrange semantic information. How does semantic distance between concepts affect this organization, particularly for those with pathological deficits in semantic knowledge? Using triplet judgment responses collected from healthy participants, we compute an ordinal similarity embedding for a set of social words and images that vary in the dimensions of age and gender. We compare semantic distances between items in the space to patterns of error in a word-picture matching task performed by patients with semantic dementia (SD). Error patterns reveal that SD patients retain gender information more robustly than age information, and that age-related errors are a function of linear distance in age from a concept word. The distances between probed and exemplar items in the resulting conceptual map reflect error patterns in SD patient responses such that items semantically closer to a probed concept -- in gender category or in linear age -- are more likely to be erroneously chosen by patients in a word-picture matching task. To our knowledge, this is the first triplet embedding work to embed representations of words and images in a unified space, and to use this space to explain patterns of behavior in patients with impaired social semantic cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15151v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Y. Ivette Col\'on, Matthew Rouse, Matthew A. Lambon Ralph, Timothy T. Rogers</dc:creator>
    </item>
    <item>
      <title>Resting state fMRI-based brain information flow mapping</title>
      <link>https://arxiv.org/abs/2404.15173</link>
      <description>arXiv:2404.15173v1 Announce Type: new 
Abstract: Human brain is a massive information generation and processing machine. Studying the information flow may provide unique insight into brain function and brain diseases. We present here a tool for mapping the regional information flow in the entire brain using fMRI. Using the tool, we can estimate the information flow from a single region to the rest of the brain, between different regions, between different days, or between different individuals' brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15173v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ze Wang</dc:creator>
    </item>
    <item>
      <title>Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks</title>
      <link>https://arxiv.org/abs/2404.14964</link>
      <description>arXiv:2404.14964v1 Announce Type: cross 
Abstract: Training spiking neural networks to approximate complex functions is essential for studying information processing in the brain and neuromorphic computing. Yet, the binary nature of spikes constitutes a challenge for direct gradient-based training. To sidestep this problem, surrogate gradients have proven empirically successful, but their theoretical foundation remains elusive. Here, we investigate the relation of surrogate gradients to two theoretically well-founded approaches. On the one hand, we consider smoothed probabilistic models, which, due to lack of support for automatic differentiation, are impractical for training deep spiking neural networks, yet provide gradients equivalent to surrogate gradients in single neurons. On the other hand, we examine stochastic automatic differentiation, which is compatible with discrete randomness but has never been applied to spiking neural network training. We find that the latter provides the missing theoretical basis for surrogate gradients in stochastic spiking neural networks. We further show that surrogate gradients in deterministic networks correspond to a particular asymptotic case and numerically confirm the effectiveness of surrogate gradients in stochastic multi-layer spiking neural networks. Finally, we illustrate that surrogate gradients are not conservative fields and, thus, not gradients of a surrogate loss. Our work provides the missing theoretical foundation for surrogate gradients and an analytically well-founded solution for end-to-end training of stochastic spiking neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14964v1</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Gygax, Friedemann Zenke</dc:creator>
    </item>
  </channel>
</rss>
