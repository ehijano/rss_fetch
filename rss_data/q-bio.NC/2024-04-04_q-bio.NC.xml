<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Apr 2024 04:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Information Theory and Direction Selectivity</title>
      <link>https://arxiv.org/abs/2404.02915</link>
      <description>arXiv:2404.02915v1 Announce Type: new 
Abstract: In this brief paper, the authors study the tuning curves of starburst amacrine cells (SACs) and introduce a quantity called the irresolution or ambiguity of a SAC. They show that the rate of data generated by a starburst amacrine cell is inversely proportional to its irresolution. This is done by providing bounds on the rate required to encode the generated data. This technique can be applied to different cell types with different tuning curves. In this manner, information theoretic views can be introduced to the cases of biological cells which are not normally considered as transmitters of information as say retinal ganglion cells are. The intuition that even such cells generate information is thus quantified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02915v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aman Chawla</dc:creator>
    </item>
    <item>
      <title>A Coupled Neural Field Model for the Standard Consolidation Theory</title>
      <link>https://arxiv.org/abs/2404.02938</link>
      <description>arXiv:2404.02938v1 Announce Type: new 
Abstract: The standard consolidation theory states that short-term memories located in the hippocampus enable the consolidation of long-term memories in the neocortex. In other words, the neocortex slowly learns long-term memories with a transient support of the hippocampus that quickly learns unstable memories. However, it is not clear yet what could be the neurobiological mechanisms underlying these differences in learning rates and memory time-scales. Here, we propose a novel modelling approach of the standard consolidation theory, that focuses on its potential neurobiological mechanisms. In addition to synaptic plasticity and spike frequency adaptation, our model incorporates adult neurogenesis in the dentate gyrus as well as the difference in size between the neocortex and the hippocampus, that we associate with distance-dependent synaptic plasticity. We also take into account the interconnected spatial structure of the involved brain areas, by incorporating the above neurobiological mechanisms in a coupled neural field framework, where each area is represented by a separate neural field with intra- and inter-area connections. To our knowledge, this is the first attempt to apply neural fields to this process. Using numerical simulations and mathematical analysis, we explore the short-term and long-term dynamics of the model upon alternance of phases of hippocampal replay and retrieval cue of an external input. This external input is encodable as a memory pattern in the form of a multiple bump attractor pattern in the individual neural fields. In the model, hippocampal memory patterns become encoded first, before neocortical ones, because of the smaller distances between the bumps of the hippocampal memory patterns. As a result, retrieval of the input pattern in the neocortex at short time-scales necessitates the additional input delivered by the memory pattern of the hippocampus. Neocortical memory patterns progressively consolidate at longer times, up to a point where their retrieval does not need the support of the hippocampus anymore. At longer times, perturbation of the hippocampal neural fields by neurogenesis erases the hippocampus pattern, leading to a final state where the memory pattern is exclusively evoked in the neocortex. Therefore, the dynamics of our model successfully reproduces the main features of the standard consolidation theory. This suggests that neurogenesis in the hippocampus and distance-dependent synaptic plasticity coupled to synaptic depression and spike frequency adaptation, are indeed critical neurobiological processes in memory consolidation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02938v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lisa Blum Moyse (LIRIS, AISTROSIGHT), Hugues Berry (AISTROSIGHT)</dc:creator>
    </item>
    <item>
      <title>Utilizing Computer Vision for Continuous Monitoring of Vaccine Side Effects in Experimental Mice</title>
      <link>https://arxiv.org/abs/2404.03121</link>
      <description>arXiv:2404.03121v1 Announce Type: cross 
Abstract: The demand for improved efficiency and accuracy in vaccine safety assessments is increasing. Here, we explore the application of computer vision technologies to automate the monitoring of experimental mice for potential side effects after vaccine administration. Traditional observation methods are labor-intensive and lack the capability for continuous monitoring. By deploying a computer vision system, our research aims to improve the efficiency and accuracy of vaccine safety assessments. The methodology involves training machine learning models on annotated video data of mice behaviors pre- and post-vaccination. Preliminary results indicate that computer vision effectively identify subtle changes, signaling possible side effects. Therefore, our approach has the potential to significantly enhance the monitoring process in vaccine trials in animals, providing a practical solution to the limitations of human observation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03121v1</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chuang Li, Shuai Shao, Willian Mikason, Rubing Lin, Yantong Liu</dc:creator>
    </item>
    <item>
      <title>Effect of Synaptic Heterogeneity on Neuronal Coordination</title>
      <link>https://arxiv.org/abs/2308.00421</link>
      <description>arXiv:2308.00421v3 Announce Type: replace-cross 
Abstract: Recent advancements in measurement techniques have resulted in an increasing amount of data on neural activities recorded in parallel, revealing largely heterogeneous correlation patterns across neurons. Yet, the mechanistic origin of this heterogeneity is largely unknown because existing theoretical approaches linking structure and dynamics in neural circuits are restricted to population-averaged connectivity and activity. Here we present a systematic inclusion of heterogeneity in network connectivity to derive quantitative predictions for neuron-resolved covariances and their statistics in spiking neural networks. Our study shows that the heterogeneity in covariances is not a result of variability in single-neuron firing statistics but stems from the ubiquitously observed sparsity and variability of connections in brain networks. Linear-response theory maps these features to the effective connectivity between neurons, which in turn determines neuronal covariances. Beyond-mean-field tools reveal that synaptic heterogeneity modulates the variability of covariances and thus the complexity of neuronal coordination across many orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.00421v3</guid>
      <category>cond-mat.dis-nn</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PRXLife.2.013013</arxiv:DOI>
      <arxiv:journal_reference>Layer, M., Helias, M., &amp; Dahmen, D. (2024). Effect of Synaptic Heterogeneity on Neuronal Coordination. PRX Life, 2, 013013</arxiv:journal_reference>
      <dc:creator>Moritz Layer, Moritz Helias, David Dahmen</dc:creator>
    </item>
    <item>
      <title>Decoding Natural Images from EEG for Object Recognition</title>
      <link>https://arxiv.org/abs/2308.13234</link>
      <description>arXiv:2308.13234v3 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG) signals, known for convenient non-invasive acquisition but low signal-to-noise ratio, have recently gained substantial attention due to the potential to decode natural images. This paper presents a self-supervised framework to demonstrate the feasibility of learning image representations from EEG signals, particularly for object recognition. The framework utilizes image and EEG encoders to extract features from paired image stimuli and EEG responses. Contrastive learning aligns these two modalities by constraining their similarity. With the framework, we attain significantly above-chance results on a comprehensive EEG-image dataset, achieving a top-1 accuracy of 15.6% and a top-5 accuracy of 42.8% in challenging 200-way zero-shot tasks. Moreover, we perform extensive experiments to explore the biological plausibility by resolving the temporal, spatial, spectral, and semantic aspects of EEG signals. Besides, we introduce attention modules to capture spatial correlations, providing implicit evidence of the brain activity perceived from EEG data. These findings yield valuable insights for neural decoding and brain-computer interfaces in real-world scenarios. The code will be released on https://github.com/eeyhsong/NICE-EEG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13234v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghao Song, Bingchuan Liu, Xiang Li, Nanlin Shi, Yijun Wang, Xiaorong Gao</dc:creator>
    </item>
    <item>
      <title>Ambitions for theory in the physics of life</title>
      <link>https://arxiv.org/abs/2401.15538</link>
      <description>arXiv:2401.15538v2 Announce Type: replace-cross 
Abstract: Theoretical physicists have been fascinated by the phenomena of life for more than a century. As we engage with more realistic descriptions of living systems, however, things get complicated. After reviewing different reactions to this complexity, I explore the optimization of information flow as a potentially general theoretical principle. The primary example is a genetic network guiding development of the fly embryo, but each idea also is illustrated by examples from neural systems. In each case, optimization makes detailed, largely parameter-free predictions that connect quantitatively with experiment</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15538v2</guid>
      <category>physics.bio-ph</category>
      <category>q-bio.MN</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Bialek</dc:creator>
    </item>
    <item>
      <title>Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion</title>
      <link>https://arxiv.org/abs/2403.07721</link>
      <description>arXiv:2403.07721v4 Announce Type: replace-cross 
Abstract: How to decode human vision through neural signals has attracted a long-standing interest in neuroscience and machine learning. Modern contrastive learning and generative models improved the performance of fMRI-based visual decoding and reconstruction. However, the high cost and low temporal resolution of fMRI limit their applications in brain-computer interfaces (BCIs), prompting a high need for EEG-based visual reconstruction. In this study, we present an EEG-based visual reconstruction framework. It consists of a plug-and-play EEG encoder called the Adaptive Thinking Mapper (ATM), which is aligned with image embeddings, and a two-stage EEG guidance image generator that first transforms EEG features into image priors and then reconstructs the visual stimuli with a pre-trained image generator. Our approach allows EEG embeddings to achieve superior performance in image classification and retrieval tasks. Our two-stage image generation strategy vividly reconstructs images seen by humans. Furthermore, we analyzed the impact of signals from different time windows and brain regions on decoding and reconstruction. The versatility of our framework is demonstrated in the magnetoencephalogram (MEG) data modality. We report that EEG-based visual decoding achieves SOTA performance, highlighting the portability, low cost, and high temporal resolution of EEG, enabling a wide range of BCI applications. The code of ATM is available at https://github.com/dongyangli-del/EEG_Image_decode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07721v4</guid>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongyang Li, Chen Wei, Shiying Li, Jiachen Zou, Quanying Liu</dc:creator>
    </item>
  </channel>
</rss>
