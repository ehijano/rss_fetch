<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Apr 2024 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multi-Objective Genetic Algorithm for Materialized View Optimization in Data Warehouses</title>
      <link>https://arxiv.org/abs/2403.19906</link>
      <description>arXiv:2403.19906v1 Announce Type: new 
Abstract: Materialized views can significantly improve database query performance but identifying the optimal set of views to materialize is challenging. Prior work on automating and optimizing materialized view selection has limitations in execution time and total cost. In this paper, we present a novel genetic algorithm based approach to materialized view selection that aims to minimize execution time and total cost. Our technique encodes materialized view configurations as chromosomes and evolves the population over generations to discover high quality solutions. We employ an adaptive mutation rate, multi-objective fitness function, and lexicase selection to enhance genetic search. Comprehensive experiments on the TPC-H benchmark demonstrate the effectiveness of our algorithm. Compared to stateof-the-art methods, our approach improves average execution time by 11% and reduces total materialized view costs by an average of 16 million. These gains highlight the benefits of a datadriven evolutionary approach. Our genetic algorithm framework significantly outperforms current materialized view selection techniques in both efficiency and total cost reduction. This work represents an important advance in enabling performant and cost-effective utilization of materialized views in enterprise systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19906v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahdi Manavi</dc:creator>
    </item>
    <item>
      <title>PURPLE: Making a Large Language Model a Better SQL Writer</title>
      <link>https://arxiv.org/abs/2403.20014</link>
      <description>arXiv:2403.20014v1 Announce Type: new 
Abstract: Large Language Model (LLM) techniques play an increasingly important role in Natural Language to SQL (NL2SQL) translation. LLMs trained by extensive corpora have strong natural language understanding and basic SQL generation abilities without additional tuning specific to NL2SQL tasks. Existing LLMs-based NL2SQL approaches try to improve the translation by enhancing the LLMs with an emphasis on user intention understanding. However, LLMs sometimes fail to generate appropriate SQL due to their lack of knowledge in organizing complex logical operator composition. A promising method is to input the LLMs with demonstrations, which include known NL2SQL translations from various databases. LLMs can learn to organize operator compositions from the input demonstrations for the given task. In this paper, we propose PURPLE (Pre-trained models Utilized to Retrieve Prompts for Logical Enhancement), which improves accuracy by retrieving demonstrations containing the requisite logical operator composition for the NL2SQL task on hand, thereby guiding LLMs to produce better SQL translation. PURPLE achieves a new state-of-the-art performance of 80.5% exact-set match accuracy and 87.8% execution match accuracy on the validation set of the popular NL2SQL benchmark Spider. PURPLE maintains high accuracy across diverse benchmarks, budgetary constraints, and various LLMs, showing robustness and cost-effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20014v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tonghui Ren, Yuankai Fan, Zhenying He, Ren Huang, Jiaqi Dai, Can Huang, Yinan Jing, Kai Zhang, Yifan Yang, X. Sean Wang</dc:creator>
    </item>
    <item>
      <title>Dataversifying Natural Sciences: Pioneering a Data Lake Architecture for Curated Data-Centric Experiments in Life \&amp; Earth Sciences</title>
      <link>https://arxiv.org/abs/2403.20063</link>
      <description>arXiv:2403.20063v1 Announce Type: new 
Abstract: This vision paper introduces a pioneering data lake architecture designed to meet Life \&amp; Earth sciences' burgeoning data management needs. As the data landscape evolves, the imperative to navigate and maximize scientific opportunities has never been greater. Our vision paper outlines a strategic approach to unify and integrate diverse datasets, aiming to cultivate a collaborative space conducive to scientific discovery.The core of the design and construction of a data lake is the development of formal and semi-automatic tools, enabling the meticulous curation of quantitative and qualitative data from experiments. Our unique ''research-in-the-loop'' methodology ensures that scientists across various disciplines are integrally involved in the curation process, combining automated, mathematical, and manual tasks to address complex problems, from seismic detection to biodiversity studies. By fostering reproducibility and applicability of research, our approach enhances the integrity and impact of scientific experiments. This initiative is set to improve data management practices, strengthening the capacity of Life \&amp; Earth sciences to solve some of our time's most critical environmental and biological challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20063v1</guid>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>8th International workshop on Data Analytics solutions for Real-LIfe APplications (DARLI-AP@EDBT/ICDT 2024), Mar 2024, Paestum, Italy</arxiv:journal_reference>
      <dc:creator>Genoveva Vargas-Solar (LIRIS), J\'er\^ome Darmont (ERIC), Alejandro Adorjan (LIRIS), Javier A. Espinosa-Oviedo (LIRIS), Carmem Hara (ERIC), Sabine Loudcher (ERIC), Regina Motz (DIMAP), Martin Musicante (DIMAP), Jos\'e-Luis Zechinelli-Martini</dc:creator>
    </item>
    <item>
      <title>HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding</title>
      <link>https://arxiv.org/abs/2403.19723</link>
      <description>arXiv:2403.19723v1 Announce Type: cross 
Abstract: Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU tasks.It leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training objectives.We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19723v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.MM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rihui Jin, Yu Li, Guilin Qi, Nan Hu, Yuan-Fang Li, Jiaoyan Chen, Jianan Wang, Yongrui Chen, Dehai Min</dc:creator>
    </item>
    <item>
      <title>Representing Knowledge and Querying Data using Double-Functorial Semantics</title>
      <link>https://arxiv.org/abs/2403.19884</link>
      <description>arXiv:2403.19884v1 Announce Type: cross 
Abstract: Category theory offers a mathematical foundation for knowledge representation and database systems. Popular existing approaches model a database instance as a functor into the category of sets and functions, or as a 2-functor into the 2-category of sets, relations, and implications. The functional and relational models are unified by double functors into the double category of sets, functions, relations, and implications. In an accessible, example-driven style, we show that the abstract structure of a 'double category of relations' is a flexible and expressive language in which to represent knowledge, and we show how queries on data in the spirit of Codd's relational algebra are captured by double-functorial semantics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19884v1</guid>
      <category>math.CT</category>
      <category>cs.DB</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lambert, Evan Patterson</dc:creator>
    </item>
    <item>
      <title>Efficient Unsupervised Community Search with Pre-trained Graph Transformer</title>
      <link>https://arxiv.org/abs/2403.18869</link>
      <description>arXiv:2403.18869v2 Announce Type: replace-cross 
Abstract: Community search has aroused widespread interest in the past decades. Among existing solutions, the learning-based models exhibit outstanding performance in terms of accuracy by leveraging labels to 1) train the model for community score learning, and 2) select the optimal threshold for community identification. However, labeled data are not always available in real-world scenarios. To address this notable limitation of learning-based models, we propose a pre-trained graph Transformer based community search framework that uses Zero label (i.e., unsupervised), termed TransZero. TransZero has two key phases, i.e., the offline pre-training phase and the online search phase. Specifically, in the offline pretraining phase, we design an efficient and effective community search graph transformer (CSGphormer) to learn node representation. To pre-train CSGphormer without the usage of labels, we introduce two self-supervised losses, i.e., personalization loss and link loss, motivated by the inherent uniqueness of node and graph topology, respectively. In the online search phase, with the representation learned by the pre-trained CSGphormer, we compute the community score without using labels by measuring the similarity of representations between the query nodes and the nodes in the graph. To free the framework from the usage of a label-based threshold, we define a new function named expected score gain to guide the community identification process. Furthermore, we propose two efficient and effective algorithms for the community identification process that run without the usage of labels. Extensive experiments over 10 public datasets illustrate the superior performance of TransZero regarding both accuracy and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18869v2</guid>
      <category>cs.SI</category>
      <category>cs.DB</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianwei Wang, Kai Wang, Xuemin Lin, Wenjie Zhang, Ying Zhang</dc:creator>
    </item>
  </channel>
</rss>
