<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Jun 2025 01:56:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Evaluation of "As-Intended" Vehicle Dynamics using the Active Inference Framework</title>
      <link>https://arxiv.org/abs/2506.00035</link>
      <description>arXiv:2506.00035v1 Announce Type: new 
Abstract: We constructed a computational model of the driver's brain for steering tasks using the active inference framework, grounded in the free energy principle - a theory from computational neuroscience. This model enables quantitative estimation of how accurately the brain learns vehicle dynamics and performs appropriate steering, using a measure called variational free energy. Through driving simulator experiments, we observed strong correlations between variational free energy and both expert drivers' subjective "as-intended" scores and general participants' objective control performance. These results suggest that variational free energy provides a promising quantitative metric for evaluating whether a vehicle behaves "as-intended."</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00035v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazuharu Kidera, Takuma Miyaguchi, Hideyoshi Yanagisawa</dc:creator>
    </item>
    <item>
      <title>Constructing a bridge between functioning of oscillatory neuronal networks and quantum-like cognition along with quantum-inspired computation and AI</title>
      <link>https://arxiv.org/abs/2506.00040</link>
      <description>arXiv:2506.00040v1 Announce Type: new 
Abstract: Quantum-like (QL) modeling, one of the outcomes of the quantum information revolution, extends quantum theory methods beyond physics to decision theory and cognitive psychology. While effective in explaining paradoxes in decision making and effects in cognitive psychology, such as conjunction, disjunction, order, and response replicability, it lacks a direct link to neural information processing in the brain. This study bridges neurophysiology, neuropsychology, and cognitive psychology, exploring how oscillatory neuronal networks give rise to QL behaviors. Inspired by the computational power of neuronal oscillations and quantum-inspired computation (QIC), we propose a quantum-theoretical framework for coupling of cognition/decision making and neural oscillations - {\it QL oscillatory cognition.} This is a step, may be very small, towards clarification of the relation between mind and matter and the nature of perception and cognition. We formulate four conjectures within QL oscillatory cognition and in principle they can checkedAsanoexperimentally. But such experimental tests need further theoretical and experimental elaboration. One of the conjectures (Conjecture 4) is on resolution of the binding problem by exploring QL states entanglement generated by the oscillations in a few neuronal networks. Our findings suggest that fundamental cognitive processes align with quantum principles, implying that humanoid AI should process information using quantum-theoretic laws. Quantum-Like AI (QLAI) can be efficiently realized via oscillatory networks performing QIC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00040v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrei Khrennikov, Atsushi Iriki, Irina Basieva</dc:creator>
    </item>
    <item>
      <title>Using LLMs to Advance the Cognitive Science of Collectives</title>
      <link>https://arxiv.org/abs/2506.00052</link>
      <description>arXiv:2506.00052v1 Announce Type: new 
Abstract: LLMs are already transforming the study of individual cognition, but their application to studying collective cognition has been underexplored. We lay out how LLMs may be able to address the complexity that has hindered the study of collectives and raise possible risks that warrant new methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00052v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilia Sucholutsky, Katherine M. Collins, Nori Jacoby, Bill D. Thompson, Robert D. Hawkins</dc:creator>
    </item>
    <item>
      <title>Human sensory-musculoskeletal modeling and control of whole-body movements</title>
      <link>https://arxiv.org/abs/2506.00071</link>
      <description>arXiv:2506.00071v1 Announce Type: new 
Abstract: Coordinated human movement depends on the integration of multisensory inputs, sensorimotor transformation, and motor execution, as well as sensory feedback resulting from body-environment interaction. Building dynamic models of the sensory-musculoskeletal system is essential for understanding movement control and investigating human behaviours. Here, we report a human sensory-musculoskeletal model, termed SMS-Human, that integrates precise anatomical representations of bones, joints, and muscle-tendon units with multimodal sensory inputs involving visual, vestibular, proprioceptive, and tactile components. A stage-wise hierarchical deep reinforcement learning framework was developed to address the inherent challenges of high-dimensional control in musculoskeletal systems with integrated multisensory information. Using this framework, we demonstrated the simulation of three representative movement tasks, including bipedal locomotion, vision-guided object manipulation, and human-machine interaction during bicycling. Our results showed a close resemblance between natural and simulated human motor behaviours. The simulation also revealed musculoskeletal dynamics that could not be directly measured. This work sheds deeper insights into the sensorimotor dynamics of human movements, facilitates quantitative understanding of human behaviours in interactive contexts, and informs the design of systems with embodied intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00071v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chenhui Zuo, Guohao Lin, Chen Zhang, Shanning Zhuang, Yanan Sui</dc:creator>
    </item>
    <item>
      <title>Autonomous Behavior and Whole-Brain Dynamics Emerge in Embodied Zebrafish Agents with Model-based Intrinsic Motivation</title>
      <link>https://arxiv.org/abs/2506.00138</link>
      <description>arXiv:2506.00138v1 Announce Type: new 
Abstract: Autonomy is a hallmark of animal intelligence, enabling adaptive and intelligent behavior in complex environments without relying on external reward or task structure. Existing reinforcement learning approaches to exploration in sparse reward and reward-free environments, including class of methods known as intrinsic motivation, exhibit inconsistent exploration patterns and thus fail to produce robust autonomous behaviors observed in animals. Moreover, systems neuroscience has largely overlooked the neural basis of autonomy, focusing instead on experimental paradigms where animals are motivated by external reward rather than engaging in unconstrained, naturalistic and task-independent behavior. To bridge these gaps, we introduce a novel model-based intrinsic drive explicitly designed to capture robust autonomous exploration observed in animals. Our method (3M-Progress) motivates naturalistic behavior by tracking divergence between the agent's current world model and an ethological prior. We demonstrate that artificial embodied agents trained with 3M-Progress capture the explainable variance in behavioral patterns and whole-brain neural-glial dynamics recorded from autonomously-behaving larval zebrafish, introducing the first goal-driven, population-level model of neural-glial computation. Our findings establish a computational framework connecting model-based intrinsic motivation to naturalistic behavior, providing a foundation for building artificial agents with animal-like autonomy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00138v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reece Keller, Alyn Tornell, Felix Pei, Xaq Pitkow, Leo Kozachkov, Aran Nayebi</dc:creator>
    </item>
    <item>
      <title>Reconstruction of Partial Dissimilarity Matrices for Cognitive Neuroscience</title>
      <link>https://arxiv.org/abs/2506.00484</link>
      <description>arXiv:2506.00484v2 Announce Type: new 
Abstract: In cognitive neuroscience research, Representational Dissimilarity Matrices (RDMs) are often incomplete because pairwise similarity judgments cannot always be exhaustively collected as the number of pairs rapidly increases with the number of conditions. Existing methods to fill these missing values, such as deep neural network imputation, are powerful but computationally demanding and relatively opaque. We introduce a simple algorithm based on geometric inference that fills missing dissimilarity matrix entries using known distances. We use tests on publicly available empirical cognitive neuroscience datasets, as well as simulations, to demonstrate the method's effectiveness and robustness across varying sparsity and matrix sizes. We have made this geometric reconstruction algorithm, implemented in Python and MATLAB, publicly available. This method provides a fast and accurate solution for completing partial dissimilarity matrices in the cognitive neurosciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00484v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denise Moerel, Tijl Grootswagers</dc:creator>
    </item>
    <item>
      <title>Driving factors of auditory category learning success</title>
      <link>https://arxiv.org/abs/2506.01508</link>
      <description>arXiv:2506.01508v1 Announce Type: new 
Abstract: Our brain learns to update its mental model of the environment by abstracting sensory experiences for adaptation and survival. Learning to categorize sounds is one essential abstracting process for high-level human cognition, such as speech perception, but it is also challenging due to the variable nature of auditory signals and their dynamic contexts. To overcome these learning challenges and enhance learner performance, it is essential to identify the impact of learning-related factors in developing better training protocols. Here, we conducted an extensive meta-analysis of auditory category learning studies, including a total of 111 experiments and 4,521 participants, and examined to what extent three hidden factors (i.e., variability, intensity, and engagement) derived from 12 experimental variables contributed to learning success (i.e., effect sizes). Variables related to intensity and training variability outweigh others in predicting learning effect size. Activation likelihood estimation (ALE) meta-analysis of the neuroimaging studies revealed training-induced systematic changes in the frontotemporal-parietal networks. Increased brain activities in speech and motor-related auditory-frontotemporal regions and decreased activities in cuneus and precuneus areas are associated with increased learning effect sizes. These findings not only enhance our understanding of the driving forces behind speech and auditory category learning success, along with its neural changes, but also guide researchers and practitioners in designing more effective training protocols that consider the three key aspects of learning to facilitate learner success.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01508v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nan Wang, Gangyi Feng</dc:creator>
    </item>
    <item>
      <title>EEG Foundation Models for BCI Learn Diverse Features of Electrophysiology</title>
      <link>https://arxiv.org/abs/2506.01867</link>
      <description>arXiv:2506.01867v1 Announce Type: new 
Abstract: Brain computer interface (BCI) research, as well as increasing portions of the field of neuroscience, have found success deploying large-scale artificial intelligence (AI) pre-training methods in conjunction with vast public repositories of data. This approach of pre-training foundation models using label-free, self-supervised objectives offers the potential to learn robust representations of neurophysiology, potentially addressing longstanding challenges in neural decoding. However, to date, much of this work has focused explicitly on standard BCI benchmarks and tasks, which likely overlooks the multitude of features these powerful methods might learn about brain function as well as other electrophysiological information. We introduce a new method for self-supervised BCI foundation model pre-training for EEG inspired by a transformer-based approach adapted from the HuBERT framework originally developed for speech processing. Our pipeline is specifically focused on low-profile, real-time usage, involving minimally pre-processed data and just eight EEG channels on the scalp. We show that our foundation model learned a representation of EEG that supports standard BCI tasks (P300, motor imagery), but also that this model learns features of neural data related to individual variability, and other salient electrophysiological components (e.g., alpha rhythms). In addition to describing and evaluating a novel approach to pre-training BCI models and neural decoding, this work opens the aperture for what kind of tasks and use-cases might exist for neural data in concert with powerful AI methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01867v1</guid>
      <category>q-bio.NC</category>
      <category>eess.SP</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mattson Ogg, Rahul Hingorani, Diego Luna, Griffin W. Milsap, William G. Coon, Clara A. Scholl</dc:creator>
    </item>
    <item>
      <title>Getting More from Less: Transfer Learning Improves Sleep Stage Decoding Accuracy in Peripheral Wearable Devices</title>
      <link>https://arxiv.org/abs/2506.00730</link>
      <description>arXiv:2506.00730v1 Announce Type: cross 
Abstract: Transfer learning, a technique commonly used in generative artificial intelligence, allows neural network models to bring prior knowledge to bear when learning a new task. This study demonstrates that transfer learning significantly enhances the accuracy of sleep-stage decoding from peripheral wearable devices by leveraging neural network models pretrained on electroencephalographic (EEG) signals. Consumer wearable technologies typically rely on peripheral physiological signals such as pulse plethysmography (PPG) and respiratory data, which, while convenient, lack the fidelity of clinical electroencephalography (EEG) for detailed sleep-stage classification. We pretrained a transformer-based neural network on a large, publicly available EEG dataset and subsequently fine-tuned this model on noisier peripheral signals. Our transfer learning approach improved overall classification accuracy from 67.6\% (baseline model trained solely on peripheral signals) to 76.6\%. Notable accuracy improvements were observed across sleep stages, particularly lighter sleep stages such as REM and N1. These results highlight transfer learning's potential to substantially enhance the accuracy and utility of consumer wearable devices without altering existing hardware. Future integration of self-supervised learning methods may further boost performance, facilitating more precise, longitudinal sleep monitoring for personalized health applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00730v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>William G Coon, Diego Luna, Akshita Panagrahi, Matthew Reid, Mattson Ogg</dc:creator>
    </item>
    <item>
      <title>EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG</title>
      <link>https://arxiv.org/abs/2506.00854</link>
      <description>arXiv:2506.00854v1 Announce Type: cross 
Abstract: We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one of the earliest open-vocabulary EEG-to-text generation frameworks tailored for Chinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), our architecture aligns multichannel brain signals with natural language representations via masked pretraining and contrastive learning. Using a subset of the ChineseEEG dataset, where each sentence contains approximately ten Chinese characters aligned with 128-channel EEG recorded at 256 Hz, we segment EEG into per-character embeddings and predict full sentences in a zero-shot setting. The decoder is trained with teacher forcing and padding masks to accommodate variable-length sequences. Evaluation on over 1,500 training-validation sentences and 300 held-out test samples shows promising lexical alignment, with a best BLEU-1 score of 6.38\%. While syntactic fluency remains a challenge, our findings demonstrate the feasibility of non-phonetic, cross-modal language decoding from EEG. This work opens a new direction in multilingual brain-to-text research and lays the foundation for future cognitive-language interfaces in Chinese.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00854v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacky Tai-Yu Lu, Jung Chiang, Chi-Sheng Chen, Anna Nai-Yun Tung, Hsiang Wei Hu, Yuan Chiao Cheng</dc:creator>
    </item>
    <item>
      <title>Latent Structured Hopfield Network for Semantic Association and Retrieval</title>
      <link>https://arxiv.org/abs/2506.01303</link>
      <description>arXiv:2506.01303v1 Announce Type: cross 
Abstract: Episodic memory enables humans to recall past experiences by associating semantic elements such as objects, locations, and time into coherent event representations. While large pretrained models have shown remarkable progress in modeling semantic memory, the mechanisms for forming associative structures that support episodic memory remain underexplored. Inspired by hippocampal CA3 dynamics and its role in associative memory, we propose the Latent Structured Hopfield Network (LSHN), a biologically inspired framework that integrates continuous Hopfield attractor dynamics into an autoencoder architecture. LSHN mimics the cortical-hippocampal pathway: a semantic encoder extracts compact latent representations, a latent Hopfield network performs associative refinement through attractor convergence, and a decoder reconstructs perceptual input. Unlike traditional Hopfield networks, our model is trained end-to-end with gradient descent, achieving scalable and robust memory retrieval. Experiments on MNIST, CIFAR-10, and a simulated episodic memory task demonstrate superior performance in recalling corrupted inputs under occlusion and noise, outperforming existing associative memory models. Our work provides a computational perspective on how semantic elements can be dynamically bound into episodic memory traces through biologically grounded attractor mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01303v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chong Li, Xiangyang Xue, Jianfeng Feng, Taiping Zeng</dc:creator>
    </item>
    <item>
      <title>GenDMR: A dynamic multimodal role-swapping network for identifying risk gene phenotypes</title>
      <link>https://arxiv.org/abs/2506.01456</link>
      <description>arXiv:2506.01456v1 Announce Type: cross 
Abstract: Recent studies have shown that integrating multimodal data fusion techniques for imaging and genetic features is beneficial for the etiological analysis and predictive diagnosis of Alzheimer's disease (AD). However, there are several critical flaws in current deep learning methods. Firstly, there has been insufficient discussion and exploration regarding the selection and encoding of genetic information. Secondly, due to the significantly superior classification value of AD imaging features compared to genetic features, many studies in multimodal fusion emphasize the strengths of imaging features, actively mitigating the influence of weaker features, thereby diminishing the learning of the unique value of genetic features. To address this issue, this study proposes the dynamic multimodal role-swapping network (GenDMR). In GenDMR, we develop a novel approach to encode the spatial organization of single nucleotide polymorphisms (SNPs), enhancing the representation of their genomic context. Additionally, to adaptively quantify the disease risk of SNPs and brain region, we propose a multi-instance attention module to enhance model interpretability. Furthermore, we introduce a dominant modality selection module and a contrastive self-distillation module, combining them to achieve a dynamic teacher-student role exchange mechanism based on dominant and auxiliary modalities for bidirectional co-updating of different modal data. Finally, GenDMR achieves state-of-the-art performance on the ADNI public dataset and visualizes attention to different SNPs, focusing on confirming 12 potential high-risk genes related to AD, including the most classic APOE and recently highlighted significant risk genes. This demonstrates GenDMR's interpretable analytical capability in exploring AD genetic features, providing new insights and perspectives for the development of multimodal data fusion techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01456v1</guid>
      <category>q-bio.GN</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lina Qin, Cheng Zhu, Chuqi Zhou, Yukun Huang, Jiayi Zhu, Ping Liang, Jinju Wang, Yixing Huang, Cheng Luo, Dezhong Yao, Ying Tan</dc:creator>
    </item>
    <item>
      <title>Engram Memory Encoding and Retrieval: A Neurocomputational Perspective</title>
      <link>https://arxiv.org/abs/2506.01659</link>
      <description>arXiv:2506.01659v1 Announce Type: cross 
Abstract: Despite substantial research into the biological basis of memory, the precise mechanisms by which experiences are encoded, stored, and retrieved in the brain remain incompletely understood. A growing body of evidence supports the engram theory, which posits that sparse populations of neurons undergo lasting physical and biochemical changes to support long-term memory. Yet, a comprehensive computational framework that integrates biological findings with mechanistic models remains elusive. This work synthesizes insights from cellular neuroscience and computational modeling to address key challenges in engram research: how engram neurons are identified and manipulated; how synaptic plasticity mechanisms contribute to stable memory traces; and how sparsity promotes efficient, interference-resistant representations. Relevant computational approaches -- such as sparse regularization, engram gating, and biologically inspired architectures like Sparse Distributed Memory and spiking neural networks -- are also examined. Together, these findings suggest that memory efficiency, capacity, and stability emerge from the interaction of plasticity and sparsity constraints. By integrating neurobiological and computational perspectives, this paper provides a comprehensive theoretical foundation for engram research and proposes a roadmap for future inquiry into the mechanisms underlying memory, with implications for the diagnosis and treatment of memory-related disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01659v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Szelogowski</dc:creator>
    </item>
    <item>
      <title>A stochastic explanation for observed local-to-global foraging states in Caenorhabditis elegans</title>
      <link>https://arxiv.org/abs/2309.15174</link>
      <description>arXiv:2309.15174v4 Announce Type: replace 
Abstract: Abrupt changes in behavior can often be associated with changes in underlying behavioral states. When placed off food, the foraging behavior of C. elegans can be described as a change between an initial local-search behavior characterized by a high rate of reorientations, followed by a global-search behavior characterized by sparse reorientations. This is commonly observed in individual worms, but when numerous worms are characterized, only about half appear to exhibit this behavior. We propose an alternative model that predicts both abrupt and continuous changes to reorientation that does not rely on behavioral states. This model is inspired by molecular dynamics modeling that defines the foraging reorientation rate as a decaying parameter. By stochastically sampling from the probability distribution defined by this rate, both abrupt and gradual changes to reorientation rates can occur, matching experimentally observed results. Crucially, this model does not depend on behavioral states or information accumulation. Even though abrupt behavioral changes do occur, they may not necessarily be indicative of abrupt changes in behavioral states, especially when abrupt changes are not universally observed in the population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15174v4</guid>
      <category>q-bio.NC</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrew Margolis, Andrew Gordus</dc:creator>
    </item>
    <item>
      <title>The Fragility of the Reverse Facilitation Effect in the Stroop Task: A Dynamic Neurocognitive Model</title>
      <link>https://arxiv.org/abs/2503.19128</link>
      <description>arXiv:2503.19128v2 Announce Type: replace 
Abstract: In typical Stroop experiments, participants perform better when the colors and words are congruent compared to when they are incongruent or neutral. Paradoxically, in some experimental conditions, neutral trials are faster than congruent trials. This phenomenon is known as Reverse Facilitation Effect (RFE). However, RFE has not been consistently replicated, leading to the so-called fragile results. There are some models that capture this effect, but they are not parsimonious. Here we employed our previous model of priming effect, without its conflict monitoring module, to demonstrate that RFE, including its fragility, is mainly due to attentional dynamics with limited resources. The simulations of the RFE resulted from weak attentional activation for neutral stimuli (e.g., non-color or nonword strings) that causes less attentional refraction for the target stimuli (e.g., font colors). The refractory period or attenuation of attention usually happens when meaningful stimuli (color name primes) are used along font color targets. This simpler model provides a straightforward understanding of the underlying processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19128v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmad Sohrabi, Robert L. West</dc:creator>
    </item>
    <item>
      <title>Multihead self-attention in cortico-thalamic circuits</title>
      <link>https://arxiv.org/abs/2504.06354</link>
      <description>arXiv:2504.06354v2 Announce Type: replace 
Abstract: Both biological cortico-thalamic networks and artificial transformer networks use canonical computations to perform a wide range of cognitive tasks. In this work, we propose that the structure of cortico-thalamic circuits is well suited to realize a computation analogous to multihead self-attention, the main algorithmic innovation of transformers. We start with the concept of a cortical unit module or microcolumn, and propose that superficial and deep pyramidal cells carry distinct computational roles. Specifically, superficial pyramidal cells encode an attention mask applied onto deep pyramidal cells to compute attention-modulated values. We show how to wire such microcolumns into a circuit equivalent to a single head of self-attention. We then suggest the parallel between one head of attention and a cortical area. On this basis, we show how to wire cortico-thalamic circuits to perform multihead self-attention. Along these constructions, we refer back to existing experimental data, and find noticeable correspondence. Finally, as a first step towards a mechanistic theory of synaptic learning in this framework, we formally derive gradient-based updates for the parameters of a multihead linear self-attention block and propose steps towards their implementation by local synaptic plasticity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06354v2</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Granier, Walter Senn</dc:creator>
    </item>
    <item>
      <title>Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain</title>
      <link>https://arxiv.org/abs/2505.18361</link>
      <description>arXiv:2505.18361v3 Announce Type: replace 
Abstract: Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy.
  For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18361v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Tue, 03 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Trinity Chung, Yuchen Shen, Nathan C. L. Kong, Aran Nayebi</dc:creator>
    </item>
  </channel>
</rss>
