<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 12 Nov 2025 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Testing noisy low-degree polynomials for sparsity</title>
      <link>https://arxiv.org/abs/2511.07835</link>
      <description>arXiv:2511.07835v1 Announce Type: new 
Abstract: We consider the problem of testing whether an unknown low-degree polynomial $p$ over $\mathbb{R}^n$ is sparse versus far from sparse, given access to noisy evaluations of the polynomial $p$ at \emph{randomly chosen points}. This is a property-testing analogue of classical problems on learning sparse low-degree polynomials with noise, extending the work of Chen, De, and Servedio (2020) from noisy \emph{linear} functions to general low-degree polynomials.
  Our main result gives a \emph{precise characterization} of when sparsity testing for low-degree polynomials admits constant sample complexity independent of dimension, together with a matching constant-sample algorithm in that regime. For any mean-zero, variance-one finitely supported distribution $\boldsymbol{X}$ over the reals, degree $d$, and any sparsity parameters $s \leq T$, we define a computable function $\mathrm{MSG}_{\boldsymbol{X},d}(\cdot)$, and:
  - For $T \ge \mathrm{MSG}_{\boldsymbol{X},d}(s)$, we give an $O_{s,\boldsymbol{X},d}(1)$-sample algorithm that distinguishes whether a multilinear degree-$d$ polynomial over $\mathbb{R}^n$ is $s$-sparse versus $\varepsilon$-far from $T$-sparse, given examples $(\boldsymbol{x},\, p(\boldsymbol{x}) + \mathrm{noise})_{\boldsymbol{x} \sim \boldsymbol{X}^{\otimes n}}$. Crucially, the sample complexity is \emph{completely independent} of the ambient dimension $n$.
  - For $T \leq \mathrm{MSG}_{\boldsymbol{X},d}(s) - 1$, we show that even without noise, any algorithm given samples $(\boldsymbol{x},p(\boldsymbol{x}))_{\boldsymbol{x} \sim \boldsymbol{X}^{\otimes n}}$ must use $\Omega_{\boldsymbol{X},d,s}(\log n)$ examples.
  Our techniques employ a generalization of the results of Dinur et al. (2007) on the Fourier tails of bounded functions over $\{0,1\}^n$ to a broad range of finitely supported distributions, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07835v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiqiao Bao, Anindya De, Shivam Nadimpalli, Rocco A. Servedio, Nathan White</dc:creator>
    </item>
    <item>
      <title>Model-agnostic super-resolution in high dimensions</title>
      <link>https://arxiv.org/abs/2511.07846</link>
      <description>arXiv:2511.07846v1 Announce Type: new 
Abstract: The problem of \emph{super-resolution}, roughly speaking, is to reconstruct an unknown signal to high accuracy, given (potentially noisy) information about its low-degree Fourier coefficients. Prior results on super-resolution have imposed strong modeling assumptions on the signal, typically requiring that it is a linear combination of spatially separated point sources.
  In this work we analyze a very general version of the super-resolution problem, by considering completely general signals over the $d$-dimensional torus $[0,1)^d$; we do not assume any spatial separation between point sources, or even that the signal is a finite linear combination of point sources. We obtain two sets of results, corresponding to two natural notions of reconstruction.
  - {\bf Reconstruction in Wasserstein distance:} We give essentially matching upper and lower bounds on the cutoff frequency $T$ and the magnitude $\kappa$ of the noise for which accurate reconstruction in Wasserstein distance is possible. Roughly speaking, our results here show that for $d$-dimensional signals, estimates of $\approx \exp(d)$ many Fourier coefficients are necessary and sufficient for accurate reconstruction under the Wasserstein distance.
  - {\bf "Heavy hitter" reconstruction:} For nonnegative signals (equivalently, probability distributions), we introduce a new notion of "heavy hitter" reconstruction that essentially amounts to achieving high-accuracy reconstruction of all "sufficiently dense" regions of the distribution. Here too we give essentially matching upper and lower bounds on the cutoff frequency $T$ and the magnitude $\kappa$ of the noise for which accurate reconstruction is possible. Our results show that -- in sharp contrast with Wasserstein reconstruction -- accurate estimates of only $\approx \exp(\sqrt{d})$ many Fourier coefficients are necessary and sufficient for heavy hitter reconstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07846v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Chen, Anindya De, Yizhi Huang, Shivam Nadimpalli, Rocco A. Servedio, Tianqi Yang</dc:creator>
    </item>
    <item>
      <title>Deterministic Padded Decompositions and Negative-Weight Shortest Paths</title>
      <link>https://arxiv.org/abs/2511.07859</link>
      <description>arXiv:2511.07859v1 Announce Type: new 
Abstract: We obtain the first near-linear time deterministic algorithm for negative-weight single-source shortest paths on integer-weighted graphs. Our main ingredient is a deterministic construction of a padded decomposition on directed graphs, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07859v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Li</dc:creator>
    </item>
    <item>
      <title>Parallel Sampling via Autospeculation</title>
      <link>https://arxiv.org/abs/2511.07869</link>
      <description>arXiv:2511.07869v1 Announce Type: new 
Abstract: We present parallel algorithms to accelerate sampling via counting in two settings: any-order autoregressive models and denoising diffusion models. An any-order autoregressive model accesses a target distribution $\mu$ on $[q]^n$ through an oracle that provides conditional marginals, while a denoising diffusion model accesses a target distribution $\mu$ on $\mathbb{R}^n$ through an oracle that provides conditional means under Gaussian noise. Standard sequential sampling algorithms require $\widetilde{O}(n)$ time to produce a sample from $\mu$ in either setting. We show that, by issuing oracle calls in parallel, the expected sampling time can be reduced to $\widetilde{O}(n^{1/2})$. This improves the previous $\widetilde{O}(n^{2/3})$ bound for any-order autoregressive models and yields the first parallel speedup for diffusion models in the high-accuracy regime, under the relatively mild assumption that the support of $\mu$ is bounded.
  We introduce a novel technique to obtain our results: speculative rejection sampling. This technique leverages an auxiliary ``speculative'' distribution~$\nu$ that approximates~$\mu$ to accelerate sampling. Our technique is inspired by the well-studied ``speculative decoding'' techniques popular in large language models, but differs in key ways. Firstly, we use ``autospeculation,'' namely we build the speculation $\nu$ out of the same oracle that defines~$\mu$. In contrast, speculative decoding typically requires a separate, faster, but potentially less accurate ``draft'' model $\nu$. Secondly, the key differentiating factor in our technique is that we make and accept speculations at a ``sequence'' level rather than at the level of single (or a few) steps. This last fact is key to unlocking our parallel runtime of $\widetilde{O}(n^{1/2})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07869v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nima Anari, Carlo Baronio, CJ Chen, Alireza Haqi, Frederic Koehler, Anqi Li, Thuy-Duong Vuong</dc:creator>
    </item>
    <item>
      <title>Forgetting Alternation and Blossoms: A New Framework for Fast Matching Augmentation and Its Applications to Sequential/Distributed/Streaming Computation</title>
      <link>https://arxiv.org/abs/2511.08210</link>
      <description>arXiv:2511.08210v1 Announce Type: new 
Abstract: Finding a maximum cardinality matching in a graph is one of the most fundamental problems. An algorithm proposed by Micali and Vazirani (1980) is well-known to solve the problem in $O(m\sqrt{n})$ time, which is still one of the fastest algorithms in general. While the MV algorithm itself is not so complicated and is indeed convincing, its correctness proof is extremely challenging, which can be seen from the history: after the first algorithm paper had appeared in 1980, Vazirani has made several attempts to give a complete proof for more than 40 years. It seems, roughly speaking, caused by the nice but highly complex structure of the shortest alternating paths in general graphs that are deeply intertwined with the so-called (nested) blossoms.
  In this paper, we propose a new structure theorem on the shortest alternating paths in general graphs without taking into the details of blossoms. The high-level idea is to forget the alternation (of matching and non-matching edges) as early as possible. A key ingredient is a notion of alternating base trees (ABTs) introduced by Izumi, Kitamura, and Yamaguchi (2024) to develop a nearly linear-time distributed algorithm. Our structure theorem refines the properties of ABTs exploited in their algorithm, and we also give simpler alternative proofs for them. Based on our structure theorem, we propose a new algorithm, which is slightly slower but more implementable and much easier to confirm its correctness than the MV algorithm.
  As applications of our framework, we also present new $(1 - \epsilon)$-approximation algorithms in the distributed and semi-streaming settings. Both algorithms are deterministic, and substantially improve the best known upper bounds on the running time. The algorithms are built on the top of a novel framework of amplifying approximation factors of given matchings, which is of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08210v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>math.CO</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taisuke Izumi, Naoki Kitamura, Yutaro Yamaguchi</dc:creator>
    </item>
    <item>
      <title>Fully Dynamic Set Cover: Worst-Case Recourse and Update Time</title>
      <link>https://arxiv.org/abs/2511.08485</link>
      <description>arXiv:2511.08485v1 Announce Type: new 
Abstract: In (fully) dynamic set cover, the goal is to maintain an approximately optimal solution to a dynamically evolving instance of set cover, where in each step either an element is added to or removed from the instance. The two main desiderata of a dynamic set cover algorithm are to minimize at each time-step, the recourse, which is the number of sets removed from or added to the solution, and the update time to compute the updated solution. This problem has been extensively studied over the last decade leading to many results that achieve ever-improving bounds on the recourse and update time, while maintaining a solution whose cost is comparable to that of offline approximation algorithms.
  In this paper, we give the first algorithms to simultaneously achieve non-trivial worst-case bounds for recourse and update time. Specifically, we give fully-dynamic set cover algorithms that simultaneously achieve $O(\log n)$ recourse and $f\cdot \textrm{poly}\log(n)$ update time in the worst-case, for both approximation regimes: $O(\log n)$ and $O(f)$ approximation. (Here, $n, f$ respectively denote the maximum number of elements and maximum frequency of an element across all instances.) Prior to our work, all results for this problem either settled for amortized bounds on recourse and update time, or obtained $f\cdot \textrm{poly}\log(n)$ update time in the worst-case but at the cost of $\Omega(m)$ worst-case recourse. (Here, $m$ denotes the number of sets. Note that any algorithm has recourse at most $m$.)</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08485v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayan Bhattacharya, Ruoxu Cen, Debmalya Panigrahi</dc:creator>
    </item>
    <item>
      <title>Deterministic Negative-Weight Shortest Paths in Nearly Linear Time via Path Covers</title>
      <link>https://arxiv.org/abs/2511.08551</link>
      <description>arXiv:2511.08551v1 Announce Type: new 
Abstract: We present the first deterministic nearly-linear time algorithm for single-source shortest paths with negative edge weights on directed graphs: given a directed graph $G$ with $n$ vertices, $m$ edges whose weights are integer in $\{-W,\dots,W\}$, our algorithm either computes all distances from a source $s$ or reports a negative cycle in time $\tilde{O}(m)\cdot \log(nW)$ time.
  All known near-linear time algorithms for this problem have been inherently randomized, as they crucially rely on low-diameter decompositions.
  To overcome this barrier, we introduce a new structural primitive for directed graphs called the path cover. This plays a role analogous to neighborhood covers in undirected graphs, which have long been central to derandomizing algorithms that use low-diameter decomposition in the undirected setting. We believe that path covers will serve as a fundamental tool for the design of future deterministic algorithms on directed graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08551v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernhard Haeupler, Yonggang Jiang, Thatchaphol Saranurak</dc:creator>
    </item>
    <item>
      <title>Universal Connection Schedules for Reconfigurable Networking</title>
      <link>https://arxiv.org/abs/2511.08556</link>
      <description>arXiv:2511.08556v1 Announce Type: new 
Abstract: Reconfigurable networks are a novel communication paradigm in which the pattern of connectivity between hosts varies rapidly over time. Prior theoretical work explored the inherent tradeoffs between throughput (or, hop-count) and latency, and showed the existence of infinitely many Pareto-optimal designs as the network size tends to infinity. Existing Pareto-optimal designs use a connection schedule which is fine-tuned to the desired hop-count $h$, permitting lower latency as $h$ increases. However, in reality datacenter workloads contain a mix of low-latency and high-latency requests. Using a connection schedule fine-tuned for one request type leads to inefficiencies when serving other types.
  A more flexible and efficient alternative is a {\em universal schedule}, a single connection schedule capable of attaining many Pareto-optimal tradeoff points simultaneously, merely by varying the choice of routing paths. In this work we present the first universal schedules for oblivious routing. Our constructions yield universal schedules which are near-optimal for all possible hop-counts $h$. The key technical idea is to specialize to a type of connection schedule based on cyclic permutations and to develop a novel Fourier-analytic method for analyzing randomized routing on these connection schedules. We first show that a uniformly random connection schedule suffices with multiplicative error in throughput, and latency optimal up to a $\log N$ factor. We then show that a more carefully designed random connection schedule suffices with additive error in throughput, but improved latency optimal up to only constant factors. Finally, we show that our first randomized construction can be made deterministic using a derandomized version of the Lovett-Meka discrepancy minimization algorithm to obtain the same result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08556v1</guid>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaleen Baral, Robert Kleinberg, Sylvan Martin, Henry Rogers, Tegan Wilson, Ruogu Zhang</dc:creator>
    </item>
    <item>
      <title>A New Initial Approximation Bound in the Durand Kerner Algorithm for Finding Polynomial Zeros</title>
      <link>https://arxiv.org/abs/2511.07728</link>
      <description>arXiv:2511.07728v1 Announce Type: cross 
Abstract: The Durand-Kerner algorithm is a widely used iterative technique for simultaneously finding all the roots of a polynomial. However, its convergence heavily depends on the choice of initial approximations. This paper introduces two novel approaches for determining the initial values: New bound 1 and the lambda maximal bound, aimed at improving the stability and convergence speed of the algorithm. Theoretical analysis and numerical experiments were conducted to evaluate the effectiveness of these bounds. The lambda maximal bound consistently ensures that all the roots lie within the complex circle, leading to faster and more stable convergence. Comparative results demonstrate that while New bound 1 guarantees convergence, but it yields excessively large radii.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07728v1</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>B. A. Sanjoyo, M. Yunus, N. Hidayat</dc:creator>
    </item>
    <item>
      <title>Graph Classes Closed under Self-intersection</title>
      <link>https://arxiv.org/abs/2511.08011</link>
      <description>arXiv:2511.08011v1 Announce Type: cross 
Abstract: A graph class is monotone if it is closed under taking subgraphs. It is known that a monotone class defined by finitely many obstructions has bounded treewidth if and only if one of the obstructions is a so-called tripod, that is, a disjoint union of trees with exactly one vertex of degree 3 and paths. This dichotomy also characterizes exactly those monotone graph classes for which many NP-hard algorithmic problems admit polynomial-time algorithms. These algorithmic dichotomies, however, do not extend to the universe of all hereditary classes, which are classes closed under taking induced subgraphs. This leads to the natural question of whether we can extend known algorithmic dichotomies for monotone classes to larger families of hereditary classes. We give an affirmative answer to this question by considering the family of hereditary graph classes that are closed under self-intersection, which is known to be located strictly between the monotone and hereditary classes. We prove a new structural characterization of graphs in self-intersection-closed classes excluding a tripod. We use our characterization to give a complete dichotomy of Maximum Independent Set, and its weighted variant for self-intersection-closed classes defined by finitely many obstructions: these problems are in P if the class excludes a tripod and NP-hard otherwise. This generalizes several known results on Maximum Independent Set. We also use it to obtain dichotomies for Maximum Induced Matching on self-intersection-closed classes of bipartite graphs defined by finitely many obstructions. Similarly, we obtain dichotomies for Satisfiability and Counting Satisfiability on self-intersection-closed classes of (bipartite) incidence graphs defined by finitely many obstructions, and for boundedness of clique-width for self-intersection-closed classes of bipartite graphs defined by finitely many obstructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08011v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konrad K. Dabrowski, Vadim V. Lozin, Martin Milani\v{c}, Andrea Munaro, Dani\"el Paulusma, Viktor Zamaraev</dc:creator>
    </item>
    <item>
      <title>Fair Multi-agent Persuasion with Submodular Constraints</title>
      <link>https://arxiv.org/abs/2511.08538</link>
      <description>arXiv:2511.08538v1 Announce Type: cross 
Abstract: We study the problem of selection in the context of Bayesian persuasion. We are given multiple agents with hidden values (or quality scores), to whom resources must be allocated by a welfare-maximizing decision-maker. An intermediary with knowledge of the agents' values seeks to influence the outcome of the selection by designing informative signals and providing tie-breaking policies, so that when the receiver maximizes welfare over the resulting posteriors, the expected utilities of the agents (where utility is defined as allocation times value) achieve certain fairness properties. The fairness measure we will use is majorization, which simultaneously approximately maximizes all symmetric, monotone, concave functions of the utilities. We consider the general setting where the allocation to the agents needs to respect arbitrary submodular constraints, as given by the corresponding polymatroid.
  We present a signaling policy that, under a mild bounded rationality assumption on the receiver, achieves a logarithmically approximate majorized policy in this setting. The approximation ratio is almost best possible, and that significantly outperforms generic results that only yield linear approximations. A key component of our result is a structural characterization showing that the vector of agent utilities for a given signaling policy defines the base polytope of a different polymatroid, a result that may be of independent interest. In addition, we show that an arbitrarily good additive approximation to this vector can be produced in (weakly) polynomial time via the multiplicative weights update method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08538v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannan Bai, Kamesh Munagala, Yiheng Shen, Davidson Zhu</dc:creator>
    </item>
    <item>
      <title>Parameterized Spanning Tree Congestion</title>
      <link>https://arxiv.org/abs/2410.08314</link>
      <description>arXiv:2410.08314v3 Announce Type: replace 
Abstract: In this paper we study the Spanning Tree Congestion problem, where we are given a graph $G=(V,E)$ and are asked to find a spanning tree $T$ of minimum maximum congestion. Here, the congestion of an edge $e\in T$ is the number of edges $uv\in E$ such that the (unique) path from $u$ to $v$ in $T$ traverses $e$. We consider this well-studied NP-hard problem from the point of view of (structural) parameterized complexity and obtain the following results.
  We resolve a natural open problem by showing that Spanning Tree Congestion is not FPT parameterized by treewidth (under standard assumptions). More strongly, we present a generic reduction which applies to (almost) any parameter of the form ``vertex-deletion distance to class $\mathcal{C}$'', thus obtaining W[1]-hardness for parameters more restricted than treewidth, including tree-depth plus feedback vertex set, or incomparable to treewidth, such as twin cover. Via a slight tweak of the same reduction we also show that the problem is NP-complete on graphs of modular-width $4$.
  Even though it is known that Spanning Tree Congestion remains NP-hard on instances with only one vertex of unbounded degree, it is currently open whether the problem remains hard on bounded-degree graphs. We resolve this question by showing NP-hardness on graphs of maximum degree 8.
  Complementing the problem's W[1]-hardness for treewidth...</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08314v3</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lampis, Valia Mitsou, Edouard Nemery, Yota Otachi, Manolis Vasilakis, Daniel Vaz</dc:creator>
    </item>
    <item>
      <title>How fast can you find a good hypothesis?</title>
      <link>https://arxiv.org/abs/2509.03734</link>
      <description>arXiv:2509.03734v2 Announce Type: replace 
Abstract: In the hypothesis selection problem, we are given sample and query access to finite set of candidate distributions (hypotheses), $\mathcal{H} = \{H_1, \ldots, H_n\}$, and samples from an unknown distribution $P$, both over a domain $\mathcal{X}$. The goal is to output a distribution $Q$ whose distance to $P$ is comparable to that of the nearest hypothesis in $\mathcal{H}$. Specifically, if the minimum distance is $\mathsf{OPT}$, we aim to output $Q$ such that, with probability at least $1-\delta$, its total variation distance to $P$ is at most $C \cdot \mathsf{OPT} + \varepsilon$. The optimal approximation for proper algorithms (where $Q \in \mathcal{H}$) is $C=3$ using $\Theta(\log(n/\delta)/\varepsilon^2)$ samples from $P$ and for improper algorithms (where $Q$ is not necessarily in $\mathcal{H}$) is $C=2$ using $\tilde{\Theta}(\log(n/\delta)/\varepsilon^2)$ samples from $P$.
  In the improper setting, the algorithm achieving $C=2$ [Bousquet, Braverman, Kol, Efremenko, Moran, FOCS 2021] runs in time which grows polynomially with $|\mathcal{X}|$ -- it does not run in finite time for real-valued distributions. A promising path towards improved runtime is to consider improper algorithms which output a mixture $Q$ of the hypotheses as such a distribution can be represented in $n$ words of memory. We show (1) a lower bound that no algorithm which outputs a mixture can achieve approximation better than $C = 3-2/n$ unless the number of samples is polynomial in $|\mathcal{X}|$, as well as (2) an algorithm which runs in time $\text{poly}(n)$ and achieves the same approximation guarantee.
  In the proper setting, [Aliakbarpour, Bun, Smith, NeurIPS 2024] provided an algorithm with $C=3$ running in $\tilde{O}(n/(\delta^3\varepsilon^3))$ time. We improve this time complexity to $\tilde{O}(n/(\delta \varepsilon^2))$, significantly reducing the dependence on the confidence and error parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03734v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anders Aamand, Maryam Aliakbarpour, Justin Y. Chen, Sandeep Silwal</dc:creator>
    </item>
    <item>
      <title>A High-Throughput GPU Framework for Adaptive Lossless Compression of Floating-Point Data</title>
      <link>https://arxiv.org/abs/2511.04140</link>
      <description>arXiv:2511.04140v2 Announce Type: replace-cross 
Abstract: The torrential influx of floating-point data from domains like IoT and HPC necessitates high-performance lossless compression to mitigate storage costs while preserving absolute data fidelity. Leveraging GPU parallelism for this task presents significant challenges, including bottlenecks in heterogeneous data movement, complexities in executing precision-preserving conversions, and performance degradation due to anomaly-induced sparsity. To address these challenges, this paper introduces a novel GPU-based framework for floating-point adaptive lossless compression. The proposed solution employs three key innovations: a lightweight asynchronous pipeline that effectively hides I/O latency during CPU-GPU data transfer; a fast and theoretically guaranteed float-to-integer conversion method that eliminates errors inherent in floating-point arithmetic; and an adaptive sparse bit-plane encoding strategy that mitigates the sparsity caused by outliers. Extensive experiments on 12 diverse datasets demonstrate that the proposed framework significantly outperforms state-of-the-art competitors, achieving an average compression ratio of 0.299 (a 9.1% relative improvement over the best competitor), an average compression throughput of 10.82 GB/s (2.4x higher), and an average decompression throughput of 12.32 GB/s (2.4x higher).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04140v2</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Li (Chongqing University), Weiyan Wang (Chongqing University), Ruiyuan Li (Chongqing University), Chao Chen (Chongqing University), Xianlei Long (Chongqing University), Linjiang Zheng (Chongqing University), Quanqing Xu (OceanBase, Ant Group), Chuanhui Yang (OceanBase, Ant Group)</dc:creator>
    </item>
  </channel>
</rss>
