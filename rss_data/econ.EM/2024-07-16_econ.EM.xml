<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Jul 2024 01:59:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Short Note on Event-Study Synthetic Difference-in-Differences Estimators</title>
      <link>https://arxiv.org/abs/2407.09565</link>
      <description>arXiv:2407.09565v1 Announce Type: new 
Abstract: I propose an event study extension of Synthetic Difference-in-Differences (SDID) estimators. I show that, in simple and staggered adoption designs, estimators from Arkhangelsky et al. (2021) can be disaggregated into dynamic treatment effect estimators, comparing the lagged outcome differentials of treated and synthetic controls to their pre-treatment average. Estimators presented in this note can be computed using the sdid_event Stata package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09565v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Diego Ciccia</dc:creator>
    </item>
    <item>
      <title>Regularizing stock return covariance matrices via multiple testing of correlations</title>
      <link>https://arxiv.org/abs/2407.09696</link>
      <description>arXiv:2407.09696v1 Announce Type: new 
Abstract: This paper develops a large-scale inference approach for the regularization of stock return covariance matrices. The framework allows for the presence of heavy tails and multivariate GARCH-type effects of unknown form among the stock returns. The approach involves simultaneous testing of all pairwise correlations, followed by setting non-statistically significant elements to zero. This adaptive thresholding is achieved through sign-based Monte Carlo resampling within multiple testing procedures, controlling either the traditional familywise error rate, a generalized familywise error rate, or the false discovery proportion. Subsequent shrinkage ensures that the final covariance matrix estimate is positive definite and well-conditioned while preserving the achieved sparsity. Compared to alternative estimators, this new regularization method demonstrates strong performance in simulation experiments and real portfolio optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09696v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jeconom.2024.105753</arxiv:DOI>
      <dc:creator>Richard Luger</dc:creator>
    </item>
    <item>
      <title>Estimation of Integrated Volatility Functionals with Kernel Spot Volatility Estimators</title>
      <link>https://arxiv.org/abs/2407.09759</link>
      <description>arXiv:2407.09759v1 Announce Type: new 
Abstract: For a multidimensional It\^o semimartingale, we consider the problem of estimating integrated volatility functionals. Jacod and Rosenbaum (2013) studied a plug-in type of estimator based on a Riemann sum approximation of the integrated functional and a spot volatility estimator with a forward uniform kernel. Motivated by recent results that show that spot volatility estimators with general two-side kernels of unbounded support are more accurate, in this paper, an estimator using a general kernel spot volatility estimator as the plug-in is considered. A biased central limit theorem for estimating the integrated functional is established with an optimal convergence rate. Unbiased central limit theorems for estimators with proper de-biasing terms are also obtained both at the optimal convergence regime for the bandwidth and when applying undersmoothing. Our results show that one can significantly reduce the estimator's bias by adopting a general kernel instead of the standard uniform kernel. Our proposed bias-corrected estimators are found to maintain remarkable robustness against bandwidth selection in a variety of sampling frequencies and functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09759v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e E. Figueroa-L\'opez, Jincheng Pang, Bei Wu</dc:creator>
    </item>
    <item>
      <title>The Dynamic, the Static, and the Weak factor models and the analysis of high-dimensional time series</title>
      <link>https://arxiv.org/abs/2407.10653</link>
      <description>arXiv:2407.10653v1 Announce Type: new 
Abstract: Several fundamental and closely interconnected issues related to factor models are reviewed and discussed: dynamic versus static loadings, rate-strong versus rate-weak factors, the concept of weakly common component recently introduced by Gersing et al. (2023), the irrelevance of cross-sectional ordering and the assumption of cross-sectional exchangeability, and the problem of undetected strong factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10653v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Marc Hallin</dc:creator>
    </item>
    <item>
      <title>An Introduction to Permutation Processes (version 0.5)</title>
      <link>https://arxiv.org/abs/2407.09664</link>
      <description>arXiv:2407.09664v1 Announce Type: cross 
Abstract: These lecture notes were prepared for a special topics course in the Department of Statistics at the University of Washington, Seattle. They comprise the first eight chapters of a book currently in progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09664v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fang Han</dc:creator>
    </item>
    <item>
      <title>Sparse Asymptotic PCA: Identifying Sparse Latent Factors Across Time Horizon</title>
      <link>https://arxiv.org/abs/2407.09738</link>
      <description>arXiv:2407.09738v1 Announce Type: cross 
Abstract: This paper proposes a novel method for sparse latent factor modeling using a new sparse asymptotic Principal Component Analysis (APCA). This approach analyzes the co-movements of large-dimensional panel data systems over time horizons within a general approximate factor model framework. Unlike existing sparse factor modeling approaches based on sparse PCA, which assume sparse loading matrices, our sparse APCA assumes that factor processes are sparse over the time horizon, while the corresponding loading matrices are not necessarily sparse. This development is motivated by the observation that the assumption of sparse loadings may not be appropriate for financial returns, where exposure to market factors is generally universal and non-sparse. We propose a truncated power method to estimate the first sparse factor process and a sequential deflation method for multi-factor cases. Additionally, we develop a data-driven approach to identify the sparsity of risk factors over the time horizon using a novel cross-sectional cross-validation method. Theoretically, we establish that our estimators are consistent under mild conditions. Monte Carlo simulations demonstrate that the proposed method performs well in finite samples. Empirically, we analyze daily stock returns for a balanced panel of S&amp;P 500 stocks from January 2004 to December 2016. Through textual analysis, we examine specific events associated with the identified sparse factors that systematically influence the stock market. Our approach offers a new pathway for economists to study and understand the systematic risks of economic and financial systems over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09738v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxing Gao</dc:creator>
    </item>
    <item>
      <title>Low Volatility Stock Portfolio Through High Dimensional Bayesian Cointegration</title>
      <link>https://arxiv.org/abs/2407.10175</link>
      <description>arXiv:2407.10175v1 Announce Type: cross 
Abstract: We employ a Bayesian modelling technique for high dimensional cointegration estimation to construct low volatility portfolios from a large number of stocks. The proposed Bayesian framework effectively identifies sparse and important cointegration relationships amongst large baskets of stocks across various asset spaces, resulting in portfolios with reduced volatility. Such cointegration relationships persist well over the out-of-sample testing time, providing practical benefits in portfolio construction and optimization. Further studies on drawdown and volatility minimization also highlight the benefits of including cointegrated portfolios as risk management instruments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10175v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <category>q-fin.PM</category>
      <category>q-fin.ST</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parley R Yang, Alexander Y Shestopaloff</dc:creator>
    </item>
    <item>
      <title>A nonparametric test for rough volatility</title>
      <link>https://arxiv.org/abs/2407.10659</link>
      <description>arXiv:2407.10659v1 Announce Type: cross 
Abstract: We develop a nonparametric test for deciding whether volatility of an asset follows a standard semimartingale process, with paths of finite quadratic variation, or a rough process with paths of infinite quadratic variation. The test utilizes the fact that volatility is rough if and only if volatility increments are negatively autocorrelated at high frequencies. It is based on the sample autocovariance of increments of spot volatility estimates computed from high-frequency asset return data. By showing a feasible CLT for this statistic under the null hypothesis of semimartingale volatility paths, we construct a test with fixed asymptotic size and an asymptotic power equal to one. The test is derived under very general conditions for the data-generating process. In particular, it is robust to jumps with arbitrary activity and to the presence of market microstructure noise. In an application of the test to SPY high-frequency data, we find evidence for rough volatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10659v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>q-fin.MF</category>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carsten H. Chong, Viktor Todorov</dc:creator>
    </item>
    <item>
      <title>Regressions under Adverse Conditions</title>
      <link>https://arxiv.org/abs/2311.13327</link>
      <description>arXiv:2311.13327v2 Announce Type: replace 
Abstract: We introduce a new regression method that relates the mean of an outcome variable to covariates, given the "adverse condition" that a distress variable falls in its tail. This allows to tailor classical mean regressions to adverse economic scenarios, which receive increasing interest in managing macroeconomic and financial risks, among many others. In the terminology of the systemic risk literature, our method can be interpreted as a regression for the Marginal Expected Shortfall. We propose a two-step procedure to estimate the new models, show consistency and asymptotic normality of the estimator, and propose feasible inference under weak conditions allowing for cross-sectional and time series applications. The accuracy of the asymptotic approximations of the two-step estimator is verified in simulations. Two empirical applications show that our regressions under adverse conditions are valuable in such diverse fields as the study of the relation between systemic risk and asset price bubbles, and dissecting macroeconomic growth vulnerabilities into individual components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13327v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Dimitriadis, Yannick Hoga</dc:creator>
    </item>
  </channel>
</rss>
