<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Nov 2025 02:40:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Beyond the ACE Score: Replicable Combinations of Adverse Childhood Experiences That Worsen Depression Risk</title>
      <link>https://arxiv.org/abs/2511.19574</link>
      <description>arXiv:2511.19574v1 Announce Type: new 
Abstract: Adverse childhood experiences (ACEs) are categories of childhood abuse, neglect, and household dysfunction. Screening by a single additive ACE score (e.g., a $\ge 4$ cutoff) has poor individual-level discrimination. We instead identify replicable combinations of ACEs that elevate adult depression risk. Our data turnover framework enables a single research team to explore, confirm, and replicate within one observational dataset while controlling the family-wise error rate. We integrate isotonic subgroup selection (ISS) to estimate a higher-risk subgroup under a monotonicity assumption -- additional ACE exposure or higher intensity cannot reduce depression risk. We pre-specify a risk threshold $\tau$ corresponding to roughly a two-fold increase in the odds of depression relative to the no-ACE baseline. Within data turnover, the prespecified component improves power while maintaining FWER control, as demonstrated in simulations. Guided by EDA, we adopt frequency coding for ACE items, retaining intensity information that reduces false positives relative to binary or score codings. The result is a replicable, pattern-based higher-risk subgroup. On held-out BRFSS 2022, we show that, at the same level of specificity (0.95), using our replicable subgroup as the screening rule increases sensitivity by 26\% compared with an ACE-score cutoff, yielding concrete triggers that are straightforward to implement and help target scarce clinical screening resources toward truly higher-risk profiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19574v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruizhe Zhang, Jooyoung Kong, Dylan S. Small, William Bekerman</dc:creator>
    </item>
    <item>
      <title>A Win-Expectancy Framework for Contextualizing Runs Batted In: Introducing ARBI and CRBI</title>
      <link>https://arxiv.org/abs/2511.19642</link>
      <description>arXiv:2511.19642v1 Announce Type: new 
Abstract: Runs Batted IN (RBI) records the number of runs a hitter directly drives in during their plate appearances and reflects a batter's ability to convert opportunities into scoring. Because producing runs determines game outcomes, RBI has long served as a central statistic in evaluating offensive performance. However, traditional RBI treats all batted-in runs equally and ignores th game context in which they occur, such as leverage, score state, and the actual impact of a run on a team's chance of winning. In this paper, we introduce two new context-aware metrics-Adjusted RBI (ARBI) and Contextual RBI (CRBI)-that address the fundamental limitations of RBI by incorporating Win Expectancy (WE). ARBI rescales each RBI according to the change in WE before and after the scoring event, assigning more value to runs that meaningfully shift the likelihood of winning and less to runs scored in low-leverage situations. We then extend this framework to CRBI, which further differentiates RBIs with the same WE change by accounting for the terminal WE at the end of the event. This refinement captures the idea that an RBI increasing WE from, for example, 0.45 to 0.65 has a larger competitive impact than one increasing WE from 0.05 to 0.25, even though both represent a 20% increase. Together, ARBI and CRBI provide calibrated, context-sensitive measures of offensive contribution that more accurately reflect the true value of run production. These metrics modernize the interpretation of RBI and have broad applications in player evaluation, forecasting, contract evaluation, and decision-making in baseball analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19642v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wuhuan Deng</dc:creator>
    </item>
    <item>
      <title>Introducing Discipline Score Based on League Overall Swinging Probability</title>
      <link>https://arxiv.org/abs/2511.19672</link>
      <description>arXiv:2511.19672v1 Announce Type: new 
Abstract: Plate discipline is an important feature of a hitter's success. Hitter who are able to recognize good pitches to swing at and balls to take are generally recognized as disciplined hitters. Although there are some metrics that can provide insight into the patience of a hitter, most do not capture the ability of a batter to take balls. In this research, we introduce two new metrics, Discipline Score (DS) and Adjusted Discipline Score (ADS), which evaluate batters' discipline when the pitch is a ball compared with the predicted tendencies of all batters in the league.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19672v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wuhuan Deng, Scott Nestler</dc:creator>
    </item>
    <item>
      <title>Anchoring Convenience Survey Samples to a Baseline Census for Vaccine Coverage Monitoring in Global Health</title>
      <link>https://arxiv.org/abs/2511.19742</link>
      <description>arXiv:2511.19742v1 Announce Type: new 
Abstract: While conducting probabilistic surveys is the gold standard for assessing vaccine coverage, implementing these surveys poses challenges for global health. There is a need for more convenient option that is more affordable and practical. Motivated by childhood vaccine monitoring programs in rural areas of Chad and Niger, we conducted a simulation study to evaluate calibration-weighted design-based and logistic regression-based imputation estimators of the finite-population proportion of MCV1 coverage. These estimators use a hybrid approach that anchors non-probabilistic follow-up survey to probabilistic baseline census to account for selection bias. We explored varying degrees of non-ignorable selection bias (odds ratios from 1.0-1.5), percentage of villages sampled (25-75%), and village-level survey response rate to the follow-up survey (50-80%). Our performance metrics included bias, coverage, and proportion of simulated 95% confidence intervals falling within equivalence margins of 5% and 7.5% (equivalence tolerance). For both adjustment methods, the performance worsened with higher selection bias and lower response rate and generally improved as a larger proportion of villages was sampled. Under the worst scenario with 1.5 OR, 25% village sampled, and 50% survey response rate, both methods showed empirical biases of 2.1% or less, below 95% coverage, and low equivalence tolerances. In more realistic scenarios, the performance of our estimators showed lower biases and close to 95% coverage. For example, at OR$\leq$1.2, both methods showed high performance, except at the lowest village sampling and participation rates. Our simulations show that a hybrid anchoring survey approach is a feasible survey option for vaccine monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19742v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Dyrkton, Shomoita Alam, Susan Shepherd, Ibrahim Sana, Kevin Phelan, Jay JH Park</dc:creator>
    </item>
    <item>
      <title>Non-stationarities in extreme hourly precipitation over the Piave Basin, northern Italy</title>
      <link>https://arxiv.org/abs/2511.20069</link>
      <description>arXiv:2511.20069v1 Announce Type: new 
Abstract: We study the spatio-temporal features of extremal sub-daily precipitation data over the Piave river basin in northeast Italy using a rich database of observed hourly rainfall. Empirical evidence suggests that both the marginal and dependence structures for extreme precipitation in the area exhibit seasonal patterns, and spatial dependence appears to weaken as events become more extreme. We investigate factors affecting the marginal distributions, the spatial dependence and the interplay between them. Capturing these features is essential to provide a realistic description of extreme precipitation processes in order to better estimate their associated risks. With this aim, we identify various climatic covariates at different spatio-temporal scales and explore their usefulness. We go beyond existing literature by investigating and comparing the performance of recently proposed covariate-dependent models for both the marginal and dependence structures of extremes. Furthermore, a flexible max-id model, which encompasses both asymptotic dependence and independence, is used to learn about the spatio-temporal variability of rainfall processes at extreme levels. We find that modelling non-stationarity only at the marginal level does not fully capture the variability of precipitation extremes, and that it is important to also capture the seasonal variation of extremal dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20069v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D\'aire Healy, Ilaria Prosdocimi, Isadora Antoniano-Villalobos</dc:creator>
    </item>
    <item>
      <title>Efficient multi-fidelity Gaussian process regression for noisy outputs and non-nested experimental designs</title>
      <link>https://arxiv.org/abs/2511.20183</link>
      <description>arXiv:2511.20183v1 Announce Type: new 
Abstract: This paper presents a multi-fidelity Gaussian process surrogate modeling that generalizes the recursive formulation of the auto-regressive model when the high-fidelity and low-fidelity data sets are noisy and not necessarily nested. The estimation of high-fidelity parameters by the EM (expectation-maximization) algorithm is shown to be still possible in this context and a closed-form update formula is derived when the scaling factor is a parametric linear predictor function. This yields a decoupled optimization strategy for the parameter selection that is more efficient and scalable than the direct maximum likelihood maximization. The proposed approach is compared to other multi-fidelity models, and benchmarks for different application cases of increasing complexity are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20183v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Baillie, Baptiste Kerleguer, Cyril Feau, Josselin Garnier</dc:creator>
    </item>
    <item>
      <title>Investigating access to support centers for Violence Against Women in Apulia: A Spatial analysis over multiple years</title>
      <link>https://arxiv.org/abs/2511.20481</link>
      <description>arXiv:2511.20481v1 Announce Type: new 
Abstract: In this study, we address the challenge of modelling the spatial variability in violence against women across municipalities in a Southern Italian region by proposing a Bayesian spatio-temporal Poisson regression model. Using data on access to Local Anti-Violence Centers in the Apulia region from 2021 to 2024, we investigate the impact of municipality-level socioeconomic characteristics and local vulnerabilities on both the incidence and reporting of gender-based violence. To explicitly account for spatial dependence, we compare four spatial models within the Integrated Nested Laplace Approximation framework for Bayesian model estimation. We assess the relative fit of the competing models, discussing their prior assumptions, spatial confounding effects, and inferential implications. Our findings indicate that access to support services decreases with distance from the residential municipality, highlighting spatial constraints in reporting and the strategic importance of support center location. Furthermore, lower education levels appear to contribute to under-reporting in disadvantaged areas, while higher economic development may be associated with a lower incidence of reported violence. This study emphasises the critical role of spatial modelling in capturing reporting dynamics and informing policy interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20481v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo Cefalo, Crescenza Calculli, Alessio Pollice</dc:creator>
    </item>
    <item>
      <title>Discovering Spatial Patterns of Readmission Risk Using a Bayesian Competing Risks Model with Spatially Varying Coefficients</title>
      <link>https://arxiv.org/abs/2511.20616</link>
      <description>arXiv:2511.20616v1 Announce Type: new 
Abstract: Time-to-event models are commonly used to study associations between risk factors and disease outcomes in the setting of electronic health records (EHR). In recent years, focus has intensified on social determinants of health, highlighting the need for methods that account for patients' locations. We propose a Bayesian approach for introducing point-referenced spatial effects into a competing risks proportional hazards model. Our method leverages Gaussian process (GP) priors for spatially varying intercept and slope. To improve computational efficiency under a large number of spatial locations, we implemented a Hilbert space low-rank approximation of the GP. We modeled the baseline hazard curves as piecewise constant, and introduced a novel multiplicative gamma process prior to induce shrinkage and smoothing. A loss-based clustering method was then used on the spatial random effects to identify high-risk regions. We demonstrate the utility of this method through simulation and a real-world analysis of EHR data from Duke Hospital to study readmission risk of elderly patients with upper extremity fractures. Our results showed that the proposed method improved inference efficiency and provided valuable insights for downstream policy decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20616v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yueming Shen, Christian Pean, David Dunson, Samuel Berchuck</dc:creator>
    </item>
    <item>
      <title>Heckman Selection Contaminated Normal Model</title>
      <link>https://arxiv.org/abs/2409.12348</link>
      <description>arXiv:2409.12348v1 Announce Type: cross 
Abstract: The Heckman selection model is one of the most well-renounced econometric models in the analysis of data with sample selection. This model is designed to rectify sample selection biases based on the assumption of bivariate normal error terms. However, real data diverge from this assumption in the presence of heavy tails and/or atypical observations. Recently, this assumption has been relaxed via a more flexible Student's t-distribution, which has appealing statistical properties. This paper introduces a novel Heckman selection model using a bivariate contaminated normal distribution for the error terms. We present an efficient ECM algorithm for parameter estimation with closed-form expressions at the E-step based on truncated multinormal distribution formulas. The identifiability of the proposed model is also discussed, and its properties have been examined. Through simulation studies, we compare our proposed model with the normal and Student's t counterparts and investigate the finite-sample properties and the variation in missing rate. Results obtained from two real data analyses showcase the usefulness and effectiveness of our model. The proposed algorithms are implemented in the R package HeckmanEM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12348v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Heeju Lim, Jose Alejandro Ordonez, Victor H. Lachos, Antonio Punzo</dc:creator>
    </item>
    <item>
      <title>Parallelism in Neurodegenerative Biomarker Tests: Hidden Errors and the Risk of Misconduct</title>
      <link>https://arxiv.org/abs/2511.19549</link>
      <description>arXiv:2511.19549v1 Announce Type: cross 
Abstract: Biomarkers are critical tools in the diagnosis and monitoring of neurodegenerative diseases. Reliable quantification depends on assay validity, especially the demonstration of parallelism between diluted biological samples and the assay's standard curve. Inadequate parallelism can lead to biased concentration estimates, jeopardizing both clinical and research applications. Here we systematically review the evidence of analytical parallelism in body fluid (serum, plasma, cerebrospinal fluid) biomarker assays for neurodegeneration and evaluate the extent, reproducibility, and reporting quality of partial parallelism.
  This systematic review was registered on PROSPERO (CRD42024568766) and conducted in accordance with PRISMA guidelines. We included studies published between December 2010 to July 2024 without language restrictions. ...
  In conclusion, partial parallelism was infrequently observed and inconsistently reported in most biomarker assays for neurodegeneration. Narrow dilution ranges and variable methodologies limit generalizability. Transparent reporting of dilution protocols and adherence to established analytical validation guidelines is needed. This systematic review has practical implications for clinical trial design, regulatory approval processes, and the reliability of biomarker-based diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19549v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10408363.2025.2583078</arxiv:DOI>
      <arxiv:journal_reference>Critical Reviews in Clinical Laboratory Sciences 2025</arxiv:journal_reference>
      <dc:creator>Axel Petzold, Joachim Pum, David P Crabb</dc:creator>
    </item>
    <item>
      <title>Concept drift of simple forecast models as a diagnostic of low-frequency, regime-dependent atmospheric reorganisation</title>
      <link>https://arxiv.org/abs/2511.19638</link>
      <description>arXiv:2511.19638v1 Announce Type: cross 
Abstract: Data-driven weather prediction models implicitly assume that the statistical relationship between predictors and targets is stationary. Under anthropogenic climate change, this assumption is violated, yet the structure of the resulting concept drift remains poorly understood. Here we introduce concept drift of simple forecast models as a diagnostic of atmospheric reorganisation. Using ERA5 reanalysis, we quantify drift in spatially explicit linear models of daily mean sea-level pressure and 2\,m temperature. Models are trained on the 1950s and 2000s and evaluated on 2020 tp 2024; their performance difference defines a local, interpretable drift metric. By decomposing errors by frequency band, circulation regime and region, and by mapping drift globally, we show that drift is dominated by low-frequency variability and is strongly regime-dependent. Over the North Atlantic-European sector, low-frequency drift peaks in positive NAO despite a stable large-scale NAO pattern, while Western European summer temperature drift is tightly linked to changes in land-atmosphere coupling rather than mean warming alone. In winter, extreme high-pressure frequencies increase mainly in neutral and negative NAO, whereas structural drift is concentrated in positive NAO and Alpine hotspots. Benchmarking against variance-based diagnostics shows that drift aligns much more with changes in temporal persistence than with changes in volatility or extremes. These findings demonstrate that concept drift can serve as a physically meaningful diagnostic of evolving predictability, revealing aspects of atmospheric reorganisation that are invisible to standard deviation and storm-track metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19638v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haokun Zhou</dc:creator>
    </item>
    <item>
      <title>Clarifying identification and estimation of treatment effects in the Sequential Parallel Comparison Design</title>
      <link>https://arxiv.org/abs/2511.19677</link>
      <description>arXiv:2511.19677v1 Announce Type: cross 
Abstract: Sequential parallel comparison design (SPCD) clinical trials aim to adjust active treatment effect estimates for placebo response to minimize the impact of placebo responders on the estimates. This is potentially accomplished using a two stage design by measuring treatment effects among all participants during the first stage, then classifying some placebo arm participants as placebo non-responders who will be re-randomized in the second stage. In this paper, we use causal inference tools to clarify under what assumptions treatment effects can be identified in SPCD trials and what effects the conventional estimators target at each stage of the SPCD trial. We further illustrate the highly influential impact of placebo response misclassification on the second stage estimate. We conclude that the conventional SPCD estimators do not target meaningful treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19677v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Stockton, Michele Santacatterina, Soutrik Mandal, Charles M. Cleland, Erinn M. Hade, Nicholas Illenberger, Sharon Meropol, Andrea B. Troxel, Eva Petkova, Chang Yu, Thaddeus Tarpey</dc:creator>
    </item>
    <item>
      <title>Clustering Approaches for Mixed-Type Data: A Comparative Study</title>
      <link>https://arxiv.org/abs/2511.19755</link>
      <description>arXiv:2511.19755v1 Announce Type: cross 
Abstract: Clustering is widely used in unsupervised learning to find homogeneous groups of observations within a dataset. However, clustering mixed-type data remains a challenge, as few existing approaches are suited for this task. This study presents the state-of-the-art of these approaches and compares them using various simulation models. The compared methods include the distance-based approaches k-prototypes, PDQ, and convex k-means, and the probabilistic methods KAy-means for MIxed LArge data (KAMILA), the mixture of Bayesian networks (MBNs), and latent class model (LCM). The aim is to provide insights into the behavior of different methods across a wide range of scenarios by varying some experimental factors such as the number of clusters, cluster overlap, sample size, dimension, proportion of continuous variables in the dataset, and clusters' distribution. The degree of cluster overlap and the proportion of continuous variables in the dataset and the sample size have a significant impact on the observed performances. When strong interactions exist between variables alongside an explicit dependence on cluster membership, none of the evaluated methods demonstrated satisfactory performance. In our experiments KAMILA, LCM, and k-prototypes exhibited the best performance, with respect to the adjusted rand index (ARI). All the methods are available in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19755v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1155/jpas/2242100</arxiv:DOI>
      <arxiv:journal_reference>Journal of Probability and Statistics, 2025</arxiv:journal_reference>
      <dc:creator>Badih Ghattas, Alvaro Sanchez San-Benito</dc:creator>
    </item>
    <item>
      <title>Threshold Tensor Factor Model in CP Form</title>
      <link>https://arxiv.org/abs/2511.19796</link>
      <description>arXiv:2511.19796v1 Announce Type: cross 
Abstract: This paper proposes a new Threshold Tensor Factor Model in Canonical Polyadic (CP) form for tensor time series. By integrating a thresholding autoregressive structure for the latent factor process into the tensor factor model in CP form, the model captures regime-switching dynamics in the latent factor processes while retaining the parsimony and interpretability of low-rank tensor representations. We develop estimation procedures for the model and establish the theoretical properties of the resulting estimators. Numerical experiments and a real-data application illustrate the practical performance and usefulness of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19796v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stevenson Bolivar, Rong Chen, Yuefeng Han</dc:creator>
    </item>
    <item>
      <title>Time-Varying Network Driver Estimation (TNDE) Quantifies Stage-Specific Regulatory Effects From Single-Cell Snapshots</title>
      <link>https://arxiv.org/abs/2511.19813</link>
      <description>arXiv:2511.19813v1 Announce Type: cross 
Abstract: Identifying key driver genes governing biological processes such as development and disease progression remains a challenge. While existing methods can reconstruct cellular trajectories or infer static gene regulatory networks (GRNs), they often fail to quantify time-resolved regulatory effects within specific temporal windows. Here, we present Time-varying Network Driver Estimation (TNDE), a computational framework quantifying dynamic gene driver effects from single-cell snapshot data under a linear Markov assumption. TNDE leverages a shared graph attention encoder to preserve the local topological structure of the data. Furthermore, by incorporating partial optimal transport, TNDE accounts for unmatched cells arising from proliferation or apoptosis, thereby enabling trajectory alignment in non-equilibrium processes. Benchmarking on simulated datasets demonstrates that TNDE outperforms existing baseline methods across diverse complex regulatory scenarios. Applied to mouse erythropoiesis data, TNDE identifies stage-specific driver genes, the functional relevance of which is corroborated by biological validation. TNDE offers an effective quantitative tool for dissecting dynamic regulatory mechanisms underlying complex biological processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19813v1</guid>
      <category>q-bio.MN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxin Li, Shanjun Mao</dc:creator>
    </item>
    <item>
      <title>Institutional Learning and Volatility Transmission in ASEAN Equity Markets: A Network-Integrated Regime-Dependent Approach</title>
      <link>https://arxiv.org/abs/2511.19824</link>
      <description>arXiv:2511.19824v1 Announce Type: cross 
Abstract: This paper investigates how institutional learning and regional spillovers shape volatility dynamics in ASEAN equity markets. Using daily data for Indonesia, Malaysia, the Philippines, and Thailand from 2010 to 2024, we construct a high-frequency institutional learning index via a MIDAS-EPU approach. Unlike existing studies that treat institutional quality as a static background characteristic, this paper models institutions as a dynamic mechanism that reacts to policy shocks, information pressure, and crisis events. Building on this perspective, we introduce two new volatility frameworks: the Institutional Response Dynamics Model (IRDM), which embeds crisis memory, policy shocks, and information flows; and the Network-Integrated IRDM (N-IRDM), which incorporates dynamic-correlation and institutional-similarity networks to capture cross-market transmission. Empirical results show that institutional learning amplifies short-run sensitivity to shocks yet accelerates post-crisis normalization. Crisis-memory terms explain prolonged volatility clustering, while network interactions improve tail behavior and short-horizon forecasts. Robustness checks using placebo and lagged networks indicate that spillovers reflect a strong regional common factor rather than dependence on specific correlation topologies. Diebold-Mariano and ENCNEW tests confirm that the N-IRDM significantly outperforms baseline GARCH benchmarks. The findings highlight a dual role of institutions and offer policy insights on transparency enhancement, macroprudential communication, and coordinated regional governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19824v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junlin Yang</dc:creator>
    </item>
    <item>
      <title>Rethinking Nonstationarity in Time Series: A Deterministic Trend Perspective</title>
      <link>https://arxiv.org/abs/2506.07987</link>
      <description>arXiv:2506.07987v3 Announce Type: replace 
Abstract: This paper challenges the dominance of stochastic trend models by introducing the Seasonal-Trend-Stationary ARMA (STSA) framework, which represents univariate nonstationary time series as stationary fluctuations around deterministic trend and seasonal components, allowing for a finite number of structural breaks in the trend. We present methods for estimating the locations and number of breaks using a dynamic programming algorithm and a sequential prediction-interval-based procedure, respectively, and outline strategies for specifying and estimating the full model. Empirical analysis of U.S. exports of goods to Mainland China (2006-2025) demonstrates that the STSA model accurately identifies structural breaks linked to major economic events and provides a meaningful decomposition of the underlying economic cycle dynamics. Evaluation on the monthly M4 Competition data shows that STSA significantly outperforms Prophet and, while generally less accurate than stochastic trend models such as ARIMA, ETS, TBATS, and Theta, it produces superior forecasts for series with abrupt structural breaks where stochastic approaches struggle to adapt. Unlike traditional time series models, STSA offers an interpretable decomposition that reveals the causal narrative behind the series' evolution, enhancing the credibility of out-of-sample forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07987v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhandos Abdikhadir, Terence Tai Leung Chong</dc:creator>
    </item>
    <item>
      <title>How to Find Fantastic AI Papers: Self-Rankings as a Powerful Predictor of Scientific Impact Beyond Peer Review</title>
      <link>https://arxiv.org/abs/2510.02143</link>
      <description>arXiv:2510.02143v2 Announce Type: replace 
Abstract: Peer review in academic research aims not only to ensure factual correctness but also to identify work of high scientific potential that can shape future research directions. This task is especially critical in fast-moving fields such as artificial intelligence (AI), yet it has become increasingly difficult given the rapid growth of submissions. In this paper, we investigate an underexplored measure for identifying high-impact research: authors' own rankings of their multiple submissions to the same AI conference. Grounded in game-theoretic reasoning, we hypothesize that self-rankings are informative because authors possess unique understanding of their work's conceptual depth and long-term promise. To test this hypothesis, we conducted a large-scale experiment at a leading AI conference, where 1,342 researchers self-ranked their 2,592 submissions by perceived quality. Tracking outcomes over more than a year, we found that papers ranked highest by their authors received twice as many citations as their lowest-ranked counterparts; self-rankings were especially effective at identifying highly cited papers (those with over 150 citations). Moreover, we showed that self-rankings outperformed peer review scores in predicting future citation counts. Our results remained robust after accounting for confounders such as preprint posting time and self-citations. Together, these findings demonstrate that authors' self-rankings provide a reliable and valuable complement to peer review for identifying and elevating high-impact research in AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02143v2</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Buxin Su, Natalie Collina, Garrett Wen, Didong Li, Kyunghyun Cho, Jianqing Fan, Bingxin Zhao, Weijie Su</dc:creator>
    </item>
    <item>
      <title>Dynamical Systems Models for Market Evolution: A Mechanistic Alternative to Autoregressive Methods</title>
      <link>https://arxiv.org/abs/2510.06778</link>
      <description>arXiv:2510.06778v3 Announce Type: replace-cross 
Abstract: We present a novel approach to modeling market dynamics using ordinary differential equations that explicitly incorporates product competitiveness and consumer behavior. Our framework treats market segments as interacting populations in a dynamical system analogous to predator-prey models, where competitive advantages drive market share transitions through mechanistic modeling of market flows including new product adoption, refresh cycles, and obsolescence dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06778v3</guid>
      <category>math.DS</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aparna Komarla, Max Hill</dc:creator>
    </item>
    <item>
      <title>Flexible unimodal density estimation in hidden Markov models</title>
      <link>https://arxiv.org/abs/2511.17071</link>
      <description>arXiv:2511.17071v2 Announce Type: replace-cross 
Abstract: 1. Hidden Markov models (HMMs) are powerful tools for modelling time-series data with underlying state structure. However, selecting appropriate parametric forms for the state-dependent distributions is often challenging and can lead to model misspecification. To address this, P-spline-based nonparametric estimation of state-dependent densities has been proposed. While offering great flexibility, these approaches can result in overly complex densities (e.g. bimodal) that hinder interpretability. 2. We propose a straightforward method that builds on shape-constrained spline theory to enforce unimodality in the estimated state-dependent densities through enforcing unimodality of the spline coefficients. This constraint strikes a practical balance between model flexibility, interpretability, and parsimony. 3. Through two simulation studies and a real-world case study using narwhal (Monodon monoceros) dive data, we demonstrate the proposed approach yields more stable estimates compared to fully flexible, unconstrained models improving model performance and interpretability. 4. Our method bridges a key methodological gap, by providing a parsimonious HMM framework that balances the interpretability of parametric models with the flexibility of nonparametric estimation. This provides ecologists with a powerful tool to derive ecologically meaningful inference from telemetry data while avoiding the pitfalls of overly complex models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17071v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan-Ole Koslik, Fanny Dupont, Marie Auger-M\'eth\'e, Marianne Marcoux, Nigel Hussey, Nancy Heckman</dc:creator>
    </item>
  </channel>
</rss>
