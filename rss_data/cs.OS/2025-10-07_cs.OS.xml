<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.OS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.OS</link>
    <description>cs.OS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.OS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Oct 2025 04:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Early Exploration of Deep-Learning-Driven Prefetching for Far Memory</title>
      <link>https://arxiv.org/abs/2510.04360</link>
      <description>arXiv:2510.04360v1 Announce Type: new 
Abstract: Far-memory systems, where applications store less-active data in more energy-efficient memory media, are increasingly adopted by data centers. However, applications are bottlenecked by on-demand data fetching from far- to local-memory. We present Memix, a far-memory system that embodies a deep-learning-system co-design for efficient and accurate prefetching, minimizing on-demand far-memory accesses. One key observation is that memory accesses are shaped by both application semantics and runtime context, providing an opportunity to optimize each independently. Preliminary evaluation of Memix on data-intensive workloads shows that it outperforms the state-of-the-art far-memory system by up to 42%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04360v1</guid>
      <category>cs.OS</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yutong Huang, Zhiyuan Guo, Yiying Zhang</dc:creator>
    </item>
    <item>
      <title>A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents</title>
      <link>https://arxiv.org/abs/2510.04607</link>
      <description>arXiv:2510.04607v1 Announce Type: new 
Abstract: Computer-use agents (CUAs) powered by large language models (LLMs) have emerged as a promising approach to automating computer tasks, yet they struggle with graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to decompose high-level goals into lengthy, error-prone sequences of fine-grained actions, resulting in low success rates and an excessive number of LLM calls.
  We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms existing GUIs into three declarative primitives: access, state, and observation, which are better suited for LLMs. Our key idea is policy-mechanism separation: LLMs focus on high-level semantic planning (policy) while GOI handles low-level navigation and interaction (mechanism). GOI does not require modifying the application source code or relying on application programming interfaces (APIs).
  We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on Windows. Compared to a leading GUI-based agent baseline, GOI improves task success rates by 67% and reduces interaction steps by 43.5%. Notably, GOI completes over 61% of successful tasks with a single LLM call.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04607v1</guid>
      <category>cs.OS</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Wang, Mingyu Li, Haibo Chen</dc:creator>
    </item>
    <item>
      <title>Retrofitting XoM for Stripped Binaries without Embedded Data Relocation</title>
      <link>https://arxiv.org/abs/2412.02110</link>
      <description>arXiv:2412.02110v3 Announce Type: replace-cross 
Abstract: In this paper, we present PXoM, a practical technique to seamlessly retrofit XoM into stripped binaries on the x86-64 platform. As handling the mixture of code and data is a well-known challenge for XoM, most existing methods require the strict separation of code and data areas via either compile-time transformation or binary patching, so that the unreadable permission can be safely enforced at the granularity of memory pages. In contrast to previous approaches, we provide a fine-grained memory permission control mechanism to restrict the read permission of code while allowing legitimate data reads within code pages. This novelty enables PXoM to harden stripped binaries but without resorting to error-prone embedded data relocation. We leverage Intel's hardware feature, Memory Protection Keys, to offer an efficient fine-grained permission control. We measure PXoM's performance with both micro- and macro-benchmarks, and it only introduces negligible runtime overhead. Our security evaluation shows that PXoM leaves adversaries with little wiggle room to harvest all of the required gadgets, suggesting PXoM is practical for real-world deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02110v3</guid>
      <category>cs.CR</category>
      <category>cs.OS</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.14722/ndss.2025.240825</arxiv:DOI>
      <dc:creator>Chenke Luo, Jiang Ming, Mengfei Xie, Guojun Peng, Jianming Fu</dc:creator>
    </item>
    <item>
      <title>Scaling Data Center TCP to Terabits with Laminar</title>
      <link>https://arxiv.org/abs/2504.19058</link>
      <description>arXiv:2504.19058v2 Announce Type: replace-cross 
Abstract: Laminar is the first TCP stack designed for the reconfigurable match-action table (RMT) architecture, widely used in high-speed programmable switches and SmartNICs. Laminar reimagines TCP processing as a pipeline of simple match-action operations, enabling line-rate performance with low latency and minimal energy consumption, while maintaining compatibility with standard TCP and POSIX sockets. Leveraging novel techniques like optimistic concurrency, pseudo segment updates, and bump-in-the-wire processing, Laminar handles the transport logic, including retransmission, reassembly, flow, and congestion control, entirely within the RMT pipeline.
  We prototype Laminar on an Intel Tofino2 switch, and demonstrate its scalability to terabit speeds, its flexibility, and robustness to network dynamics. Laminar delivers RDMA-equivalent performance, saving up to 16 host CPU cores versus the TAS kernel-bypass TCP stack with short RPC workloads, achieving 1.3$\times$ higher peak throughput at 5$\times$ lower 99.99p tail latency. At scale, Laminar drives nearly $1$Bpps of TCP processing while keeping RPC tail latency near $20\mu s$. For streaming workloads, Laminar achieves $25$Mpps per-core, enough to saturate the line-rate. It significantly benefits real applications: a key-value store on Laminar doubles throughput-per-watt while maintaining a 99.99p tail latency lower than TAS's best case tail latency, and SPDK's NVMe-oTCP reaches RDMA-level efficiency. Demonstrating Laminar's flexibility, we implement TCP stack extensions, including a sequencer API for a linearizable distributed shared log, Timely congestion control, and delayed ACKs. Finally, Laminar generalizes to FPGA SmartNICs, delivering $3\times$ ToNIC's packet rate under equal timing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19058v2</guid>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rajath Shashidhara, Antoine Kaufmann, Simon Peter</dc:creator>
    </item>
    <item>
      <title>Learning Semantics, Not Addresses: Runtime Neural Prefetching for Far Memory</title>
      <link>https://arxiv.org/abs/2506.00384</link>
      <description>arXiv:2506.00384v2 Announce Type: replace-cross 
Abstract: Memory prefetching has long boosted CPU caches and is increasingly vital for far-memory systems, where large portions of memory are offloaded to cheaper, remote tiers. While effective prefetching requires accurate prediction of future accesses, prior ML approaches have been limited to simulation or small-scale hardware. We introduce FarSight, the first Linux-based far-memory system to leverage deep learning by decoupling application semantics from runtime memory layout. This separation enables offline-trained models to predict access patterns over a compact ordinal vocabulary, which are resolved at runtime through lightweight mappings. Across four data-intensive workloads, FarSight delivers up to 3.6x higher performance than the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00384v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <pubDate>Tue, 07 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yutong Huang, Zhiyuan Guo, Yiying Zhang</dc:creator>
    </item>
  </channel>
</rss>
