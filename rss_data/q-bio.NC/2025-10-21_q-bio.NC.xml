<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Oct 2025 01:45:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Topological decoding of grid cell activity via path lifting to covering spaces</title>
      <link>https://arxiv.org/abs/2510.16216</link>
      <description>arXiv:2510.16216v1 Announce Type: new 
Abstract: High-dimensional neural activity often reside in a low-dimensional subspace, referred to as neural manifolds. Grid cells in the medial entorhinal cortex provide a periodic spatial code that are organized near a toroidal manifold, independent of the spatial environment. Due to the periodic nature of its code, it is unclear how the brain utilizes the toroidal manifold to understand its state in a spatial environment. We introduce a novel framework that decodes spatial information from grid cell activity using topology. Our approach uses topological data analysis to extract toroidal coordinates from grid cell population activity and employs path-lifting to reconstruct trajectories in physical space. The reconstructed paths differ from the original by an affine transformation. We validated the method on both continuous attractor network simulations and experimental recordings of grid cells, demonstrating that local trajectories can be reliably reconstructed from a single grid cell module without external position information or training data. These results suggest that co-modular grid cells contain sufficient information for path integration and suggest a potential computational mechanism for spatial navigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16216v1</guid>
      <category>q-bio.NC</category>
      <category>math.AT</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxing Jared Yao, Iris H. R. Yoon</dc:creator>
    </item>
    <item>
      <title>A Minimal Quantitative Model of Perceptual Suppression and Breakthrough in Visual Rivalry</title>
      <link>https://arxiv.org/abs/2510.17154</link>
      <description>arXiv:2510.17154v1 Announce Type: new 
Abstract: When conflicting images are presented to either eye, binocular fusion is disrupted. Rather than experiencing a blend of both percepts, often only one eye's image is experienced, whilst the other is suppressed from awareness. Importantly, suppression is transient - the two rival images compete for dominance, with stochastic switches between mutually exclusive percepts occurring every few seconds with law-like regularity. From the perspective of dynamical systems theory, visual rivalry offers an experimentally tractable window into the dynamical mechanisms governing perceptual awareness. In a recently developed visual rivalry paradigm - tracking continuous flash suppression (tCFS) - it was shown that the transition between awareness and suppression is hysteretic, with a higher contrast threshold required for a stimulus to breakthrough suppression into awareness than to be suppressed from awareness. Here, we present an analytically-tractable model of visual rivalry that quantitatively explains the hysteretic transition between periods of awareness and suppression in tCFS. Grounded in the theory of neural dynamics, we derive closed-form expressions for the duration of perceptual dominance and suppression, and for the degree of hysteresis (i.e. the depth of perceptual suppression), as a function of model parameters. Finally, our model yields a series of novel behavioural predictions, the first of which - distributions of dominance and suppression durations during tCFS should be approximately equal - we empirically validate in human psychophysical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17154v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Christopher J. Whyte, Hugh R. Wilson, Shay Tobin, Brandon R. Munn, Shervin Safavi, Eli J. Muller, Jayson Jeganathan, Matt Davidson, James M. Shine, David Alais</dc:creator>
    </item>
    <item>
      <title>A theory for self-sustained balanced states in absence of strong external currents</title>
      <link>https://arxiv.org/abs/2510.17492</link>
      <description>arXiv:2510.17492v1 Announce Type: new 
Abstract: Recurrent neural networks with balanced excitation and inhibition exhibit irregular asynchronous dynamics, which is fundamental for cortical computations. Classical balance mechanisms require strong external inputs to sustain finite firing rates, raising concerns about their biological plausibility. Here, we investigate an alternative mechanism based on short-term synaptic depression (STD) acting on excitatory-excitatory synapses, which dynamically balances the network activity without the need of external inputs. By employing numerical simulations and theoretical investigations we characterize the dynamics of a massively coupled network made up of $N$ rate-neuron models. Depending on the synaptic strength $J_0$, the network exhibits two distinct regimes: at sufficiently small $J_0$, it converges to a homogeneous fixed point, while for sufficiently large $J_0$, it exhibits Rate Chaos. For finite networks, we observe several different routes to chaos depending on the network realization. The width of the transition region separating the homogeneous stable solution from Rate Chaos appears to shrink for increasing $N$ and eventually to vanish in the thermodynamic limit. The characterization of the Rate Chaos regime performed by employing Dynamical Mean Field approaches allow us on one side to confirm that this novel balancing mechanism is able to sustain finite irregular activity even in the thermodynamic limit, and on the other side to reveal that the balancing occurs via dynamic cancellation of the input correlations generated by the massive coupling. Our findings show that STD provides an intrinsic self-regulating mechanism for balanced networks, sustaining irregular yet stable activity without the need of biologically unrealistic inputs. This work extends the balanced network paradigm, offering insights into how cortical circuits could maintain robust dynamics via synaptic adaptation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17492v1</guid>
      <category>q-bio.NC</category>
      <category>nlin.CD</category>
      <category>physics.bio-ph</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Angulo-Garcia, Alessandro Torcini</dc:creator>
    </item>
    <item>
      <title>Paradoxical increase of capacity due to spurious overlaps in attractor networks</title>
      <link>https://arxiv.org/abs/2510.17593</link>
      <description>arXiv:2510.17593v1 Announce Type: new 
Abstract: In Hopfield-type associative memory models, memories are stored in the connectivity matrix and can be retrieved subsequently thanks to the collective dynamics of the network. In these models, the retrieval of a particular memory can be hampered by overlaps between the network state and other memories, termed spurious overlaps since these overlaps collectively introduce noise in the retrieval process. In classic models, spurious overlaps increase the variance of synaptic inputs but do not affect the mean. We show here that in models equipped with a learning rule inferred from neurobiological data, spurious overlaps collectively reduce the mean synaptic inputs to neurons, and that this mean reduction causes in turn an increase in storage capacity through a sparsening of network activity. Our paper demonstrates a link between a specific feature of experimentally inferred plasticity rules and network storage capacity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17593v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Marco Benedetti, Nicolas Brunel, Enzo Marinari, Ulises Pereira Obilinovic</dc:creator>
    </item>
    <item>
      <title>Attention for Causal Relationship Discovery from Biological Neural Dynamics</title>
      <link>https://arxiv.org/abs/2311.06928</link>
      <description>arXiv:2311.06928v3 Announce Type: cross 
Abstract: This paper explores the potential of the transformer models for learning Granger causality in networks with complex nonlinear dynamics at every node, as in neurobiological and biophysical networks. Our study primarily focuses on a proof-of-concept investigation based on simulated neural dynamics, for which the ground-truth causality is known through the underlying connectivity matrix. For transformer models trained to forecast neuronal population dynamics, we show that the cross attention module effectively captures the causal relationship among neurons, with an accuracy equal or superior to that for the most popular Granger causality analysis method. While we acknowledge that real-world neurobiology data will bring further challenges, including dynamic connectivity and unobserved variability, this research offers an encouraging preliminary glimpse into the utility of the transformer model for causal representation learning in neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06928v3</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Lu, Anika Tabassum, Shruti Kulkarni, Lu Mi, J. Nathan Kutz, Eric Shea-Brown, Seung-Hwan Lim</dc:creator>
    </item>
    <item>
      <title>WaveNet's Precision in EEG Classification</title>
      <link>https://arxiv.org/abs/2510.15947</link>
      <description>arXiv:2510.15947v1 Announce Type: cross 
Abstract: This study introduces a WaveNet-based deep learning model designed to automate the classification of EEG signals into physiological, pathological, artifact, and noise categories. Traditional methods for EEG signal classification, which rely on expert visual review, are becoming increasingly impractical due to the growing complexity and volume of EEG recordings. Leveraging a publicly available annotated dataset from Mayo Clinic and St. Anne's University Hospital, the WaveNet model was trained, validated, and tested on 209,232 samples with a 70/20/10 percent split. The model achieved a classification accuracy exceeding previous CNN and LSTM-based approaches, and was benchmarked against a Temporal Convolutional Network (TCN) baseline. Notably, the model distinguishes noise and artifacts with high precision, although it reveals a modest but explainable degree of misclassification between physiological and pathological signals, reflecting inherent clinical overlap. WaveNet's architecture, originally developed for raw audio synthesis, is well suited for EEG data due to its use of dilated causal convolutions and residual connections, enabling it to capture both fine-grained and long-range temporal dependencies. The research also details the preprocessing pipeline, including dynamic dataset partitioning and normalization steps that support model generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15947v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Casper van Laar, Khubaib Ahmed</dc:creator>
    </item>
    <item>
      <title>Quantifying the compressibility of the human brain</title>
      <link>https://arxiv.org/abs/2510.16327</link>
      <description>arXiv:2510.16327v1 Announce Type: cross 
Abstract: In the human brain, the allowed patterns of activity are constrained by the correlations between brain regions. Yet it remains unclear which correlations -- and how many -- are needed to predict large-scale neural activity. Here, we present an information-theoretic framework to identify the most important correlations, which provide the most accurate predictions of neural states. Applying our framework to cortical activity in humans, we discover that the vast majority of variance in activity is explained by a small number of correlations. This means that the brain is highly compressible: only a sparse network of correlations is needed to predict large-scale activity. We find that this compressibility is strikingly consistent across different individuals and cognitive tasks, and that, counterintuitively, the most important correlations are not necessarily the strongest. Together, these results suggest that nearly all correlations are not needed to predict neural activity, and we provide the tools to uncover the key correlations that are.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16327v1</guid>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas J. Weaver, Joshua I. Faskowitz, Richard F. Betzel, Christopher W. Lynn</dc:creator>
    </item>
    <item>
      <title>The global communication pathways of the human brain transcend the cortical-subcortical-cerebellar division</title>
      <link>https://arxiv.org/abs/2505.22893</link>
      <description>arXiv:2505.22893v3 Announce Type: replace 
Abstract: Understanding how cortex, subcortex and cerebellum integrate is a major challenge for neuroscience, however, studies of the brain's structural connectivity have mostly focused on cortico-cortical links. Here, we used diffusion imaging to construct the structural connectome of the entire human brain including 360 cortical, 233 subcortical, and 125 cerebellar regions of interest (ROIs). We found that the brain forms a modular and hierarchical network architecture, organized into modules of mixed cortical, subcortical and/or cerebellar regions, and whose cross-modular pathways are centralized through highly connected hub ROIs (a `rich-club'). This global rich-club is subcortically dominated and, surprisingly, composed of hub ROIs from all subcortical structures rather than one region like the thalamus, centralizing the communication pathways. This study improves our understanding of the human brain's organization. It provides structural evidence to question the prevalent cortico-centric notion by revealing a connectome centered at the subcortex but made of transversal pathways.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22893v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julian Schulte, Mario Senden, Gustavo Deco, Xenia Kobeleva, Gorka Zamora-L\'opez</dc:creator>
    </item>
    <item>
      <title>Normative Modelling in Neuroimaging: A Practical Guide for Researchers</title>
      <link>https://arxiv.org/abs/2509.07237</link>
      <description>arXiv:2509.07237v2 Announce Type: replace 
Abstract: Normative modelling is an increasingly common statistical technique in neuroimaging that estimates population-level benchmarks in brain structure. It enables the quantification of individual deviations from expected distributions whilst accounting for biological and technical covariates without requiring large, matched control groups. This makes it a powerful alternative to traditional case-control studies for identifying brain structural alterations associated with pathology. Despite the availability of numerous modelling approaches and several toolboxes with pre-trained models, the distinct strengths and limitations of normative modelling make it difficult to determine how and when to implement them appropriately. This review offers practical guidance and outlines statistical considerations for clinical researchers using normative modelling in neuroimaging. Through a worked example using clinical epilepsy data, we outline considerations for responsible implementation of pre-trained normative models, to support their broad and rigorous adoption in neuroimaging research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07237v2</guid>
      <category>q-bio.NC</category>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nida Alyas, Jonathan Horsley, Bethany Little, Peter N. Taylor, Yujiang Wang, Karoline Leiberg</dc:creator>
    </item>
    <item>
      <title>Absolute abstraction: a renormalisation group approach</title>
      <link>https://arxiv.org/abs/2407.01656</link>
      <description>arXiv:2407.01656v4 Announce Type: replace-cross 
Abstract: Abstraction is the process of extracting the essential features from raw data while ignoring irrelevant details. It is well known that abstraction emerges with depth in neural networks, where deep layers capture abstract characteristics of data by combining lower level features encoded in shallow layers (e.g. edges). Yet we argue that depth alone is not enough to develop truly abstract representations. We advocate that the level of abstraction crucially depends on how broad the training set is. We address the issue within a renormalisation group approach where a representation is expanded to encompass a broader set of data. We take the unique fixed point of this transformation -- the Hierarchical Feature Model -- as a candidate for a representation which is absolutely abstract. This theoretical picture is tested in numerical experiments based on Deep Belief Networks and auto-encoders trained on data of different breadth. These show that representations in neural networks approach the Hierarchical Feature Model as the data get broader and as depth increases, in agreement with theoretical predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01656v4</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.data-an</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlo Orientale Caputo, Elias Seiffert, Enrico Frausin, Matteo Marsili</dc:creator>
    </item>
  </channel>
</rss>
