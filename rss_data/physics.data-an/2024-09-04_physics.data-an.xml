<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Sep 2024 01:46:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Burst-tree structure and higher-order temporal correlations</title>
      <link>https://arxiv.org/abs/2409.01674</link>
      <description>arXiv:2409.01674v1 Announce Type: new 
Abstract: Understanding characteristics of temporal correlations in time series is crucial for developing accurate models in natural and social sciences. The burst-tree decomposition method was recently introduced to reveal higher-order temporal correlations in time series in a form of an event sequence, in particular, the hierarchical structure of bursty trains of events for the entire range of timescales [Jo et al., Sci.~Rep.~\textbf{10}, 12202 (2020)]. Such structure has been found to be simply characterized by the burst-merging kernel governing which bursts are merged together as the timescale for detecting bursts increases. In this work, we study the effects of kernels on the higher-order temporal correlations in terms of burst size distributions, memory coefficients for bursts, and the autocorrelation function. We employ several kernels, including the constant, additive, and product kernels as well as those inspired by the empirical results. We find that kernels with preferential mixing lead to the heavy-tailed burst size distributions, while kernels with assortative mixing lead to positive correlations between burst sizes. The decaying exponent of the autocorrelation function depends not only on the kernel but also on the power-law exponent of the interevent time distribution. In addition, thanks to the analogy to the coagulation process, analytical solutions of burst size distributions for some kernels could be obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01674v1</guid>
      <category>physics.data-an</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tibebe Birhanu, Hang-Hyun Jo</dc:creator>
    </item>
    <item>
      <title>Cosmological joint analysis with cosmic growth and expansion rate</title>
      <link>https://arxiv.org/abs/2204.10597</link>
      <description>arXiv:2204.10597v2 Announce Type: cross 
Abstract: The measurements of expansion rate $H(z)$ and the growth rate $f\sigma_8(z)$ describe the evolution of the universe, and both of them can constrain the cosmological models through data analysis. Due to the lack of data points, these datasets are combined by the traditional combined method ($\chi^2$ method) to select a best-fitting cosmological model. In 2017, Linder proposed a joint method, which describes the evolution of the universe through $H(z)-f\sigma_8$ diagram instead of the redshift z. Compared to individual datasets, Linder demonstrated the advantages of the joint method to distinguish cosmologies. In this paper, we compare the significance between the traditional combined method and Linder's joint method by constraining the density parameter $\Omega_M$ using Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). The result shows that the joint method is more significant than the traditional combined method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10597v2</guid>
      <category>astro-ph.CO</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.dark.2022.101147</arxiv:DOI>
      <arxiv:journal_reference>Physics of the Dark Universe, Volume 39, 2023</arxiv:journal_reference>
      <dc:creator>Jing Niu, Tong-Jie Zhang</dc:creator>
    </item>
    <item>
      <title>Data-driven ODE modeling of the high-frequency complex dynamics of a fluid flow</title>
      <link>https://arxiv.org/abs/2409.00668</link>
      <description>arXiv:2409.00668v1 Announce Type: cross 
Abstract: In our previous paper [N. Tsutsumi, K. Nakai and Y. Saiki, Chaos 32, 091101 (2022)], we proposed a method for constructing a system of differential equations of chaotic behavior from only observable deterministic time series, which we call the radial function-based regression (RfR) method. However, when the targeted variable's behavior is rather complex, the direct application of the RfR method does not function well. In this study, we propose a novel method of modeling such dynamics, including the high-frequency intermittent behavior of a fluid flow, by considering another variable (base variable) showing relatively simple, less intermittent behavior. We construct an autonomous joint model composed of two parts: the first is an autonomous system of a base variable, and the other concerns the targeted variable being affected by a term involving the base variable to demonstrate complex dynamics. The constructed joint model succeeded in not only inferring a short trajectory but also reconstructing chaotic sets and statistical properties obtained from a long trajectory such as the density distributions of the actual dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00668v1</guid>
      <category>nlin.CD</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natsuki Tsutsumi, Kengo Nakai, Yoshitaka Saiki</dc:creator>
    </item>
    <item>
      <title>Compact 15-minute cities are greener</title>
      <link>https://arxiv.org/abs/2409.01817</link>
      <description>arXiv:2409.01817v1 Announce Type: cross 
Abstract: The 15-minute city concept, advocating for cities where essential services are accessible within 15 minutes on foot and by bike, has gained significant attention in recent years. However, despite being celebrated for promoting sustainability, there is an ongoing debate regarding its effectiveness in reducing car usage and, subsequently, emissions in cities. In particular, large-scale evaluations of the effectiveness of the 15-minute concept in reducing emissions are lacking. To address this gap, we investigate whether cities with better walking accessibility, like 15-minute cities, are associated with lower transportation emissions. Comparing 700 cities worldwide, we find that cities with better walking accessibility to services emit less CO$_2$ per capita for transport. Moreover, we observe that among cities with similar average accessibility, cities spreading over larger areas tend to emit more. Our findings highlight the effectiveness of decentralised urban planning, especially the proximity-based 15-minute city, in promoting sustainable mobility. However, they also emphasise the need to integrate local accessibility with urban compactness and efficient public transit, which are vital in large cities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01817v1</guid>
      <category>physics.soc-ph</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Marzolla, Matteo Bruno, Hygor Piaget Monteiro Melo, Vittorio Loreto</dc:creator>
    </item>
    <item>
      <title>Causality for Earth Science -- A Review on Time-series and Spatiotemporal Causality Methods</title>
      <link>https://arxiv.org/abs/2404.05746</link>
      <description>arXiv:2404.05746v2 Announce Type: replace 
Abstract: This survey paper covers the breadth and depth of time-series and spatiotemporal causality methods, and their applications in Earth Science. More specifically, the paper presents an overview of causal discovery and causal inference, explains the underlying causal assumptions, and enlists evaluation techniques and key terminologies of the domain area. The paper elicits the various state-of-the-art methods introduced for time-series and spatiotemporal causal analysis along with their strengths and limitations. The paper further describes the existing applications of several methods for answering specific Earth Science questions such as extreme weather events, sea level rise, teleconnections etc. This survey paper can serve as a primer for Data Science researchers interested in data-driven causal study as we share a list of resources, such as Earth Science datasets (synthetic, simulated and observational data) and open source tools for causal analysis. It will equally benefit the Earth Science community interested in taking an AI-driven approach to study the causality of different dynamic and thermodynamic processes as we present the open challenges and opportunities in performing causality-based Earth Science study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05746v2</guid>
      <category>physics.data-an</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>physics.ao-ph</category>
      <category>physics.geo-ph</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahara Ali, Uzma Hasan, Xingyan Li, Omar Faruque, Akila Sampath, Yiyi Huang, Md Osman Gani, Jianwu Wang</dc:creator>
    </item>
    <item>
      <title>Physics simulation capabilities of LLMs</title>
      <link>https://arxiv.org/abs/2312.02091</link>
      <description>arXiv:2312.02091v2 Announce Type: replace-cross 
Abstract: [Abridged abstract] Large Language Models (LLMs) can solve some undergraduate-level to graduate-level physics textbook problems and are proficient at coding. Combining these two capabilities could one day enable AI systems to simulate and predict the physical world.
  We present an evaluation of state-of-the-art (SOTA) LLMs on PhD-level to research-level computational physics problems. We condition LLM generation on the use of well-documented and widely-used packages to elicit coding capabilities in the physics and astrophysics domains. We contribute $\sim 50$ original and challenging problems in celestial mechanics (with REBOUND), stellar physics (with MESA), 1D fluid dynamics (with Dedalus) and non-linear dynamics (with SciPy). Since our problems do not admit unique solutions, we evaluate LLM performance on several soft metrics: counts of lines that contain different types of errors (coding, physics, necessity and sufficiency) as well as a more "educational" Pass-Fail metric focused on capturing the salient physical ingredients of the problem at hand.
  As expected, today's SOTA LLM (GPT4) zero-shot fails most of our problems, although about 40\% of the solutions could plausibly get a passing grade. About $70-90 \%$ of the code lines produced are necessary, sufficient and correct (coding \&amp; physics). Physics and coding errors are the most common, with some unnecessary or insufficient lines. We observe significant variations across problem class and difficulty. We identify several failure modes of GPT4 in the computational physics domain.
  Our reconnaissance work provides a snapshot of current computational capabilities in classical physics and points to obvious improvement targets if AI systems are ever to reach a basic level of autonomy in physics simulation capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02091v2</guid>
      <category>cs.AI</category>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>astro-ph.SR</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamad Ali-Dib, Kristen Menou</dc:creator>
    </item>
    <item>
      <title>pyMSER -- An open-source library for automatic equilibration detection in molecular simulations</title>
      <link>https://arxiv.org/abs/2403.19387</link>
      <description>arXiv:2403.19387v2 Announce Type: replace-cross 
Abstract: Automated molecular simulations are used extensively for predicting material properties. Typically, these simulations exhibit two regimes: a dynamic equilibration part, followed by a steady state. For extracting observable properties, the simulations must first reach a steady state so that thermodynamic averages can be taken. However, as equilibration depends on simulation conditions, predicting the optimal number of simulation steps a priori is impossible. Here, we demonstrate the application of the Marginal Standard Error Rule (MSER) for automatically identifying the optimal truncation point in Grand Canonical Monte Carlo (GCMC) simulations. This novel automatic procedure determines the point in which steady state is reached, ensuring that figures-of-merits are extracted in an objective, accurate, and reproducible fashion. In the case of GCMC simulations of gas adsorption in metal-organic frameworks, we find that this methodology reduces the computational cost by up to 90%. As MSER statistics are independent of the simulation method that creates the data, this library is, in principle, applicable to any time series analysis in which equilibration truncation is required. The open-source Python implementation of our method, pyMSER, is publicly available for reuse and validation at https://github.com/IBM/pymser.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19387v2</guid>
      <category>cond-mat.mes-hall</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Felipe Lopes Oliveira, Binquan Luan, Pierre Moth\'e Esteves, Mathias Steiner, Rodrigo Neumann Barros Ferreira</dc:creator>
    </item>
    <item>
      <title>Comprehensive Study of $k$-essence Model: Dynamical System Analysis and Observational Constraints from Latest Type Ia Supernova and BAO Observations</title>
      <link>https://arxiv.org/abs/2406.07179</link>
      <description>arXiv:2406.07179v2 Announce Type: replace-cross 
Abstract: We constrain the parameters of the $k$-essence scalar field model with inverse square and exponential potentials using data sets including Pantheon+SHOES and the Dark Energy Survey (DES) of Type Ia supernovae, Baryon Acoustic Oscillation (BAO) data from SDSS and DESI surveys, and direct measurements of the Hubble parameter and redshift obtained from the differential age method (CC). We also provide a brief perspective on the dynamical evolution of both models and derive stability constraints on the model parameters, which are then used to set appropriate priors. We adopt a Bayesian inference procedure to estimate the model parameters that best fit the data. A comprehensive analysis in light of observational data shows that the $k$-essence model fits well across all data combinations. However, according to the BIC criterion, the $\Lambda$CDM model provides a slightly better fit compared to the $k$-essence model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07179v2</guid>
      <category>astro-ph.CO</category>
      <category>gr-qc</category>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saddam Hussain, Sarath Nelleri, Kaushik Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Network inference from oscillatory signals based on circle map</title>
      <link>https://arxiv.org/abs/2407.07445</link>
      <description>arXiv:2407.07445v2 Announce Type: replace-cross 
Abstract: Synchronization is ubiquitous in nature, which is mathematically described by coupled oscillators. Synchronization strongly depends on the interaction network, and the network plays a crucial role in controlling the dynamics. To understand and control synchronization dynamics in the real world, it is essential to identify the network from the observed data. While previous studies have developed the methods for inferring the network of asynchronous systems, it remains challenging to infer the network of well-synchronized oscillators. In this study, we develop a method for non-invasively inferring the network of synchronized and desynchronized oscillators. This method is based on the circle map, which describes the phase change in an oscillatory cycle. Our method discards a large part of data used for inference, which may seem counterintuitive. However, the effectiveness of the method is supported by the phase reduction theory, a well-established theory for analyzing weakly coupled oscillators. We verify the proposed method by applying it to simulated data of the limit-cycle oscillators. This study provides an important step towards understanding synchronization in real-world systems from a network perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07445v2</guid>
      <category>nlin.AO</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akari Matsuki, Hiroshi Kori, Ryota Kobayashi</dc:creator>
    </item>
  </channel>
</rss>
