<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.BM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.BM</link>
    <description>q-bio.BM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.BM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 Aug 2024 01:35:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>TourSynbio: A Multi-Modal Large Model and Agent Framework to Bridge Text and Protein Sequences for Protein Engineering</title>
      <link>https://arxiv.org/abs/2408.15299</link>
      <description>arXiv:2408.15299v1 Announce Type: new 
Abstract: The structural similarities between protein sequences and natural languages have led to parallel advancements in deep learning across both domains. While large language models (LLMs) have achieved much progress in the domain of natural language processing, their potential in protein engineering remains largely unexplored. Previous approaches have equipped LLMs with protein understanding capabilities by incorporating external protein encoders, but this fails to fully leverage the inherent similarities between protein sequences and natural languages, resulting in sub-optimal performance and increased model complexity. To address this gap, we present TourSynbio-7B, the first multi-modal large model specifically designed for protein engineering tasks without external protein encoders. TourSynbio-7B demonstrates that LLMs can inherently learn to understand proteins as language. The model is post-trained and instruction fine-tuned on InternLM2-7B using ProteinLMDataset, a dataset comprising 17.46 billion tokens of text and protein sequence for self-supervised pretraining and 893K instructions for supervised fine-tuning. TourSynbio-7B outperforms GPT-4 on the ProteinLMBench, a benchmark of 944 manually verified multiple-choice questions, with 62.18% accuracy. Leveraging TourSynbio-7B's enhanced protein sequence understanding capability, we introduce TourSynbio-Agent, an innovative framework capable of performing various protein engineering tasks, including mutation analysis, inverse folding, protein folding, and visualization. TourSynbio-Agent integrates previously disconnected deep learning models in the protein engineering domain, offering a unified conversational user interface for improved usability. Finally, we demonstrate the efficacy of TourSynbio-7B and TourSynbio-Agent through two wet lab case studies on vanilla key enzyme modification and steroid compound catalysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15299v1</guid>
      <category>q-bio.BM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiqing Shen, Zan Chen, Michail Mamalakis, Yungeng Liu, Tianbin Li, Yanzhou Su, Junjun He, Pietro Li\`o, Yu Guang Wang</dc:creator>
    </item>
    <item>
      <title>Modular DNA origami-based electrochemical detection of DNA and proteins</title>
      <link>https://arxiv.org/abs/2312.06554</link>
      <description>arXiv:2312.06554v3 Announce Type: replace-cross 
Abstract: The diversity and heterogeneity of biomarkers has made the development of general methods for single-step quantification of analytes difficult. For individual biomarkers, electrochemical methods that detect a conformational change in an affinity binder upon analyte binding have shown promise. However, because the conformational change must operate within a nanometer-scale working distance, an entirely new sensor, with a unique conformational change, must be developed for each analyte. Here, we demonstrate a modular electrochemical biosensor, built from DNA origami, which is easily adapted to diverse molecules by merely replacing its analyte binding domains. Instead of relying on a unique nanometer-scale movement of a single redox reporter, all sensor variants rely on the same 100-nanometer scale conformational change, which brings dozens of reporters close enough to a gold electrode surface that a signal can be measured via square wave voltammetry, a standard electrochemical technique. To validate our sensor's mechanism, we used single-stranded DNA as an analyte, and optimized the number of redox reporters and various linker lengths. Adaptation of the sensor to streptavidin and PDGF-BB analytes was achieved by simply adding biotin or anti-PDGF aptamers to appropriate DNA linkers. Geometrically-optimized streptavidin sensors exhibited signal gain and limit of detection markedly better than comparable reagentless electrochemical sensors. After use, the same sensors could be regenerated under mild conditions: performance was largely maintained over four cycles of DNA strand displacement and rehybridization. By leveraging the modularity of DNA nanostructures, our work provides a straightforward route to the single-step quantification of arbitrary nucleic acids and proteins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06554v3</guid>
      <category>physics.bio-ph</category>
      <category>cond-mat.mes-hall</category>
      <category>cond-mat.soft</category>
      <category>q-bio.BM</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Byoung-jin Jeon, Matteo M. Guareschi, Jaimie M. Stewart, Emily Wu, Ashwin Gopinath, Netzahualc\'oyotl Arroyo-Curr\'as, Philippe Dauphin-Ducharme, Kevin W. Plaxco, Philip S. Lukeman, Paul W. K. Rothemund</dc:creator>
    </item>
  </channel>
</rss>
