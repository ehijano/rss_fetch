<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Jun 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Hypervisor Extension for a RISC-V Processor</title>
      <link>https://arxiv.org/abs/2406.17796</link>
      <description>arXiv:2406.17796v1 Announce Type: new 
Abstract: This paper describes our experience implementing a Hypervisor extension for a 64-bit RISC-V processor. We describe the design process and the main required parts with a brief explanation of each one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17796v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jaume Gauchola, JuanJos\'e Costa, Enric Morancho, Ramon Canal, Xavier Carril, Max Doblas, Beatriz Otero, Alex Pajuelo, Eva Rodr\'iguez, Javier Salamero, Javier Verd\'u</dc:creator>
    </item>
    <item>
      <title>High-Resolution, Multi-Channel FPGA-Based Time-to-Digital Converter</title>
      <link>https://arxiv.org/abs/2406.17798</link>
      <description>arXiv:2406.17798v1 Announce Type: new 
Abstract: In this paper we present a novel high-resolution multi-channel FPGA-based time-to-digital converter (TDC). We designed and implemented a complex electronic circuit on the FPGA, whose overall accuracy is several orders of magnitude greater than the accuracy of the FPGA used in digital mode. Our sensor device contains simple circuit elements that are cheap and easily accessible (Xilinx Spartan 3 and Spartan 6). Using our design, many channels (80-100 channels) can be implemented on a larger FPGA. The prototype of our TDC has been implemented and functionally verified by experiments and measurements. By a certified pulse generator 20 ps precision has been measured over the range of 3 ns. Using more precise clock signal this range may be extended. The achieved resolution is 5 ps. Its resolution, channel number and range can be configured dynamically, which makes it suitable for effective use in industrial purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17798v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Balazs Jakli, Adam Rak, Gyorgy Cserey</dc:creator>
    </item>
    <item>
      <title>Design, Implementation and Evaluation of the SVNAPOT Extension on a RISC-V Processor</title>
      <link>https://arxiv.org/abs/2406.17802</link>
      <description>arXiv:2406.17802v1 Announce Type: new 
Abstract: The RISC-V SVNAPOT Extension aims to remedy the performance overhead of the Memory Management Unit (MMU), under heavy memory loads. The Privileged Specification defines additional Natural-Power-of-Two (NAPOT) multiples of the 4KB base page size, with 64KB as the default candidate. In this paper we extend the MMU of the Rocket Chip Generator, in order to manage the collocation of 64KB pages along with 4KB pages in the L2 TLB. We present the design challenges we had to overcome and the trade-offs of our design choices. We conduct a preliminary sensitivity analysis of the L2 TLB with different configurations/page sizes. Finally, we summarize on techniques which could further improve memory management performance on RISC-V systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17802v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikolaos-Charalampos Papadopoulos, Stratos Psomadakis, Vasileios Karakostas, Nectarios Koziris, Dionisios N. Pnevmatikatos</dc:creator>
    </item>
    <item>
      <title>NoX: a Compact Open-Source RISC-V Processor for Multi-Processor Systems-on-Chip</title>
      <link>https://arxiv.org/abs/2406.17878</link>
      <description>arXiv:2406.17878v1 Announce Type: new 
Abstract: IoT applications are one of the driving forces in making systems energy and power-efficient, given their resource constraints. However, because of security, latency, and transmission, we advocate for local computing through multi-processor systems-on-chip (MPSoCs) for edge computing. The RISC-V ISA has grown in academia and industry due to its flexibility. Still, available open-source cores cannot be seamlessly integrated into MPSoCs for a fast time to market. This paper presents NoX, a compact open-source plug-and-play 32-bit RISC-V core designed in System Verilog for efficient data processing in MPSoCs. NoX has a 4-stage single-issue in-order pipeline with full bypass, providing an efficient resource-constrained architecture. Compared to industry and academia resource-constrained RISC-V cores, NoX offers a better resource usage and performance trade-off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17878v1</guid>
      <category>cs.AR</category>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anderson I. Silva, Altamiro Susin, Fernanda L. Kastensmidt, Antonio Carlos S. Beck, Jose Rodrigo Azambuja</dc:creator>
    </item>
    <item>
      <title>Resilient and Secure Programmable System-on-Chip Accelerator Offload</title>
      <link>https://arxiv.org/abs/2406.18117</link>
      <description>arXiv:2406.18117v1 Announce Type: new 
Abstract: Computational offload to hardware accelerators is gaining traction due to increasing computational demands and efficiency challenges. Programmable hardware, like FPGAs, offers a promising platform in rapidly evolving application areas, with the benefits of hardware acceleration and software programmability. Unfortunately, such systems composed of multiple hardware components must consider integrity in the case of malicious components. In this work, we propose Samsara, the first secure and resilient platform that derives, from Byzantine Fault Tolerant (BFT), protocols to enhance the computing resilience of programmable hardware. Samsara uses a novel lightweight hardware-based BFT protocol for Systems-on-Chip, called H-Quorum, that implements the theoretical-minimum latency between applications and replicated compute nodes. To withstand malicious behaviors, Samsara supports hardware rejuvenation, which is used to replace, relocate, or diversify faulty compute nodes. Samsara's architecture ensures the security of the entire workflow while keeping the latency overhead, of both computation and rejuvenation, close to the non-replicated counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18117v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>In\^es Pinto Gouveia, Ahmad T. Sheikh, Ali Shoker, Suhaib A. Fahmy, Paulo Esteves-Verissimo</dc:creator>
    </item>
    <item>
      <title>A Jammer-Mitigating 267 Mb/s 3.78 mm$^2$ 583 mW 32$\times$8 Multi-User MIMO Receiver in 22FDX</title>
      <link>https://arxiv.org/abs/2406.18149</link>
      <description>arXiv:2406.18149v1 Announce Type: new 
Abstract: We present the first multi-user (MU) multiple-input multiple-output (MIMO) receiver ASIC that mitigates jamming attacks. The ASIC implements a recent nonlinear algorithm that performs joint jammer mitigation (via spatial filtering) and data detection (using a box prior on the data symbols). Our design supports 8 user equipments (UEs) and 32 basestation (BS) antennas, QPSK and 16-QAM with soft-outputs, and enables the mitigation of single-antenna barrage jammers and smart jammers. The fabricated 22 nm FD-SOI ASIC includes preprocessing, has a core area of 3.78 mm$^2$, achieves a throughput of 267 Mb/s while consuming 583 mW, and is the only existing design that enables reliable data detection under jamming attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18149v1</guid>
      <category>cs.AR</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Bucheli, Oscar Casta\~neda, Gian Marti, Christoph Studer</dc:creator>
    </item>
    <item>
      <title>A Lightweight Algorithm for Classifying Ex Vivo Tissues Samples</title>
      <link>https://arxiv.org/abs/2406.18372</link>
      <description>arXiv:2406.18372v1 Announce Type: new 
Abstract: In this paper, we present a novel algorithm for classifying ex vivo tissue that comprises multi-channel bioimpedance analysis and a hardware neural network. When implemented in a mixed-signal 180 nm CMOS process, the classifier has an estimated power budget of 39 mW and an area of 30 mm2. This means that the classifier can be integrated into the tip of a surgical margin assessment probe, for in vivo use during radical prostatectomy. We tested our classifier on digital phantoms of prostate tissue and also on an animal model of ex vivo bovine tissue. The classifier achieved an accuracy of 90% on the prostate tissue phantoms, and an accuracy of 84% on the animal model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18372v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tzu-Hao Li, Ethan Murphy, Allaire Doussan, Ryan Halter, Kofi Odame</dc:creator>
    </item>
    <item>
      <title>On Approximate 8-bit Floating-Point Operations Using Integer Operations</title>
      <link>https://arxiv.org/abs/2406.18441</link>
      <description>arXiv:2406.18441v1 Announce Type: new 
Abstract: In this work, approximate eight-bit floating-point operations performed using simple integer operations is discussed. For two-bit mantissa formats, faithful rounding can always be obtained for the considered operations. For all operations, correctly rounded results can be obtained for different rounding modes, either directly or by adding a conditional carry in. For three-bit mantissa formats, faithful rounding can be sometimes be obtained directly, while for other operations a conditional carry in must be added. Correctly rounded results can be obtained for most operations and rounding modes using slightly more complicated expressions for the carry in. Hardware implementation results for multiplication using both standard cell and FPGA technology are presented illustrating the potential benefit of integer computation. Especially for FPGA, significant resource savings are obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18441v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theodor Lindberg, Oscar Gustafsson</dc:creator>
    </item>
    <item>
      <title>Managing Classical Processing Requirements for Quantum Error Correction</title>
      <link>https://arxiv.org/abs/2406.17995</link>
      <description>arXiv:2406.17995v1 Announce Type: cross 
Abstract: Quantum Error Correction requires decoders to process syndromes generated by the error-correction circuits. These decoders must process syndromes faster than they are being generated to prevent a backlog of undecoded syndromes that can exponentially increase the memory and time required to execute the program. This has resulted in the development of fast hardware decoders that accelerate decoding. Applications utilizing error-corrected quantum computers will require hundreds to thousands of logical qubits and provisioning a hardware decoder for every logical qubit can be very costly. In this work, we present a framework to reduce the number of hardware decoders and navigate the compute-memory trade-offs without sacrificing the performance or reliability of program execution. Through workload-centric characterizations, we propose efficient decoder scheduling policies which can reduce the number of hardware decoders required to run a program by up to 10x while consuming less than 100 MB of memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17995v1</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satvik Maurya, Swamit Tannu</dc:creator>
    </item>
    <item>
      <title>Tailors: Accelerating Sparse Tensor Algebra by Overbooking Buffer Capacity</title>
      <link>https://arxiv.org/abs/2310.00192</link>
      <description>arXiv:2310.00192v2 Announce Type: replace 
Abstract: Sparse tensor algebra is a challenging class of workloads to accelerate due to low arithmetic intensity and varying sparsity patterns. Prior sparse tensor algebra accelerators have explored tiling sparse data to increase exploitable data reuse and improve throughput, but typically allocate tile size in a given buffer for the worst-case data occupancy. This severely limits the utilization of available memory resources and reduces data reuse. Other accelerators employ complex tiling during preprocessing or at runtime to determine the exact tile size based on its occupancy. This paper proposes a speculative tensor tiling approach, called overbooking, to improve buffer utilization by taking advantage of the distribution of nonzero elements in sparse tensors to construct larger tiles with greater data reuse. To ensure correctness, we propose a low-overhead hardware mechanism, Tailors, that can tolerate data overflow by design while ensuring reasonable data reuse. We demonstrate that Tailors can be easily integrated into the memory hierarchy of an existing sparse tensor algebra accelerator. To ensure high buffer utilization with minimal tiling overhead, we introduce a statistical approach, Swiftiles, to pick a tile size so that tiles usually fit within the buffer's capacity, but can potentially overflow, i.e., it overbooks the buffers. Across a suite of 22 sparse tensor algebra workloads, we show that our proposed overbooking strategy introduces an average speedup of $52.7\times$ and $2.3\times$ and an average energy reduction of $22.5\times$ and $2.5\times$ over ExTensor without and with optimized tiling, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00192v2</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3613424.3623793</arxiv:DOI>
      <arxiv:journal_reference>56th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO '23), 2023</arxiv:journal_reference>
      <dc:creator>Zi Yu Xue, Yannan Nellie Wu, Joel S. Emer, Vivienne Sze</dc:creator>
    </item>
    <item>
      <title>FuseMax: Leveraging Extended Einsums to Optimize Attention Accelerator Design</title>
      <link>https://arxiv.org/abs/2406.10491</link>
      <description>arXiv:2406.10491v2 Announce Type: replace 
Abstract: Attention for transformers is a critical workload that has recently received significant "attention" as a target for custom acceleration. Yet, while prior work succeeds in reducing attention's memory-bandwidth requirements, it creates load imbalance between attention operators (resulting in severe compute under-utilization) and requires on-chip memory that scales with sequence length (which is expected to grow over time).
  This paper ameliorates these issues, enabling attention with nearly 100% compute utilization, no off-chip memory traffic bottlenecks, and on-chip buffer size requirements that are independent of sequence length. The main conceptual contribution is to use a recently proposed abstraction -- the cascade of Einsums -- to describe, formalize and taxonomize the space of attention algorithms that appear in the literature. In particular, we show how Einsum cascades can be used to infer non-trivial lower bounds on the number of passes a kernel must take through its input data, which has implications for either required on-chip buffer capacity or memory traffic. We show how this notion can be used to meaningfully divide the space of attention algorithms into several categories and use these categories to inform our design process.
  Based on the above characterization, we propose FuseMax -- a novel mapping of attention onto a spatial array-style architecture. On attention, in an iso-area comparison, FuseMax achieves an average $6.7\times$ speedup over the prior state-of-the-art FLAT while using $79\%$ of the energy. Similarly, on the full end-to-end transformer inference, FuseMax achieves an average $5.3\times$ speedup over FLAT using $83\%$ of the energy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10491v2</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nandeeka Nayak, Xinrui Wu, Toluwanimi O. Odemuyiwa, Michael Pellauer, Joel S. Emer, Christopher W. Fletcher</dc:creator>
    </item>
    <item>
      <title>Learning Generalizable Program and Architecture Representations for Performance Modeling</title>
      <link>https://arxiv.org/abs/2310.16792</link>
      <description>arXiv:2310.16792v2 Announce Type: replace-cross 
Abstract: Performance modeling is an essential tool in many areas, including performance characterization/optimization, design space exploration, and resource allocation problems, to name a few. However, existing performance modeling approaches have limitations, such as high computational cost for discrete-event simulators, narrow flexibility of hardware emulators, or restricted accuracy/generality of analytical/data-driven models. To address these limitations, this paper proposes PerfVec, a novel deep learning-based performance modeling framework that learns high-dimensional and independent/orthogonal program and microarchitecture representations. Once learned, a program representation can be used to predict its performance on any microarchitecture, and likewise, a microarchitecture representation can be applied in the performance prediction of any program. Additionally, PerfVec yields a foundation model that captures the performance essence of instructions, which can be directly used by developers in numerous performance modeling related tasks without incurring its training cost. The evaluation demonstrates that PerfVec is more general and efficient than previous approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16792v2</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingda Li, Thomas Flynn, Adolfy Hoisie</dc:creator>
    </item>
  </channel>
</rss>
