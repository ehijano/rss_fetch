<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Aug 2025 04:01:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Neural operators for solving nonlinear inverse problems</title>
      <link>https://arxiv.org/abs/2508.19347</link>
      <description>arXiv:2508.19347v1 Announce Type: new 
Abstract: We consider solving a probably infinite dimensional operator equation, where the operator is not modeled by physical laws but is specified indirectly via training pairs of the input-output relation of the operator. Neural operators have proven to be efficient to approximate operators with such information. In this paper, we analyze Tikhonov regularization with neural operators as surrogates for solving ill-posed operator equations. The analysis is based on balancing approximation errors of neural operators, regularization parameters, and noise. Moreover, we extend the approximation properties of neural operators from sets of continuous functions to Sobolev and Lebesgue spaces, which is crucial for solving inverse problems. Finally, we address the problem of finding an appropriate network structure of neural operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19347v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Otmar Scherzer, Thi Lan Nhi Vu, Jikai Yan</dc:creator>
    </item>
    <item>
      <title>Numerical Optimization for Tensor Disentanglement</title>
      <link>https://arxiv.org/abs/2508.19409</link>
      <description>arXiv:2508.19409v1 Announce Type: new 
Abstract: Tensor networks provide compact and scalable representations of high-dimensional data, enabling efficient computation in fields such as quantum physics, numerical partial differential equations (PDEs), and machine learning. This paper focuses on tensor disentangling, the task of identifying transformations that reduce bond dimensions by exploiting gauge freedom in the network. We formulate this task as an optimization problem over orthogonal matrices acting on a single tensor's indices, aiming to minimize the rank of its matricized form. We present Riemannian optimization methods and a joint optimization framework that alternates between optimizing the orthogonal transformation for a fixed low-rank approximation and optimizing the low-rank approximation for a fixed orthogonal transformation, offering a competitive alternative when the target rank is known. To seek the often unknown optimal rank, we introduce a binary search strategy integrated with the disentangling procedure. Numerical experiments on random tensors and tensors in an approximate isometric tensor network state are performed to compare different optimization methods and explore the possibility of combining different methods in a hybrid approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19409v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>quant-ph</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Wei, Alec Dektor, Chungen Shen, Zaiwen Wen, Chao Yang</dc:creator>
    </item>
    <item>
      <title>A deep first-order system least squares method for the obstacle problem</title>
      <link>https://arxiv.org/abs/2508.19412</link>
      <description>arXiv:2508.19412v1 Announce Type: new 
Abstract: We propose a deep learning approach to the obstacle problem inspired by the first-order system least-squares (FOSLS) framework. This method reformulates the problem as a convex minimization task; by simultaneously approximating the solution, gradient, and Lagrange multiplier, our approach provides a flexible, mesh-free alternative that scales efficiently to high-dimensional settings. Key theoretical contributions include the coercivity and local Lipschitz continuity of the proposed least-squares functional, along with convergence guarantees via $\Gamma$-convergence theory under mild regularity assumptions. Numerical experiments in dimensions up to 20 demonstrate the method's robustness and scalability, even on non-Lipschitz domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19412v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Acosta, Eugenia Bel\'en, Francisco M. Bersetche, Juan Pablo Borthagaray</dc:creator>
    </item>
    <item>
      <title>A Square-Root Free Algorithm for Computing Real Givens Rotations</title>
      <link>https://arxiv.org/abs/2508.19431</link>
      <description>arXiv:2508.19431v1 Announce Type: new 
Abstract: We develop an accurate square-root-free algorithm for constructing real Givens rotations. On processors that support the fused multiply-add operation in hardware, the algorithm is competitive with square-root based algorithms using a hardware square-root. Unlike the square-root-free algorithms in \cite{Hsieh1993,GENTLEMAN1973,Barlow1987,Hammarling1974ANO,Ling1989EfficientLL,Hanson,Hanson2}, our approach will construct the Givens rotation directly and is therefore applicable to a much wider variety of algorithms that use Givens rotations. We investigate the accuracy of the algorithm by simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19431v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos F. Borges</dc:creator>
    </item>
    <item>
      <title>A new class of regularized preconditioners for double saddle-point problems</title>
      <link>https://arxiv.org/abs/2508.19469</link>
      <description>arXiv:2508.19469v1 Announce Type: new 
Abstract: The block structure of double saddle-point problems has prompted extensive research into efficient preconditioners. This paper introduces a novel class of three-by-three block preconditioners tailored for such systems from the time-dependent Maxwell equations or liquid crystal director modeling. The main motivation of this work is to highlight the limitations of recent preconditioners under high Reynolds numbers, as the original studies did not explore this scenario, and to demonstrate that our preconditioner outperforms the existing ones in such regimes. We thoroughly analyze the convergence and spectral properties of the proposed preocnditioner. We illustrate the efficiency of the proposed preconditioners, and verify the theoretical bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19469v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Achraf Badahmane</dc:creator>
    </item>
    <item>
      <title>NLAFormer: Transformers Learn Numerical Linear Algebra Operations</title>
      <link>https://arxiv.org/abs/2508.19557</link>
      <description>arXiv:2508.19557v1 Announce Type: new 
Abstract: Transformers are effective and efficient at modeling complex relationships and learning patterns from structured data in many applications. The main aim of this paper is to propose and design NLAFormer, which is a transformer-based architecture for learning numerical linear algebra operations: pointwise computation, shifting, transposition, inner product, matrix multiplication, and matrix-vector multiplication. Using a linear algebra argument, we demonstrate that transformers can express such operations. Moreover, the proposed approach discards the simulation of computer control flow adopted by the loop-transformer, significantly reducing both the input matrix size and the number of required layers. By assembling linear algebra operations, NLAFormer can learn the conjugate gradient method to solve symmetric positive definite linear systems. Experiments are conducted to illustrate the numerical performance of NLAFormer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19557v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhantao Ma, Yihang Gao, Michael K. Ng</dc:creator>
    </item>
    <item>
      <title>Energy-Equidistributed Moving Sampling Physics-informed Neural Networks for Solving Conservative Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2508.19561</link>
      <description>arXiv:2508.19561v1 Announce Type: new 
Abstract: This paper presents a novel Energy-Equidistributed adaptive sampling framework for multi-dimensional conservative PDEs, introducing both location-based and velocity-based formulations of Energy-Equidistributed moving mesh PDEs (EMMPDEs). The framework utilizes the energy density function as the monitor function, ensuring that mesh adaptation dynamically tracks energy evolution during temporal integration. These theoretical developments are integrated with deep neural networks to establish the Energy-Equidistributed Moving Sampling Physics-Informed Neural Networks (EEMS-PINNs), which integrate physics-informed learning with energy-adaptive mesh optimization. Extensive numerical experiments demonstrate that EEMS-PINNs effectively maintain solution accuracy in long-time simulations while preserving conserved energy. The framework's robustness is further evidenced by its stable performance in non-conservative systems. The code for this paper can be found at https://github.com/sufe-Ran-Zhang/EMMPDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19561v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qinjiao Gao, Longzhe Xu, Dongjiang Wang, Ran Zhang</dc:creator>
    </item>
    <item>
      <title>Eighth-Order Accurate Methods for Boundary Value Problems Arising from the Lane-Emden Equation</title>
      <link>https://arxiv.org/abs/2508.19729</link>
      <description>arXiv:2508.19729v1 Announce Type: new 
Abstract: This paper presents high-order numerical methods for solving boundary value problems associated with the Lane-Emden equation, which frequently arises in astrophysics and various nonlinear models. A major challenge in studying this equation lies in its singularity at one endpoint. Prior to constructing the numerical methods, we establish the existence and uniqueness of the solution and propose a continuous iterative method. This continuous method is then discretized using the trapezoidal quadrature rule enhanced with correction terms. As a result, we derive three discrete iterative schemes tailored for three specific cases of the Lane-Emden equation. We rigorously prove that the proposed methods achieve eighth-order accuracy and convergence. A series of numerical experiments is conducted to validate the theoretical findings and demonstrate the accuracy and convergence order of the proposed schemes, which outperform existing methods. These schemes thus provide efficient tools for solving Lane-Emden boundary value problems and can be readily extended to higher-order nonlinear singular models, such as Emden-Fowler equations, which arise in many applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19729v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dang Quang A, Nguyen Thanh Huong, Vu Vinh Quang</dc:creator>
    </item>
    <item>
      <title>An adaptive finite element discretization based parallel orbital-updating method for eigenvalue problems</title>
      <link>https://arxiv.org/abs/2508.19832</link>
      <description>arXiv:2508.19832v1 Announce Type: new 
Abstract: It is significant and challenging to solve eigenvalue problems of partial differential operators when many highly accurate eigenpair approximations are required. The adaptive finite element discretization based parallel orbital-updating method, which can significantly reduce the computational cost and enhance the parallel scalability, has been shown to be efficient in electronic structure calculations. In this paper, we provide a mathematical justification for the method for clustered eigenvalue problems of linear partial differential operators, including the convergence and error estimates of the numerical approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19832v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoying Dai, Yan Li, Bin Yang, Aihui Zhou</dc:creator>
    </item>
    <item>
      <title>Dominant H-Eigenvectors of Tensor Kronecker Products Do Not Decouple</title>
      <link>https://arxiv.org/abs/2508.19902</link>
      <description>arXiv:2508.19902v1 Announce Type: new 
Abstract: We illustrate a counterexample to an open question related to the dominant H-eigenvector of a Kronecker product of tensors. For matrices and Z-eigenvectors of tensors, the dominant eigenvector of a Kronecker product decouples into a product of eigenvectors of the tensors underlying the Kronecker product. This does not occur for H-eigenvectors and indeed, the largest H-eigenvalue can exceed the product of the H-eigenvalues of the component tensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19902v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayush Kulkarni, Charles Colley, David F. Gleich</dc:creator>
    </item>
    <item>
      <title>Performance evaluation of high-order compact and second-order gas-kinetic schemes in compressible flow simulations</title>
      <link>https://arxiv.org/abs/2508.19911</link>
      <description>arXiv:2508.19911v1 Announce Type: new 
Abstract: The trade-off among accuracy, robustness, and computational cost remains a key challenge in simulating complex flows. Second-order schemes are computationally efficient but lack the accuracy required for resolving intricate flow structures, particularly in turbulence. High-order schemes, especially compact high-order schemes, offer superior accuracy and resolution at a relatively modest computational cost. To clarify the practical performance of high-order schemes in scale-resolving simulations, this study evaluates two representative gas-kinetic schemes: the newly developed fifth-order compact gas-kinetic scheme (CGKS-5th) and the conventional second-order gas-kinetic scheme (GKS-2nd). Test cases ranging from subsonic to supersonic flows are used to quantitatively assess their accuracy and efficiency. The results demonstrate that CGKS-5th achieves comparable resolution to GKS-2nd at roughly an order of magnitude lower computational cost. Under equivalent computational resources, CGKS-5th delivers significantly higher accuracy and resolution, particularly in turbulent flows involving shocks and small-scale vortices. This study provides the first clear verification of the advantages of high-order compact gas-kinetic schemes in simulating viscous flows with discontinuities. Additionally, multi-GPU parallelization using CUDA and MPI is implemented to enable large-scale applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19911v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaqing Yang, Fengxiang Zhao, Kun Xu</dc:creator>
    </item>
    <item>
      <title>High-order nonuniform time-stepping and MBP-preserving linear schemes for the time-fractional Allen-Cahn equation</title>
      <link>https://arxiv.org/abs/2508.19965</link>
      <description>arXiv:2508.19965v1 Announce Type: new 
Abstract: In this paper, we present a class of nonuniform time-stepping, high-order linear stabilized schemes that can preserve both the discrete energy stability and maximum-bound principle (MBP) for the time-fractional Allen-Cahn equation. To this end, we develop a new prediction strategy to obtain a second-order and MBP-preserving predicted solution, which is then used to handle the nonlinear potential explicitly. Additionally, we introduce an essential nonnegative auxiliary functional that enables the design of an appropriate stabilization term to dominate the predicted nonlinear potential, and thus to preserve the discrete MBP. Combining the newly developed prediction strategy and auxiliary functional, we propose two unconditionally energy-stable linear stabilized schemes, L1 and L2-$1_\sigma$ schemes. We show that the L1 scheme unconditionally preserves the discrete MBP, whereas the L2-$1_\sigma$ scheme requires a mild time-step restriction. Furthermore, we develop an improved L2-$1_\sigma$ scheme with enhanced MBP preservation for large time steps, achieved through a novel unbalanced stabilization term that leverages the boundedness and monotonicity of the auxiliary functional. Representative numerical examples validate the accuracy, effectiveness, and physics-preserving of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19965v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bingyin Zhang, Hongfei Fu, Hong Wag</dc:creator>
    </item>
    <item>
      <title>Fourier transform-based linear combination of Hamiltonian simulation</title>
      <link>https://arxiv.org/abs/2508.19596</link>
      <description>arXiv:2508.19596v1 Announce Type: cross 
Abstract: Linear combination of Hamiltonian simulation (LCHS) connects the general linear non-unitary dynamics with unitary operators and serves as the mathematical backbone of designing near-optimal quantum linear differential equation algorithms. However, the existing LCHS formalism needs to find a kernel function subject to complicated technical conditions on a half complex plane. In this work, we establish an alternative formalism of LCHS based on the Fourier transform. Our new formalism completely removes the technical requirements beyond the real axis, providing a simple and flexible way of constructing LCHS kernel functions. Specifically, we construct a different family of the LCHS kernel function, providing a $1.81$ times reduction in the quantum differential equation algorithms based on LCHS, and an $8.27$ times reduction in its quantum circuit depth at a truncation error of $\epsilon \le 10^{-8}$. Additionally, we extend the scope of the LCHS formula to the scenario of simulating linear unstable dynamics for a short or intermediate time period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19596v1</guid>
      <category>quant-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Huang, Dong An</dc:creator>
    </item>
    <item>
      <title>$\mathcal{C}^1$-approximation with rational functions and rational neural networks</title>
      <link>https://arxiv.org/abs/2508.19672</link>
      <description>arXiv:2508.19672v1 Announce Type: cross 
Abstract: We show that suitably regular functions can be approximated in the $\mathcal{C}^1$-norm both with rational functions and rational neural networks, including approximation rates with respect to width and depth of the network, and degree of the rational functions. As consequence of our results, we further obtain $\mathcal{C}^1$-approximation results for rational neural networks with the $\text{EQL}^\div$ and ParFam architecture, both of which are important in particular in the context of symbolic regression for physical law learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19672v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erion Morina, Martin Holler</dc:creator>
    </item>
    <item>
      <title>Reproducing kernel methods for machine learning, PDEs, and statistics with Python</title>
      <link>https://arxiv.org/abs/2402.07084</link>
      <description>arXiv:2402.07084v2 Announce Type: replace 
Abstract: This monograph offers an introduction to a collection of numerical algorithms implemented in the library CodPy (an acronym that stands for the Curse Of Dimensionality in PYthon), which has found widespread applications across various areas, including machine learning, statistics, and computational physics. We develop here a strategy based on the theory of reproducing kernel Hilbert spaces (RKHS) and the theory of optimal transport. Initially designed for mathematical finance, this library has since been enhanced and broadened to be applicable to problems arising in engineering and industry. In order to present the general principles and techniques employed in CodPy and its applications, we have structured this monograph into two main parts. First of all, we focus on the fundamental principles of kernel-based representations of data and solutions, also that the presentation therein is supplemented with illustrative examples only. Next, we discuss the application of these principles to many classes of concrete problems, spanning from the numerical approximation of partial differential equations to (supervised, unsupervised) machine learning, extending to generative methods with a focus on stochastic aspects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07084v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe G. LeFloch, Jean-Marc Mercier, Shohruh Miryusupov</dc:creator>
    </item>
    <item>
      <title>Which Spaces can be Embedded in $L_p$-type Reproducing Kernel Banach Space? A Characterization via Metric Entropy</title>
      <link>https://arxiv.org/abs/2410.11116</link>
      <description>arXiv:2410.11116v3 Announce Type: replace 
Abstract: In this paper, we establish a novel connection between the metric entropy growth and the embeddability of function spaces into reproducing kernel Hilbert/Banach spaces. Metric entropy characterizes the information complexity of function spaces and has implications for their approximability and learnability. Classical results show that embedding a function space into a reproducing kernel Hilbert space (RKHS) implies a bound on its metric entropy growth. Surprisingly, we prove a \textbf{converse}: a bound on the metric entropy growth of a function space allows its embedding to a $L_p-$type Reproducing Kernel Banach Space (RKBS). This shows that the ${L}_p-$type RKBS provides a broad modeling framework for learnable function classes with controlled metric entropies. Our results shed new light on the power and limitations of kernel methods for learning complex function spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11116v3</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiping Lu, Daozhe Lin, Qiang Du</dc:creator>
    </item>
    <item>
      <title>Regularity and tailored regularization of Deep Neural Networks, with application to parametric PDEs in uncertainty quantification</title>
      <link>https://arxiv.org/abs/2502.12496</link>
      <description>arXiv:2502.12496v2 Announce Type: replace 
Abstract: In this paper we consider Deep Neural Networks (DNNs) with a smooth activation function as surrogates for high-dimensional functions that are somewhat smooth but costly to evaluate. We consider the standard (non-periodic) DNNs as well as propose a new model of periodic DNNs which are especially suited for a class of periodic target functions when Quasi-Monte Carlo lattice points are used as training points. We study the regularity of DNNs by obtaining explicit bounds on all mixed derivatives with respect to the input parameters. The bounds depend on the neural network parameters as well as the choice of activation function. By imposing restrictions on the network parameters to match the regularity features of the target functions, we prove that DNNs with $N$ tailor-constructed lattice training points can achieve the generalization error (or $L_2$ approximation error) bound ${\tt tol} + \mathcal{O}(N^{-r/2})$, where ${\tt tol}\in (0,1)$ is the tolerance achieved by the training error in practice, and $r = 1/p^*$, with $p^*$ being the ``summability exponent'' of a sequence that characterises the decay of the input variables in the target functions, and with the implied constant independent of the dimensionality of the input data. We apply our analysis to popular models of parametric elliptic PDEs in uncertainty quantification. In our numerical experiments, we restrict the network parameters during training by adding tailored regularization terms, and we show that for an algebraic equation mimicking the parametric PDE problems the DNNs trained with tailored regularization perform significantly better.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12496v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexander Keller, Frances Y. Kuo, Dirk Nuyens, Ian H. Sloan</dc:creator>
    </item>
    <item>
      <title>A posteriori certification for neural networks approximations to PDEs</title>
      <link>https://arxiv.org/abs/2502.20336</link>
      <description>arXiv:2502.20336v2 Announce Type: replace 
Abstract: We propose rigorous lower and upper bounds for neural network (NN) approximations to PDEs by efficiently computing the Riesz representations of suitable extension and restrictions of the NN residual towards geometrically simpler domains, which are either embedded or enveloping the original domain. Error bounds are proven and detailed for elliptic as well as parabolic problems. Numerical experiments show the good quantitative behaviour of the derived upper and lower error bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20336v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lewin Ernst, Nikolaos Rekatsinas, Karsten Urban</dc:creator>
    </item>
    <item>
      <title>Stability analyses of divergence and vorticity damping on gnomonic cubed-sphere grids</title>
      <link>https://arxiv.org/abs/2505.05624</link>
      <description>arXiv:2505.05624v2 Announce Type: replace 
Abstract: Divergence and vorticity damping, which operate upon horizontal divergence and relative vorticity, are explicit diffusion mechanisms used in dynamical cores to ensure numerical stability. There are mesh-dependent upper bounds on the coefficients of these diffusion operators, else the diffusion itself instigates model instability. This work considers such stability limits for three gnomonic cubed-sphere meshes -- 1) equidistant, 2) equiangular, and 3) equi-edge mappings. Von Neumann analysis is used to derive linear stability limits, and these depend on the cell areas and aspect ratios of the cubed-sphere grid. The linear theory is compared to practical divergence and vorticity damping limits in NOAA GFDL's finite-volume dynamical core on the cubed-sphere (FV3), using a baroclinic wave initial condition and the equiangular and equi-edge grids. For divergence damping, both the magnitude of maximum stable coefficients and the locations of instability agree with linear theory. Due to implicit vorticity diffusion from the transport scheme, practical limits for vorticity damping are lower than the explicit stability limits. The maximum allowable vorticity damping coefficient is dependent on the choice of horizontal transport scheme for the equi-edge grid; it is hypothesised that this indicates the relative implicit diffusion of the transport scheme in this test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05624v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.ao-ph</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timothy C. Andrews, Christiane Jablonowski</dc:creator>
    </item>
    <item>
      <title>A Note on the Reliability of Goal-Oriented Error Estimates for Galerkin Finite Element Methods with Nonlinear Functionals</title>
      <link>https://arxiv.org/abs/2506.09913</link>
      <description>arXiv:2506.09913v2 Announce Type: replace 
Abstract: We consider estimating the discretization error in a nonlinear functional $J(u)$ in the setting of an abstract variational problem: find $u \in \mathcal{V}$ such that $B(u,\varphi) = L(\varphi) \; \forall \varphi \in \mathcal{V}$, as approximated by a Galerkin finite element method. Here, $\mathcal{V}$ is a Hilbert space, $B(\cdot,\cdot)$ is a bilinear form, and $L(\cdot)$ is a linear functional. We consider well-known error estimates $\eta$ of the form $J(u) - J(u_h) \approx \eta = L(z) - B(u_h, z)$, where $u_h$ denotes a finite element approximation to $u$, and $z$ denotes the solution to an auxiliary adjoint variational problem. We show that there exist nonlinear functionals for which error estimates of this form are not reliable, even in the presence of an exact adjoint solution solution $z$. An estimate $\eta$ is said to be reliable if there exists a constant $C \in \mathbb{R}_{&gt;0}$ independent of $u_h$ such that $|J(u) - J(u_h)| \leq C|\eta|$. We present several example pairs of bilinear forms and nonlinear functionals where reliability of $\eta$ is not achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09913v2</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Brian N. Granzow, Stephen D. Bond, D. Thomas Seidl, Bernhard Endtmayer</dc:creator>
    </item>
    <item>
      <title>A parameterized block-splitting preconditioner for indefinite least squares problem</title>
      <link>https://arxiv.org/abs/2507.16938</link>
      <description>arXiv:2507.16938v2 Announce Type: replace 
Abstract: We present a stationary iteration based upon a block splitting for a class of indefinite least squares problem. Convergence of the proposed method is investigated and optimal value of the involving parameter is used. The induced preconditioner is applied to accelerate the convergence of the GMRES method for solving the problem. We also analysed the eigenpair distribution of the preconditioned matrix. Some numerical are presented to show the effectiveness of the preconditioner. Numerical comparison with other well-known methods are also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16938v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 28 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Davod Khojasteh Salkuyeh</dc:creator>
    </item>
  </channel>
</rss>
