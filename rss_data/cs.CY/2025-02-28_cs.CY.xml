<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 05:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Implementation of a Generative AI Assistant in K-12 Education: The CGScholar AI Helper Initiative</title>
      <link>https://arxiv.org/abs/2502.19422</link>
      <description>arXiv:2502.19422v1 Announce Type: new 
Abstract: This paper focuses on the piloting of the CGScholar AI Helper, a Generative AI (GenAI) assistant tool that aims to provide feedback on writing in high school contexts. The aim was to use GenAI to provide formative and summative feedback on students' texts in English Language Arts (ELA) and History. The trials discussed in this paper relate to Grade 11, a crucial learning phase when students are working towards college readiness. These trials took place in two very different schools in the Midwest of the United States, one in a low socio-economic background with low-performance outcomes and the other in a high socio-economic background with high-performance outcomes. The assistant tool used two main mechanisms "prompt engineering" based on participant teachers' assessment rubric and "fine-tuning" a Large Language Model (LLM) from a customized corpus of teaching materials using Retrieval Augmented Generation (RAG). This paper focuses on the CGScholar AI Helper's potential to enhance students' writing abilities and support teachers in ELA and other subject areas requiring written assignments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19422v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vania Castro, Ana Karina de Oliveira Nascimento, Raigul Zheldibayeva, Duane Searsmith, Akash Saini, Bill Cope, Mary Kalantzis</dc:creator>
    </item>
    <item>
      <title>Are Large Language Models Ready for Business Integration? A Study on Generative AI Adoption</title>
      <link>https://arxiv.org/abs/2502.19423</link>
      <description>arXiv:2502.19423v1 Announce Type: new 
Abstract: The explorations and applications of Artificial Intelligence (AI) in various domains becomes increasingly vital as it continues to evolve. While much attention has been focused on Large Language Models (LLMs) such as ChatGPT, this research examines the readiness of other LLMs such as Google Gemini (previously Google BARD), a conversational AI chatbot, for potential business applications. Gemini is an example of a Generative AI (Gen AI) that demonstrates capabilities encompassing content generation, language translation, and information retrieval. This study aims to assess its efficacy for text simplification in catering to the demands of modern businesses. A dataset of 42,654 reviews from distinct Disneyland branches was employed. The chatbot's API was utilised with a uniform prompt to generate simplified re-views. Results presented a spectrum of responses, including 75% successful simplifications, 25% errors, and instances of model self-reference. Quantitative analysis encompassing response categorisation, error prevalence, and response length distribution was conducted. Furthermore, Natural Language Processing (NLP) metrics were applied to gauge the quality of the generated content with the original reviews. The findings offer insights into Gen AI models performance, highlighting proficiency in simplifying re-views while unveiling certain limitations in coherence and consistency since only about 7.79% of the datasets was simplified. This research contributes to the ongoing discourse on AI adoption in business contexts. The study's out-comes provide implications for future development and implementation of AI-driven tools in businesses seeking to enhance content creation and communication processes. As AI continues to transform industries, an understanding of the readiness and limitations of AI models is essential for informed decision-making, automations and effective integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19423v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Julius Sechang Mboli, John G. O. Marko, Rose Anazin Yemson</dc:creator>
    </item>
    <item>
      <title>Understanding the Disparities in Mathematics Performance: An Interpretability-Based Examination</title>
      <link>https://arxiv.org/abs/2502.19424</link>
      <description>arXiv:2502.19424v1 Announce Type: new 
Abstract: Problem. Educational disparities in Mathematics performance are a persistent challenge. This study aims to unravel the complex factors contributing to these disparities among students internationally, with a focus on the interpretability of the contributing factors. Methodology. Utilizing data from the Programme for International Student Assessment (PISA), we conducted rigorous preprocessing and variable selection to prepare for applying binary classification interpretability models. These models were trained using the Stratified K-Fold technique to ensure balanced representation and assessed using six key metrics. Solution. By applying interpretability models such as Shapley Additive Explanations (SHAP) analysis, we identified critical factors impacting student performance, including reading accessibility, critical thinking skills, gender, and geographical location. Results. Our findings reveal significant disparities linked to resource availability, with students from lower socioeconomic backgrounds possessing fewer books and demonstrating lower performance in Mathematics. The geographical analysis highlighted regional educational disparities, with certain areas consistently underperforming in PISA assessments. Gender also emerged as a determinant, with females contributing differently to performance levels across the spectrum. Conclusion. The study provides insights into the multifaceted determinants of student Mathematics performance and suggests potential avenues for future research to explore global interpretability models and further investigate the socioeconomic, cultural, and educational factors at play.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19424v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.engappai.2024.108109</arxiv:DOI>
      <arxiv:journal_reference>Engineering Applications of Artificial Intelligence (2024)</arxiv:journal_reference>
      <dc:creator>Ismael Gomez-Talal, Luis Bote-Curiel, Jose Luis Rojo-Alvarez</dc:creator>
    </item>
    <item>
      <title>Do LLMs exhibit demographic parity in responses to queries about Human Rights?</title>
      <link>https://arxiv.org/abs/2502.19463</link>
      <description>arXiv:2502.19463v1 Announce Type: new 
Abstract: This research describes a novel approach to evaluating hedging behaviour in large language models (LLMs), specifically in the context of human rights as defined in the Universal Declaration of Human Rights (UDHR). Hedging and non-affirmation are behaviours that express ambiguity or a lack of clear endorsement on specific statements. These behaviours are undesirable in certain contexts, such as queries about whether different groups are entitled to specific human rights; since all people are entitled to human rights. Here, we present the first systematic attempt to measure these behaviours in the context of human rights, with a particular focus on between-group comparisons. To this end, we design a novel prompt set on human rights in the context of different national or social identities. We develop metrics to capture hedging and non-affirmation behaviours and then measure whether LLMs exhibit demographic parity when responding to the queries. We present results on three leading LLMs and find that all models exhibit some demographic disparities in how they attribute human rights between different identity groups. Futhermore, there is high correlation between different models in terms of how disparity is distributed amongst identities, with identities that have high disparity in one model also facing high disparity in both the other models. While baseline rates of hedging and non-affirmation differ, these disparities are consistent across queries that vary in ambiguity and they are robust across variations of the precise query wording. Our findings highlight the need for work to explicitly align LLMs to human rights principles, and to ensure that LLMs endorse the human rights of all groups equally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19463v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rafiya Javed, Jackie Kay, David Yanni, Abdullah Zaini, Anushe Sheikh, Maribeth Rauh, Iason Gabriel, Laura Weidinger</dc:creator>
    </item>
    <item>
      <title>Tripartite Perspective on the Copyright-Sharing Economy in China</title>
      <link>https://arxiv.org/abs/2502.19719</link>
      <description>arXiv:2502.19719v1 Announce Type: new 
Abstract: Internet and digital technologies have facilitated copyright sharing in an unprecedented way, creating significant tensions between the free flow of information and the exclusive nature of intellectual property. Copyright owners, users, and online platforms are the three major players in the copyright system. These stakeholders and their relations form the main structure of the copyright-sharing economy. Using China as an example, this paper provides a tripartite perspective on the copyright ecology based on three categories of sharing, namely unauthorized sharing, altruistic sharing, and freemium sharing. The line between copyright owners, users, and platforms has been blurred by rapidly changing technologies and market forces. By examining the strategies and practices of these parties, this paper illustrate the opportunities and challenges for China's copyright industry and digital economy. The paper concludes that under the shadow of the law, a sustainable copyright-sharing model must carefully align the interests of businesses and individual users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19719v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.clsr.2019.05.00</arxiv:DOI>
      <arxiv:journal_reference>Computer Law &amp; Security Review, Vol. 35, Iss. 4 (2019)</arxiv:journal_reference>
      <dc:creator>Jyh-An Lee</dc:creator>
    </item>
    <item>
      <title>The erasure of intensive livestock farming in text-to-image generative AI</title>
      <link>https://arxiv.org/abs/2502.19771</link>
      <description>arXiv:2502.19771v1 Announce Type: new 
Abstract: Generative AI (e.g., ChatGPT) is increasingly integrated into people's daily lives. While it is known that AI perpetuates biases against marginalized human groups, their impact on non-human animals remains understudied. We found that ChatGPT's text-to-image model (DALL-E 3) introduces a strong bias toward romanticizing livestock farming as dairy cows on pasture and pigs rooting in mud. This bias remained when we requested realistic depictions and was only mitigated when the automatic prompt revision was inhibited. Most farmed animal in industrialized countries are reared indoors with limited space per animal, which fail to resonate with societal values. Inhibiting prompt revision resulted in images that more closely reflected modern farming practices; for example, cows housed indoors accessing feed through metal headlocks, and pigs behind metal railings on concrete floors in indoor facilities. While OpenAI introduced prompt revision to mitigate bias, in the case of farmed animal production systems, it paradoxically introduces a strong bias towards unrealistic farming practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19771v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kehan Sheng, Frank A. M. Tuyttens, Marina A. G. von Keyserlingk</dc:creator>
    </item>
    <item>
      <title>GreenDFL: a Framework for Assessing the Sustainability of Decentralized Federated Learning Systems</title>
      <link>https://arxiv.org/abs/2502.20242</link>
      <description>arXiv:2502.20242v1 Announce Type: new 
Abstract: Decentralized Federated Learning (DFL) is an emerging paradigm that enables collaborative model training without centralized data aggregation, enhancing privacy and resilience. However, its sustainability remains underexplored, as energy consumption and carbon emissions vary across different system configurations. Understanding the environmental impact of DFL is crucial for optimizing its design and deployment. This study aims to assess the sustainability of DFL systems by analyzing factors that influence energy consumption and carbon emissions. Additionally, it proposes sustainability-aware optimization strategies, including a node selection algorithm and an aggregation method, to reduce the environmental footprint of DFL without compromising model performance. The proposed framework, named GreenDFL, systematically evaluates the impact of hardware accelerators, model architecture, communication medium, data distribution, network topology, and federation size on sustainability. Empirical experiments are conducted on multiple datasets using different system configurations, measuring energy consumption and carbon emissions across various phases of the DFL lifecycle. Results indicate that local training dominates energy consumption and carbon emissions, while communication has a relatively minor impact. Optimizing model complexity, using GPUs instead of CPUs, and strategically selecting participating nodes significantly improve sustainability. Additionally, deploying nodes in regions with lower carbon intensity and integrating early stopping mechanisms further reduce emissions. The proposed framework provides a comprehensive and practical computational approach for assessing the sustainability of DFL systems. Furthermore, it offers best practices for improving environmental efficiency in DFL, making sustainability considerations more actionable in real-world deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20242v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Feng, Alberto Huertas Celdr\'an, Xi Cheng, G\'er\^ome Bovet, Burkhard Stiller</dc:creator>
    </item>
    <item>
      <title>AirCalypse: Can Twitter Help in Urban Air Quality Measurement and Who are the Influential Users?</title>
      <link>https://arxiv.org/abs/2502.19421</link>
      <description>arXiv:2502.19421v1 Announce Type: cross 
Abstract: In this digital age, Online Social Media's ubiquity has led it to it's role as a "Sensor". Starting from disaster response to political predictions, online social media like Twitter, have been instrumental and are actively researched areas. In this work, we have focused on something quite insidious in the current context, i.e., air pollution in developing regions. Starting as an empirical study on using Twitter as a "Sensor" to measure air quality, the focal point of this work is to identify the users who have been actively tweeting in the air pollution events in Delhi, the capital of India. From these users, we try to identify the influential ones, who play a significant role in creating the initial awareness and hence act as "Sensors". We have utilized a tailored "TRank" algorithm for finding out the influential users by considering \textit{Retweet, Favorite, and Follower influence} of the users. After ranking the users based on their social influence, we further study the behavior, i.e., perception of pollution from those users' posts with respect to the actual air pollution levels using the physical sensors. The tracking of influential users in air quality monitoring assists in developing a crowd sensed air quality measurement framework, which can augment the physical air quality sensors for raising awareness against air pollution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19421v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3366424.3382120</arxiv:DOI>
      <dc:creator>Prithviraj Pramanik, Tamal Mondal, Subrata Nandi, Mousumi Saha</dc:creator>
    </item>
    <item>
      <title>Will the Technological Singularity Come Soon? Modeling the Dynamics of Artificial Intelligence Development via Multi-Logistic Growth Process</title>
      <link>https://arxiv.org/abs/2502.19425</link>
      <description>arXiv:2502.19425v1 Announce Type: cross 
Abstract: We are currently in an era of escalating technological complexity and profound societal transformations, where artificial intelligence (AI) technologies exemplified by large language models (LLMs) have reignited discussions on the 'Technological Singularity'. 'Technological Singularity' is a philosophical concept referring to an irreversible and profound transformation that occurs when AI capabilities surpass those of humans comprehensively. However, quantitative modeling and analysis of the historical evolution and future trends of AI technologies remain scarce, failing to substantiate the singularity hypothesis adequately. This paper hypothesizes that the development of AI technologies could be characterized by the superposition of multiple logistic growth processes. To explore this hypothesis, we propose a multi-logistic growth process model and validate it using two real-world datasets: AI Historical Statistics and Arxiv AI Papers. Our analysis of the AI Historical Statistics dataset assesses the effectiveness of the multi-logistic model and evaluates the current and future trends in AI technology development. Additionally, cross-validation experiments on the Arxiv AI Paper, GPU Transistor and Internet User dataset enhance the robustness of our conclusions derived from the AI Historical Statistics dataset. The experimental results reveal that around 2024 marks the fastest point of the current AI wave, and the deep learning-based AI technologies are projected to decline around 2035-2040 if no fundamental technological innovation emerges. Consequently, the technological singularity appears unlikely to arrive in the foreseeable future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19425v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangyin Jin, Xiaohan Ni, Kun Wei, Jie Zhao, Haoming Zhang, Leiming Jia</dc:creator>
    </item>
    <item>
      <title>Extending the Hegselmann-Krause Model of Opinion Dynamics to include AI Oracles</title>
      <link>https://arxiv.org/abs/2502.19701</link>
      <description>arXiv:2502.19701v1 Announce Type: cross 
Abstract: The Hegselmann-Krause (HK) model of opinion dynamics describes how opinions held by individuals in a community change over time in response to the opinions of others and their access to the true value, T, to which these opinions relate. Here, I extend the simple HK model to incorporate an Artificially Intelligent (AI) Oracle that averages the opinions of members of the community. Agent-based simulations show that (1) if individuals only have access to the Oracle (and not T), and incorporate the Oracle's opinion as they update their opinions, then all opinions will converge on a common value; (2) in contrast, if all individuals also have access to T, then all opinions will ultimately converge to T, but the presence of an Oracle may delay the time to convergence; (3) if only some individuals have access to T, opinions may not converge to T, but under certain conditions, universal access to the Oracle will guarantee convergence to T; and (4) whether or not the Oracle only accesses the opinions of individuals who have access to T, or whether it accesses the opinions of everyone in the community, makes no marked difference to the extent to which the average opinion differs from T.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19701v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Allen G. Rodrigo</dc:creator>
    </item>
    <item>
      <title>Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs</title>
      <link>https://arxiv.org/abs/2502.19721</link>
      <description>arXiv:2502.19721v1 Announce Type: cross 
Abstract: Large language models (LLMs) are known to perpetuate stereotypes and exhibit biases. Various strategies have been proposed to mitigate potential harms that may result from these biases, but most work studies biases in LLMs as a black-box problem without considering how concepts are represented within the model. We adapt techniques from representation engineering to study how the concept of "gender" is represented within LLMs. We introduce a new method that extracts concept representations via probability weighting without labeled data and efficiently selects a steering vector for measuring and manipulating the model's representation. We also present a projection-based method that enables precise steering of model predictions and demonstrate its effectiveness in mitigating gender bias in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19721v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Cyberey, Yangfeng Ji, David Evans</dc:creator>
    </item>
    <item>
      <title>Towards Collaborative Anti-Money Laundering Among Financial Institutions</title>
      <link>https://arxiv.org/abs/2502.19952</link>
      <description>arXiv:2502.19952v1 Announce Type: cross 
Abstract: Money laundering is the process that intends to legalize the income derived from illicit activities, thus facilitating their entry into the monetary flow of the economy without jeopardizing their source. It is crucial to identify such activities accurately and reliably in order to enforce anti-money laundering (AML). Despite considerable efforts to AML, a large number of such activities still go undetected. Rule-based methods were first introduced and are still widely used in current detection systems. With the rise of machine learning, graph-based learning methods have gained prominence in detecting illicit accounts through the analysis of money transfer graphs. Nevertheless, these methods generally assume that the transaction graph is centralized, whereas in practice, money laundering activities usually span multiple financial institutions. Due to regulatory, legal, commercial, and customer privacy concerns, institutions tend not to share data, restricting their utility in practical usage. In this paper, we propose the first algorithm that supports performing AML over multiple institutions while protecting the security and privacy of local data. To evaluate, we construct Alipay-ECB, a real-world dataset comprising digital transactions from Alipay, the world's largest mobile payment platform, alongside transactions from E-Commerce Bank (ECB). The dataset includes over 200 million accounts and 300 million transactions, covering both intra-institution transactions and those between Alipay and ECB. This makes it the largest real-world transaction graph available for analysis. The experimental results demonstrate that our methods can effectively identify cross-institution money laundering subgroups. Additionally, experiments on synthetic datasets also demonstrate that our method is efficient, requiring only a few minutes on datasets with millions of transactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19952v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3696410.3714576</arxiv:DOI>
      <dc:creator>Zhihua Tian, Yuan Ding, Xiang Yu, Enchao Gong, Jian Liu, Kui Ren</dc:creator>
    </item>
    <item>
      <title>Hiring under Congestion and Algorithmic Monoculture: Value of Strategic Behavior</title>
      <link>https://arxiv.org/abs/2502.20063</link>
      <description>arXiv:2502.20063v1 Announce Type: cross 
Abstract: We study the impact of strategic behavior in a setting where firms compete to hire from a shared pool of applicants, and firms use a common algorithm to evaluate them. Each applicant is associated with a scalar score that is observed by all firms, provided by the algorithm. Firms simultaneously make interview decisions, where the number of interviews is capacity-constrained. Job offers are given to those who pass the interview, and an applicant who receives multiple offers accepts one of them uniformly at random. We fully characterize the set of Nash equilibria under this model. Defining social welfare as the total number of applicants who find a job, we then compare the social welfare at a Nash equilibrium to a naive baseline where all firms interview applicants with the highest scores. We show that the Nash equilibrium greatly improves upon social welfare compared to the naive baseline, especially when the interview capacity is small and the number of firms is large. We also show that the price of anarchy is small, providing further appeal for the equilibrium solution.
  We then study how the firms may converge to a Nash equilibrium. We show that when firms make interview decisions sequentially and each firm takes the best response action assuming they are the last to act, this process converges to an equilibrium when interview capacities are small. However, we show that the task of computing the best response is difficult if firms have to use its own historical samples to estimate it, while this task becomes trivial if firms have information on the degree of competition for each applicant. Therefore, converging to an equilibrium can be greatly facilitated if firms have information on the level of competition for each applicant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20063v1</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jackie Baek, Hamsa Bastani, Shihan Chen</dc:creator>
    </item>
    <item>
      <title>Mixture of Experts for Recognizing Depression from Interview and Reading Tasks</title>
      <link>https://arxiv.org/abs/2502.20213</link>
      <description>arXiv:2502.20213v1 Announce Type: cross 
Abstract: Depression is a mental disorder and can cause a variety of symptoms, including psychological, physical, and social. Speech has been proved an objective marker for the early recognition of depression. For this reason, many studies have been developed aiming to recognize depression through speech. However, existing methods rely on the usage of only the spontaneous speech neglecting information obtained via read speech, use transcripts which are often difficult to obtain (manual) or come with high word-error rates (automatic), and do not focus on input-conditional computation methods. To resolve these limitations, this is the first study in depression recognition task obtaining representations of both spontaneous and read speech, utilizing multimodal fusion methods, and employing Mixture of Experts (MoE) models in a single deep neural network. Specifically, we use audio files corresponding to both interview and reading tasks and convert each audio file into log-Mel spectrogram, delta, and delta-delta. Next, the image representations of the two tasks pass through shared AlexNet models. The outputs of the AlexNet models are given as input to a multimodal fusion method. The resulting vector is passed through a MoE module. In this study, we employ three variants of MoE, namely sparsely-gated MoE and multilinear MoE based on factorization. Findings suggest that our proposed approach yields an Accuracy and F1-score of 87.00% and 86.66% respectively on the Androids corpus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20213v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Loukas Ilias, Dimitris Askounis</dc:creator>
    </item>
    <item>
      <title>Demographic Dynamics and Artificial Intelligence: Challenges and Opportunities in Europe and Africa for 2050</title>
      <link>https://arxiv.org/abs/2403.03935</link>
      <description>arXiv:2403.03935v2 Announce Type: replace 
Abstract: This paper explores the complex relationship between demographics and artificial intelligence (AI) advances in Europe and Africa, projecting into the year 2050. The advancement of AI technologies has occurred at diverse rates, with Africa lagging behind Europe. Moreover, the imminent economic consequences of demographic shifts require a more careful examination of immigration patterns, with Africa emerging as a viable labor pool for European countries. However, within these dynamics, questions are raised about the differences in AI proficiency between African immigrants and Europeans by 2050. This paper examines demographic trends and AI developments to unravel insights into the multifaceted challenges and opportunities that lie ahead in the realms of technology, the economy, and society as we look ahead to 2050.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03935v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed El Louadi</dc:creator>
    </item>
    <item>
      <title>Using sensitive data to de-bias AI systems: Article 10(5) of the EU AI Act</title>
      <link>https://arxiv.org/abs/2410.14501</link>
      <description>arXiv:2410.14501v4 Announce Type: replace 
Abstract: In June 2024, the EU AI Act came into force. The AI Act includes obligations for the provider of an AI system. Article 10 of the AI Act includes a new obligation for providers to evaluate whether their training, validation and testing datasets meet certain quality criteria, including an appropriate examination of biases in the datasets and correction measures. With the obligation comes a new provision in Article 10(5) AI Act, allowing providers to collect sensitive data to fulfil the obligation. The exception aims to prevent discrimination. In this paper, I research the scope and implications of Article 10(5) AI Act. The paper primarily concerns European Union law, but may be relevant in other parts of the world, as policymakers aim to regulate biases in AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14501v4</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.clsr.2025.106115</arxiv:DOI>
      <arxiv:journal_reference>Computer Law &amp; Security Review 56 (2025) 106115</arxiv:journal_reference>
      <dc:creator>Marvin van Bekkum</dc:creator>
    </item>
    <item>
      <title>Who is Responsible? The Data, Models, Users or Regulations? Responsible Generative AI for a Sustainable Future</title>
      <link>https://arxiv.org/abs/2502.08650</link>
      <description>arXiv:2502.08650v3 Announce Type: replace 
Abstract: Responsible Artificial Intelligence (RAI) has emerged as a crucial framework for addressing ethical concerns in the development and deployment of Artificial Intelligence (AI) systems. A significant body of literature exists, primarily focusing on either RAI guidelines and principles or the technical aspects of RAI, largely within the realm of traditional AI. However, a notable gap persists in bridging theoretical frameworks with practical implementations in real-world settings, as well as transitioning from RAI to Responsible Generative AI (Gen AI). To bridge this gap, we present this article, which examines the challenges and opportunities in implementing ethical, transparent, and accountable AI systems in the post-ChatGPT era, an era significantly shaped by Gen AI. Our analysis includes governance and technical frameworks, the exploration of explainable AI as the backbone to achieve RAI, key performance indicators in RAI, alignment of Gen AI benchmarks with governance frameworks, reviews of AI-ready test beds, and RAI applications across multiple sectors. Additionally, we discuss challenges in RAI implementation and provide a philosophical perspective on the future of RAI. This comprehensive article aims to offer an overview of RAI, providing valuable insights for researchers, policymakers, users, and industry practitioners to develop and deploy AI systems that benefit individuals and society while minimizing potential risks and societal impacts. A curated list of resources and datasets covered in this survey is available on GitHub {https://github.com/anas-zafar/Responsible-AI}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08650v3</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaina Raza, Rizwan Qureshi, Anam Zahid, Joseph Fioresi, Ferhat Sadak, Muhammad Saeed, Ranjan Sapkota, Aditya Jain, Anas Zafar, Muneeb Ul Hassan, Aizan Zafar, Hasan Maqbool, Ashmal Vayani, Jia Wu, Maged Shoman</dc:creator>
    </item>
    <item>
      <title>The Hidden Risks of Large Reasoning Models: A Safety Assessment of R1</title>
      <link>https://arxiv.org/abs/2502.12659</link>
      <description>arXiv:2502.12659v3 Announce Type: replace 
Abstract: The rapid development of large reasoning models, such as OpenAI-o3 and DeepSeek-R1, has led to significant improvements in complex reasoning over non-reasoning large language models~(LLMs). However, their enhanced capabilities, combined with the open-source access of models like DeepSeek-R1, raise serious safety concerns, particularly regarding their potential for misuse. In this work, we present a comprehensive safety assessment of these reasoning models, leveraging established safety benchmarks to evaluate their compliance with safety regulations. Furthermore, we investigate their susceptibility to adversarial attacks, such as jailbreaking and prompt injection, to assess their robustness in real-world applications. Through our multi-faceted analysis, we uncover four key findings: (1) There is a significant safety gap between the open-source R1 models and the o3-mini model, on both safety benchmark and attack, suggesting more safety effort on R1 is needed. (2) The distilled reasoning model shows poorer safety performance compared to its safety-aligned base models. (3) The stronger the model's reasoning ability, the greater the potential harm it may cause when answering unsafe questions. (4) The thinking process in R1 models pose greater safety concerns than their final answers. Our study provides insights into the security implications of reasoning models and highlights the need for further advancements in R1 models' safety to close the gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12659v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Shreedhar Jangam, Jayanth Srinivasa, Gaowen Liu, Dawn Song, Xin Eric Wang</dc:creator>
    </item>
    <item>
      <title>African Data Ethics: A Discursive Framework for Black Decolonial Data Science</title>
      <link>https://arxiv.org/abs/2502.16043</link>
      <description>arXiv:2502.16043v2 Announce Type: replace 
Abstract: Most artificial intelligence (AI) and other data-driven systems (DDS) are created by and for the benefit of global superpowers. The shift towards pluralism in global data ethics acknowledges the importance of including perspectives from the Global Majority to develop responsible data science (RDS) practices that mitigate systemic harms inherent to the current data science ecosystem. African practitioners, in particular, are disseminating progressive data ethics principles and best practices for identifying and navigating anti-blackness, colonialism, and data dispossession in the data science life cycle. However, their perspectives continue to be left at the periphery of global data ethics conversations. In an effort to center African voices, we present a framework for African data ethics informed by an interdisciplinary corpus of African scholarship. By conducting a thematic analysis of 47 documents, our work leverages concepts of African philosophy to develop a framework with seven major principles: 1) decolonize &amp; challenge internal power asymmetry, 2) center all communities, 3) uphold universal good, 4) communalism in practice, 5) data self-determination, 6) invest in data institutions &amp; infrastructures and 7) prioritize education &amp; youth. We compare a subset of six particularist data ethics frameworks against ours and find similar coverage but diverging interpretations of shared values. We also discuss two case studies from the African data science community to demonstrate the framework as a tool for evaluating responsible data science decisions. Our framework highlights Africa as a pivotal site for challenging anti-blackness and algorithmic colonialism by promoting the practice of collectivism, self-determination, and cultural preservation in data science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16043v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Teanna Barrett, Chinasa T. Okolo, B. Biira, Eman Sherif, Amy X. Zhang, Leilani Battle</dc:creator>
    </item>
    <item>
      <title>Mapping out AI Functions in Intelligent Disaster (Mis)Management and AI-Caused Disasters</title>
      <link>https://arxiv.org/abs/2502.16644</link>
      <description>arXiv:2502.16644v2 Announce Type: replace 
Abstract: This study provides a classification of disasters in terms of their causal parameters, introducing hypothetical cases of independent or hybrid AI-caused disasters. We overview the role of AI in disaster management, and possible ethical repercussions of the use of AI in intelligent disaster management - IDM. Next, we scrutinize certain ways of preventing or alleviating the ethical repercussions of AI use in disaster mismanagement, such as privacy breaches, biases, discriminations, etc. These include pre-design a priori, in-design, and post-design methods as well as regulations. We then discuss the government's role in preventing the ethical repercussions of AI use in IDM and identify and assess its deficits and challenges. We then discuss the advantages and disadvantages of pre-design or embedded ethics. Finally, we briefly consider the question of accountability and liability in AI-caused disasters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16644v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasser Pouresmaeil, Saleh Afroogh, Junfeng Jiao</dc:creator>
    </item>
    <item>
      <title>DeepSeek reshaping healthcare in China's tertiary hospitals</title>
      <link>https://arxiv.org/abs/2502.16732</link>
      <description>arXiv:2502.16732v2 Announce Type: replace 
Abstract: The rapid integration of artificial intelligence (AI) into healthcare is transforming clinical decision-making and hospital operations. DeepSeek has emerged as a leading AI system, widely deployed across China's tertiary hospitals since January 2025. Initially implemented in Shanghai's major medical institutions, it has since expanded nationwide, enhancing diagnostic accuracy, streamlining workflows, and improving patient management. AI-powered pathology, imaging analysis, and clinical decision support systems have demonstrated significant potential in optimizing medical processes and reducing the cognitive burden on healthcare professionals. However, the widespread adoption of AI in healthcare raises critical regulatory and ethical challenges, particularly regarding accountability in AI-assisted diagnosis and the risk of automation bias. The absence of a well-defined liability framework underscores the need for policies that ensure AI functions as an assistive tool rather than an autonomous decision-maker. With continued technological advancements, AI is expected to integrate multimodal data sources, such as genomics and radiomics, paving the way for precision medicine and personalized treatment strategies. The future of AI in healthcare depends on the development of transparent regulatory structures, industry collaboration, and adaptive governance frameworks that balance innovation with responsibility, ensuring equitable and effective AI-driven medical services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16732v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jishizhan Chen, Qingzeng Zhang</dc:creator>
    </item>
    <item>
      <title>Characterizing the effect of retractions on publishing careers</title>
      <link>https://arxiv.org/abs/2306.06710</link>
      <description>arXiv:2306.06710v3 Announce Type: replace-cross 
Abstract: Retracting academic papers is a fundamental tool of quality control, but it may have far-reaching consequences for retracted authors and their careers. Previous studies have highlighted the adverse effects of retractions on citation counts and coauthors' citations; however, the broader impacts beyond these have not been fully explored. We address this gap leveraging Retraction Watch, the most extensive data set on retractions and link it to Microsoft Academic Graph and Altmetric. Retracted authors, particularly those with less experience, often leave scientific publishing in the aftermath of retraction, especially if their retractions attract widespread attention. However, retracted authors who remain active in publishing maintain and establish more collaborations compared to their similar non-retracted counterparts. Nevertheless, retracted authors generally retain less senior and less productive coauthors, but gain more impactful coauthors post-retraction. Our findings suggest that retractions may impose a disproportionate impact on early-career authors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06710v3</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shahan Ali Memon, Kinga Makovi, Bedoor AlShebli</dc:creator>
    </item>
    <item>
      <title>The Impact of Unstated Norms in Bias Analysis of Language Models</title>
      <link>https://arxiv.org/abs/2404.03471</link>
      <description>arXiv:2404.03471v4 Announce Type: replace-cross 
Abstract: Bias in large language models (LLMs) has many forms, from overt discrimination to implicit stereotypes. Counterfactual bias evaluation is a widely used approach to quantifying bias and often relies on template-based probes that explicitly state group membership. It measures whether the outcome of a task performed by an LLM is invariant to a change in group membership. In this work, we find that template-based probes can lead to unrealistic bias measurements. For example, LLMs appear to mistakenly cast text associated with White race as negative at higher rates than other groups. We hypothesize that this arises artificially via a mismatch between commonly unstated norms, in the form of markedness, in the pretraining text of LLMs (e.g., Black president vs. president) and templates used for bias measurement (e.g., Black president vs. White president). The findings highlight the potential misleading impact of varying group membership through explicit mention in counterfactual bias quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03471v4</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farnaz Kohankhaki, D. B. Emerson, Jacob-Junqi Tian, Laleh Seyyed-Kalantari, Faiza Khan Khattak</dc:creator>
    </item>
    <item>
      <title>Topic Classification of Case Law Using a Large Language Model and a New Taxonomy for UK Law: AI Insights into Summary Judgment</title>
      <link>https://arxiv.org/abs/2405.12910</link>
      <description>arXiv:2405.12910v3 Announce Type: replace-cross 
Abstract: This paper addresses a critical gap in legal analytics by developing and applying a novel taxonomy for topic classification of summary judgment cases in the United Kingdom. Using a curated dataset of summary judgment cases, we use the Large Language Model Claude 3 Opus to explore functional topics and trends. We find that Claude 3 Opus correctly classified the topic with an accuracy of 87.13% and an F1 score of 0.87. The analysis reveals distinct patterns in the application of summary judgments across various legal domains. As case law in the United Kingdom is not originally labelled with keywords or a topic filtering option, the findings not only refine our understanding of the thematic underpinnings of summary judgments but also illustrate the potential of combining traditional and AI-driven approaches in legal classification. Therefore, this paper provides a new and general taxonomy for UK law. The implications of this work serve as a foundation for further research and policy discussions in the field of judicial administration and computational legal research methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12910v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10506-025-09434-0</arxiv:DOI>
      <arxiv:journal_reference>Artificial Intelligence and Law (2025)</arxiv:journal_reference>
      <dc:creator>Holli Sargeant, Ahmed Izzidien, Felix Steffek</dc:creator>
    </item>
    <item>
      <title>Explain the Black Box for the Sake of Science: the Scientific Method in the Era of Generative Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2406.10557</link>
      <description>arXiv:2406.10557v5 Announce Type: replace-cross 
Abstract: The scientific method is the cornerstone of human progress across all branches of the natural and applied sciences, from understanding the human body to explaining how the universe works. The scientific method is based on identifying systematic rules or principles that describe the phenomenon of interest in a reproducible way that can be validated through experimental evidence. In the era of generative artificial intelligence, there are discussions on how AI systems may discover new knowledge. We argue that human complex reasoning for scientific discovery remains of vital importance, at least before the advent of artificial general intelligence. Yet, AI can be leveraged for scientific discovery via explainable AI. More specifically, knowing the `principles' the AI systems used to make decisions can be a point of contact with domain experts and scientists, that can lead to divergent or convergent views on a given scientific problem. Divergent views may spark further scientific investigations leading to interpretability-guided explanations (IGEs), and possibly to new scientific knowledge. We define this field as Explainable AI for Science, where domain experts -- potentially assisted by generative AI -- formulate scientific hypotheses and explanations based on the interpretability of a predictive AI system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10557v5</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>math.DS</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianmarco Mengaldo</dc:creator>
    </item>
    <item>
      <title>Enhancing Safety in Reinforcement Learning with Human Feedback via Rectified Policy Optimization</title>
      <link>https://arxiv.org/abs/2410.19933</link>
      <description>arXiv:2410.19933v2 Announce Type: replace-cross 
Abstract: Balancing helpfulness and safety (harmlessness) is a critical challenge in aligning large language models (LLMs). Current approaches often decouple these two objectives, training separate preference models for helpfulness and safety, while framing safety as a constraint within a constrained Markov Decision Process (CMDP) framework. This paper identifies a potential issue when using the widely adopted expected safety constraints for LLM safety alignment, termed "safety compensation", where the constraints are satisfied on expectation, but individual prompts may trade off safety, resulting in some responses being overly restrictive while others remain unsafe. To address this issue, we propose Rectified Policy Optimization (RePO), which replaces the expected safety constraint with critical safety constraints imposed on every prompt. At the core of RePO is a policy update mechanism driven by rectified policy gradients, which penalizes the strict safety violation of every prompt, thereby enhancing safety across nearly all prompts. Our experiments demonstrate that RePO outperforms strong baseline methods and significantly enhances LLM safety alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19933v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiyue Peng, Hengquan Guo, Jiawei Zhang, Dongqing Zou, Ziyu Shao, Honghao Wei, Xin Liu</dc:creator>
    </item>
  </channel>
</rss>
