<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2024 01:42:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bipartite Matching in Massive Graphs: A Tight Analysis of EDCS</title>
      <link>https://arxiv.org/abs/2406.07630</link>
      <description>arXiv:2406.07630v1 Announce Type: new 
Abstract: Maximum matching is one of the most fundamental combinatorial optimization problems with applications in various contexts such as balanced clustering, data mining, resource allocation, and online advertisement. In many of these applications, the input graph is massive. The sheer size of these inputs makes it impossible to store the whole graph in the memory of a single machine and process it there. Graph sparsification has been an extremely powerful tool to alleviate this problem. In this paper, we study a highly successful and versatile sparsifier for the matching problem: the *edge-degree constrained subgraph (EDCS)* introduced first by Bernstein and Stein [ICALP'15].
  The EDCS has a parameter $\beta \geq 2$ which controls the density of the sparsifier. It has been shown through various proofs in the literature that by picking a subgraph with $O(n\beta)$ edges, the EDCS includes a matching of size at least $2/3-O(1/\beta)$ times the maximum matching size. As such, by increasing $\beta$ the approximation ratio of EDCS gets closer and closer to $2/3$.
  In this paper, we propose a new approach for analyzing the approximation ratio of EDCS. Our analysis is *tight* for any value of $\beta$. Namely, we pinpoint the precise approximation ratio of EDCS for any sparsity parameter $\beta$. Our analysis reveals that one does not necessarily need to increase $\beta$ to improve approximation, as suggested by previous analysis. In particular, the best choice turns out to be $\beta = 6$, which achieves an approximation ratio of $.677$! This is arguably surprising as it is even better than $2/3 \sim .666$, the bound that was widely believed to be the limit for EDCS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07630v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amir Azarmehr, Soheil Behnezhad, Mohammad Roghani</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Integer Programming Formulation of Graph Matching Problems</title>
      <link>https://arxiv.org/abs/2406.07666</link>
      <description>arXiv:2406.07666v1 Announce Type: new 
Abstract: Graph theory has been a powerful tool in solving difficult and complex problems arising in all disciplines. In particular, graph matching is a classical problem in pattern analysis with enormous applications. Many graph problems have been formulated as a mathematical program and then solved using exact, heuristic, and/or approximated-guaranteed procedures. On the other hand, graph theory has been a powerful tool in visualizing and understanding complex mathematical programming problems, especially integer programs. Formulating a graph problem as a natural integer program (IP) is often a challenging task. However, an IP formulation of the problem has many advantages. Several researchers have noted the need for natural IP formulation of graph theoretic problems. The present study aims to provide a unified framework for IP formulation of graph-matching problems. Although there are many surveys on graph matching problems, none is concerned with IP formulation. This paper is the first to provide a comprehensive IP formulation for such problems. The framework includes a variety of graph optimization problems in the literature. While these problems have been studied by different research communities, however, the framework presented here helps to bring efforts from different disciplines to tackle such diverse and complex problems. We hope the present study can significantly help to simplify some of the difficult problems arising in practice, especially in pattern analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07666v1</guid>
      <category>cs.DS</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bahram Alidaee, Haibo Wang, Hugh Sloan</dc:creator>
    </item>
    <item>
      <title>Approximating Optimum Online for Capacitated Resource Allocation</title>
      <link>https://arxiv.org/abs/2406.07757</link>
      <description>arXiv:2406.07757v1 Announce Type: new 
Abstract: We study online capacitated resource allocation, a natural generalization of online stochastic max-weight bipartite matching. This problem is motivated by ride-sharing and Internet advertising applications, where online arrivals may have the capacity to serve multiple offline users.
  Our main result is a polynomial-time online algorithm which is $(1/2 + \kappa)$-approximate to the optimal online algorithm for $\kappa = 0.0115$. This can be contrasted to the (tight) $1/2$-competitive algorithms to the optimum offline benchmark from the prophet inequality literature. Optimum online is a recently popular benchmark for online Bayesian problems which can use unbounded computation, but not "prophetic" knowledge of future inputs.
  Our algorithm (which also works for the case of stochastic rewards) rounds a generalized LP relaxation from the unit-capacity case via a two-proposal algorithm, as in previous works in the online matching literature. A key technical challenge in deriving our guarantee is bounding the positive correlation among users introduced when rounding our LP relaxation online. Unlike in the case of unit capacities, this positive correlation is unavoidable for guarantees beyond $1/2$. Conceptually, our results show that the study of optimum online as a benchmark can reveal problem-specific insights that are irrelevant to competitive analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07757v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Braun, Thomas Kesselheim, Tristan Pollner, Amin Saberi</dc:creator>
    </item>
    <item>
      <title>Efficient Parallel Multi-Hop Reasoning: A Scalable Approach for Knowledge Graph Analysis</title>
      <link>https://arxiv.org/abs/2406.07727</link>
      <description>arXiv:2406.07727v1 Announce Type: cross 
Abstract: Multi-hop reasoning (MHR) is a process in artificial intelligence and natural language processing where a system needs to make multiple inferential steps to arrive at a conclusion or answer. In the context of knowledge graphs or databases, it involves traversing multiple linked entities and relationships to understand complex queries or perform tasks requiring a deeper understanding. Multi-hop reasoning is a critical function in various applications, including question answering, knowledge base completion, and link prediction. It has garnered significant interest in artificial intelligence, machine learning, and graph analytics.
  This paper focuses on optimizing MHR for time efficiency on large-scale graphs, diverging from the traditional emphasis on accuracy which is an orthogonal goal. We introduce a novel parallel algorithm that harnesses domain-specific learned embeddings to efficiently identify the top K paths between vertices in a knowledge graph to find the best answers to a three-hop query. Our contributions are: (1) We present a new parallel algorithm to enhance MHR performance, scalability and efficiency. (2) We demonstrate the algorithm's superior performance on leading-edge Intel and AMD architectures through empirical results.
  We showcase the algorithm's practicality through a case study on identifying academic affiliations of potential Turing Award laureates in Deep Learning, highlighting its capability to handle intricate entity relationships. This demonstrates the potential of our approach to enabling high-performance MHR, useful to navigate the growing complexity of modern knowledge graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07727v1</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesmin Jahan Tithi, Fabio Checconi, Fabrizio Petrini</dc:creator>
    </item>
    <item>
      <title>A square root algorithm faster than Newton's method for multiprecision numbers, using floating-point arithmetic</title>
      <link>https://arxiv.org/abs/2406.07751</link>
      <description>arXiv:2406.07751v1 Announce Type: cross 
Abstract: In this paper, an optimized version of classical Bombelli's algorithm for computing integer square roots is presented. In particular, floating-point arithmetic is used to compute the initial guess of each digit of the root, following similar ideas to those used in "The Art of Computer Programming" Vol. 2, p. 4.3.1 for division. A program with an implementation of the algorithm in Java is also presented, and its running time is compared with that of the algorithm provided by the Java standard library, which uses the Newton's method. From tests, the algorithm presented here turns out to be much faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07751v1</guid>
      <category>cs.MS</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabio Romano</dc:creator>
    </item>
    <item>
      <title>DPSW-Sketch: A Differentially Private Sketch Framework for Frequency Estimation over Sliding Windows (Technical Report)</title>
      <link>https://arxiv.org/abs/2406.07953</link>
      <description>arXiv:2406.07953v1 Announce Type: cross 
Abstract: The sliding window model of computation captures scenarios in which data are continually arriving in the form of a stream, and only the most recent $w$ items are used for analysis. In this setting, an algorithm needs to accurately track some desired statistics over the sliding window using a small space. When data streams contain sensitive information about individuals, the algorithm is also urgently needed to provide a provable guarantee of privacy. In this paper, we focus on the two fundamental problems of privately (1) estimating the frequency of an arbitrary item and (2) identifying the most frequent items (i.e., \emph{heavy hitters}), in the sliding window model. We propose \textsc{DPSW-Sketch}, a sliding window framework based on the count-min sketch that not only satisfies differential privacy over the stream but also approximates the results for frequency and heavy-hitter queries within bounded errors in sublinear time and space w.r.t.~$w$. Extensive experiments on five real-world and synthetic datasets show that \textsc{DPSW-Sketch} provides significantly better utility-privacy trade-offs than state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07953v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3637528.3671694</arxiv:DOI>
      <dc:creator>Yiping Wang, Yanhao Wang, Cen Chen</dc:creator>
    </item>
    <item>
      <title>Resource Leveling: Complexity of a UET two-processor scheduling variant and related problems</title>
      <link>https://arxiv.org/abs/2406.08104</link>
      <description>arXiv:2406.08104v1 Announce Type: cross 
Abstract: This paper mainly focuses on a resource leveling variant of a two-processor scheduling problem. The latter problem is to schedule a set of dependent UET jobs on two identical processors with minimum makespan. It is known to be polynomial-time solvable.
  In the variant we consider, the resource constraint on processors is relaxed and the objective is no longer to minimize makespan. Instead, a deadline is imposed on the makespan and the objective is to minimize the total resource use exceeding a threshold resource level of two. This resource leveling criterion is known as the total overload cost. Sophisticated matching arguments allow us to provide a polynomial algorithm computing the optimal solution as a function of the makespan deadline. It extends a solving method from the literature for the two-processor scheduling problem.
  Moreover, the complexity of related resource leveling problems sharing the same objective is studied. These results lead to polynomial or pseudo-polynomial algorithms or NP-hardness proofs, allowing for an interesting comparison with classical machine scheduling problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08104v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pascale Bendotti, Luca Brunod Indrigo, Philippe Chr\'etienne, Bruno Escoffier</dc:creator>
    </item>
    <item>
      <title>Threshold Testing and Semi-Online Prophet Inequalities</title>
      <link>https://arxiv.org/abs/2307.01776</link>
      <description>arXiv:2307.01776v3 Announce Type: replace 
Abstract: We study threshold testing, an elementary probing model with the goal to choose a large value out of $n$ i.i.d. random variables. An algorithm can test each variable $X_i$ once for some threshold $t_i$, and the test returns binary feedback whether $X_i \ge t_i$ or not. Thresholds can be chosen adaptively or non-adaptively by the algorithm. Given the results for the tests of each variable, we then select the variable with highest conditional expectation. We compare the expected value obtained by the testing algorithm with expected maximum of the variables. Threshold testing is a semi-online variant of the gambler's problem and prophet inequalities. Indeed, the optimal performance of non-adaptive algorithms for threshold testing is governed by the standard i.i.d. prophet inequality of approximately $0.745+o(1)$ as $n \to \infty$. We show how adaptive algorithms can significantly improve upon this ratio. Our adaptive testing strategy guarantees a competitive ratio of at least $0.869-o(1)$. Moreover, we show that there are distributions that admit only a constant ratio $c &lt; 1$, even when $n \to \infty$. Finally, when each box can be tested multiple times (with $n$ tests in total), we design an algorithm that achieves a ratio of $1-o(1)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01776v3</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.ESA.2023.16</arxiv:DOI>
      <dc:creator>Martin Hoefer, Kevin Schewior</dc:creator>
    </item>
    <item>
      <title>Additive approximation algorithm for geodesic centers in $\delta$-hyperbolic graphs</title>
      <link>https://arxiv.org/abs/2404.03812</link>
      <description>arXiv:2404.03812v2 Announce Type: replace 
Abstract: For an integer $k\geq 1$, the objective of \textsc{$k$-Geodesic Center} is to find a set $\mathcal{C}$ of $k$ isometric paths such that the maximum distance between any vertex $v$ and $\mathcal{C}$ is minimised. Introduced by Gromov, \emph{$\delta$-hyperbolicity} measures how treelike a graph is from a metric point of view. Our main contribution in this paper is to provide an additive $O(\delta)$-approximation algorithm for \textsc{$k$-Geodesic Center} on $\delta$-hyperbolic graphs. On the way, we define a coarse version of the pairing property introduced by Gerstel \&amp; Zaks (Networks, 1994) and show it holds for $\delta$-hyperbolic graphs. This result allows to reduce the \textsc{$k$-Geodesic Center} problem to its rooted counterpart, a main idea behind our algorithm. We also adapt a technique of Dragan \&amp; Leitert, (TCS, 2017) to show that for every $k\geq 1$, $k$-\textsc{Geodesic Center} is NP-hard even on partial grids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03812v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dibyayan Chakraborty, Yann Vax\`es</dc:creator>
    </item>
    <item>
      <title>Optimized Deletion From an AVL Tree</title>
      <link>https://arxiv.org/abs/2406.05162</link>
      <description>arXiv:2406.05162v2 Announce Type: replace 
Abstract: An AVL tree is a binary search tree that guarantees $ O\left( \log n \right ) $ search. The guarantee is obtained at the cost of rebalancing the AVL tree, potentially after every insertion or deletion. This article proposes a deletion algorithm that reduces rebalancing after deletion by 19 percent compared to previously reported deletion algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05162v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Russell A. Brown</dc:creator>
    </item>
    <item>
      <title>Variational Quantum Algorithms for Semidefinite Programming</title>
      <link>https://arxiv.org/abs/2112.08859</link>
      <description>arXiv:2112.08859v3 Announce Type: replace-cross 
Abstract: A semidefinite program (SDP) is a particular kind of convex optimization problem with applications in operations research, combinatorial optimization, quantum information science, and beyond. In this work, we propose variational quantum algorithms for approximately solving SDPs. For one class of SDPs, we provide a rigorous analysis of their convergence to approximate locally optimal solutions, under the assumption that they are weakly constrained (i.e., $N\gg M$, where $N$ is the dimension of the input matrices and $M$ is the number of constraints). We also provide algorithms for a more general class of SDPs that requires fewer assumptions. Finally, we numerically simulate our quantum algorithms for applications such as MaxCut, and the results of these simulations provide evidence that convergence still occurs in noisy settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.08859v3</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhrumil Patel, Patrick J. Coles, Mark M. Wilde</dc:creator>
    </item>
    <item>
      <title>A smoothed-Bayesian approach to frequency recovery from sketched data</title>
      <link>https://arxiv.org/abs/2309.15408</link>
      <description>arXiv:2309.15408v2 Announce Type: replace-cross 
Abstract: We provide a novel statistical perspective on a classical problem at the intersection of computer science and information theory: recovering the empirical frequency of a symbol in a large discrete dataset using only a compressed representation, or sketch, obtained via random hashing. Departing from traditional algorithmic approaches, recent works have proposed Bayesian nonparametric (BNP) methods that can provide more informative frequency estimates by leveraging modeling assumptions about the distribution of the sketched data. In this paper, we propose a {\em smoothed-Bayesian} method, inspired by existing BNP approaches but designed in a frequentist framework to overcome the computational limitations of the BNP approaches when dealing with large-scale data from realistic distributions, including those with power-law tail behaviors. For sketches obtained with a single hash function, our approach is supported by rigorous frequentist properties, including unbiasedness and optimality under a squared error loss function within an intuitive class of linear estimators. For sketches with multiple hash functions, we introduce an approach based on \emph{multi-view} learning to construct computationally efficient frequency estimators. We validate our method on synthetic and real data, comparing its performance to that of existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15408v2</guid>
      <category>stat.ME</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Stefano Favaro, Matteo Sesia</dc:creator>
    </item>
    <item>
      <title>Fitting an ellipsoid to random points: predictions using the replica method</title>
      <link>https://arxiv.org/abs/2310.01169</link>
      <description>arXiv:2310.01169v2 Announce Type: replace-cross 
Abstract: We consider the problem of fitting a centered ellipsoid to $n$ standard Gaussian random vectors in $\mathbb{R}^d$, as $n, d \to \infty$ with $n/d^2 \to \alpha &gt; 0$. It has been conjectured that this problem is, with high probability, satisfiable (SAT; that is, there exists an ellipsoid passing through all $n$ points) for $\alpha &lt; 1/4$, and unsatisfiable (UNSAT) for $\alpha &gt; 1/4$. In this work we give a precise analytical argument, based on the non-rigorous replica method of statistical physics, that indeed predicts a SAT/UNSAT transition at $\alpha = 1/4$, as well as the shape of a typical fitting ellipsoid in the SAT phase (i.e., the lengths of its principal axes). Besides the replica method, our main tool is the dilute limit of extensive-rank "HCIZ integrals" of random matrix theory. We further study different explicit algorithmic constructions of the matrix characterizing the ellipsoid. In particular, we show that a procedure based on minimizing its nuclear norm yields a solution in the whole SAT phase. Finally, we characterize the SAT/UNSAT transition for ellipsoid fitting of a large class of rotationally-invariant random vectors. Our work suggests mathematically rigorous ways to analyze fitting ellipsoids to random vectors, which is the topic of a companion work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01169v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Maillard, Dmitriy Kunisky</dc:creator>
    </item>
    <item>
      <title>On $[1,2]$-Domination in Interval and Circle Graphs</title>
      <link>https://arxiv.org/abs/2403.04694</link>
      <description>arXiv:2403.04694v2 Announce Type: replace-cross 
Abstract: A subset $S$ of vertices in a graph $G=(V, E)$ is Dominating Set if each vertex in $V(G)\setminus S$ is adjacent to at least one vertex in $S$. Chellali et al. in 2013, by restricting the number of neighbors in $S$ of a vertex outside $S$, introduced the concept of $[1,j]$-dominating set. A set $D \subseteq V$ of a graph $G = (V, E)$ is called $[1,j]$-Dominating Set of $G$ if every vertex not in $D$ has at least one neighbor and at most $j$ neighbors in $D$. The Minimum $[1,j]$-Domination problem is the problem of finding the minimum set $D$. Given a positive integer $k$ and a graph $G = (V, E)$, the $[1,j]$-Domination Decision problem is to decide whether $G$ has $[1,j]$-dominating set of cardinality at most $k$. A polynomial-time algorithm was obtained in split graphs for a constant $j$ in contrast to the classic Dominating Set problem which is NP-hard in split graphs. This result motivates us to investigate the effect of restriction $j$ on the complexity of $[1,j]$-domination problem on various classes of graphs. Although for $j\geq 3$, it has been proved that the minimum of classical domination is equal to minimum $[1,j]$-domination in interval graphs, the complexity of finding the minimum $[1,2]$-domination in interval graphs is still outstanding. In this paper, we propose a polynomial-time algorithm for computing a minimum $[1,2]$ on non-proper interval graphs by a dynamic programming technique. Next, on the negative side, we show that the minimum $[1,2]$-dominating set problem on circle graphs is $NP$-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04694v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Alambardar Meybodi, Abolfazl Poureidi</dc:creator>
    </item>
    <item>
      <title>Beating Posits at Their Own Game: Takum Arithmetic</title>
      <link>https://arxiv.org/abs/2404.18603</link>
      <description>arXiv:2404.18603v2 Announce Type: replace-cross 
Abstract: Recent evaluations have highlighted the tapered posit number format as a promising alternative to the uniform precision IEEE 754 floating-point numbers, which suffer from various deficiencies. Although the posit encoding scheme offers superior coding efficiency at values close to unity, its efficiency markedly diminishes with deviation from unity. This reduction in efficiency leads to suboptimal encodings and a consequent diminution in dynamic range, thereby rendering posits suboptimal for general-purpose computer arithmetic.
  This paper introduces and formally proves 'takum' as a novel general-purpose logarithmic tapered-precision number format, synthesising the advantages of posits in low-bit applications with high encoding efficiency for numbers distant from unity. Takums exhibit an asymptotically constant dynamic range in terms of bit string length, which is delineated in the paper to be suitable for a general-purpose number format. It is demonstrated that takums either match or surpass existing alternatives. Moreover, takums address several issues previously identified in posits while unveiling novel and beneficial arithmetic properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18603v2</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laslo Hunhold</dc:creator>
    </item>
  </channel>
</rss>
