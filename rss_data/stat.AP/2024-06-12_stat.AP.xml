<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Jun 2024 04:01:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Classification Modeling with RNN-Based, Random Forest, and XGBoost for Imbalanced Data: A Case of Early Crash Detection in ASEAN-5 Stock Markets</title>
      <link>https://arxiv.org/abs/2406.07888</link>
      <description>arXiv:2406.07888v1 Announce Type: new 
Abstract: This research aims to evaluate the performance of several Recurrent Neural Network (RNN) architectures including Simple RNN, Gated Recurrent Units (GRU), and Long Short-Term Memory (LSTM), compared to classic algorithms such as Random Forest and XGBoost in building classification models for early crash detection in ASEAN-5 stock markets. The study is examined using imbalanced data, which is common due to the rarity of market crashes. The study analyzes daily data from 2010 to 2023 across the major stock markets of the ASEAN-5 countries, including Indonesia, Malaysia, Singapore, Thailand, and Philippines. Market crash is identified as the target variable when the major stock price indices fall below the Value at Risk (VaR) thresholds of 5%, 2.5% and 1%. predictors involving technical indicators of major local and global markets as well as commodity markets. This study includes 213 predictors with their respective lags (5, 10, 15, 22, 50, 200) and uses a time step of 7, expanding the total number of predictors to 1491. The challenge of data imbalance is addressed with SMOTE-ENN. The results show that all RNN-Based architectures outperform Random Forest and XGBoost. Among the various RNN architectures, Simple RNN stands out as the most superior, mainly due to the data characteristics that are not overly complex and focus more on short-term information. This study enhances and extends the range of phenomena observed in previous studies by incorporating variables like different geographical zones and time periods, as well as methodological adjustments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07888v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deri Siswara, Agus M. Soleh, Aji Hamim Wigena</dc:creator>
    </item>
    <item>
      <title>Fault detection in propulsion motors in the presence of concept drift</title>
      <link>https://arxiv.org/abs/2406.08030</link>
      <description>arXiv:2406.08030v1 Announce Type: new 
Abstract: Machine learning and statistical methods can be used to enhance monitoring and fault prediction in marine systems. These methods rely on a dataset with records of historical system behaviour, potentially containing periods of both fault-free and faulty operation. An unexpected change in the underlying system, called a concept drift, may impact the performance of these methods, triggering the need for model retraining or other adaptations. In this article, we present an approach for detecting overheating in stator windings of marine propulsion motors that is able to successfully operate during concept drift without the need for full model retraining. Two distinct approaches are presented and tested. All models are trained and verified using a dataset from operational propulsion motors, with known, sudden concept drifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08030v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Tveten, Morten Stakkeland</dc:creator>
    </item>
    <item>
      <title>Improving subgroup analysis using methods to extend inferences to specific target populations</title>
      <link>https://arxiv.org/abs/2406.08297</link>
      <description>arXiv:2406.08297v1 Announce Type: new 
Abstract: Subgroup analyses are common in epidemiologic and clinical research. Unfortunately, restriction to subgroup members to test for heterogeneity can yield imprecise effect estimates. If the true effect differs between members and non-members due to different distributions of other measured effect measure modifiers (EMMs), leveraging data from non-members can improve the precision of subgroup effect estimates. We obtained data from the PRIME RCT of panitumumab in patients with metastatic colon and rectal cancer from Project Datasphere(TM) to demonstrate this method. We weighted non-Hispanic White patients to resemble Hispanic patients in measured potential EMMs (e.g., age, KRAS distribution, sex), combined Hispanic and weighted non-Hispanic White patients in one data set, and estimated 1-year differences in progression-free survival (PFS). We obtained percentile-based 95% confidence limits for this 1-year difference in PFS from 2,000 bootstraps. To show when the method is less helpful, we also reweighted male patients to resemble female patients and mutant-type KRAS (no treatment benefit) patients to resemble wild-type KRAS (treatment benefit) patients. The PRIME RCT included 795 non-Hispanic White and 42 Hispanic patients with complete data on EMMs. While the Hispanic-only analysis estimated a one-year PFS change of -17% (95% C.I. -45%, 8.8%) with panitumumab, the combined weighted estimate was more precise (-8.7%, 95% CI -22%, 5.3%) while differing from the full population estimate (1.0%, 95% CI: -5.9%, 7.5%). When targeting wild-type KRAS patients the combined weighted estimate incorrectly suggested no benefit (one-year PFS change: 0.9%, 95% CI: -6.0%, 7.2%). Methods to extend inferences from study populations to specific targets can improve the precision of estimates of subgroup effect estimates when their assumptions are met. Violations of those assumptions can lead to bias, however.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08297v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Webster-Clark, Anthony A. Matthews, Alan R. Ellis, Alan C. Kinlaw, Robert W. Platt</dc:creator>
    </item>
    <item>
      <title>On Conditional least squares estimation for the AD(1,n) model</title>
      <link>https://arxiv.org/abs/2406.07653</link>
      <description>arXiv:2406.07653v1 Announce Type: cross 
Abstract: This paper deals with the problem of global parameter estimation of AD(1, n) where n is a positive integer which is a subclass of affine diffusions introduced by Duffie, Filipovic, and Schachermayer. In general affine models are applied to the pricing of bond and stock options, which is illustrated for the Vasicek, Cox-Ingersoll-Ross and Heston models. Our main results are about the conditional least squares estimation of AD(1, n) drift parameters based on two types of observations : continuous time observations and discrete time observations with high frequency and infinite horizon. Then, for each case, we study the asymptotic properties according to ergodic and non-ergodic cases. This paper introduces as well some moment results relative to the AD(1, n) model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07653v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Ben Alaya, Houssem Dahbi, Hamdi Fathallah</dc:creator>
    </item>
    <item>
      <title>A Diagnostic Tool for Functional Causal Discovery</title>
      <link>https://arxiv.org/abs/2406.07787</link>
      <description>arXiv:2406.07787v1 Announce Type: cross 
Abstract: Causal discovery methods aim to determine the causal direction between variables using observational data. Functional causal discovery methods, such as those based on the Linear Non-Gaussian Acyclic Model (LiNGAM), rely on structural and distributional assumptions to infer the causal direction. However, approaches for assessing causal discovery methods' performance as a function of sample size or the impact of assumption violations, inevitable in real-world scenarios, are lacking. To address this need, we propose Causal Direction Detection Rate (CDDR) diagnostic that evaluates whether and to what extent the interaction between assumption violations and sample size affects the ability to identify the hypothesized causal direction. Given a bivariate dataset of size N on a pair of variables, X and Y, CDDR diagnostic is the plotted comparison of the probability of each causal discovery outcome (e.g. X causes Y, Y causes X, or inconclusive) as a function of sample size less than N. We fully develop CDDR diagnostic in a bivariate case and demonstrate its use for two methods, LiNGAM and our new test-based causal discovery approach. We find CDDR diagnostic for the test-based approach to be more informative since it uses a richer set of causal discovery outcomes. Under certain assumptions, we prove that the probability estimates of detecting each possible causal discovery outcome are consistent and asymptotically normal. Through simulations, we study CDDR diagnostic's behavior when linearity and non-Gaussianity assumptions are violated. Additionally, we illustrate CDDR diagnostic on four real datasets, including three for which the causal direction is known.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07787v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shreya Prakash, Fan Xia, Elena Erosheva</dc:creator>
    </item>
    <item>
      <title>scores: A Python package for verifying and evaluating models and predictions with xarray and pandas</title>
      <link>https://arxiv.org/abs/2406.07817</link>
      <description>arXiv:2406.07817v1 Announce Type: cross 
Abstract: `scores` is a Python package containing mathematical functions for the verification, evaluation and optimisation of forecasts, predictions or models. It primarily supports the geoscience communities; in particular, the meteorological, climatological and oceanographic communities. In addition to supporting the Earth system science communities, it also has wide potential application in machine learning and other domains such as economics. `scores` not only includes common scores (e.g. Mean Absolute Error), it also includes novel scores not commonly found elsewhere (e.g. FIxed Risk Multicategorical (FIRM) score, Flip-Flop Index), complex scores (e.g. threshold-weighted continuous ranked probability score), and statistical tests (such as the Diebold Mariano test). It also contains isotonic regression which is becoming an increasingly important tool in forecast verification and can be used to generate stable reliability diagrams. Additionally, it provides pre-processing tools for preparing data for scores in a variety of formats including cumulative distribution functions (CDF). At the time of writing, `scores` includes over 50 metrics, statistical techniques and data processing tools. All of the scores and statistical techniques in this package have undergone a thorough scientific and software review. Every score has a companion Jupyter Notebook tutorial that demonstrates its use in practice. `scores` primarily supports `xarray` datatypes for Earth system data, allowing it to work with NetCDF4, HDF5, Zarr and GRIB data sources among others. `scores` uses Dask for scaling and performance. It has expanding support for `pandas`. The software repository can be found at https://github.com/nci/scores/</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07817v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tennessee Leeuwenburg, Nicholas Loveday, Elizabeth E. Ebert, Harrison Cook, Mohammadreza Khanarmuei, Robert J. Taggart, Nikeeth Ramanathan, Maree Carroll, Stephanie Chong, Aidan Griffiths, John Sharples</dc:creator>
    </item>
    <item>
      <title>Inductive Global and Local Manifold Approximation and Projection</title>
      <link>https://arxiv.org/abs/2406.08097</link>
      <description>arXiv:2406.08097v1 Announce Type: cross 
Abstract: Nonlinear dimensional reduction with the manifold assumption, often called manifold learning, has proven its usefulness in a wide range of high-dimensional data analysis. The significant impact of t-SNE and UMAP has catalyzed intense research interest, seeking further innovations toward visualizing not only the local but also the global structure information of the data. Moreover, there have been consistent efforts toward generalizable dimensional reduction that handles unseen data. In this paper, we first propose GLoMAP, a novel manifold learning method for dimensional reduction and high-dimensional data visualization. GLoMAP preserves locally and globally meaningful distance estimates and displays a progression from global to local formation during the course of optimization. Furthermore, we extend GLoMAP to its inductive version, iGLoMAP, which utilizes a deep neural network to map data to its lower-dimensional representation. This allows iGLoMAP to provide lower-dimensional embeddings for unseen points without needing to re-train the algorithm. iGLoMAP is also well-suited for mini-batch learning, enabling large-scale, accelerated gradient calculations. We have successfully applied both GLoMAP and iGLoMAP to the simulated and real-data settings, with competitive experiments against the state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08097v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jungeum Kim, Xiao Wang</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian Emulation of the Expected Net Present Value Utility Function via a Multi-Model Ensemble Member Decomposition</title>
      <link>https://arxiv.org/abs/2406.08367</link>
      <description>arXiv:2406.08367v1 Announce Type: cross 
Abstract: Computer models are widely used to study complex real world physical systems. However, there are major limitations to their direct use including: their complex structure; large numbers of inputs and outputs; and long evaluation times. Bayesian emulators are an effective means of addressing these challenges providing fast and efficient statistical approximation for computer model outputs. It is commonly assumed that computer models behave like a ``black-box'' function with no knowledge of the output prior to its evaluation. This ensures that emulators are generalisable but potentially limits their accuracy compared with exploiting such knowledge of constrained or structured output behaviour. We assume a ``grey-box'' computer model and establish a hierarchical emulation framework encompassing structured emulators which exploit known constrained and structured behaviour of constituent computer model outputs. This achieves greater physical interpretability and more accurate emulator predictions. This research is motivated by and applied to the commercially important TNO OLYMPUS Well Control Optimisation Challenge from the petroleum industry. We re-express this as a decision support under uncertainty problem. First, we reduce the computational expense of the analysis by identifying a representative subset of models using an efficient multi-model ensemble subsampling technique. Next we apply our hierarchical emulation methodology to the expected Net Present Value utility function with well control decision parameters as inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08367v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Owen, Ian Vernon</dc:creator>
    </item>
    <item>
      <title>Coordinated Trading Strategies for Battery Storage in Reserve and Spot Markets</title>
      <link>https://arxiv.org/abs/2406.08390</link>
      <description>arXiv:2406.08390v1 Announce Type: cross 
Abstract: Quantity and price risks are key uncertainties market participants face in electricity markets with increased volatility, for instance, due to high shares of renewables. From day ahead until real-time, there is a large variation in the best available information, leading to price changes that flexible assets, such as battery storage, can exploit economically. This study contributes to understanding how coordinated bidding strategies can enhance multi-market trading and large-scale energy storage integration. Our findings shed light on the complexities arising from interdependencies and the high-dimensional nature of the problem. We show how stochastic dual dynamic programming is a suitable solution technique for such an environment. We include the three markets of the frequency containment reserve, day-ahead, and intraday in stochastic modelling and develop a multi-stage stochastic program. Prices are represented in a multidimensional Markov Chain, following the scheduling of the markets and allowing for time-dependent randomness. Using the example of a battery storage in the German energy sector, we provide valuable insights into the technical aspects of our method and the economic feasibility of battery storage operation. We find that capacity reservation in the frequency containment reserve dominates over the battery's cycling in spot markets at the given resolution on prices in 2022. In an adjusted price environment, we find that coordination can yield an additional value of up to 12.5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08390v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul E. Seifert, Emil Kraft, Steffen Bakker, Stein-Erik Fleten</dc:creator>
    </item>
    <item>
      <title>Probabilistic Reconstruction of Paleodemographic Signals</title>
      <link>https://arxiv.org/abs/2312.05152</link>
      <description>arXiv:2312.05152v2 Announce Type: replace 
Abstract: We present a comprehensive Bayesian approach to paleodemography, emphasizing the proper handling of uncertainties. We then apply that framework to survey data from Cyprus, and quantify the uncertainties in the paleodemographic estimates to demonstrate the applicability of the Bayesian approach and to show the large uncertainties present in current paleodemographic models and data. We also discuss methods to reduce the uncertainties and improve the efficacy of paleodemographic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05152v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L. M. Arthur, F. Chelazzi, D. Lawrence, M. D. Price</dc:creator>
    </item>
    <item>
      <title>Effective experience rating for large insurance portfolios via surrogate modeling</title>
      <link>https://arxiv.org/abs/2211.06568</link>
      <description>arXiv:2211.06568v3 Announce Type: replace-cross 
Abstract: Experience rating in insurance uses a Bayesian credibility model to upgrade the current premiums of a contract by taking into account policyholders' attributes and their claim history. Most data-driven models used for this task are mathematically intractable, and premiums must be obtained through numerical methods such as simulation via MCMC. However, these methods can be computationally expensive and even prohibitive for large portfolios when applied at the policyholder level. Additionally, these computations become ``black-box" procedures as there is no analytical expression showing how the claim history of policyholders is used to upgrade their premiums. To address these challenges, this paper proposes a surrogate modeling approach to inexpensively derive an analytical expression for computing the Bayesian premiums for any given model, approximately. As a part of the methodology, the paper introduces a \emph{likelihood-based summary statistic} of the policyholder's claim history that serves as the main input of the surrogate model and that is sufficient for certain families of distribution, including the exponential dispersion family. As a result, the computational burden of experience rating for large portfolios is reduced through the direct evaluation of such analytical expression, which can provide a transparent and interpretable way of computing Bayesian premiums.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.06568v3</guid>
      <category>stat.ME</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.insmatheco.2024.05.004</arxiv:DOI>
      <arxiv:journal_reference>Insurance: Mathematics and Economics, Volume 118, September 2024, Pages 25-43</arxiv:journal_reference>
      <dc:creator>Sebastian Calcetero-Vanegas, Andrei L. Badescu, X. Sheldon Lin</dc:creator>
    </item>
    <item>
      <title>On clustering levels of a hierarchical categorical risk factor</title>
      <link>https://arxiv.org/abs/2304.09046</link>
      <description>arXiv:2304.09046v3 Announce Type: replace-cross 
Abstract: Handling nominal covariates with a large number of categories is challenging for both statistical and machine learning techniques. This problem is further exacerbated when the nominal variable has a hierarchical structure. We commonly rely on methods such as the random effects approach (Campo and Antonio, 2023) to incorporate these covariates in a predictive model. Nonetheless, in certain situations, even the random effects approach may encounter estimation problems. We propose the data-driven Partitioning Hierarchical Risk-factors Adaptive Top-down (PHiRAT) algorithm to reduce the hierarchically structured risk factor to its essence, by grouping similar categories at each level of the hierarchy. We work top-down and engineer several features to characterize the profile of the categories at a specific level in the hierarchy. In our workers' compensation case study, we characterize the risk profile of an industry via its observed damage rates and claim frequencies. In addition, we use embeddings (Mikolov et al., 2013; Cer et al., 2018) to encode the textual description of the economic activity of the insured company. These features are then used as input in a clustering algorithm to group similar categories. Our method substantially reduces the number of categories and results in a grouping that is generalizable to out-of-sample data. Moreover, we obtain a better differentiation between high-risk and low-risk companies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09046v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1017/S1748499523000283</arxiv:DOI>
      <dc:creator>Bavo D. C. Campo, Katrien Antonio</dc:creator>
    </item>
    <item>
      <title>Claim Reserving via Inverse Probability Weighting: A Micro-Level Chain-Ladder Method</title>
      <link>https://arxiv.org/abs/2307.10808</link>
      <description>arXiv:2307.10808v3 Announce Type: replace-cross 
Abstract: Claim reserving primarily relies on macro-level models, with the Chain-Ladder method being the most widely adopted. These methods were heuristically developed without minimal statistical foundations, relying on oversimplified data assumptions and neglecting policyholder heterogeneity, often resulting in conservative reserve predictions. Micro-level reserving, utilizing stochastic modeling with granular information, can improve predictions but tends to involve less attractive and complex models for practitioners. This paper aims to strike a practical balance between aggregate and individual models by introducing a methodology that enables the Chain-Ladder method to incorporate individual information. We achieve this by proposing a novel framework, formulating the claim reserving problem within a population sampling context. We introduce a reserve estimator in a frequency and severity distribution-free manner that utilizes inverse probability weights (IPW) driven by individual information, akin to propensity scores. We demonstrate that the Chain-Ladder method emerges as a particular case of such an IPW estimator, thereby inheriting a statistically sound foundation based on population sampling theory that enables the use of granular information, and other extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10808v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Calcetero-Vanegas, Andrei L. Badescu, X. Sheldon Lin</dc:creator>
    </item>
    <item>
      <title>Who is driving the conversation? Analysing the nodality of British MPs and journalists on Twitter</title>
      <link>https://arxiv.org/abs/2402.08765</link>
      <description>arXiv:2402.08765v2 Announce Type: replace-cross 
Abstract: Who sets the policy agenda? In this paper, we explore the roles of policy actors in agenda setting by studying their relative influence in policy-related discussions. Our approach builds on ``nodality'' \textemdash a concept in political science that determines the capacity of an actor to share information and to be at the centre of information networks. We propose a novel methodology that quantifies the nodality of all individual actors in any conversation by analysing a comprehensive set of their centrality measures in the related information network. We combine this with the analysis of the activity time-series, of the related conversation (or topic), to demonstrate how nodality scores relate to the capacity to drive topic-related activity. Here we analyse policy-related discussions on X (previously Twitter) and quantify the nodality of two sets of actors in the UK political system \textemdash Members of Parliament (MPs) and accredited journalists - on four policy topics: The Russia-Ukraine War, the Cost-of-Living Crisis, Brexit and COVID-19. Our results show that the capacity to influence the activity related to a topic is significantly and positively associated with nodality. In particular, we identify two dimensions of nodality that drive the capacity to influence topic-related activity. The first is ``active nodality", which reflects the level of topic-related engagement an individual actor has on the platform. The second dimension is ``inherent nodality" which is entirely independent of the platform and reflects the actor's institutional position (such as an MP in a front-bench role, or a journalist's position at a prominent media outlet).</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08765v2</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonardo Castro-Gonzalez, Sukankana Chakraborty, Helen Margetts, Hardik Rajpal, Daniele Guariso, Jonathan Bright</dc:creator>
    </item>
    <item>
      <title>Non-robustness of diffusion estimates on networks with measurement error</title>
      <link>https://arxiv.org/abs/2403.05704</link>
      <description>arXiv:2403.05704v4 Announce Type: replace-cross 
Abstract: Network diffusion models are used to study things like disease transmission, information spread, and technology adoption. However, small amounts of mismeasurement are extremely likely in the networks constructed to operationalize these models. We show that estimates of diffusions are highly non-robust to this measurement error. First, we show that even when measurement error is vanishingly small, such that the share of missed links is close to zero, forecasts about the extent of diffusion will greatly underestimate the truth. Second, a small mismeasurement in the identity of the initial seed generates a large shift in the locations of expected diffusion path. We show that both of these results still hold when the vanishing measurement error is only local in nature. Such non-robustness in forecasting exists even under conditions where the basic reproductive number is consistently estimable. Possible solutions, such as estimating the measurement error or implementing widespread detection efforts, still face difficulties because the number of missed links are so small. Finally, we conduct Monte Carlo simulations on simulated networks, and real networks from three settings: travel data from the COVID-19 pandemic in the western US, a mobile phone marketing campaign in rural India, and in an insurance experiment in China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05704v4</guid>
      <category>econ.EM</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arun G. Chandrasekhar, Paul Goldsmith-Pinkham, Tyler H. McCormick, Samuel Thau, Jerry Wei</dc:creator>
    </item>
    <item>
      <title>Arbitrary-Length Generalization for Addition in a Tiny Transformer</title>
      <link>https://arxiv.org/abs/2406.00075</link>
      <description>arXiv:2406.00075v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel training methodology that enables a Transformer model to generalize the addition of two-digit numbers to numbers with unseen lengths of digits. The proposed approach employs an autoregressive generation technique, processing from right to left, which mimics a common manual method for adding large numbers. To the best of my knowledge, this methodology has not been previously explored in the literature. All results are reproducible, and the corresponding R code is available at github.com/AGPatriota/ALGA-R/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00075v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Galvao Patriota</dc:creator>
    </item>
  </channel>
</rss>
