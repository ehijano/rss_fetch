<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Jul 2025 04:01:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Local Fr'echet Regression via RKHS embedding and Its Applications to Data Analysis on Manifolds</title>
      <link>https://arxiv.org/abs/2507.03288</link>
      <description>arXiv:2507.03288v1 Announce Type: new 
Abstract: Local Fr'echet Regression (LFR) is a nonparametric regression method for settings in which the explanatory variable lies in a Euclidean space and the response variable lies in a metric space. It is used to estimate smooth trajectories in general metric spaces from noisy observations of random objects taking values in such spaces. Since metric spaces form a broad class of spaces that often lack algebraic structures such as addition or scalar multiplication characteristics typical of vector spaces the asymptotic theory for conventional random variables cannot be directly applied. As a result, deriving the asymptotic distribution of the LFR estimator is challenging. In this paper, we first extend nonparametric regression models for real-valued responses to Hilbert spaces and derive the asymptotic distribution of the LFR estimator in a Hilbert space setting. Furthermore, we propose a new estimator based on the LFR estimator in a reproducing kernel Hilbert space (RKHS), by mapping data from a general metric space into an RKHS. Finally, we consider applications of the proposed method to data lying on manifolds and construct confidence regions in metric spaces based on the derived asymptotic distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03288v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Iida, Hiroshi Shiraishi, Hiroaki Ogata</dc:creator>
    </item>
    <item>
      <title>No Eigenvalues Outside the Limiting Support of Generally Correlated and Noncentral Sample Covariance Matrices</title>
      <link>https://arxiv.org/abs/2507.03356</link>
      <description>arXiv:2507.03356v1 Announce Type: new 
Abstract: Spectral properties of random matrices play an important role in statistics, machine learning, communications, and many other areas. Engaging results regarding the convergence of the empirical spectral distribution (ESD) and the ``no-eigenvalue'' property have been obtained for random matrices with different correlation structures. However, the related spectral analysis for generally correlated and noncentral random matrices is still incomplete, and this paper aims to fill this research gap. Specifically, we consider matrices whose columns are independent but with non-zero means and non-identical correlations. Under high-dimensional asymptotics where both the number of rows and columns grow simultaneously to infinity, we first establish the almost sure convergence of the ESD for the concerned random matrices to a deterministic limit, assuming mild conditions. Furthermore, we prove that with probability 1, no eigenvalues will appear in any closed interval outside the support of the limiting distribution for matrices with sufficiently large dimensions. The above results can be applied to different areas such as statistics, wireless communications, and signal processing. In this paper, we apply the derived results to two communication scenarios: 1) We determine the limiting performance of the signal-to-interference-plus-noise ratio for multi-user multiple-input multiple-output (MIMO) systems with linear minimum mean-square error receivers; and 2) We establish the invertibility of zero-forcing precoding matrices in downlink MIMO systems, providing theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03356v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeyan Zhuang, Xin Zhang, Dongfang Xu, Shenghui Song</dc:creator>
    </item>
    <item>
      <title>The relation of bias with risk in empirically constrained inferences</title>
      <link>https://arxiv.org/abs/2507.03699</link>
      <description>arXiv:2507.03699v1 Announce Type: new 
Abstract: We give some results relating asymptotic characterisations of maximum entropy probability measures to characterisations of Bayes optimal classifiers. Our main theorems show that maximum entropy is a universally Bayes optimal decision rule given constraints on one's knowledge about some observed data in terms of an expected loss. We will extend this result to the case of uncertainty in the observations of expected losses by generalising Sanov's theorem to distributions of constraint values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03699v1</guid>
      <category>math.ST</category>
      <category>cond-mat.stat-mech</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dalton A R Sakthivadivel</dc:creator>
    </item>
    <item>
      <title>On the Estimation of Anisotropic Covariance Functions on Compact Two-Point Homogeneous Spaces</title>
      <link>https://arxiv.org/abs/2507.03723</link>
      <description>arXiv:2507.03723v1 Announce Type: new 
Abstract: In this paper, the asymptotic theory presented in (Caponera et al., 2022) for spline-type anysotropic covariance estimator on the 2-dimensional sphere is generalized to the case of connected and compact two-point homogeneous spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03723v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessia Caponera</dc:creator>
    </item>
    <item>
      <title>Tied Pools and Drawn Games</title>
      <link>https://arxiv.org/abs/2507.03894</link>
      <description>arXiv:2507.03894v1 Announce Type: new 
Abstract: We consider the problem of estimating `preference' or `strength' parameters in three-way comparison experiments, each composed of a series of paired comparisons, but where only the single `preferred' or `strongest' candidate is known in each trial. Such experiments arise in psychology and market research, but here we use chess competitions as the prototypical context, in particular a series of `pools' between three players that occurred in 1821. The possibilities of tied pools, redundant and therefore unplayed games, and drawn games must all be considered. This leads us to reconsider previous models for estimating strength parameters when drawn games are a possible result. In particular, Davidson's method for ties has been questioned, and we propose an alternative. We argue that the most correct use of this method is to estimate strength parameters first, and then fix these to estimate a draw-propensity parameter, rather than estimating all parameters simultaneously, as Davidson does. This results in a model that is consistent with, and provides more context for, a simple method for handling draws proposed by Glickman. Finally, in pools with incomplete information, the number of drawn games can be estimated by adopting a draw-propensity parameter from related data with more complete information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03894v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roderick Edwards</dc:creator>
    </item>
    <item>
      <title>Characterization of Generalized Alpha-Beta Divergence and Associated Entropy Measures</title>
      <link>https://arxiv.org/abs/2507.04637</link>
      <description>arXiv:2507.04637v1 Announce Type: new 
Abstract: Minimum divergence estimators provide a natural choice of estimators in a statistical inference problem. Different properties of various families of these divergence measures such as Hellinger distance, power divergence, density power divergence, logarithmic density power divergence, etc. have been established in literature. In this work, we propose a new class of divergence measures called "generalized alpha-beta divergence", which is a superfamily of these popular divergence families. We provide the necessary and sufficient conditions for the validity of the proposed generalized divergence measure, which allows us to construct novel families of divergence and associated entropy measures. We also show various characterizing properties like duality, inversion, semi-continuity, etc., from which, many existing results follow as special cases. We also discuss about the entropy measure derived from this general family of divergence and its properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04637v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhrajyoty Roy, Supratik Basu, Abhik Ghosh, Ayanendranath Basu</dc:creator>
    </item>
    <item>
      <title>Generalization bounds for score-based generative models: a synthetic proof</title>
      <link>https://arxiv.org/abs/2507.04794</link>
      <description>arXiv:2507.04794v1 Announce Type: new 
Abstract: We establish minimax convergence rates for score-based generative models (SGMs) under the $1$-Wasserstein distance. Assuming the target density $p^\star$ lies in a nonparametric $\beta$-smooth H\"older class with either compact support or subGaussian tails on $\mathbb{R}^d$, we prove that neural network-based score estimators trained via denoising score matching yield generative models achieving rate $n^{-(\beta+1)/(2\beta+d)}$ up to polylogarithmic factors. Our unified analysis handles arbitrary smoothness $\beta &gt; 0$, supports both deterministic and stochastic samplers, and leverages shape constraints on $p^\star$ to induce regularity of the score. The resulting proofs are more concise, and grounded in generic stability of diffusions and standard approximation theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04794v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur St\'ephanovitch, Eddie Aamari, Cl\'ement Levrard</dc:creator>
    </item>
    <item>
      <title>Monitoring for a Phase Transition in a Time Series of Wigner Matrices</title>
      <link>https://arxiv.org/abs/2507.04983</link>
      <description>arXiv:2507.04983v1 Announce Type: new 
Abstract: We develop methodology and theory for the detection of a phase transition in a time-series of high-dimensional random matrices. In the model we study, at each time point \( t = 1,2,\ldots \), we observe a deformed Wigner matrix \( \mathbf{M}_t \), where the unobservable deformation represents a latent signal. This signal is detectable only in the supercritical regime, and our objective is to detect the transition to this regime in real time, as new matrix--valued observations arrive. Our approach is based on a partial sum process of extremal eigenvalues of $\mathbf{M}_t$, and its theoretical analysis combines state-of-the-art tools from random-matrix-theory and Gaussian approximations. The resulting detector is self-normalized, which ensures appropriate scaling for convergence and a pivotal limit, without any additional parameter estimation. Simulations show excellent performance for varying dimensions. Applications to pollution monitoring and social interactions in primates illustrate the usefulness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04983v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nina D\"ornemann, Piotr Kokoszka, Tim Kutta, Sunmin Lee</dc:creator>
    </item>
    <item>
      <title>Gaussian approximation for non-linearity parameter estimation in perturbed random fields on the sphere</title>
      <link>https://arxiv.org/abs/2507.05074</link>
      <description>arXiv:2507.05074v1 Announce Type: new 
Abstract: The nonlinear parameter measures the amplitude of primordial non-Gaussianity in the cosmic microwave background radiation (CMB), offering a crucial test of early universe models. While standard single field inflation predicts nearly Gaussian fluctuations, more complex scenarios yield subtle non Gaussian signals, particularly captured by the CMB bispectrum. In the local model, these signals arise through a quadratic correction to a Gaussian field. To estimate the nonlinear parameter, we adopt a Komatsu Spergel Wandelt (KSW) type estimator, based on spherical harmonics and Wigner 3j symbols, and adapted to narrow band configurations that depend on the range of multipoles considered. In this paper, we rigorously study its asymptotic properties by applying fourth-moment theorems from Wiener chaos theory. More in detail, we establish a quantitative central limit theorem for the KSW estimator, with an explicit convergence rate controlled by number of admissible multipoles. Our results establish both theoretical guarantees and practical robustness for high resolution CMB analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05074v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudio Durastanti</dc:creator>
    </item>
    <item>
      <title>Scale Dilation Dynamics in Flexible Bandwidth Needlet Constructions</title>
      <link>https://arxiv.org/abs/2507.05075</link>
      <description>arXiv:2507.05075v1 Announce Type: new 
Abstract: Flexible bandwidth needlets offer a versatile multiscale framework for analyzing functions on the sphere. A key element in their construction is the dilation sequence, which controls how the multipole consecutive scales are spaced and overlapped. At any resolution level, this sequence determines the center positions of the needlet weight functions and influences their localization in the spatial domain and spectral concentration properties by means of the relative bandwidth ratio. In this paper, we explore the different asymptotic regimes that arise when the dilation sequence exhibits shrinking, stable (standard), or spreading behavior. Moreover, we assume the dilation sequence grows regularly enough to ensure well-defined asymptotic properties. For each regime, we characterize the impact on the geometry of the center scales and the shape of the multipole windows, with particular attention to their overlap structure and spectral coverage. These insights help to clarify the trade-offs between localization, redundancy, and scalability in the design of needlet-type systems, particularly in relation to the study of the asymptotic uncorrelation of needlet coefficients when applied to random fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05075v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudio Durastanti</dc:creator>
    </item>
    <item>
      <title>Implicit Regularisation in Diffusion Models: An Algorithm-Dependent Generalisation Analysis</title>
      <link>https://arxiv.org/abs/2507.03756</link>
      <description>arXiv:2507.03756v1 Announce Type: cross 
Abstract: The success of denoising diffusion models raises important questions regarding their generalisation behaviour, particularly in high-dimensional settings. Notably, it has been shown that when training and sampling are performed perfectly, these models memorise training data -- implying that some form of regularisation is essential for generalisation. Existing theoretical analyses primarily rely on algorithm-independent techniques such as uniform convergence, heavily utilising model structure to obtain generalisation bounds. In this work, we instead leverage the algorithmic aspects that promote generalisation in diffusion models, developing a general theory of algorithm-dependent generalisation for this setting. Borrowing from the framework of algorithmic stability, we introduce the notion of score stability, which quantifies the sensitivity of score-matching algorithms to dataset perturbations. We derive generalisation bounds in terms of score stability, and apply our framework to several fundamental learning settings, identifying sources of regularisation. In particular, we consider denoising score matching with early stopping (denoising regularisation), sampler-wide coarse discretisation (sampler regularisation) and optimising with SGD (optimisation regularisation). By grounding our analysis in algorithmic properties rather than model structure, we identify multiple sources of implicit regularisation unique to diffusion models that have so far been overlooked in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03756v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tyler Farghly, Patrick Rebeschini, George Deligiannidis, Arnaud Doucet</dc:creator>
    </item>
    <item>
      <title>A note on the unique properties of the Kullback--Leibler divergence for sampling via gradient flows</title>
      <link>https://arxiv.org/abs/2507.04330</link>
      <description>arXiv:2507.04330v1 Announce Type: cross 
Abstract: We consider the problem of sampling from a probability distribution $\pi$. It is well known that this can be written as an optimisation problem over the space of probability distribution in which we aim to minimise a divergence from $\pi$. and The optimisation problem is normally solved through gradient flows in the space of probability distribution with an appropriate metric. We show that the Kullback--Leibler divergence is the only divergence in the family of Bregman divergences whose gradient flow w.r.t. many popular metrics does not require knowledge of the normalising constant of $\pi$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04330v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca Romana Crucinio</dc:creator>
    </item>
    <item>
      <title>A Test for Jumps in Metric-Space Conditional Means</title>
      <link>https://arxiv.org/abs/2507.04560</link>
      <description>arXiv:2507.04560v1 Announce Type: cross 
Abstract: Standard methods for detecting discontinuities in conditional means are not applicable to outcomes that are complex, non-Euclidean objects like distributions, networks, or covariance matrices. This article develops a nonparametric test for jumps in conditional means when outcomes lie in a non-Euclidean metric space. Using local Fr\'echet regression$\unicode{x2014}$which generalizes standard regression to metric-space valued data$\unicode{x2014}$the method estimates a mean path on either side of a candidate cutoff, extending existing k-sample tests to a flexible regression setting. Key theoretical contributions include a central limit theorem for the local estimator of the conditional Fr\'echet variance and the asymptotic validity and consistency of the proposed test. Simulations confirm nominal size control and robust power in finite samples. Two applications demonstrate the method's value by revealing effects invisible to scalar-based tests. First, I detect a sharp change in work-from-home compositions at Washington State's income threshold for non-compete enforceability during COVID-19, highlighting remote work's role as a bargaining margin. Second, I find that countries restructure their input-output networks after losing preferential US trade access. These findings underscore that analyzing regression functions within their native metric spaces can reveal structural discontinuities that scalar summaries would miss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04560v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Van Dijcke</dc:creator>
    </item>
    <item>
      <title>Critical Point Processes Obtained from a Gaussian Random Field with a View Towards Statistics</title>
      <link>https://arxiv.org/abs/2507.04753</link>
      <description>arXiv:2507.04753v1 Announce Type: cross 
Abstract: This paper establishes the theoretical foundation for statistical applications of an intriguing new type of spatial point processes called critical point processes. These point processes, residing in Euclidean space, consist of the critical points of latent smooth Gaussian random fields or of subsets of critical points like minima, saddle points etc. Despite of the simplicity of their definition, the mathematical analysis of critical point processes is non-trivial involving for example deep results on the geometry of random fields, Sobolev space theory, chaos expansions, and multiple Wiener-It{\^o} integrals. We provide explicit expressions for fundamental moment characteristics used in spatial point process statistics like the intensity parameter, the pair correlation function, and higher order intensity functions. The crucial dependence structure (attraction or repulsiveness) of a critical point process is discussed in depth and is in particular related to the dimension of the points and the type of critical points (extrema, saddle points, or all of the critical points). We propose simulation strategies based on spectral methods or smoothing of grid-based simulations and show that resulting approximate critical point process simulations asymptotically converge to the exact critical point process distribution. Finally, under the increasing domain framework, we obtain asymptotic results for linear and bilinear statistics of a critical point process. In particular, we obtain a multivariate central limit theorem for the intensity parameter estimate and a modified version of Ripley's K-function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04753v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julien Chevallier (SVH, LJK), Jean-Fran\c{c}ois Coeurjolly (LJK, SVH), Rasmus Waagepetersen</dc:creator>
    </item>
    <item>
      <title>Sequential multiple importance sampling for high-dimensional Bayesian inference</title>
      <link>https://arxiv.org/abs/2507.05114</link>
      <description>arXiv:2507.05114v1 Announce Type: cross 
Abstract: This paper introduces a sequential multiple importance sampling (SeMIS) algorithm for high-dimensional Bayesian inference. The method estimates Bayesian evidence using all generated samples from each proposal distribution while obtaining posterior samples through an importance-resampling scheme. A key innovation of SeMIS is the use of a softly truncated prior distribution as the intermediate proposal, providing a new way bridging prior and posterior distributions. By enabling samples from high-likelihood regions to traverse low-probability zones, SeMIS enhances mode mixing in challenging inference problems. Comparative evaluations against subset simulation (SuS) and adaptive Bayesian updating with structural reliability methods (aBUS) demonstrate that SeMIS achieves superior performance in evidence estimation (lower bias and variance) and posterior sampling (higher effective sample sizes and closer approximation to the true posterior), particularly for multimodal distributions. The efficacy of SeMIS is further validated in a high-dimensional finite element model updating application, where it successfully localizes structural damages by quantifying stiffness loss. The proposed algorithm not only advances Bayesian computation for complex posterior distributions but also provides a robust tool for uncertainty quantification in civil engineering systems, offering new possibilities for probabilistic structural health monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05114v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li Binbin, He Xiao, Liao Zihan</dc:creator>
    </item>
    <item>
      <title>The Landau equation and Fisher information</title>
      <link>https://arxiv.org/abs/2507.05167</link>
      <description>arXiv:2507.05167v1 Announce Type: cross 
Abstract: In this expository note (submitted to Notices of the AMS) we present the ideas used in our recent work ruling out blow up for the Landau equation with Coulomb potential. Blow up is ruled out by the discovery that the Fisher information is not increasing in time along a solution. This monotonicity is established by means of a new ``lifted equation'' which is an auxiliary linear equation in double the number of variables that encodes the nonlinear nonlocal collision operator. For the Landau equation in particular this lifted equation amounts to a family of heat equations over the sphere. Some background on kinetic equations and the Fisher information, and connections to Bakry-Emery theory is also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05167v1</guid>
      <category>math.AP</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nestor Guillen, Luis Silvestre</dc:creator>
    </item>
    <item>
      <title>The Optimality of Blocking Designs in Equally and Unequally Allocated Randomized Experiments with General Response</title>
      <link>https://arxiv.org/abs/2212.01887</link>
      <description>arXiv:2212.01887v4 Announce Type: replace 
Abstract: We consider the performance of the difference-in-means estimator in a two-arm randomized experiment under common experimental endpoints such as continuous (regression), incidence, proportion and survival. We examine performance under both equal and unequal allocation to treatment groups and we consider both the Neyman randomization model and the population model. We show that in the Neyman model, where the only source of randomness is the treatment manipulation, there is no free lunch: complete randomization is minimax for the estimator's mean squared error. In the population model, where each subject experiences response noise with zero mean, the optimal design is the deterministic perfect-balance allocation. However, this allocation is generally NP-hard to compute and moreover, depends on unknown response parameters. When considering the tail criterion of Kapelner et al. (2021), we show the optimal design is less random than complete randomization and more random than the deterministic perfect-balance allocation. We prove that Fisher's blocking design provides the asymptotically optimal degree of experimental randomness. Theoretical results are supported by simulations in all considered experimental settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.01887v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Azriel, Abba M. Krieger, Adam Kapelner</dc:creator>
    </item>
    <item>
      <title>Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Streaming Data</title>
      <link>https://arxiv.org/abs/2310.12140</link>
      <description>arXiv:2310.12140v4 Announce Type: replace 
Abstract: Online nonparametric estimators are gaining popularity due to their efficient computation and competitive generalization abilities. An important example includes variants of stochastic gradient descent. These algorithms often take one sample point at a time and incrementally update the parameter estimate of interest. In this work, we consider model selection/hyperparameter tuning for such online algorithms. We propose a weighted rolling validation procedure, an online variant of leave-one-out cross-validation, that costs minimal extra computation for many typical stochastic gradient descent estimators and maintains their online nature. Similar to batch cross-validation, it can boost base estimators to achieve better heuristic performance and adaptive convergence rate. Our analysis is straightforward, relying mainly on some general statistical stability assumptions. The simulation study underscores the significance of diverging weights in practice and demonstrates its favorable sensitivity even when there is only a slim difference between candidate estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12140v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Jing Lei</dc:creator>
    </item>
    <item>
      <title>Frequentist Guarantees of Distributed (Non)-Bayesian Inference</title>
      <link>https://arxiv.org/abs/2311.08214</link>
      <description>arXiv:2311.08214v5 Announce Type: replace 
Abstract: Motivated by the need to analyze large, decentralized datasets, distributed Bayesian inference has become a critical research area across multiple fields, including statistics, electrical engineering, and economics. This paper establishes Frequentist properties, such as posterior consistency, asymptotic normality, and posterior contraction rates, for the distributed (non-)Bayes Inference problem among agents connected via a communication network. Our results show that, under appropriate assumptions on the communication graph, distributed Bayesian inference retains parametric efficiency while enhancing robustness in uncertainty quantification. We also explore the trade-off between statistical efficiency and communication efficiency by examining how the design and size of the communication graph impact the posterior contraction rate. Furthermore, We extend our analysis to time-varying graphs and apply our results to exponential family models, distributed logistic regression, and decentralized detection models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08214v5</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bohan Wu, C\'esar A. Uribe</dc:creator>
    </item>
    <item>
      <title>High-dimensional bootstrap and asymptotic expansion</title>
      <link>https://arxiv.org/abs/2404.05006</link>
      <description>arXiv:2404.05006v4 Announce Type: replace 
Abstract: The recent seminal work of Chernozhukov, Chetverikov and Kato has shown that bootstrap approximation for the maximum of a sum of independent random vectors is justified even when the dimension is much larger than the sample size. In this context, numerical experiments suggest that third-moment match bootstrap approximations would outperform normal approximation even without studentization, but the existing theoretical results cannot explain this phenomenon. In this paper, we develop an asymptotic expansion formula for the bootstrap coverage probability and show that it can give an explanation for the above phenomenon. In particular, we find the following interesting blessing of dimensionality phenomenon: The third-moment match wild bootstrap is second-order accurate in high-dimensions even without studentization if the covariance matrix has identical diagonal entries and bounded eigenvalues. We also show that a double wild bootstrap method is second-order accurate regardless of the covariance structure. The validity of these results is established under the existence of Stein kernels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05006v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Koike</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Honest Inference for $Z$-functionals</title>
      <link>https://arxiv.org/abs/2407.12278</link>
      <description>arXiv:2407.12278v2 Announce Type: replace 
Abstract: We develop a general assumption-lean framework for constructing uniformly valid confidence sets for functionals defined by moment equalities, referred to as $Z$-functionals. Our approach combines self-normalized statistics with a test inversion principle, enabling honest inference under mild regularity conditions and without explicit variance estimation. To enhance geometric tractability, we propose novel split-normalized and Gateaux-normalized statistics that yield computationally feasible and interpretable confidence sets. A central contribution of this work is a comprehensive non-asymptotic width analysis: we derive high-probability upper bounds on the diameter of the proposed confidence sets, and quantify their proximity to Wald intervals under minimal assumptions. Applications to high-dimensional non-sparse linear and generalized linear regression demonstrate that our procedures achieve valid coverage and near-optimal rate of convergence for the width/diameter, while the classical methods including Wald and bootstrap fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12278v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Woonyoung Chang, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Matrix Majorization in Large Samples with Varying Support Restrictions</title>
      <link>https://arxiv.org/abs/2407.16581</link>
      <description>arXiv:2407.16581v2 Announce Type: replace 
Abstract: We say that a matrix $P$ with non-negative entries majorizes another such matrix $Q$ if there is a stochastic matrix $T$ such that $Q=TP$. We study matrix majorization in large samples and in the catalytic regime in the case where the columns of the matrices need not have equal support, as has been assumed in earlier works. We focus on two cases: either there are no support restrictions (except for requiring a non-empty intersection for the supports) or the final column dominates the others. Using real-algebraic methods, we identify sufficient and almost necessary conditions for majorization in large samples or when using catalytic states under these support conditions. These conditions are given in terms of multivariate divergences that generalize the R\'enyi divergences. We notice that varying support conditions dramatically affect the relevant set of divergences. Our results find an application in the theory of catalytic state transformation in quantum thermodynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16581v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>quant-ph</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2025.3585062</arxiv:DOI>
      <dc:creator>Frits Verhagen, Marco Tomamichel, Erkka Haapasalo</dc:creator>
    </item>
    <item>
      <title>On consistent estimation of dimension values</title>
      <link>https://arxiv.org/abs/2412.13898</link>
      <description>arXiv:2412.13898v2 Announce Type: replace 
Abstract: The problem of estimating, from a random sample of points, the dimension of a compact subset $S$
  of the Euclidean space is considered. The emphasis is put on consistency results in the statistical sense. That is, statements of convergence to the true dimension value when the sample size grows to infinity.
  Among the many available definitions of dimension, we have focused (on the grounds of its statistical tractability) on three notions: the Minkowski dimension, the correlation dimension and the, perhaps less popular, concept of pointwise dimension.
  We prove the statistical consistency of some natural estimators of these quantities. Our proofs partially rely on the use of an instrumental estimator formulated in terms of the empirical volume function $V_n(r)$, defined as the Lebesgue measure of the set of points whose distance to the sample is at most $r$. In particular, we explore the case in which the true volume function $V(r)$ of the target set $S$ is a polynomial on some interval starting at zero. An empirical study is also included. Our study aims to provide some theoretical support, and some practical insights, for the problem of deciding whether or not the set $S$ has a dimension smaller than that of the ambient space. This is a major statistical motivation of the dimension studies, in connection with the so-called ``Manifold Hypothesis''.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13898v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alejandro Cholaquidis, Antonio Cuevas, Beatriz Pateiro-L\'opez</dc:creator>
    </item>
    <item>
      <title>Probabilistic morphisms and Bayesian supervised learning</title>
      <link>https://arxiv.org/abs/2502.15408</link>
      <description>arXiv:2502.15408v3 Announce Type: replace 
Abstract: In this paper, we develop category theory of Markov kernels to study categorical aspects of Bayesian inversions. As a result, we present a unified model for Bayesian supervised learning, encompassing Bayesian density estimation. We illustrate this model with Gaussian process regressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15408v3</guid>
      <category>math.ST</category>
      <category>math.CT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H\^ong V\^an L\^e</dc:creator>
    </item>
    <item>
      <title>An improved central limit theorem for the empirical sliced Wasserstein distance</title>
      <link>https://arxiv.org/abs/2503.18831</link>
      <description>arXiv:2503.18831v2 Announce Type: replace 
Abstract: Optimal transport theory has become a fundamental tool for handling diverse types of data, with growing applications across various fields. However, the Wasserstein distance presents significant computational and statistical challenges in high-dimensional settings. To address these issues, alternative distances such as the sliced Wasserstein distance, which leverages one-dimensional projections, have been introduced. In this work, we establish a novel central limit theorem for the p-sliced Wasserstein distance, for p&gt;1, using the Efron-Stein inequality-a technique that has proven effective in related problems. This approach yields a central limit theorem centered at the expected value of the empirical cost, under mild regularity conditions. Notably, unlike the general Wasserstein distance in arbitrary dimensions, we demonstrate that, under specific assumptions, the centering constants can be replaced by the population cost, which is essential for statistical inference. This generalizes and significantly refines existing results for the one-dimensional case. Consequently, we present the first asymptotically valid inference framework for the sliced Wasserstein distance between probability measures that are not necessarily compactly supported, for p&gt;1. Finally, we address key practical aspects to ensure its applicability to statistical inference, including Monte Carlo approximation of the slicing integral and consistent estimation of the asymptotic variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18831v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Rodr\'iguez-V\'itores, Eustasio del Barrio, Jean-Michel Loubes</dc:creator>
    </item>
    <item>
      <title>Measures of non-simplifyingness for conditional copulas and vines</title>
      <link>https://arxiv.org/abs/2504.07704</link>
      <description>arXiv:2504.07704v2 Announce Type: replace 
Abstract: In copula modeling, the simplifying assumption has recently been the object of much interest. Although it is very useful to reduce the computational burden, it remains far from obvious whether it is actually satisfied in practice. We propose a theoretical framework which aims at giving a precise meaning to the following question: how non-simplified or close to be simplified is a given conditional copula? For this, we propose a new framework centered at the notion of measure of non-constantness. Then we discuss generalizations of the simplifying assumption to the case where the conditional marginal distributions may not be continuous, and corresponding measures of non-simplifyingness in this case. The simplifying assumption is of particular importance for vine copula models, and we therefore propose a notion of measure of non-simplifyingness of a given copula for a particular vine structure, as well as different scores measuring how non-simplified such a vine decompositions would be for a general vine. Finally, we propose estimators for these measures of non-simplifyingness given an observed dataset. A small simulation study shows the performance of a few estimators of these measures of non-simplifyingness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07704v2</guid>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Derumigny</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Linear Models and Regression Adjusted Causal Inference in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2210.08589</link>
      <description>arXiv:2210.08589v5 Announce Type: replace-cross 
Abstract: Linear models are foundational tools in statistics and ubiquitous across the applied sciences. However, conventional statistical inference -- such as $t$-tests and $F$-tests -- are only valid at fixed sample sizes, making them unsuitable for sequential settings such as online A/B testing. We develop an anytime-valid theory of inference for the linear model, introducing sequential analogues of classical tests and confidence sets that provide Type-I error control and coverage guarantees uniformly over all sample sizes. Our construction is based on likelihood ratios of invariantly sufficient statistics, yielding simple closed-form expressions of ordinary least squares estimators and standard errors. The resulting tests are optimal in the GROW/REGROW sense for both frequentist and Bayesian alternative hypotheses. We then relax the linear model assumptions to provide heteroskedasticity-robust asymptotic sequential tests and confidence sequences, which enable sequential regression-adjusted inference for causal estimands in randomized controlled experiments. This formally allows experiments to be continuously monitored for significance, stopped early, and safeguards against statistical malpractices in data collection. We demonstrate the practical utility of our approach through simulations and applications to real A/B test data from Netflix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08589v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lindon, Dae Woong Ham, Martin Tingley, Iavor Bojinov</dc:creator>
    </item>
    <item>
      <title>The surrogate Gibbs-posterior of a corrected stochastic MALA: Towards uncertainty quantification for neural networks</title>
      <link>https://arxiv.org/abs/2310.09335</link>
      <description>arXiv:2310.09335v2 Announce Type: replace-cross 
Abstract: MALA is a popular gradient-based Markov chain Monte Carlo method to access the Gibbs-posterior distribution. Stochastic MALA (sMALA) scales to large data sets, but changes the target distribution from the Gibbs-posterior to a surrogate posterior which only exploits a reduced sample size. We introduce a corrected stochastic MALA (csMALA) with a simple correction term for which distance between the resulting surrogate posterior and the original Gibbs-posterior decreases in the full sample size while retaining scalability. In a nonparametric regression model, we prove a PAC-Bayes oracle inequality for the surrogate posterior. Uncertainties can be quantified by sampling from the surrogate posterior. Focusing on Bayesian neural networks, we analyze the diameter and coverage of credible balls for shallow neural networks and we show optimal contraction rates for deep neural networks. Our credibility result is independent of the correction and can also be applied to the standard Gibbs-posterior. A simulation study in a high-dimensional parameter space demonstrates that an estimator drawn from csMALA based on its surrogate Gibbs-posterior indeed exhibits these advantages in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09335v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Bieringer, Gregor Kasieczka, Maximilian F. Steffen, Mathias Trabs</dc:creator>
    </item>
    <item>
      <title>Robust Penalized Estimators for High--Dimensional Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2312.04661</link>
      <description>arXiv:2312.04661v2 Announce Type: replace-cross 
Abstract: Robust estimators for generalized linear models (GLMs) are not easy to develop due to the nature of the distributions involved. Recently, there has been growing interest in robust estimation methods, particularly in contexts involving a potentially large number of explanatory variables. Transformed M-estimators (MT-estimators) provide a natural extension of M-estimation techniques to the GLM framework, offering robust methodologies. We propose a penalized variant of MT-estimators to address high-dimensional data scenarios. Under suitable assumptions, we demonstrate the consistency and asymptotic normality of this novel class of estimators. Our theoretical development focuses on redescending rho-functions and penalization functions that satisfy specific regularity conditions. We present an Iterative Re-Weighted Least Squares algorithm, together with a deterministic initialization procedure, which is crucial since the estimating equations may have multiple solutions. We evaluate the finite sample performance of this method for Poisson distribution and well known penalization functions through Monte Carlo simulations that consider various types of contamination, as well as an empirical application using a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04661v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marina Valdora, Claudio Agostinelli</dc:creator>
    </item>
    <item>
      <title>Polynomial approximation of noisy functions</title>
      <link>https://arxiv.org/abs/2410.02317</link>
      <description>arXiv:2410.02317v3 Announce Type: replace-cross 
Abstract: Approximating a univariate function on the interval $[-1,1]$ with a polynomial is among the most classical problems in numerical analysis. When the function evaluations come with noise, a least-squares fit is known to reduce the effect of noise as more samples are taken. The generic algorithm for the least-squares problem requires $O(Nn^2)$ operations, where $N+1$ is the number of sample points and $n$ is the degree of the polynomial approximant. This algorithm is unstable when $n$ is large, for example $n\gg \sqrt{N}$ for equispaced sample points. In this study, we blend numerical analysis and statistics to introduce a stable and fast $O(N\log N)$ algorithm called NoisyChebtrunc based on the Chebyshev interpolation. It has the same error reduction effect as least-squares and the convergence is spectral until the error reaches $O(\sigma \sqrt{{n}/{N}})$, where $\sigma$ is the noise level, after which the error continues to decrease at the Monte-Carlo $O(1/\sqrt{N})$ rate. To determine the polynomial degree, NoisyChebtrunc employs a statistical criterion, namely Mallows' $C_p$. We analyze NoisyChebtrunc in terms of the variance and concentration in the infinity norm to the underlying noiseless function. These results show that with high probability the infinity-norm error is bounded by a small constant times $\sigma \sqrt{{n}/{N}}$, when the noise {is} independent and follows a subgaussian or subexponential distribution. We illustrate the performance of NoisyChebtrunc with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02317v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takeru Matsuda, Yuji Nakatsukasa</dc:creator>
    </item>
    <item>
      <title>Grammatical structures in mathematics: a personal view</title>
      <link>https://arxiv.org/abs/2410.07569</link>
      <description>arXiv:2410.07569v3 Announce Type: replace-cross 
Abstract: The ability to read, write, and speak mathematics is critical to students becoming comfortable with statistical models and skills. Faster development of those skills may act as encouragement to further engage with the discipline. Vocabulary has been the focus of scholarship in existing literature on the linguistics of mathematics and statistics but there are structures such as grammar that go beyond the content of words and symbols. Here I introduce ideas for grammar structures through a sequence of examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07569v3</guid>
      <category>stat.OT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tess O'Brien</dc:creator>
    </item>
    <item>
      <title>Improving Numerical Stability of Normalized Mutual Information Estimator on High Dimensions</title>
      <link>https://arxiv.org/abs/2410.07642</link>
      <description>arXiv:2410.07642v3 Announce Type: replace-cross 
Abstract: Mutual information provides a powerful, general-purpose metric for quantifying the amount of shared information between variables. Estimating normalized mutual information using a k-Nearest Neighbor (k-NN) based approach involves the calculation of the scaling-invariant k-NN radius. Calculation of the radius suffers from numerical overflow when the joint dimensionality of the data becomes high, typically in the range of several hundred dimensions. To address this issue, we propose a logarithmic transformation technique that improves the numerical stability of the radius calculation in high-dimensional spaces. By applying the proposed transformation during the calculation of the radius, numerical overflow is avoided, and precision is maintained. Proposed transformation is validated through both theoretical analysis and empirical evaluation, demonstrating its ability to stabilize the calculation without compromising precision, increasing bias, or adding significant computational overhead, while also helping to maintain estimator variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07642v3</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marko Tuononen, Ville Hautam\"aki</dc:creator>
    </item>
    <item>
      <title>Partial identification of principal causal effects under violations of principal ignorability</title>
      <link>https://arxiv.org/abs/2412.06628</link>
      <description>arXiv:2412.06628v2 Announce Type: replace-cross 
Abstract: Principal stratification is a general framework for studying causal mechanisms involving post-treatment variables. When estimating principal causal effects, the principal ignorability assumption is commonly invoked, which we study in detail in this manuscript. Our first key contribution is studying a commonly used strategy of using parametric models to jointly model the outcome and principal strata without requiring the principal ignorability assumption. We show that even if the joint distribution of principal strata is known, this strategy necessarily leads to only partial identification of causal effects, even under very simple and correctly specified outcome models. While principal ignorability leads to point identification in this setting, we discuss alternative, weaker assumptions and show how they can lead to informative partial identification regions. An additional contribution is that we provide theoretical support to strategies used in the literature for identifying association parameters that govern the joint distribution of principal strata. We prove that this is possible, but only if the principal ignorability assumption is violated. Additionally, due to partial identifiability of causal effects even when these association parameters are known, we show that these association parameters are only identifiable under strong parametric constraints. Lastly, we extend these results to more flexible semiparametric and nonparametric Bayesian models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06628v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minxuan Wu, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Minimax and Bayes Optimal Best-arm Identification: Adaptive Experimental Design for Treatment Choice</title>
      <link>https://arxiv.org/abs/2506.24007</link>
      <description>arXiv:2506.24007v2 Announce Type: replace-cross 
Abstract: This study investigates adaptive experimental design for treatment choice, also known as fixed-budget best-arm identification. We consider an adaptive procedure consisting of a treatment-allocation phase followed by a treatment-choice phase, and we design an adaptive experiment for this setup to efficiently identify the best treatment arm, defined as the one with the highest expected outcome. In our designed experiment, the treatment-allocation phase consists of two stages. The first stage is a pilot phase, where we allocate each treatment arm uniformly with equal proportions to eliminate clearly suboptimal arms and estimate outcome variances. In the second stage, we allocate treatment arms in proportion to the variances estimated in the first stage. After the treatment-allocation phase, the procedure enters the treatment-choice phase, where we choose the treatment arm with the highest sample mean as our estimate of the best treatment arm. We prove that this single design is simultaneously asymptotically minimax and Bayes optimal for the simple regret, with upper bounds that match our lower bounds up to exact constants. Therefore, our designed experiment achieves the sharp efficiency limits without requiring separate tuning for minimax and Bayesian objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24007v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
  </channel>
</rss>
