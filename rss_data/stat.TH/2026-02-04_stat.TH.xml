<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Feb 2026 13:25:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Decision-Focused Optimal Transport</title>
      <link>https://arxiv.org/abs/2602.02800</link>
      <description>arXiv:2602.02800v1 Announce Type: new 
Abstract: We propose a fundamental metric for measuring the distance between two distributions. This metric, referred to as the decision-focused (DF) divergence, is tailored to stochastic linear optimization problems in which the objective coefficients are random and may follow two distinct distributions. Traditional metrics such as KL divergence and Wasserstein distance are not well-suited for quantifying the resulting cost discrepancy, because changes in the coefficient distribution do not necessarily change the optimizer of the underlying linear program. Instead, the impact on the objective value depends on how the two distributions are coupled (aligned). Motivated by optimal transport, we introduce decision-focused distances under several settings, including the optimistic DF distance, the robust DF distance, and their entropy-regularized variants. We establish connections between the proposed DF distance and classical distributional metrics. For the calculation of the DF distance, we develop efficient computational methods. We further derive sample complexity guarantees for estimating these distances and show that the DF distance estimation avoids the curse of dimensionality that arises in Wasserstein distance estimation. The proposed DF distance provides a foundation for a broad range of applications. As an illustrative example, we study the interpolation between two distributions. Numerical studies, including a toy newsvendor problem and a real-world medical testing dataset, demonstrate the practical value of the proposed DF distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02800v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suhan Liu, Mo Liu</dc:creator>
    </item>
    <item>
      <title>Sharp Inequalities between Total Variation and Hellinger Distances for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2602.03202</link>
      <description>arXiv:2602.03202v1 Announce Type: new 
Abstract: We study the relation between the total variation (TV) and Hellinger distances between two Gaussian location mixtures. Our first result establishes a general upper bound: for any two mixing distributions supported on a compact set, the Hellinger distance between the two mixtures is controlled by the TV distance raised to a power $1-o(1)$, where the $o(1)$ term is of order $1/\log\log(1/\mathrm{TV})$. We also construct two sequences of mixing distributions that demonstrate the sharpness of this bound. Taken together, our results resolve an open problem raised in Jia et al. (2023) and thus lead to an entropic characterization of learning Gaussian mixtures in total variation. Our inequality also yields optimal robust estimation of Gaussian mixtures in Hellinger distance, which has a direct implication for bounding the minimax regret of empirical Bayes under Huber contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03202v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joonhyuk Jung, Chao Gao</dc:creator>
    </item>
    <item>
      <title>Orthogonal Approximate Message Passing Algorithms for Rectangular Spiked Matrix Models with Rotationally Invariant Noise</title>
      <link>https://arxiv.org/abs/2602.03283</link>
      <description>arXiv:2602.03283v1 Announce Type: new 
Abstract: We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that exactly characterizes the high-dimensional dynamics of the algorithm. Building on this framework, we derive an optimal variant of OAMP that minimizes the predicted mean-squared error at each iteration. For the special case of i.i.d. Gaussian noise, the fixed point of the proposed OAMP algorithm coincides with that of the standard AMP algorithm. For general RI noise models, we conjecture that the optimal OAMP algorithm is statistically optimal within a broad class of iterative methods, and achieves Bayes-optimal performance in certain regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03283v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haohua Chen, Songbin Liu, Junjie Ma</dc:creator>
    </item>
    <item>
      <title>Optimal neural network approximation of smooth compositional functions on sets with low intrinsic dimension</title>
      <link>https://arxiv.org/abs/2602.03539</link>
      <description>arXiv:2602.03539v1 Announce Type: new 
Abstract: We study approximation and statistical learning properties of deep ReLU networks under structural assumptions that mitigate the curse of dimensionality. We prove minimax-optimal uniform approximation rates for $s$-H\"older smooth functions defined on sets with low Minkowski dimension using fully connected networks with flexible width and depth, improving existing results by logarithmic factors even in classical full-dimensional settings. A key technical ingredient is a new memorization result for deep ReLU networks that enables efficient point fitting with dense architectures. We further introduce a class of compositional models in which each component function is smooth and acts on a domain of low intrinsic dimension. This framework unifies two common assumptions in the statistical learning literature, structural constraints on the target function and low dimensionality of the covariates, within a single model. We show that deep networks can approximate such functions at rates determined by the most difficult function in the composition. As an application, we derive improved convergence rates for empirical risk minimization in nonparametric regression that adapt to smoothness, compositional structure, and intrinsic dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03539v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Nagler, Sophie Langer</dc:creator>
    </item>
    <item>
      <title>Effect-Wise Inference for Smoothing Spline ANOVA on Tensor-Product Sobolev Space</title>
      <link>https://arxiv.org/abs/2602.02753</link>
      <description>arXiv:2602.02753v1 Announce Type: cross 
Abstract: Functional ANOVA provides a nonparametric modeling framework for multivariate covariates, enabling flexible estimation and interpretation of effect functions such as main effects and interaction effects. However, effect-wise inference in such models remains challenging. Existing methods focus primarily on inference for entire functions rather than individual effects. Methods addressing effect-wise inference face substantial limitations: the inability to accommodate interactions, a lack of rigorous theoretical foundations, or restriction to pointwise inference. To address these limitations, we develop a unified framework for effect-wise inference in smoothing spline ANOVA on a subspace of tensor product Sobolev space. For each effect function, we establish rates of convergence, pointwise confidence intervals, and a Wald-type test for whether the effect is zero, with power achieving the minimax distinguishable rate up to a logarithmic factor. Main effects achieve the optimal univariate rates, and interactions achieve optimal rates up to logarithmic factors. The theoretical foundation relies on an orthogonality decomposition of effect subspaces, which enables the extension of the functional Bahadur representation framework to effect-wise inference in smoothing spline ANOVA with interactions. Simulation studies and real-data application to the Colorado temperature dataset demonstrate superior performance compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02753v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youngjin Cho, Meimei Liu</dc:creator>
    </item>
    <item>
      <title>Plug-In Classification of Drift Functions in Diffusion Processes Using Neural Networks</title>
      <link>https://arxiv.org/abs/2602.02791</link>
      <description>arXiv:2602.02791v1 Announce Type: cross 
Abstract: We study a supervised multiclass classification problem for diffusion processes, where each class is characterized by a distinct drift function and trajectories are observed at discrete times. Extending the one-dimensional multiclass framework of Denis et al. (2024) to multidimensional diffusions, we propose a neural network-based plug-in classifier that estimates the drift functions for each class from independent sample paths and assigns labels based on a Bayes-type decision rule. Under standard regularity assumptions, we establish convergence rates for the excess misclassification risk, explicitly capturing the effects of drift estimation error and time discretization. Numerical experiments demonstrate that the proposed method achieves faster convergence and improved classification performance compared to Denis et al. (2024) in the one-dimensional setting, remains effective in higher dimensions when the underlying drift functions admit a compositional structure, and consistently outperforms direct neural network classifiers trained end-to-end on trajectories without exploiting the diffusion model structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02791v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuzhen Zhao, Jiarong Fan, Yating Liu</dc:creator>
    </item>
    <item>
      <title>When pre-training hurts LoRA fine-tuning: a dynamical analysis via single-index models</title>
      <link>https://arxiv.org/abs/2602.02855</link>
      <description>arXiv:2602.02855v1 Announce Type: cross 
Abstract: Pre-training on a source task is usually expected to facilitate fine-tuning on similar downstream problems. In this work, we mathematically show that this naive intuition is not always true: excessive pre-training can computationally slow down fine-tuning optimization. We study this phenomenon for low-rank adaptation (LoRA) fine-tuning on single-index models trained under one-pass SGD. Leveraging a summary statistics description of the fine-tuning dynamics, we precisely characterize how the convergence rate depends on the initial fine-tuning alignment and the degree of non-linearity of the target task. The key take away is that even when the pre-training and down- stream tasks are well aligned, strong pre-training can induce a prolonged search phase and hinder convergence. Our theory thus provides a unified picture of how pre-training strength and task difficulty jointly shape the dynamics and limitations of LoRA fine-tuning in a nontrivial tractable model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02855v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gibbs Nwemadji, Bruno Loureiro, Jean Barbier</dc:creator>
    </item>
    <item>
      <title>Shiha Distribution: Statistical Properties and Applications to Reliability Engineering and Environmental Data</title>
      <link>https://arxiv.org/abs/2602.02875</link>
      <description>arXiv:2602.02875v1 Announce Type: cross 
Abstract: This paper introduces a new two-parameter distribution, referred to as the Shiha distribution, which provides a flexible model for skewed lifetime data with either heavy or light tails. The proposed distribution is applicable to various fields, including reliability engineering, environmental studies, and related areas. We derive its main statistical properties, including the moment generating function, moments, hazard rate function, quantile function, and entropy. The stress--strength reliability parameter is also derived in closed form. A simulation study is conducted to evaluate its performance. Applications to several real data sets demonstrate that the Shiha distribution consistently provides a superior fit compared with established competing models, confirming its practical effectiveness for lifetime data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02875v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>F. A. Shiha</dc:creator>
    </item>
    <item>
      <title>Unified Inference Framework for Single and Multi-Player Performative Prediction: Method and Asymptotic Optimality</title>
      <link>https://arxiv.org/abs/2602.03049</link>
      <description>arXiv:2602.03049v1 Announce Type: cross 
Abstract: Performative prediction characterizes environments where predictive models alter the very data distributions they aim to forecast, triggering complex feedback loops. While prior research treats single-agent and multi-agent performativity as distinct phenomena, this paper introduces a unified statistical inference framework that bridges these contexts, treating the former as a special case of the latter. Our contribution is two-fold. First, we put forward the Repeated Risk Minimization (RRM) procedure for estimating the performative stability, and establish a rigorous inferential theory for admitting its asymptotic normality and confirming its asymptotic efficiency. Second, for the performative optimality, we introduce a novel two-step plug-in estimator that integrates the idea of Recalibrated Prediction Powered Inference (RePPI) with Importance Sampling, and further provide formal derivations for the Central Limit Theorems of both the underlying distributional parameters and the plug-in results. The theoretical analysis demonstrates that our estimator achieves the semiparametric efficiency bound and maintains robustness under mild distributional misspecification. This work provides a principled toolkit for reliable estimation and decision-making in dynamic, performative environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03049v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhixian Zhang, Xiaotian Hou, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>Evaluating LLMs When They Do Not Know the Answer: Statistical Evaluation of Mathematical Reasoning via Comparative Signals</title>
      <link>https://arxiv.org/abs/2602.03061</link>
      <description>arXiv:2602.03061v1 Announce Type: cross 
Abstract: Evaluating mathematical reasoning in LLMs is constrained by limited benchmark sizes and inherent model stochasticity, yielding high-variance accuracy estimates and unstable rankings across platforms. On difficult problems, an LLM may fail to produce a correct final answer, yet still provide reliable pairwise comparison signals indicating which of two candidate solutions is better. We leverage this observation to design a statistically efficient evaluation framework that combines standard labeled outcomes with pairwise comparison signals obtained by having models judge auxiliary reasoning chains. Treating these comparison signals as control variates, we develop a semiparametric estimator based on the efficient influence function (EIF) for the setting where auxiliary reasoning chains are observed. This yields a one-step estimator that achieves the semiparametric efficiency bound, guarantees strict variance reduction over naive sample averaging, and admits asymptotic normality for principled uncertainty quantification. Across simulations, our one-step estimator substantially improves ranking accuracy, with gains increasing as model output noise grows. Experiments on GPQA Diamond, AIME 2025, and GSM8K further demonstrate more precise performance estimation and more reliable model rankings, especially in small-sample regimes where conventional evaluation is pretty unstable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03061v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihan Dong, Zhixian Zhang, Yang Zhou, Can Jin, Ruijia Wu, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>On the compatibility between the spatial moments and the codomain of a real random field</title>
      <link>https://arxiv.org/abs/2602.03740</link>
      <description>arXiv:2602.03740v1 Announce Type: cross 
Abstract: While any symmetric and positive semidefinite mapping can be the non-centered covariance of a Gaussian random field, it is known that these conditions are no longer sufficient when the random field is valued in a two-point set. The question therefore arises of what are the necessary and sufficient conditions for a mapping $\rho: \X \times \X \to \R$ to be the non-centered covariance of a random field with values in a subset ${\cE}$ of $\R$. Such conditions are presented in the general case when ${\cE}$ is a closed subset of the real line, then examined for some specific cases. In particular, if ${\cE}=\R$ or $\Z$, it is shown that the conditions reduce to $\rho$ being symmetric and positive semidefinite. If ${\cE}$ is a closed interval or a two-point set, the necessary and sufficient conditions are more restrictive: the symmetry, positive semidefiniteness, upper and lower boundedness of $\rho$ are no longer enough to guarantee the existence of a random field valued in ${\cE}$ and having $\rho$ as its non-centered covariance. Similar characterizations are obtained for semivariograms and higher-order spatial moments, as well as for multivariate random fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03740v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xavier Emery, Christian Lantu\'ejoul</dc:creator>
    </item>
    <item>
      <title>Neural Entropic Optimal Transport and Gromov-Wasserstein Alignment</title>
      <link>https://arxiv.org/abs/2312.07397</link>
      <description>arXiv:2312.07397v2 Announce Type: replace 
Abstract: Optimal transport (OT) and Gromov-Wasserstein (GW) alignment are powerful frameworks for geometrically driven matching of probability distributions, yet their large-scale usage is hampered by high statistical and computational costs. Entropic regularization has emerged as a promising solution, allowing parametric convergence rates via the plug-in estimator, which can be computed using the Sinkhorn algorithm (or its iterations in the GW case). However, Sinkhorn's $O(n^2)$ time complexity for an $n$-sized dataset becomes prohibitive for modern, massive datasets. In this work, we propose a new computational framework for the entropic OT and GW problems that replaces the Sinkhorn step with a neural network trained via backpropagation on mini-batches. By shifting the computational load from the entire dataset to the mini-batch, our approach enables reliable estimation of both the optimal transport/alignment cost and plan at dataset sizes and dimensions far exceeding those tractable with standard Sinkhorn methods. We derive non-asymptotic error bounds for these estimates, showing they achieve minimax-optimal parametric convergence rates for compactly supported distributions. Numerical experiments confirm the accuracy of our method in high-dimensional, large-sample regimes where Sinkhorn is infeasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07397v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2026.3661439)</arxiv:DOI>
      <dc:creator>Tao Wang, Ziv Goldfeld</dc:creator>
    </item>
    <item>
      <title>Asymptotics of resampling without replacement in robust and logistic regression</title>
      <link>https://arxiv.org/abs/2404.02070</link>
      <description>arXiv:2404.02070v3 Announce Type: replace 
Abstract: This paper studies the asymptotics of resampling without replacement in the proportional regime where dimension $p$ and sample size $n$ are of the same order. For a given dataset $(X,y)\in \mathbb{R}^{n\times p}\times \mathbb{R}^n$ and fixed subsample ratio $q\in(0,1)$, the practitioner samples independently of $(X,y)$ iid subsets $I_1,...,I_M$ of $\{1,...,n\}$ of size $q n$ and trains estimators $\hat{\beta}(I_1),...,\hat{\beta}(I_M)$ on the corresponding subsets of rows of $(X, y)$. Understanding the performance of the bagged estimate $\bar{\beta} = \frac1M\sum_{m=1}^M \hat{\beta}(I_1),...,\hat{\beta}(I_M)$, for instance its squared error, requires us to understand correlations between two distinct $\hat{\beta}(I_m)$ and $\hat{\beta}(I_{m'})$ trained on different subsets $I_m$ and $I_{m'}$.
  In robust linear regression and logistic regression, we characterize the limit in probability of the correlation between two estimates trained on different subsets of the data. The limit is characterized as the unique solution of a simple nonlinear equation. We further provide data-driven estimators that are consistent for estimating this limit. These estimators of the limiting correlation allow us to estimate the squared error of the bagged estimate $\bar{\beta}$, and for instance perform parameter tuning to choose the optimal subsample ratio $q$. As a by-product of the proof argument, we obtain the limiting distribution of the bivariate pair $(x_i^T \hat{\beta}(I_m), x_i^T \hat{\beta}(I_{m'}))$ for observations $i\in I_m\cap I_{m'}$, i.e., for observations used to train both estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02070v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre C. Bellec, Takuya Koriyama</dc:creator>
    </item>
    <item>
      <title>LSD of the Commutator of two data Matrices</title>
      <link>https://arxiv.org/abs/2503.00014</link>
      <description>arXiv:2503.00014v2 Announce Type: replace 
Abstract: We study the spectral properties of a class of random matrices of the form $S_n^{-} = n^{-1}(X_1 X_2^* - X_2 X_1^*)$ where $X_k = \Sigma_k^{1/2}Z_k$, $Z_k$'s are independent $p\times n$ complex-valued random matrices, and $\Sigma_k$ are $p\times p$ positive semi-definite matrices that commute and are independent of the $Z_k$'s for $k=1,2$. We assume that $Z_k$'s have independent entries with zero mean and unit variance. The skew-symmetric/skew-Hermitian matrix $S_n^{-}$ will be referred to as a random commutator matrix associated with the samples $X_1$ and $X_2$. We show that, when the dimension $p$ and sample size $n$ increase simultaneously, so that $p/n \to c \in (0,\infty)$, there exists a limiting spectral distribution (LSD) for $S_n^{-}$, supported on the imaginary axis, under the assumptions that the joint spectral distribution of $\Sigma_1, \Sigma_2$ converges weakly and the entries of $Z_k$'s have moments of sufficiently high order. This nonrandom LSD can be described through its Stieltjes transform, which satisfies a system of Mar\v{c}enko-Pastur-type functional equations. Moreover, we show that the companion matrix $S_n^{+} = n^{-1}(X_1X_2^* + X_2X_1^*)$, under identical assumptions, has an LSD supported on the real line, which can be similarly characterized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00014v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javed Hazarika, Debashis Paul</dc:creator>
    </item>
    <item>
      <title>Conditional Extreme Value Estimation for Dependent Time Series</title>
      <link>https://arxiv.org/abs/2503.22366</link>
      <description>arXiv:2503.22366v3 Announce Type: replace 
Abstract: We study the consistency and weak convergence of the conditional tail function and conditional Hill estimators under broad dependence assumptions for a heavy-tailed response sequence and a covariate sequence. Consistency is established under $\alpha$-mixing, while asymptotic normality follows from $\beta$-mixing and second-order conditions. A key aspect of our approach is its versatile functional formulation in terms of the conditional tail process. Simulations demonstrate its performance across dependence scenarios. We apply our method to extreme event modelling in the oil industry, revealing distinct tail behaviours under varying conditioning values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22366v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10687-025-00524-5</arxiv:DOI>
      <arxiv:journal_reference>Bladt, M., Glargaard, L. &amp; Henningsen, T. Conditional extreme value estimation for dependent time series. Extremes (2026)</arxiv:journal_reference>
      <dc:creator>Martin Bladt, Laurits Glargaard, Theodor Henningsen</dc:creator>
    </item>
    <item>
      <title>Heavy-tailed and Horseshoe priors for regression and sparse Besov rates</title>
      <link>https://arxiv.org/abs/2505.15543</link>
      <description>arXiv:2505.15543v2 Announce Type: replace 
Abstract: The large variety of functions encountered in nonparametric statistics, calls for methods that are flexible enough to achieve optimal or near-optimal performance over a wide variety of functional classes, such as Besov balls, as well as over a large array of loss functions. In this work, we show that a class of heavy-tailed prior distributions on basis function coefficients introduced in \cite{AC} and called Oversmoothed heavy-Tailed (OT) priors, leads to Bayesian posterior distributions that satisfy these requirements; the case of horseshoe distributions is also investigated, for the first time in the context of nonparametrics, and we show that they fit into this framework. Posterior contraction rates are derived in two settings. The case of Sobolev--smooth signals and $L_2$--risk is considered first, along with a lower bound result showing that the imposed form of the scalings on prior coefficients by the OT prior is necessary to get full adaptation to smoothness. Second, the broader case of Besov-smooth signals with $L_{p'}$--risks, $p' \geq 1$, is considered, and minimax posterior contraction rates, adaptive to the underlying smoothness, and including rates in the so-called {\em sparse} zone, are derived. We provide an implementation of the proposed method and illustrate our results through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15543v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergios Agapiou, Isma\"el Castillo, Paul Egels</dc:creator>
    </item>
    <item>
      <title>Functional-SVD for Heterogeneous Trajectories: Case Studies in Health</title>
      <link>https://arxiv.org/abs/2410.03619</link>
      <description>arXiv:2410.03619v5 Announce Type: replace-cross 
Abstract: Trajectory data, including time series and longitudinal measurements, are increasingly common in health-related domains such as biomedical research and epidemiology. Real-world trajectory data frequently exhibit heterogeneity across subjects such as patients, sites, and subpopulations, yet many traditional methods are not designed to accommodate such heterogeneity in data analysis. To address this, we propose a unified framework, termed Functional Singular Value Decomposition (FSVD), for statistical learning with heterogeneous trajectories. We establish the theoretical foundations of FSVD and develop a corresponding estimation algorithm that accommodates noisy and irregular observations. We further adapt FSVD to a wide range of trajectory-learning tasks, including dimension reduction, factor modeling, regression, clustering, and data completion, while preserving its ability to account for heterogeneity, leverage inherent smoothness, and handle irregular sampling. Through extensive simulations, we demonstrate that FSVD-based methods consistently outperform existing approaches across these tasks. Finally, we apply FSVD to a COVID-19 case-count dataset and electronic health record datasets, showcasing its effective performance in global and subgroup pattern discovery and factor analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03619v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Pixu Shi, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation</title>
      <link>https://arxiv.org/abs/2505.17961</link>
      <description>arXiv:2505.17961v3 Announce Type: replace-cross 
Abstract: Causal inference typically assumes centralized access to individual-level data. Yet, in practice, data are often decentralized across multiple sites, making centralization infeasible due to privacy, logistical, or legal constraints. We address this problem by estimating the Average Treatment Effect (ATE) from decentralized observational data via a Federated Learning (FL) approach, allowing inference through the exchange of aggregate statistics rather than individual-level data.
  We propose a novel method to estimate propensity scores via a federated weighted average of local scores using Membership Weights (MW), defined as probabilities of site membership conditional on covariates. MW can be flexibly estimated with parametric or non-parametric classification models using standard FL algorithms. The resulting propensity scores are used to construct Federated Inverse Propensity Weighting (Fed-IPW) and Augmented IPW (Fed-AIPW) estimators. In contrast to meta-analysis methods, which fail when any site violates positivity, our approach exploits heterogeneity in treatment assignment across sites to improve overlap. We show that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample sizes, treatment mechanisms, and covariate distributions. Theoretical analysis and experiments on simulated and real-world data demonstrate clear advantages over meta-analysis and related approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17961v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>R\'emi Khellaf, Aur\'elien Bellet, Julie Josse</dc:creator>
    </item>
    <item>
      <title>How weak are weak factors? Uniform inference for signal strength in signal plus noise models</title>
      <link>https://arxiv.org/abs/2507.18554</link>
      <description>arXiv:2507.18554v2 Announce Type: replace-cross 
Abstract: The paper analyzes four classical signal-plus-noise models: the factor model, spiked sample covariance matrices, the sum of a Wigner matrix and a low-rank perturbation, and canonical correlation analysis with low-rank dependencies. The objective is to construct confidence intervals for the signal strength that are uniformly valid across all regimes - strong, weak, and critical signals. We demonstrate that traditional Gaussian approximations fail in the critical regime. Instead, we introduce a universal transitional distribution that enables valid inference across the entire spectrum of signal strengths. The approach is illustrated through applications in macroeconomics and finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18554v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Bykhovskaya, Vadim Gorin, Sasha Sodin</dc:creator>
    </item>
    <item>
      <title>Bias-variance Tradeoff in Tensor Estimation</title>
      <link>https://arxiv.org/abs/2509.17382</link>
      <description>arXiv:2509.17382v2 Announce Type: replace-cross 
Abstract: We study denoising of a third-order tensor when the ground-truth tensor is not necessarily Tucker low-rank. Specifically, we observe $$ Y=X^\ast+Z\in \mathbb{R}^{p_{1} \times p_{2} \times p_{3}}, $$ where $X^\ast$ is the ground-truth tensor, and $Z$ is the noise tensor. We propose a simple variant of the higher-order tensor SVD estimator $\widetilde{X}$. We show that uniformly over all user-specified Tucker ranks $(r_{1},r_{2},r_{3})$, $$ \| \widetilde{X} - X^* \|_{ \mathrm{F}}^2 = O \Big( \kappa^2 \Big\{ r_{1}r_{2}r_{3}+\sum_{k=1}^{3} p_{k} r_{k} \Big\} \; + \; \xi_{(r_{1},r_{2},r_{3})}^2\Big) \quad \text{ with high probability.} $$ Here, the bias term $\xi_{(r_1,r_2,r_3)}$ corresponds to the best achievable approximation error of $X^\ast$ over the class of tensors with Tucker ranks $(r_1,r_2,r_3)$; $\kappa^2$ quantifies the noise level; and the variance term $\kappa^2 \{r_{1}r_{2}r_{3}+\sum_{k=1}^{3} p_{k} r_{k}\}$ scales with the effective number of free parameters in the estimator $\widetilde{X}$. Our analysis achieves a clean rank-adaptive bias--variance tradeoff: as we increase the ranks of estimator $\widetilde{X}$, the bias $\xi(r_{1},r_{2},r_{3})$ decreases and the variance increases. As a byproduct we also obtain a convenient bias-variance decomposition for the vanilla low-rank SVD matrix estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17382v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shivam Kumar, Haotian Xu, Carlos Misael Madrid Padilla, Yuehaw Khoo, Oscar Hernan Madrid Padilla, Daren Wang</dc:creator>
    </item>
    <item>
      <title>Online Price Competition under Generalized Linear Demands</title>
      <link>https://arxiv.org/abs/2511.10718</link>
      <description>arXiv:2511.10718v5 Announce Type: replace-cross 
Abstract: We study sequential price competition among $N$ sellers, each influenced by the pricing decisions of their rivals. Specifically, the demand function for each seller $i$ follows the single index model $\lambda_i(\mathbf{p}) = \mu_i(\langle \boldsymbol{\theta}_{i,0}, \mathbf{p} \rangle)$, with known increasing link $\mu_i$ and unknown parameter $\boldsymbol{\theta}_{i,0}$, where the vector $\mathbf{p}$ denotes the vector of prices offered by all the sellers simultaneously at a given instant. Each seller observes only their own realized demand -- unobservable to competitors -- and the prices set by rivals. Our framework generalizes existing approaches that focus solely on linear demand models. We propose a novel decentralized policy, PML-GLUCB, that combines penalized MLE with an upper-confidence pricing rule, removing the need for coordinated exploration phases across sellers -- which is integral to previous linear models -- and accommodating both binary and real-valued demand observations. Relative to a dynamic benchmark policy, each seller achieves $O(N^{2}\sqrt{T}\log(T))$ regret, which essentially matches the optimal rate known in the linear setting. A significant technical contribution of our work is the development of a variant of the elliptical potential lemma -- typically applied in single-agent systems -- adapted to our competitive multi-agent environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10718v5</guid>
      <category>cs.GT</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Bracale, Moulinath Banerjee, Cong Shi, Yuekai Sun</dc:creator>
    </item>
  </channel>
</rss>
