<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mental intervention in quantum scattering of ions without violating conservation laws</title>
      <link>https://arxiv.org/abs/2406.08601</link>
      <description>arXiv:2406.08601v1 Announce Type: new 
Abstract: There have been several proposals in the past that mind might influence matter by exploiting the randomness of quantum events. Here, calculations are presented how mental selection of quantum mechanical scattering directions of ions in the axon hillock of neuronal cells could influence diffusion and initiate an action potential. Only a few thousand ions would need to be affected. No conservation laws are violated, but a momentary and very small local decrease of temperature should occur, consistent with a quantum mechanically possible but extremely improbable evolution. An estimate of the concurrent violation of the second law of thermodynamics is presented. Some thoughts are given to how this hypothesized mental intervention could be tested.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08601v1</guid>
      <category>q-bio.NC</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johann Summhammer</dc:creator>
    </item>
    <item>
      <title>How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models</title>
      <link>https://arxiv.org/abs/2406.09067</link>
      <description>arXiv:2406.09067v1 Announce Type: cross 
Abstract: Forming and using symbol-like structured representations for reasoning has been considered essential for generalising over novel inputs. The primary tool that allows generalisation outside training data distribution is the ability to abstract away irrelevant information into a compact form relevant to the task. An extreme form of such abstract representations is symbols. Humans make use of symbols to bind information while abstracting away irrelevant parts to utilise the information consistently and meaningfully. This work estimates the state of such structured representations in vision encoders. Specifically, we evaluate image encoders in large vision-language pre-trained models to address the question of which desirable properties their representations lack by applying the criteria of symbolic structured reasoning described for LLMs to the image models. We test the representation space of image encoders like VIT, BLIP, CLIP, and FLAVA to characterise the distribution of the object representations in these models. In particular, we create decoding tasks using multi-object scenes from the COCO dataset, relating the token space to its input content for various objects in the scene. We use these tasks to characterise the network's token and layer-wise information modelling. Our analysis highlights that the CLS token, used for the downstream task, only focuses on a few objects necessary for the trained downstream task. Still, other individual objects are well-modelled separately by the tokens in the network originating from those objects. We further observed a widespread distribution of scene information. This demonstrates that information is far more entangled in tokens than optimal for representing objects similar to symbols. Given these symbolic properties, we show the network dynamics that cause failure modes of these models on basic downstream tasks in a multi-object scene.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09067v1</guid>
      <category>cs.CV</category>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarun Khajuria, Braian Olmiro Dias, Jaan Aru</dc:creator>
    </item>
    <item>
      <title>Towards an Improved Understanding and Utilization of Maximum Manifold Capacity Representations</title>
      <link>https://arxiv.org/abs/2406.09366</link>
      <description>arXiv:2406.09366v1 Announce Type: cross 
Abstract: Maximum Manifold Capacity Representations (MMCR) is a recent multi-view self-supervised learning (MVSSL) method that matches or surpasses other leading MVSSL methods. MMCR is intriguing because it does not fit neatly into any of the commonplace MVSSL lineages, instead originating from a statistical mechanical perspective on the linear separability of data manifolds. In this paper, we seek to improve our understanding and our utilization of MMCR. To better understand MMCR, we leverage tools from high dimensional probability to demonstrate that MMCR incentivizes alignment and uniformity of learned embeddings. We then leverage tools from information theory to show that such embeddings maximize a well-known lower bound on mutual information between views, thereby connecting the geometric perspective of MMCR to the information-theoretic perspective commonly discussed in MVSSL. To better utilize MMCR, we mathematically predict and experimentally confirm non-monotonic changes in the pretraining loss akin to double descent but with respect to atypical hyperparameters. We also discover compute scaling laws that enable predicting the pretraining loss as a function of gradients steps, batch size, embedding dimension and number of views. We then show that MMCR, originally applied to image data, is performant on multimodal image-text data. By more deeply understanding the theoretical and empirical behavior of MMCR, our work reveals insights on improving MVSSL methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09366v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rylan Schaeffer, Victor Lecomte, Dhruv Bhandarkar Pai, Andres Carranza, Berivan Isik, Alyssa Unell, Mikail Khona, Thomas Yerxa, Yann LeCun, SueYeon Chung, Andrey Gromov, Ravid Shwartz-Ziv, Sanmi Koyejo</dc:creator>
    </item>
    <item>
      <title>A minimal model of cognition based on oscillatory and reinforcement processes</title>
      <link>https://arxiv.org/abs/2402.02520</link>
      <description>arXiv:2402.02520v2 Announce Type: replace 
Abstract: Building mathematical models of brains is difficult because of the sheer complexity of the problem. One potential starting point is through basal cognition, which give abstract representation of a range of organisms without central nervous systems, including fungi, slime moulds and bacteria. We propose one such model, demonstrating how a combination of oscillatory and current-based reinforcement processes can be used to couple resources in an efficient manner, mimicking the way these organisms function. A key ingredient in our model, not found in previous basal cognition models, is that we explicitly model oscillations in the number of particles (i.e. the nutrients, chemical signals or similar, which make up the biological system) and the flow of these particles within the modelled organisms. Using this approach, we find that our model builds efficient solutions, provided the environmental oscillations are sufficiently out of phase. We further demonstrate that amplitude differences can promote efficient solutions and that the system is robust to frequency differences. In the context of these findings, we discuss connections between our model and basal cognition in biological systems and slime moulds, in particular, how oscillations might contribute to self-organised problem-solving by these organisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02520v2</guid>
      <category>q-bio.NC</category>
      <category>cs.SI</category>
      <category>math.DS</category>
      <category>nlin.AO</category>
      <category>physics.bio-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linn\'ea Gyllingberg, Yu Tian, David J. T. Sumpter</dc:creator>
    </item>
    <item>
      <title>Bridging Neuroscience and AI: Environmental Enrichment as a Model for Forward Knowledge Transfer</title>
      <link>https://arxiv.org/abs/2405.07295</link>
      <description>arXiv:2405.07295v2 Announce Type: replace 
Abstract: Continual learning (CL) refers to an agent's capability to learn from a continuous stream of data and transfer knowledge without forgetting old information. One crucial aspect of CL is forward transfer, i.e., improved and faster learning on a new task by leveraging information from prior knowledge. While this ability comes naturally to biological brains, it poses a significant challenge for artificial intelligence (AI). Here, we suggest that environmental enrichment (EE) can be used as a biological model for studying forward transfer, inspiring human-like AI development. EE refers to animal studies that enhance cognitive, social, motor, and sensory stimulation and is a model for what, in humans, is referred to as 'cognitive reserve'. Enriched animals show significant improvement in learning speed and performance on new tasks, typically exhibiting forward transfer. We explore anatomical, molecular, and neuronal changes post-EE and discuss how artificial neural networks (ANNs) can be used to predict neural computation changes after enriched experiences. Finally, we provide a synergistic way of combining neuroscience and AI research that paves the path toward developing AI capable of rapid and efficient new task learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07295v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rajat Saxena, Bruce L. McNaughton</dc:creator>
    </item>
    <item>
      <title>Progress Towards Decoding Visual Imagery via fNIRS</title>
      <link>https://arxiv.org/abs/2406.07662</link>
      <description>arXiv:2406.07662v2 Announce Type: replace-cross 
Abstract: We demonstrate the possibility of reconstructing images from fNIRS brain activity and start building a prototype to match the required specs. By training an image reconstruction model on downsampled fMRI data, we discovered that cm-scale spatial resolution is sufficient for image generation. We obtained 71% retrieval accuracy with 1-cm resolution, compared to 93% on the full-resolution fMRI, and 20% with 2-cm resolution. With simulations and high-density tomography, we found that time-domain fNIRS can achieve 1-cm resolution, compared to 2-cm resolution for continuous-wave fNIRS. Lastly, we share designs for a prototype time-domain fNIRS device, consisting of a laser driver, a single photon detector, and a time-to-digital converter system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07662v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michel Adamic, Wellington Avelino, Anna Brandenberger, Bryan Chiang, Hunter Davis, Stephen Fay, Andrew Gregory, Aayush Gupta, Raphael Hotter, Grace Jiang, Fiona Leng, Stephen Polcyn, Thomas Ribeiro, Paul Scotti, Michelle Wang, Marley Xiong, Jonathan Xu</dc:creator>
    </item>
  </channel>
</rss>
