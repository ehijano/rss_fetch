<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Sep 2025 04:03:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sequential Change Detection with Differential Privacy</title>
      <link>https://arxiv.org/abs/2509.02768</link>
      <description>arXiv:2509.02768v1 Announce Type: new 
Abstract: Sequential change detection is a fundamental problem in statistics and signal processing, with the CUSUM procedure widely used to achieve minimax detection delay under a prescribed false-alarm rate when pre- and post-change distributions are fully known. However, releasing CUSUM statistics and the corresponding stopping time directly can compromise individual data privacy. We therefore introduce a differentially private (DP) variant, called DP-CUSUM, that injects calibrated Laplace noise into both the vanilla CUSUM statistics and the detection threshold, preserving the recursive simplicity of the classical CUSUM statistics while ensuring per-sample differential privacy. We derive closed-form bounds on the average run length to false alarm and on the worst-case average detection delay, explicitly characterizing the trade-off among privacy level, false-alarm rate, and detection efficiency. Our theoretical results imply that under a weak privacy constraint, our proposed DP-CUSUM procedure achieves the same first-order asymptotic optimality as the classical, non-private CUSUM procedure. Numerical simulations are conducted to demonstrate the detection efficiency of our proposed DP-CUSUM under different privacy constraints, and the results are consistent with our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02768v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liyan Xie, Ruizhi Zhang</dc:creator>
    </item>
    <item>
      <title>Reduce-Rank Matrix Integer-Valued Autoregressive Model</title>
      <link>https://arxiv.org/abs/2509.03338</link>
      <description>arXiv:2509.03338v1 Announce Type: new 
Abstract: Integer-valued time series are widely present in many fields, such as finance, economics, disease transmission, and traffic flow. With data dimensions surging, the traditional multivariate generalized integer autoregressive (MGINAR) model faces parameter overload, poor interpretability, and structural information loss. Matrix integer-valued autoregression (MINAR) model captures row-column cross-correlations and reduces the number of parameters to be estimated. However, further growth in dimensionality causes data redundancy, which degrades the MINAR model's performance and increases the number of parameters. To solve the limitations of the MINAR model described above, this paper proposes the reduced-rank matrix integer-valued autoregression (RRMINAR) model. Reducing rank is achieved by adding low-rank constraints to the coefficient matrices in the MINAR model, leading to RRMINAR reducing parameter quantity while incorporating matrix structure information. We develop an iterative conditional least squares estimation and analyze its asymptotic properties. Simulation results demonstrate that the proposed RRMINAR model exhibits more robust parameter estimation and higher prediction accuracy than MGINAR and MINAR models when the data structure is low-rank. Empirical analysis using criminal data validates the proposed RRMINAR model's effectiveness and uncovers structural temporal-spatial information in criminal behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03338v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiyan Cui, Tianyun Guo, Suping Wang</dc:creator>
    </item>
    <item>
      <title>Robust Survival Estimation under Interval Censoring: Expectation-Maximization and Bayesian Accelerated Failure Time Assessment via Simulation and Application</title>
      <link>https://arxiv.org/abs/2509.02634</link>
      <description>arXiv:2509.02634v1 Announce Type: cross 
Abstract: Interval censoring occurs when event times are only known to fall between scheduled assessments, a common design in clinical trials, epidemiology, and reliability studies. Standard right-censoring methods, such as Kaplan-Meier and Cox regression, are not directly applicable and can produce biased results. This study compares three complementary approaches for interval-censored survival data. First, the Turnbull nonparametric maximum likelihood estimator (NPMLE) via the EM algorithm recovers the survival distribution without strong assumptions. Second, Weibull and log-normal accelerated failure time (AFT) models with interval likelihoods provide smooth, covariate-adjusted survival curves and interpretable time-ratio effects. Third, Bayesian AFT models extend these tools by quantifying posterior uncertainty, incorporating prior information, and enabling interval-aware model comparisons via PSIS-LOO cross-validation. Simulations across generating distributions, censoring intensities, sample sizes, and covariate structures evaluated the integrated squared error (ISE) for curve recovery, integrated Brier score (IBS) for prediction, and coverage for uncertainty calibration. Results show that the EM achieves the lowest ISE for distribution recovery, AFT models improve predictive performance when families are correctly specified, and Bayesian AFT offers calibrated uncertainty and principled model selection. An application to the ovarian cancer dataset, restructured into interval-censored form, demonstrates the workflow in practice: the EM algorithm reveals the baseline shape, parametric AFT provides covariate-adjusted predictions, and Bayesian AFT validates model adequacy through posterior predictive checks. Together, these methods form a tiered strategy: EM for shape discovery, AFT for covariate-driven prediction, and Bayesian AFT for complete uncertainty quantification and model comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02634v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>J. T. Korley</dc:creator>
    </item>
    <item>
      <title>Fast kernel methods: Sobolev, physics-informed, and additive models</title>
      <link>https://arxiv.org/abs/2509.02649</link>
      <description>arXiv:2509.02649v1 Announce Type: cross 
Abstract: Kernel methods are powerful tools in statistical learning, but their cubic complexity in the sample size n limits their use on large-scale datasets. In this work, we introduce a scalable framework for kernel regression with O(n log n) complexity, fully leveraging GPU acceleration. The approach is based on a Fourier representation of kernels combined with non-uniform fast Fourier transforms (NUFFT), enabling exact, fast, and memory-efficient computations. We instantiate our framework in three settings: Sobolev kernel regression, physics-informed regression, and additive models. When known, the proposed estimators are shown to achieve minimax convergence rates, consistent with classical kernel theory. Empirical results demonstrate that our methods can process up to tens of billions of samples within minutes, providing both statistical accuracy and computational scalability. These contributions establish a flexible approach, paving the way for the routine application of kernel methods in large-scale learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02649v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Doum\`eche (LPSM, EDF R&amp;D OSIRIS), Francis Bach (ENS-PSL), G\'erard Biau (LPSM, IUF), Claire Boyer (LMO)</dc:creator>
    </item>
    <item>
      <title>The Nearest-Neighbor Derivative Process: Modeling Spatial Rates of Change in Massive Datasets</title>
      <link>https://arxiv.org/abs/2509.02752</link>
      <description>arXiv:2509.02752v1 Announce Type: cross 
Abstract: Gaussian processes (GPs) are instrumental in modeling spatial processes, offering precise interpolation and prediction capabilities across fields such as environmental science and biology. Recently, there has been growing interest in extending GPs to infer spatial derivatives, which are vital for analyzing spatial dynamics and detecting subtle changes in data patterns. Despite their utility, traditional GPs suffer from computational inefficiencies, due to the cubic scaling with the number of spatial locations. Fortunately, the computational challenge has spurred extensive research on scalable GP methods. However, these scalable approaches do not directly accommodate the inference of derivative processes. A straightforward approach is to use scalable GP models followed by finite-difference methods, known as the plug-in estimator. This approach, while intuitive, suffers from sensitivity to parameter choices, and the approximate gradient may not be a valid GP, leading to compromised inference. To bridge this gap, we introduce the Nearest-Neighbor Derivative Process (NNDP), an innovative framework that models the spatial processes and their derivatives within a single scalable GP model. NNDP significantly reduces the computational time complexity from $O(n^3)$ to $O(n)$, making it feasible for large datasets. We provide various theoretical supports for NNDP and demonstrate its effectiveness through extensive simulations and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02752v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawen Chen, Aritra Halder, Yun Li, Sudipto Banerjee, Didong Li</dc:creator>
    </item>
    <item>
      <title>An iterated $I$-projection procedure for solving the generalized minimum information checkerboard copula problem</title>
      <link>https://arxiv.org/abs/2509.02829</link>
      <description>arXiv:2509.02829v1 Announce Type: cross 
Abstract: The minimum information copula principle initially suggested in \cite{MeeBed97} is a maximum entropy-like approach for finding the least informative copula, if it exists, that satisfies a certain number of expectation constraints specified either from domain knowledge or the available data. We first propose a generalization of this principle allowing the inclusion of additional constraints fixing certain higher-order margins of the copula. We next show that the associated optimization problem has a unique solution under a natural condition. As the latter problem is intractable in general we consider its version with all the probability measures involved in its formulation replaced by checkerboard approximations. This amounts to attempting to solve a so-called discrete $I$-projection linear problem. We then exploit the seminal results of \cite{Csi75} to derive an iterated procedure for solving the latter and provide theoretical guarantees for its convergence. The usefulness of the procedure is finally illustrated via numerical experiments in dimensions up to four with substantially finer discretizations than those encountered in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02829v1</guid>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivan Kojadinovic, Tommaso Martini</dc:creator>
    </item>
    <item>
      <title>Markov Missing Graph: A Graphical Approach for Missing Data Imputation</title>
      <link>https://arxiv.org/abs/2509.03410</link>
      <description>arXiv:2509.03410v1 Announce Type: cross 
Abstract: We introduce the Markov missing graph (MMG), a novel framework that imputes missing data based on undirected graphs. MMG leverages conditional independence relationships to locally decompose the imputation model. To establish the identification, we introduce the Principle of Available Information (PAI), which guides the use of all relevant observed data. We then propose a flexible statistical learning paradigm, MMG Imputation Risk Minimization under PAI, that frames the imputation task as an empirical risk minimization problem. This framework is adaptable to various modeling choices. We develop theories of MMG, including the connection between MMG and Little's complete-case missing value assumption, recovery under missing completely at random, efficiency theory, and graph-related properties. We show the validity of our method with simulation studies and illustrate its application with a real-world Alzheimer's data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03410v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanjiao Yang, Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>Another look at Stein's method for Studentized nonlinear statistics with an application to U-statistics</title>
      <link>https://arxiv.org/abs/2301.02098</link>
      <description>arXiv:2301.02098v5 Announce Type: replace 
Abstract: We take another look at using Stein's method to establish uniform Berry-Esseen bounds for Studentized nonlinear statistics, highlighting variable censoring and an exponential randomized concentration inequality for a sum of censored variables as the essential tools to carry the arguments involved. As an important application, we prove a uniform Berry-Esseen bound for Studentized U-statistics in a form that exhibits the dependence on the degree of the kernel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.02098v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dennis Leung, Qi-Man Shao, Liqian Zhang</dc:creator>
    </item>
    <item>
      <title>The case for and against fixed step-size: Stochastic approximation algorithms in optimization and machine learning</title>
      <link>https://arxiv.org/abs/2309.02944</link>
      <description>arXiv:2309.02944v3 Announce Type: replace 
Abstract: Theory and application of stochastic approximation (SA) have become increasingly relevant due in part to applications in optimization and reinforcement learning. This paper takes a new look at SA with constant step-size $\alpha&gt;0$, defined by the recursion, $$\theta_{n+1} = \theta_{n}+ \alpha f(\theta_n,\Phi_{n+1})$$ in which $\theta_n\in\mathbb{R}^d$ and $\{\Phi_{n}\}$ is a Markov chain. The goal is to approximately solve root finding problem $\bar{f}(\theta^*)=0$, where $\bar{f}(\theta)=\mathbb{E}[f(\theta,\Phi)]$ and $\Phi$ has the steady-state distribution of $\{\Phi_{n}\}$.
  The following conclusions are obtained under an ergodicity assumption on the Markov chain, compatible assumptions on $f$, and for $\alpha&gt;0$ sufficiently small:
  $\textbf{1.}$ The pair process $\{(\theta_n,\Phi_n)\}$ is geometrically ergodic in a topological sense.
  $\textbf{2.}$ For every $1\le p\le 4$, there is a constant $b_p$ such that $\limsup_{n\to\infty}\mathbb{E}[\|\theta_n-\theta^*\|^p]\le b_p \alpha^{p/2}$ for each initial condition.
  $\textbf{3.}$ The Polyak-Ruppert-style averaged estimates $\theta^{\text{PR}}_n=n^{-1}\sum_{k=1}^{n}\theta_k$ converge to a limit $\theta^{\text{PR}}_\infty$ almost surely and in mean square, which satisfies $\theta^{\text{PR}}_\infty=\theta^*+\alpha \bar{\Upsilon}^*+O(\alpha^2)$ for an identified non-random $\bar{\Upsilon}^*\in\mathbb{R}^d$. Moreover, the covariance is approximately optimal: The limiting covariance matrix of $\theta{\text PR}_n$ is approximately minimal in a matricial sense.
  The two main take-aways for practitioners are application-dependent. It is argued that, in applications to optimization, constant gain algorithms may be preferable even when the objective has multiple local minima; while a vanishing gain algorithm is preferable in applications to reinforcement learning due to the presence of bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02944v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caio Kalil Lauand, Ioannis Kontoyiannis, Sean Meyn</dc:creator>
    </item>
    <item>
      <title>Likelihood distortion and Bayesian local robustness</title>
      <link>https://arxiv.org/abs/2405.15141</link>
      <description>arXiv:2405.15141v4 Announce Type: replace 
Abstract: Robust Bayesian analysis has been mainly devoted to detecting and measuring robustness w.r.t. the prior distribution. Many contributions in the literature aim to define suitable classes of priors which allow the computation of variations of quantities of interest while the prior changes within those classes. The literature has devoted much less attention to the robustness of Bayesian methods w.r.t. the likelihood function due to mathematical and computational complexity, and because it is often arguably considered a more objective choice compared to the prior. In this contribution, we propose a new approach to Bayesian local robustness, mainly focusing on robustness w.r.t. the likelihood function. Successively, we extend it to account for robustness w.r.t. the prior, as well as the prior and the likelihood jointly. This approach is based on the notion of distortion function introduced in the literature on risk theory. The novel robustness measure is a local sensitivity measure that turns out to be very tractable and easy to compute for several classes of distortion functions. Asymptotic properties are derived, and numerical experiments illustrate the theory and its applicability for modelling purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15141v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/25-BA1526</arxiv:DOI>
      <dc:creator>Antonio Di Noia, Fabrizio Ruggeri, Antonietta Mira</dc:creator>
    </item>
    <item>
      <title>k-Sample inference via Multimarginal Optimal Transport</title>
      <link>https://arxiv.org/abs/2501.05645</link>
      <description>arXiv:2501.05645v2 Announce Type: replace 
Abstract: This paper proposes a Multimarginal Optimal Transport ($MOT$) approach for simultaneously comparing $k\geq 2$ measures supported on finite subsets of $\mathbb{R}^d$, $d \geq 1$. We derive asymptotic distributions of the optimal value of the empirical $MOT$ program under the null hypothesis that all $k$ measures are same, and the alternative hypothesis that at least two measures are different. We use these results to construct the test of the null hypothesis and provide consistency and power guarantees of this $k$-sample test. We consistently estimate asymptotic distributions using bootstrap, and propose a low complexity linear program to approximate the test cut-off. We demonstrate the advantages of our approach on synthetic and real datasets, including the real data on cancers in the United States in 2004 - 2020.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05645v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia Kravtsova</dc:creator>
    </item>
    <item>
      <title>Learning sparse generalized linear models with binary outcomes via iterative hard thresholding</title>
      <link>https://arxiv.org/abs/2502.18393</link>
      <description>arXiv:2502.18393v2 Announce Type: replace 
Abstract: In statistics, generalized linear models (GLMs) are widely used for modeling data and can expressively capture potential nonlinear dependence of the model's outcomes on its covariates. Within the broad family of GLMs, those with binary outcomes, which include logistic and probit regressions, are motivated by common tasks such as binary classification with (possibly) non-separable data. In addition, in modern machine learning and statistics, data is often high-dimensional yet has a low intrinsic dimension, making sparsity constraints in models another reasonable consideration. In this work, we propose to use and analyze an iterative hard thresholding (projected gradient descent on the ReLU loss) algorithm, called binary iterative hard thresholding (BIHT), for parameter estimation in sparse GLMs with binary outcomes. We establish that BIHT is statistically efficient and converges to the correct solution for parameter estimation in a general class of sparse binary GLMs. Unlike many other methods for learning GLMs, including maximum likelihood estimation, generalized approximate message passing, and GLM-tron (Kakade et al. 2011; Bahmani et al. 2016), BIHT does not require knowledge of the GLM's link function, offering flexibility and generality in allowing the algorithm to learn arbitrary binary GLMs. As two applications, logistic and probit regression are additionally studied. In this regard, it is shown that in logistic regression, the algorithm is in fact statistically optimal in the sense that the order-wise sample complexity matches (up to logarithmic factors) the lower bound obtained previously. To the best of our knowledge, this is the first work achieving statistical optimality for logistic regression in all noise regimes with a computationally efficient algorithm. Moreover, for probit regression, our sample complexity is on the same order as that obtained for logistic regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18393v2</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Namiko Matsumoto, Arya Mazumdar</dc:creator>
    </item>
    <item>
      <title>Comment on "Worst-case Nonparametric Bounds for the Student T-statistic", arXiv:2508.13226</title>
      <link>https://arxiv.org/abs/2509.02410</link>
      <description>arXiv:2509.02410v2 Announce Type: replace 
Abstract: The main result in "Worst-case Nonparametric Bounds for the Student T-statistic'', arXiv:2508.13226 is incorrect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02410v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iosif Pinelis</dc:creator>
    </item>
    <item>
      <title>Geodesic slice sampling on the sphere</title>
      <link>https://arxiv.org/abs/2301.08056</link>
      <description>arXiv:2301.08056v4 Announce Type: replace-cross 
Abstract: Probability measures on the sphere form an important class of statistical models and are used, for example, in modeling directional data or shapes. Due to their widespread use, but also as an algorithmic building block, efficient sampling of distributions on the sphere is highly desirable. We propose a shrinkage based and an idealized geodesic slice sampling Markov chain, designed to generate approximate samples from distributions on the sphere. In particular, the shrinkage-based version of the algorithm can be implemented such that it runs efficiently in any dimension and has no tuning parameters. We verify reversibility and prove that under weak regularity conditions geodesic slice sampling is uniformly ergodic. Numerical experiments show that the proposed slice samplers achieve excellent mixing on challenging targets including the Bingham distribution and mixtures of von Mises-Fisher distributions. In these settings our approach outperforms standard samplers such as random-walk Metropolis-Hastings and Hamiltonian Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08056v4</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Habeck, Mareike Hasenpflug, Shantanu Kodgirwar, Daniel Rudolf</dc:creator>
    </item>
    <item>
      <title>Differential Equations for Gaussian Statistical Models with Rational Maximum Likelihood Estimator</title>
      <link>https://arxiv.org/abs/2304.12054</link>
      <description>arXiv:2304.12054v2 Announce Type: replace-cross 
Abstract: We study multivariate Gaussian statistical models whose maximum likelihood estimator (MLE) is a rational function of the observed data. We establish a one-to-one correspondence between such models and the solutions to a nonlinear first-order partial differential equation (PDE). Using our correspondence, we reinterpret familiar classes of models with rational MLE, such as directed (and decomposable undirected) Gaussian graphical models. We also find new models with rational MLE. For linear concentration models with rational MLE, we show that homaloidal polynomials from birational geometry lead to solutions to the PDE. We thus shed light on the problem of classifying Gaussian models with rational MLE by relating it to the open problem in birational geometry of classifying homaloidal polynomials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.12054v2</guid>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1137/23M1569228</arxiv:DOI>
      <arxiv:journal_reference>SIAM Journal on Applied Algebra and Geometry 8 (2024), 465--492</arxiv:journal_reference>
      <dc:creator>Carlos Am\'endola, Lukas Gustafsson, Kathl\'en Kohn, Orlando Marigliano, Anna Seigal</dc:creator>
    </item>
    <item>
      <title>Confounder selection via iterative graph expansion</title>
      <link>https://arxiv.org/abs/2309.06053</link>
      <description>arXiv:2309.06053v3 Announce Type: replace-cross 
Abstract: Confounder selection, namely choosing a set of covariates to control for confounding between a treatment and an outcome, is arguably the most important step in the design of an observational study. Previous methods, such as Pearl's back-door criterion, typically require pre-specifying a causal graph, which can often be difficult in practice. We propose an interactive procedure for confounder selection that does not require pre-specifying the graph or the set of observed variables. This procedure iteratively expands the causal graph by finding what we call "primary adjustment sets" for a pair of possibly confounded variables. This can be viewed as inverting a sequence of marginalizations of the underlying causal graph. Structural information in the form of primary adjustment sets is elicited from the user, bit by bit, until either a set of covariates is found to control for confounding or it can be determined that no such set exists. Other information, such as the causal relations between confounders, is not required by the procedure. We show that if the user correctly specifies the primary adjustment sets in every step, our procedure is both sound and complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06053v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F. Richard Guo, Qingyuan Zhao</dc:creator>
    </item>
    <item>
      <title>Sharp Bounds on the Variance of General Regression Adjustment in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2411.00191</link>
      <description>arXiv:2411.00191v2 Announce Type: replace-cross 
Abstract: A growing statistical literature focuses on causal inference in the context of experiments where the target of inference is the average treatment effect in a finite population and random assignment determines which subjects are allocated to one of the experimental conditions. In this framework, variances of average treatment effect estimators remain unidentified because they depend on the covariance between treated and untreated potential outcomes, which are never jointly observed. Conventional variance estimators are upwardly biased. Aronow, Green and Lee [Ann. Statist. 42(3): 850-871 (June 2014)] provide an estimator for the variance of the difference-in-means estimator that is asymptotically sharp. In practice, researchers often use some form of covariate adjustment, such as linear regression, when estimating the average treatment effect. Adapting propositions from empirical process theory, we extend the result in (Aronow et al., 2014), providing asymptotically sharp variance bounds for general regression adjustment. We apply these results to linear regression adjustment and show benefits both in a simulation and in three empirical applications drawn from different disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00191v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas M. Mikhaeil, Donald P. Green</dc:creator>
    </item>
    <item>
      <title>The Broader Landscape of Robustness in Algorithmic Statistics</title>
      <link>https://arxiv.org/abs/2412.02670</link>
      <description>arXiv:2412.02670v2 Announce Type: replace-cross 
Abstract: The last decade has seen a number of advances in computationally efficient algorithms for statistical methods subject to robustness constraints. An estimator may be robust in a number of different ways: to contamination of the dataset, to heavy-tailed data, or in the sense that it preserves privacy of the dataset. We survey recent results in these areas with a focus on the problem of mean estimation, drawing technical and conceptual connections between the various forms of robustness, showing that the same underlying algorithmic ideas lead to computationally efficient estimators in all these settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02670v2</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gautam Kamath</dc:creator>
    </item>
    <item>
      <title>Bringing closure to FDR control: beating the e-Benjamini-Hochberg procedure</title>
      <link>https://arxiv.org/abs/2504.11759</link>
      <description>arXiv:2504.11759v3 Announce Type: replace-cross 
Abstract: False discovery rate (FDR) has been a key metric for error control in multiple hypothesis testing, and many methods have developed for FDR control across a diverse cross-section of settings and applications. We develop a closure principle for all FDR controlling procedures, i.e., we provide a characterization based on e-values for all admissible FDR controlling procedures. A general version of this closure principle can recover any multiple testing error metric and allows one to choose the error metric post-hoc. We leverage this idea to formulate the closed eBH procedure, a (usually strict) improvement over the eBH procedure for FDR control when provided with e-values. This also yields a closed BY procedure that dominates the Benjamini-Yekutieli (BY) procedure for FDR control with arbitrarily dependent p-values, thus proving that the latter is inadmissibile. We demonstrate the practical performance of our new procedures in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11759v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Xu, Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
  </channel>
</rss>
