<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Feb 2026 02:44:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Synchronization of Unbalanced Dynamical Optimal Transport across Multiple Spaces</title>
      <link>https://arxiv.org/abs/2602.18725</link>
      <description>arXiv:2602.18725v1 Announce Type: new 
Abstract: Many biological systems are observed through heterogeneous modalities, requiring transport models that couple dynamics across spaces while allowing mass variation. To address this challenge, we introduce Unbalanced Synchronized Optimal Transport (UnSyncOT), a novel dynamical framework that synchronizes transport-reaction flows between spaces via either geometric embeddings (Monge type) or Markov kernels (Kantorovich type). For both cases we prove that UnSyncOT can be reduced to a single-space problem: the Monge model becomes a Benamou-Brenier problem with a metric-modified kinetic energy, and the Kantorovich model yields a nonlocal action induced by the synchronization operator, both of which fit within a dissipation-distance formulation. We also analyze the pure transport (Wasserstein) and pure reaction (Fisher-Rao) limits and derive structural properties. For the Kantorovich case we propose an approximate UnSyncOT by introducing a Hellinger-Kantorovich based trapezoidal time discretization of the secondary action for efficient computation. Finally we present staggered-grid discretizations and primal-dual solvers, validate the convergence, stability, and efficiency, and demonstrate coherent dynamics reconstructions across spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18725v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zixuan Cang, Jingfeng Wang, Xiaoqi Wei, Yanxiang Zhao</dc:creator>
    </item>
    <item>
      <title>Multiunit I.I.D. Prophet Inequalities via Extreme Value Asymptotics</title>
      <link>https://arxiv.org/abs/2602.18756</link>
      <description>arXiv:2602.18756v1 Announce Type: new 
Abstract: We study the i.i.d. $k$-selection prophet inequality problem, where a decision-maker sequentially observes $n$ independent nonnegative rewards and may accept at most $k$ of them without knowledge of future realizations. The objective is to maximize the expected total reward relative to that of a prophet who observes all rewards in advance. This problem captures the performance limits achievable in online resource allocation and underlies posted-price mechanisms in online marketplaces. We characterize the optimal welfare achievable relative to the prophet in terms of $k$ and the extreme value index of the reward distribution, in the asymptotic regime where the number of offers $n$ grows large. This optimal performance ratio turns out to be at least $1-\frac{\log k}{8k}[1+\epsilon]$ for any $\epsilon &gt; 0$ and sufficiently large $k$, improving upon the respective, tight $1 - \frac{1}{\sqrt{2\pi k}}$ guarantee of static-threshold algorithms. We additionally analyze the certainty-equivalent (CE) heuristic, a widely used online allocation algorithm known to yield optimal regret growth in $n$ when evaluated under the fluid scaling assumption. Even in the absence of the fluid scaling, the CE heuristics's performance improves with $k$ to eventually match the leading order terms of the optimal dynamic program's performance ratio. A finer analysis nevertheless reveals that regret can be divergent and large relative to the optimal dynamic program when $n/k \to \infty$. This highlights the sensitivity in viewing the CE heuristic's performance under the commonly adopted, though subjective, fluid scaling assumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18756v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jieming Kong, Karthyek Murthy</dc:creator>
    </item>
    <item>
      <title>Variational Sufficiency and Solution Stability in Optimization</title>
      <link>https://arxiv.org/abs/2602.18796</link>
      <description>arXiv:2602.18796v1 Announce Type: new 
Abstract: Variational stability, in the sense of local good behavior of optimal values and solutions in problems of optimization under shifts in parameters, is important not only for validating model robustness in practical applications but also for confidence of outcomes in the design of solution algorithms. Fundamental results are presented here about how such stability relates to a recently developed sufficient condition for local optimality called strong variational sufficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18796v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mat\'u\v{s} Benko, R. Tyrrell Rockafellar</dc:creator>
    </item>
    <item>
      <title>Limits of Convergence-Rate Control for Open-Weight Safety</title>
      <link>https://arxiv.org/abs/2602.18868</link>
      <description>arXiv:2602.18868v1 Announce Type: new 
Abstract: Open-weight foundation models can be fine-tuned for harmful purposes after release, yet no existing training resistance methods provide theoretical guarantees. Treating these interventions as convergence-rate control problems allows us to connect optimization speed to the spectral structure of model weights. We leverage this insight to develop a novel understanding of convergence rate control through spectral reparameterization and derive an algorithm, SpecDef, that can both provably and empirically slow first- and second-order optimization in non-adversarial settings. In adversarial settings, we establish a fundamental limit on a broad class of convergence rate control methods including our own: an attacker with sufficient knowledge can restore fast convergence at a linear increase in model size. In order to overcome this limitation, future works will need to investigate methods that are not equivalent to controlling convergence rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18868v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Domenic Rosati, Xijie Zeng, Hong Huang, Sebastian Dionicio, Subhabrata Majumdar, Frank Rudzicz, Hassan Sajjad</dc:creator>
    </item>
    <item>
      <title>Effective Second-Harmonic Generation Coefficient and C-eigenvalue of Nonlinear Susceptibility Tensors</title>
      <link>https://arxiv.org/abs/2602.18995</link>
      <description>arXiv:2602.18995v1 Announce Type: new 
Abstract: The effective second-harmonic generation (SHG) coefficient is a crucial data that quantifies the efficiency of transforming fundamental frequency light into its second harmonic. With the help of the symmetry of nonlinear optical susceptibility tensors, we mainly discuss the computability of such a effective SHG coefficient in uniaxial crystals. For one thing, the calculation of effective SHG coefficient is converted into the optimization models with some geometric constraints by means of the peculiarity of fundamental frequency light. Secondly, the number of variables of such maximum models are cutted in half to $2$ to calculate it easier, and a comparison between the effective SHG coefficient and C-eigenvalue of susceptibility tensor is given also. Finally, some examples of typical crystal classes are presented to verify the correctness and broader applicabilities of the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18995v1</guid>
      <category>math.OC</category>
      <category>physics.optics</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Die Xiao, Yisheng Song</dc:creator>
    </item>
    <item>
      <title>New reformulations for 0-1 quadratic programming problem using quadratic nonconvex reformulation techniques and valid inequalities</title>
      <link>https://arxiv.org/abs/2602.19051</link>
      <description>arXiv:2602.19051v1 Announce Type: new 
Abstract: It is well-known that the quadratic convex reformulation (QCR) technique can speed up some general-purpose solvers such as CPLEX and Gurobi. Recently, the method of quadratic nonconvex reformulation (QNR) was proposed, which provides an alternative way for accelerating a solver via reformulation technique. This paper proposes several new reformulations for 0-1 quadratic programming problems using the QNR technique. Such a technique provides more flexibility in adding nonconvex quadratic constraints into the problem formulation, so that some valid inequalities, such as the triangle inequalities, can be incorporated into the formulation to tighten the lower bound of the problem. We analyze the effects of the proposed reformulations on the lower bounds implemented in the solver, and propose some methods to maximize the McCormick relaxation bounds of the reformulations. Our numerical experiments compare the proposed reformulations with the existing quadratic convex reformulations, showing the effectiveness of the proposed reformulations on 0-1 quadratic programming problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19051v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cheng Lu, Yu Fei, Jing Zhou, Zhibin Deng, Guangtai Qu</dc:creator>
    </item>
    <item>
      <title>CBO algorithm with average drift and applications to portfolio optimization</title>
      <link>https://arxiv.org/abs/2602.19204</link>
      <description>arXiv:2602.19204v1 Announce Type: new 
Abstract: We propose a consensus based optimization algorithm with average drift (in short Ad-CBO) and provide a theoretical framework for it. In the theoretical analysis, we show that particle solutions to Ad-CBO converge to a global minimizer. In numerical simulations, we examine Ad-CBO's performance in optimizing static and dynamic objective functions. As a real-time application, we test the efficiency of Ad-CBO to find the optimal portfolio given stochastically evolving multi-asset prices in a financial market. The proposed Ad-CBO exhibits higher searching speed, lower tracking errors and regret bound than the CBO without stochastic diffusion</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19204v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyeong-Ohk Bae, Seung-Yeal Ha, Chanho Min, Jane Yoo, Jaeyoung Yoon</dc:creator>
    </item>
    <item>
      <title>Time-iteration methods for controllability</title>
      <link>https://arxiv.org/abs/2602.19272</link>
      <description>arXiv:2602.19272v1 Announce Type: new 
Abstract: These notes are based on a short course delivered at the Summer School EUR MINT 2025 "Control, Inverse Problems and Spectral Theory", held in June 2025 in Toulouse, France. The course presents three important strategies in control theory, formulated as time-iteration methods, where each time step brings the state of the system closer to the desired target.
  For linear PDEs, we survey the classical Lebeau-Robbiano method and its more recent developments. This method combines spectral inequalities and dissipation estimates to prove null controllability of a dissipative linear system.
  For nonlinear PDEs, we reinterpret the Liu-Takahashi-Tucsnak method, which establishes local controllability of a nonlinear system by analyzing the control cost of its linearization. We provide an easily applicable black-box formulation of their method.
  Finally, for nonlinear ODEs, we present the tangent vectors method, which establishes local exact controllability starting from approximately reachable directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19272v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Marbach</dc:creator>
    </item>
    <item>
      <title>Dynamic Repair and Maintenance of Heterogeneous Machines Dispersed on a Network: A Rollout Method for Online Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2602.19277</link>
      <description>arXiv:2602.19277v1 Announce Type: new 
Abstract: We consider a problem in which a single repairer is responsible for the maintenance and repair of a collection of machines, positioned at different locations on a network of nodes and edges. Machines deteriorate according to stochastic processes and incur increasing costs as they approach complete failure. The times needed for repairs to be performed, and the amounts of time needed for the repairer to switch between different machines, are random and machine-dependent. The problem is formulated as a Markov decision process (MDP) in which the objective is to minimize long-run average costs. We prove the equivalence of an alternative formulation based on rewards and use this to develop an index heuristic policy, which is shown to be optimal in certain special cases. We then use rollout-based reinforcement learning techniques to develop a novel online policy improvement (OPI) approach, which uses the index heuristic as a base policy and also as an insurance option at decision epochs where the best action cannot be selected with sufficient confidence. Results from extensive numerical experiments, involving randomly-generated network layouts and parameter values, show that the OPI heuristic is able to achieve close-to-optimal performance in fast-changing systems with state transitions occurring 100 times per second, suggesting that it is suitable for online implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19277v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dongnuan Tian, Rob Shone</dc:creator>
    </item>
    <item>
      <title>Smoothing-Enabled Randomized Stochastic Gradient Schemes for Solving Nonconvex Nonsmooth Potential Games under Uncertainty</title>
      <link>https://arxiv.org/abs/2602.19325</link>
      <description>arXiv:2602.19325v1 Announce Type: new 
Abstract: The state of the art in solving nonconvex nonsmooth games under uncertainty remains in its infancy. Existing studies primarily rely on stringent growth conditions or local convexity-like properties, making the development of alternative algorithms desirable. In this work, we study a class of stochastic $N$-player noncooperative games characterized by a potential function. We first consider the nonconvex smooth setting and develop a randomized stochastic gradient (RSG) scheme. The RSG scheme achieves the optimal sample complexity of $\mathcal{O}(N^{2}\epsilon^{-4})$ for reaching a point whose expected residual has norm at most $\epsilon$. Building on this result, we introduce a randomized smoothed RSG (RS-RSG) scheme for solving stochastic potential games afflicted by nonconvexity and nonsmoothness. We show that RS-RSG asymptotically converges to an equilibrium of the smoothed game with sample complexity $\mathcal{O}(L^{4}_{\max}n^{3/2}_{\max}N^{3}\eta^{-1}\epsilon^{-4})$, where $\eta&gt;0$ is the smoothing parameter. Under Lipschitz continuity of the Clarke subdifferentials, we show that the expected residual evaluated at the smoothed equilibrium is $\mathcal{O}(\eta^{2})$. In addition, we discuss the biased RSG and RS-RSG variants and demonstrate the effectiveness of the biased RS-RSG scheme on a class of stochastic potential hierarchical games where the exact lower-level solution is unavailable in finite time. Collectively, our results provide a new pathway that goes beyond classical conditions for solving stochastic nonconvex nonsmooth games. Some preliminary numerics are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19325v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuoyu Xiao</dc:creator>
    </item>
    <item>
      <title>Implementation of Time-Varying Controllers for a Nonholonomic Mobile Robot: Experimental Studies</title>
      <link>https://arxiv.org/abs/2602.19334</link>
      <description>arXiv:2602.19334v1 Announce Type: new 
Abstract: We consider a kinematic model of a wheeled mobile robot controlled by translational and angular velocities. For this class of nonholonomic systems, a family of time-varying feedback controllers was proposed in our previous works using gradient flow approximation techniques. In the present study, these controllers are implemented on a TurtleBot3 Burger (TB3) mobile robot to provide experimental validation of the stabilization problem with oscillating input signals. In addition, the admissibility problem of a gradient flow is investigated to justify the construction of a Lyapunov function candidate. The presented experimental results demonstrate the possibility of stabilizing the reference position of the robot using feedback controls with practically acceptable parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19334v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Zuyev, Victoria Grushkovskaya, Sebastian Eisner</dc:creator>
    </item>
    <item>
      <title>Enhancing network resilience through topological switching</title>
      <link>https://arxiv.org/abs/2602.19420</link>
      <description>arXiv:2602.19420v1 Announce Type: new 
Abstract: This work studies how to preemptively increase the resilience of a network by means of time-varying topological actuation. To do this, we focus on linear dynamical systems that are compatible with a given network, and consider policies that switch periodically between the given one and an alternative, topologically-compatible dynamics. In particular, we seek to solve design problems aimed at finding a) the optimal switching schedule between two preselected topologies, and b) an optimal topology and optimal switching schedule. By imposing periodicity, we first provide a metric of resilience in terms of the spectral abscissa of the averaged linear time-invariant dynamics. By restricting our policies to commutative networks, we then show how the optimal scheduling problem reduces to a convex optimization, providing a bound on the net resilience that can be achieved. After this, we find that the optimal, sparse commutative network to switch with is fully disconnected and allocates the spectral sum among the nodes of the network equally. We then impose additional restrictions on topology edge selection, which leads to a biconvex optimization for which certain matrix rank conditions guide the choice of weighting parameters to obtain desirable solutions. Finally, we provide two methods to solve this problem efficiently (based on a McCormick relaxation, and alternating minimization), and illustrate the results in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19420v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fei Chen, Jorge Cort\'es, Sonia Mart\'inez</dc:creator>
    </item>
    <item>
      <title>Optimal design with uncertainties: a risk-averse approach</title>
      <link>https://arxiv.org/abs/2602.19869</link>
      <description>arXiv:2602.19869v1 Announce Type: new 
Abstract: We study a class of stochastic optimal design problems for elliptic partial differential equations in divergence form, where the coefficients represent mixtures of two conducting materials. The objective is to minimize a generalized risk measure of the system response, incorporating uncertainty in the loading through probability distributions. We establish existence of relaxed optimal designs via homogenization theory and derive first-order stationarity conditions satisfied by the optima. Based on these conditions, we develop an optimality criteria algorithm for numerical computations. The stochastic component is treated using a truncated Karhunen--Lo\`eve expansion, allowing evaluation of the value-at-risk (VaR) and conditional value-at-risk (CVaR) contributions arising from the sensitivity analysis and featured in the algorithm. The method is illustrated for an example involving CVaR-based compliance minimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19869v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amal Alphonse, Petar Kun\v{s}tek, Marko Vrdoljak</dc:creator>
    </item>
    <item>
      <title>Local Second-Order Limit Dynamics of the Alternating Direction Method of Multipliers for Semidefinite Programming</title>
      <link>https://arxiv.org/abs/2602.20103</link>
      <description>arXiv:2602.20103v1 Announce Type: new 
Abstract: The alternating direction method of multipliers (ADMM) is widely used for solving large-scale semidefinite programs (SDPs), yet on instances with multiple primal--dual optimal solution pairs, it often enters prolonged slow-convergence regions where the Karush--Kuhn--Tucker (KKT) residuals nearly stall. To explain and predict the fine-grained dynamical behavior inside these regions, we develop a local second-order limit dynamics framework for ADMM near an arbitrary KKT point -- not necessarily the eventual limit point of the iterates. Assuming the existence of a strictly complementary primal--dual solution pair, we derive a second-order local expansion of the ADMM dynamics by leveraging a refined and simplified variational characterization of the (parabolic) second-order directional derivative of the PSD projection operator. This expansion reveals a closed convex cone of directions along which the local first-order update vanishes, and it induces a second-order limit map that governs the persistent drift after transient effects are filtered out. We characterize fundamental properties of this mapping, including its kernel, range, and continuity. A primal--dual decoupling further yields a clean scaling law for the effect of the penalty parameter in ADMM. We connect these properties to second-order dynamical features of ADMM, including fixed points, almost-invariant sets, and microscopic phases. Three empirical phenomena in slow-convergence regions are then explained or predicted: (i) angles between consecutive iterate differences are small yet nonzero, except for sparse spikes; (ii) primal and dual infeasibilities are insensitive to penalty-parameter updates; and (iii) iterates can be transiently trapped in a low-dimensional subspace for an extended period. Extensive numerical experiments on the Mittelmann dataset corroborate our theoretical predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20103v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shucheng Kang, Heng Yang</dc:creator>
    </item>
    <item>
      <title>On a discrete max-plus transportation problem</title>
      <link>https://arxiv.org/abs/2602.20136</link>
      <description>arXiv:2602.20136v1 Announce Type: new 
Abstract: We provide an explicit algorithm to solve the idempotent analogue of the discrete Monge-Kantorovich optimal mass transportation problem with the usual real number field replaced by the tropical (max-plus) semiring, in which addition is defined as the maximum and product is defined as usual addition, with minus infinity and zero playing the roles of additive and multiplicative identities. Such a problem may be naturally called tropical or "max-plus" optimal transportation problem. We show that the solutions to the latter, called the optimal tropical plans, may not correspond to perfect matchings even if the data (max-plus probability measures) have all weights equal to zero, in contrast with the classical discrete optimal transportation analogue, where perfect matching optimal plans in similar situations always exist. Nevertheless, in some randomized situation the existence of perfect matching optimal tropical plans may occur rather frequently. At last, we prove that the uniqueness of solutions of the optimal tropical transportation problem is quite rare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20136v1</guid>
      <category>math.OC</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Zapiski Nauchnykh Seminarov POMI, Volume 536, pages 54-78, 2024</arxiv:journal_reference>
      <dc:creator>Sergio Mayorga, Eugene Stepanov, Pedro Barrios</dc:creator>
    </item>
    <item>
      <title>Updating DMD Operators for Changes in Domain Properties</title>
      <link>https://arxiv.org/abs/2602.18441</link>
      <description>arXiv:2602.18441v1 Announce Type: cross 
Abstract: Fast and reliable surrogate models are critical for optimization, control and uncertainty analysis in geological carbon-storage projects, yet high-fidelity multiphase simulators remain too expensive. Dynamic Mode Decomposition (DMD) offers an attractive data-driven reduction framework, but its operators are trained for a single set of reservoir properties. When permeability or well location changes, conventional practice is to regenerate snapshots and retrain the surrogate, erasing most of the speed advantage. This work presents a lightweight alternative that updates an existing DMD or DMD-with-control model without incorporating new simulation data or retraining. Two complementary update strategies are introduced. For cases where permeability changes uniformly across the domain, the proposed updates adjust the models internal dynamics and control response to match the new flow timescale. When permeability varies in space, the approach modifies the spatial representation so that high-permeability zones are given greater influence on the models reduced basis. Numerical experiments demonstrate that the proposed updates recover plume migration and pressure build-up within three percent of a freshly trained surrogate yet execute hundreds of times faster than full retraining. These methods therefore enable real-time optimization and rapid what-if studies while preserving the physical fidelity demanded by carbon-storage workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18441v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.CE</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.21682.06083</arxiv:DOI>
      <dc:creator>Dimitrios Voulanas, Eduardo Gildin</dc:creator>
    </item>
    <item>
      <title>Stochastic Gradient Variational Inference with Price's Gradient Estimator from Bures-Wasserstein to Parameter Space</title>
      <link>https://arxiv.org/abs/2602.18718</link>
      <description>arXiv:2602.18718v1 Announce Type: cross 
Abstract: For approximating a target distribution given only its unnormalized log-density, stochastic gradient-based variational inference (VI) algorithms are a popular approach. For example, Wasserstein VI (WVI) and black-box VI (BBVI) perform gradient descent in measure space (Bures-Wasserstein space) and parameter space, respectively. Previously, for the Gaussian variational family, convergence guarantees for WVI have shown superiority over existing results for black-box VI with the reparametrization gradient, suggesting the measure space approach might provide some unique benefits. In this work, however, we close this gap by obtaining identical state-of-the-art iteration complexity guarantees for both. In particular, we identify that WVI's superiority stems from the specific gradient estimator it uses, which BBVI can also leverage with minor modifications. The estimator in question is usually associated with Price's theorem and utilizes second-order information (Hessians) of the target log-density. We will refer to this as Price's gradient. On the flip side, WVI can be made more widely applicable by using the reparametrization gradient, which requires only gradients of the log-density. We empirically demonstrate that the use of Price's gradient is the major source of performance improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18718v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyurae Kim, Qiang Fu, Yi-An Ma, Jacob R. Gardner, Trevor Campbell</dc:creator>
    </item>
    <item>
      <title>A unified duality framework for barotropic, quantum and Korteweg fluids</title>
      <link>https://arxiv.org/abs/2602.18917</link>
      <description>arXiv:2602.18917v1 Announce Type: cross 
Abstract: We investigate a dual variational formulation, in the spirit of Brenier, for several compressible fluid models: the compressible barotropic Euler system, the quantum Euler system, and the Euler-Korteweg system. We identify a unified abstract framework encompassing all three systems, which enables a simultaneous analysis. By introducing time-adaptive weights, we establish the consistency of the duality scheme on large time intervals. We prove the existence of variational dual solutions to the corresponding Cauchy problems for continuous, vacuum-free initial data in spaces of finite Radon measures, and establish the absence of a duality gap. As an application, we formulate and prove a 'Dafermos principle' for these models: no subsolution can dissipate the total entropy earlier or at a faster rate than the corresponding strong solution on its interval of existence. We also discuss connections between our abstract consistency result and Brenier's shock-free substitutes for entropy solutions of Burgers' equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18917v1</guid>
      <category>math.AP</category>
      <category>math-ph</category>
      <category>math.FA</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitry Vorotnikov</dc:creator>
    </item>
    <item>
      <title>Convex Analysis of Relaxation Dynamics in Chemical Reaction Networks and Generalized Gradient Flows</title>
      <link>https://arxiv.org/abs/2602.18932</link>
      <description>arXiv:2602.18932v1 Announce Type: cross 
Abstract: We obtain bounds on the Kullback--Leibler divergence to equilibrium for mass-action chemical reaction networks (CRNs) with equilibrium. The associated decay rates are characterized in terms of the singular values of the stoichiometric matrix, convexity parameters, and time-integrated activities via deformed-exponential-type functions. We further extend these bounds within a generalized gradient flow framework. We highlight the biological relevance of this framework: the resulting bounds apply to quasi-steady-state regimes, where long transients and plateau-like behavior are common and functionally important. We illustrate the framework using a catalytic CRN exhibiting plateaus, where the bounds capture slow relaxation induced by local convexity and provide a bound-based approach to quantifying relaxation in CRNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18932v1</guid>
      <category>q-bio.MN</category>
      <category>cond-mat.stat-mech</category>
      <category>math.DG</category>
      <category>math.OC</category>
      <category>physics.chem-ph</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keisuke Sugie, Dimitri Loutchko, Tetsuya J. Kobayashi</dc:creator>
    </item>
    <item>
      <title>Exponential Convergence of (Stochastic) Gradient Descent for Separable Logistic Regression</title>
      <link>https://arxiv.org/abs/2602.18946</link>
      <description>arXiv:2602.18946v1 Announce Type: cross 
Abstract: Gradient descent and stochastic gradient descent are central to modern machine learning, yet their behavior under large step sizes remains theoretically unclear. Recent work suggests that acceleration often arises near the edge of stability, where optimization trajectories become unstable and difficult to analyze. Existing results for separable logistic regression achieve faster convergence by explicitly leveraging such unstable regimes through constant or adaptive large step sizes. In this paper, we show that instability is not inherent to acceleration. We prove that gradient descent with a simple, non-adaptive increasing step-size schedule achieves exponential convergence for separable logistic regression under a margin condition, while remaining entirely within a stable optimization regime. The resulting method is anytime and does not require prior knowledge of the optimization horizon or target accuracy. We also establish exponential convergence of stochastic gradient descent using a lightweight adaptive step-size rule that avoids line search and specialized procedures, improving upon existing polynomial-rate guarantees. Together, our results demonstrate that carefully structured step-size growth alone suffices to obtain exponential acceleration for both gradient descent and stochastic gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18946v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sacchit Kale, Piyushi Manupriya, Pierre Marion, Francis bach, Anant Raj</dc:creator>
    </item>
    <item>
      <title>Implicit Bias and Convergence of Matrix Stochastic Mirror Descent</title>
      <link>https://arxiv.org/abs/2602.18997</link>
      <description>arXiv:2602.18997v1 Announce Type: cross 
Abstract: We investigate Stochastic Mirror Descent (SMD) with matrix parameters and vector-valued predictions, a framework relevant to multi-class classification and matrix completion problems. Focusing on the overparameterized regime, where the total number of parameters exceeds the number of training samples, we prove that SMD with matrix mirror functions $\psi(\cdot)$ converges exponentially to a global interpolator. Furthermore, we generalize classical implicit bias results of vector SMD by demonstrating that the matrix SMD algorithm converges to the unique solution minimizing the Bregman divergence induced by $\psi(\cdot)$ from initialization subject to interpolating the data. These findings reveal how matrix mirror maps dictate inductive bias in high-dimensional, multi-output problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18997v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danil Akhtiamov, Reza Ghane, Babak Hassibi</dc:creator>
    </item>
    <item>
      <title>Incremental Learning of Sparse Attention Patterns in Transformers</title>
      <link>https://arxiv.org/abs/2602.19143</link>
      <description>arXiv:2602.19143v1 Announce Type: cross 
Abstract: This paper introduces a high-order Markov chain task to investigate how transformers learn to integrate information from multiple past positions with varying statistical significance. We demonstrate that transformers learn this task incrementally: each stage is defined by the acquisition of specific information through sparse attention patterns. Notably, we identify a shift in learning dynamics from competitive, where heads converge on the most statistically dominant pattern, to cooperative, where heads specialize in distinct patterns. We model these dynamics using simplified differential equations that characterize the trajectory and prove stage-wise convergence results. Our analysis reveals that transformers ascend a complexity ladder by passing through simpler, misspecified hypothesis classes before reaching the full model class. We further show that early stopping acts as an implicit regularizer, biasing the model toward these simpler classes. These results provide a theoretical foundation for the emergence of staged learning and complex behaviors in transformers, offering insights into generalization for natural language processing and algorithmic reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19143v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>O\u{g}uz Kaan Y\"uksel, Rodrigo Alvarez Lucendo, Nicolas Flammarion</dc:creator>
    </item>
    <item>
      <title>Mean-field games with rough common noise I: the linear-quadratic case</title>
      <link>https://arxiv.org/abs/2602.19210</link>
      <description>arXiv:2602.19210v1 Announce Type: cross 
Abstract: Motivated by mean-field games (MFG) with common noise on the one hand and pathwise stochastic control theory on the other, we formulate here a linear-quadratic (LQ) MFG with rough common noise, along with a satisfactory well-posedness theory for the linear-quadratic case. A novel Volterra-type (or mild) formulation allows to keep technical (rough-stochastic) consideration to a minimum. We derive a characterization of the optimal state and optimal control through a rough forward-backward SDE (rough FBSDE), and provide an existence and uniqueness result under the usual assumptions. Our theory is accompanied by stability estimates with respect to initial data and common noise while we also establish continuity of what we call the It\^o-Lions-Lyons map for rough mean-field games. Finally, we discuss randomization of the rough common noise under appropriate conditions on the coefficients. When the latter is given by the Stratonovich lift of a Brownian motion independent of the idiosyncratic noise, we show that solutions of the rough LQ MFG coincide with those obtained by conditioning on the common noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19210v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter K. Friz, Ioannis Gasteratos, Ulrich Horst, Stefanos Theodorakopoulos</dc:creator>
    </item>
    <item>
      <title>Squirmers with arbitrary shape and slip: modeling, simulation, and optimization</title>
      <link>https://arxiv.org/abs/2602.19336</link>
      <description>arXiv:2602.19336v1 Announce Type: cross 
Abstract: We consider arbitrary-shaped microswimmers of spherical topology and propose a framework for expressing their slip velocity in terms of tangential basis functions defined on the boundary of the swimmer using the Helmholtz decomposition. Given a time-independent slip velocity profile, we show that the trajectory followed by the microswimmer is a circular helix. We derive analytical expressions for the translational and rotational velocities of a prolate spheroid swimmer in terms of its Helmholtz decomposition modes and explore the effect of aspect ratio on these rigid body velocities. Then, for a given arbitrary swimmer shape of spherical topology, we investigate which slip profile minimizes the total power loss. A partial minimization is performed in which the direction of net motion of the swimmer is prescribed, followed by a global optimization procedure in which the best net motion direction is determined. The optimization results suggest that the competition between linear and rotational optimal motion is linked to symmetries in the shape of the microswimmer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19336v1</guid>
      <category>physics.flu-dyn</category>
      <category>cond-mat.soft</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kausik Das, Hai Zhu, Marc Bonnet, Shravan Veerapaneni</dc:creator>
    </item>
    <item>
      <title>Self-Configurable Mesh-Networks for Scalable Distributed Submodular Bandit Optimization</title>
      <link>https://arxiv.org/abs/2602.19366</link>
      <description>arXiv:2602.19366v1 Announce Type: cross 
Abstract: We study how to scale distributed bandit submodular coordination under realistic communication constraints in bandwidth, data rate, and connectivity. We are motivated by multi-agent tasks of active situational awareness in unknown, partially-observable, and resource-limited environments, where the agents must coordinate through agent-to-agent communication. Our approach enables scalability by (i) limiting information relays to only one-hop communication and (ii) keeping inter-agent messages small, having each agent transmit only its own action information. Despite these information-access restrictions, our approach enables near-optimal action coordination by optimizing the agents' communication neighborhoods over time, through distributed online bandit optimization, subject to the agents' bandwidth constraints. Particularly, our approach enjoys an anytime suboptimality bound that is also strictly positive for arbitrary network topologies, even disconnected. To prove the bound, we define the Value of Coordination (VoC), an information-theoretic metric that quantifies for each agent the benefit of information access to its neighbors. We validate in simulations the scalability and near-optimality of our approach: it is observed to converge faster, outperform benchmarks for bandit submodular coordination, and can even outperform benchmarks that are privileged with a priori knowledge of the environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19366v1</guid>
      <category>eess.SY</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zirui Xu, Vasileios Tzoumas</dc:creator>
    </item>
    <item>
      <title>LEVDA: Latent Ensemble Variational Data Assimilation via Differentiable Dynamics</title>
      <link>https://arxiv.org/abs/2602.19406</link>
      <description>arXiv:2602.19406v1 Announce Type: cross 
Abstract: Long-range geophysical forecasts are fundamentally limited by chaotic dynamics and numerical errors. While data assimilation can mitigate these issues, classical variational smoothers require computationally expensive tangent-linear and adjoint models. Conversely, recent efficient latent filtering methods often enforce weak trajectory-level constraints and assume fixed observation grids. To bridge this gap, we propose Latent Ensemble Variational Data Assimilation (LEVDA), an ensemble-space variational smoother that operates in the low-dimensional latent space of a pretrained differentiable neural dynamics surrogate. By performing four-dimensional ensemble-variational (4DEnVar) optimization within an ensemble subspace, LEVDA jointly assimilates states and unknown parameters without the need for adjoint code or auxiliary observation-to-latent encoders. Leveraging the fully differentiable, continuous-in-time-and-space nature of the surrogate, LEVDA naturally accommodates highly irregular sampling at arbitrary spatiotemporal locations. Across three challenging geophysical benchmarks, LEVDA matches or outperforms state-of-the-art latent filtering baselines under severe observational sparsity while providing more reliable uncertainty quantification. Simultaneously, it achieves substantially improved assimilation accuracy and computational efficiency compared to full-state 4DEnVar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19406v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phillip Si, Peng Chen</dc:creator>
    </item>
    <item>
      <title>OptiRepair: Closed-Loop Diagnosis and Repair of Supply Chain Optimization Models with LLM Agents</title>
      <link>https://arxiv.org/abs/2602.19439</link>
      <description>arXiv:2602.19439v1 Announce Type: cross 
Abstract: Problem Definition. Supply chain optimization models frequently become infeasible because of modeling errors. Diagnosis and repair require scarce OR expertise: analysts must interpret solver diagnostics, trace root causes across echelons, and fix formulations without sacrificing operational soundness. Whether AI agents can perform this task remains untested.
  Methodology/Results. OptiRepair splits this task into a domain-agnostic feasibility phase (iterative IIS-guided repair of any LP) and a domain-specific validation phase (five rationality checks grounded in inventory theory). We test 22 API models from 7 families on 976 multi-echelon supply chain problems and train two 8B-parameter models using self-taught reasoning with solver-verified rewards. The trained models reach 81.7% Rational Recovery Rate (RRR) -- the fraction of problems resolved to both feasibility and operational rationality -- versus 42.2% for the best API model and 21.3% on average. The gap concentrates in Phase 1 repair: API models average 27.6% recovery rate versus 97.2% for trained models.
  Managerial Implications. Two gaps separate current AI from reliable model repair: solver interaction (API models restore only 27.6% of infeasible formulations) and operational rationale (roughly one in four feasible repairs violate supply chain theory). Each requires a different intervention: solver interaction responds to targeted training; operational rationale requires explicit specification as solver-verifiable checks. For organizations adopting AI in operational planning, formalizing what "rational" means in their context is the higher-return investment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19439v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruicheng Ao, David Simchi-Levi, Xinshang Wang</dc:creator>
    </item>
    <item>
      <title>Less is More: Convergence Benefits of Fewer Data Weight Updates over Longer Horizon</title>
      <link>https://arxiv.org/abs/2602.19510</link>
      <description>arXiv:2602.19510v1 Announce Type: cross 
Abstract: Data mixing--the strategic reweighting of training domains--is a critical component in training robust machine learning models. This problem is naturally formulated as a bilevel optimization task, where the outer loop optimizes domain weights to minimize validation loss, and the inner loop optimizes model parameters to minimize the weighted training loss. Classical bilevel optimization relies on hypergradients, which theoretically require the inner optimization to reach convergence. However, due to computational constraints, state-of-the-art methods use a finite, often small, number of inner update steps before updating the weights. The theoretical implications of this approximation are not well understood. In this work, we rigorously analyze the convergence behavior of data mixing with a finite number of inner steps $T$. We prove that the "greedy" practical approach of using $T=1$ can fail even in a simple quadratic example. Under a fixed parameter update budget $N$ and assuming the per-domain losses are strongly convex, we show that the optimal $T$ scales as $\Theta(\log N)$ (resp., $\Theta({(N \log N)}^{1/2})$) for the data mixing problem with access to full (resp., stochastic) gradients. We complement our theoretical results with proof-of-concept experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19510v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rudrajit Das, Neel Patel, Meisam Razaviyayn, Vahab Mirrokni</dc:creator>
    </item>
    <item>
      <title>Laplace Transforms of Stopping Times for Subordinator with Applications to Inventory Control</title>
      <link>https://arxiv.org/abs/2602.19625</link>
      <description>arXiv:2602.19625v1 Announce Type: cross 
Abstract: Intermittent demand fluctuations pose significant challenges in disaster logistics and medical supply systems. In this study, we formulate cumulative demand as a generalized L\'evy process composed of a drift term, Poisson jumps, and compound Poisson jumps, and analyze a continuous-time inventory model. The proposed framework provides a unified formulation that encompasses both drifted Poisson processes and drifted compound Poisson processes.
  From a mathematical perspective, we treat the reorder time as a first-passage problem of a subordinator and derive its Laplace transform via the Laplace exponent. In particular, for the drifted Poisson case, we obtain an explicit representation of the inverse Laplace exponent using the Lambert W function, which yields an analytic expression for the Laplace transform of the first-passage time. Furthermore, when the jump sizes follow exponential and Gamma distributions, we derive explicit formulas for the mean and variance of the reorder times, thereby clarifying the moment structure of first-passage times for generalized L\'evy demand processes.
  From an operations research perspective, we explicitly characterize the expected total cost over a finite time horizon based on the distribution of cumulative demand. This study presents an analytical framework that integrates first-passage theory of L\'evy processes with continuous-time inventory control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19625v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryoya Koide</dc:creator>
    </item>
    <item>
      <title>CACTO-BIC: Scalable Actor-Critic Learning via Biased Sampling and GPU-Accelerated Trajectory Optimization</title>
      <link>https://arxiv.org/abs/2602.19699</link>
      <description>arXiv:2602.19699v1 Announce Type: cross 
Abstract: Trajectory Optimization (TO) and Reinforcement Learning (RL) offer complementary strengths for solving optimal control problems. TO efficiently computes locally optimal solutions but can struggle with non-convexity, while RL is more robust to non-convexity at the cost of significantly higher computational demands. CACTO (Continuous Actor-Critic with Trajectory Optimization) was introduced to combine these advantages by learning a warm-start policy that guides the TO solver towards low-cost trajectories. However, scalability remains a key limitation, as increasing system complexity significantly raises the computational cost of TO. This work introduces CACTO-BIC to address these challenges. CACTO-BIC improves data efficiency by biasing initial-state sampling leveraging a property of the value function associated with locally optimal policies; moreover, it reduces computation time by exploiting GPU acceleration. Empirical evaluations show improved sample efficiency and faster computation compared to CACTO. Comparisons with PPO demonstrate that our approach can achieve similar solutions in less time. Finally, experiments on the AlienGO quadruped robot demonstrate that CACTO-BIC can scale to high-dimensional systems and is suitable for real-time applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19699v1</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Elisa Alboni, Pietro Noah Crestaz, Elias Fontanari, Andrea Del Prete</dc:creator>
    </item>
    <item>
      <title>Understanding the Curse of Unrolling</title>
      <link>https://arxiv.org/abs/2602.19733</link>
      <description>arXiv:2602.19733v1 Announce Type: cross 
Abstract: Algorithm unrolling is ubiquitous in machine learning, particularly in hyperparameter optimization and meta-learning, where Jacobians of solution mappings are computed by differentiating through iterative algorithms. Although unrolling is known to yield asymptotically correct Jacobians under suitable conditions, recent work has shown that the derivative iterates may initially diverge from the true Jacobian, a phenomenon known as the curse of unrolling. In this work, we provide a non-asymptotic analysis that explains the origin of this behavior and identifies the algorithmic factors that govern it. We show that truncating early iterations of the derivative computation mitigates the curse while simultaneously reducing memory requirements. Finally, we demonstrate that warm-starting in bilevel optimization naturally induces an implicit form of truncation, providing a practical remedy. Our theoretical findings are supported by numerical experiments on representative examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19733v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheheryar Mehmood, Florian Knoll, Peter Ochs</dc:creator>
    </item>
    <item>
      <title>Path-conditioned training: a principled way to rescale ReLU neural networks</title>
      <link>https://arxiv.org/abs/2602.19799</link>
      <description>arXiv:2602.19799v1 Announce Type: cross 
Abstract: Despite recent algorithmic advances, we still lack principled ways to leverage the well-documented rescaling symmetries in ReLU neural network parameters. While two properly rescaled weights implement the same function, the training dynamics can be dramatically different. To offer a fresh perspective on exploiting this phenomenon, we build on the recent path-lifting framework, which provides a compact factorization of ReLU networks. We introduce a geometrically motivated criterion to rescale neural network parameters which minimization leads to a conditioning strategy that aligns a kernel in the path-lifting space with a chosen reference. We derive an efficient algorithm to perform this alignment. In the context of random network initialization, we analyze how the architecture and the initialization scale jointly impact the output of the proposed method. Numerical experiments illustrate its potential to speed up training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19799v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Lebeurrier, Titouan Vayer, R\'emi Gribonval</dc:creator>
    </item>
    <item>
      <title>Resource-Aware Distributed Submodular Maximization: A Paradigm for Multi-Robot Decision-Making</title>
      <link>https://arxiv.org/abs/2204.07520</link>
      <description>arXiv:2204.07520v4 Announce Type: replace 
Abstract: Multi-robot decision-making is the process where multiple robots coordinate actions. In this paper, we aim for efficient and effective multi-robot decision-making despite the robots' limited on-board resources and the often resource-demanding complexity of their tasks. We introduce the first algorithm enabling the robots to choose with which few other robots to coordinate and provably balance the trade-off of centralized vs. decentralized coordination. Particularly, centralization favors globally near-optimal decision-making but at the cost of increased on-board resource requirements; whereas, decentralization favors minimal resource requirements but at a global suboptimality cost. All robots can thus afford our algorithm, irrespective of their resources. We are motivated by the future of autonomy that involves multiple robots coordinating actions to complete resource-demanding tasks, such as target tracking, area coverage, and monitoring. To provide closed-form guarantees, we focus on maximization problems involving monotone and 2nd-order submodular functions. To capture the cost of decentralization, we introduce the notion of Centralization Of Information among non-Neighbors (COIN). We validate our algorithm in simulated scenarios of image covering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.07520v4</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zirui Xu, Vasileios Tzoumas</dc:creator>
    </item>
    <item>
      <title>A quickest detection problem with false negatives</title>
      <link>https://arxiv.org/abs/2210.01844</link>
      <description>arXiv:2210.01844v3 Announce Type: replace 
Abstract: We formulate and solve a variant of the quickest detection problem which features false negatives. A standard Brownian motion acquires a drift at an independent exponential random time which is not directly observable. Based on the observation in continuous time of the sample path of the process, an optimizer must detect the drift as quickly as possible after it has appeared. The optimizer can inspect the system multiple times upon payment of a fixed cost per inspection. If a test is performed on the system before the drift has appeared then, naturally, the test will return a negative outcome. However, if a test is performed after the drift has appeared, then the test may fail to detect it and return a false negative with probability $\epsilon\in(0,1)$. The optimisation ends when the drift is eventually detected. The problem is formulated mathematically as an optimal multiple stopping problem, and it is shown to be equivalent to a recursive optimal stopping problem. Exploiting such connection and free boundary methods we find explicit formulae for the expected cost and the optimal strategy. We also show that when $\epsilon = 0$ our expected cost is an affine transformation of the one in Shiryaev's classical optimal detection problem with a rescaled model parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.01844v3</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiziano De Angelis, Jhanvi Garg, Quan Zhou</dc:creator>
    </item>
    <item>
      <title>A Mini-Batch Quasi-Newton Proximal Method for Constrained Total-Variation Nonlinear Image Reconstruction</title>
      <link>https://arxiv.org/abs/2307.02043</link>
      <description>arXiv:2307.02043v5 Announce Type: replace 
Abstract: Over the years, computational imaging with accurate nonlinear physical models has garnered considerable interest due to its ability to achieve high-quality reconstructions. However, using such nonlinear models for reconstruction is computationally demanding. A popular choice for solving the corresponding inverse problems is the accelerated stochastic proximal method (ASPM), with the caveat that each iteration is still expensive. To overcome this issue, we propose a mini-batch quasi-Newton proximal method (BQNPM) tailored to image reconstruction problems with constrained total variation regularization. Compared to ASPM, BQNPM requires fewer iterations to converge. Moreover, we propose an efficient approach to compute a weighted proximal mapping at a cost similar to that of the proximal mapping in ASPM. We also analyze the convergence of BQNPM in the nonconvex setting. We assess the performance of BQNPM on three-dimensional inverse-scattering problems with linear and nonlinear physical models. Our results on simulated and real data demonstrate the effectiveness and efficiency of BQNPM, while also validating our theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02043v5</guid>
      <category>math.OC</category>
      <category>eess.IV</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Hong, Thanh-an Pham, Irad Yavneh, Michael Unser</dc:creator>
    </item>
    <item>
      <title>Inexact Restoration via random models for unconstrained noisy optimization</title>
      <link>https://arxiv.org/abs/2402.12069</link>
      <description>arXiv:2402.12069v3 Announce Type: replace 
Abstract: We study the Inexact Restoration framework with random models for minimizing functions whose evaluation is subject to errors. We propose a constrained formulation that includes well-known stochastic problems and an algorithm applicable when the evaluation of both the function and its gradient is random and a specified accuracy of such evaluations is guaranteed with sufficiently high probability. The proposed algorithm combines the Inexact Restoration framework with a trust-region methodology based on random first-order models. We analyse the properties of the algorithm and provide the expected number of iterations performed to reach an approximate first-order optimality point. Numerical experiments show that the proposed algorithm compares well with a state-of-the-art competitor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12069v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benedetta Morini, Simone Rebegoldi</dc:creator>
    </item>
    <item>
      <title>Optimal Local Convergence Rates of Stochastic First-Order Methods under Local $\alpha$-PL</title>
      <link>https://arxiv.org/abs/2408.01839</link>
      <description>arXiv:2408.01839v2 Announce Type: replace 
Abstract: We study the local convergence rate of stochastic first-order methods under a local $\alpha$-Polyak-Lojasiewicz ($\alpha$-PL) condition in a neighborhood of a target connected component $\mathcal{M}$ of the local minimizer set. The parameter $\alpha \in [1,2]$ is the exponent of the gradient norm in the $\alpha$-PL inequality: $\alpha=2$ recovers the classical PL case, $\alpha=1$ corresponds to Holder-type error bounds, and intermediate values interpolate between these regimes. Our performance criterion is the number of oracle queries required to output $\hat{x}$ with $F(\hat{x})-l \le \varepsilon$, where $l := F(y)$ for any $y \in \mathcal{M}$. We work in a local regime where the algorithm is initialized near $\mathcal{M}$ and, with high probability, its iterates remain in that neighborhood. We establish a lower bound $\Omega(\varepsilon^{-2/\alpha})$ for all stochastic first-order methods in this regime, and we obtain a matching upper bound $\mathcal{O}(\varepsilon^{-2/\alpha})$ for $1 \le \alpha &lt; 2$ via a SARAH-type variance-reduced method with time-varying batch sizes and step sizes. In the convex setting, assuming a local $\alpha$-PL condition on the $\varepsilon$-sublevel set, we further show a complexity lower bound $\widetilde{\Omega}(\varepsilon^{-2/\alpha})$ for reaching an $\varepsilon$-global optimum, matching the $\varepsilon$-dependence of known accelerated stochastic subgradient methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01839v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saeed Masiha, Saber Salehkaleybar, Niao He, Negar Kiyavash, Patrick Thiran</dc:creator>
    </item>
    <item>
      <title>The $s$-Energy and Its Applications</title>
      <link>https://arxiv.org/abs/2410.22341</link>
      <description>arXiv:2410.22341v2 Announce Type: replace 
Abstract: Many multi-agent systems evolve by repeatedly updating each state to a weighted average of its neighbors, a process known as averaging dynamics, whose behavior becomes difficult to analyze when the interaction network varies over time. In recent years, the $s$-energy has emerged as a useful tool for bounding the convergence rates of such systems, complementing the classical techniques that rely on fixed graphs. We derive new bounds on the $s$-energy under minimal connectivity assumptions. As a consequence, we obtain convergence guarantees for several models of collective dynamics and resolve a number of open questions in the areas. Our results highlight the dependence of the $s$-energy on the connectivity of the underlying networks and use it to explain the exponential gap in the convergence rates of stationary and time-varying consensus systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22341v2</guid>
      <category>math.OC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernard Chazelle, Kritkorn Karntikoon</dc:creator>
    </item>
    <item>
      <title>Effectively Leveraging Momentum Terms in Stochastic Line Search Frameworks for Fast Optimization of Finite-Sum Problems</title>
      <link>https://arxiv.org/abs/2411.07102</link>
      <description>arXiv:2411.07102v4 Announce Type: replace 
Abstract: In this work, we address unconstrained finite-sum optimization problems, with particular focus on instances originating in large scale deep learning scenarios. Our main interest lies in the exploration of the relationship between recent line search approaches for stochastic optimization in the overparametrized regime and momentum directions. First, we point out that combining these two elements with computational benefits is not straightforward. To this aim, we propose a solution based on mini-batch persistency. We then introduce an algorithmic framework that exploits a mix of data persistency, conjugate-gradient type rules for the definition of the momentum parameter and stochastic line searches. The resulting algorithm provably possesses convergence properties under suitable assumptions and is empirically shown to outperform other popular methods from the literature, obtaining state-of-the-art results in both convex and nonconvex large scale training problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07102v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Lapucci, Davide Pucci</dc:creator>
    </item>
    <item>
      <title>Differential estimates for fast first-order multilevel nonconvex optimisation</title>
      <link>https://arxiv.org/abs/2412.01481</link>
      <description>arXiv:2412.01481v3 Announce Type: replace 
Abstract: With a view on bilevel and PDE-constrained optimisation, we develop iterative estimates $\widetilde{F'}(x^k)$ of $F'(x^k)$ for composite functions $F :=J \circ S$, where $S$ is the solution mapping of the inner optimisation problem or PDE. The idea is to form a single-loop method by interweaving updates of the iterate $x^k$ by an outer optimisation method, with updates of the estimate by single steps of standard optimisation methods and linear system solvers. When the inner methods satisfy simple tracking inequalities, the differential estimates can almost directly be employed in standard convergence proofs for general forward-backward type methods. We adapt those proofs to a general inexact setting in normed spaces, that, besides our differential estimates, also covers mismatched adjoints and unreachable optimality conditions in measure spaces. As a side product of these efforts, we provide improved convergence results for nonconvex Primal-Dual Proximal Splitting (PDPS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01481v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil Dizon, Tuomo Valkonen</dc:creator>
    </item>
    <item>
      <title>Neural Embedded Mixed-Integer Optimization for Location-Routing Problems</title>
      <link>https://arxiv.org/abs/2412.05665</link>
      <description>arXiv:2412.05665v2 Announce Type: replace 
Abstract: We present a novel framework that combines machine learning with mixed-integer optimization to solve the Capacitated Location-Routing Problem (CLRP). The CLRP is a classical NP-hard problem that integrates strategic facility location with operational vehicle routing decisions, aiming to minimize the sum of fixed and variable costs. The proposed method trains a neural network to approximate the optimal cost of a Capacitated Vehicle Routing Problem (CVRP) for serving any subset of customers from a candidate facility. Crucially, the neural network is trained on an independently generated dataset of CVRP instances from the literature, entirely separate from any CLRP test instances, thereby avoiding the overfitting and information leakage that can affect learning-based methods. The trained network is then embedded as a surrogate within a mixed-integer optimization model for location-allocation decisions, which is solved using off-the-shelf solvers, thus leveraging decades of advances in vehicle routing and the availability of mature solvers. Computational experiments across four benchmark sets demonstrate competitive solution quality compared to best-known solutions while providing computational speedups of 2x to 120x over state-of-the-art heuristics. After a one-time training cost of only 6 hours, per-instance solve times range from under a second to under five minutes, even for the largest instances with 600 customers and 30 depots, where the method achieves a 1% median gap compared to over four hours for leading heuristics. Our results demonstrate the value of routing cost approximations from the neural surrogate in informing high-quality location-allocation decisions. Our code and data are publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05665v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Waquar Kaleem, Doyoung Lee, Changhyun Kwon, Anirudh Subramanyam</dc:creator>
    </item>
    <item>
      <title>Revisiting Stochastic Proximal Point Methods: Generalized Smoothness and Similarity</title>
      <link>https://arxiv.org/abs/2502.03401</link>
      <description>arXiv:2502.03401v2 Announce Type: replace 
Abstract: The growing prevalence of nonsmooth optimization problems in machine learning has spurred significant interest in generalized smoothness assumptions. Among these, the (L0, L1)-smoothness assumption has emerged as one of the most prominent. While proximal methods are well-suited and effective for nonsmooth problems in deterministic settings, their stochastic counterparts remain underexplored. This work focuses on the stochastic proximal point method (SPPM), valued for its stability and minimal hyperparameter tuning-advantages often missing in stochastic gradient descent (SGD). We propose a novel phi-smoothness framework and provide a comprehensive analysis of SPPM without relying on traditional smoothness assumptions. Our results are highly general, encompassing existing findings as special cases. Furthermore, we examine SPPM under the widely adopted expected similarity assumption, thereby extending its applicability to a broader range of scenarios. Our theoretical contributions are illustrated and validated by practical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03401v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhirayr Tovmasyan, Grigory Malinovsky, Laurent Condat, Peter Richt\'arik</dc:creator>
    </item>
    <item>
      <title>Going from a Representative Agent to Counterfactuals in Combinatorial Choice</title>
      <link>https://arxiv.org/abs/2505.23546</link>
      <description>arXiv:2505.23546v3 Announce Type: replace 
Abstract: We study decision-making problems where data comprises points from a collection of binary polytopes, capturing aggregate information stemming from various combinatorial selection environments. We propose a nonparametric approach for counterfactual inference in this setting based on a representative agent model, where the available data is viewed as arising from maximizing separable concave utility functions over the respective binary polytopes. Our first contribution is to precisely characterize the selection probabilities representable under this model and show that verifying the consistency of any given aggregated selection dataset reduces to solving a polynomial-sized linear program. Building on this characterization, we develop a nonparametric method for counterfactual prediction. When data is inconsistent with the model, finding a best-fitting approximation for prediction reduces to solving a compact mixed-integer convex program. Numerical experiments based on synthetic data demonstrate the method's flexibility, predictive accuracy, and strong representational power even under model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23546v3</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yanqiu Ruan, Karthyek Murthy, Karthik Natarajan</dc:creator>
    </item>
    <item>
      <title>Multi-material structural optimization for additive manufacturing based on a phase field approach</title>
      <link>https://arxiv.org/abs/2508.02206</link>
      <description>arXiv:2508.02206v2 Announce Type: replace 
Abstract: A topology optimization problem in a phase field setting is considered to obtain rigid structures, which are resilient to external forces and constructable with additive manufacturing. Hence, large deformations of overhangs due to gravity shall be avoided during construction. The deformations depend on the stage of the construction and are modelled by linear elasticity equations on growing domains with height-dependent stress tensors and forces. Herewith, possible hardening effects can be included. Analytical results concerning the existence of minimizers and the differentiability of the reduced cost functional are presented in case of a finite number of construction layers. By proving Korn's inequality with a constant independent of the height, it is shown that the cost functional, formulated continuously in height, is well-defined. The problem is numerically solved using a projected gradient type method in function space, for which applicability is shown. Second-order information can be included by adapting the underlying inner product in every iteration. Additional adjustments enhancing the solver's performance, such as a nested procedure and subsystem solver specifcations, are stated. Numerical evidence is provided that for all discretization level and also for any number of construction layers, the iteration numbers stay roughly constant. The benefits of the nested procedure as well as of the inclusion of second order information are illustrated. Furthermore, the choice of weights for the penalization of overhangs is discussed. For various problem settings, results are presented for one or two materials and void in two as well as in three dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02206v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luise Blank, Maximilian Urmann</dc:creator>
    </item>
    <item>
      <title>Non-negative polynomials without hyperbolic certificates of non-negativity</title>
      <link>https://arxiv.org/abs/2508.04027</link>
      <description>arXiv:2508.04027v3 Announce Type: replace 
Abstract: In this paper we study the relationship between the set of all non-negative multivariate homogeneous polynomials and those, which we call hyperwrons, whose non-negativity can be deduced from an identity involving the Wronskians of hyperbolic polynomials. We give a sufficient condition on positive integers $m$ and $2y$ such that there are non-negative polynomials of degree $2y$ in $m$ variables that are not hyperwrons. Furthermore, we give an explicit example of a non-negative quartic form that is not a sum of hyperwrons. We partially extend our results to hyperzouts, which are polynomials whose non-negativity can be deduced from an identity involving the B\'ezoutians of hyperbolic polynomials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04027v3</guid>
      <category>math.OC</category>
      <category>math.AG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H. L. Brian Ng, James Saunderson</dc:creator>
    </item>
    <item>
      <title>Exact and Heuristic Algorithms for Constrained Biclustering</title>
      <link>https://arxiv.org/abs/2508.05493</link>
      <description>arXiv:2508.05493v2 Announce Type: replace 
Abstract: Biclustering, also known as co-clustering or two-way clustering, simultaneously partitions the rows and columns of a data matrix to reveal submatrices with coherent patterns. Incorporating background knowledge into clustering to enhance solution quality and interpretability has attracted growing interest in mathematical optimization and machine learning research. Extending this paradigm to biclustering enables prior information to guide the joint grouping of rows and columns. We study constrained biclustering with pairwise constraints, namely must-link and cannot-link constraints, which specify whether objects should belong to the same or different biclusters. As a model problem, we address the constrained version of the k-densest disjoint biclique problem, which aims to identify k disjoint complete bipartite subgraphs (called bicliques) in a weighted complete bipartite graph, maximizing the total density while satisfying pairwise constraints. We propose both exact and heuristic algorithms. The exact approach is a tailored branch-and-cut algorithm based on a low-dimensional semidefinite programming (SDP) relaxation, strengthened with valid inequalities and solved in a cutting-plane fashion. Exploiting integer programming tools, a rounding scheme converts SDP solutions into feasible biclusterings at each node. For large-scale instances, we introduce an efficient heuristic based on the low-rank factorization of the SDP. The resulting nonlinear optimization problem is tackled with an augmented Lagrangian method, where the subproblem is solved by decomposition through a block-coordinate projected gradient algorithm. Extensive experiments on synthetic and real-world datasets show that the exact method significantly outperforms general-purpose solvers, while the heuristic achieves high-quality solutions efficiently on large instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05493v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10589-026-00764-6</arxiv:DOI>
      <arxiv:journal_reference>Computational Optimization and Applications, 2026</arxiv:journal_reference>
      <dc:creator>Antonio M. Sudoso</dc:creator>
    </item>
    <item>
      <title>A Two-fold Randomization Framework for Impulse Control Problems</title>
      <link>https://arxiv.org/abs/2509.12018</link>
      <description>arXiv:2509.12018v5 Announce Type: replace 
Abstract: We propose and analyze a randomization scheme for a general class of impulse control problems. The solution to this randomized problem is characterized as the fixed point of a compound operator which consists of a regularized nonlocal operator and a regularized stopping operator. This approach allows us to derive a semi-linear Hamilton-Jacobi-Bellman (HJB) equation. Through an equivalent randomization scheme with a Poisson compound measure, we establish a verification theorem that implies the uniqueness of the solution. Via an iterative approach, we prove the existence of the solution. The existence-and-uniqueness result ensures the randomized problem is well-defined. We then demonstrate that our randomized impulse control problem converges to its classical counterpart as the randomization parameter $\pmb \lambda$ vanishes. This convergence, combined with the value function's $C^{2,\alpha}_{loc}$ regularity, confirms our framework provides a robust approximation and a foundation for developing learning algorithms. Under this framework, we propose an offline reinforcement learning (RL) algorithm. Its policy improvement step is naturally derived from the iterative approach from the existence proof, which enjoys a geometric convergence rate. We implement a model-free version of the algorithm and numerically demonstrate its effectiveness using a widely-studied example. The results show that our RL algorithm can learn the randomized solution, which accurately approximates its classical counterpart. A sensitivity analysis with respect to the volatility parameter $\sigma$ in the state process effectively demonstrates the exploration-exploitation tradeoff.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12018v5</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyang Cao, Yuchao Dong, Zhouhao Yang</dc:creator>
    </item>
    <item>
      <title>Closed-loop solvability of delayed control problems: A stochastic Volterra system approach</title>
      <link>https://arxiv.org/abs/2510.02674</link>
      <description>arXiv:2510.02674v3 Announce Type: replace 
Abstract: A general and new stochastic linear quadratic optimal control problem is studied, where the coefficients are allowed to be time-varying, and both state delay and control delay can appear simultaneously in the state equation and the cost functional. The closed-loop outcome control of this delayed problem is given by a new Riccati system whose solvability is carefully established. To this end, a novel method is introduced to transform the delayed problem into a control problem driven by a stochastic Volterra integral system without delay. This method offers several advantages: it bypasses the difficulty of decoupling the forward delayed state equation and the backward anticipated adjoint equation, avoids the introduction of infinite-dimensional spaces and unbounded control operators, and ensures that the closed-loop outcome control depends only on past state and control, without relying on future state or complex conditional expectation calculations. Finally, several particular important stochastic systems are discussed. It is found that the model can cover a class of stochastic integro-differential systems, whose closed-loop solvability has not been available before.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02674v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weijun Meng, Tianxiao Wang, Ji-Feng Zhang</dc:creator>
    </item>
    <item>
      <title>Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences</title>
      <link>https://arxiv.org/abs/2512.13123</link>
      <description>arXiv:2512.13123v5 Announce Type: replace 
Abstract: The problem of stopping stochastic gradient descent (SGD) in an online manner, based solely on the observed trajectory, is a challenging theoretical problem with significant consequences for applications. While SGD is routinely monitored as it runs, the classical theory of SGD provides guarantees only at pre-specified iteration horizons and offers no valid way to decide, based on the observed trajectory, when further computation is justified. We address this longstanding gap by developing anytime-valid confidence sequences for stochastic gradient methods, which remain valid under continuous monitoring and directly induce statistically valid, trajectory-dependent stopping rules: stop as soon as the current upper confidence bound on an appropriate performance measure falls below a user-specified tolerance. The confidence sequences are constructed using nonnegative supermartingales, are time-uniform, and depend only on observable quantities along the SGD trajectory, without requiring prior knowledge of the optimization horizon. In convex optimization, this yields anytime-valid certificates for weighted suboptimality of projected SGD under general stepsize schedules, without assuming smoothness or strong convexity. In nonconvex optimization, it yields time-uniform certificates for weighted first-order stationarity under smoothness assumptions. We further characterize the stopping-time complexity of the resulting stopping rules under standard stepsize schedules. To the best of our knowledge, this is the first framework that provides statistically valid, time-uniform stopping rules for SGD across both convex and nonconvex settings based solely on its observed trajectory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13123v5</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liviu Aolaritei, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Complete Characterizations of Well-Posedness in Parametric Composite Optimization</title>
      <link>https://arxiv.org/abs/2512.14124</link>
      <description>arXiv:2512.14124v3 Announce Type: replace 
Abstract: This paper characterizes the well-posedness of Karush-Kuhn-Tucker system for perturbed composite optimization. Using the parabolic regularity, we introduce a novel second-order variational function, shown to be the pivotal object governing second-order behavior. This foundational result yields the strong second-order sufficient condition introduced here for the general class of composite optimization problems to naturally extend the classical second-order sufficient condition in nonlinear programming. Then we obtain several equivalent characterizations of the second-order qualification condition (SOQC) and highlight its equivalence to the constraint nondegeneracy condition under the $\mathcal{C}^{2}$-cone reducibility assumption. These insights lead us to multiple equivalent conditions for the major Lipschitz-like/Aubin property of KKT systems, including SOQC combined with the new second-order subdifferential condition and SOQC combined with tilt stability of local minimizers. Under the $\mathcal{C}^{2}$-cone reducibility, we settle the long-standing question by proving the equivalence between the Aubin property of KKT systems around local minimizers and the classical notion of strong regularity under some additional assumptions. Finally, we demonstrate that the Lipschitz-like property is equivalent to the nonsingularity of the generalized Jacobian associated with the KKT system under a certain verifiable assumption. These results provide a unified and rigorous framework for analyzing stability and sensitivity of solutions to composite optimization problems, as well as for the design and justification of numerical algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14124v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Boris S. Mordukhovich, Peipei Tang, Chengjing Wang</dc:creator>
    </item>
    <item>
      <title>Stochastic Galerkin Method and Hierarchical Preconditioning for PDE-constrained Optimization</title>
      <link>https://arxiv.org/abs/2512.23804</link>
      <description>arXiv:2512.23804v2 Announce Type: replace 
Abstract: We develop efficient hierarchical preconditioners for optimal control problems governed by partial differential equations with uncertain coefficients. Adopting a discretize-then-optimize framework that integrates finite element discretization, stochastic Galerkin projection, and advanced time-discretization schemes, the approach addresses challenges of scaling large and ill-conditioned linear systems arising in uncertainty quantification. By exploiting sparsity of linear systems in stochastic Galerkin method, we formulate hierarchical preconditioners based on truncated stochastic expansion that strike an effective balance between computational cost and preconditioning quality. Numerical experiments demonstrate that the proposed preconditioners significantly accelerate the convergence of iterative solvers compared to existing methods, providing robust and efficient solvers for both steady-state and time-dependent optimal control problems under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23804v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhendong Li, Akwum Onwunta, Bed\v{r}ich Soused\'ik</dc:creator>
    </item>
    <item>
      <title>Decentralized Optimization over Time-Varying Row-Stochastic Digraphs</title>
      <link>https://arxiv.org/abs/2512.24483</link>
      <description>arXiv:2512.24483v3 Announce Type: replace 
Abstract: Decentralized optimization over directed graphs is essential for applications such as robotic swarms, sensor networks, and distributed learning. In many practical scenarios, the underlying network takes the form of a Time-Varying Broadcast Network (TVBN), where only row-stochastic mixing matrices can be constructed due to the unavailability of out-degree information. Achieving exact convergence for decentralized optimization over TVBNs has remained a long-standing open problem, as the limiting distribution of time-varying row-stochastic mixing matrices depends on unpredictable future graph realizations, rendering standard bias-correction techniques infeasible. This paper develops the first decentralized optimization algorithm that achieves exact convergence using only time-varying row-stochastic matrices. We first propose PULM (Pull-with-Memory), a gossip protocol that achieves average consensus with exponential convergence by alternating between row-stochastic mixing and local adjustment steps. Building on PULM, we develop PULM-DGD, which converges to a stationary solution at a rate of $\mathcal{O}(\ln(T)/T)$ for smooth nonconvex objectives, where $T$ denotes the communication round. Our results significantly broaden the applicability of decentralized optimization to highly dynamic communication environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24483v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liyuan Liang, Yilong Song, Kun Yuan</dc:creator>
    </item>
    <item>
      <title>Minimal Perimeter Triangle in Nonconvex Quadrilateral:Generalized Fagnano Problem</title>
      <link>https://arxiv.org/abs/2601.11552</link>
      <description>arXiv:2601.11552v2 Announce Type: replace 
Abstract: In 1775, Fagnano introduced the following geometric optimization problem: inscribe a triangle of minimal perimeter in a given acute-angled triangle. A widely accessible solution is provided by the Hungarian mathematician L. Fejer in 1900. This paper presents a specific generalization of the classical Fagnano problem, which states that given a nonconvex quadrilateral (having one reflex angle and others are acute angles), find a triangle of minimal perimeter with exactly one vertex on each of the sides that do not form reflex angle, and the third vertex lies on either of the sides forming the reflex angle. We provide its geometric solution. Additionally, we establish an upper bound for the classic Fagnano problem, demonstrating that the minimal perimeter of the triangle inscribed in a given acute-angled triangle cannot exceed twice the length of any of its sides.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11552v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Triloki Nath, Manohar Choudhary</dc:creator>
    </item>
    <item>
      <title>Optimal Methods for Unknown Piecewise Smooth Problems I: Convex Optimization</title>
      <link>https://arxiv.org/abs/2601.14680</link>
      <description>arXiv:2601.14680v2 Announce Type: replace 
Abstract: We introduce an optimal and nearly parameter-free algorithm for minimizing piecewise smooth (PWS) convex functions under the quadratic growth (QG) condition, where the locations and structure of the smooth regions are entirely \textit{unknown}. Our algorithm, \apex{} (Accelerated Prox-Level method for Exploring Piecewise Smoothness), is an accelerated bundle-level method designed to adaptively exploit the underlying PWS structure. APEX enjoys optimal theoretical guarantees, achieving a tight oracle complexity bound that matches the lower bound established in this work for convex PWS optimization. Furthermore, APEX generates a verifiable and accurate termination certificate, enabling a robust, almost parameter-free implementation. To the best of our knowledge, APEX is the first algorithm to simultaneously achieve the optimal convergence rate for PWS optimization and provide certificate guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14680v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenwei Lin, Zhe Zhang</dc:creator>
    </item>
    <item>
      <title>On the Convergence of HalpernSGD</title>
      <link>https://arxiv.org/abs/2601.18906</link>
      <description>arXiv:2601.18906v2 Announce Type: replace 
Abstract: HalpernSGD is a gradient-type optimizer obtained by combining the classical Halpern fixed-point iteration with a stochastic gradient step. While its empirical advantages have already been observed in \cite{colao2025optimizer,foglia2024halpernsgd}, this paper provides a theoretical analysis of the method. Assuming a convex $L$-smooth objective and standard stochastic-approximation conditions, we prove almost sure convergence of the iterates to a minimizer, characterized as the metric projection of the anchor onto the solution set. These results fill a gap in the literature on HalpernSGD and lay the groundwork for future extensions to adaptive schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18906v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vittorio Colao, Katherine Rossella Foglia</dc:creator>
    </item>
    <item>
      <title>Sum of Squares Rank of Biquadratic Forms and The Zarankiewicz Number</title>
      <link>https://arxiv.org/abs/2602.07844</link>
      <description>arXiv:2602.07844v2 Announce Type: replace 
Abstract: Denote the maximum sos rank of $m \times n$ sum of squares (SOS) biquadratic forms by $BSR(m, n)$. In this paper, we show that $BSR(m, n) \ge z(m, n)$ and conjecture that $BSR(m, n) = z(m, n)$, where $z(m, n)$ is the Zarankiewicz number. Our result coincides with the existing results for $m = 2$, $n = 2$, and $m = n = 3$, and is superior to other previously known lower bounds. Our result also connects graph theory and SOS polynomial theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07844v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunfeng Cui, Liqun Qi, Yi Xu</dc:creator>
    </item>
    <item>
      <title>First-order optimality conditions for non-commutative optimization problems</title>
      <link>https://arxiv.org/abs/2311.18707</link>
      <description>arXiv:2311.18707v5 Announce Type: replace-cross 
Abstract: We consider the problem of optimizing the state average of a polynomial of non-commuting variables, over all states and operators satisfying a number of polynomial constraints, and over all Hilbert spaces where such states and operators are defined. Such non-commutative polynomial optimization (NPO) problems are routinely solved through hierarchies of semidefinite programming (SDP) relaxations. By formulating the general NPO problem in Lagrangian terms, we heuristically derive first-order optimality conditions via small variations in the problem variables. Although the derivation is not rigorous, it gives rise to two types of optimality conditions -- state and operator -- which are rigorously analyzed in the paper. Both types of conditions can be enforced through additional positive semidefinite constraints in the SDP hierarchies. State optimality conditions are shown to be satisfied by all NPO problems. For NPO problems with optimal solutions (such as, e.g., Archimedean ones) they allow enforcing a new type of constraints: namely, restricting the optimization over states to the set of common ground states of an arbitrary number of operators. Operator optimality conditions are the non-commutative analogs of the Karush--Kuhn--Tucker (KKT) conditions, which are known to hold in many classical optimization problems. In this regard, we prove that a weak form of operator optimality holds for all NPO problems; stronger versions require the problem constraints to satisfy some qualification criterion, just like in the classical case (e.g.: Mangasarian--Fromovitz constraint qualification). We test the power of the new optimality conditions by computing local properties of ground states of many-body spin systems and the maximum quantum violation of Bell inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18707v5</guid>
      <category>quant-ph</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mateus Ara\'ujo, Igor Klep, Andrew J. P. Garner, Tam\'as V\'ertesi, Miguel Navascu\'es</dc:creator>
    </item>
    <item>
      <title>Semi-on-Demand Hybrid Transit Route Design with Shared Autonomous Mobility Services</title>
      <link>https://arxiv.org/abs/2403.15804</link>
      <description>arXiv:2403.15804v4 Announce Type: replace-cross 
Abstract: Shared Autonomous Vehicles (SAVs) enable transit agencies to design more agile and responsive services at lower operating costs. This study designs and evaluates a semi-on-demand hybrid route directional service in the public transit network, offering on-demand flexible route service in low-density areas and fixed route service in higher-density areas. We develop analytically tractable cost expressions that capture access, waiting, and riding costs for users, and distance-based operating and time-based vehicle costs for operators. Two formulations are presented for strategic and tactical decisions in flexible route portion, fleet size, headway, and vehicle size optimization, enabling the determination of route types between fixed, hybrid, and flexible routes based on demand, cost, and operational parameters. Analytical results demonstrate that the lower operating costs of SAVs favor more flexible route services. The practical applications and benefits of semi-on-demand feeders are presented with numerical examples and a large-scale case study in the Chicago metropolitan area, USA. Findings reveal scenarios in which flexible route portions serving passengers located further away reduce total costs, particularly user costs, whereas higher demand densities favor more traditional line-based operations. Current cost forecasts suggest smaller vehicles with fully flexible routes are optimal, but operating constraints or higher operating costs would favor larger vehicles with hybrid routes. The study provides an analytical tool to design SAVs as directional services and transit feeders, and tractable continuous approximation formulations for planning and research in transit network design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15804v4</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.trc.2026.105578</arxiv:DOI>
      <arxiv:journal_reference>Transportation Research Part C: Emerging Technologies, 2026</arxiv:journal_reference>
      <dc:creator>Max T. M. Ng, Florian Dandl, Hani S. Mahmassani, Klaus Bogenberger</dc:creator>
    </item>
    <item>
      <title>Remote State Estimation over Unreliable Channels with Unreliable Feedback: Strategies and Limits</title>
      <link>https://arxiv.org/abs/2501.13192</link>
      <description>arXiv:2501.13192v2 Announce Type: replace-cross 
Abstract: In this article, we establish a comprehensive theoretical framework for remote estimation in a networked system composed of a source that is observed by a sensor, a remote monitor that needs to estimate the state of the source in real time, and a communication channel that connects the source to the monitor. The source is a partially observable dynamical process, and the communication channel is a packet-erasure channel with feedback. We consider a novel communication model that captures implicit information. Our main objective is to identify the optimal strategies and the fundamental performance limits of the underlying system in the sense of a causal tradeoff between the packet rate and the mean square error when both forward and backward channels are unreliable. We characterise an optimal coding policy profile consisting of a scheduling policy for an encoder and an estimation policy for a decoder, collocated with the source and the monitor, respectively. We derive the recursive equations that must be solved online by the encoder and the decoder. In addition, we prove that the value function, originally defined over an expanding information set, admits a lower-dimensional representation depending only on two variables. We discuss the structural properties of the optimal policies, and analyse the computational complexity of an algorithm proposed for their computation. We then examine a range of special cases derived from our main theoretical results. We complement the theoretical results with a numerical analysis, and compare the performance of different remote estimation tasks in various operating regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13192v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Touraj Soleymani, Mohamad Assaad, John S. Baras</dc:creator>
    </item>
    <item>
      <title>$O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation</title>
      <link>https://arxiv.org/abs/2504.19375</link>
      <description>arXiv:2504.19375v2 Announce Type: replace-cross 
Abstract: Two-time-scale stochastic approximation (SA) is an algorithm with coupled iterations which has found broad applications in reinforcement learning, optimization and game control. In this work, we derive mean squared error bounds for non-linear two-time-scale iterations with contractive mappings. In the setting where both stepsizes are order $\Theta(1/k)$, commonly referred to as single time-scale SA with multiple coupled sequences, we obtain the first $O(1/k)$ rate without imposing additional smoothness assumptions. In the setting with true time-scale separation, the previous best bound was $O(1/k^{2/3})$. We improve this to $O(1/k^a)$ for any $a&lt;1$ approaching the optimal $O(1/k)$ rate. The key step in our analysis involves rewriting the original iteration in terms of an averaged noise sequence whose variance decays sufficiently fast. Additionally, we use an induction-based approach to show that the iterates are bounded in expectation. Our results apply to Polyak averaging, as well as to algorithms from reinforcement learning, and optimization, including gradient descent-ascent and two-time-scale Lagrangian optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19375v2</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddharth Chandak</dc:creator>
    </item>
    <item>
      <title>Optimal alignment of Lorentz orientation and generalization to matrix Lie groups</title>
      <link>https://arxiv.org/abs/2506.14994</link>
      <description>arXiv:2506.14994v4 Announce Type: replace-cross 
Abstract: There exist elegant methods of aligning point clouds in $\mathbb R^3$. Unfortunately, these methods fail to generalize to the case of Minkowski space, as we will show. Instead, we propose two solutions to the following problem: given inertial reference frames $A$ and $B$, and given (possibly noisy) measurements of a set of 4-vectors $\{v_i\}$ made in those reference frames with components $\{v_{A,i}\}$ and $\{v_{B,i}\}$, find the optimal Lorentz transformation $\Lambda$ such that $\Lambda v_{A,i}=v_{B,i}$. The first method is direct least squares optimization through a parametrization of $SO(3,1)_+$ in terms of the familiar boost and rotation vectors. The second method takes a detour through the Lorentz algebra; in addition to being conceptually simple and possessing a computational advantage over the first method, it can easily be generalized to the alignment of vector representations in other matrix Lie groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14994v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>hep-ph</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Congzhou M Sha</dc:creator>
    </item>
    <item>
      <title>Adversarial Observability and Performance Trade-offs in Optimal Control</title>
      <link>https://arxiv.org/abs/2506.19829</link>
      <description>arXiv:2506.19829v2 Announce Type: replace-cross 
Abstract: We develop a feedback controller that minimizes the observability of a set of adversarial sensors of a linear system, while adhering to strict closed-loop performance constraints. We quantify the effectiveness of adversarial sensors using the trace of their observability Gramian and its inverse, capturing both average observability and the least observable state directions of the system. We derive theoretical lower bounds on these metrics under performance constraints, characterizing the fundamental limits of observability reduction as a function of the performance trade-off. Finally, we show that the performance-constrained optimization of the Gramian's trace can be formulated as a one-shot semidefinite program, while we address the optimization of its inverse through sequential semidefinite programming. Simulations on an aircraft show how the proposed scheme yields controllers that deteriorate adversarial observability while having near-optimal performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19829v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filippos Fotiadis, Ufuk Topcu</dc:creator>
    </item>
    <item>
      <title>Integral bases, perfect matchings, and the Petersen graph</title>
      <link>https://arxiv.org/abs/2508.15602</link>
      <description>arXiv:2508.15602v3 Announce Type: replace-cross 
Abstract: Let $G=(V,E)$ be a matching-covered graph, denote by $P$ its perfect matching polytope, and by $L$ the integer lattice generated by the integral points in $P$. In this paper, we give short, polyhedral proofs for two difficult results established by Lov\'{a}sz (1987), and by Carvalho, Lucchesi, and Murty (2002) in a series of three papers totaling over 120 pages. More specifically, we prove that $L$ has a lattice basis consisting solely of incidence vectors of some perfect matchings of $G$, $2x\in L$ for all $x\in \mathrm{lin}(P)\cap \mathbb{Z}^E$, and if $G$ has no Petersen brick then $L = \mathrm{lin}(P)\cap \mathbb{Z}^E$. Our proof avoids major technical aspects of the previous proofs, the most important of these being a characterization of the dual lattice, and a `Petersen-brick-sensitive' ear decomposition result for matching-covered graphs. This is achieved by a novel study of the facial structure of the polytope $P$ and its relationship with the lattice $L$. Along the way, we give a new polyhedral characterization of the Petersen graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15602v3</guid>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ahmad Abdi, Olha Silina</dc:creator>
    </item>
    <item>
      <title>Safe and Near-Optimal Control with Online Dynamics Learning</title>
      <link>https://arxiv.org/abs/2509.16650</link>
      <description>arXiv:2509.16650v2 Announce Type: replace-cross 
Abstract: Achieving both optimality and safety under unknown system dynamics is a central challenge in real-world deployment of agents. To address this, we introduce a notion of maximum safe dynamics learning, where sufficient exploration is performed within the space of safe policies. Our method executes $\textit{pessimistically}$ safe policies while $\textit{optimistically}$ exploring informative states and, despite not reaching them due to model uncertainty, ensures continuous online learning of dynamics. The framework achieves first-of-its-kind results: learning the dynamics model sufficiently $-$ up to an arbitrary small tolerance (subject to noise) $-$ in a finite time, while ensuring provably safe operation throughout with high probability and without requiring resets. Building on this, we propose an algorithm to maximize rewards while learning the dynamics $\textit{only to the extent needed}$ to achieve close-to-optimal performance. Unlike typical reinforcement learning (RL) methods, our approach operates online in a non-episodic setting and ensures safety throughout the learning process. We demonstrate the effectiveness of our approach in challenging domains such as autonomous car racing and drone navigation under aerodynamic effects $-$ scenarios where safety is critical and accurate modeling is difficult.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16650v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manish Prajapat, Johannes K\"ohler, Melanie N. Zeilinger, Andreas Krause</dc:creator>
    </item>
    <item>
      <title>Constrained Density Estimation via Optimal Transport</title>
      <link>https://arxiv.org/abs/2601.06830</link>
      <description>arXiv:2601.06830v2 Announce Type: replace-cross 
Abstract: A novel framework for density estimation under expectation constraints is proposed. The framework minimizes the Wasserstein distance between the estimated density and a prior, subject to the constraints that the expected value of a set of functions adopts or exceeds given values. The framework is generalized to include regularization inequalities to mitigate the artifacts in the target measure. An annealing-like algorithm is developed to address non-smooth constraints, with its effectiveness demonstrated through both synthetic and proof-of-concept real world examples in finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06830v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinan Hu, Esteban G. Tabak</dc:creator>
    </item>
    <item>
      <title>Fast Catch-Up, Late Switching: Optimal Batch Size Scheduling via Functional Scaling Laws</title>
      <link>https://arxiv.org/abs/2602.14208</link>
      <description>arXiv:2602.14208v2 Announce Type: replace-cross 
Abstract: Batch size scheduling (BSS) plays a critical role in large-scale deep learning training, influencing both optimization dynamics and computational efficiency. Yet, its theoretical foundations remain poorly understood. In this work, we show that the functional scaling law (FSL) framework introduced in Li et al. (2025a) provides a principled lens for analyzing BSS. Specifically, we characterize the optimal BSS under a fixed data budget and show that its structure depends sharply on task difficulty. For easy tasks, optimal schedules keep increasing batch size throughout. In contrast, for hard tasks, the optimal schedule maintains small batch sizes for most of training and switches to large batches only in a late stage. To explain the emergence of late switching, we uncover a dynamical mechanism -- the fast catch-up effect -- which also manifests in large language model (LLM) pretraining. After switching from small to large batches, the loss rapidly aligns with the constant large-batch trajectory. Using FSL, we show that this effect stems from rapid forgetting of accumulated gradient noise, with the catch-up speed determined by task difficulty. Crucially, this effect implies that large batches can be safely deferred to late training without sacrificing performance, while substantially reducing data consumption. Finally, extensive LLM pretraining experiments -- covering both Dense and MoE architectures with up to 1.1B parameters and 1T tokens -- validate our theoretical predictions. Across all settings, late-switch schedules consistently outperform constant-batch and early-switch baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14208v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbo Wang, Binghui Li, Zhanpeng Zhou, Mingze Wang, Yuxuan Sun, Jiaqi Zhang, Xunliang Cai, Lei Wu</dc:creator>
    </item>
    <item>
      <title>Topology optimization of type-II superconductors with superconductor-dielectric/vacuum interfaces based on Ginzburg-Landau theory under Weyl gauge</title>
      <link>https://arxiv.org/abs/2602.14261</link>
      <description>arXiv:2602.14261v2 Announce Type: replace-cross 
Abstract: Geometry design is a crucial and challenging strategy for improving the performance of type-II superconductors. Topology optimization is one of the most powerful approaches used to determine structural geometries. Therefore, a topology optimization approach is presented to inversely design structural geometries of both low- and high-temperature type-II superconductors with superconductor-dielectric/vacuum interfaces. In the presented approach, the magnetic response of type-II superconductors is modeled using the Ginzburg-Landau theory, where the temporal evolution of the order parameter and vector potential is described by the time-dependent Ginzburg-Landau equations under the Weyl gauge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14261v2</guid>
      <category>cond-mat.supr-con</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <category>physics.comp-ph</category>
      <category>quant-ph</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yongbo Deng, Jan G. Korvink</dc:creator>
    </item>
  </channel>
</rss>
