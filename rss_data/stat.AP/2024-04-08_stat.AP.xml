<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Apr 2024 05:17:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Non-Parametric Estimation of Multiple Periodic Components in Turkey's Electricity Consumption</title>
      <link>https://arxiv.org/abs/2404.03786</link>
      <description>arXiv:2404.03786v1 Announce Type: new 
Abstract: Electric generation and consumption are an essential component of contemporary living, influencing diverse facets of our daily routines, convenience, and economic progress. There is a high demand for characterizing the periodic pattern of electricity consumption. VBPBB employs a bandpass filter aligned to retain the frequency of a PC component and eliminating interference from other components. This leads to a significant reduction in the size of bootstrapped confidence intervals. Furthermore, other PC bootstrap methods preserve one but not multiple periodically correlated components, resulting in superior performance compared to other methods by providing a more precise estimation of the sampling distribution for the desired characteristics. The study of the periodic means of Turkey electricity consumption using VBPBB is presented and compared with outcomes from alternative bootstrapping approaches. These findings offer significant evidence supporting the existence of daily, weekly, and annual PC patterns, along with information on their timing and confidence intervals for their effects. This information is valuable for enhancing predictions and preparations for future responses to electricity consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03786v1</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Yao, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Latent Space-based Likelihood Estimation Using Single Observation for Bayesian Updating of Nonlinear Hysteretic Model</title>
      <link>https://arxiv.org/abs/2404.03871</link>
      <description>arXiv:2404.03871v1 Announce Type: new 
Abstract: This study presents a novel approach to quantifying uncertainties in Bayesian model updating, which is effective in sparse or single observations. Conventional uncertainty quantification metrics such as the Euclidean and Bhattacharyya distance-based metrics are potential in scenarios with ample observations. However, their validation is limited in situations with insufficient data, particularly for nonlinear responses like post-yield behavior. Our method addresses this challenge by using the latent space of a Variational Auto-encoder (VAE), a generative model that enables nonparametric likelihood evaluation. This approach is valuable in updating model parameters based on nonlinear seismic responses of structure, wherein data scarcity is a common challenge. Our numerical experiments confirm the ability of the proposed method to accurately update parameters and quantify uncertainties using limited observations. Additionally, these numerical experiments reveal a tendency for increased information about nonlinear behavior to result in decreased uncertainty in terms of estimations. This study provides a robust tool for quantifying uncertainty in scenarios characterized by considerable uncertainty, thereby expanding the applicability of Bayesian updating methods in data-constrained environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03871v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sangwon Lee, Taro Yaoyama, Yuma Matsumoto, Takenori Hida, Tatsuya Itoi</dc:creator>
    </item>
    <item>
      <title>Regularization for electricity price forecasting</title>
      <link>https://arxiv.org/abs/2404.03968</link>
      <description>arXiv:2404.03968v1 Announce Type: new 
Abstract: The most commonly used form of regularization typically involves defining the penalty function as a L1 or L2 norm. However, numerous alternative approaches remain untested in practical applications. In this study, we apply ten different penalty functions to predict electricity prices and evaluate their performance under two different model structures and in two distinct electricity markets. The study reveals that LQ and elastic net consistently produce more accurate forecasts compared to other regularization types. In particular, they were the only types of penalty functions that consistently produced more accurate forecasts than the most commonly used LASSO. Furthermore, the results suggest that cross-validation outperforms Bayesian information criteria for parameter optimization, and performs as well as models with ex-post parameter selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03968v1</guid>
      <category>stat.AP</category>
      <category>q-fin.ST</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bartosz Uniejewski</dc:creator>
    </item>
    <item>
      <title>Machine learning augmented diagnostic testing to identify sources of variability in test performance</title>
      <link>https://arxiv.org/abs/2404.03678</link>
      <description>arXiv:2404.03678v1 Announce Type: cross 
Abstract: Diagnostic tests which can detect pre-clinical or sub-clinical infection, are one of the most powerful tools in our armoury of weapons to control infectious diseases. Considerable effort has been therefore paid to improving diagnostic testing for human, plant and animal diseases, including strategies for targeting the use of diagnostic tests towards individuals who are more likely to be infected. Here, we follow other recent proposals to further refine this concept, by using machine learning to assess the situational risk under which a diagnostic test is applied to augment its interpretation . We develop this to predict the occurrence of breakdowns of cattle herds due to bovine tuberculosis, exploiting the availability of exceptionally detailed testing records. We show that, without compromising test specificity, test sensitivity can be improved so that the proportion of infected herds detected by the skin test, improves by over 16 percentage points. While many risk factors are associated with increased risk of becoming infected, of note are several factors which suggest that, in some herds there is a higher risk of infection going undetected, including effects that are correlated to the veterinary practice conducting the test, and number of livestock moved off the herd.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03678v1</guid>
      <category>cs.LG</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher J. Banks, Aeron Sanchez, Vicki Stewart, Kate Bowen, Graham Smith, Rowland R. Kao</dc:creator>
    </item>
    <item>
      <title>Signal cancellation factor analysis</title>
      <link>https://arxiv.org/abs/2404.03781</link>
      <description>arXiv:2404.03781v1 Announce Type: cross 
Abstract: Signal cancellation provides a radically new and efficient approach to exploratory factor analysis, without matrix decomposition nor presetting the required number of factors. Its current implementation requires that each factor has at least two unique indicators. Its principle is that it is always possible to combine two indicator variables exclusive to the same factor with weights that cancel their common factor information. Successful combinations, consisting of nose only, are recognized by their null correlations with all remaining variables. The optimal combinations of multifactorial indicators, though, typically retain correlations with some other variables. Their signal, however, can be cancelled through combinations with unifactorial indicators of their contributing factors. The loadings are estimated from the relative signal cancellation weights of the variables involved along with their observed correlations. The factor correlations are obtained from those of their unifactorial indicators, corrected by their factor loadings. The method is illustrated with synthetic data from a complex six-factor structure that even includes two doublet factors. Another example using actual data documents that signal cancellation can rival confirmatory factor analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03781v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e Achim</dc:creator>
    </item>
    <item>
      <title>TransformerLSR: Attentive Joint Model of Longitudinal Data, Survival, and Recurrent Events with Concurrent Latent Structure</title>
      <link>https://arxiv.org/abs/2404.03804</link>
      <description>arXiv:2404.03804v1 Announce Type: cross 
Abstract: In applications such as biomedical studies, epidemiology, and social sciences, recurrent events often co-occur with longitudinal measurements and a terminal event, such as death. Therefore, jointly modeling longitudinal measurements, recurrent events, and survival data while accounting for their dependencies is critical. While joint models for the three components exist in statistical literature, many of these approaches are limited by heavy parametric assumptions and scalability issues. Recently, incorporating deep learning techniques into joint modeling has shown promising results. However, current methods only address joint modeling of longitudinal measurements at regularly-spaced observation times and survival events, neglecting recurrent events. In this paper, we develop TransformerLSR, a flexible transformer-based deep modeling and inference framework to jointly model all three components simultaneously. TransformerLSR integrates deep temporal point processes into the joint modeling framework, treating recurrent and terminal events as two competing processes dependent on past longitudinal measurements and recurrent event times. Additionally, TransformerLSR introduces a novel trajectory representation and model architecture to potentially incorporate a priori knowledge of known latent structures among concurrent longitudinal variables. We demonstrate the effectiveness and necessity of TransformerLSR through simulation studies and analyzing a real-world medical dataset on patients after kidney transplantation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03804v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyue Zhang, Yao Zhao, Yanxun Xu</dc:creator>
    </item>
    <item>
      <title>Modelling handball outcomes using univariate and bivariate approaches</title>
      <link>https://arxiv.org/abs/2404.04213</link>
      <description>arXiv:2404.04213v1 Announce Type: cross 
Abstract: Handball has received growing interest during the last years, including academic research for many different aspects of the sport. On the other hand modelling the outcome of the game has attracted less interest mainly because of the additional challenges that occur. Data analysis has revealed that the number of goals scored by each team are under-dispersed relative to a Poisson distribution and hence new models are needed for this purpose. Here we propose to circumvent the problem by modelling the score difference. This removes the need for special models since typical models for integer data like the Skellam distribution can provide sufficient fit and thus reveal some of the characteristics of the game. In the present paper we propose some models starting from a Skellam regression model and also considering zero inflated versions as well as other discrete distributions in $\mathbb Z$. Furthermore, we develop some bivariate models using copulas to model the two halves of the game and thus providing insights on the game. Data from German Bundesliga are used to show the potential of the new models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04213v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitris Karlis, Rouven Michels, Marius Otting</dc:creator>
    </item>
    <item>
      <title>Integrating representative and non-representative survey data for efficient inference</title>
      <link>https://arxiv.org/abs/2404.02283</link>
      <description>arXiv:2404.02283v2 Announce Type: replace-cross 
Abstract: Non-representative surveys are commonly used and widely available but suffer from selection bias that generally cannot be entirely eliminated using weighting techniques. Instead, we propose a Bayesian method to synthesize longitudinal representative unbiased surveys with non-representative biased surveys by estimating the degree of selection bias over time. We show using a simulation study that synthesizing biased and unbiased surveys together out-performs using the unbiased surveys alone, even if the selection bias may evolve in a complex manner over time. Using COVID-19 vaccination data, we are able to synthesize two large sample biased surveys with an unbiased survey to reduce uncertainty in now-casting and inference estimates while simultaneously retaining the empirical credible interval coverage. Ultimately, we are able to conceptually obtain the properties of a large sample unbiased survey if the assumed unbiased survey, used to anchor the estimates, is unbiased for all time-points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02283v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Dyrkton, Paul Gustafson, Harlan Campbell</dc:creator>
    </item>
  </channel>
</rss>
