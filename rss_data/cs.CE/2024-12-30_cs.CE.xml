<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Dec 2024 05:01:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Machine Learning-Based Detection of Pump-and-Dump Schemes in Real-Time</title>
      <link>https://arxiv.org/abs/2412.18848</link>
      <description>arXiv:2412.18848v1 Announce Type: new 
Abstract: Cryptocurrency markets often face manipulation through prevalent pump-and-dump (P&amp;D) schemes, where self-organized Telegram groups, some exceeding two million members, artificially inflate target cryptocurrency prices. These groups sell premium access to inside information, worsening information asymmetry and financial risks for subscribers and all investors. This paper presents a real-time prediction pipeline to forecast target coins and alert investors to possible P&amp;D schemes. In a Poloniex case study, the model accurately identified the target coin among the top five from 50 random coins in 24 out of 43 (55.81%) P&amp;D events. The pipeline uses advanced natural language processing (NLP) to classify Telegram messages, identifying 2,079 past pump events and detecting new ones in real-time. Our analysis also evaluates the susceptibility of token standards - ERC-20, ERC-721, BRC-20, Inscriptions, and Runes - to manipulation and identifies exchanges commonly involved in P&amp;D schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18848v1</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manuel Bolz, Kevin Br\"undler, Liam Kane, Panagiotis Patsias, Liam Tessendorf, Krzysztof Gogol, Taehoon Kim, Claudio Tessone</dc:creator>
    </item>
    <item>
      <title>Mixed-precision numerics in scientific applications: survey and perspectives</title>
      <link>https://arxiv.org/abs/2412.19322</link>
      <description>arXiv:2412.19322v1 Announce Type: new 
Abstract: The explosive demand for artificial intelligence (AI) workloads has led to a significant increase in silicon area dedicated to lower-precision computations on recent high-performance computing hardware designs. However, mixed-precision capabilities, which can achieve performance improvements of 8x compared to double-precision in extreme compute-intensive workloads, remain largely untapped in most scientific applications. A growing number of efforts have shown that mixed-precision algorithmic innovations can deliver superior performance without sacrificing accuracy. These developments should prompt computational scientists to seriously consider whether their scientific modeling and simulation applications could benefit from the acceleration offered by new hardware and mixed-precision algorithms. In this article, we review the literature on relevant applications, existing mixed-precision algorithms, theories, and the available software infrastructure. We then offer our perspective and recommendations on the potential of mixed-precision algorithms to enhance the performance of scientific simulation applications. Broadly, we find that mixed-precision methods can have a large impact on computational science in terms of time-to-solution and energy consumption. This is true not only for a few arithmetic-dominated applications but also, to a more moderate extent, to the many memory bandwidth-bound applications. In many cases, though, the choice of algorithms and regions of applicability will be domain-specific, and thus require input from domain experts. It is helpful to identify cross-cutting computational motifs and their mixed-precision algorithms in this regard. Finally, there are new algorithms being developed to utilize AI hardware and and AI methods to accelerate first-principles computational science, and these should be closely watched as hardware platforms evolve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19322v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aditya Kashi, Hao Lu, Wesley Brewer, David Rogers, Michael Matheson, Mallikarjun Shankar, Feiyi Wang</dc:creator>
    </item>
    <item>
      <title>Parameter Efficient Fine-Tuning for Deep Learning-Based Full-Waveform Inversion</title>
      <link>https://arxiv.org/abs/2412.19510</link>
      <description>arXiv:2412.19510v1 Announce Type: new 
Abstract: Seismic full waveform inversion (FWI) has seen promising advancements through deep learning. Existing approaches typically focus on task-specific models trained and evaluated in isolation that lead to limited generalization across different geological scenarios. In this work we introduce a task-agnostic foundational model for FWI that captures general features across tasks. We first demonstrate that full fine-tuning of this foundational model outperforms task-specific models built from scratch by delivering superior performance across multiple benchmarks. Building upon this we employ parameter-efficient fine-tuning (PEFT) to further reduce computational overhead. By fine-tuning only a small fraction of the model parameters PEFT achieves comparable results to full fine-tuning while significantly lowering memory and computational requirements. Additionally, PEFT excels in out-of-distribution tasks where it outperforms both full fine-tuning and task-specific models. These findings establish the value of foundational modeling for FWI and highlight PEFT as an effective strategy for efficient and scalable adaptation across diverse tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19510v1</guid>
      <category>cs.CE</category>
      <category>physics.geo-ph</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Koustav Ghosal, Abhranta Panigrahi, Arnav Chavan,  ArunSingh, Deepak Gupta</dc:creator>
    </item>
    <item>
      <title>Dissipation Dilution-Driven Topology Optimization for Maximizing the $Q$ Factor of Nanomechanical Resonators</title>
      <link>https://arxiv.org/abs/2412.18682</link>
      <description>arXiv:2412.18682v1 Announce Type: cross 
Abstract: The quality factor ($Q$ factor) of nanomechanical resonators is influenced by geometry and stress, a phenomenon called dissipation dilution. Studies have explored maximizing this effect, leading to softly-clamped resonator designs. This paper proposes a topology optimization methodology to design two-dimensional nanomechanical resonators with high $Q$ factors by maximizing dissipation dilution. A formulation based on the ratio of geometrically nonlinear to linear modal stiffnesses of a prestressed finite element model is used, with its corresponding adjoint sensitivity analysis formulation. Systematic design in square domains yields geometries with comparable $Q$ factors to literature. We analyze the trade-offs between resonance frequency and quality factor, and how these are reflected in the geometry of resonators. We further apply the methodology to optimize a resonator on a full hexagonal domain. By using the entire mesh -- i.e., without assuming any symmetries -- we find that the optimizer converges to a two-axis symmetric design comprised of four tethers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18682v1</guid>
      <category>cond-mat.mes-hall</category>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hendrik J. Algra, Zichao Li, Matthijs Langelaar, Farbod Alijani, Alejandro M. Arag\'on</dc:creator>
    </item>
    <item>
      <title>Evaluating authorship disambiguation quality through anomaly analysis on researchers' career transition</title>
      <link>https://arxiv.org/abs/2412.18757</link>
      <description>arXiv:2412.18757v1 Announce Type: cross 
Abstract: Authorship disambiguation is crucial for advancing studies in science of science. However, assessing the quality of authorship disambiguation in large-scale databases remains challenging since it is difficult to manually curate a gold-standard dataset that contains disambiguated authors. Through estimating the timing of when 5.8 million biomedical researchers became independent Principal Investigators (PIs) with authorship metadata extracted from the OpenAlex -- the largest open-source bibliometric database -- we unexpectedly discovered an anomaly: over 60% of researchers appeared as the last authors in their first career year. We hypothesized that this improbable finding results from poor name disambiguation, suggesting that such an anomaly may serve as an indicator of low-quality authorship disambiguation. Our findings indicated that authors who lack affiliation information, which makes it more difficult to disambiguate, were far more likely to exhibit this anomaly compared to those who included their affiliation information. In contrast, authors with Open Researcher and Contributor ID (ORCID) -- expected to have higher quality disambiguation -- showed significantly lower anomaly rates. We further applied this approach to examine the authorship disambiguation quality by gender over time, and we found that the quality of disambiguation for female authors was lower than that for male authors before 2010, suggesting that gender disparity findings based on pre-2010 data may require careful reexamination. Our results provide a framework for systematically evaluating authorship disambiguation quality in various contexts, facilitating future improvements in efforts to authorship disambiguation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18757v1</guid>
      <category>cs.DL</category>
      <category>cs.CE</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huaxia Zhou, Mengyi Sun</dc:creator>
    </item>
    <item>
      <title>SILC-EFSA: Self-aware In-context Learning Correction for Entity-level Financial Sentiment Analysis</title>
      <link>https://arxiv.org/abs/2412.19140</link>
      <description>arXiv:2412.19140v1 Announce Type: cross 
Abstract: In recent years, fine-grained sentiment analysis in finance has gained significant attention, but the scarcity of entity-level datasets remains a key challenge. To address this, we have constructed the largest English and Chinese financial entity-level sentiment analysis datasets to date. Building on this foundation, we propose a novel two-stage sentiment analysis approach called Self-aware In-context Learning Correction (SILC). The first stage involves fine-tuning a base large language model to generate pseudo-labeled data specific to our task. In the second stage, we train a correction model using a GNN-based example retriever, which is informed by the pseudo-labeled data. This two-stage strategy has allowed us to achieve state-of-the-art performance on the newly constructed datasets, advancing the field of financial sentiment analysis. In a case study, we demonstrate the enhanced practical utility of our data and methods in monitoring the cryptocurrency market. Our datasets and code are available at https://github.com/NLP-Bin/SILC-EFSA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19140v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Senbin Zhu, Chenyuan He, Hongde Liu, Pengcheng Dong, Hanjie Zhao, Yuchen Yan, Yuxiang Jia, Hongying Zan, Min Peng</dc:creator>
    </item>
    <item>
      <title>The Internet of Value: Integrating Blockchain and Lightning Network Micropayments for Knowledge Markets</title>
      <link>https://arxiv.org/abs/2412.19384</link>
      <description>arXiv:2412.19384v1 Announce Type: cross 
Abstract: Q&amp;A websites rely on user-generated responses, with incentives such as reputation scores or monetary rewards often offered. While some users may find it intrinsically rewarding to assist others, studies indicate that payment can improve the quality and speed of answers. However, traditional payment processors impose minimum thresholds that many Q&amp;A inquiries fall below. The introduction of Bitcoin enabled direct digital value transfer, yet frequent micropayments remain challenging. Recent advancements like the Lightning Network now allow frictionless micropayments by reducing costs and minimising reliance on intermediaries. This development fosters an "Internet of Value," where transferring even small amounts of money is as simple as sharing data. This study investigates integrating Lightning Network-based micropayment strategies into Q&amp;A platforms, aiming to create a knowledge market free of minimum payment barriers. A survey was conducted to address the gap below the $2 payment level identified in prior research. Responses confirmed that incentives for asking and answering weaken as payments decrease. Findings reveal even minimal payments, such as {\pounds}0.01, significantly encourage higher quality and effort in responses. The study recommends micropayment incentives for service-oriented applications, particularly Q&amp;A platforms. By leveraging the Lightning Network to remove barriers, a more open marketplace can emerge, improving engagement and outcomes. Further research is needed to confirm if users follow through on reported intentions when spending funds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19384v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ellis Solaiman, Jorge Robins</dc:creator>
    </item>
    <item>
      <title>Can AI Help with Your Personal Finances?</title>
      <link>https://arxiv.org/abs/2412.19784</link>
      <description>arXiv:2412.19784v1 Announce Type: cross 
Abstract: In recent years, Large Language Models (LLMs) have emerged as a transformative development in artificial intelligence (AI), drawing significant attention from industry and academia. Trained on vast datasets, these sophisticated AI systems exhibit impressive natural language processing and content generation capabilities. This paper explores the potential of LLMs to address key challenges in personal finance, focusing on the United States. We evaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini, Anthropic's Claude, and Meta's Llama, to assess their effectiveness in providing accurate financial advice on topics such as mortgages, taxes, loans, and investments. Our findings show that while these models achieve an average accuracy rate of approximately 70%, they also display notable limitations in certain areas. Specifically, LLMs struggle to provide accurate responses for complex financial queries, with performance varying significantly across different topics. Despite these limitations, the analysis reveals notable improvements in newer versions of these models, highlighting their growing utility for individuals and financial advisors. As these AI systems continue to evolve, their potential for advancing AI-driven applications in personal finance becomes increasingly promising.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19784v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oudom Hean, Utsha Saha, Binita Saha</dc:creator>
    </item>
    <item>
      <title>A boundary integral based particle initialization algorithm for Smooth Particle Hydrodynamics</title>
      <link>https://arxiv.org/abs/2403.07779</link>
      <description>arXiv:2403.07779v3 Announce Type: replace 
Abstract: Algorithms for initializing particle distribution in SPH simulations are important for improving simulation accuracy. However, no such algorithms exist for boundary integral SPH models, which can model complex geometries without requiring layers of virtual particles. This study introduces the Boundary Integral based Particle Initialization (BIPI) algorithm. It employs a particle packing algorithm meticulously designed to redistribute particles to fit the geometry boundary. The BIPI algorithm directly utilizes the geometry's boundary information using the SPH boundary integral formulation. Special consideration is given to particles adjacent to the boundary to prevent artificial volume compression. The BIPI algorithm can hence generate a particle distribution with reduced concentration gradients for domains with complex geometrical shapes. Finally, several examples are presented to demonstrate the effectiveness of the proposed algorithm, including the application of the BIPI algorithm in flow problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07779v3</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parikshit Boregowda, G R Liu</dc:creator>
    </item>
    <item>
      <title>CoinCLIP: A Multimodal Framework for Assessing Viability in Web3 Memecoins</title>
      <link>https://arxiv.org/abs/2412.07591</link>
      <description>arXiv:2412.07591v2 Announce Type: replace 
Abstract: The rapid growth of memecoins within the Web3 ecosystem, driven by platforms like Pump.fun, has made it easier for anyone to create tokens. However, this democratization has also led to an explosion of low-quality or bot-generated projects, often motivated by short-term financial gain. This overwhelming influx of speculative tokens creates a challenge in distinguishing viable memecoins from those that are unlikely to succeed. To address this issue, we introduce CoinVibe, a comprehensive multimodal dataset designed to evaluate the viability of memecoins. CoinVibe integrates textual descriptions, visual content (logos), and community data (user comments, timestamps, and number of likes) to provide a holistic view of a memecoin's potential. In addition, we present CoinCLIP, a novel framework that leverages the Contrastive Language-Image Pre-Training (CLIP) model, augmented with lightweight modules and community data integration, to improve classification accuracy. By combining visual and textual representations with community insights, CoinCLIP provides a robust, data-driven approach to filter out low-quality or bot-driven projects. This research aims to help creators and investors identify high-potential memecoins, while also offering valuable insights into the factors that contribute to their long-term success. The code and dataset are publicly available at https://github.com/hwlongCUHK/CoinCLIP.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07591v2</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hou-Wan Long, Hongyang Li, Wei Cai</dc:creator>
    </item>
    <item>
      <title>Physics-Enhanced Machine Learning: a position paper for dynamical systems investigations</title>
      <link>https://arxiv.org/abs/2405.05987</link>
      <description>arXiv:2405.05987v3 Announce Type: replace-cross 
Abstract: This position paper takes a broad look at Physics-Enhanced Machine Learning (PEML) -- also known as Scientific Machine Learning -- with particular focus to those PEML strategies developed to tackle dynamical systems' challenges. The need to go beyond Machine Learning (ML) strategies is driven by: (i) limited volume of informative data, (ii) avoiding accurate-but-wrong predictions; (iii) dealing with uncertainties; (iv) providing Explainable and Interpretable inferences. A general definition of PEML is provided by considering four physics and domain knowledge biases, and three broad groups of PEML approaches are discussed: physics-guided, physics-encoded and physics-informed. The advantages and challenges in developing PEML strategies for guiding high-consequence decision making in engineering applications involving complex dynamical systems, are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05987v3</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1742-6596/2909/1/012034</arxiv:DOI>
      <arxiv:journal_reference>J. Phys.: Conf. Ser. 2909 012034, 2024</arxiv:journal_reference>
      <dc:creator>Alice Cicirello</dc:creator>
    </item>
    <item>
      <title>Populating cellular metamaterials on the extrema of attainable elasticity through neuroevolution</title>
      <link>https://arxiv.org/abs/2412.11112</link>
      <description>arXiv:2412.11112v3 Announce Type: replace-cross 
Abstract: The trade-offs between different mechanical properties of materials pose fundamental challenges in engineering material design, such as balancing stiffness versus toughness, weight versus energy-absorbing capacity, and among the various elastic coefficients. Although gradient-based topology optimization approaches have been effective in finding specific designs and properties, they are not efficient tools for surveying the vast design space of metamaterials, and thus unable to reveal the attainable bound of interdependent material properties. Other common methods, such as parametric design or data-driven approaches, are limited by either the lack of diversity in geometry or the difficulty to extrapolate from known data, respectively. In this work, we formulate the simultaneous exploration of multiple competing material properties as a multi-objective optimization (MOO) problem and employ a neuroevolution algorithm to efficiently solve it. The Compositional Pattern-Producing Networks (CPPNs) is used as the generative model for unit cell designs, which provide very compact yet lossless encoding of geometry. A modified Neuroevolution of Augmenting Topologies (NEAT) algorithm is employed to evolve the CPPNs such that they create metamaterial designs on the Pareto front of the MOO problem, revealing empirical bounds of different combinations of elastic properties. Looking ahead, our method serves as a universal framework for the computational discovery of diverse metamaterials across a range of fields, including robotics, biomedicine, thermal engineering, and photonics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11112v3</guid>
      <category>cs.NE</category>
      <category>cs.CE</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maohua Yan, Ruicheng Wang, Ke Liu</dc:creator>
    </item>
  </channel>
</rss>
