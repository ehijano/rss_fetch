<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Jul 2024 04:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 26 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Drawing ellipses and elliptical arcs with piecewise cubic B\'ezier curve approximations</title>
      <link>https://arxiv.org/abs/2407.17675</link>
      <description>arXiv:2407.17675v1 Announce Type: new 
Abstract: This tutorial describes how to use piecewise cubic B\'ezier curves to draw arbitrarily oriented ellipses and elliptical arcs. The geometric principles discussed here result in strikingly simple interfaces for graphics functions that can draw (approximate) circles, ellipses, and arcs of circles and ellipses. C++ source code listings are included for these functions. Their code size can be relatively small because they are designed to be used with a graphics library or platform that draws B\'ezier curves, and the library or platform is tasked with the actual rendering of the curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17675v1</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jerry R. Van Aken</dc:creator>
    </item>
    <item>
      <title>3D Hole Filling using Deep Learning Inpainting</title>
      <link>https://arxiv.org/abs/2407.17896</link>
      <description>arXiv:2407.17896v1 Announce Type: new 
Abstract: The current work presents a novel methodology for completing 3D surfaces produced from 3D digitization technologies in places where there is a scarcity of meaningful geometric data. Incomplete or missing data in these three-dimensional (3D) models can lead to erroneous or flawed renderings, limiting their usefulness in a variety of applications such as visualization, geometric computation, and 3D printing. Conventional surface estimation approaches often produce implausible results, especially when dealing with complex surfaces. To address this issue, we propose a technique that incorporates neural network-based 2D inpainting to effectively reconstruct 3D surfaces. Our customized neural networks were trained on a dataset containing over 1 million curvature images. These images show the curvature of vertices as planar representations in 2D. Furthermore, we used a coarse-to-fine surface deformation technique to improve the accuracy of the reconstructed pictures and assure surface adaptability. This strategy enables the system to learn and generalize patterns from input data, resulting in the development of precise and comprehensive three-dimensional surfaces. Our methodology excels in the shape completion process, effectively filling complex holes in three-dimensional surfaces with a remarkable level of realism and precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17896v1</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marina Hern\'andez-Bautista, F. J. Melero</dc:creator>
    </item>
    <item>
      <title>Uncertainty Visualization of Critical Points of 2D Scalar Fields for Parametric and Nonparametric Probabilistic Models</title>
      <link>https://arxiv.org/abs/2407.18015</link>
      <description>arXiv:2407.18015v1 Announce Type: new 
Abstract: This paper presents a novel end-to-end framework for closed-form computation and visualization of critical point uncertainty in 2D uncertain scalar fields. Critical points are fundamental topological descriptors used in the visualization and analysis of scalar fields. The uncertainty inherent in data (e.g., observational and experimental data, approximations in simulations, and compression), however, creates uncertainty regarding critical point positions. Uncertainty in critical point positions, therefore, cannot be ignored, given their impact on downstream data analysis tasks. In this work, we study uncertainty in critical points as a function of uncertainty in data modeled with probability distributions. Although Monte Carlo (MC) sampling techniques have been used in prior studies to quantify critical point uncertainty, they are often expensive and are infrequently used in production-quality visualization software. We, therefore, propose a new end-to-end framework to address these challenges that comprises a threefold contribution. First, we derive the critical point uncertainty in closed form, which is more accurate and efficient than the conventional MC sampling methods. Specifically, we provide the closed-form and semianalytical (a mix of closed-form and MC methods) solutions for parametric (e.g., uniform, Epanechnikov) and nonparametric models (e.g., histograms) with finite support. Second, we accelerate critical point probability computations using a parallel implementation with the VTK-m library, which is platform portable. Finally, we demonstrate the integration of our implementation with the ParaView software system to demonstrate near-real-time results for real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18015v1</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tushar M. Athawale, Zhe Wang, David Pugmire, Kenneth Moreland, Qian Gong, Scott Klasky, Chris R. Johnson, Paul Rosen</dc:creator>
    </item>
    <item>
      <title>PatchEX: High-Quality Real-Time Temporal Supersampling through Patch-based Parallel Extrapolation</title>
      <link>https://arxiv.org/abs/2407.17501</link>
      <description>arXiv:2407.17501v1 Announce Type: cross 
Abstract: High-refresh rate displays have become very popular in recent years due to the need for superior visual quality in gaming, professional displays and specialized applications like medical imaging. However, high-refresh rate displays alone do not guarantee a superior visual experience; the GPU needs to render frames at a matching rate. Otherwise, we observe disconcerting visual artifacts such as screen tearing and stuttering. Temporal supersampling is an effective technique to increase frame rates by predicting new frames from other rendered frames. There are two methods in this space: interpolation and extrapolation. Interpolation-based methods provide good image quality at the cost of a higher latency because they also require the next rendered frame. On the other hand, extrapolation methods are much faster at the cost of quality. This paper introduces PatchEX, a novel frame extrapolation method that aims to provide the quality of interpolation at the speed of extrapolation. It smartly partitions the extrapolation task into sub-tasks and executes them in parallel to improve both quality and latency. It then uses a patch-based inpainting method and a custom shadow prediction approach to fuse the generated sub-frames. This approach significantly reduces the overall latency while maintaining the quality of the output. Our results demonstrate that PatchEX achieves a 65.29% and 48.46% improvement in PSNR over the latest extrapolation methods ExtraNet and ExtraSS, respectively, while being 6x and 2x faster, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17501v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akanksha Dixit, Smruti R. Sarangi</dc:creator>
    </item>
    <item>
      <title>Topology-Preserving Downsampling of Binary Images</title>
      <link>https://arxiv.org/abs/2407.17786</link>
      <description>arXiv:2407.17786v1 Announce Type: cross 
Abstract: We present a novel discrete optimization-based approach to generate downsampled versions of binary images that are guaranteed to have the same topology as the original, measured by the zeroth and first Betti numbers of the black regions, while having good similarity to the original image as measured by IoU and Dice scores. To our best knowledge, all existing binary image downsampling methods do not have such topology-preserving guarantees. We also implemented a baseline morphological operation (dilation)-based approach that always generates topologically correct results. However, we found the similarity scores to be much worse. We demonstrate several applications of our approach. First, generating smaller versions of medical image segmentation masks for easier human inspection. Second, improving the efficiency of binary image operations, including persistent homology computation and shortest path computation, by substituting the original images with smaller ones. In particular, the latter is a novel application that is made feasible only by the full topology-preservation guarantee of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17786v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chia-Chia Chen, Chi-Han Peng</dc:creator>
    </item>
    <item>
      <title>Bluefish: A Relational Framework for Graphic Representations</title>
      <link>https://arxiv.org/abs/2307.00146</link>
      <description>arXiv:2307.00146v3 Announce Type: replace 
Abstract: Diagrams are essential tools for problem-solving and communication as they externalize conceptual structures using spatial relationships. But when picking a diagramming framework, users are faced with a dilemma. They can either use a highly expressive but low-level toolkit, whose API does not match their domain-specific concepts, or select a high-level typology, which offers a recognizable vocabulary but supports a limited range of diagrams. To address this gap, we introduce Bluefish: a diagramming framework inspired by component-based user interface (UI) libraries. Bluefish lets users create diagrams using relations: declarative, composable, and extensible diagram fragments that relax the concept of a UI component. Unlike a component, a relation does not have sole ownership over its children nor does it need to fully specify their layout. To render diagrams, Bluefish extends a traditional tree-based scenegraph to a compound graph that captures both hierarchical and adjacent relationships between nodes. To evaluate our system, we construct a diverse example gallery covering many domains including mathematics, physics, computer science, and even cooking. We show that Bluefish's relations are effective declarative primitives for diagrams. Bluefish is open source, and we aim to shape it into both a usable tool and a research platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00146v3</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <category>cs.PL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676465</arxiv:DOI>
      <dc:creator>Josh Pollock, Catherine Mei, Grace Huang, Elliot Evans, Daniel Jackson, Arvind Satyanarayan</dc:creator>
    </item>
    <item>
      <title>Creating Centered Trochoids and Co-Centered Ellipses Through the Uniform Combinations of Rolling and Sliding Motions by Using Virtual Rotating Circles Technique (VRCT)</title>
      <link>https://arxiv.org/abs/2407.06966</link>
      <description>arXiv:2407.06966v5 Announce Type: replace 
Abstract: In this article we present an innovative mental vision for creating uniform rolling and sliding motions with a definite combination for a circle that is moving along another coplanar circle. Also, we introduce two different methods for combining rolling and sliding motions through the VRCT in order to make a simple practical situation for creating centered trochoids and co-centered ellipses. In this article the traditional mathematical perspective for creating centered trochoids (through a solid rule which is based on the pure rolling a circle along another coplanar circle) is violated and changed to a novel mathematical perspective which is based on the combination of uniform rolling and sliding motions of a circle that is moving along another stationary circle! In this new vision we have not to define a centered trochoid as a swept path by an attached point to a pure rolling circle along another circle. Instead, a centered trochoid can be defined as a traced path by a certain point on the circumference of a pure rolling or rolling and sliding circle (with specific combination of rolling and sliding motions) along another coplanar circle. In our mathematical vision the physical concept of polarization plays an important role. Also, through this new mathematical perspective an ellipse can be visualized as a closed plane curve that can be generated through the pure rolling, pure sliding or rolling and sliding motions due to definite combinations of two co-polarized rotational motions with different commensurable angular frequencies! All of the above subjects can be implemented (and are observable) with the help of an innovative device that we have named it mechanical oscilloscope. The function of our invented instrument is independent from any other electronic devices such as computer and does not require programming to plot centered trochoids and co-centered ellipses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06966v5</guid>
      <category>cs.GR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>H. Arbab, Arzhang Arbab</dc:creator>
    </item>
    <item>
      <title>Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion</title>
      <link>https://arxiv.org/abs/2407.13759</link>
      <description>arXiv:2407.13759v2 Announce Type: replace-cross 
Abstract: We present a method for generating Streetscapes-long sequences of views through an on-the-fly synthesized city-scale scene. Our generation is conditioned by language input (e.g., city name, weather), as well as an underlying map/layout hosting the desired trajectory. Compared to recent models for video generation or 3D view synthesis, our method can scale to much longer-range camera trajectories, spanning several city blocks, while maintaining visual quality and consistency. To achieve this goal, we build on recent work on video diffusion, used within an autoregressive framework that can easily scale to long sequences. In particular, we introduce a new temporal imputation method that prevents our autoregressive approach from drifting from the distribution of realistic city imagery. We train our Streetscapes system on a compelling source of data-posed imagery from Google Street View, along with contextual map data-which allows users to generate city views conditioned on any desired city layout, with controllable camera poses. Please see more results at our project page at https://boyangdeng.com/streetscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13759v2</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Boyang Deng, Richard Tucker, Zhengqi Li, Leonidas Guibas, Noah Snavely, Gordon Wetzstein</dc:creator>
    </item>
  </channel>
</rss>
