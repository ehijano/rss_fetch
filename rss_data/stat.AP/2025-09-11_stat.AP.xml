<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Sep 2025 01:21:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Drinking water contamination as a population-wide determinant of mortality in California</title>
      <link>https://arxiv.org/abs/2509.08186</link>
      <description>arXiv:2509.08186v1 Announce Type: new 
Abstract: Drinking water contamination, a known determinant of adverse health outcomes, remains widespread and inequitably distributed amidst aging infrastructure. Regulatory oversight is the primary tool to protect drinking water-related public health risks, with maximum contaminant levels established to regulate concentrations for contaminants of concern. However, the extent to which existing concentrations of contaminants at the public water system level directly affect population health remains poorly understood. Here we present a large-scale data analysis of 20 million water samples from 2012-2022 in California to assess the impact of drinking water quality on all-cause mortality. Surfactants were associated with increases in mortality, potentially serving as proxies for wastewater contaminants. We further find evidence of mixture effects that were unidentifiable through single-contaminant analysis, suggesting mixtures of toxic metals as well as salinity constituents are associated with mortality. These results could inform public health efforts to mitigate mortality associated with consumption of contaminated drinking water.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08186v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kiley Kennedy, Vincent Zheng, Benjamin Q Huynh</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Global and Local Probabilistic Time Series Forecasting for Contiguous Spatial Demand Regions</title>
      <link>https://arxiv.org/abs/2509.08214</link>
      <description>arXiv:2509.08214v1 Announce Type: new 
Abstract: This study evaluates three probabilistic forecasting strategies using LightGBM: global pooling, cluster-level pooling, and station-level modeling across a range of scenarios, from fully homogeneous simulated data to highly heterogeneous real-world Divvy bike-share demand observed during 2023 to 2024. Clustering was performed using the K-means algorithm applied to principal component analysis transformed covariates, which included time series features, counts of nearby transportation infrastructure, and local demographic characteristics. Forecasting performance was assessed using prediction interval coverage probability (PICP), normalized interval width (PINAW), and the mean squared error (MSE) of the median forecast. The results show that global LightGBM models incorporating station identifiers consistently outperform both cluster-level and station-level models across most scenarios. These global models effectively leverage the full cross-sectional dataset while enabling local adjustments through the station identifier, resulting in superior prediction interval coverage, sharper intervals, and lower forecast errors. In contrast, cluster-based models often suffer from residual within group heterogeneity, leading to degraded accuracy. Station-level models capture fine-grained local dynamics in heterogeneous settings. These findings underscore that global LightGBM models with embedded station identifiers provide a robust, scalable, and computationally efficient framework for transportation demand forecasting. By balancing global structure with local specificity, this approach offers a practical and effective solution for real-world mobility applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08214v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiahe Ling, Wei Biao Wu</dc:creator>
    </item>
    <item>
      <title>Deploying Robust Decision Support Systems for Transit Headway Control: Rider Impacts, Human Factors and Recommendations for Scalability</title>
      <link>https://arxiv.org/abs/2509.08231</link>
      <description>arXiv:2509.08231v1 Announce Type: new 
Abstract: Service reliability is critical to transit service delivery. This paper describes headway control pilots conducted in two high-ridership Chicago bus routes between 2022 and 2023. A decision support system was developed for a bus holding strategy based on a reinforcement learning approach. For the pilots, a user interface enabled supervisors to monitor service and record applied actions. The first pilot tested terminal-based holding on a route affected by missed trips from absenteeism. The analysis found improvements in reliability, and the application of control was shown to outperform days with more service. The second pilot applied en-route holding in a high-ridership bus route in Chicago. The evaluation showed wait time improvements with rippled benefits to stops downstream, and a reduction in transfer times from connecting bus and rail lines. Compliance analysis based on the supervisor logs on the app revealed mixed compliance levels from drivers, which were related to the mentality of schedule adherence and seniority. Recommendations are provided for practitioners to scale similar efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08231v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Rodriguez, Haris N. Koutsopoulos, Jinhua Zhao</dc:creator>
    </item>
    <item>
      <title>Network Contagion in Financial Labor Markets: Predicting Turnover in Hong Kong</title>
      <link>https://arxiv.org/abs/2509.08001</link>
      <description>arXiv:2509.08001v1 Announce Type: cross 
Abstract: Employee turnover is a critical challenge in financial markets, yet little is known about the role of professional networks in shaping career moves. Using the Hong Kong Securities and Futures Commission (SFC) public register (2007-2024), we construct temporal networks of 121,883 professionals and 4,979 firms to analyze and predict employee departures. We introduce a graph-based feature propagation framework that captures peer influence and organizational stability. Our analysis shows a contagion effect: professionals are 23% more likely to leave when over 30% of their peers depart within six months. Embedding these network signals into machine learning models improves turnover prediction by 30% over baselines. These results highlight the predictive power of temporal network effects in workforce dynamics, and demonstrate how network-based analytics can inform regulatory monitoring, talent management, and systemic risk assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08001v1</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdulla AlKetbi, Patrick Yam, Gautier Marti, Raed Jaradat</dc:creator>
    </item>
    <item>
      <title>In-Context Learning Enhanced Credibility Transformer</title>
      <link>https://arxiv.org/abs/2509.08122</link>
      <description>arXiv:2509.08122v1 Announce Type: cross 
Abstract: The starting point of our network architecture is the Credibility Transformer which extends the classical Transformer architecture by a credibility mechanism to improve model learning and predictive performance. This Credibility Transformer learns credibilitized CLS tokens that serve as learned representations of the original input features. In this paper we present a new paradigm that augments this architecture by an in-context learning mechanism, i.e., we increase the information set by a context batch consisting of similar instances. This allows the model to enhance the CLS token representations of the instances by additional in-context information and fine-tuning. We empirically verify that this in-context learning enhances predictive accuracy by adapting to similar risk patterns. Moreover, this in-context learning also allows the model to generalize to new instances which, e.g., have feature levels in the categorical covariates that have not been present when the model was trained -- for a relevant example, think of a new vehicle model which has just been developed by a car manufacturer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08122v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kishan Padayachy, Ronald Richman, Salvatore Scognamiglio, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Machine Learning with Multitype Protected Attributes: Intersectional Fairness through Regularisation</title>
      <link>https://arxiv.org/abs/2509.08163</link>
      <description>arXiv:2509.08163v1 Announce Type: cross 
Abstract: Ensuring equitable treatment (fairness) across protected attributes (such as gender or ethnicity) is a critical issue in machine learning. Most existing literature focuses on binary classification, but achieving fairness in regression tasks-such as insurance pricing or hiring score assessments-is equally important. Moreover, anti-discrimination laws also apply to continuous attributes, such as age, for which many existing methods are not applicable. In practice, multiple protected attributes can exist simultaneously; however, methods targeting fairness across several attributes often overlook so-called "fairness gerrymandering", thereby ignoring disparities among intersectional subgroups (e.g., African-American women or Hispanic men). In this paper, we propose a distance covariance regularisation framework that mitigates the association between model predictions and protected attributes, in line with the fairness definition of demographic parity, and that captures both linear and nonlinear dependencies. To enhance applicability in the presence of multiple protected attributes, we extend our framework by incorporating two multivariate dependence measures based on distance covariance: the previously proposed joint distance covariance (JdCov) and our novel concatenated distance covariance (CCdCov), which effectively address fairness gerrymandering in both regression and classification tasks involving protected attributes of various types. We discuss and illustrate how to calibrate regularisation strength, including a method based on Jensen-Shannon divergence, which quantifies dissimilarities in prediction distributions across groups. We apply our framework to the COMPAS recidivism dataset and a large motor insurance claims dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08163v1</guid>
      <category>cs.LG</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ho Ming Lee, Katrien Antonio, Benjamin Avanzi, Lorenzo Marchi, Rui Zhou</dc:creator>
    </item>
    <item>
      <title>Algorithmic Tradeoffs, Applied NLP, and the State-of-the-Art Fallacy</title>
      <link>https://arxiv.org/abs/2509.08199</link>
      <description>arXiv:2509.08199v1 Announce Type: cross 
Abstract: Computational sociology is growing in popularity, yet the analytic tools employed differ widely in power, transparency, and interpretability. In computer science, methods gain popularity after surpassing benchmarks of predictive accuracy, becoming the "state of the art." Computer scientists favor novelty and innovation for different reasons, but prioritizing technical prestige over methodological fit could unintentionally limit the scope of sociological inquiry. To illustrate, we focus on computational text analysis and revisit a prior study of college admissions essays, comparing analyses with both older and newer methods. These methods vary in flexibility and opacity, allowing us to compare performance across distinct methodological regimes. We find that newer techniques did not outperform prior results in meaningful ways. We also find that using the current state of the art, generative AI and large language models, could introduce bias and confounding that is difficult to extricate. We therefore argue that sociological inquiry benefits from methodological pluralism that aligns analytic choices with theoretical and empirical questions. While we frame this sociologically, scholars in other disciplines may confront what we call the "state-of-the-art fallacy", the belief that the tool computer scientists deem to be the best will work across topics, domains, and questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08199v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>AJ Alvero, Ruohong Dong, Klint Kanopka, David Lang</dc:creator>
    </item>
    <item>
      <title>Generative AI as a Safety Net for Survey Question Refinement</title>
      <link>https://arxiv.org/abs/2509.08702</link>
      <description>arXiv:2509.08702v1 Announce Type: cross 
Abstract: Writing survey questions that easily and accurately convey their intent to a variety of respondents is a demanding and high-stakes task. Despite the extensive literature on best practices, the number of considerations to keep in mind is vast and even small errors can render collected data unusable for its intended purpose. The process of drafting initial questions, checking for known sources of error, and developing solutions to those problems requires considerable time, expertise, and financial resources. Given the rising costs of survey implementation and the critical role that polls play in media, policymaking, and research, it is vital that we utilize all available tools to protect the integrity of survey data and the financial investments made to obtain it. Since its launch in 2022, ChatGPT and other generative AI model platforms have been integrated into everyday life processes and workflows, particularly pertaining to text revision. While many researchers have begun exploring how generative AI may assist with questionnaire design, we have implemented a prompt experiment to systematically test what kind of feedback on survey questions an average ChatGPT user can expect. Results from our zero--shot prompt experiment, which randomized the version of ChatGPT and the persona given to the model, shows that generative AI is a valuable tool today, even for an average AI user, and suggests that AI will play an increasingly prominent role in the evolution of survey development best practices as precise tools are developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08702v1</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erica Ann Metheney, Lauren Yehle</dc:creator>
    </item>
    <item>
      <title>Quantifying model prediction sensitivity to model-form uncertainty</title>
      <link>https://arxiv.org/abs/2509.08708</link>
      <description>arXiv:2509.08708v1 Announce Type: cross 
Abstract: Model-form uncertainty (MFU) in assumptions made during physics-based model development is widely considered a significant source of uncertainty; however, there are limited approaches that can quantify MFU in predictions extrapolating beyond available data. As a result, it is challenging to know how important MFU is in practice, especially relative to other sources of uncertainty in a model, making it difficult to prioritize resources and efforts to drive down error in model predictions. To address these challenges, we present a novel method to quantify the importance of uncertainties associated with model assumptions. We combine parameterized modifications to assumptions (called MFU representations) with grouped variance-based sensitivity analysis to measure the importance of assumptions. We demonstrate how, in contrast to existing methods addressing MFU, our approach can be applied without access to calibration data. However, if calibration data is available, we demonstrate how it can be used to inform the MFU representation, and how variance-based sensitivity analysis can be meaningfully applied even in the presence of dependence between parameters (a common byproduct of calibration).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08708v1</guid>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teresa Portone, Rebekah D. White, Joseph L. Hart</dc:creator>
    </item>
    <item>
      <title>Unified calibration and spatial mapping of fine particulate matter data from multiple low-cost air pollution sensor networks in Baltimore, Maryland</title>
      <link>https://arxiv.org/abs/2412.13034</link>
      <description>arXiv:2412.13034v2 Announce Type: replace 
Abstract: Low-cost air pollution sensor networks are increasingly being deployed globally, supplementing sparse regulatory monitoring with localized air quality data. In some areas, like Baltimore, Maryland, there are only few regulatory (reference) devices but multiple low-cost networks. While there are many available methods to calibrate data from each network individually, separate calibration of each network leads to conflicting air quality predictions. We develop a general Bayesian spatial filtering model combining data from multiple networks and reference devices, providing dynamic calibrations (informed by the latest reference data) and unified predictions (combining information from all available sensors) for the entire region. This method accounts for network-specific bias and noise (observation models), as different networks can use different types of sensors, and uses a Gaussian process (state-space model) to capture spatial correlations. We apply the method to calibrate PM$_{2.5}$ data from Baltimore in June and July 2023 -- a period including days of hazardous concentrations due to wildfire smoke. Our method helps mitigate the effects of preferential sampling of one network in Baltimore, results in better predictions and narrower confidence intervals. Our approach can be used to calibrate low-cost air pollution sensor data in Baltimore and any other areas with multiple low-cost networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13034v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claire Heffernan, Kirsten Koehler, Drew R. Gentner, Roger D. Peng, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Sample efficient likelihood-free inference for virus dynamics with different types of experiments</title>
      <link>https://arxiv.org/abs/2508.11042</link>
      <description>arXiv:2508.11042v2 Announce Type: replace 
Abstract: This study applied Bayesian optimization likelihood-free inference(BOLFI) to virus dynamics experimental data and efficiently inferred the model parameters with uncertainty measure. The computational benefit is remarkable compared to existing methodology on the same problem. No likelihood knowledge is needed in the inference. Improvement of the BOLFI algorithm with Gaussian process based classifier for treatment of extreme values are provided. Discrepancy design for combining different forms of data from completely different experiment processes are suggested and tested with synthetic data, then applied to real data. Reasonable parameter values are estimated for influenza A virus data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11042v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingying Xu</dc:creator>
    </item>
    <item>
      <title>piCurve: an R package for modeling photosynthesis-irradiance curves</title>
      <link>https://arxiv.org/abs/2508.14321</link>
      <description>arXiv:2508.14321v2 Announce Type: replace 
Abstract: Photosynthesis-irradiance (PI) curves are foundational for quantifying primary production, parameterizing ecosystem and biogeochemical models, and interpreting physiological acclimation to light. Despite their broad use, researchers lack a unified, reproducible toolkit to fit, compare, and diagnose the many PI formulations that have accumulated over the last century. We introduce piCurve, an R package that standardizes the modeling of PI relationships, with a library of widely used light-limited, light-saturated, and photoinhibited formulations and a consistent statistical framework for estimation and comparison. With the total of 24 PI models, piCurve supports mean squared error (MSE) and maximum likelihood estimation (MLE), provides uncertainty quantification via information matrix (Hessian), and includes automated, data-informed initialization to improve convergence. Utilities classify PI data into light-limited, light-saturated, and photoinhibited regions, while plotting and 'tidy' helpers streamline workflow and reporting. Together, these features enable reproducible analyses and fair model comparisons, including for curves exhibiting a plateau followed by photoinhibition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14321v2</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.CO</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad M. Amirian, Andrew J. Irwin</dc:creator>
    </item>
    <item>
      <title>Weighted Brier Score -- an Overall Summary Measure for Risk Prediction Models with Clinical Utility Consideration</title>
      <link>https://arxiv.org/abs/2408.01626</link>
      <description>arXiv:2408.01626v2 Announce Type: replace-cross 
Abstract: As advancements in novel biomarker-based algorithms and models accelerate disease risk prediction and stratification in medicine, it is crucial to evaluate these models within the context of their intended clinical application. Prediction models output the absolute risk of disease; subsequently, patient counseling and shared decision-making are based on the estimated individual risk and cost-benefit assessment. The overall impact of the application is often referred to as clinical utility, which received significant attention in terms of model assessment lately. The classic Brier score is a popular measure of prediction accuracy; however, it is insufficient for effectively assessing clinical utility. To address this limitation, we propose a class of weighted Brier scores that aligns with the decision-theoretic framework of clinical utility. Additionally, we decompose the weighted Brier score into discrimination and calibration components, examining how weighting influences the overall score and its individual components. Through this decomposition, we link the weighted Brier score to the $H$ measure, which has been proposed as a coherent alternative to the area under the receiver operating characteristic curve. This theoretical link to the $H$ measure further supports our weighting method and underscores the essential elements of discrimination and calibration in risk prediction evaluation. The practical use of the weighted Brier score as an overall summary is demonstrated using data from the Prostate Cancer Active Surveillance Study (PASS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01626v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kehao Zhu, Yingye Zheng, Kwun Chuen Gary Chan</dc:creator>
    </item>
    <item>
      <title>Introducing RobustiPy: An efficient next generation multiversal library with model selection, averaging, resampling, and explainable artificial intelligence</title>
      <link>https://arxiv.org/abs/2506.19958</link>
      <description>arXiv:2506.19958v2 Announce Type: replace-cross 
Abstract: Scientific inference is often undermined by the vast but rarely explored "multiverse" of defensible modelling choices, which can generate results as variable as the phenomena under study. We introduce RobustiPy, an open-source Python library that systematizes multiverse analysis and model-uncertainty quantification at scale. RobustiPy unifies bootstrap-based inference, combinatorial specification search, model selection and averaging, joint-inference routines, and explainable AI methods within a modular, reproducible framework. Beyond exhaustive specification curves, it supports rigorous out-of-sample validation and quantifies the marginal contribution of each covariate. We demonstrate its utility across five simulation designs and ten empirical case studies spanning economics, sociology, psychology, and medicine, including a re-analysis of widely cited findings with documented discrepancies. Benchmarking on ~672 million simulated regressions shows that RobustiPy delivers state-of-the-art computational efficiency while expanding transparency in empirical research. By standardizing and accelerating robustness analysis, RobustiPy transforms how researchers interrogate sensitivity across the analytical multiverse, offering a practical foundation for more reproducible and interpretable computational science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19958v2</guid>
      <category>stat.ME</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Valdenegro Ibarra, Jiani Yan, Duiyi Dai, Charles Rahal</dc:creator>
    </item>
    <item>
      <title>Fast phase prediction of charged polymer blends by white-box machine learning surrogates</title>
      <link>https://arxiv.org/abs/2509.07164</link>
      <description>arXiv:2509.07164v2 Announce Type: replace-cross 
Abstract: Compatibilized polymer blends are a complex, yet versatile and widespread category of material. When the components of a binary blend are immiscible, they are typically driven towards a macrophase-separated state, but with the introduction of electrostatic interactions, they can be either homogenized or shifted to microphase separation. However, both experimental and simulation approaches face significant challenges in efficiently exploring the vast design space of charge-compatibilized polymer blends, encompassing chemical interactions, architectural properties, and composition. In this work, we introduce a white-box machine learning approach integrated with polymer field theory to predict the phase behavior of these systems, which is significantly more accurate than conventional black-box machine learning approaches. The random phase approximation (RPA) calculation is used as a testbed to determine polymer phases. Instead of directly predicting the polymer phase output of RPA calculations from a large input space by a machine learning model, we build a parallel partial Gaussian process model to predict the most computationally intensive component of the RPA calculation that only involves polymer architecture parameters as inputs. This approach substantially reduces the computational cost of the RPA calculation across a vast input space with nearly 100% accuracy for out-of-sample prediction, enabling rapid screening of polymer blend charge-compatibilization designs. More broadly, the white-box machine learning strategy offers a promising approach for dramatic acceleration of polymer field-theoretic methods for mapping out polymer phase behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07164v2</guid>
      <category>cond-mat.soft</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Clayton Ellis, Xinyi Fang, Christopher Balzer, Timothy Quah, M. Scott Shell, Glenn H. Fredrickson, Mengyang Gu</dc:creator>
    </item>
  </channel>
</rss>
