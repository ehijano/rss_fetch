<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ML updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ML" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Jan 2025 02:27:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Concentration of Measure for Distributions Generated via Diffusion Models</title>
      <link>https://arxiv.org/abs/2501.07741</link>
      <description>arXiv:2501.07741v1 Announce Type: new 
Abstract: We show via a combination of mathematical arguments and empirical evidence that data distributions sampled from diffusion models satisfy a Concentration of Measure Property saying that any Lipschitz $1$-dimensional projection of a random vector is not too far from its mean with high probability. This implies that such models are quite restrictive and gives an explanation for a fact previously observed in arXiv:2410.14171 that conventional diffusion models cannot capture "heavy-tailed" data (i.e. data $\mathbf{x}$ for which the norm $\|\mathbf{x}\|_2$ does not possess a subgaussian tail) well. We then proceed to train a generalized linear model using stochastic gradient descent (SGD) on the diffusion-generated data for a multiclass classification task and observe empirically that a Gaussian universality result holds for the test error.
  In other words, the test error depends only on the first and second order statistics of the diffusion-generated data in the linear setting. Results of such forms are desirable because they allow one to assume the data itself is Gaussian for analyzing performance of the trained classifier. Finally, we note that current approaches to proving universality do not apply to this case as the covariance matrices of the data tend to have vanishing minimum singular values for the diffusion-generated data, while the current proofs assume that this is not the case (see Subsection 3.4 for more details). This leaves extending previous mathematical universality results as an intriguing open question.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07741v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reza Ghane, Anthony Bao, Danil Akhtiamov, Babak Hassibi</dc:creator>
    </item>
    <item>
      <title>On the Statistical Capacity of Deep Generative Models</title>
      <link>https://arxiv.org/abs/2501.07763</link>
      <description>arXiv:2501.07763v1 Announce Type: new 
Abstract: Deep generative models are routinely used in generating samples from complex, high-dimensional distributions. Despite their apparent successes, their statistical properties are not well understood. A common assumption is that with enough training data and sufficiently large neural networks, deep generative model samples will have arbitrarily small errors in sampling from any continuous target distribution. We set up a unifying framework that debunks this belief. We demonstrate that broad classes of deep generative models, including variational autoencoders and generative adversarial networks, are not universal generators. Under the predominant case of Gaussian latent variables, these models can only generate concentrated samples that exhibit light tails. Using tools from concentration of measure and convex geometry, we give analogous results for more general log-concave and strongly log-concave latent variable distributions. We extend our results to diffusion models via a reduction argument. We use the Gromov--Levy inequality to give similar guarantees when the latent variables lie on manifolds with positive Ricci curvature. These results shed light on the limited capacity of common deep generative models to handle heavy tails. We illustrate the empirical relevance of our work with simulations and financial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07763v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edric Tam, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>On the use of Statistical Learning Theory for model selection in Structural Health Monitoring</title>
      <link>https://arxiv.org/abs/2501.08050</link>
      <description>arXiv:2501.08050v1 Announce Type: new 
Abstract: Whenever data-based systems are employed in engineering applications, defining an optimal statistical representation is subject to the problem of model selection. This paper focusses on how well models can generalise in Structural Health Monitoring (SHM). Although statistical model validation in this field is often performed heuristically, it is possible to estimate generalisation more rigorously using the bounds provided by Statistical Learning Theory (SLT). Therefore, this paper explores the selection process of a kernel smoother for modelling the impulse response of a linear oscillator from the perspective of SLT. It is demonstrated that incorporating domain knowledge into the regression problem yields a lower guaranteed risk, thereby enhancing generalisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08050v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>C. A. Lindley, N. Dervilis, K. Worden</dc:creator>
    </item>
    <item>
      <title>Globally Convergent Variational Inference</title>
      <link>https://arxiv.org/abs/2501.08201</link>
      <description>arXiv:2501.08201v1 Announce Type: new 
Abstract: In variational inference (VI), an approximation of the posterior distribution is selected from a family of distributions through numerical optimization. With the most common variational objective function, known as the evidence lower bound (ELBO), only convergence to a local optimum can be guaranteed. In this work, we instead establish the global convergence of a particular VI method. This VI method, which may be considered an instance of neural posterior estimation (NPE), minimizes an expectation of the inclusive (forward) KL divergence to fit a variational distribution that is parameterized by a neural network. Our convergence result relies on the neural tangent kernel (NTK) to characterize the gradient dynamics that arise from considering the variational objective in function space. In the asymptotic regime of a fixed, positive-definite neural tangent kernel, we establish conditions under which the variational objective admits a unique solution in a reproducing kernel Hilbert space (RKHS). Then, we show that the gradient descent dynamics in function space converge to this unique function. In ablation studies and practical problems, we demonstrate that our results explain the behavior of NPE in non-asymptotic finite-neuron settings, and show that NPE outperforms ELBO-based optimization, which often converges to shallow local optima.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08201v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Declan McNamara, Jackson Loper, Jeffrey Regier</dc:creator>
    </item>
    <item>
      <title>Avoiding subtraction and division of stochastic signals using normalizing flows: NFdeconvolve</title>
      <link>https://arxiv.org/abs/2501.08288</link>
      <description>arXiv:2501.08288v1 Announce Type: new 
Abstract: Across the scientific realm, we find ourselves subtracting or dividing stochastic signals. For instance, consider a stochastic realization, $x$, generated from the addition or multiplication of two stochastic signals $a$ and $b$, namely $x=a+b$ or $x = ab$. For the $x=a+b$ example, $a$ can be fluorescence background and $b$ the signal of interest whose statistics are to be learned from the measured $x$. Similarly, when writing $x=ab$, $a$ can be thought of as the illumination intensity and $b$ the density of fluorescent molecules of interest. Yet dividing or subtracting stochastic signals amplifies noise, and we ask instead whether, using the statistics of $a$ and the measurement of $x$ as input, we can recover the statistics of $b$. Here, we show how normalizing flows can generate an approximation of the probability distribution over $b$, thereby avoiding subtraction or division altogether. This method is implemented in our software package, NFdeconvolve, available on GitHub with a tutorial linked in the main text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08288v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>physics.data-an</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pedro Pessoa, Max Schweiger, Lance W. Q. Xu, Tristan Manha, Ayush Saurabh, Julian Antolin Camarena, Steve Press\'e</dc:creator>
    </item>
    <item>
      <title>Analyzing Spatio-Temporal Dynamics of Dissolved Oxygen for the River Thames using Superstatistical Methods and Machine Learning</title>
      <link>https://arxiv.org/abs/2501.07599</link>
      <description>arXiv:2501.07599v1 Announce Type: cross 
Abstract: By employing superstatistical methods and machine learning, we analyze time series data of water quality indicators for the River Thames, with a specific focus on the dynamics of dissolved oxygen. After detrending, the probability density functions of dissolved oxygen fluctuations exhibit heavy tails that are effectively modeled using $q$-Gaussian distributions. Our findings indicate that the multiplicative Empirical Mode Decomposition method stands out as the most effective detrending technique, yielding the highest log-likelihood in nearly all fittings. We also observe that the optimally fitted width parameter of the $q$-Gaussian shows a negative correlation with the distance to the sea, highlighting the influence of geographical factors on water quality dynamics. In the context of same-time prediction of dissolved oxygen, regression analysis incorporating various water quality indicators and temporal features identify the Light Gradient Boosting Machine as the best model. SHapley Additive exPlanations reveal that temperature, pH, and time of year play crucial roles in the predictions. Furthermore, we use the Transformer to forecast dissolved oxygen concentrations. For long-term forecasting, the Informer model consistently delivers superior performance, achieving the lowest MAE and SMAPE with the 192 historical time steps that we used. This performance is attributed to the Informer's ProbSparse self-attention mechanism, which allows it to capture long-range dependencies in time-series data more effectively than other machine learning models. It effectively recognizes the half-life cycle of dissolved oxygen, with particular attention to key intervals. Our findings provide valuable insights for policymakers involved in ecological health assessments, aiding in accurate predictions of river water quality and the maintenance of healthy aquatic ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07599v1</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41598-024-72084-w</arxiv:DOI>
      <arxiv:journal_reference>Sci. Rep. 14, 21288 (2024)</arxiv:journal_reference>
      <dc:creator>Hankun He, Takuya Boehringer, Benjamin Sch\"afer, Kate Heppell, Christian Beck</dc:creator>
    </item>
    <item>
      <title>Impact of Data Breadth and Depth on Performance of Siamese Neural Network Model: Experiments with Three Keystroke Dynamic Datasets</title>
      <link>https://arxiv.org/abs/2501.07600</link>
      <description>arXiv:2501.07600v1 Announce Type: cross 
Abstract: Deep learning models, such as the Siamese Neural Networks (SNN), have shown great potential in capturing the intricate patterns in behavioral data. However, the impacts of dataset breadth (i.e., the number of subjects) and depth (e.g., the amount of training samples per subject) on the performance of these models is often informally assumed, and remains under-explored. To this end, we have conducted extensive experiments using the concepts of "feature space" and "density" to guide and gain deeper understanding on the impact of dataset breadth and depth on three publicly available keystroke datasets (Aalto, CMU and Clarkson II). Through varying the number of training subjects, number of samples per subject, amount of data in each sample, and number of triplets used in training, we found that when feasible, increasing dataset breadth enables the training of a well-trained model that effectively captures more inter-subject variability. In contrast, we find that the extent of depth's impact from a dataset depends on the nature of the dataset. Free-text datasets are influenced by all three depth-wise factors; inadequate samples per subject, sequence length, training triplets and gallery sample size, which may all lead to an under-trained model. Fixed-text datasets are less affected by these factors, and as such make it easier to create a well-trained model. These findings shed light on the importance of dataset breadth and depth in training deep learning models for behavioral biometrics and provide valuable insights for designing more effective authentication systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07600v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmed Anu Wahab, Daqing Hou, Nadia Cheng, Parker Huntley, Charles Devlen</dc:creator>
    </item>
    <item>
      <title>An Explainable Pipeline for Machine Learning with Functional Data</title>
      <link>https://arxiv.org/abs/2501.07602</link>
      <description>arXiv:2501.07602v1 Announce Type: cross 
Abstract: Machine learning (ML) models have shown success in applications with an objective of prediction, but the algorithmic complexity of some models makes them difficult to interpret. Methods have been proposed to provide insight into these "black-box" models, but there is little research that focuses on supervised ML when the model inputs are functional data. In this work, we consider two applications from high-consequence spaces with objectives of making predictions using functional data inputs. One application aims to classify material types to identify explosive materials given hyperspectral computed tomography scans of the materials. The other application considers the forensics science task of connecting an inkjet printed document to the source printer using color signatures extracted by Raman spectroscopy. An instinctive route to consider for analyzing these data is a data driven ML model for classification, but due to the high consequence nature of the applications, we argue it is important to appropriately account for the nature of the data in the analysis to not obscure or misrepresent patterns. As such, we propose the Variable importance Explainable Elastic Shape Analysis (VEESA) pipeline for training ML models with functional data that (1) accounts for the vertical and horizontal variability in the functional data and (2) provides an explanation in the original data space of how the model uses variability in the functional data for prediction. The pipeline makes use of elastic functional principal components analysis (efPCA) to generate uncorrelated model inputs and permutation feature importance (PFI) to identify the principal components important for prediction. The variability captured by the important principal components in visualized the original data space. We ultimately discuss ideas for natural extensions of the VEESA pipeline and challenges for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07602v1</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Katherine Goode, J. Derek Tucker, Daniel Ries, Heike Hofmann</dc:creator>
    </item>
    <item>
      <title>A Step Toward Interpretability: Smearing the Likelihood</title>
      <link>https://arxiv.org/abs/2501.07643</link>
      <description>arXiv:2501.07643v1 Announce Type: cross 
Abstract: The problem of interpretability of machine learning architecture in particle physics has no agreed-upon definition, much less any proposed solution. We present a first modest step toward these goals by proposing a definition and corresponding practical method for isolation and identification of relevant physical energy scales exploited by the machine. This is accomplished by smearing or averaging over all input events that lie within a prescribed metric energy distance of one another and correspondingly renders any quantity measured on a finite, discrete dataset continuous over the dataspace. Within this approach, we are able to explicitly demonstrate that (approximate) scaling laws are a consequence of extreme value theory applied to analysis of the distribution of the irreducible minimal distance over which a machine must extrapolate given a finite dataset. As an example, we study quark versus gluon jet identification, construct the smeared likelihood, and show that discrimination power steadily increases as resolution decreases, indicating that the true likelihood for the problem is sensitive to emissions at all scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07643v1</guid>
      <category>hep-ph</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew J. Larkoski</dc:creator>
    </item>
    <item>
      <title>Finite Sample Identification of Partially Observed Bilinear Dynamical Systems</title>
      <link>https://arxiv.org/abs/2501.07652</link>
      <description>arXiv:2501.07652v1 Announce Type: cross 
Abstract: We consider the problem of learning a realization of a partially observed bilinear dynamical system (BLDS) from noisy input-output data. Given a single trajectory of input-output samples, we provide a finite time analysis for learning the system's Markov-like parameters, from which a balanced realization of the bilinear system can be obtained. Our bilinear system identification algorithm learns the system's Markov-like parameters by regressing the outputs to highly correlated, nonlinear, and heavy-tailed covariates. Moreover, the stability of BLDS depends on the sequence of inputs used to excite the system. These properties, unique to partially observed bilinear dynamical systems, pose significant challenges to the analysis of our algorithm for learning the unknown dynamics. We address these challenges and provide high probability error bounds on our identification algorithm under a uniform stability assumption. Our analysis provides insights into system theoretic quantities that affect learning accuracy and sample complexity. Lastly, we perform numerical experiments with synthetic data to reinforce these insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07652v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yahya Sattar, Yassir Jedra, Maryam Fazel, Sarah Dean</dc:creator>
    </item>
    <item>
      <title>Fast sampling and model selection for Bayesian mixture models</title>
      <link>https://arxiv.org/abs/2501.07668</link>
      <description>arXiv:2501.07668v1 Announce Type: cross 
Abstract: We describe two Monte Carlo algorithms for sampling from the integrated posterior distributions of a range of Bayesian mixture models. Both algorithms allow us to directly sample not only the assignment of observations to components but also the number of components, thereby fitting the model and performing model selection over the number of components in a single computation. The first algorithm is a traditional collapsed Gibbs sampler, albeit with an unusual move-set; the second builds on the first, adding rejection-free sampling from the prior over component assignments, to create an algorithm that has excellent mixing time in typical applications and outperforms current state-of-the-art methods, in some cases by a wide margin. We demonstrate our methods with a selection of applications to latent class analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07668v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. E. J. Newman</dc:creator>
    </item>
    <item>
      <title>Dataset Distillation as Pushforward Optimal Quantization</title>
      <link>https://arxiv.org/abs/2501.07681</link>
      <description>arXiv:2501.07681v1 Announce Type: cross 
Abstract: Dataset distillation aims to find a synthetic training set such that training on the synthetic data achieves similar performance to training on real data, with orders of magnitude less computational requirements. Existing methods can be broadly categorized as either bi-level optimization problems that have neural network training heuristics as the lower level problem, or disentangled methods that bypass the bi-level optimization by matching distributions of data. The latter method has the major advantages of speed and scalability in terms of size of both training and distilled datasets. We demonstrate that when equipped with an encoder-decoder structure, the empirically successful disentangled methods can be reformulated as an optimal quantization problem, where a finite set of points is found to approximate the underlying probability measure by minimizing the expected projection distance. In particular, we link existing disentangled dataset distillation methods to the classical optimal quantization and Wasserstein barycenter problems, demonstrating consistency of distilled datasets for diffusion-based generative priors. We propose a simple extension of the state-of-the-art data distillation method D4M, achieving better performance on the ImageNet-1K dataset with trivial additional computation, and state-of-the-art performance in higher image-per-class settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07681v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Ye Tan, Emma Slade</dc:creator>
    </item>
    <item>
      <title>ML-assisted Randomization Tests for Detecting Treatment Effects in A/B Experiments</title>
      <link>https://arxiv.org/abs/2501.07722</link>
      <description>arXiv:2501.07722v1 Announce Type: cross 
Abstract: Experimentation is widely utilized for causal inference and data-driven decision-making across disciplines. In an A/B experiment, for example, an online business randomizes two different treatments (e.g., website designs) to their customers and then aims to infer which treatment is better. In this paper, we construct randomization tests for complex treatment effects, including heterogeneity and interference. A key feature of our approach is the use of flexible machine learning (ML) models, where the test statistic is defined as the difference between the cross-validation errors from two ML models, one including the treatment variable and the other without it. This approach combines the predictive power of modern ML tools with the finite-sample validity of randomization procedures, enabling a robust and efficient way to detect complex treatment effects in experimental settings. We demonstrate this combined benefit both theoretically and empirically through applied examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07722v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenxuan Guo, JungHo Lee, Panos Toulis</dc:creator>
    </item>
    <item>
      <title>Impatient Bandits: Optimizing for the Long-Term Without Delay</title>
      <link>https://arxiv.org/abs/2501.07761</link>
      <description>arXiv:2501.07761v1 Announce Type: cross 
Abstract: Increasingly, recommender systems are tasked with improving users' long-term satisfaction. In this context, we study a content exploration task, which we formalize as a bandit problem with delayed rewards. There is an apparent trade-off in choosing the learning signal: waiting for the full reward to become available might take several weeks, slowing the rate of learning, whereas using short-term proxy rewards reflects the actual long-term goal only imperfectly. First, we develop a predictive model of delayed rewards that incorporates all information obtained to date. Rewards as well as shorter-term surrogate outcomes are combined through a Bayesian filter to obtain a probabilistic belief. Second, we devise a bandit algorithm that quickly learns to identify content aligned with long-term success using this new predictive model. We prove a regret bound for our algorithm that depends on the \textit{Value of Progressive Feedback}, an information theoretic metric that captures the quality of short-term leading indicators that are observed prior to the long-term reward. We apply our approach to a podcast recommendation problem, where we seek to recommend shows that users engage with repeatedly over two months. We empirically validate that our approach significantly outperforms methods that optimize for short-term proxies or rely solely on delayed rewards, as demonstrated by an A/B test in a recommendation system that serves hundreds of millions of users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07761v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kelly W. Zhang, Thomas Baldwin-McDonald, Kamil Ciosek, Lucas Maystre, Daniel Russo</dc:creator>
    </item>
    <item>
      <title>PINN-FEM: A Hybrid Approach for Enforcing Dirichlet Boundary Conditions in Physics-Informed Neural Networks</title>
      <link>https://arxiv.org/abs/2501.07765</link>
      <description>arXiv:2501.07765v1 Announce Type: cross 
Abstract: Physics-Informed Neural Networks (PINNs) solve partial differential equations (PDEs) by embedding governing equations and boundary/initial conditions into the loss function. However, enforcing Dirichlet boundary conditions accurately remains challenging, often leading to soft enforcement that compromises convergence and reliability in complex domains. We propose a hybrid approach, PINN-FEM, which combines PINNs with finite element methods (FEM) to impose strong Dirichlet boundary conditions via domain decomposition. This method incorporates FEM-based representations near the boundary, ensuring exact enforcement without compromising convergence. Through six experiments of increasing complexity, PINN-FEM outperforms standard PINN models, showcasing superior accuracy and robustness. While distance functions and similar techniques have been proposed for boundary condition enforcement, they lack generality for real-world applications. PINN-FEM bridges this gap by leveraging FEM near boundaries, making it well-suited for industrial and scientific problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07765v1</guid>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nahil Sobh, Rini Jasmine Gladstone, Hadi Meidani</dc:creator>
    </item>
    <item>
      <title>Black-box Optimization with Simultaneous Statistical Inference for Optimal Performance</title>
      <link>https://arxiv.org/abs/2501.07795</link>
      <description>arXiv:2501.07795v1 Announce Type: cross 
Abstract: Black-box optimization is often encountered for decision-making in complex systems management, where the knowledge of system is limited. Under these circumstances, it is essential to balance the utilization of new information with computational efficiency. In practice, decision-makers often face the dual tasks of optimization and statistical inference for the optimal performance, in order to achieve it with a high reliability. Our goal is to address the dual tasks in an online fashion. Wu et al (2022) [arXiv preprint: 2210.06737] point out that the sample average of performance estimates generated by the optimization algorithm needs not to admit a central limit theorem. We propose an algorithm that not only tackles this issue, but also provides an online consistent estimator for the variance of the performance. Furthermore, we characterize the convergence rate of the coverage probabilities of the asymptotic confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07795v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teng Lian, Jian-Qiang Hu, Yuhang Wu, Zeyu Zheng</dc:creator>
    </item>
    <item>
      <title>Derivation of Output Correlation Inferences for Multi-Output (aka Multi-Task) Gaussian Process</title>
      <link>https://arxiv.org/abs/2501.07964</link>
      <description>arXiv:2501.07964v1 Announce Type: cross 
Abstract: Gaussian process (GP) is arguably one of the most widely used machine learning algorithms in practice. One of its prominent applications is Bayesian optimization (BO). Although the vanilla GP itself is already a powerful tool for BO, it is often beneficial to be able to consider the dependencies of multiple outputs. To do so, Multi-task GP (MTGP) is formulated, but it is not trivial to fully understand the derivations of its formulations and their gradients from the previous literature. This paper serves friendly derivations of the MTGP formulations and their gradients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07964v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhei Watanabe</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of Real-time Recurrent Learning (RTRL) for a class of Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2501.08040</link>
      <description>arXiv:2501.08040v1 Announce Type: cross 
Abstract: Recurrent neural networks (RNNs) are commonly trained with the truncated backpropagation-through-time (TBPTT) algorithm. For the purposes of computational tractability, the TBPTT algorithm truncates the chain rule and calculates the gradient on a finite block of the overall data sequence. Such approximation could lead to significant inaccuracies, as the block length for the truncated backpropagation is typically limited to be much smaller than the overall sequence length. In contrast, Real-time recurrent learning (RTRL) is an online optimization algorithm which asymptotically follows the true gradient of the loss on the data sequence as the number of sequence time steps $t \rightarrow \infty$. RTRL forward propagates the derivatives of the RNN hidden/memory units with respect to the parameters and, using the forward derivatives, performs online updates of the parameters at each time step in the data sequence. RTRL's online forward propagation allows for exact optimization over extremely long data sequences, although it can be computationally costly for models with large numbers of parameters. We prove convergence of the RTRL algorithm for a class of RNNs. The convergence analysis establishes a fixed point for the joint distribution of the data sequence, RNN hidden layer, and the RNN hidden layer forward derivatives as the number of data samples from the sequence and the number of training steps tend to infinity. We prove convergence of the RTRL algorithm to a stationary point of the loss. Numerical studies illustrate our theoretical results. One potential application area for RTRL is the analysis of financial data, which typically involve long time series and models with small to medium numbers of parameters. This makes RTRL computationally tractable and a potentially appealing optimization method for training models. Thus, we include an example of RTRL applied to limit order book data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08040v1</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Chun-Hei Lam, Justin Sirignano, Konstantinos Spiliopoulos</dc:creator>
    </item>
    <item>
      <title>Multiple-Input Variational Auto-Encoder for Anomaly Detection in Heterogeneous Data</title>
      <link>https://arxiv.org/abs/2501.08149</link>
      <description>arXiv:2501.08149v1 Announce Type: cross 
Abstract: Anomaly detection (AD) plays a pivotal role in AI applications, e.g., in classification, and intrusion/threat detection in cybersecurity. However, most existing methods face challenges of heterogeneity amongst feature subsets posed by non-independent and identically distributed (non-IID) data. We propose a novel neural network model called Multiple-Input Auto-Encoder for AD (MIAEAD) to address this. MIAEAD assigns an anomaly score to each feature subset of a data sample to indicate its likelihood of being an anomaly. This is done by using the reconstruction error of its sub-encoder as the anomaly score. All sub-encoders are then simultaneously trained using unsupervised learning to determine the anomaly scores of feature subsets. The final AUC of MIAEAD is calculated for each sub-dataset, and the maximum AUC obtained among the sub-datasets is selected. To leverage the modelling of the distribution of normal data to identify anomalies of the generative models, we develop a novel neural network architecture/model called Multiple-Input Variational Auto-Encoder (MIVAE). MIVAE can process feature subsets through its sub-encoders before learning distribution of normal data in the latent space. This allows MIVAE to identify anomalies that deviate from the learned distribution. We theoretically prove that the difference in the average anomaly score between normal samples and anomalies obtained by the proposed MIVAE is greater than that of the Variational Auto-Encoder (VAEAD), resulting in a higher AUC for MIVAE. Extensive experiments on eight real-world anomaly datasets demonstrate the superior performance of MIAEAD and MIVAE over conventional methods and the state-of-the-art unsupervised models, by up to 6% in terms of AUC score. Alternatively, MIAEAD and MIVAE have a high AUC when applied to feature subsets with low heterogeneity based on the coefficient of variation (CV) score.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08149v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Phai Vu Dinh, Diep N. Nguyen, Dinh Thai Hoang, Quang Uy Nguyen, Eryk Dutkiewicz</dc:creator>
    </item>
    <item>
      <title>Data-driven system identification using quadratic embeddings of nonlinear dynamics</title>
      <link>https://arxiv.org/abs/2501.08202</link>
      <description>arXiv:2501.08202v1 Announce Type: cross 
Abstract: We propose a novel data-driven method called QENDy (Quadratic Embedding of Nonlinear Dynamics) that not only allows us to learn quadratic representations of highly nonlinear dynamical systems, but also to identify the governing equations. The approach is based on an embedding of the system into a higher-dimensional feature space in which the dynamics become quadratic. Just like SINDy (Sparse Identification of Nonlinear Dynamics), our method requires trajectory data, time derivatives for the training data points, which can also be estimated using finite difference approximations, and a set of preselected basis functions, called dictionary. We illustrate the efficacy and accuracy of QENDy with the aid of various benchmark problems and compare its performance with SINDy and a deep learning method for identifying quadratic embeddings. Furthermore, we analyze the convergence of QENDy and SINDy in the infinite data limit, highlight their similarities and main differences, and compare the quadratic embedding with linearization techniques based on the Koopman operator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08202v1</guid>
      <category>math.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Klus, Joel-Pascal N'Konzi</dc:creator>
    </item>
    <item>
      <title>Big Batch Bayesian Active Learning by Considering Predictive Probabilities</title>
      <link>https://arxiv.org/abs/2501.08223</link>
      <description>arXiv:2501.08223v1 Announce Type: cross 
Abstract: We observe that BatchBALD, a popular acquisition function for batch Bayesian active learning for classification, can conflate epistemic and aleatoric uncertainty, leading to suboptimal performance. Motivated by this observation, we propose to focus on the predictive probabilities, which only exhibit epistemic uncertainty. The result is an acquisition function that not only performs better, but is also faster to evaluate, allowing for larger batches than before.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08223v1</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian W. Ober, Samuel Power, Tom Diethe, Henry B. Moss</dc:creator>
    </item>
    <item>
      <title>Multiplayer Federated Learning: Reaching Equilibrium with Less Communication</title>
      <link>https://arxiv.org/abs/2501.08263</link>
      <description>arXiv:2501.08263v1 Announce Type: cross 
Abstract: Traditional Federated Learning (FL) approaches assume collaborative clients with aligned objectives working towards a shared global model. However, in many real-world scenarios, clients act as rational players with individual objectives and strategic behaviors, a concept that existing FL frameworks are not equipped to adequately address. To bridge this gap, we introduce Multiplayer Federated Learning (MpFL), a novel framework that models the clients in the FL environment as players in a game-theoretic context, aiming to reach an equilibrium. In this scenario, each player tries to optimize their own utility function, which may not align with the collective goal. Within MpFL, we propose Per-Player Local Stochastic Gradient Descent (PEARL-SGD), an algorithm in which each player/client performs local updates independently and periodically communicates with other players. We theoretically analyze PEARL-SGD and prove that it reaches a neighborhood of equilibrium with less communication in the stochastic setup compared to its non-local counterpart. Finally, we verify our theoretical findings through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08263v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>TaeHo Yoon, Sayantan Choudhury, Nicolas Loizou</dc:creator>
    </item>
    <item>
      <title>A Similarity Measure Between Functions with Applications to Statistical Learning and Optimization</title>
      <link>https://arxiv.org/abs/2501.08317</link>
      <description>arXiv:2501.08317v1 Announce Type: cross 
Abstract: In this note, we present a novel measure of similarity between two functions. It quantifies how the sub-optimality gaps of two functions convert to each other, and unifies several existing notions of functional similarity. We show that it has convenient operation rules, and illustrate its use in empirical risk minimization and non-stationary online optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08317v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengpiao Huang, Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>Gradient Equilibrium in Online Learning: Theory and Applications</title>
      <link>https://arxiv.org/abs/2501.08330</link>
      <description>arXiv:2501.08330v1 Announce Type: cross 
Abstract: We present a new perspective on online learning that we refer to as gradient equilibrium: a sequence of iterates achieves gradient equilibrium if the average of gradients of losses along the sequence converges to zero. In general, this condition is not implied by nor implies sublinear regret. It turns out that gradient equilibrium is achievable by standard online learning methods such as gradient descent and mirror descent with constant step sizes (rather than decaying step sizes, as is usually required for no regret). Further, as we show through examples, gradient equilibrium translates into an interpretable and meaningful property in online prediction problems spanning regression, classification, quantile estimation, and others. Notably, we show that the gradient equilibrium framework can be used to develop a debiasing scheme for black-box predictions under arbitrary distribution shift, based on simple post hoc online descent updates. We also show that post hoc gradient updates can be used to calibrate predicted quantiles under distribution shift, and that the framework leads to unbiased Elo scores for pairwise preference prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08330v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos, Michael I. Jordan, Ryan J. Tibshirani</dc:creator>
    </item>
    <item>
      <title>Bandit Pareto Set Identification: the Fixed Budget Setting</title>
      <link>https://arxiv.org/abs/2311.03992</link>
      <description>arXiv:2311.03992v2 Announce Type: replace 
Abstract: We study a multi-objective pure exploration problem in a multi-armed bandit model. Each arm is associated to an unknown multi-variate distribution and the goal is to identify the distributions whose mean is not uniformly worse than that of another distribution: the Pareto optimal set. We propose and analyze the first algorithms for the \emph{fixed budget} Pareto Set Identification task. We propose Empirical Gap Elimination, a family of algorithms combining a careful estimation of the ``hardness to classify'' each arm in or out of the Pareto set with a generic elimination scheme. We prove that two particular instances, EGE-SR and EGE-SH, have a probability of error that decays exponentially fast with the budget, with an exponent supported by an information theoretic lower-bound. We complement these findings with an empirical study using real-world and synthetic datasets, which showcase the good performance of our algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03992v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>AISTATS 2024</arxiv:journal_reference>
      <dc:creator>Cyrille Kone, Emilie Kaufmann, Laura Richert</dc:creator>
    </item>
    <item>
      <title>A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation</title>
      <link>https://arxiv.org/abs/2402.03169</link>
      <description>arXiv:2402.03169v3 Announce Type: replace 
Abstract: This work presents a comprehensive understanding of the estimation of a planted low-rank signal from a general spiked tensor model near the computational threshold. Relying on standard tools from the theory of large random matrices, we characterize the large-dimensional spectral behavior of the unfoldings of the data tensor and exhibit relevant signal-to-noise ratios governing the detectability of the principal directions of the signal. These results allow to accurately predict the reconstruction performance of truncated multilinear SVD (MLSVD) in the non-trivial regime. This is particularly important since it serves as an initialization of the higher-order orthogonal iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank approximation depends entirely on its initialization. We give a sufficient condition for the convergence of HOOI and show that the number of iterations before convergence tends to $1$ in the large-dimensional limit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03169v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Lebeau, Florent Chatelain, Romain Couillet</dc:creator>
    </item>
    <item>
      <title>Generative Assignment Flows for Representing and Learning Joint Distributions of Discrete Data</title>
      <link>https://arxiv.org/abs/2406.04527</link>
      <description>arXiv:2406.04527v2 Announce Type: replace 
Abstract: We introduce a novel generative model for the representation of joint probability distributions of a possibly large number of discrete random variables. The approach uses measure transport by randomized assignment flows on the statistical submanifold of factorizing distributions, which enables to represent and sample efficiently from any target distribution and to assess the likelihood of unseen data points. The complexity of the target distribution only depends on the parametrization of the affinity function of the dynamical assignment flow system. Our model can be trained in a simulation-free manner by conditional Riemannian flow matching, using the training data encoded as geodesics on the assignment manifold in closed-form, with respect to the e-connection of information geometry. Numerical experiments devoted to distributions of structured image labelings demonstrate the applicability to large-scale problems, which may include discrete distributions in other application areas. Performance measures show that our approach scales better with the increasing number of classes than recent related work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04527v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bastian Boll, Daniel Gonzalez-Alvarado, Stefania Petra, Christoph Schn\"orr</dc:creator>
    </item>
    <item>
      <title>Counterfactually Fair Reinforcement Learning via Sequential Data Preprocessing</title>
      <link>https://arxiv.org/abs/2501.06366</link>
      <description>arXiv:2501.06366v2 Announce Type: replace 
Abstract: When applied in healthcare, reinforcement learning (RL) seeks to dynamically match the right interventions to subjects to maximize population benefit. However, the learned policy may disproportionately allocate efficacious actions to one subpopulation, creating or exacerbating disparities in other socioeconomically-disadvantaged subgroups. These biases tend to occur in multi-stage decision making and can be self-perpetuating, which if unaccounted for could cause serious unintended consequences that limit access to care or treatment benefit. Counterfactual fairness (CF) offers a promising statistical tool grounded in causal inference to formulate and study fairness. In this paper, we propose a general framework for fair sequential decision making. We theoretically characterize the optimal CF policy and prove its stationarity, which greatly simplifies the search for optimal CF policies by leveraging existing RL algorithms. The theory also motivates a sequential data preprocessing algorithm to achieve CF decision making under an additive noise assumption. We prove and then validate our policy learning approach in controlling unfairness and attaining optimal value through simulations. Analysis of a digital health dataset designed to reduce opioid misuse shows that our proposal greatly enhances fair access to counseling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06366v2</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jitao Wang, Chengchun Shi, John D. Piette, Joshua R. Loftus, Donglin Zeng, Zhenke Wu</dc:creator>
    </item>
    <item>
      <title>Computational and Statistical Asymptotic Analysis of the JKO Scheme for Iterative Algorithms to update distributions</title>
      <link>https://arxiv.org/abs/2501.06408</link>
      <description>arXiv:2501.06408v2 Announce Type: replace 
Abstract: The seminal paper of Jordan, Kinderlehrer, and Otto introduced what is now widely known as the JKO scheme, an iterative algorithmic framework for computing distributions. This scheme can be interpreted as a Wasserstein gradient flow and has been successfully applied in machine learning contexts, such as deriving policy solutions in reinforcement learning. In this paper, we extend the JKO scheme to accommodate models with unknown parameters. Specifically, we develop statistical methods to estimate these parameters and adapt the JKO scheme to incorporate the estimated values. To analyze the adopted statistical JKO scheme, we establish an asymptotic theory via stochastic partial differential equations that describes its limiting dynamic behavior. Our framework allows both the sample size used in parameter estimation and the number of algorithmic iterations to go to infinity. This study offers a unified framework for joint computational and statistical asymptotic analysis of the statistical JKO scheme. On the computational side, we examine the scheme's dynamic behavior as the number of iterations increases, while on the statistical side, we investigate the large-sample behavior of the resulting distributions computed through the scheme. We conduct numerical simulations to evaluate the finite-sample performance of the proposed methods and validate the developed asymptotic theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06408v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shang Wu, Yazhen Wang</dc:creator>
    </item>
    <item>
      <title>A User's Guide to $\texttt{KSig}$: GPU-Accelerated Computation of the Signature Kernel</title>
      <link>https://arxiv.org/abs/2501.07145</link>
      <description>arXiv:2501.07145v2 Announce Type: replace 
Abstract: The signature kernel is a positive definite kernel for sequential and temporal data that has become increasingly popular in machine learning applications due to powerful theoretical guarantees, strong empirical performance, and recently introduced various scalable variations. In this chapter, we give a short introduction to $\texttt{KSig}$, a $\texttt{Scikit-Learn}$ compatible Python package that implements various GPU-accelerated algorithms for computing signature kernels, and performing downstream learning tasks. We also introduce a new algorithm based on tensor sketches which gives strong performance compared to existing algorithms. The package is available at https://github.com/tgcsaba/ksig.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07145v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Csaba T\'oth, Danilo Jr Dela Cruz, Harald Oberhauser</dc:creator>
    </item>
    <item>
      <title>Synthesis and Analysis of Data as Probability Measures with Entropy-Regularized Optimal Transport</title>
      <link>https://arxiv.org/abs/2501.07446</link>
      <description>arXiv:2501.07446v2 Announce Type: replace 
Abstract: We consider synthesis and analysis of probability measures using the entropy-regularized Wasserstein-2 cost and its unbiased version, the Sinkhorn divergence. The synthesis problem consists of computing the barycenter, with respect to these costs, of $m$ reference measures given a set of coefficients belonging to the $m$-dimensional simplex. The analysis problem consists of finding the coefficients for the closest barycenter in the Wasserstein-2 distance to a given measure $\mu$. Under the weakest assumptions on the measures thus far in the literature, we compute the derivative of the entropy-regularized Wasserstein-2 cost. We leverage this to establish a characterization of regularized barycenters as solutions to a fixed-point equation for the average of the entropic maps from the barycenter to the reference measures. This characterization yields a finite-dimensional, convex, quadratic program for solving the analysis problem when $\mu$ is a barycenter. It is shown that these coordinates, as well as the value of the barycenter functional, can be estimated from samples with dimension-independent rates of convergence, a hallmark of entropy-regularized optimal transport, and we verify these rates experimentally. We also establish that barycentric coordinates are stable with respect to perturbations in the Wasserstein-2 metric, suggesting a robustness of these coefficients to corruptions. We employ the barycentric coefficients as features for classification of corrupted point cloud data, and show that compared to neural network baselines, our approach is more efficient in small training data regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07446v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Mallery, James M. Murphy, Shuchin Aeron</dc:creator>
    </item>
    <item>
      <title>Optimal Scaling for the Proximal Langevin Algorithm in High Dimensions</title>
      <link>https://arxiv.org/abs/2204.10793</link>
      <description>arXiv:2204.10793v2 Announce Type: replace-cross 
Abstract: The Metropolis-adjusted Langevin (MALA) algorithm is a sampling algorithm that incorporates the gradient of the logarithm of the target density in its proposal distribution. In an earlier joint work \citet{pill:stu:12}, the author had extended the seminal work of \cite{Robe:Rose:98} and showed that in stationarity, MALA applied to an $N-$dimensional approximation of the target will take ${\cal O}(N^{\frac13})$ steps to explore its target measure. It was also shown that the MALA algorithm is optimized at an average acceptance probability of $0.574$. In \citet{pere:16}, the author introduced the proximal MALA algorithm where the gradient of the log target density is replaced by the proximal function. In this paper, we show that for a wide class of twice differentiable target densities, the proximal MALA enjoys the same optimal scaling as that of MALA in high dimensions and also has an average optimal acceptance probability of $0.574$. The results of this paper thus give the following practically useful guideline: for smooth target densities where it is expensive to compute the gradient while implementing MALA, users may replace the gradient with the corresponding proximal function (that can be often computed relatively cheaply via convex optimization) \emph{without} losing any efficiency gains from optimal scaling. This confirms some of the empirical observations made in \cite{pere:16}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10793v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research, 2025</arxiv:journal_reference>
      <dc:creator>Natesh S. Pillai</dc:creator>
    </item>
    <item>
      <title>Inhomogeneous graph trend filtering via a l2,0 cardinality penalty</title>
      <link>https://arxiv.org/abs/2304.05223</link>
      <description>arXiv:2304.05223v4 Announce Type: replace-cross 
Abstract: We study estimation of piecewise smooth signals over a graph. We propose a $\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimate piecewise smooth graph signals that exhibit inhomogeneous levels of smoothness across the nodes. We prove that the proposed GTF model is simultaneously a k-means clustering on the signal over the nodes and a minimum graph cut on the edges of the graph, where the clustering and the cut share the same assignment matrix. We propose two methods to solve the proposed GTF model: a spectral decomposition method and a method based on simulated annealing. In the experiment on synthetic and real-world datasets, we show that the proposed GTF model has a better performances compared with existing approaches on the tasks of denoising, support recovery and semi-supervised classification. We also show that the proposed GTF model can be solved more efficiently than existing models for the dataset with a large edge set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.05223v4</guid>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoqing Huang, Andersen Ang, Kun Huang, Jie Zhang, Yijie Wang</dc:creator>
    </item>
    <item>
      <title>Improved off-policy training of diffusion samplers</title>
      <link>https://arxiv.org/abs/2402.05098</link>
      <description>arXiv:2402.05098v4 Announce Type: replace-cross 
Abstract: We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion models for amortized inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05098v4</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcin Sendera, Minsu Kim, Sarthak Mittal, Pablo Lemos, Luca Scimeca, Jarrid Rector-Brooks, Alexandre Adam, Yoshua Bengio, Nikolay Malkin</dc:creator>
    </item>
    <item>
      <title>Deep Learning-Based Residual Useful Lifetime Prediction for Assets with Uncertain Failure Modes</title>
      <link>https://arxiv.org/abs/2405.06068</link>
      <description>arXiv:2405.06068v2 Announce Type: replace-cross 
Abstract: Industrial prognostics focuses on utilizing degradation signals to forecast and continually update the residual useful life of complex engineering systems. However, existing prognostic models for systems with multiple failure modes face several challenges in real-world applications, including overlapping degradation signals from multiple components, the presence of unlabeled historical data, and the similarity of signals across different failure modes. To tackle these issues, this research introduces two prognostic models that integrate the mixture (log)-location-scale distribution with deep learning. This integration facilitates the modeling of overlapping degradation signals, eliminates the need for explicit failure mode identification, and utilizes deep learning to capture complex nonlinear relationships between degradation signals and residual useful lifetimes. Numerical studies validate the superior performance of these proposed models compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06068v2</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuqi Su, Xiaolei Fang</dc:creator>
    </item>
    <item>
      <title>CREIMBO: Cross-Regional Ensemble Interactions in Multi-view Brain Observations</title>
      <link>https://arxiv.org/abs/2405.17395</link>
      <description>arXiv:2405.17395v2 Announce Type: replace-cross 
Abstract: Modern recordings of neural activity provide diverse observations of neurons across brain areas, conditions, and subjects; presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics. Current analysis methods often fail to harness the richness of such data, as they provide either uninterpretable representations or oversimplify models (e.g., by assuming stationary dynamics). Here, instead of regarding asynchronous neural recordings that lack alignment in neural identity or brain areas as a limitation, we leverage these diverse views into the brain to learn a unified model of neural dynamics. We assume that brain activity is driven by multiple hidden global sub-circuits. These sub-circuits represent global basis interactions between neural ensembles -- functional groups of neurons -- such that the time-varying decomposition of these circuits defines how the ensembles' interactions evolve over time non-stationarily. We discover the neural ensembles underlying non-simultaneous observations, along with their non-stationary evolving interactions, with our new model, CREIMBO. CREIMBO identifies the hidden composition of per-session neural ensembles through graph-driven dictionary learning and models the ensemble dynamics on a low-dimensional manifold spanned by a sparse time-varying composition of the global sub-circuits. Thus, CREIMBO disentangles overlapping temporal neural processes while preserving interpretability due to the use of a shared underlying sub-circuit basis. Moreover, CREIMBO distinguishes session-specific computations from global (session-invariant) ones by identifying session covariates and variations in sub-circuit activations. We demonstrate CREIMBO's ability to recover true components in synthetic data, and uncover meaningful brain dynamics including cross-subject neural mechanisms and inter- vs. intra-region dynamical motifs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17395v2</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noga Mudrik, Ryan Ly, Oliver Ruebel, Adam S. Charles</dc:creator>
    </item>
    <item>
      <title>Convergence of the denoising diffusion probabilistic models</title>
      <link>https://arxiv.org/abs/2406.01320</link>
      <description>arXiv:2406.01320v4 Announce Type: replace-cross 
Abstract: We theoretically analyze the original version of the denoising diffusion probabilistic models (DDPMs) presented in Ho, J., Jain, A., and Abbeel, P., Advances in Neural Information Processing Systems, 33 (2020), pp. 6840-6851. Our main theorem states that the sequence constructed by the original DDPM sampling algorithm weakly converges to a given data distribution as the number of time steps goes to infinity, under some asymptotic conditions on the parameters for the variance schedule, the $L^2$-based score estimation error, and the noise estimating function with respect to the number of time steps. In proving the theorem, we reveal that the sampling sequence can be seen as an exponential integrator type approximation of a reverse time stochastic differential equation over a finite time interval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01320v4</guid>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yumiharu Nakano</dc:creator>
    </item>
    <item>
      <title>Can Go AIs be adversarially robust?</title>
      <link>https://arxiv.org/abs/2406.12843</link>
      <description>arXiv:2406.12843v3 Announce Type: replace-cross 
Abstract: Prior work found that superhuman Go AIs can be defeated by simple adversarial strategies, especially "cyclic" attacks. In this paper, we study whether adding natural countermeasures can achieve robustness in Go, a favorable domain for robustness since it benefits from incredible average-case capability and a narrow, innately adversarial setting. We test three defenses: adversarial training on hand-constructed positions, iterated adversarial training, and changing the network architecture. We find that though some of these defenses protect against previously discovered attacks, none withstand freshly trained adversaries. Furthermore, most of the reliably effective attacks these adversaries discover are different realizations of the same overall class of cyclic attacks. Our results suggest that building robust AI systems is challenging even with extremely superhuman systems in some of the most tractable settings, and highlight two key gaps: efficient generalization of defenses, and diversity in training. For interactive examples of attacks and a link to our codebase, see https://goattack.far.ai.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12843v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom Tseng, Euan McLean, Kellin Pelrine, Tony T. Wang, Adam Gleave</dc:creator>
    </item>
    <item>
      <title>What type of inference is planning?</title>
      <link>https://arxiv.org/abs/2406.17863</link>
      <description>arXiv:2406.17863v4 Announce Type: replace-cross 
Abstract: Multiple types of inference are available for probabilistic graphical models, e.g., marginal, maximum-a-posteriori, and even marginal maximum-a-posteriori. Which one do researchers mean when they talk about "planning as inference"? There is no consistency in the literature, different types are used, and their ability to do planning is further entangled with specific approximations or additional constraints. In this work we use the variational framework to show that, just like all commonly used types of inference correspond to different weightings of the entropy terms in the variational problem, planning corresponds exactly to a different set of weights. This means that all the tricks of variational inference are readily applicable to planning. We develop an analogue of loopy belief propagation that allows us to perform approximate planning in factored-state Markov decisions processes without incurring intractability due to the exponentially large state space. The variational perspective shows that the previous types of inference for planning are only adequate in environments with low stochasticity, and allows us to characterize each type by its own merits, disentangling the type of inference from the additional approximations that its practical use requires. We validate these results empirically on synthetic MDPs and tasks posed in the International Planning Competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17863v4</guid>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Advances in Neural Information Processing Systems, 2024</arxiv:journal_reference>
      <dc:creator>Miguel L\'azaro-Gredilla, Li Yang Ku, Kevin P. Murphy, Dileep George</dc:creator>
    </item>
    <item>
      <title>Fair CoVariance Neural Networks</title>
      <link>https://arxiv.org/abs/2409.08558</link>
      <description>arXiv:2409.08558v2 Announce Type: replace-cross 
Abstract: Covariance-based data processing is widespread across signal processing and machine learning applications due to its ability to model data interconnectivities and dependencies. However, harmful biases in the data may become encoded in the sample covariance matrix and cause data-driven methods to treat different subpopulations unfairly. Existing works such as fair principal component analysis (PCA) mitigate these effects, but remain unstable in low sample regimes, which in turn may jeopardize the fairness goal. To address both biases and instability, we propose Fair coVariance Neural Networks (FVNNs), which perform graph convolutions on the covariance matrix for both fair and accurate predictions. Our FVNNs provide a flexible model compatible with several existing bias mitigation techniques. In particular, FVNNs allow for mitigating the bias in two ways: first, they operate on fair covariance estimates that remove biases from their principal components; second, they are trained in an end-to-end fashion via a fairness regularizer in the loss function so that the model parameters are tailored to solve the task directly in a fair manner. We prove that FVNNs are intrinsically fairer than analogous PCA approaches thanks to their stability in low sample regimes. We validate the robustness and fairness of our model on synthetic and real-world data, showcasing the flexibility of FVNNs along with the tradeoff between fair and accurate performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08558v2</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Cavallo, Madeline Navarro, Santiago Segarra, Elvin Isufi</dc:creator>
    </item>
    <item>
      <title>Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax</title>
      <link>https://arxiv.org/abs/2410.06993</link>
      <description>arXiv:2410.06993v2 Announce Type: replace-cross 
Abstract: Deep InfoMax (DIM) is a well-established method for self-supervised representation learning (SSRL) based on maximization of the mutual information between the input and the output of a deep neural network encoder. Despite the DIM and contrastive SSRL in general being well-explored, the task of learning representations conforming to a specific distribution (i.e., distribution matching, DM) is still under-addressed. Motivated by the importance of DM to several downstream tasks (including generative modeling, disentanglement, outliers detection and other), we enhance DIM to enable automatic matching of learned representations to a selected prior distribution. To achieve this, we propose injecting an independent noise into the normalized outputs of the encoder, while keeping the same InfoMax training objective. We show that such modification allows for learning uniformly and normally distributed representations, as well as representations of other absolutely continuous distributions. Our approach is tested on various downstream tasks. The results indicate a moderate trade-off between the performance on the downstream tasks and quality of DM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06993v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivan Butakov, Alexander Semenenko, Alexander Tolmachev, Andrey Gladkov, Marina Munkhoeva, Alexey Frolov</dc:creator>
    </item>
    <item>
      <title>Balanced Neural ODEs: nonlinear model order reduction and Koopman operator approximations</title>
      <link>https://arxiv.org/abs/2410.10174</link>
      <description>arXiv:2410.10174v3 Announce Type: replace-cross 
Abstract: Variational Autoencoders (VAEs) are a powerful framework for learning latent representations of reduced dimensionality, while Neural ODEs excel in learning transient system dynamics. This work combines the strengths of both to generate fast surrogate models with adjustable complexity reacting on time-varying inputs signals. By leveraging the VAE's dimensionality reduction using a nonhierarchical prior, our method adaptively assigns stochastic noise, naturally complementing known NeuralODE training enhancements and enabling probabilistic time series modeling. We show that standard Latent ODEs struggle with dimensionality reduction in systems with time-varying inputs. Our approach mitigates this by continuously propagating variational parameters through time, establishing fixed information channels in latent space. This results in a flexible and robust method that can learn different system complexities, e.g. deep neural networks or linear matrices. Hereby, it enables efficient approximation of the Koopman operator without the need for predefining its dimensionality. As our method balances dimensionality reduction and reconstruction accuracy, we call it Balanced Neural ODE (B-NODE). We demonstrate the effectiveness of this methods on several academic and real-world test cases, e.g. a power plant or MuJoCo data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10174v3</guid>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julius Aka, Johannes Brunnemann, J\"org Eiden, Arne Speerforck, Lars Mikelsons</dc:creator>
    </item>
    <item>
      <title>Sharp Matrix Empirical Bernstein Inequalities</title>
      <link>https://arxiv.org/abs/2411.09516</link>
      <description>arXiv:2411.09516v3 Announce Type: replace-cross 
Abstract: We present two sharp empirical Bernstein inequalities for symmetric random matrices with bounded eigenvalues. By sharp, we mean that both inequalities adapt to the unknown variance in a tight manner: the deviation captured by the first-order $1/\sqrt{n}$ term asymptotically matches the matrix Bernstein inequality exactly, including constants, the latter requiring knowledge of the variance. Our first inequality holds for the sample mean of independent matrices, and our second inequality holds for a mean estimator under martingale dependence at stopping times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09516v3</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Rate-In: Information-Driven Adaptive Dropout Rates for Improved Inference-Time Uncertainty Estimation</title>
      <link>https://arxiv.org/abs/2412.07169</link>
      <description>arXiv:2412.07169v3 Announce Type: replace-cross 
Abstract: Accurate uncertainty estimation is crucial for deploying neural networks in risk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a widely used technique for approximating predictive uncertainty by performing stochastic forward passes with dropout during inference. However, using static dropout rates across all layers and inputs can lead to suboptimal uncertainty estimates, as it fails to adapt to the varying characteristics of individual inputs and network layers. Existing approaches optimize dropout rates during training using labeled data, resulting in fixed inference-time parameters that cannot adjust to new data distributions, compromising uncertainty estimates in Monte Carlo simulations.
  In this paper, we propose Rate-In, an algorithm that dynamically adjusts dropout rates during inference by quantifying the information loss induced by dropout in each layer's feature maps. By treating dropout as controlled noise injection and leveraging information-theoretic principles, Rate-In adapts dropout rates per layer and per input instance without requiring ground truth labels. By quantifying the functional information loss in feature maps, we adaptively tune dropout rates to maintain perceptual quality across diverse medical imaging tasks and architectural configurations. Our extensive empirical study on synthetic data and real-world medical imaging tasks demonstrates that Rate-In improves calibration and sharpens uncertainty estimates compared to fixed or heuristic dropout rates without compromising predictive performance. Rate-In offers a practical, unsupervised, inference-time approach to optimizing dropout for more reliable predictive uncertainty estimation in critical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07169v3</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tal Zeevi, Ravid Shwartz-Ziv, Yann LeCun, Lawrence H. Staib, John A. Onofrey</dc:creator>
    </item>
    <item>
      <title>Decoupled Functional Central Limit Theorems for Two-Time-Scale Stochastic Approximation</title>
      <link>https://arxiv.org/abs/2412.17070</link>
      <description>arXiv:2412.17070v3 Announce Type: replace-cross 
Abstract: In two-time-scale stochastic approximation (SA), two iterates are updated at different rates, governed by distinct step sizes, with each update influencing the other. Previous studies have demonstrated that the convergence rates of the error terms for these updates depend solely on their respective step sizes, a property known as decoupled convergence. However, a functional version of this decoupled convergence has not been explored. Our work fills this gap by establishing decoupled functional central limit theorems for two-time-scale SA, offering a more precise characterization of its asymptotic behavior. To achieve these results, we leverage the martingale problem approach and establish tightness as a crucial intermediate step. Furthermore, to address the interdependence between different time scales, we introduce an innovative auxiliary sequence to eliminate the primary influence of the fast-time-scale update on the slow-time-scale update.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17070v3</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuze Han, Xiang Li, Jiadong Liang, Zhihua Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal Sampling for Generalized Linear Model under Measurement Constraint with Surrogate Variables</title>
      <link>https://arxiv.org/abs/2501.00972</link>
      <description>arXiv:2501.00972v2 Announce Type: replace-cross 
Abstract: Measurement-constrained datasets, often encountered in semi-supervised learning, arise when data labeling is costly, time-intensive, or hindered by confidentiality or ethical concerns, resulting in a scarcity of labeled data. In certain cases, surrogate variables are accessible across the entire dataset and can serve as approximations to the true response variable; however, these surrogates often contain measurement errors and thus cannot be directly used for accurate prediction. We propose an optimal sampling strategy that effectively harnesses the available information from surrogate variables. This approach provides consistent estimators under the assumption of a generalized linear model, achieving theoretically lower asymptotic variance than existing optimal sampling algorithms that do not use surrogate data information. By employing the A-optimality criterion from optimal experimental design, our strategy maximizes statistical efficiency. Numerical studies demonstrate that our approach surpasses existing optimal sampling methods, exhibiting reduced empirical mean squared error and enhanced robustness in algorithmic performance. These findings highlight the practical advantages of our strategy in scenarios where measurement constraints exist and surrogates are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00972v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yixin Shen, Yang Ning</dc:creator>
    </item>
    <item>
      <title>Approximation Rates in Fr\'echet Metrics: Barron Spaces, Paley-Wiener Spaces, and Fourier Multipliers</title>
      <link>https://arxiv.org/abs/2501.04023</link>
      <description>arXiv:2501.04023v2 Announce Type: replace-cross 
Abstract: Operator learning is a recent development in the simulation of Partial Differential Equations (PDEs) by means of neural networks. The idea behind this approach is to learn the behavior of an operator, such that the resulting neural network is an (approximate) mapping in infinite-dimensional spaces that is capable of (approximately) simulating the solution operator governed by the PDE. In our work, we study some general approximation capabilities for linear differential operators by approximating the corresponding symbol in the Fourier domain. Analogous to the structure of the class of H\"ormander-Symbols, we consider the approximation with respect to a topology that is induced by a sequence of semi-norms. In that sense, we measure the approximation error in terms of a Fr\'echet metric, and our main result identifies sufficient conditions for achieving a predefined approximation error. Secondly, we then focus on a natural extension of our main theorem, in which we manage to reduce the assumptions on the sequence of semi-norms. Based on existing approximation results for the exponential spectral Barron space, we then present a concrete example of symbols that can be approximated well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04023v2</guid>
      <category>math.NA</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed Abdeljawad, Thomas Dittrich</dc:creator>
    </item>
    <item>
      <title>Monotone Curve Estimation via Convex Duality</title>
      <link>https://arxiv.org/abs/2501.06975</link>
      <description>arXiv:2501.06975v2 Announce Type: replace-cross 
Abstract: A principal curve serves as a powerful tool for uncovering underlying structures of data through 1-dimensional smooth and continuous representations. On the basis of optimal transport theories, this paper introduces a novel principal curve framework constrained by monotonicity with rigorous theoretical justifications. We establish statistical guarantees for our monotone curve estimate, including expected empirical and generalized mean squared errors, while proving the existence of such estimates. These statistical foundations justify adopting the popular early stopping procedure in machine learning to implement our numeric algorithm with neural networks. Comprehensive simulation studies reveal that the proposed monotone curve estimate outperforms competing methods in terms of accuracy when the data exhibits a monotonic structure. Moreover, through two real-world applications on future prices of copper, gold, and silver, and avocado prices and sales volume, we underline the robustness of our curve estimate against variable transformation, further confirming its effective applicability for noisy and complex data sets. We believe that this monotone curve-fitting framework offers significant potential for numerous applications where monotonic relationships are intrinsic or need to be imposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06975v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tongseok Lim, Kyeongsik Nam, Jinwon Sohn</dc:creator>
    </item>
  </channel>
</rss>
