<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Aug 2024 01:47:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 20 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Spectral properties of high dimensional rescaled sample correlation matrices</title>
      <link>https://arxiv.org/abs/2408.09173</link>
      <description>arXiv:2408.09173v1 Announce Type: new 
Abstract: High-dimensional sample correlation matrices are a crucial class of random matrices in multivariate statistical analysis. The central limit theorem (CLT) provides a theoretical foundation for statistical inference. In this paper, assuming that the data dimension increases proportionally with the sample size, we derive the limiting spectral distribution of the matrix $\widehat{\mathbf{R}}_n\mathbf{M}$ and establish the CLTs for the linear spectral statistics (LSS) of $\widehat{\mathbf{R}}_n\mathbf{M}$ in two structures: linear independent component structure and elliptical structure. In contrast to existing literature, our proposed spectral properties do not require $\mathbf{M}$ to be an identity matrix. Moreover, we also derive the joint limiting distribution of LSSs of $\widehat{\mathbf{R}}_n \mathbf{M}_1,\ldots,\widehat{\mathbf{R}}_n \mathbf{M}_K$. As an illustration, an application is given for the CLT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09173v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weijiang Chen, Shurong Zheng, Tingting Zou</dc:creator>
    </item>
    <item>
      <title>A mixture of a normal distribution with random mean and variance -- Examples of inconsistency of maximum likelihood estimates</title>
      <link>https://arxiv.org/abs/2408.09195</link>
      <description>arXiv:2408.09195v1 Announce Type: new 
Abstract: We consider the estimation of the mixing distribution of a normal distribution where both the shift and scale are unobserved random variables. We argue that in general, the model is not identifiable. We give an elegant non-constructive proof that the model is identifiable if the shift parameter is bounded by a known value. However, we argue that the generalized maximum likelihood estimator is inconsistent even if the shift parameter is bounded and the shift and scale parameters are independent. The mixing distribution, however, is identifiable if we have more than one observations per any realization of the latent shift and scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09195v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ya'acov Ritov</dc:creator>
    </item>
    <item>
      <title>On the Impact of Predictor Serial Correlation on the LASSO</title>
      <link>https://arxiv.org/abs/2408.09288</link>
      <description>arXiv:2408.09288v1 Announce Type: new 
Abstract: We explore inference within sparse linear models, focusing on scenarios where both predictors and errors carry serial correlations. We establish a clear link between predictor serial correlation and the finite sample performance of the LASSO, showing that even orthogonal or weakly correlated stationary AR processes can lead to significant spurious correlations due to their serial correlations. To address this challenge, we propose a novel approach named ARMAr-LASSO (ARMA residuals LASSO), which applies the LASSO to predictor time series that have been pre-whitened with ARMA filters and lags of dependent variable. Utilizing the near-epoch dependence framework, we derive both asymptotic results and oracle inequalities for the ARMAr-LASSO, and demonstrate that it effectively reduces estimation errors while also providing an effective forecasting and feature selection strategy. Our findings are supported by extensive simulations and an application to real-world macroeconomic data, which highlight the superior performance of the ARMAr-LASSO for handling sparse linear models in the context of time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09288v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simone Tonini (L'EMbeDS and Institute of Economics, Sant'Anna School of Advanced Studies, Pisa), Francesca Chiaromonte (L'EMbeDS and Institute of Economics, Sant'Anna School of Advanced Studies, Pisa, Dept. of Statistics, The Pennsylvania State University), Alessandro Giovannelli (University of L'Aquila)</dc:creator>
    </item>
    <item>
      <title>Approximate independence of permutation mixtures</title>
      <link>https://arxiv.org/abs/2408.09341</link>
      <description>arXiv:2408.09341v1 Announce Type: new 
Abstract: We prove bounds on statistical distances between high-dimensional exchangeable mixture distributions (which we call permutation mixtures) and their i.i.d. counterparts. Our results are based on a novel method for controlling $\chi^2$ divergences between exchangeable mixtures, which is tighter than the existing methods of moments or cumulants. At a technical level, a key innovation in our proofs is a new Maclaurin-type inequality for elementary symmetric polynomials of variables that sum to zero and an upper bound on permanents of doubly-stochastic positive semidefinite matrices. Our results imply a de Finetti-style theorem (in the language of Diaconis and Freedman, 1987) and general asymptotic results for compound decision problems, generalizing and strengthening a result of Hannan and Robbins (1955).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09341v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanjun Han, Jonathan Niles-Weed</dc:creator>
    </item>
    <item>
      <title>Likelihood inference of the non-stationary Hawkes process with non-exponential kernel</title>
      <link>https://arxiv.org/abs/2408.09710</link>
      <description>arXiv:2408.09710v1 Announce Type: new 
Abstract: The Hawkes process is a popular point process model for event sequences that exhibit temporal clustering. The intensity process of a Hawkes process consists of two components, the baseline intensity and the accumulated excitation effect due to past events, with the latter specified via an excitation kernel. The classical Hawkes process assumes a constant baseline intensity and an exponential excitation kernel. This results in an intensity process that is Markovian, a fact that has been used extensively to establish the strong consistency and asymtpotic normality of maximum likelihood estimators or similar. However, these assumptions can be overly restrictive and unrealistic for modelling the many applications which require the baseline intensity to vary with time and the excitation kernel to have non-exponential decay. However, asymptotic properties of maximum likelihood inference for the parameters specifying the baseline intensity and the self-exciting decay under this setup are substantially more difficult since the resulting intensity process is non-Markovian. To overcome this challenge, we develop an approximation procedure to show the intensity process is asymptotically ergodic in a suitably defined sense. This allows for the identification of an ergodic limit to the likelihood function and its derivatives, as required for obtaining large sample inference under minimal regularity conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09710v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tsz-Kit Jeffrey Kwan, Feng Chen, William Dunsmuir</dc:creator>
    </item>
    <item>
      <title>Non-ergodic inference for stationary-increment harmonizable stable processes</title>
      <link>https://arxiv.org/abs/2408.09950</link>
      <description>arXiv:2408.09950v1 Announce Type: new 
Abstract: We consider the class of stationary-increment harmonizable stable processes with infinite control measure, which most notably includes real harmonizable fractional stable motions. We give conditions for the integrability of the paths of such processes with respect to a finite, absolutely continuous measure and derive the distributional characteristics of the path integral with respect to said measure. The convolution of the path of a stationary-increment harmonizable stable process with a suitable measure yields a real stationary harmonizable stable process with finite control measure. This allows us to construct consistent estimators for the index of stability as well as the kernel function in the integral representation of a stationary increment harmonizable stable process (up to a constant factor). For real harmonizable fractional stable motions consistent estimators for the index of stability and its Hurst parameter are given. These are computed directly from the periodogram frequency estimates of the smoothed process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09950v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ly Viet Hoang, Evgeny Spodarev</dc:creator>
    </item>
    <item>
      <title>Seriation of T{\oe}plitz and latent position matrices: optimal rates and computational trade-offs</title>
      <link>https://arxiv.org/abs/2408.10004</link>
      <description>arXiv:2408.10004v1 Announce Type: new 
Abstract: In this paper, we consider the problem of seriation of a permuted structured matrix based on noisy observations. The entries of the matrix relate to an expected quantification of interaction between two objects: the higher the value, the closer the objects. A popular structured class for modelling such matrices is the permuted Robinson class, namely the set of matrices whose coefficients are decreasing away from its diagonal, up to a permutation of its lines and columns. We consider in this paper two submodels of Robinson matrices: the T{\oe}plitz model, and the latent position model. We provide a computational lower bound based on the low-degree paradigm, which hints that there is a statistical-computational gap for seriation when measuring the error based on the Frobenius norm. We also provide a simple and polynomial-time algorithm that achives this lower bound. Along the way, we also characterize the information-theory optimal risk thereby giving evidence for the extent of the computation/information gap for this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10004v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cl\'ement Berenfeld, Alexandra Carpentier, Nicolas Verzelen</dc:creator>
    </item>
    <item>
      <title>The local Fr\'echet regression for random objects with circular predictors</title>
      <link>https://arxiv.org/abs/2408.10118</link>
      <description>arXiv:2408.10118v1 Announce Type: new 
Abstract: Fr\'echet regression extends the principles of linear regression to accommodate responses valued in generic metric spaces. While this approach has primarily focused on exploring relationships between Euclidean predictors and non-Euclidean responses, our work introduces a novel statistical method for handling random objects with circular predictors. We concentrate on local constant and local linear Fr\'echet regression, providing rigorous proofs for the upper bounds of both bias and stochastic deviation of the estimators under mild conditions. This research lays the groundwork for broadening the application of Fr\'echet regression to scenarios involving non-Euclidean covariates, thereby expanding its utility in complex data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10118v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chang Jun Im, Jeong Min Jeon</dc:creator>
    </item>
    <item>
      <title>Optimal transport natural gradient for statistical manifolds with continuous sample space</title>
      <link>https://arxiv.org/abs/1805.08380</link>
      <description>arXiv:1805.08380v4 Announce Type: cross 
Abstract: We study the Wasserstein natural gradient in parametric statistical models with continuous sample spaces. Our approach is to pull back the $L^2$-Wasserstein metric tensor in the probability density space to a parameter space, equipping the latter with a positive definite metric tensor, under which it becomes a Riemannian manifold, named the Wasserstein statistical manifold. In general, it is not a totally geodesic sub-manifold of the density space, and therefore its geodesics will differ from the Wasserstein geodesics, except for the well-known Gaussian distribution case, a fact which can also be validated under our framework. We use the sub-manifold geometry to derive a gradient flow and natural gradient descent method in the parameter space. When parametrized densities lie in $\bR$, the induced metric tensor establishes an explicit formula. In optimization problems, we observe that the natural gradient descent outperforms the standard gradient descent when the Wasserstein distance is the objective function. In such a case, we prove that the resulting algorithm behaves similarly to the Newton method in the asymptotic regime. The proof calculates the exact Hessian formula for the Wasserstein distance, which further motivates another preconditioner for the optimization process. To the end, we present examples to illustrate the effectiveness of the natural gradient in several parametric statistical models, including the Gaussian measure, Gaussian mixture, Gamma distribution, and Laplace distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:1805.08380v4</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Chen, Wuchen Li</dc:creator>
    </item>
    <item>
      <title>Learning Robust Treatment Rules for Censored Data</title>
      <link>https://arxiv.org/abs/2408.09155</link>
      <description>arXiv:2408.09155v1 Announce Type: cross 
Abstract: There is a fast-growing literature on estimating optimal treatment rules directly by maximizing the expected outcome. In biomedical studies and operations applications, censored survival outcome is frequently observed, in which case the restricted mean survival time and survival probability are of great interest. In this paper, we propose two robust criteria for learning optimal treatment rules with censored survival outcomes; the former one targets at an optimal treatment rule maximizing the restricted mean survival time, where the restriction is specified by a given quantile such as median; the latter one targets at an optimal treatment rule maximizing buffered survival probabilities, where the predetermined threshold is adjusted to account the restricted mean survival time. We provide theoretical justifications for the proposed optimal treatment rules and develop a sampling-based difference-of-convex algorithm for learning them. In simulation studies, our estimators show improved performance compared to existing methods. We also demonstrate the proposed method using AIDS clinical trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09155v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Cui, Junyi Liu, Tao Shen, Zhengling Qi, Xi Chen</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Inference for Double/Debiased Machine Learning of Causal Parameters</title>
      <link>https://arxiv.org/abs/2408.09598</link>
      <description>arXiv:2408.09598v1 Announce Type: cross 
Abstract: Double (debiased) machine learning (DML) has seen widespread use in recent years for learning causal/structural parameters, in part due to its flexibility and adaptability to high-dimensional nuisance functions as well as its ability to avoid bias from regularization or overfitting. However, the classic double-debiased framework is only valid asymptotically for a predetermined sample size, thus lacking the flexibility of collecting more data if sharper inference is needed, or stopping data collection early if useful inferences can be made earlier than expected. This can be of particular concern in large scale experimental studies with huge financial costs or human lives at stake, as well as in observational studies where the length of confidence of intervals do not shrink to zero even with increasing sample size due to partial identifiability of a structural parameter. In this paper, we present time-uniform counterparts to the asymptotic DML results, enabling valid inference and confidence intervals for structural parameters to be constructed at any arbitrary (possibly data-dependent) stopping time. We provide conditions which are only slightly stronger than the standard DML conditions, but offer the stronger guarantee for anytime-valid inference. This facilitates the transformation of any existing DML method to provide anytime-valid guarantees with minimal modifications, making it highly adaptable and easy to use. We illustrate our procedure using two instances: a) local average treatment effect in online experiments with non-compliance, and b) partial identification of average treatment effect in observational studies with potential unmeasured confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09598v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinandan Dalal, Patrick Bl\"obaum, Shiva Kasiviswanathan, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Shift-Dispersion Decompositions of Wasserstein and Cram\'er Distances</title>
      <link>https://arxiv.org/abs/2408.09770</link>
      <description>arXiv:2408.09770v1 Announce Type: cross 
Abstract: Divergence functions are measures of distance or dissimilarity between probability distributions that serve various purposes in statistics and applications. We propose decompositions of Wasserstein and Cram\'er distances$-$which compare two distributions by integrating over their differences in distribution or quantile functions$-$into directed shift and dispersion components. These components are obtained by dividing the differences between the quantile functions into contributions arising from shift and dispersion, respectively. Our decompositions add information on how the distributions differ in a condensed form and consequently enhance the interpretability of the underlying divergences. We show that our decompositions satisfy a number of natural properties and are unique in doing so in location-scale families. The decompositions allow to derive sensitivities of the divergence measures to changes in location and dispersion, and they give rise to weak stochastic order relations that are linked to the usual stochastic and the dispersive order. Our theoretical developments are illustrated in two applications, where we focus on forecast evaluation of temperature extremes and on the design of probabilistic surveys in economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09770v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Resin, Daniel Wolffram, Johannes Bracher, Timo Dimitriadis</dc:creator>
    </item>
    <item>
      <title>Predicting path-dependent processes by deep learning</title>
      <link>https://arxiv.org/abs/2408.09941</link>
      <description>arXiv:2408.09941v1 Announce Type: cross 
Abstract: In this paper, we investigate a deep learning method for predicting path-dependent processes based on discretely observed historical information. This method is implemented by considering the prediction as a nonparametric regression and obtaining the regression function through simulated samples and deep neural networks. When applying this method to fractional Brownian motion and the solutions of some stochastic differential equations driven by it, we theoretically proved that the $L_2$ errors converge to 0, and we further discussed the scope of the method. With the frequency of discrete observations tending to infinity, the predictions based on discrete observations converge to the predictions based on continuous observations, which implies that we can make approximations by the method. We apply the method to the fractional Brownian motion and the fractional Ornstein-Uhlenbeck process as examples. Comparing the results with the theoretical optimal predictions and taking the mean square error as a measure, the numerical simulations demonstrate that the method can generate accurate results. We also analyze the impact of factors such as prediction period, Hurst index, etc. on the accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09941v1</guid>
      <category>stat.ML</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xudong Zheng, Yuecai Han</dc:creator>
    </item>
    <item>
      <title>Analysis of The Limiting Spectral Distribution of Large Random Matrices of The Mar\v{c}enko-Pastur Type</title>
      <link>https://arxiv.org/abs/2408.10068</link>
      <description>arXiv:2408.10068v1 Announce Type: cross 
Abstract: Consider the random matrix \(\bW_n = \bB_n + n^{-1}\bX_n^*\bA_n\bX_n\), where \(\bA_n\) and \(\bB_n\) are Hermitian matrices of dimensions \(p \times p\) and \(n \times n\), respectively, and \(\bX_n\) is a \(p \times n\) random matrix with independent and identically distributed entries of mean 0 and variance 1. Assume that \(p\) and \(n\) grow to infinity proportionally, and that the spectral measures of \(\bA_n\) and \(\bB_n\) converge as \(p, n \to \infty\) towards two probability measures \(\calA\) and \(\calB\). Building on the groundbreaking work of \cite{marchenko1967distribution}, which demonstrated that the empirical spectral distribution of \(\bW_n\) converges towards a probability measure \(F\) characterized by its Stieltjes transform, this paper investigates the properties of \(F\) when \(\calB\) is a general measure. We show that \(F\) has an analytic density at the region near where the Stieltjes transform of $\calB$ is bounded. The density closely resembles \(C\sqrt{|x - x_0|}\) near certain edge points \(x_0\) of its support for a wide class of \(\calA\) and \(\calB\). We provide a complete characterization of the support of \(F\). Moreover, we show that \(F\) can exhibit discontinuities at points where \(\calB\) is discontinuous.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10068v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haoran Li</dc:creator>
    </item>
    <item>
      <title>Robust spectral clustering with rank statistics</title>
      <link>https://arxiv.org/abs/2408.10136</link>
      <description>arXiv:2408.10136v1 Announce Type: cross 
Abstract: This paper analyzes the statistical performance of a robust spectral clustering method for latent structure recovery in noisy data matrices. We consider eigenvector-based clustering applied to a matrix of nonparametric rank statistics that is derived entrywise from the raw, original data matrix. This approach is robust in the sense that, unlike traditional spectral clustering procedures, it can provably recover population-level latent block structure even when the observed data matrix includes heavy-tailed entries and has a heterogeneous variance profile.
  Our main theoretical contributions are threefold and hold under flexible data generating conditions. First, we establish that robust spectral clustering with rank statistics can consistently recover latent block structure, viewed as communities of nodes in a graph, in the sense that unobserved community memberships for all but a vanishing fraction of nodes are correctly recovered with high probability when the data matrix is large. Second, we refine the former result and further establish that, under certain conditions, the community membership of any individual, specified node of interest can be asymptotically exactly recovered with probability tending to one in the large-data limit. Third, we establish asymptotic normality results associated with the truncated eigenstructure of matrices whose entries are rank statistics, made possible by synthesizing contemporary entrywise matrix perturbation analysis with the classical nonparametric theory of so-called simple linear rank statistics. Collectively, these results demonstrate the statistical utility of rank-based data transformations when paired with spectral techniques for dimensionality reduction. Additionally, for a dataset of human connectomes, our approach yields parsimonious dimensionality reduction and improved recovery of ground-truth neuroanatomical cluster structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10136v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Cape, Xianshi Yu, Jonquil Z. Liao</dc:creator>
    </item>
    <item>
      <title>Estimating large causal polytrees from small samples</title>
      <link>https://arxiv.org/abs/2209.07028</link>
      <description>arXiv:2209.07028v4 Announce Type: replace-cross 
Abstract: We consider the problem of estimating a large causal polytree from a relatively small i.i.d. sample. This is motivated by the problem of determining causal structure when the number of variables is very large compared to the sample size, such as in gene regulatory networks. We give an algorithm that recovers the tree with high accuracy in such settings. The algorithm works under essentially no distributional or modeling assumptions other than some mild non-degeneracy conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.07028v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourav Chatterjee, Mathukumalli Vidyasagar</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation via Partial Derivatives</title>
      <link>https://arxiv.org/abs/2209.07672</link>
      <description>arXiv:2209.07672v2 Announce Type: replace-cross 
Abstract: Traditional nonparametric estimation methods often lead to a slow convergence rate in large dimensions and require unrealistically enormous sizes of datasets for reliable conclusions. We develop an approach based on partial derivatives, either observed or estimated, to effectively estimate the function at near-parametric convergence rates. The novel approach and computational algorithm could lead to methods useful to practitioners in many areas of science and engineering. Our theoretical results reveal a behavior universal to this class of nonparametric estimation problems. We explore a general setting involving tensor product spaces and build upon the smoothing spline analysis of variance (SS-ANOVA) framework. For $d$-dimensional models under full interaction, the optimal rates with gradient information on $p$ covariates are identical to those for the $(d-p)$-interaction models without gradients and, therefore, the models are immune to the "curse of interaction." For additive models, the optimal rates using gradient information are root-$n$, thus achieving the "parametric rate." We demonstrate aspects of the theoretical results through synthetic and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.07672v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaowu Dai</dc:creator>
    </item>
    <item>
      <title>Unbiased Test Error Estimation in the Poisson Means Problem via Coupled Bootstrap Techniques</title>
      <link>https://arxiv.org/abs/2212.01943</link>
      <description>arXiv:2212.01943v3 Announce Type: replace-cross 
Abstract: We propose a coupled bootstrap (CB) method for the test error of an arbitrary algorithm that estimates the mean in a Poisson sequence, often called the Poisson means problem. The idea behind our method is to generate two carefully-designed data vectors from the original data vector, by using synthetic binomial noise. One such vector acts as the training sample and the second acts as the test sample. To stabilize the test error estimate, we average this over multiple bootstrap B of the synthetic noise. A key property of the CB estimator is that it is unbiased for the test error in a Poisson problem where the original mean has been shrunken by a small factor, driven by the success probability $p$ in the binomial noise. Further, in the limit as $B \to \infty$ and $p \to 0$, we show that the CB estimator recovers a known unbiased estimator for test error based on Hudson's lemma, under no assumptions on the given algorithm for estimating the mean (in particular, no smoothness assumptions). Our methodology applies to two central loss functions that can be used to define test error: Poisson deviance and squared loss. Via a bias-variance decomposition, for each loss function, we analyze the effects of the binomial success probability and the number of bootstrap samples and on the accuracy of the estimator. We also investigate our method empirically across a variety of settings, using simulated as well as real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.01943v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia L. Oliveira, Jing Lei, Ryan J. Tibshirani</dc:creator>
    </item>
  </channel>
</rss>
