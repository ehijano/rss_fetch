<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Apr 2025 04:00:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>HyDra: SOT-CAM Based Vector Symbolic Macro for Hyperdimensional Computing</title>
      <link>https://arxiv.org/abs/2504.14020</link>
      <description>arXiv:2504.14020v1 Announce Type: new 
Abstract: Hyperdimensional computing (HDC) is a brain-inspired paradigm valued for its noise robustness, parallelism, energy efficiency, and low computational overhead. Hardware accelerators are being explored to further enhance its performance, but current solutions are often limited by application specificity and the latency of encoding and similarity search. This paper presents a generalized, reconfigurable on-chip training and inference architecture for HDC, utilizing spin-orbit-torque magnetic (SOT-MRAM) content-addressable memory (CAM). The proposed SOT-CAM array integrates storage and computation, enabling in-memory execution of key HDC operations: binding (bitwise multiplication), permutation (bit rotation), and efficient similarity search. To mitigate interconnect parasitic effect in similarity search, a four-stage voltage scaling scheme has been proposed to ensure accurate Hamming distance representation. Additionally, a novel bit drop method replaces bit rotation during read operations, and an HDC-specific adder reduces energy and area by 1.51x and 1.43x, respectively. Benchmarked at 7nm, the architecture achieves energy reductions of 21.5x, 552.74x, 1.45x, and 282.57x for addition, permutation, multiplication, and search operations, respectively, compared to CMOS-based HDC. Against state-of-the-art HD accelerators, it achieves a 2.27x lower energy consumption and outperforms CPU and eGPU implementations by 2702x and 23161x, respectively, with less than 3% drop in accuracy</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14020v1</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Md Mizanur Rahaman Nayan, Che-Kai Liu, Zishen Wan, Arijit Raychowdhury, Azad J Naeemi</dc:creator>
    </item>
    <item>
      <title>End-Edge Model Collaboration: Bandwidth Allocation for Data Upload and Model Transmission</title>
      <link>https://arxiv.org/abs/2504.14310</link>
      <description>arXiv:2504.14310v1 Announce Type: new 
Abstract: The widespread adoption of large artificial intelligence (AI) models has enabled numerous applications of the Internet of Things (IoT). However, large AI models require substantial computational and memory resources, which exceed the capabilities of resource-constrained IoT devices. End-edge collaboration paradigm is developed to address this issue, where a small model on the end device performs inference tasks, while a large model on the edge server assists with model updates. To improve the accuracy of the inference tasks, the data generated on the end devices will be periodically uploaded to edge serve to update model, and a distilled model of the updated one will be transmitted back to the end device. Subjected to the limited bandwidth for the communication link between the end device and the edge server, it is important to investigate the system should allocate more bandwidth for data upload or model transmission. In this paper, we characterize the impact of data upload and model transmission on inference accuracy. Subsequently, we formulate a bandwidth allocation problem. By solving this problem, we derive an efficient optimization framework for the end-edge collaboration system. The simulation results demonstrate our framework significantly enhances mean average precision (mAP) under various bandwidths and datasizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14310v1</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dailin Yang, Shuhang Zhang, Hongliang Zhang, Lingyang Song</dc:creator>
    </item>
    <item>
      <title>Charging While Driving Lanes: A Boon to Electric Vehicle Owners or a Disruption to Traffic Flow</title>
      <link>https://arxiv.org/abs/2504.14360</link>
      <description>arXiv:2504.14360v1 Announce Type: new 
Abstract: Large-scale adoption of commercial and personal Electric Vehicles (EVs) is expected to significantly affect traffic flow dynamics, emissions, and energy consumption in the transportation sector. Range anxiety and challenges associated with charging EVs are among the key issues that reduce the adoption rate of EVs and, in turn, limit their system-level impacts. A promising solution to address these challenges is the introduction of charging while driving (CWD) lanes. Although technological advancements have made it possible to charge vehicles wirelessly while driving, introducing such lanes to the traffic stream can potentially disturb traffic flow and result in new congestion patterns. This study puts forward a framework to investigate the effects of CWD lanes on traffic flow, considering %autonomy, speed harmonization, and environmental factors for different market penetration rates (MPRs) of personal and commercial EVs. Different policies have been investigated to suggest the best design for CWD lanes. Results indicate that introducing CWD lanes can decrease overall traffic throughput and increase congestion due to additional lane-changing maneuvers by electric vehicles aiming to utilize the CWD lane. Although higher MPRs of EVs help stabilize traffic flow and reduce the number of shockwaves, speed disruption tends to increase in the CWD lane and propagate to adjacent lanes. Emission analyses show significant reductions (up to 63\%) in pollution levels with increasing MPRs of personal and commercial EVs. Our analysis shows that while CWD lanes can facilitate the adoption of EVs, they can deteriorate traffic efficiency, emphasizing the importance of careful design and policy considerations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14360v1</guid>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayan Bafandkar, Alireza Talebpour</dc:creator>
    </item>
    <item>
      <title>A Bio-inspired Asymmetric Double-Gate Ferroelectric FET for Emulating Astrocyte and Dendrite Dynamics in Neuromorphic Systems</title>
      <link>https://arxiv.org/abs/2504.14466</link>
      <description>arXiv:2504.14466v1 Announce Type: new 
Abstract: Neuromorphic systems seek to replicate the functionalities of biological neural networks to attain significant improvements in performance and efficiency of AI computing platforms. However, these systems have generally remained limited to emulation of simple neurons and synapses; and ignored higher order functionalities enabled by other components of the brain like astrocytes and dendrites. In this work, drawing inspiration from biology, we introduce a compact Double-Gate Ferroelectric Field Effect Transistor (DG-FeFET) cell that can emulate the dynamics of both astrocytes and dendrites within neuromorphic architectures. We demonstrate that with a ferroelectric top gate for synaptic weight programming as in conventional synapses and a non-ferroelectric back gate, the DG-FeFET realizes a synapse with a dynamic gain modulation mechanism. This can be leveraged as an analog for a compact astrocyte-tripartite synapse, as well as enabling dendrite-like gain modulation operations. By employing a fully-depleted silicon-on-insulator (FDSOI) FeFET as our double-gate device, we validate the linear control of the synaptic weight via the back gate terminal (i.e., the gate underneath the buried oxide (BOX) layer) through comprehensive theoretical and experimental studies. We showcase the promise such a tripartite synaptic device holds for numerous important neuromorphic applications, including autonomous self-repair of faulty neuromorphic hardware mediated by astrocytic functionality. Coordinate transformations based on dragonfly prey-interception circuitry models are also demonstrated based on dendritic function emulation by the device. This work paves the way forward for developing truly "brain-like" neuromorphic hardware that go beyond the current dogma focusing only on neurons and synapses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14466v1</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhouhang Jiang, A N M Nafiul Islam, Zhuangyu Han, Zijian Zhao, Franz M\"uller, Jiahui Duan, Halid Mulaosmanovic, Stefan D\"unkel, Sven Beyer, Sourav Dutta, Vijaykrishnan Narayanan, Thomas K\"ampfe, Suma George Cardwell, Frances Chance, Abhronil Sengupta, Kai Ni</dc:creator>
    </item>
    <item>
      <title>Rethinking trust in the digital age: An investigation of zero trust architecture's social consequences on organizational culture, collaboration, and knowledge sharing</title>
      <link>https://arxiv.org/abs/2504.14601</link>
      <description>arXiv:2504.14601v1 Announce Type: new 
Abstract: As cyber threats escalate, Zero Trust Architecture (ZTA) replaces outdated perimeter security with strict never trust, always verify protocols. However, ZTA's dual nature as both technical infrastructure and social intervention creates an unresolved tension: its very mechanisms for security may systematically erode the trust foundations enabling effective collaboration. This integrative research combines case study analysis, employee surveys, and social network mapping reveals how ZTA disrupts knowledge-sharing, disproportionately hindering low-altruism employees, while surveillance erodes collective psychological ownership. Networked organizations, reliant on fluid trust, face fragmentation risks. Mitigation strategies include adaptive authorization frameworks using behavioral analytics and transparent communication reframing security as shared responsibility. Interdepartmental collaboration in security design preserves organizational trust structures identified through sociometric mapping. This research provides a framework balancing technical rigor with cultural sensitivity, proving cybersecurity can coexist with innovation by aligning verification with organizational psychology. The findings pioneer a paradigm where security and trust evolve synergistically critical for digital resilience in hybrid work environments. Future security must harmonize protocols with trust cultivation, ensuring defenses adapt to social dynamics driving modern enterprises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14601v1</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ganiyu Oladimeji</dc:creator>
    </item>
    <item>
      <title>VoxLogicA UI: Supporting Declarative Medical Image Analysis</title>
      <link>https://arxiv.org/abs/2504.13846</link>
      <description>arXiv:2504.13846v1 Announce Type: cross 
Abstract: This Master's Thesis in Computer Science dives into the design and creation of a user-friendly interface for VoxLogicA, an image analysis tool using spatial model checking with a focus on neuroimaging. The research tackles the problem of existing tools being too complex, which makes them hard for medical professionals and researchers to use. By using spatial logic, the goal is to make these powerful analytical tools more practical and accessible in real-world clinical settings. The main objectives are to design a modern web interface that's easy to use, build it with the latest web technologies (e.g. Svelte and Niivue), and test its effectiveness through user studies and real-world case analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13846v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <category>cs.GR</category>
      <category>cs.SE</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Strippoli</dc:creator>
    </item>
    <item>
      <title>From Interaction to Collaboration: How Hybrid Intelligence Enhances Chatbot Feedback</title>
      <link>https://arxiv.org/abs/2504.13848</link>
      <description>arXiv:2504.13848v1 Announce Type: cross 
Abstract: Generative AI (GenAI) chatbots are becoming increasingly integrated into virtual assistant technologies, yet their success hinges on the ability to gather meaningful user feedback to improve interaction quality, system outcomes, and overall user acceptance. Successful chatbot interactions can enable organizations to build long-term relationships with their customers and users, supporting customer loyalty and furthering the organization's goals. This study explores the impact of two distinct narratives and feedback collection mechanisms on user engagement and feedback behavior: a standard AI-focused interaction versus a hybrid intelligence (HI) framed interaction. Initial findings indicate that while small-scale survey measures allowed for no significant differences in user willingness to leave feedback, use the system, or trust the system, participants exposed to the HI narrative statistically significantly provided more detailed feedback. These initial findings offer insights into designing effective feedback systems for GenAI virtual assistants, balancing user effort with system improvement potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13848v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Janet Rafner, Ryan Q. Guloy, Eden W. Wen, Catherine M. Chiodo, Jacob Sherson</dc:creator>
    </item>
    <item>
      <title>Bio-crafting Architecture: Experiences of growing mycelium in minimal surface molds</title>
      <link>https://arxiv.org/abs/2504.13855</link>
      <description>arXiv:2504.13855v1 Announce Type: cross 
Abstract: This study documents a three-week workshop with architecture students, where we designed and 3D printed various minimal surfaces using wood-based filaments, and used them as molds in which to grow mycelium. We detail the design process and the growth of the mycelium in different shapes, together with participants' experiences of working with a living material. After exhibiting the results of the work in a public-facing exhibition, we conducted interviews with members of the general public about their perceptions on interacting with a material such as mycelium in design. Our findings show that 3D-printed minimal surfaces with wood-based filaments can function as structural cores for mycelium-based composites and mycelium binds to the filament. Participants in the workshop exhibited stronger feelings for living materials compared to non-living ones, displaying both biophilia and, to a lesser extent, biophobia when interacting with the mycelium. Members of the general public discuss pragmatic aspects including mold, fragility, or production costs, and speculate on the future of bio-technology and its impact on everyday life. While all are positive about the impact on bio-technologies on the future, they have diverging opinions on how much ethical considerations should influence research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13855v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anca-Simona Horvath, Alina Elena Voinea, Radu Arie\c{s}an</dc:creator>
    </item>
    <item>
      <title>Intelligence of Things: A Spatial Context-Aware Control System for Smart Devices</title>
      <link>https://arxiv.org/abs/2504.13942</link>
      <description>arXiv:2504.13942v1 Announce Type: cross 
Abstract: This paper introduces Intelligence of Things (INOT), a novel spatial context-aware control system that enhances smart home automation through intuitive spatial reasoning. Current smart home systems largely rely on device-specific identifiers, limiting user interaction to explicit naming conventions rather than natural spatial references. INOT addresses this limitation through a modular architecture that integrates Vision Language Models with IoT control systems to enable natural language commands with spatial context (e.g., "turn on the light near the window"). The system comprises key components including an Onboarding Inference Engine, Zero-Shot Device Detection, Spatial Topology Inference, and Intent-Based Command Synthesis. A comprehensive user study with 15 participants demonstrated INOT's significant advantages over conventional systems like Google Home Assistant, with users reporting reduced cognitive workload (NASA-TLX scores decreased by an average of 13.17 points), higher ease-of-use ratings, and stronger preference (14 out of 15 participants). By eliminating the need to memorize device identifiers and enabling context-aware spatial commands, INOT represents a significant advancement in creating more intuitive and accessible smart home control systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13942v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sukanth Kalivarathan, Muhmmad Abrar Raja Mohamed, Aswathy Ravikumar, S Harini</dc:creator>
    </item>
    <item>
      <title>The Future of Internet of Things and Multimodal Language Models in 6G Networks: Opportunities and Challenges</title>
      <link>https://arxiv.org/abs/2504.13971</link>
      <description>arXiv:2504.13971v1 Announce Type: cross 
Abstract: Based on recent trends in artificial intelligence and IoT research. The cooperative potential of integrating the Internet of Things (IoT) and Multimodal Language Models (MLLMs) is presented in this survey paper for future 6G systems. It focuses on the applications of this integration in different fields, such as healthcare, agriculture, and smart cities, and investigates the four pillars of IoT integration, such as sensors, communication, processing, and security. The paper provides a comprehensive description of IoT and MLLM technologies and applications, addresses the role of multimodality in each pillar, and concludes with an overview of the most significant challenges and directions for future research. The general survey is a roadmap for researchers interested in tracing the application areas of MLLMs and IoT, highlighting the potential and challenges in this rapidly growing field. The survey recognizes the need to deal with data availability, computational expense, privacy, and real-time processing to harness the complete potential of IoT, MLLM, and 6G technology</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13971v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abdelrahman Soliman</dc:creator>
    </item>
    <item>
      <title>ScaloWork: Useful Proof-of-Work with Distributed Pool Mining</title>
      <link>https://arxiv.org/abs/2504.14328</link>
      <description>arXiv:2504.14328v1 Announce Type: cross 
Abstract: Bitcoin blockchain uses hash-based Proof-of-Work (PoW) that prevents unwanted participants from hogging the network resources. Anyone entering the mining game has to prove that they have expended a specific amount of computational power. However, the most popular Bitcoin blockchain consumes 175.87 TWh of electrical energy annually, and most of this energy is wasted on hash calculations, which serve no additional purpose. Several studies have explored re-purposing the wasted energy by replacing the hash function with meaningful computational problems that have practical applications. Minimum Dominating Set (MDS) in networks has numerous real-life applications. Building on this concept, Chrisimos [TrustCom '23] was proposed to replace hash-based PoW with the computation of a dominating set on real-life graph instances. However, Chrisimos has several drawbacks regarding efficiency and solution quality. This work presents a new framework for Useful PoW, ScaloWork, that decides the block proposer for the Bitcoin blockchain based on the solution for the dominating set problem. ScaloWork relies on the property of graph isomorphism and guarantees solution extractability. We also propose a distributed approach for calculating the dominating set, allowing miners to collaborate in a pool. This enables ScaloWork to handle larger graphs relevant to real-life applications, thereby enhancing scalability. Our framework also eliminates the problem of free-riders, ensuring fairness in the distribution of block rewards. We perform a detailed security analysis of our framework and prove our scheme as secure as hash-based PoW. We implement a prototype of our framework, and the results show that our system outperforms Chrisimos in all aspects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14328v1</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diptendu Chatterjee, Avishek Majumder, Subhra Mazumdar</dc:creator>
    </item>
    <item>
      <title>Decentralization in PoS Blockchain Consensus: Quantification and Advancement</title>
      <link>https://arxiv.org/abs/2504.14351</link>
      <description>arXiv:2504.14351v1 Announce Type: cross 
Abstract: Decentralization is a foundational principle of permissionless blockchains, with consensus mechanisms serving a critical role in its realization. This study quantifies the decentralization of consensus mechanisms in proof-of-stake (PoS) blockchains using a comprehensive set of metrics, including Nakamoto coefficients, Gini, Herfindahl Hirschman Index (HHI), Shapley values, and Zipfs coefficient. Our empirical analysis across ten prominent blockchains reveals significant concentration of stake among a few validators, posing challenges to fair consensus. To address this, we introduce two alternative weighting models for PoS consensus: Square Root Stake Weight (SRSW) and Logarithmic Stake Weight (LSW), which adjust validator influence through non-linear transformations. Results demonstrate that SRSW and LSW models improve decentralization metrics by an average of 51% and 132%, respectively, supporting more equitable and resilient blockchain systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14351v1</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TNSM.2025.3561098</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Network and Service Management (2025)</arxiv:journal_reference>
      <dc:creator>Shashank Motepalli, Hans-Arno Jacobsen</dc:creator>
    </item>
    <item>
      <title>LLM-Enabled In-Context Learning for Data Collection Scheduling in UAV-assisted Sensor Networks</title>
      <link>https://arxiv.org/abs/2504.14556</link>
      <description>arXiv:2504.14556v1 Announce Type: cross 
Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly being used in various private and commercial applications, e.g. traffic control, package delivery, and Search and Rescue (SAR) operations. Machine Learning (ML) methods used in UAV-assisted Sensor Networks (UASNETs) and especially in Deep Reinforcement Learning (DRL) face challenges such as complex and lengthy model training, gaps between simulation and reality, and low sample efficiency, which conflict with the urgency of emergencies such as SAR operations. This paper proposes In-Context Learning (ICL)-based Data Collection Scheduling (ICLDC) scheme, as an alternative to DRL in emergencies. The UAV collects and transmits logged sensory data, to an LLM, to generate a task description in natural language, from which it obtains a data collection schedule to be executed by the UAV. The system continuously adapts by adding feedback to task descriptions and utilizing feedback for future decisions. This method is tested against jailbreaking attacks, where task description is manipulated to undermine network performance, highlighting the vulnerability of LLMs to such attacks. The proposed ICLDC outperforms the Maximum Channel Gain by reducing cumulative packet loss by approximately 56\%. ICLDC presents a promising direction for intelligent scheduling and control in UAV-assisted data collection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14556v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yousef Emami, Hao Gao, SeyedSina Nabavirazani, Luis Almeida</dc:creator>
    </item>
    <item>
      <title>GainSight: Application-Guided Profiling for Composing Heterogeneous On-Chip Memories in AI Hardware Accelerators</title>
      <link>https://arxiv.org/abs/2504.14866</link>
      <description>arXiv:2504.14866v1 Announce Type: cross 
Abstract: As AI workloads drive soaring memory requirements, there is a need for higher-density on-chip memory for domain-specific accelerators that goes beyond what current SRAM technology can provide. We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories. However, there has been little work in factoring dynamic application profiles into such design decisions. We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and computes data lifetimes in domain-specific accelerators. By combining instrumentation and simulation across retargetable hardware backends, GainSight aligns heterogeneous memory designs with workload-specific traffic and lifetime metrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA H100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3) Up to 90% of GPU cache fetches are never reused, highlighting inefficiencies in terms of cache pollution. These insights that GainSight provides can be used to better understand the design spaces of both emerging on-chip memories and software algorithmic optimizations for the next generation of AI accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14866v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Peijing Li, Matthew Hung, Yiming Tan, Konstantin Ho{\ss}feld, Jake Jiajun Cheng, Shuhan Liu, Lixian Yan, Xinxin Wang, H. -S. Philip Wong, Thierry Tambe</dc:creator>
    </item>
    <item>
      <title>Beyond Terabit/s Integrated Neuromorphic Photonic Processor for DSP-Free Optical Interconnects</title>
      <link>https://arxiv.org/abs/2504.15044</link>
      <description>arXiv:2504.15044v1 Announce Type: cross 
Abstract: The rapid expansion of generative AI drives unprecedented demands for high-performance computing. Training large-scale AI models now requires vast interconnected GPU clusters across multiple data centers. Multi-scale AI training and inference demand uniform, ultra-low latency, and energy-efficient links to enable massive GPUs to function as a single cohesive unit. However, traditional electrical and optical interconnects, relying on conventional digital signal processors (DSPs) for signal distortion compensation, increasingly fail to meet these stringent requirements. To overcome these limitations, we present an integrated neuromorphic optical signal processor (OSP) that leverages deep reservoir computing and achieves DSP-free, all-optical, real-time processing. Experimentally, our OSP achieves a 100 Gbaud PAM4 per lane, 1.6 Tbit/s data center interconnect over a 5 km optical fiber in the C-band (equivalent to over 80 km in the O-band), far exceeding the reach of state-of-the-art DSP solutions, which are fundamentally constrained by chromatic dispersion in IMDD systems. Simultaneously, it reduces processing latency by four orders of magnitude and energy consumption by three orders of magnitude. Unlike DSPs, which introduce increased latency at high data rates, our OSP maintains consistent, ultra-low latency regardless of data rate scaling, making it ideal for future optical interconnects. Moreover, the OSP retains full optical field information for better impairment compensation and adapts to various modulation formats, data rates, and wavelengths. Fabricated using a mature silicon photonic process, the OSP can be monolithically integrated with silicon photonic transceivers, enhancing the compactness and reliability of all-optical interconnects. This research provides a highly scalable, energy-efficient, and high-speed solution, paving the way for next-generation AI infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15044v1</guid>
      <category>physics.optics</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benshan Wang, Qiarong Xiao, Tengji Xu, Li Fan, Shaojie Liu, Jianji Dong, Junwen Zhang, Chaoran Huang</dc:creator>
    </item>
    <item>
      <title>NeuGaze: Reshaping the future BCI</title>
      <link>https://arxiv.org/abs/2504.15101</link>
      <description>arXiv:2504.15101v1 Announce Type: cross 
Abstract: Traditional brain-computer interfaces (BCIs), reliant on costly electroencephalography or invasive implants, struggle with complex human-computer interactions due to setup complexity and limited precision. We present NeuGaze, a novel webcam-based system that leverages eye gaze, head movements, and facial expressions to enable intuitive, real-time control using only a standard 30 Hz webcam, often pre-installed in laptops. Requiring minimal calibration, NeuGaze achieves performance comparable to conventional inputs, supporting precise cursor navigation, key triggering via an efficient skill wheel, and dynamic gaming interactions, such as defeating formidable opponents in first-person games. By harnessing preserved neck-up functionalities in motor-impaired individuals, NeuGaze eliminates the need for specialized hardware, offering a low-cost, accessible alternative to BCIs. This paradigm empowers diverse applications, from assistive technology to entertainment, redefining human-computer interaction for motor-impaired users. Project is at \href{https://github.com/NeuSpeech/NeuGaze}{github.com/NeuSpeech/NeuGaze}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15101v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiqian Yang</dc:creator>
    </item>
    <item>
      <title>Breaking Down Quantum Compilation: Profiling and Identifying Costly Passes</title>
      <link>https://arxiv.org/abs/2504.15141</link>
      <description>arXiv:2504.15141v1 Announce Type: cross 
Abstract: With the increasing capabilities of quantum systems, the efficient, practical execution of quantum programs is becoming more critical. Each execution includes compilation time, which accounts for substantial overhead of the overall program runtime. To address this challenge, proposals that leverage precompilation techniques have emerged, whereby entire circuits or select components are precompiled to mitigate the compilation time spent during execution. Considering the impact of compilation time on quantum program execution, identifying the contribution of each individual compilation task to the execution time is necessary in directing the community's research efforts towards the development of an efficient compilation and execution pipeline. In this work, we perform a preliminary analysis of the quantum circuit compilation process in Qiskit, examining the cumulative runtime of each individual compilation task and identifying the tasks that most strongly impact the overall compilation time. Our results indicate that, as the desired level of optimization increases, circuit optimization and gate synthesis passes become the dominant tasks in compiling a Quantum Fourier Transform, with individual passes consuming up to 87% of the total compilation time. Mapping passes require the most compilation time for a GHZ state preparation circuit, accounting for over 99% of total compilation time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15141v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Felix Zilk, Alessandro Tundo, Vincenzo De Maio, Ivona Brandic</dc:creator>
    </item>
    <item>
      <title>QRCC: Evaluating Large Quantum Circuits on Small Quantum Computers through Integrated Qubit Reuse and Circuit Cutting</title>
      <link>https://arxiv.org/abs/2312.10298</link>
      <description>arXiv:2312.10298v3 Announce Type: replace-cross 
Abstract: Quantum computing has recently emerged as a promising computing paradigm for many application domains. However, the size of quantum circuits that can be run with high fidelity is constrained by the limited quantity and quality of physical qubits. Recently proposed schemes, such as wire cutting and qubit reuse, mitigate the problem but produce sub-optimal results as they address the problem individually. In addition, gate cutting, an alternative circuit-cutting strategy that is suitable for circuits computing expectation values, has not been fully explored in the field.
  In this paper, we propose QRCC, an integrated approach that exploits qubit reuse and circuit-cutting (including wire cutting and gate cutting) to run large circuits on small quantum computers. Circuit-cutting techniques introduce non-negligible post-processing overhead, which increases exponentially with the number of cuts. QRCC exploits qubit reuse to find better cutting solutions to minimize the cut numbers and thus the post-processing overhead. Our evaluation results show that on average we reduce the number of cuts by 29% and additional reduction when considering gate cuts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10298v3</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3622781.3674179</arxiv:DOI>
      <arxiv:journal_reference>ASPLOS 2024 volume 4, pages 236-251</arxiv:journal_reference>
      <dc:creator>Aditya Pawar, Yingheng Li, Zewei Mo, Yanan Guo, Youtao Zhang, Xulong Tang, Jun Yang</dc:creator>
    </item>
  </channel>
</rss>
