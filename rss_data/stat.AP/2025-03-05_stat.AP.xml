<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Mar 2025 02:50:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Comparative Modelling of Essential Characteristics of Volatility: Simulation and Empirical Study</title>
      <link>https://arxiv.org/abs/2503.02031</link>
      <description>arXiv:2503.02031v1 Announce Type: new 
Abstract: This study utilised the dynamics of five time-varying models to estimate six essential features of financial return volatility that are relevant for robust risk management. These features include pronounced persistence, mean reversion, leverage effect or volatility asymmetry, conditional skewness, conditional fat-tailedness, and the long memory behaviour of volatility decomposition into long-term and short-term components. Both simulation and empirical evidence are provided. Through the applications of these models using the S&amp;P Indian index, the study shows that the market returns are characterised by these volatility features. Our findings from the long-memory behaviour revealed that although the response to shocks is greater in the short-term component, it is however short-lived. On the contrary, despite a high degree of persistence in the long-term component, market information or unexpected news arrival only has a low long-run impact on the market. Based on this, the long-run investment risks within the Indian stock market seem to be under control. Hence, our findings suggest that rational investors should try to stay calm with the arrival of unexpected news in the market because the long-run effect of such news will not be severe, and the market will eventually return to its normal state.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02031v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard T. A. Samuel, Charles Chimedza, Caston Sigauke</dc:creator>
    </item>
    <item>
      <title>Identification of Genetic Factors Associated with Corpus Callosum Morphology: Conditional Strong Independence Screening for Non-Euclidean Responses</title>
      <link>https://arxiv.org/abs/2503.02245</link>
      <description>arXiv:2503.02245v1 Announce Type: new 
Abstract: The corpus callosum, the largest white matter structure in the brain, plays a critical role in interhemispheric communication. Variations in its morphology are associated with various neurological and psychological conditions, making it a key focus in neurogenetics. Age is known to influence the structure and morphology of the corpus callosum significantly, complicating the identification of specific genetic factors that contribute to its shape and size. We propose a conditional strong independence screening method to address these challenges for ultrahigh-dimensional predictors and non-Euclidean responses. Our approach incorporates prior knowledge, such as age. It introduces a novel concept of conditional metric dependence, quantifying non-linear conditional dependencies among random objects in metric spaces without relying on predefined models. We apply this framework to identify genetic factors associated with the morphology of the corpus callosum. Simulation results demonstrate the efficacy of this method across various non-Euclidean data types, highlighting its potential to drive genetic discovery in neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02245v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Gao, Jin Zhu, Yue Hu, Wenliang Pan, Xueqin Wang</dc:creator>
    </item>
    <item>
      <title>Accounting for Missing Data in Public Health Research Using a Synthesis of Statistical and Mathematical Models</title>
      <link>https://arxiv.org/abs/2503.02789</link>
      <description>arXiv:2503.02789v1 Announce Type: new 
Abstract: Introduction: Missing data is a challenge to medical research. Accounting for missing data by imputing or weighting conditional on covariates relies on the variable with missingness being observed at least some of the time for all unique covariate values. This requirement is referred to as positivity, and violations can result in bias. Here, we review a novel approach to addressing positivity violations in the context of systolic blood pressure. Methods: To illustrate the proposed approach, we estimate the mean systolic blood pressure among children and adolescents aged 2-17 years old in the United States using data from 2017-2018 National Health and Nutrition Examination Survey (NHANES). As blood pressure was never measured for those aged 2-7, there exists a positivity violation by design. Using a recently proposed synthesis of statistical and mathematical models, we integrate external information with NHANES to address our motivating question. Results: With the synthesis model, the estimated mean systolic blood pressure was 100.5 (95\% confidence interval: 99.9, 101.0), which is notably lower than either a complete-case analysis or extrapolation from a statistical model. The synthesis results were supported by a diagnostic comparing the performance of the mathematical model in the positive region. Conclusion: Positivity violations pose a threat to quantitative medical research, and standard approaches to addressing nonpositivity rely on restrictive untestable assumptions. Using a synthesis model, like the one detailed here, offers a viable alternative through integration of external information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02789v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul N Zivich, Bonnie E Shook-Sa, Stephen R Cole, Eric T Lofgren, Jessie K Edwards</dc:creator>
    </item>
    <item>
      <title>Constructing balanced datasets for predicting failure modes in structural systems under seismic hazards</title>
      <link>https://arxiv.org/abs/2503.01882</link>
      <description>arXiv:2503.01882v1 Announce Type: cross 
Abstract: Accurate prediction of structural failure modes under seismic excitations is essential for seismic risk and resilience assessment. Traditional simulation-based approaches often result in imbalanced datasets dominated by non-failure or frequently observed failure scenarios, limiting the effectiveness in machine learning-based prediction. To address this challenge, this study proposes a framework for constructing balanced datasets that include distinct failure modes. The framework consists of three key steps. First, critical ground motion features (GMFs) are identified to effectively represent ground motion time histories. Second, an adaptive algorithm is employed to estimate the probability densities of various failure domains in the space of critical GMFs and structural parameters. Third, samples generated from these probability densities are transformed into ground motion time histories by using a scaling factor optimization process. A balanced dataset is constructed by performing nonlinear response history analyses on structural systems with parameters matching the generated samples, subjected to corresponding transformed ground motion time histories. Deep neural network models are trained on balanced and imbalanced datasets to highlight the importance of dataset balancing. To further evaluate the framework's applicability, numerical investigations are conducted using two different structural models subjected to recorded and synthetic ground motions. The results demonstrate the framework's robustness and effectiveness in addressing dataset imbalance and improving machine learning performance in seismic failure mode prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01882v1</guid>
      <category>cs.LG</category>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jungho Kim, Taeyong Kim</dc:creator>
    </item>
    <item>
      <title>PanelMatch: Matching Methods for Causal Inference with Time-Series Cross-Section Data</title>
      <link>https://arxiv.org/abs/2503.02073</link>
      <description>arXiv:2503.02073v1 Announce Type: cross 
Abstract: Analyzing time-series cross-sectional (also known as longitudinal or panel) data is an important process across a number of fields, including the social sciences, economics, finance, and medicine. PanelMatch is an R package that implements a set of tools enabling researchers to apply matching methods for causal inference with time-series cross-sectional data. Relative to other commonly used methods for longitudinal analyses, like regression with fixed effects, the matching-based approach implemented in PanelMatch makes fewer parametric assumptions and offers more diagnostics. In this paper, we discuss the PanelMatch package, showing users a recommended pipeline for doing causal inference analysis with it and highlighting useful diagnostic and visualization tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02073v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Rauh, In Song Kim, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>What Influences the Field Goal Attempts of Professional Players? Analysis of Basketball Shot Charts via Log Gaussian Cox Processes with Spatially Varying Coefficients</title>
      <link>https://arxiv.org/abs/2503.02137</link>
      <description>arXiv:2503.02137v1 Announce Type: cross 
Abstract: Basketball shot charts provide valuable information regarding local patterns of in-game performance to coaches, players, sports analysts, and statisticians. The spatial patterns of where shots were attempted and whether the shots were successful suggest options for offensive and defensive strategies as well as historical summaries of performance against particular teams and players. The data represent a marked spatio-temporal point process with locations representing locations of attempted shots and an associated mark representing the shot's outcome (made/missed). Here, we develop a Bayesian log Gaussian Cox process model allowing joint analysis of the spatial pattern of locations and outcomes of shots across multiple games. We build a hierarchical model for the log intensity function using Gaussian processes, and allow spatially varying effects for various game-specific covariates. We aim to model the spatial relative risk under different covariate values. For inference via posterior simulation, we design a Markov chain Monte Carlo (MCMC) algorithm based on a kernel convolution approach. We illustrate the proposed method using extensive simulation studies. A case study analyzing the shot data of NBA legends Stephen Curry, LeBron James, and Michael Jordan highlights the effectiveness of our approach in real-world scenarios and provides practical insights into optimizing shooting strategies by examining how different playing conditions, game locations, and opposing team strengths impact shooting efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02137v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahao Cao, Qingpo Cai, Lance A. Waller, DeMarc A. Hickson, Guanyu Hu, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Fast and robust invariant generalized linear models</title>
      <link>https://arxiv.org/abs/2503.02611</link>
      <description>arXiv:2503.02611v1 Announce Type: cross 
Abstract: Statistical integration of diverse data sources is an essential step in the building of generalizable prediction tools, especially in precision health. The invariant features model is a new paradigm for multi-source data integration which posits that a small number of covariates affect the outcome identically across all possible environments. Existing methods for estimating invariant effects suffer from immense computational costs or only offer good statistical performance under strict assumptions. In this work, we provide a general framework for estimation under the invariant features model that is computationally efficient and statistically flexible. We also provide a robust extension of our proposed method to protect against possibly corrupted or misspecified data sources. We demonstrate the robust properties of our method via simulations, and use it to build a transferable prediction model for end stage renal disease using electronic health records from the All of Us research program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02611v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parker Knight, Ndey Isatou Jobe, Rui Duan</dc:creator>
    </item>
    <item>
      <title>Exact matching as an alternative to propensity score matching</title>
      <link>https://arxiv.org/abs/2503.02850</link>
      <description>arXiv:2503.02850v1 Announce Type: cross 
Abstract: The comparison of different medical treatments from observational studies or across different clinical studies is often biased by confounding factors such as systematic differences in patient demographics or in the inclusion criteria for the trials. Propensity score matching is a popular method to adjust for such confounding. It compares weighted averages of patient responses. The weights are calculated from logistic regression models with the intention to reduce differences between the confounders in the treatment groups. However, the groups are only "roughly matched" with no generally accepted principle to determine when a match is "good enough".
  In this manuscript, we propose an alternative approach to the matching problem by considering it as a constrained optimization problem. We investigate the conditions for exact matching in the sense that the average values of confounders are identical in the treatment groups after matching. Our approach is similar to the matching-adjusted indirect comparison approach by Signorovitch et al. (2010) but with two major differences: First, we do not impose any specific functional form on the matching weights; second, the proposed approach can be applied to individual patient data from several treatment groups as well as to a mix of individual patient and aggregated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02850v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ekkehard Glimm, Lillian Yau</dc:creator>
    </item>
  </channel>
</rss>
