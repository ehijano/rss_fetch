<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Jun 2024 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimization of Energy Consumption in Delay-Tolerant Networks</title>
      <link>https://arxiv.org/abs/2406.03580</link>
      <description>arXiv:2406.03580v1 Announce Type: new 
Abstract: Delay tolerant network is a network architecture and protocol suite specifically designed to handle challenging communications environments, such as deep space communications, disaster response, and remote area communications. Although DTN [1]can provide efficient and reliable data transmission in environments with high latency, unstable connections, and high bit error rates, its energy consumption optimization problem is still a challenge, especially in scenarios with limited resources.To solve this problem, this study combines the Epidemic[2] and MaxProp[3] routing protocols with Machine Learning Models to optimize the energy consumption of DTNs. Hundreds of simulations were conducted in the ONE simulator, and an external real-world dataset from San Francisco taxi mobility traces [54] was imported. Random Forest[4] and Gradient Boosting Machine (GBM)[5] models were employed for data analysis. Through optimization involving Hyperparameter Tuning and Feature Selection, the Random Forest model achieved an R-squared value of 0.53, while the GBM model achieved an R-squared value of 0.65.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03580v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junran Wang, Milena Radenkovic</dc:creator>
    </item>
    <item>
      <title>Active ML for 6G: Towards Efficient Data Generation, Acquisition, and Annotation</title>
      <link>https://arxiv.org/abs/2406.03630</link>
      <description>arXiv:2406.03630v1 Announce Type: new 
Abstract: This paper explores the integration of active machine learning (ML) for 6G networks, an area that remains under-explored yet holds potential. Unlike passive ML systems, active ML can be made to interact with the network environment. It actively selects informative and representative data points for training, thereby reducing the volume of data needed while accelerating the learning process. While active learning research mainly focuses on data annotation, we call for a network-centric active learning framework that considers both annotation (i.e., what is the label) and data acquisition (i.e., which and how many samples to collect). Moreover, we explore the synergy between generative artificial intelligence (AI) and active learning to overcome existing limitations in both active learning and generative AI. This paper also features a case study on a mmWave throughput prediction problem to demonstrate the practical benefits and improved performance of active learning for 6G networks. Furthermore, we discuss how the implications of active learning extend to numerous 6G network use cases. We highlight the potential of active learning based 6G networks to enhance computational efficiency, data annotation and acquisition efficiency, adaptability, and overall network intelligence. We conclude with a discussion on challenges and future research directions for active learning in 6G networks, including development of novel query strategies, distributed learning integration, and inclusion of human- and machine-in-the-loop learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03630v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Alhussein, Ning Zhang, Sami Muhaidat, Weihua Zhuang</dc:creator>
    </item>
    <item>
      <title>A Survey on Intelligent Internet of Things: Applications, Security, Privacy, and Future Directions</title>
      <link>https://arxiv.org/abs/2406.03820</link>
      <description>arXiv:2406.03820v1 Announce Type: new 
Abstract: The rapid advances in the Internet of Things (IoT) have promoted a revolution in communication technology and offered various customer services. Artificial intelligence (AI) techniques have been exploited to facilitate IoT operations and maximize their potential in modern application scenarios. In particular, the convergence of IoT and AI has led to a new networking paradigm called Intelligent IoT (IIoT), which has the potential to significantly transform businesses and industrial domains. This paper presents a comprehensive survey of IIoT by investigating its significant applications in mobile networks, as well as its associated security and privacy issues. Specifically, we explore and discuss the roles of IIoT in a wide range of key application domains, from smart healthcare and smart cities to smart transportation and smart industries. Through such extensive discussions, we investigate important security issues in IIoT networks, where network attacks, confidentiality, integrity, and intrusion are analyzed, along with a discussion of potential countermeasures. Privacy issues in IIoT networks were also surveyed and discussed, including data, location, and model privacy leakage. Finally, we outline several key challenges and highlight potential research directions in this important area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03820v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ons Aouedi, Thai-Hoc Vu, Alessio Sacco, Dinh C. Nguyen, Kandaraj Piamrat, Guido Marchetto, Quoc-Viet Pham</dc:creator>
    </item>
    <item>
      <title>UAV-based Maritime Communications: Relaying to Enhance the Link Quality</title>
      <link>https://arxiv.org/abs/2304.08650</link>
      <description>arXiv:2304.08650v2 Announce Type: replace 
Abstract: Providing a stable connectivity in maritime communications is of utmost importance to unleash the full potential of smart ports. Nonetheless, due to the crowded nature of harbor environments, it is likely that some ships are shadowed by others, resulting in reduced received power that subsequently diminishes their data rates-even threatens basic connectivity requirements. Given that uncrewed aerial vehicles (UAVs) have been regarded as an integral part of future generations of wireless communication networks, they can be employed in maritime communications as well. In this paper, we investigate the use of UAV-mounted relays in order to help mitigate the reduced data rates of blocked links in maritime communications. Various communication architectures are considered based on the positioning mechanism of the UAV; in this regard, fixed, k-means algorithm-based, and landing spot-based positioning approaches are examined. Additionally, since UAVs are predominantly battery-operated, the energy consumption performances of these approaches are also measured. Results reveal that the landing spot-based UAV relay positioning approach finds the best trade-off between the data rate and energy consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.08650v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdullah Taha \c{C}a\u{g}an, G\"orkem Berkay Ko\c{c}, Handan Yak{\i}n, Berk \c{C}ilo\u{g}lu, Muhammad Zeeshan Ashgar, \"Ozg\"un Ersoy, Jyri H\"am\"al\"ainen, Metin \"Ozt\"urk</dc:creator>
    </item>
    <item>
      <title>Wireless Network Digital Twin for 6G: Generative AI as A Key Enabler</title>
      <link>https://arxiv.org/abs/2311.17451</link>
      <description>arXiv:2311.17451v2 Announce Type: replace 
Abstract: Digital twin, which enables emulation, evaluation, and optimization of physical entities through synchronized digital replicas, has gained increasing attention as a promising technology for intricate wireless networks. For 6G, numerous innovative wireless technologies and network architectures have posed new challenges in establishing wireless network digital twins. To tackle these challenges, artificial intelligence (AI), particularly the flourishing generative AI, emerges as a potential solution. In this article, we discuss emerging prerequisites for wireless network digital twins considering the complicated network architecture, tremendous network scale, extensive coverage, and diversified application scenarios in the 6G era. We further explore the applications of generative AI, such as Transformer and diffusion model, to empower the 6G digital twin from multiple perspectives including physical-digital modeling, synchronization, and slicing capability. Subsequently, we propose a hierarchical generative AI-enabled wireless network digital twin at both the message-level and policy-level, and provide a typical use case with numerical results to validate the effectiveness and efficiency. Finally, open research issues for wireless network digital twins in the 6G era are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17451v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenyu Tao, Wei Xu, Yongming Huang, Xiaoyun Wang, Xiaohu You</dc:creator>
    </item>
    <item>
      <title>Non-uniformity is All You Need: Efficient and Timely Encrypted Traffic Classification With ECHO</title>
      <link>https://arxiv.org/abs/2406.01852</link>
      <description>arXiv:2406.01852v2 Announce Type: replace 
Abstract: With 95% of Internet traffic now encrypted, an effective approach to classifying this traffic is crucial for network security and management. This paper introduces ECHO -- a novel optimization process for ML/DL-based encrypted traffic classification. ECHO targets both classification time and memory utilization and incorporates two innovative techniques.
  The first component, HO (Hyperparameter Optimization of binnings), aims at creating efficient traffic representations. While previous research often uses representations that map packet sizes and packet arrival times to fixed-sized bins, we show that non-uniform binnings are significantly more efficient. These non-uniform binnings are derived by employing a hyperparameter optimization algorithm in the training stage. HO significantly improves accuracy given a required representation size, or, equivalently, achieves comparable accuracy using smaller representations.
  Then, we introduce EC (Early Classification of traffic), which enables faster classification using a cascade of classifiers adapted for different exit times, where classification is based on the level of confidence. EC reduces the average classification latency by up to 90\%. Remarkably, this method not only maintains classification accuracy but also, in certain cases, improves it.
  Using three publicly available datasets, we demonstrate that the combined method, Early Classification with Hyperparameter Optimization (ECHO), leads to a significant improvement in classification efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01852v2</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shilo Daum, Tal Shapira, Anat Bremler-Barr, David Hay</dc:creator>
    </item>
  </channel>
</rss>
