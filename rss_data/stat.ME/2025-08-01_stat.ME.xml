<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Aug 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Stability Analysis and Local Influence Diagnostics for an Extreme-Value Regression Model of Anomalous Wind Gusts</title>
      <link>https://arxiv.org/abs/2507.22967</link>
      <description>arXiv:2507.22967v1 Announce Type: new 
Abstract: Extreme events in complex physical systems, such as anomalous wind gusts, often cause significant material and human damage. Their modeling is crucial for risk assessment and understanding the underlying dynamics. In this work, we introduce a local influence analysis to assess the stability of a class of extreme-value Birnbaum-Saunders regression models, which are particularly suited for analyzing such data. The proposed approach uses the conformal normal curvature (CNC) of the log-likelihood function to diagnose the influence of individual observations on the postulated model. By examining the eigenvalues and eigenvectors associated with the CNC, we identify influential data points-physical events that disproportionately affect the model's parameters. We illustrate the methodology through a simulation study and apply it to a time series of wind gust data from Itajai, Brazil, where a severe event caused multiple damages and casualties. Our approach successfully pinpoints this specific event as a highly influential observation and quantifies its impact on the fitted model. This work provides a valuable diagnostic tool for physicists and data scientists working with extreme-value models of complex natural phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22967v1</guid>
      <category>stat.ME</category>
      <category>physics.app-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e I. C. Lima, Raydonal Ospina, Michelli Barros, Ant\^onio M. S. Mac\^edo</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing for community structure in temporal networks using e-values</title>
      <link>https://arxiv.org/abs/2507.23034</link>
      <description>arXiv:2507.23034v1 Announce Type: new 
Abstract: Community structure in networks naturally arises in various applications. But while the topic has received significant attention for static networks, the literature on community structure in temporally evolving networks is more scarce. In particular, there are currently no statistical methods available to test for the presence of community structure in a sequence of networks evolving over time. In this work, we propose a simple yet powerful test using e-values, an alternative to p-values that is more flexible in certain ways. Specifically, an e-value framework retains valid testing properties even after combining dependent information, a relevant feature in the context of testing temporal networks. We apply the proposed test to synthetic and real-world networks, demonstrating various features inherited from the e-value formulation and exposing some of the inherent difficulties of testing on temporal networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23034v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko, Jonathan P. Williams, Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Analyzing Zero-Truncated Recurrent Events by Stratified Regression with Time-Varying Coefficients</title>
      <link>https://arxiv.org/abs/2507.23060</link>
      <description>arXiv:2507.23060v1 Announce Type: new 
Abstract: This paper presents a strategy for analyzing zero-truncated recurrent events data. Motivated by a pediatric mental health care (PMHC) program, we are particularly concerned with how the event occurrence depends on the occurrences in the past. We consider a stratified Cox regression model with time-varying coefficients and propose a procedure for estimating the model parameters using the zero-truncated data integrated with population census information. We evaluate the finite-sample performance of the proposed estimator through simulation and establish its asymptotic properties. Data from the PMHC program are used throughout the paper to motivate and to illustrate the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23060v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anqi A. Chen, X. Joan Hu, Rhonda J. Rosychuk, Leilei Zeng</dc:creator>
    </item>
    <item>
      <title>A new approach for Bayesian joint modeling of longitudinal and cure-survival outcomes using the defective Gompertz distribution</title>
      <link>https://arxiv.org/abs/2507.23196</link>
      <description>arXiv:2507.23196v1 Announce Type: new 
Abstract: In recent medical studies, the combination of longitudinal measurements with time-to-event data has increased the demand for more sophisticated models without unbiased estimates. Joint models for longitudinal and survival data have been developed to address such problems. One complex issue that may arise in the clinical trials is the presence of individuals who are statistically immune to the event of interest, those who may not experience the event even after extended follow-up periods. So far, the literature has addressed joint modeling with the presence of cured individuals mainly through mixture models for cure fraction and their extensions. In this study, we propose a joint modeling framework that accommodates the existence or absence of a cure fraction in an integrated way, using the defective Gompertz distribution. Our aim is to provide a more parsimonious alternative within an estimation process that involves a parameter vector with multiple components. Parameter estimation is performed using Bayesian inference via the efficient integrated nested Laplace approximation algorithm, by formulating the model as a latent Gaussian model. A simulation study is conducted to evaluate the frequentist properties of the proposed method under low-information prior settings. The model is further illustrated using a publicly available, yet underexplored, dataset on antiepileptic drug failure, where quality-of-life scores serve as longitudinal biomarkers. This application allows us to estimate the proportion of patients achieving seizure control under both traditional and modern antiepileptic therapies, demonstrating the model's ability to assess and compare long-term treatment effectiveness within a clinical trial context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23196v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dionisio Silva Neto, Denis Rustand, Haavard Rue, Danilo Alvares, Vera L. Tomazella</dc:creator>
    </item>
    <item>
      <title>A-optimal Designs under Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2507.23240</link>
      <description>arXiv:2507.23240v1 Announce Type: new 
Abstract: We characterize and identify A-optimal designs under generalized linear models. When a predetermined finite set of experimental settings is given, we derive analytic solutions or establish necessary and sufficient conditions for obtaining A-optimal approximate allocations. We show that a lift-one algorithm based on our formulae may outperform commonly used algorithms for finding A-optimal allocations. When continuous factors or design regions get involved, we develop a ForLion algorithm that is guaranteed to find A-optimal designs with mixed factors. Numerical studies show that our algorithms are able to find highly efficient designs with reduced numbers of distinct experimental settings, which may save both experimental time and cost significantly. Along with a rounding-off algorithm that converts approximate allocations to exact ones, we demonstrate that stratified samplers based on A-optimal allocations may provide more accurate parameter estimates than commonly used samplers, including D-optimal ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23240v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Yang, Xiaotian Chen, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Bayesian reliability acceptance sampling plan sampling plans under adaptive accelerated type-II censored competing risk data</title>
      <link>https://arxiv.org/abs/2507.23293</link>
      <description>arXiv:2507.23293v1 Announce Type: new 
Abstract: In recent times, products have become increasingly complex and highly reliable, so failures typically occur after long periods of operation under normal conditions and may arise from multiple causes. This paper employs simple step-stress partial accelerated life testing (SSSPALT) within the competing risks framework to determine the Bayesian reliability acceptance sampling plan (BRASP) under type-II censoring. Elevating the stress during the life test incurs an additional cost that increases the cost of the life test. In this context, an adaptive scenario is also considered in that sampling plan. The adaptive scenario is as follows: the stress is increased after a certain time if the number of failures up to that point is less than a pre-specified number of failures. The Bayes decision function and Bayes risk are derived for the general loss function. An optimal BRASP under that adaptive SSSPALT is obtained for the quadratic loss function by minimizing Bayes risk. An algorithm is provided to determine the optimal proposed BRASP. Further, comparative studies are conducted between the proposed BRASP, the conventional non-accelerated BRASP, and the conventional accelerated BRASP under type-II censoring to evaluate the effectiveness of the proposed approach. Finally, the methodology is illustrated using real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23293v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rathin Das, Soumya Roy, Biswabrata Pradhan</dc:creator>
    </item>
    <item>
      <title>On exact regions between measures of concordance and Chatterjee's rank correlation for lower semilinear copulas</title>
      <link>https://arxiv.org/abs/2507.23316</link>
      <description>arXiv:2507.23316v1 Announce Type: new 
Abstract: We explore how the classical concordance measures - Kendall's $\tau$, Spearman's rank correlation $\rho$, and Spearman's footrule $\phi$ - relate to Chatterjee's rank correlation $\xi$ when restricted to lower semilinear copulas. First, we provide a complete characterization of the attainable $\tau$-$\rho$ region for this class, thus resolving the conjecture in [18]. Building on this result, we then derive the exact $\tau$-$\phi$ and $\phi$-$\rho$ regions, obtain a closed-form relationship between $\xi$ and $\tau$, and establish the exact $\tau$-$\xi$ region. In particular, we prove that $\xi$ never exceeds $\tau$, $\rho$, or $\phi$. Our results clarify the relationship between undirected and directed dependence measures and reveal novel insights into the dependence structures that result from lower semilinear copulas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23316v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Fuchs, Carsten Limbach, Fabian Sch\"urrer</dc:creator>
    </item>
    <item>
      <title>Maximin distance designs for mixed continuous, ordinal, and binary variables</title>
      <link>https://arxiv.org/abs/2507.23405</link>
      <description>arXiv:2507.23405v1 Announce Type: new 
Abstract: Computer experiments are pivotal for modeling complex real-world systems. Maximizing information extraction and ensuring accurate surrogate modeling necessitates space-filling designs, where design points extensively cover the input domain. While substantial research has been conducted on maximin distance designs for continuous variables, which aim to maximize the minimum distance between points, methods accommodating mixed-variable types remain underdeveloped. This paper introduces the first general methodology for constructing maximin distance designs integrating continuous, ordinal, and binary variables. This approach allows flexibility in the number of runs, the mix of variable types, and the granularity of levels for ordinal variables. We propose three advanced algorithms, each rigorously supported by theoretical frameworks, that are computationally efficient and scalable. Our numerical evaluations demonstrate that our methods significantly outperform existing techniques in achieving greater separation distances across design points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23405v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hui Lan, Xu He</dc:creator>
    </item>
    <item>
      <title>Overcoming error-in-variable problem in data-driven model discovery by orthogonal distance regression</title>
      <link>https://arxiv.org/abs/2507.23426</link>
      <description>arXiv:2507.23426v1 Announce Type: new 
Abstract: Despite the recent proliferation of machine learning methods like SINDy that promise automatic discovery of governing equations from time-series data, there remain significant challenges to discovering models from noisy datasets. One reason is that the linear regression underlying these methods assumes that all noise resides in the training target (the regressand), which is the time derivative, whereas the measurement noise is in the states (the regressors). Recent methods like modified-SINDy and DySMHO address this error-in-variable problem by leveraging information from the model's temporal evolution, but they are also imposing the equation as a hard constraint, which effectively assumes no error in the regressand. Without relaxation, this hard constraint prevents assimilation of data longer than Lyapunov time. Instead, the fulfilment of the model equation should be treated as a soft constraint to account for the small yet critical error introduced by numerical truncation. The uncertainties in both the regressor and the regressand invite the use of orthogonal distance regression (ODR). By incorporating ODR with the Bayesian framework for model selection, we introduce a novel method for model discovery, termed ODR-BINDy, and assess its performance against current SINDy variants using the Lorenz63, Rossler, and Van Der Pol systems as case studies. Our findings indicate that ODR-BINDy consistently outperforms all existing methods in recovering the correct model from sparse and noisy datasets. For instance, our ODR-BINDy method reliably recovers the Lorenz63 equation from data with noise contamination levels of up to 30%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23426v1</guid>
      <category>stat.ME</category>
      <category>nlin.CD</category>
      <category>stat.ML</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lloyd Fung</dc:creator>
    </item>
    <item>
      <title>Miscellanea: "Within-trial" prognostic score adjustment is targeted maximum likelihood estimation</title>
      <link>https://arxiv.org/abs/2507.23446</link>
      <description>arXiv:2507.23446v1 Announce Type: new 
Abstract: Adjustment for ``super'' or ``prognostic'' composite covariates has become more popular in randomized trials recently. These prognostic covariates are often constructed from historical data by fitting a predictive model of the outcome on the raw covariates. A natural question that we have been asked by applied researchers is whether this can be done without the historical data: can the prognostic covariate be constructed or derived from the trial data itself, possibly using different folds of the data, before adjusting for it? Here we clarify that such ``within-trial'' prognostic adjustment is nothing more than a form of targeted maximum likelihood estimation (TMLE), a well-studied procedure for optimal inference. We demonstrate the equivalence with a simulation study and discuss the pros and cons of within-trial prognostic adjustment (standard efficient estimation) relative to standard TMLE and standard prognostic adjustment with historical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23446v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emilie H{\o}jbjerre-Frandsen, Alejandro Schuler</dc:creator>
    </item>
    <item>
      <title>A decomposition of Fisher's information to inform sample size for developing or updating fair and precise clinical prediction models -- Part 3: continuous outcomes</title>
      <link>https://arxiv.org/abs/2507.23548</link>
      <description>arXiv:2507.23548v1 Announce Type: new 
Abstract: Clinical prediction models enable healthcare professionals to estimate individual outcomes using patient characteristics. Current sample size guidelines for developing or updating models with continuous outcomes aim to minimise overfitting and ensure accurate estimation of population-level parameters, but do not explicitly address the precision of predictions. This is a critical limitation, as wide confidence intervals around predictions can undermine clinical utility and fairness, particularly if precision varies across subgroups. We propose methodology for calculating the sample size required to ensure precise and fair predictions in models with continuous outcomes. Building on linear regression theory and the Fisher's unit information matrix, our approach calculates how sample size impacts the epistemic (model-based) uncertainty of predictions and allows researchers to either (i) evaluate whether an existing dataset is sufficiently large, or (ii) determine the sample size needed to target a particular confidence interval width around predictions. The method requires real or synthetic data representing the target population. To assess fairness,the approach can evaluate prediction precision across subgroups. Extensions to prediction intervals are included to additionally address aleatoric uncertainty. Our methodology provides a practical framework for examining required sample sizes when developing or updating prediction models with continuous outcomes, focusing on achieving precise and equitable predictions. It supports the development of more reliable and fair models, enhancing their clinical applicability and trustworthiness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23548v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rebecca Whittle, Richard D Riley, Lucinda Archer, Gary S Collins, Amardeep Legha, Kym IE Snell, Joie Ensor</dc:creator>
    </item>
    <item>
      <title>Control Charts for Percentiles of Truncated Beta Distributed Environmental Data Using Studentized Bootstrap Method</title>
      <link>https://arxiv.org/abs/2507.23732</link>
      <description>arXiv:2507.23732v1 Announce Type: new 
Abstract: This paper proposes a control chart for monitoring percentiles of a process that follows a truncated beta distribution, utilizing a studentized parametric bootstrap method to account for the case when in-control parameters are unknown. To evaluate the in-control performance, extensive Monte Carlo simulations are conducted across various combinations of percentiles, false alarm rates, and sample sizes, with performance measured in terms of the average run length. The out-of-control performance is thoroughly assessed by introducing shifts in the distributional parameters and comparing the proposed chart with the conventional beta-based chart. The effectiveness and practical applicability of the proposed chart is illustrated through real-world examples from environmental data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23732v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bidhan Modok, Amarjit Kundu, Shovan Chowdhury</dc:creator>
    </item>
    <item>
      <title>Relative Bias Under Imperfect Identification in Observational Causal Inference</title>
      <link>https://arxiv.org/abs/2507.23743</link>
      <description>arXiv:2507.23743v1 Announce Type: new 
Abstract: To conduct causal inference in observational settings, researchers must rely on certain identifying assumptions. In practice, these assumptions are unlikely to hold exactly. This paper considers the bias of selection-on-observables, instrumental variables, and proximal inference estimates under violations of their identifying assumptions. We develop bias expressions for IV and proximal inference that show how violations of their respective assumptions are amplified by any unmeasured confounding in the outcome variable. We propose a set of sensitivity tools that quantify the sensitivity of different identification strategies, and an augmented bias contour plot visualizes the relationship between these strategies. We argue that the act of choosing an identification strategy implicitly expresses a belief about the degree of violations that must be present in alternative identification strategies. Even when researchers intend to conduct an IV or proximal analysis, a sensitivity analysis comparing different identification strategies can help to better understand the implications of each set of assumptions. Throughout, we compare the different approaches on a re-analysis of the impact of state surveillance on the incidence of protest in Communist Poland.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23743v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Melody Huang, Cory McCartan</dc:creator>
    </item>
    <item>
      <title>A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies</title>
      <link>https://arxiv.org/abs/2507.22943</link>
      <description>arXiv:2507.22943v1 Announce Type: cross 
Abstract: Background: One of the ways to enhance analyses conducted with large claims databases is by validating the measurement characteristics of code-based algorithms used to identify health outcomes or other key study parameters of interest. These metrics can be used in quantitative bias analyses to assess the robustness of results for an inferential study given potential bias from outcome misclassification. However, extensive time and resource allocation are typically re-quired to create reference-standard labels through manual chart review of free-text notes from linked electronic health records. Methods: We describe an expedited process that introduces efficiency in a validation study us-ing two distinct mechanisms: 1) use of natural language processing (NLP) to reduce time spent by human reviewers to review each chart, and 2) a multi-wave adaptive sampling approach with pre-defined criteria to stop the validation study once performance characteristics are identified with sufficient precision. We illustrate this process in a case study that validates the performance of a claims-based outcome algorithm for intentional self-harm in patients with obesity. Results: We empirically demonstrate that the NLP-assisted annotation process reduced the time spent on review per chart by 40% and use of the pre-defined stopping rule with multi-wave samples would have prevented review of 77% of patient charts with limited compromise to precision in derived measurement characteristics. Conclusion: This approach could facilitate more routine validation of code-based algorithms used to define key study parameters, ultimately enhancing understanding of the reliability of find-ings derived from database studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22943v1</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shirley V Wang, Georg Hahn, Sushama Kattinakere Sreedhara, Mufaddal Mahesri, Haritha S. Pillai, Rajendra Aldis, Joyce Lii, Sarah K. Dutcher, Rhoda Eniafe, Jamal T. Jones, Keewan Kim, Jiwei He, Hana Lee, Sengwee Toh, Rishi J Desai, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Approximating optimal SMC proposal distributions in individual-based epidemic models</title>
      <link>https://arxiv.org/abs/2206.05161</link>
      <description>arXiv:2206.05161v3 Announce Type: replace 
Abstract: Many epidemic models are naturally defined as individual-based models: where we track the state of each individual within a susceptible population. Inference for individual-based models is challenging due to the high-dimensional state-space of such models, which increases exponentially with population size. We consider sequential Monte Carlo algorithms for inference for individual-based epidemic models where we make direct observations of the state of a sample of individuals. Standard implementations, such as the bootstrap filter or the auxiliary particle filter are inefficient due to mismatch between the proposal distribution of the state and future observations. We develop new efficient proposal distributions that take account of future observations, leveraging the properties that (i) we can analytically calculate the optimal proposal distribution for a single individual given future observations and the future infection rate of that individual; and (ii) the dynamics of individuals are independent if we condition on their infection rates. Thus we construct estimates of the future infection rate for each individual, and then use an independent proposal for the state of each individual given this estimate. Empirical results show order of magnitude improvement in efficiency of the sequential Monte Carlo sampler for both SIS and SEIR models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.05161v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Rimella, Christopher Jewell, Paul Fearnhead</dc:creator>
    </item>
    <item>
      <title>A Dynamic Stochastic Block Model for Multidimensional Networks</title>
      <link>https://arxiv.org/abs/2209.09354</link>
      <description>arXiv:2209.09354v2 Announce Type: replace 
Abstract: The availability of relational data can offer new insights into the functioning of the economy. Nevertheless, modeling the dynamics in network data with multiple types of relationships is still a challenging issue. Stochastic block models provide a parsimonious and flexible approach to network analysis. We propose a new stochastic block model for multidimensional networks, where layer-specific hidden Markov-chain processes drive the changes in community formation. The changes in the block membership of a node in a given layer may be influenced by its own past membership in other layers. This allows for clustering overlap, clustering decoupling, or more complex relationships between layers, including settings of unidirectional, or bidirectional, non-linear Granger block causality. We address the overparameterization issue of a saturated specification by assuming a Multi-Laplacian prior distribution within a Bayesian framework. Data augmentation and Gibbs sampling are used to make the inference problem more tractable. Through simulations, we show that standard linear models and the pairwise approach are unable to detect block causality in most scenarios. In contrast, our model can recover the true Granger causality structure. As an application to international trade, we show that our model offers a unified framework, encompassing community detection and Gravity equation modeling. We found new evidence of block Granger causality of trade agreements and flows and core-periphery structure in both layers on a large sample of countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.09354v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ovielt Baltodano L\'opez, Roberto Casarin</dc:creator>
    </item>
    <item>
      <title>Conditional Predictive Inference for Missing Outcomes</title>
      <link>https://arxiv.org/abs/2403.04613</link>
      <description>arXiv:2403.04613v2 Announce Type: replace 
Abstract: We study the problem of conditional predictive inference on multiple outcomes missing at random (MAR) -- or equivalently, under covariate shift. While the weighted conformal prediction offers a tool for inference under covariate shift with a marginal coverage guarantee, procedures with conditional coverage guarantees are often desired in many applications to ensure reliable inference for a specific group of individuals. A standard approach to overcoming the fundamental limitation of distribution-free conditional predictive inference is to relax the target and instead aim to control coverage conditional on a local area, subset, or bin in the feature space. However, when the missingness pattern depends on the features, this relaxation remains challenging due to the violation of the MAR assumption with respect to the bins. To address this issue, we propose a propensity score $\epsilon$-discretization, a carefully designed binning strategy based on the propensity score, which enables valid conditional inference. Based on this strategy, we develop a procedure -- termed pro-CP -- that enables simultaneous conditional predictive inference for multiple missing outcomes. We show that pro-CP controls the bin-conditional coverage rate in a distribution-free manner when the propensity score is either known exactly or estimated with sufficient accuracy. Furthermore, we provide a theoretical bound on the coverage rate when the propensity score is unknown and must be estimated. Notably, the error bound remains constant and depends only on the estimation quality, not on the sample size or the number of outcomes under consideration. In extensive empirical experiments on simulated data and on a job search intervention dataset, we illustrate that our procedures provide informative prediction sets with valid conditional coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04613v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Edgar Dobriban, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Dynamic borrowing from historical controls via the synthetic prior with covariates in randomized clinical trials</title>
      <link>https://arxiv.org/abs/2410.07242</link>
      <description>arXiv:2410.07242v2 Announce Type: replace 
Abstract: Motivated by a rheumatoid arthritis clinical trial, we propose a new Bayesian method called SPx, standing for synthetic prior with covariates, to borrow information from historical trials to reduce the control group size in a new trial. The method involves a novel use of Bayesian model averaging to balance between multiple possible relationships between the historical and new trial data, allowing the historical data to be dynamically trusted or discounted as appropriate. We require only trial-level summary statistics, which are available more often than patient-level data. Through simulations and an application to the rheumatoid arthritis trial we show that SPx can substantially reduce the control group size while maintaining Frequentist properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07242v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel E. Schwartz, Yuan Ji, Li Wang</dc:creator>
    </item>
    <item>
      <title>Fractional binomial regression model for count data with excess zeros</title>
      <link>https://arxiv.org/abs/2410.08488</link>
      <description>arXiv:2410.08488v3 Announce Type: replace 
Abstract: This paper proposes a new generalized linear model with the fractional binomial distribution.
  Zero-inflated Poisson/negative binomial distributions are used for count data with many zeros. To analyze the association of such a count variable with covariates, zero-inflated Poisson/negative binomial regression models are widely used. In this work, we develop a regression model with the fractional binomial distribution that can serve as an additional tool for modeling the count response variable with covariates. The consistency of maximum likelihood estimators of the proposed model is investigated theoretically and empirically with simulations. The practicality of the proposed model is examined through data analysis. The results show that our model is as versatile as or more versatile than the existing zero-inflated models, and especially, it has a better fit with left-skewed discrete data than other models. However, the proposed model faces computational obstacles and will require more work in the future to implement this model on various count data with excess zeros.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08488v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jeonghwa Lee, Chloe Breece</dc:creator>
    </item>
    <item>
      <title>The Nudge Average Treatment Effect</title>
      <link>https://arxiv.org/abs/2410.23590</link>
      <description>arXiv:2410.23590v2 Announce Type: replace 
Abstract: The instrumental variable method is a prominent approach to recover under certain conditions, valid inference about a treatment causal effect even when unmeasured confounding might be present. In a groundbreaking paper, Imbens and Angrist (1994) established that a valid instrument nonparametrically identifies the average causal effect among compliers, also known as the local average treatment effect under a certain monotonicity assumption which rules out the existence of so-called defiers. An often-cited attractive property of monotonicity is that it facilitates a causal interpretation of the instrumental variable estimand without restricting the degree of heterogeneity of the treatment causal effect. In this paper, we introduce an alternative equally straightforward and interpretable condition for identification, which accommodates both the presence of defiers and heterogenous treatment effects. Mainly, we show that under our new conditions, the instrumental variable estimand recovers the average causal effect for the subgroup of units for whom the treatment is manipulable by the instrument, a subgroup which may consist of both defiers and compliers, therefore recovering an effect estimand we aptly call the Nudge Average Treatment Effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23590v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric J Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Multi-view biclustering via non-negative matrix tri-factorisation</title>
      <link>https://arxiv.org/abs/2502.13698</link>
      <description>arXiv:2502.13698v2 Announce Type: replace 
Abstract: Multi-view data is ever more apparent as methods for production, collection and storage of data become more feasible both practically and fiscally. However, not all features are relevant to describe the patterns for all individuals. Multi-view biclustering aims to simultaneously cluster both rows and columns, discovering clusters of rows as well as their view-specific identifying features. A novel multi-view biclustering approach based on non-negative matrix factorisation is proposed named ResNMTF. Demonstrated through extensive experiments on both synthetic and real datasets, ResNMTF successfully identifies both overlapping and non-exhaustive biclusters, without pre-existing knowledge of the number of biclusters present, and is able to incorporate any combination of shared dimensions across views. Further, to address the lack of a suitable bicluster-specific intrinsic measure, the popular silhouette score is extended to the bisilhouette score. The bisilhouette score is demonstrated to align well with known extrinsic measures, and proves useful as a tool for hyperparameter tuning as well as visualisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13698v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ella S. C. Orme, Theodoulos Rodosthenous, Marina Evangelou</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric copulas with tail dependence</title>
      <link>https://arxiv.org/abs/2504.00138</link>
      <description>arXiv:2504.00138v2 Announce Type: replace 
Abstract: We introduce a novel bivariate copula model able to capture both the central and tail dependence of the joint probability distribution. Model that can capture the dependence structure within the joint tail have important implications in many application areas where the focus is risk management (e.g. macroeconomics and finance). We use a Bayesian nonparametric approach to introduce a random copula based on infinite partitions of unity. We define a hierarchical prior over an infinite partition of the unit hypercube which has a stick breaking representation leading to an infinite mixture of products of independent beta densities. Capitalising on the stick breaking representation we introduce a Gibbs sample to proceed to inference. For our empirical analysis we consider both simulated and real data (insurance claims and portfolio returns). We compare both our model's ability to capture tail dependence and its out of sample predictive performance to competitive models (e.g. Joe and Clayton copulas) and show that in both simulated and real examples our model outperforms the competitive models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00138v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maria Concepci\'on Aus\'in, Maria Kalli</dc:creator>
    </item>
    <item>
      <title>Adaptive sample splitting for randomization tests</title>
      <link>https://arxiv.org/abs/2504.21572</link>
      <description>arXiv:2504.21572v2 Announce Type: replace 
Abstract: Randomization tests are widely used to generate finite-sample valid $p$-values for causal inference on experimental data. However, when applied to subgroup analysis, these tests may lack power due to small subgroup sizes. Incorporating a shared estimator of the conditional average treatment effect (CATE) can substantially improve power across subgroups but requires sample splitting to preserve validity. To this end, we quantify each unit's contribution to estimation and testing using a certainty score, which measures how certain the unit's treatment assignment is given its covariates and outcome. We show that units with higher certainty scores are more valuable for testing but less important for CATE estimation, since their treatment assignments can be accurately imputed. Building on this insight, we propose AdaSplit, a sample splitting procedure that adaptively allocates units between estimation and testing to maximize their overall contribution across tasks. We evaluate AdaSplit through simulation studies, demonstrating that it yields more powerful randomization tests than baselines that omit CATE estimation or rely on random sample splitting. Finally, we apply AdaSplit to a blood pressure intervention trial, identifying patient subgroups with significant treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21572v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yao Zhang, Zijun Gao</dc:creator>
    </item>
    <item>
      <title>Inference for Dispersion and Curvature of Random Objects</title>
      <link>https://arxiv.org/abs/2505.09844</link>
      <description>arXiv:2505.09844v2 Announce Type: replace 
Abstract: There are many open questions pertaining to the statistical analysis of random objects, which are increasingly encountered. A major challenge is the absence of linear operations in such spaces. A basic statistical task is to quantify statistical dispersion or spread. For two measures of dispersion for data objects in geodesic metric spaces, Fr\'echet variance and metric variance, we derive a central limit theorem (CLT) for their joint distribution. This analysis reveals that the Alexandrov curvature of the geodesic space determines the relationship between these two dispersion measures. This suggests a novel test for inferring the curvature of a space based on the asymptotic distribution of the dispersion measures. We demonstrate how this test can be employed to detect the intrinsic curvature of an unknown underlying space, which emerges as a joint property of the space and the underlying probability measure that generates the random objects. We investigate the asymptotic properties of the test and its finite-sample behavior for various data types, including distributional data and point cloud data. We illustrate the proposed inference for intrinsic curvature of random objects using gait synchronization data represented as symmetric positive definite matrices and energy compositional data on the sphere.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09844v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wookyeong Song, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Constructing g-computation estimators: two case studies in selection bias</title>
      <link>https://arxiv.org/abs/2506.03347</link>
      <description>arXiv:2506.03347v2 Announce Type: replace 
Abstract: G-computation is a useful estimation method that can be adapted to address various biases in epidemiology. However, these adaptations may not be obvious for some complex causal structures. This challenge is an example of the much wider issue of translating a causal diagram into a novel estimation strategy. To highlight these challenges, we consider two recent cases from the selection bias literature: treatment-induced selection and co-occurrence of biases that lack a joint adjustment set. For each case study, we show how g-computation can be adapted, describe how to implement that adaptation, show some general statistical properties, and illustrate the estimator using simulation. To simplify both the theoretical study and practical application of our estimators, we express the proposed g-computation estimators as stacked estimating equations. These examples illustrate how epidemiologists can translate identification results into a g-computation estimator and study the theoretical and finite-sample properties of a novel estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03347v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul N Zivich, Haidong Lu</dc:creator>
    </item>
    <item>
      <title>Deciphering interventional dynamical causality from non-intervention complex systems</title>
      <link>https://arxiv.org/abs/2407.01621</link>
      <description>arXiv:2407.01621v2 Announce Type: replace-cross 
Abstract: Detecting and quantifying causality is a focal topic in the fields of science, engineering, and interdisciplinary studies. However, causal studies on non-intervention systems attract much attention but remain extremely challenging. Delay-embedding technique provides a promising approach. In this study, we propose a framework named Interventional Dynamical Causality (IntDC) in contrast to the traditional Constructive Dynamical Causality (ConDC). ConDC, including Granger causality, transfer entropy and convergence of cross-mapping, measures the causality by constructing a dynamical model without considering interventions. A computational criterion, Interventional Embedding Entropy (IEE), is proposed to measure causal strengths in an interventional manner. IEE is an intervened causal information flow but in the delay-embedding space. Further, the IEE theoretically and numerically enables the deciphering of IntDC solely from observational (non-interventional) time-series data, without requiring any knowledge of dynamical models or real interventions in the considered system. In particular, IEE can be applied to rank causal effects according to their importance and construct causal networks from data. We conducted numerical experiments to demonstrate that IEE can find causal edges accurately, eliminate effects of confounding, and quantify causal strength robustly over traditional indices. We also applied IEE to real-world tasks. IEE performed as an accurate and robust tool for causal analyses solely from the observational data. The IntDC framework and IEE algorithm provide an efficient approach to the study of causality from time series in diverse non-intervention complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01621v2</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jifan Shi, Yang Li, Juan Zhao, Siyang Leng, Rui Bao, Kazuyuki Aihara, Luonan Chen, Wei Lin</dc:creator>
    </item>
    <item>
      <title>On the Approximation of Stationary Processes using the ARMA Model</title>
      <link>https://arxiv.org/abs/2408.10610</link>
      <description>arXiv:2408.10610v4 Announce Type: replace-cross 
Abstract: We look at a problem related to Autoregressive Moving Average (ARMA) models, on quantifying the approximation error between a true stationary process $X_t$ and an ARMA model $Y_t$. We take the transfer function representation $x(L)$ of a stationary process $X_t$ and show that the $L^{\infty}$ norm of $x$ acts as a valid norm on $X_t$ that controls the $\ell^2$ norm of its Wold coefficients. We then show that a certain subspace of stationary processes, which includes ARMA models, forms a Banach algebra under the $L^{\infty}$ norm that respects the multiplicative structure of $H^{\infty}$ transfer functions and thus improves on the structural properties of the cepstral norm for ARMA models. The natural definition of invertibility in this algebra is consistent with the original definition of ARMA invertibility, and generalizes better to non-ARMA processes than Wiener's $\ell^1$ condition. Finally, we calculate some explicit approximation bounds in the simpler context of continuous transfer functions, and critique some heuristic ideas on Pad\'e approximations and parsimonious models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10610v4</guid>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anand Ganesh, Babhrubahan Bose, Anand Rajagopalan</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing with e-values</title>
      <link>https://arxiv.org/abs/2410.23614</link>
      <description>arXiv:2410.23614v5 Announce Type: replace-cross 
Abstract: This book is written to offer a humble, but unified, treatment of e-values in hypothesis testing. It is organized into three parts: Fundamental Concepts, Core Ideas, and Advanced Topics. The first part includes four chapters that introduce the basic concepts. The second part includes five chapters of core ideas such as universal inference, log-optimality, e-processes, operations on e-values, and e-values in multiple testing. The third part contains seven chapters of advanced topics. The book collates important results from a variety of modern papers on e-values and related concepts, and also contains many results not published elsewhere. It offers a coherent and comprehensive picture on a fast-growing research area, and is ready to use as the basis of a graduate course in statistics and related fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23614v5</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaditya Ramdas, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>An approach to robust Bayesian regression in astronomy</title>
      <link>https://arxiv.org/abs/2411.02380</link>
      <description>arXiv:2411.02380v3 Announce Type: replace-cross 
Abstract: Model mis-specification (e.g. the presence of outliers) is commonly encountered in astronomical analyses, often requiring the use of ad hoc algorithms which are sensitive to arbitrary thresholds (e.g. sigma-clipping). For any given dataset, the optimal approach will be to develop a bespoke statistical model of the data generation and measurement processes, but these come with a development cost; there is hence utility in having generic modelling approaches that are both principled and robust to model mis-specification. Here we develop and implement a generic Bayesian approach to linear regression, based on Student's t-distributions, that is robust to outliers and mis-specification of the noise model. Our method is validated using simulated datasets with various degrees of model mis-specification; the derived constraints are shown to be systematically less biased than those from a similar model using normal distributions. We demonstrate that, for a dataset without outliers, a worst-case inference using t-distributions would give unbiased results with $\lesssim\!10$ per cent increase in the reported parameter uncertainties. We also compare with existing analyses of real-world datasets, finding qualitatively different results where normal distributions have been used and agreement where more robust methods have been applied. A Python implementation of this model, t-cup, is made available for others to use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02380v3</guid>
      <category>astro-ph.IM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Martin, Daniel J. Mortlock</dc:creator>
    </item>
    <item>
      <title>How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</title>
      <link>https://arxiv.org/abs/2505.18102</link>
      <description>arXiv:2505.18102v2 Announce Type: replace-cross 
Abstract: Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18102v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takashi Ishida, Thanawat Lodkaew, Ikko Yamane</dc:creator>
    </item>
    <item>
      <title>Covariance scanning for adaptively optimal change point detection in high-dimensional linear models</title>
      <link>https://arxiv.org/abs/2507.02552</link>
      <description>arXiv:2507.02552v2 Announce Type: replace-cross 
Abstract: This paper investigates the detection and estimation of a single change in high-dimensional linear models. We derive minimax lower bounds for the detection boundary and the estimation rate, which uncover a phase transition governed the sparsity of the covariance-weighted differential parameter. This form of "inherent sparsity" captures a delicate interplay between the covariance structure of the regressors and the change in regression coefficients on the detectability of a change point. Complementing the lower bounds, we introduce two covariance scanning-based methods, McScan and QcSan, which achieve minimax optimal performance (up to possible logarithmic factors) in the sparse and the dense regimes, respectively. In particular, QcScan is the first method shown to achieve consistency in the dense regime and further, we devise a combined procedure which is adaptively minimax optimal across sparse and dense regimes without the knowledge of the sparsity. Computationally, covariance scanning-based methods avoid costly computation of Lasso-type estimators and attain worst-case computation complexity that is linear in the dimension and sample size. Additionally, we consider the post-detection estimation of the differential parameter and the refinement of the change point estimator. Simulation studies support the theoretical findings and demonstrate the computational and statistical efficiency of the proposed covariance scanning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02552v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haeran Cho, Housen Li</dc:creator>
    </item>
  </channel>
</rss>
