<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Dec 2025 05:01:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Whittle likelihood for mixed models with application to groundwater level time series</title>
      <link>https://arxiv.org/abs/2512.20810</link>
      <description>arXiv:2512.20810v1 Announce Type: new 
Abstract: Understanding the processes that influence groundwater levels is crucial for forecasting and responding to hazards such as groundwater droughts. Mixed models, which combine a fixed mean, expressed using independent predictors, with autocorrelated random errors, are used for inference, forecasting and filling in missing values in groundwater level time series. Estimating parameters of mixed models using maximum likelihood has high computational complexity. For large datasets, this leads to restrictive simplifying assumptions such as fixing certain free parameters in practical implementations. In this paper, we propose a method to jointly estimate all parameters of mixed models using the Whittle likelihood, a frequency-domain quasi-likelihood. Our method is robust to missing and non-Gaussian data and can handle much larger data sizes. We demonstrate the utility of our method both in a simulation study and with real-world data, comparing against maximum likelihood and an alternative two-stage approach that estimates fixed and random effect parameters separately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20810v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakub J. Pypkowski, Adam M. Sykulski, James S. Martin, Ben P. Marchant</dc:creator>
    </item>
    <item>
      <title>Improving optimal subsampling through stratification</title>
      <link>https://arxiv.org/abs/2512.20837</link>
      <description>arXiv:2512.20837v1 Announce Type: new 
Abstract: Recent works have proposed optimal subsampling algorithms to improve computational efficiency in large datasets and to design validation studies in the presence of measurement error. Existing approaches generally fall into two categories: (i) designs that optimize individualized sampling rules, where unit-specific probabilities are assigned and applied independently, and (ii) designs based on stratified sampling with simple random sampling within strata. Focusing on the logistic regression setting, we derive the asymptotic variances of estimators under both approaches and compare them numerically through extensive simulations and an application to data from the Vanderbilt Comprehensive Care Clinic cohort. Our results reinforce that stratified sampling is not merely an approximation to individualized sampling, showing instead that optimal stratified designs are often more efficient than optimal individualized designs through their elimination of between-stratum contributions to variance. These findings suggest that optimizing over the class of individualized sampling rules overlooks highly efficient sampling designs and highlight the often underappreciated advantages of stratified sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20837v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jasper B. Yang, Thomas Lumley, Bryan E. Shepherd, Pamela A. Shaw</dc:creator>
    </item>
    <item>
      <title>A Unified Inference Method for FROC-type Curves and Related Summary Indices</title>
      <link>https://arxiv.org/abs/2512.20922</link>
      <description>arXiv:2512.20922v1 Announce Type: new 
Abstract: Free-response observer performance studies are of great importance for accuracy evaluation and comparison in tasks related to the detection and localization of multiple targets or signals. The free-response receiver operating characteristic (FROC) curve and many similar curves based on the free-response observer performance assessment data are important tools to display the accuracy of detection under different thresholds. The true positive rate at a fixed false positive rate and summary indices such as the area under the FROC curve are also commonly used as the figures of merit in the statistical evaluation of these studies. Motivated by a free-response observer performance assessment research of a Software as a Medical Device (SaMD), we propose a unified method based on the initial-detection-and-candidate model to simultaneously estimate a smooth curve and derive confidence intervals for summary indices and the true positive rate at a fixed false positive rate. A maximum likelihood estimator is proposed and its asymptotic normality property is derived. Confidence intervals are constructed based on the asymptotic normality of our maximum likelihood estimator. Simulation studies are conducted to evaluate the finite sample performance of the proposed method. We apply the proposed method to evaluate the diagnostic performance of the SaMD for detecting pulmonary lesions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20922v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiarui Sun, Kaiyuan Liu, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Two-level D- and A-optimal designs of Ehlich type with run sizes three more than a multiple of four</title>
      <link>https://arxiv.org/abs/2512.21060</link>
      <description>arXiv:2512.21060v1 Announce Type: new 
Abstract: For the majority of run sizes N where N &lt;= 20, the literature reports the best D- and A-optimal designs for the main-effects model which sequentially minimizes the aliasing between main effects and interaction effects and among interaction effects. The only series of run sizes for which all the minimally aliased D- and A-optimal main-effects designs remain unknown are those with run sizes three more than a multiple of four. To address this, in our paper, we propose an algorithm to generate all non-isomorphic D- and A-optimal main-effects designs for run sizes three more than a multiple of four. We enumerate all such designs for run sizes up to 19, report the numbers of designs we obtained, and identify those that minimize the aliasing between main effects and interaction effects and among interaction effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21060v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammed Saif Ismail Hameed, Eric D. Schoen, Jose Nunez Ares, Peter Goos</dc:creator>
    </item>
    <item>
      <title>Measuring Variable Importance via Accumulated Local Effects</title>
      <link>https://arxiv.org/abs/2512.21124</link>
      <description>arXiv:2512.21124v1 Announce Type: new 
Abstract: A shortcoming of black-box supervised learning models is their lack of interpretability or transparency. To facilitate interpretation, post-hoc global variable importance measures (VIMs) are widely used to assign to each predictor or input variable a numerical score that represents the extent to which that predictor impacts the fitted model's response predictions across the training data. It is well known that the most common existing VIMs, namely marginal Shapley and marginal permutation-based methods, can produce unreliable results if the predictors are highly correlated, because they require extrapolation of the response at predictor values that fall far outside the training data. Conditional versions of Shapley and permutation VIMs avoid or reduce the extrapolation but can substantially deflate the importance of correlated predictors. For the related goal of visualizing the effects of each predictor when strong predictor correlation is present, accumulated local effects (ALE) plots were recently introduced and have been widely adopted. This paper presents a new VIM approach based on ALE concepts that avoids both the extrapolation and the VIM deflation problems when predictors are correlated. We contrast, both theoretically and numerically, ALE VIMs with Shapley and permutation VIMs. Our results indicate that ALE VIMs produce similar variable importance rankings as Shapley and permutation VIMs when predictor correlations are mild and more reliable rankings when correlations are strong. An additional advantage is that ALE VIMs are far less computationally expensive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21124v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyu Zhu, Daniel W. Apley</dc:creator>
    </item>
    <item>
      <title>Modeling gap acceptance behavior allowing for perceptual distortions and exogenous influences</title>
      <link>https://arxiv.org/abs/2512.21136</link>
      <description>arXiv:2512.21136v1 Announce Type: new 
Abstract: This work on gap acceptance is based on the premise that the decision to accept/reject a gap happens in a person's mind and therefore must be based on the perceived gap and not the measured gap. The critical gap must also exist in a person's mind and hence, together with the perceived gap, is a latent variable. Finally, it is also proposed that the critical gap is influenced by various exogenous variables such as subject and opposing vehicle types, and perceived waiting time. Mathematical models that (i) incorporate systematic and random distortions during the perception process and (ii) account for the effect of the various influencing variables are developed. The parameters of these models are estimated for two different gap acceptance data sets using the maximum likelihood technique. The data is collected as part of this study. The estimated parameters throw valuable insights into how these influencing variables affect the critical gap. The results corroborate the initial predictions on the nature of influence these variables must exert and give strength to the gap acceptance decision-making construct proposed here. This work also proposes a methodology to estimate a measurable/observable world emulator of the latent variable critical gap. The use of the emulator critical gap provides improved estimates of derived quantities like the average waiting time of subject vehicles. Finally, studies are also conducted to show that the number of rejected gaps can work as a reasonable surrogate for the influencing variable, waiting time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21136v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ankita Sharma, Partha Chakroborty, Pranamesh Chakraborty</dc:creator>
    </item>
    <item>
      <title>Proximal Survival Analysis for Dependent Left Truncation</title>
      <link>https://arxiv.org/abs/2512.21283</link>
      <description>arXiv:2512.21283v1 Announce Type: new 
Abstract: In prevalent cohort studies with delayed entry, time-to-event outcomes are often subject to left truncation where only subjects that have not experienced the event at study entry are included, leading to selection bias. Existing methods for handling left truncation mostly rely on the (quasi-)independence assumption or the weaker conditional (quasi-)independence assumption which assumes that conditional on observed covariates, the left truncation time and the event time are independent on the observed region. In practice, however, our analysis of the Honolulu Asia Aging Study (HAAS) suggests that the conditional quasi-independence assumption may fail because measured covariates often serve only as imperfect proxies for the underlying mechanisms, such as latent health status, that induce dependence between truncation and event times. To address this gap, we propose a proximal weighting identification framework that admits the dependence-inducing factors may not be fully observed. We then construct an estimator based on the framework and study its asymptotic properties. We examine the finite sample performance of the proposed estimator by comprehensive simulations, and apply it to analyzing the cognitive impairment-free survival probabilities using data from the Honolulu Asia Aging Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21283v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuyao Wang, Andrew Ying, Ronghui Xu</dc:creator>
    </item>
    <item>
      <title>Diffusion Models in Simulation-Based Inference: A Tutorial Review</title>
      <link>https://arxiv.org/abs/2512.20685</link>
      <description>arXiv:2512.20685v1 Announce Type: cross 
Abstract: Diffusion models have recently emerged as powerful learners for simulation-based inference (SBI), enabling fast and accurate estimation of latent parameters from simulated and real data. Their score-based formulation offers a flexible way to learn conditional or joint distributions over parameters and observations, thereby providing a versatile solution to various modeling problems. In this tutorial review, we synthesize recent developments on diffusion models for SBI, covering design choices for training, inference, and evaluation. We highlight opportunities created by various concepts such as guidance, score composition, flow matching, consistency models, and joint modeling. Furthermore, we discuss how efficiency and statistical accuracy are affected by noise schedules, parameterizations, and samplers. Finally, we illustrate these concepts with case studies across parameter dimensionalities, simulation budgets, and model types, and outline open questions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20685v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Arruda, Niels Bracher, Ullrich K\"othe, Jan Hasenauer, Stefan T. Radev</dc:creator>
    </item>
    <item>
      <title>Can Agentic AI Match the Performance of Human Data Scientists?</title>
      <link>https://arxiv.org/abs/2512.20959</link>
      <description>arXiv:2512.20959v1 Announce Type: cross 
Abstract: Data science plays a critical role in transforming complex data into actionable insights across numerous domains. Recent developments in large language models (LLMs) have significantly automated data science workflows, but a fundamental question persists: Can these agentic AI systems truly match the performance of human data scientists who routinely leverage domain-specific knowledge? We explore this question by designing a prediction task where a crucial latent variable is hidden in relevant image data instead of tabular features. As a result, agentic AI that generates generic codes for modeling tabular data cannot perform well, while human experts could identify the important hidden variable using domain knowledge. We demonstrate this idea with a synthetic dataset for property insurance. Our experiments show that agentic AI that relies on generic analytics workflow falls short of methods that use domain-specific insights. This highlights a key limitation of the current agentic AI for data science and underscores the need for future research to develop agentic AI systems that can better recognize and incorporate domain knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20959v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>An Luo, Jin Du, Fangqiao Tian, Xun Xian, Robert Specht, Ganghua Wang, Xuan Bi, Charles Fleming, Jayanth Srinivasa, Ashish Kundu, Mingyi Hong, Jie Ding</dc:creator>
    </item>
    <item>
      <title>Closed-form empirical Bernstein confidence sequences for scalars and matrices</title>
      <link>https://arxiv.org/abs/2512.21300</link>
      <description>arXiv:2512.21300v1 Announce Type: cross 
Abstract: We derive a new closed-form variance-adaptive confidence sequence (CS) for estimating the average conditional mean of a sequence of bounded random variables. Empirically, it yields the tightest closed-form CS we have found for tracking time-varying means, across sample sizes up to $\approx 10^6$. When the observations happen to have the same conditional mean, our CS is asymptotically tighter than the recent closed-form CS of Waudby-Smith and Ramdas [38]. It also has other desirable properties: it is centered at the unweighted sample mean and has limiting width (multiplied by $\sqrt{t/\log t}$) independent of the significance level. We extend our results to provide a CS with the same properties for random matrices with bounded eigenvalues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21300v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Chugg, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Smoothing Variances Across Time: Adaptive Stochastic Volatility</title>
      <link>https://arxiv.org/abs/2408.11315</link>
      <description>arXiv:2408.11315v5 Announce Type: replace 
Abstract: We introduce a novel Bayesian framework for estimating time-varying volatility by extending the Random Walk Stochastic Volatility (RWSV) model with Dynamic Shrinkage Processes (DSP) in log-variances. Unlike the classical Stochastic Volatility (SV) or GARCH-type models with restrictive parametric stationarity assumptions, our proposed Adaptive Stochastic Volatility (ASV) model provides smooth yet dynamically adaptive estimates of evolving volatility and its uncertainty. We further enhance the model by incorporating a nugget effect, allowing it to flexibly capture small-scale variability while preserving smoothness elsewhere. We derive the theoretical properties of the global-local shrinkage prior DSP. Through simulation studies, we show that ASV exhibits remarkable misspecification resilience and low prediction error across various data-generating processes. Furthermore, ASV's capacity to yield locally smooth and interpretable estimates facilitates a clearer understanding of the underlying patterns and trends in volatility. As an extension, we develop the Bayesian Trend Filter with ASV (BTF-ASV) which allows joint modeling of the mean and volatility with abrupt changes. Finally, our proposed models are applied to time series data from finance, econometrics, and environmental science, highlighting their flexibility and broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11315v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason B. Cho, David S. Matteson</dc:creator>
    </item>
    <item>
      <title>Clustering the Nearest Neighbor Gaussian Process</title>
      <link>https://arxiv.org/abs/2501.10656</link>
      <description>arXiv:2501.10656v2 Announce Type: replace 
Abstract: Gaussian processes are ubiquitous as the primary tool for modeling spatial data. However, the Gaussian process is limited by its $\mathcal{O}(n^3)$ cost, making direct parameter fitting algorithms infeasible for the scale of modern data collection initiatives. The Nearest Neighbor Gaussian Process (NNGP) was introduced as a scalable approximation to dense Gaussian processes which has been successful for $n\sim 10^6$ observations. This project introduces the $\textit{clustered Nearest Neighbor Gaussian Process}$ (cNNGP) which reduces the computational and storage cost of the NNGP. The accuracy of parameter estimation and reduction in computational and memory storage requirements are demonstrated with simulated data, where the cNNGP provided comparable inference to that obtained with the NNGP, in a fraction of the sampling time. To showcase the method's performance, we modeled biomass over the state of Maine using data collected by the Global Ecosystem Dynamics Investigation (GEDI) to generate wall-to-wall predictions over the state. In 16% of the time, the cNNGP produced nearly indistinguishable inference and biomass prediction maps to those obtained with the NNGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10656v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ashlynn Crisp, Daniel Taylor-Rodriguez, Andrew O. Finley</dc:creator>
    </item>
    <item>
      <title>Bayesian mixture modeling using a mixture of finite mixtures with normalized inverse Gaussian weights</title>
      <link>https://arxiv.org/abs/2501.18854</link>
      <description>arXiv:2501.18854v2 Announce Type: replace 
Abstract: In Bayesian inference for mixture models with an unknown number of components, a finite mixture model is usually employed that assumes prior distributions for mixing weights and the number of components. This model is called a mixture of finite mixtures (MFM). As a prior distribution for the weights, a (symmetric) Dirichlet distribution is widely used for conjugacy and computational simplicity, while the selection of the concentration parameter influences the estimate of the number of components. In this paper, we focus on estimating the number of components. As a robust alternative Dirichlet weights, we present a method based on a mixture of finite mixtures with normalized inverse Gaussian weights. The motivation is similar to the use of normalized inverse Gaussian processes instead of Dirichlet processes for infinite mixture modeling. Introducing latent variables, the posterior computation is carried out using block Gibbs sampling without using the reversible jump algorithm. The performance of the proposed method is illustrated through some numerical experiments and real data examples, including clustering, density estimation, and community detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18854v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fumiya Iwashige, Shintaro Hashimoto</dc:creator>
    </item>
    <item>
      <title>Closed-form expressions for causal effects and rates of convergence for causal effect estimators under dependence</title>
      <link>https://arxiv.org/abs/2504.06108</link>
      <description>arXiv:2504.06108v2 Announce Type: replace 
Abstract: Causal inference in connected populations is non-trivial, because the treatment assignments of units can affect the outcomes of other units via treatment and outcome spillover. Since outcome spillover induces dependence among outcomes, closed-form expressions for causal effects and convergence rates for causal effect estimators are challenging and unavailable. We make three contributions. First, we provide closed-form expressions for causal effects under treatment and outcome spillover without making assumptions about the joint probability law of treatment assignments, outcomes, and connections beyond linearity of conditional expectations of outcomes and the standard assumptions of ignorability and positivity. The main results permit complex dependence among outcomes and connections. Second, we show that ignoring dependence among outcomes due to outcome spillover can induce asymptotic bias in causal effect estimators. Third, we establish convergence rates for causal effect estimators by controlling dependence and characterizing a high-probability subset of data that addresses collinearity issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06108v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhankar Bhadra, Michael Schweinberger</dc:creator>
    </item>
    <item>
      <title>Modeling the uncertainty on the covariance matrix for probabilistic forecast reconciliation</title>
      <link>https://arxiv.org/abs/2506.19554</link>
      <description>arXiv:2506.19554v3 Announce Type: replace 
Abstract: In forecast reconciliation, the covariance matrix of the base forecasts errors plays a crucial role. Typically, this matrix is estimated, and then treated as known. In contrast, we propose a Bayesian reconciliation model that accounts for the uncertainty in the estimation of the covariance matrix. This leads to a reconciled predictive distribution that follows a multivariate t-distribution, obtained in closed-form, rather than a multivariate Gaussian. We evaluate our method on three tourism-related datasets, including a new publicly available dataset. Empirical results show that our approach consistently improves prediction intervals compared to Gaussian reconciliation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19554v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Carrara, Dario Azzimonti, Giorgio Corani, Lorenzo Zambon</dc:creator>
    </item>
    <item>
      <title>Sequential Adaptive Priors for Orthogonal Functions</title>
      <link>https://arxiv.org/abs/2508.15552</link>
      <description>arXiv:2508.15552v2 Announce Type: replace 
Abstract: We propose a novel class of prior distributions for sequences of orthogonal functions, which are frequently required in various statistical models such as functional principal component analysis (FPCA). Our approach constructs priors sequentially by imposing adaptive orthogonality constraints through a hierarchical formulation of conditionally normal distributions. The orthogonality is controlled via hyperparameters, allowing for flexible trade-offs between exactness and smoothness, which can be learned from the observed data. We illustrate the properties of the proposed prior and show that it leads to nearly orthogonal posterior estimates. The proposed prior is employed in Bayesian FPCA, providing more interpretable principal functions and efficient low-rank representations. Through simulation studies and analysis of human mobility data in Tokyo, we demonstrate the superior performance of our approach in inducing orthogonality and improving functional component estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15552v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shonosuke Sugasawa, Daichi Mochihashi</dc:creator>
    </item>
    <item>
      <title>Causal Variance Decompositions for Measuring Health Inequalities</title>
      <link>https://arxiv.org/abs/2510.16975</link>
      <description>arXiv:2510.16975v2 Announce Type: replace 
Abstract: Recent causal inference literature has introduced causal effect decompositions to quantify sources of observed inequalities or disparities in outcomes but usually limiting this to pairwise comparisons. In the context of hospital profiling, comparison of hospital performance may reveal inequalities in healthcare delivery between sociodemographic groups, which may be explained by access/selection or actual effect modification. We consider the case of polytomous exposures in hospital profiling where the comparison is often to the system wide average performance, and decompose the observed variance in care delivery as the quantity of interest. For this, we formulate a new eight-way causal variance decomposition where we attribute the observed variation to components describing the main effects of hospital and group membership, modification of the hospital effect by group membership, hospital access/selection, effect of case-mix covariates and residual variance. We discuss the causal interpretation of the components, formulate parametric and nonparametric model based estimators and study the properties of these estimators through simulation. Finally, we illustrate our method by an example of cancer care delivery using data from the SEER database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16975v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Yu, Zhihui Liu, Kathy Han, Olli Saarela</dc:creator>
    </item>
    <item>
      <title>Divergence-based Robust Generalised Bayesian Inference for Directional Data via von Mises-Fisher models</title>
      <link>https://arxiv.org/abs/2512.05668</link>
      <description>arXiv:2512.05668v2 Announce Type: replace 
Abstract: This paper focusses on robust estimation of location and concentration parameters of the von Mises-Fisher distribution in the Bayesian framework. The von Mises-Fisher (or Langevin) distribution has played a central role in directional statistics. Directional data have been investigated for many decades, and more recently, they have gained increasing attention in diverse areas such as bioinformatics and text data analysis. Although outliers can significantly affect the estimation results even for directional data, the treatment of outliers remains an unresolved and challenging problem. In the frequentist framework, numerous studies have developed robust estimation methods for directional data with outliers, but, in contrast, only a few robust estimation methods have been proposed in the Bayesian framework. In this paper, we propose Bayesian inference based on the density power divergence and the $\gamma$-divergence and establish their asymptotic properties and robustness. In addition, the Bayesian approach naturally provides a way to assess estimation uncertainty through the posterior distribution, which is particularly useful for small samples. Furthermore, to carry out the posterior computation, we develop the posterior computation algorithm based on the weighted Bayesian bootstrap for estimating parameters. The effectiveness of the proposed methods is demonstrated through simulation studies. Using two real datasets, we further show that the proposed method provides reliable and robust estimation even in the presence of outliers or data contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05668v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoyuki Nakagawa, Yasuhito Tsuruta, Sho Kazari, Kouji Tahata</dc:creator>
    </item>
    <item>
      <title>Modeling Issues with Eye Tracking Data</title>
      <link>https://arxiv.org/abs/2512.15950</link>
      <description>arXiv:2512.15950v3 Announce Type: replace 
Abstract: I describe and compare procedures for binary eye-tracking (ET) data. The basic GLM model is a logistic mixed model combined with random effects for persons and items. Additional models address error correlation in eye-tracking serial observations. In particular, three novel approaches are illustrated that address serial without the use of an observed lag-1 predictor: a first-order autoregressive model and a first-order moving average models obtained with generalized estimating equations, and a recurrent two-state survival model used with run-length encoded data. Altogether, the results of five different analyses point to unresolved issues in the analysis of eye-tracking data and new directions for analytic development. A more traditional model incorporating a lag-1 observed outcome for serial correlation is also included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15950v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregory Camilli</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference for Causal Explainability</title>
      <link>https://arxiv.org/abs/2512.20219</link>
      <description>arXiv:2512.20219v2 Announce Type: replace 
Abstract: Understanding how much each variable contributes to an outcome is a central question across disciplines. A causal view of explainability is favorable for its ability in uncovering underlying mechanisms and generalizing to new contexts. Based on a family of causal explainability quantities, we develop methods for their estimation and inference. In particular, we construct a one-step correction estimator using semi-parametric efficiency theory, which explicitly leverages the independence structure of variables to reduce the asymptotic variance. For a null hypothesis on the boundary, i.e., zero explainability, we show its equivalence to Fisher's sharp null, which motivates a randomization-based inference procedure. Finally, we illustrate the empirical efficacy of our approach through simulations as well as an immigration experiment dataset, where we investigate how features and their interactions shape public opinion toward admitting immigrants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20219v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weihan Zhang, Zijun Gao</dc:creator>
    </item>
    <item>
      <title>Post-detection inference for sequential changepoint localization</title>
      <link>https://arxiv.org/abs/2502.06096</link>
      <description>arXiv:2502.06096v4 Announce Type: replace-cross 
Abstract: This paper addresses a fundamental but largely unexplored challenge in sequential changepoint analysis: conducting inference following a detected change. We develop a very general framework to construct confidence sets for the unknown changepoint using only the data observed up to a data-dependent stopping time at which an arbitrary sequential detection algorithm declares a change. Our framework is nonparametric, making no assumption on the composite post-change class, the observation space, or the sequential detection procedure used, and is non-asymptotically valid. We also extend it to handle composite pre-change classes under a suitable assumption, and also derive confidence sets for the change magnitude in parametric settings. We provide theoretical guarantees on the width of our confidence intervals. Extensive simulations demonstrate that the produced sets have reasonable size, and slightly conservative coverage. In summary, we present the first general method for sequential changepoint localization, which is theoretically sound and broadly applicable in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06096v4</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aytijhya Saha, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Optimal Model Selection for Conformalized Robust Optimization</title>
      <link>https://arxiv.org/abs/2507.04716</link>
      <description>arXiv:2507.04716v2 Announce Type: replace-cross 
Abstract: In decision-making under uncertainty, Contextual Robust Optimization (CRO) provides reliability by minimizing the worst-case decision loss over a prediction set. While recent advances use conformal prediction to construct prediction sets for machine learning models, the downstream decisions critically depend on model selection. This paper introduces novel model selection frameworks for CRO that unify robustness control with decision risk minimization. We first propose Conformalized Robust Optimization with Model Selection (CROMS), a framework that selects the model to approximately minimize the averaged decision risk in CRO solutions. Given the target robustness level 1-\alpha, we present a computationally efficient algorithm called E-CROMS, which achieves asymptotic robustness control and decision optimality. To correct the control bias in finite samples, we further develop two algorithms: F-CROMS, which ensures a 1-\alpha robustness but requires searching the label space; and J-CROMS, which offers lower computational cost while achieving a 1-2\alpha robustness. Furthermore, we extend the CROMS framework to the individualized setting, where model selection is performed by minimizing the conditional decision risk given the covariates of the test data. This framework advances conformal prediction methodology by enabling covariate-aware model selection. Numerical results demonstrate significant improvements in decision efficiency across diverse synthetic and real-world applications, outperforming baseline approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04716v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yajie Bao, Yang Hu, Haojie Ren, Peng Zhao, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Distributed Online Economic Dispatch with Time-Varying Coupled Inequality Constraints</title>
      <link>https://arxiv.org/abs/2512.16241</link>
      <description>arXiv:2512.16241v2 Announce Type: replace-cross 
Abstract: We investigate the distributed online economic dispatch problem for power systems with time-varying coupled inequality constraints. The problem is formulated as a distributed online optimization problem in a multi-agent system. At each time step, each agent only observes its own instantaneous objective function and local inequality constraints; agents make decisions online and cooperate to minimize the sum of the time-varying objectives while satisfying the global coupled constraints. To solve the problem, we propose an algorithm based on the primal-dual approach combined with constraint-tracking. Under appropriate assumptions that the objective and constraint functions are convex, their gradients are uniformly bounded, and the path length of the optimal solution sequence grows sublinearly, we analyze theoretical properties of the proposed algorithm and prove that both the dynamic regret and the constraint violation are sublinear with time horizon T. Finally, we evaluate the proposed algorithm on a time-varying economic dispatch problem in power systems using both synthetic data and Australian Energy Market data. The results demonstrate that the proposed algorithm performs effectively in terms of tracking performance, constraint satisfaction, and adaptation to time-varying disturbances, thereby providing a practical and theoretically well-supported solution for real-time distributed economic dispatch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16241v2</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingjie Zhou, Xiaoqian Wang, Tao Li</dc:creator>
    </item>
  </channel>
</rss>
