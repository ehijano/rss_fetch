<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Feb 2026 04:29:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Endogenous Inequality Aversion: Decision criteria for triage and other ethical tradeoffs</title>
      <link>https://arxiv.org/abs/2601.22250</link>
      <description>arXiv:2601.22250v1 Announce Type: new 
Abstract: Medical ``Crisis Standards of Care'' call for a utilitarian allocation of scarce resources in emergencies, while favoring the worst-off under normal conditions. Inspired by such triage rules, we introduce social welfare functions whose distributive tradeoffs depend on the prevailing level of aggregate welfare. These functions are inherently self-referential: they take the welfare level as an input, even though that level is itself determined by the function. In our formulation, inequality aversion varies with welfare and is therefore self-referential. We provide an axiomatic foundation for a family of social welfare functions that move from Rawlsian to utilitarian criteria as overall welfare falls, thereby formalizing triage guidelines. We also derive the converse case, in which the social objective shifts from Rawlsianism toward utilitarianism as welfare increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22250v1</guid>
      <category>econ.TH</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Echenique, Teddy Mekonnen, M. Bumin Yenmez</dc:creator>
    </item>
    <item>
      <title>Screening with Advertisements</title>
      <link>https://arxiv.org/abs/2601.22404</link>
      <description>arXiv:2601.22404v1 Announce Type: new 
Abstract: We investigate a seller's revenue-maximizing mechanism in a setting where a desirable good is sold together with an undesirable bad (e.g., advertisements) that generates third-party revenue. The buyer's private information is two-dimensional: valuation for the good and willingness to pay to avoid the bad. Following the duality framework of Daskalakis, Deckelbaum, and Tzamos (2017), whose results extend to our setting, we formulate the seller's problem using a transformed measure $\mu$ that depends on the third-party payment $k$. We provide a near-characterization for optimality of three pricing mechanisms commonly used in practice -- the Good-Only, Ad-Tiered, and Single-Bundle Posted Price -- and introduce a new class of tractable, interpretable two-dimensional orthant conditions on $\mu$ for sufficiency. Economically, $k$ yields a clean comparative static: low $k$ excludes the bad, intermediate $k$ separates ad-tolerant and ad-averse buyers, and high $k$ bundles ads for all types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22404v1</guid>
      <category>econ.TH</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kolagani Paramahamsa</dc:creator>
    </item>
    <item>
      <title>Persuasive Privacy</title>
      <link>https://arxiv.org/abs/2601.22945</link>
      <description>arXiv:2601.22945v1 Announce Type: cross 
Abstract: We propose a novel framework for measuring privacy from a Bayesian game-theoretic perspective. This framework enables the creation of new, purpose-driven privacy definitions that are rigorously justified, while also allowing for the assessment of existing privacy guarantees through game theory. We show that pure and probabilistic differential privacy are special cases of our framework, and provide new interpretations of the post-processing inequality in this setting. Further, we demonstrate that privacy guarantees can be established for deterministic algorithms, which are overlooked by current privacy standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22945v1</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>econ.TH</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua J Bon, James Bailie, Judith Rousseau, Christian P Robert</dc:creator>
    </item>
    <item>
      <title>Multi-agent Adaptive Mechanism Design</title>
      <link>https://arxiv.org/abs/2512.21794</link>
      <description>arXiv:2512.21794v2 Announce Type: replace-cross 
Abstract: We study a sequential mechanism design problem in which a principal seeks to elicit truthful reports from multiple rational agents while starting with no prior knowledge of agents' beliefs. We introduce Distributionally Robust Adaptive Mechanism (DRAM), a general framework combining insights from both mechanism design and online learning to jointly address truthfulness and cost-optimality. Throughout the sequential game, the mechanism estimates agents' beliefs and iteratively updates a distributionally robust linear program with shrinking ambiguity sets to reduce payments while preserving truthfulness. Our mechanism guarantees truthful reporting with high probability while achieving $\tilde{O}(\sqrt{T})$ cumulative regret, and we establish a matching lower bound showing that no truthful adaptive mechanism can asymptotically do better. The framework generalizes to plug-in estimators, supporting structured priors and delayed feedback. To our knowledge, this is the first adaptive mechanism under general settings that maintains truthfulness and achieves optimal regret when incentive constraints are unknown and must be learned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21794v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiushi Han, David Simchi-Levi, Renfei Tan, Zishuo Zhao</dc:creator>
    </item>
  </channel>
</rss>
