<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Oct 2025 04:01:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Spatio-temporal CP decomposition analysis of New England region in the US</title>
      <link>https://arxiv.org/abs/2510.10322</link>
      <description>arXiv:2510.10322v1 Announce Type: new 
Abstract: Spatio temporal data consist of measurement for one or more raster fields such as weather, traffic volume, crime rate, or disease incidents. Advances in modern technology have increased the number of available information for this type of data hence the rise of multidimensional data. In this paper we take advantage of the multidimensional structure of the data but also its temporal and spatial structure. In fact, we will be using the NCAR Climate Data Gateway website which provides data discovery and access services for global and regional climate model data. The daily values of total precipitation (prec), maximum (tmax), and minimum (tmin) temperature are combined to create a multidimensional data called tensor (a multidimensional array). In this paper, we propose a spatio temporal principal component analysis to initialize CP decomposition component. We take full advantage of the spatial and temporal structure of the data in the initialization step for cp component analysis. The performance of our method is tested via comparison with most popular initialization method. We also run a clustering analysis to further show the performance of our analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10322v1</guid>
      <category>stat.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fatoumata Sanogo</dc:creator>
    </item>
    <item>
      <title>A clustering algorithm for the single cell analysis of mixtures</title>
      <link>https://arxiv.org/abs/2510.10614</link>
      <description>arXiv:2510.10614v1 Announce Type: new 
Abstract: A probabilistic clustering algorithm is proposed for the analysis of forensic DNA mixtures in which individual cells are isolated and short tandem repeats are amplified using the polymerase chain reaction to generate single cell electropherograms. The task of the algorithm is to use the peak height information in the electropherograms to group the cells according to their contributors. Using a recently developed experimental set of individual cell electropherograms, a large set of simulations shows that the proposed clustering algorithm has excellent performance in correctly grouping single cells, and for assigning likelihood ratios for persons of interest (of known genotype).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10614v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert G. Cowell</dc:creator>
    </item>
    <item>
      <title>Age-period modeling of mortality gaps: the cases of cancer and circulatory diseases</title>
      <link>https://arxiv.org/abs/2510.10904</link>
      <description>arXiv:2510.10904v1 Announce Type: new 
Abstract: Understanding and modeling mortality patterns, especially differences in mortality rates between populations, is vital for demographic analysis and public health planning. We compare three statistical models within the age-period framework to examine differences in death counts. The models are based on the double Poisson, bivariate Poisson, and Skellam distributions, each of which provides unique strengths in capturing underlying mortality trends. Focusing on mortality data from 1960 to 2015, we analyze the two leading causes of death in Italy, which exhibit significant temporal and age-related variations. Our results reveal that the Skellam distribution offers superior accuracy and simplicity in capturing mortality differentials. These findings highlight the potential of the Skellam distribution for analyzing mortality gaps effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10904v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Giacomo Lanfiuti Baldi, Andrea Nigri, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Assessing the Influence of Locational Suitability on the Spatial Distribution of Household Wealth in Bernalillo County, NM</title>
      <link>https://arxiv.org/abs/2510.11048</link>
      <description>arXiv:2510.11048v1 Announce Type: new 
Abstract: This study applies Multiscale Geographically Weighted Regression (MGWR) to examine the spatial determinants of household wealth in Bernalillo County, New Mexico. The model incorporates sociodemographic, environmental, and proximity-based variables to evaluate how locational suitability influences economic outcomes. Key factors considered include income, home value, elevation, PM2.5 concentration, and distances to essential services such as schools, markets, and hospitals. The MGWR model demonstrates strong performance, explaining approximately 63 percent of the variation in household wealth. Results show that proximity to markets, schools, and parks significantly increases wealth in over 40 percent of neighborhoods. In contrast, closeness to hospitals and bus stops is negatively associated with wealth, suggesting that nearby disamenities can reduce housing desirability. Strong spatial autocorrelation (Morans I = 0.53, p &lt; 0.001) indicates that wealthier households are significantly clustered, highlighting the influence of localized factors. Overall, the study reveals that the relationship between locational suitability and household wealth is spatially variable across the county.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11048v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onyedikachi J. Okeke, Uloma E. Nelson, Chukwudi Nwaogu, Olumide O. Oladoyin, Emmanuel Kubuafor, Dennis Baidoo, Titilope Akinyemi, Adedoyin S. Ajeyomi, Rekiya A. Idris, Isaac A. Fabunmi</dc:creator>
    </item>
    <item>
      <title>Policy Robustness &amp; Uncertainty in Model-based Decision Support for the Energy Transition</title>
      <link>https://arxiv.org/abs/2510.11177</link>
      <description>arXiv:2510.11177v1 Announce Type: new 
Abstract: Climate policy modelling is a key tool for assessing mitigation strategies in complex systems and uncertainty is inherent and unavoidable. We present a general methodology for extensive uncertainty analysis in climate policy modelling. We show how emulators can identify key uncertainties in modelling frameworks and enable policy analysis previously restricted by computational cost. We apply this methodology to FTT:Power to explore uncertainties in the electricity system transition both globally and in India and to assess how robust mitigation strategies are to a vast range of policy and techno-economic scenarios. We find that uncertainties in transition outcomes are significantly larger than previously shown, but strong policy can narrow these ranges. Globally, plant construction and grid connection lead times dominate transition uncertainty, outweighing regional price policies, including policy reversals in the US. Solar PV proves most resilient due to low costs, though still sensitive to financing and infrastructure limits. Wind and other renewables are more vulnerable. In India, we find that policy packages including even partial phaseout instruments have greater robustness to key uncertainties although longer lead times still hinder policy goals. Our results highlight that reducing lead times and phasing out fossil fuels are critical for faster, more robust power sector transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11177v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian J. Burton, Femke J. M. M. Nijsse, James M. Salter</dc:creator>
    </item>
    <item>
      <title>A heavy-tail arctan-based mixture model for modelling and measuring actuarial risk</title>
      <link>https://arxiv.org/abs/2510.11315</link>
      <description>arXiv:2510.11315v1 Announce Type: new 
Abstract: Heavy-tailed probability distributions are extremely useful and play a crucial role in modeling different types of financial data sets. This study presents a two-pronged methodology. First, a mixture probability distribution is created by combining Gaussian and Rayleigh distributions using the arctangent transformation, aimed at producing heavier-tailed features and enhancing alignment with real market data. Some statistical properties of the proposed model are also discussed. Furthermore, essential actuarial risk evaluation instruments, such as value-at-risk (VaR), tail value-at-risk (TVaR) and tail variance (TV) are employed for efficient risk management practices. Lastly, an application is provided using an insurance dataset to demonstrate the applicability of the proposed model. The proposed model demonstrates superior fitting performance compared to current baseline distributions, showcasing its practical value in financial risk evaluation. The combination of Gaussian and Rayleigh distributions through arctangent transformation is particularly successful in representing extreme market behaviour and tail dependencies that are frequently found in real-world financial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11315v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pankaj Kumar, Vivek Vijay</dc:creator>
    </item>
    <item>
      <title>Multi-State Modeling of Greenhouse Cucumber Yield Dynamics Under Microclimate Effects</title>
      <link>https://arxiv.org/abs/2510.11485</link>
      <description>arXiv:2510.11485v1 Announce Type: new 
Abstract: Greenhouse decisions often rely on static thresholds, yet crop output switches among microclimate-driven regimes. We frame daily cucumber yield as transitions among three ordered states and fit a continuous-time, covariate-dependent multistate model. Data come from four greenhouse compartments in Volos, Greece (24 lines, 62 days). States are defined once from control tertiles and applied across compartments. Transition intensities depend on within-compartment z-scores of relative humidity (RH), photosynthetically active radiation (PAR) and CO2, plus fixed effects.
  Results show an inherent upward drift through the medium state, "sticky" low-yield spells unless conditions improve, and short-horizon persistence once high yield is reached. RH and PAR are dominant levers, accelerating upgrades and damping regressions; day-to-day CO2 deviations show no clear pooled signal. Residual differences between compartments are modest.
  By mapping intensities to 7--30 day probabilities, the model yields actionable guidance for humidity and lighting and a lightweight, interpretable component for greenhouse digital twins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11485v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiliano Seri, Francesco Biso, Gianluigi Bovesecchi, Nikolaos Katsoulas, Cristina Cornaro</dc:creator>
    </item>
    <item>
      <title>Generalizing Multimorbidity Models Across Countries: A Comparative Study of Austria and Denmark</title>
      <link>https://arxiv.org/abs/2510.09680</link>
      <description>arXiv:2510.09680v1 Announce Type: cross 
Abstract: Chronic diseases frequently co-occur in patterns that are unlikely to arise by chance, a phenomenon known as multimorbidity. This growing challenge for patients and healthcare systems is amplified by demographic aging and the rising burden of chronic conditions. However, our understanding of how individuals transition from a disease-free-state to accumulating diseases as they age is limited. Recently, data-driven methods have been developed to characterize morbidity trajectories using electronic health records; however, their generalizability across healthcare settings remains largely unexplored. In this paper, we conduct a cross-country validation of a data-driven multimorbidity trajectory model using population-wide health data from Denmark and Austria. Despite considerable differences in healthcare organization, we observe a high degree of similarity in disease cluster structures. The Adjusted Rand Index (0.998) and the Normalized Mutual Information (0.88) both indicate strong alignment between the two clusterings. These findings suggest that multimorbidity trajectories are shaped by robust, shared biological and epidemiological mechanisms that transcend national healthcare contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09680v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johanna Einsiedler, Katharina Ledebur, Peter Klimek, Laust Hvas Mortensen</dc:creator>
    </item>
    <item>
      <title>A Hybrid Agent-Based and System Dynamics Framework for Modelling Project Execution and Technology Maturity in Early-Stage R&amp;D</title>
      <link>https://arxiv.org/abs/2510.09688</link>
      <description>arXiv:2510.09688v1 Announce Type: cross 
Abstract: This paper presents a hybrid approach to predict the evolution of technological maturity in R and D projects, using the oil and gas sector as an example. Integrating System Dynamics (SD) and Agent Based Modelling (ABM) allows the proposed multi level framework to capture uncertainties in work effort, team size, and project duration, which influence technological progress. While AB SD hybrid models are established in other fields, their use in R and D remains limited. The model combines system level feedback structures governing work phases, rework cycles, and duration with decentralised agents such as team members, tasks, and controllers, whose interactions generate emergent project dynamics. A base case scenario analysed early stage innovation projects with 15 parallel tasks over 156 weeks. A comparative sequential scenario showed an 88 percent reduction in rework duration. A second scenario assessed mixed parallel sequential task structures with varying team sizes. In parallel configurations, increasing team size reduced project duration and improved task completion, with optimal results for teams of four to five members. These findings align with empirical evidence showing that moderate team expansion enhances coordination efficiency without excessive communication overhead. However, larger teams may decrease performance due to communication complexity and management delays. Overall, the model outputs and framework align with expert understanding, supporting their validity as quantitative tools for analysing resource allocation, scheduling efficiency, and technology maturity progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09688v1</guid>
      <category>cs.MA</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. W. S. Pessoa, M. H. N{\ae}ss, J. C. Bijos, C. M. Rebello, D. Colombo, L. Schnitman, I. B. R. Nogueira</dc:creator>
    </item>
    <item>
      <title>Interval-Censored Survival Analysis of Grapevine Phenology: Thermal Controls on Flowering and Fruit Ripening</title>
      <link>https://arxiv.org/abs/2510.09702</link>
      <description>arXiv:2510.09702v1 Announce Type: cross 
Abstract: European grapevine (\textit{Vitis vinifera} L.) is a climate-sensitive perennial whose flowering and ripening govern yield and quality. Phenological records from monitoring programs are typically collected at irregular intervals, so true transition dates are interval-censored, and many site-years are right-censored. We develop a reproducible workflow that treats phenology as a time-to-event outcome: Status \&amp; Intensity observations from the USA-NPN are converted to interval bounds, linked to NASA POWER daily weather, and analyzed with parametric accelerated failure time (AFT) models (Weibull and log-logistic). To avoid outcome-dependent bias from aggregating weather up to the event date, antecedent conditions are summarized in fixed pre-season windows and standardized; quality-control filters ensure adequate within-window data coverage. Applied to flowering and ripening of \textit{V.~vinifera}, the framework yields interpretable time-ratio effects and publication-ready tables and figures. Warmer pre-season conditions are associated with earlier ripening, whereas flowering responses are modest and uncertain in these data; precipitation plays, at most, a secondary role. The approach demonstrates how interval-censored survival models with exogenous weather windows can extract robust climate signals from citizen-science phenology while preserving observation uncertainty, and it generalizes readily to other species and networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09702v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Behnamian, Fatemeh Fogh</dc:creator>
    </item>
    <item>
      <title>How much can we learn from quantum random circuit sampling?</title>
      <link>https://arxiv.org/abs/2510.09919</link>
      <description>arXiv:2510.09919v1 Announce Type: cross 
Abstract: Benchmarking quantum devices is a foundational task for the sustained development of quantum technologies. However, accurate in situ characterization of large-scale quantum devices remains a formidable challenge: such systems experience many different sources of errors, and cannot be simulated on classical computers. Here, we introduce new benchmarking methods based on random circuit sampling (RCS), that substantially extend the scope of conventional approaches. Unlike existing benchmarks that report only a single quantity--the circuit fidelity--our framework extracts rich diagnostic information, including spatiotemporal error profiles, correlated and contextual errors, and biased readout errors, without requiring any modifications of the experiment. Furthermore, we develop techniques that achieve this task without classically intractable simulations of the quantum circuit, by leveraging side information, in the form of bitstring samples obtained from reference quantum devices. Our approach is based on advanced high-dimensional statistical modeling of RCS data. We sharply characterize the information-theoretic limits of error estimation, deriving matching upper and lower bounds on the sample complexity across all regimes of side information. We identify surprising phase transitions in learnability as the amount of side information varies. We demonstrate our methods using publicly available RCS data from a state-of-the-art superconducting processor, obtaining in situ characterizations that are qualitatively consistent yet quantitatively distinct from component-level calibrations. Our results establish both practical benchmarking protocols for current and future quantum computers and fundamental information-theoretic limits on how much can be learned from RCS data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09919v1</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tudor Manole, Daniel K. Mark, Wenjie Gong, Bingtian Ye, Yury Polyanskiy, Soonwon Choi</dc:creator>
    </item>
    <item>
      <title>Bayesian Multivariable Bidirectional Mendelian Randomization</title>
      <link>https://arxiv.org/abs/2510.09991</link>
      <description>arXiv:2510.09991v1 Announce Type: cross 
Abstract: Mendelian randomization (MR) is a pivotal tool in genetic epidemiology, leveraging genetic variants as instrumental variables to infer causal relationships between modifiable exposures and health outcomes. Traditional MR methods, while powerful, often rest on stringent assumptions such as the absence of feedback loops, which are frequently violated in complex biological systems. In addition, many popular MR approaches focus on only two variables (i.e., one exposure and one outcome) whereas our motivating applications have many variables. In this article, we introduce a novel Bayesian framework for \emph{multivariable} MR that concurrently addresses \emph{unmeasured confounding} and \emph{feedback loops}. Central to our approach is a sparse conditional cyclic graphical model with a sparse error variance-covariance matrix. Two structural priors are employed to enable the modeling and inference of causal relationships as well as latent confounding structures. Our method is designed to operate effectively with summary-level data, facilitating its application in contexts where individual-level data are inaccessible, e.g., due to privacy concerns. It can also account for horizontal pleiotropy. Through extensive simulations and applications to the GTEx and OneK1K data, we demonstrate the superior performance of our approach in recovering biologically plausible causal relationships in the presence of possible feedback loops and unmeasured confounding. The R package that implements the proposed method is available at \texttt{MR.RGM}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09991v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bitan Sarkar, Yuchao Jiang, Yang Ni</dc:creator>
    </item>
    <item>
      <title>Generalized Jeffreys's approximate objective Bayes factor: Model-selection consistency, finite-sample accuracy, and statistical evidence in 71,126 clinical trial findings</title>
      <link>https://arxiv.org/abs/2510.10358</link>
      <description>arXiv:2510.10358v1 Announce Type: cross 
Abstract: Concerns about the misuse and misinterpretation of p-values and statistical significance have motivated alternatives for quantifying evidence. We define a generalized form of Jeffreys's approximate objective Bayes factor (eJAB), a one-line calculation that is a function of the p-value, sample size, and parameter dimension. We establish conditions under which eJAB is model-selection consistent and verify them for ten statistical tests. We assess finite-sample accuracy by comparing eJAB with Markov chain Monte Carlo computed Bayes factors in 12 simulation studies. We then apply eJAB to 71,126 results from ClinicalTrials.gov (CTG) and find that the proportion of findings with $\text{p-value} \le \alpha$ yet $eJAB_{01}&gt;1$ (favoring the null) closely tracks the significance level $\alpha$, suggesting that such contradictions are pointing to the type I errors. We catalog 4,088 such candidate type I errors and provide details for 131 with reported $\text{p-value} \le 0.01$. We also identify 487 instances of the Jeffreys-Lindley paradox. Finally, we estimate that 75% (6%) of clinical trial plans from CTG set $\alpha \ge 0.05$ as the target evidence threshold, and that 35.5% (0.22%) of results significant at $\alpha =0.05$ correspond to evidence that is no stronger than anecdotal under eJAB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10358v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Puneet Velidi, Zhengxiao Wei, Shreena Nisha Kalaria, Yimeng Liu, C\'eline M. Laumont, Brad H. Nelson, Farouk S. Nathoo</dc:creator>
    </item>
    <item>
      <title>On the Problem of Consistent Anomalies in Zero-Shot Industrial Anomaly Detection</title>
      <link>https://arxiv.org/abs/2510.10456</link>
      <description>arXiv:2510.10456v1 Announce Type: cross 
Abstract: Zero-shot image anomaly classification (AC) and segmentation (AS) are vital for industrial quality control, detecting defects without prior training data. Existing representation-based methods compare patch features with nearest neighbors in unlabeled test images but struggle with consistent anomalies -- similar defects recurring across multiple images -- resulting in poor AC/AS performance. We introduce Consistent-Anomaly Detection Graph (CoDeGraph), a novel algorithm that identifies and filters consistent anomalies from similarity computations. Our key insight is that normal patches in industrial images show stable, gradually increasing similarity to other test images, while consistent-anomaly patches exhibit abrupt similarity spikes after exhausting a limited set of similar matches, a phenomenon we term ``neighbor-burnout.'' CoDeGraph constructs an image-level graph, with images as nodes and edges connecting those with shared consistent-anomaly patterns, using community detection to filter these anomalies. We provide a theoretical foundation using Extreme Value Theory to explain the effectiveness of our approach. Experiments on MVTec AD with the ViT-L-14-336 backbone achieve 98.3% AUROC for AC and AS performance of 66.8% (+4.2%) F1 and 68.1% (+5.4%) AP over state-of-the-art zero-shot methods. Using the DINOv2 backbone further improves segmentation, yielding 69.1% (+6.5%) F1 and 71.9% (+9.2%) AP, demonstrating robustness across architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10456v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tai Le-Gia, Ahn Jaehyun</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis</title>
      <link>https://arxiv.org/abs/2510.10762</link>
      <description>arXiv:2510.10762v1 Announce Type: cross 
Abstract: Systematic reviews are crucial for synthesizing scientific evidence but remain labor-intensive, especially when extracting detailed methodological information. Large language models (LLMs) offer potential for automating methodological assessments, promising to transform evidence synthesis. Here, using causal mediation analysis as a representative methodological domain, we benchmarked state-of-the-art LLMs against expert human reviewers across 180 full-text scientific articles. Model performance closely correlated with human judgments (accuracy correlation 0.71; F1 correlation 0.97), achieving near-human accuracy on straightforward, explicitly stated methodological criteria. However, accuracy sharply declined on complex, inference-intensive assessments, lagging expert reviewers by up to 15%. Errors commonly resulted from superficial linguistic cues -- for instance, models frequently misinterpreted keywords like "longitudinal" or "sensitivity" as automatic evidence of rigorous methodological approache, leading to systematic misclassifications. Longer documents yielded lower model accuracy, whereas publication year showed no significant effect. Our findings highlight an important pattern for practitioners using LLMs for methods review and synthesis from full texts: current LLMs excel at identifying explicit methodological features but require human oversight for nuanced interpretations. Integrating automated information extraction with targeted expert review thus provides a promising approach to enhance efficiency and methodological rigor in evidence synthesis across diverse scientific fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10762v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wenqing Zhang, Trang Nguyen, Elizabeth A. Stuart, Yiqun T. Chen</dc:creator>
    </item>
    <item>
      <title>Interpretable Machine Learning for Cognitive Aging: Handling Missing Data and Uncovering Social Determinant</title>
      <link>https://arxiv.org/abs/2510.10952</link>
      <description>arXiv:2510.10952v1 Announce Type: cross 
Abstract: Early detection of Alzheimer's disease (AD) is crucial because its neurodegenerative effects are irreversible, and neuropathologic and social-behavioral risk factors accumulate years before diagnosis. Identifying higher-risk individuals earlier enables prevention, timely care, and equitable resource allocation. We predict cognitive performance from social determinants of health (SDOH) using the NIH NIA-supported PREPARE Challenge Phase 2 dataset derived from the nationally representative Mex-Cog cohort of the 2003 and 2012 Mexican Health and Aging Study (MHAS).
  Data: The target is a validated composite cognitive score across seven domains-orientation, memory, attention, language, constructional praxis, and executive function-derived from the 2016 and 2021 MHAS waves. Predictors span demographic, socioeconomic, health, lifestyle, psychosocial, and healthcare access factors.
  Methodology: Missingness was addressed with a singular value decomposition (SVD)-based imputation pipeline treating continuous and categorical variables separately. This approach leverages latent feature correlations to recover missing values while balancing reliability and scalability. After evaluating multiple methods, XGBoost was chosen for its superior predictive performance.
  Results and Discussion: The framework outperformed existing methods and the data challenge leaderboard, demonstrating high accuracy, robustness, and interpretability. SHAP-based post hoc analysis identified top contributing SDOH factors and age-specific feature patterns. Notably, flooring material emerged as a strong predictor, reflecting socioeconomic and environmental disparities. Other influential factors, age, SES, lifestyle, social interaction, sleep, stress, and BMI, underscore the multifactorial nature of cognitive aging and the value of interpretable, data-driven SDOH modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10952v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xi Mao, Zhendong Wang, Jingyu Li, Lingchao Mao, Utibe Essien, Hairong Wang, Xuelei Sherry Ni</dc:creator>
    </item>
    <item>
      <title>Spatial and Temporal Boundaries in Difference-in-Differences: A Framework from Navier-Stokes Equation</title>
      <link>https://arxiv.org/abs/2510.11013</link>
      <description>arXiv:2510.11013v1 Announce Type: cross 
Abstract: This paper develops a unified framework for identifying spatial and temporal boundaries of treatment effects in difference-in-differences designs. Starting from fundamental fluid dynamics equations (Navier-Stokes), we derive conditions under which treatment effects decay exponentially in space and time, enabling researchers to calculate explicit boundaries beyond which effects become undetectable. The framework encompasses both linear (pure diffusion) and nonlinear (advection-diffusion with chemical reactions) regimes, with testable scope conditions based on dimensionless numbers from physics (P\'eclet and Reynolds numbers). We demonstrate the framework's diagnostic capability using air pollution from coal-fired power plants. Analyzing 791 ground-based PM$_{2.5}$ monitors and 189,564 satellite-based NO$_2$ grid cells in the Western United States over 2019-2021, we find striking regional heterogeneity: within 100 km of coal plants, both pollutants show positive spatial decay (PM$_{2.5}$: $\kappa_s = 0.00200$, $d^* = 1,153$ km; NO$_2$: $\kappa_s = 0.00112$, $d^* = 2,062$ km), validating the framework. Beyond 100 km, negative decay parameters correctly signal that urban sources dominate and diffusion assumptions fail. Ground-level PM$_{2.5}$ decays approximately twice as fast as satellite column NO$_2$, consistent with atmospheric transport physics. The framework successfully diagnoses its own validity in four of eight analyzed regions, providing researchers with physics-based tools to assess whether their spatial difference-in-differences setting satisfies diffusion assumptions before applying the estimator. Our results demonstrate that rigorous boundary detection requires both theoretical derivation from first principles and empirical validation of underlying physical assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11013v1</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>math.ST</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Data Integration and spatio temporal statistics can quantify relative risk of medico-legal reforms: the example of police emergency mental health responses in Queensland (Australia)</title>
      <link>https://arxiv.org/abs/2510.11101</link>
      <description>arXiv:2510.11101v1 Announce Type: cross 
Abstract: This study examined the spatial-temporal dynamics of Emergency Examination Order or Authority (EE-O/A) admissions in Far Northern Queensland (FNQ) from 2009 to 2020, using 13,035 unique police records aggregated across 83 postcodes. A two-stage modelling framework was used: Lasso was used to identify a parsimonious set of socio economic and health-service covariates, and a Conditional Autoregressive (CAR) model incorporated these predictors with structured spatial and temporal random effects. This research demonstrates that socio-economic disadvantage and service accessibility drive EE-O/A incidence, underscoring the need for targeted mental-health interventions and resource allocation in impoverished FNQ communities. Limitations include reliance on cross-sectional census data for covariates and potential ecological bias from data fusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11101v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nidup Dorji, Sourav Das, Richard Stone, Alan R. Clough</dc:creator>
    </item>
    <item>
      <title>The Role of Congeniality in Multiple Imputation for Doubly Robust Causal Estimation</title>
      <link>https://arxiv.org/abs/2510.11633</link>
      <description>arXiv:2510.11633v1 Announce Type: cross 
Abstract: This paper provides clear and practical guidance on the specification of imputation models when multiple imputation is used in conjunction with doubly robust estimation methods for causal inference. Through theoretical arguments and targeted simulations, we show that when a confounder has missing data the corresponding imputation model must include all variables used in either the propensity score model or the outcome model, and that these variables must appear in the same functional form as in the final analysis. Violating these conditions can lead to biased treatment effect estimates, even when both components of the doubly robust estimator are correctly specified. We present a mathematical framework for doubly robust estimation combined with multiple imputation, establish the theoretical requirements for proper imputation in this setting, and demonstrate the consequences of misspecification through simulation. Based on these findings, we offer concrete recommendations to ensure valid inference when using multiple imputation with doubly robust methods in applied causal analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11633v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucy D'Agostino McGowan</dc:creator>
    </item>
    <item>
      <title>Bayesian Perspective for Orientation Determination in Cryo-EM with Application to Structural Heterogeneity Analysis</title>
      <link>https://arxiv.org/abs/2412.03723</link>
      <description>arXiv:2412.03723v2 Announce Type: replace 
Abstract: Accurate orientation estimation is a crucial component of 3D molecular structure reconstruction, both in single-particle cryo-electron microscopy (cryo-EM) and in the increasingly popular field of cryo-electron tomography (cryo-ET). The dominant approach, which involves searching for the orientation that maximizes cross-correlation relative to given templates, is sub-optimal, particularly under low signal-to-noise conditions. In this work, we propose a Bayesian framework for more accurate and flexible orientation estimation, with the minimum mean square error (MMSE) estimator serving as a key example. Through simulations, we demonstrate that the MMSE estimator consistently outperforms the cross-correlation-based method, especially in challenging low signal-to-noise scenarios, and we provide a theoretical framework that supports these improvements.
  When incorporated into iterative refinement algorithms in the 3D reconstruction pipeline, the MMSE estimator markedly improves reconstruction accuracy, reduces model bias, and enhances robustness to the ``Einstein from Noise'' artifact. Crucially, we demonstrate that orientation estimation accuracy has a decisive effect on downstream structural heterogeneity analysis. In particular, integrating the MMSE-based pose estimator into frameworks for continuous heterogeneity recovery yields accuracy improvements approaching those obtained with ground-truth poses, establishing MMSE-based pose estimation as a key enabler of high-fidelity conformational landscape reconstruction. These findings indicate that the proposed Bayesian framework could substantially advance cryo-EM and cryo-ET by enhancing the accuracy, robustness, and reliability of 3D molecular structure reconstruction, thereby facilitating deeper insights into complex biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03723v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheng Xu, Amnon Balanov, Amit Singer, Tamir Bendory</dc:creator>
    </item>
    <item>
      <title>Automated flood detection from Sentinel-1 GRD time series using Bayesian analysis for change point problems</title>
      <link>https://arxiv.org/abs/2504.19526</link>
      <description>arXiv:2504.19526v4 Announce Type: replace 
Abstract: Current Synthetic Aperture Radar (SAR)-based flood detection methods face critical limitations that hinder operational deployment. Supervised learning approaches require extensive labeled training data, exhibit poor geographical transferability, and may fail to adapt to new regions without additional training examples. Existing approaches do not fully exploit the rich temporal information available in SAR time series, instead relying on simple change detection between pre- and post-flood images or supplementary datasets that often introduce error propagation. These limitations prevent effective automated flood monitoring in data-scarce regions where disaster response is most needed. To address these limitations, we develop a novel training-free approach by adapting Bayesian analysis for change point problems, specifically for automated flood detection from Sentinel-1 Ground Range Detected time series data. Our method statistically models the temporal behavior of SAR backscatter intensity over a one-year baseline period, then computes the posterior probability of change points at flood observation dates. This approach eliminates supervised learning dependencies by using Bayesian inference to identify when backscatter deviations exceed expected normal variations, leveraging inherent statistical properties of time series data. Validation across three diverse geographical contexts using the UrbanSARFloods benchmark dataset demonstrates superior performance compared to conventional thresholding and deep learning approaches, achieving F1 scores up to 0.75. This enables immediate deployment to any region with SAR coverage, providing critical advantages for disaster response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19526v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Narumasa Tsutsumida, Tomohiro Tanaka, Nifat Sultana</dc:creator>
    </item>
    <item>
      <title>TARD: Test-time Domain Adaptation for Robust Fault Detection under Evolving Operating Conditions</title>
      <link>https://arxiv.org/abs/2507.16354</link>
      <description>arXiv:2507.16354v2 Announce Type: replace 
Abstract: Fault detection is essential in complex industrial systems to prevent failures and optimize performance by distinguishing abnormal from normal operating conditions. With the growing availability of condition monitoring data, data-driven approaches have increasingly applied in detecting system faults. However, these methods typically require large, diverse, and representative training datasets that capture the full range of operating scenarios, an assumption rarely met in practice, particularly in the early stages of deployment.
  Industrial systems often operate under highly variable and evolving conditions, making it difficult to collect comprehensive training data. This variability results in a distribution shift between training and testing data, as future operating conditions may diverge from those previously observed ones. Such domain shifts hinder the generalization of traditional models, limiting their ability to transfer knowledge across time and system instances, ultimately leading to performance degradation in practical deployments.
  To address these challenges, we propose a novel method for continuous test-time domain adaptation, designed to support robust early-stage fault detection in the presence of domain shifts and limited representativeness of training data. Our proposed framework --Test-time domain Adaptation for Robust fault Detection (TARD) -- explicitly separates input features into system parameters and sensor measurements. It employs a dedicated domain adaptation module to adapt to each input type using different strategies, enabling more targeted and effective adaptation to evolving operating conditions. We validate our approach on two real-world case studies from multi-phase flow facilities, delivering substantial improvements in both fault detection accuracy and model robustness over existing domain adaptation methods under real-world variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16354v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Sun, Olga Fink</dc:creator>
    </item>
    <item>
      <title>Modeling and forecasting of European Carbon Emission Allowance futures by ARIMA-TX-GARCH models with correlation threshold</title>
      <link>https://arxiv.org/abs/2510.07568</link>
      <description>arXiv:2510.07568v2 Announce Type: replace 
Abstract: We propose an ARIMA-TX-GARCH model and use it to forecast European Carbon Emission Allowance futures prices, incorporating Brent crude oil futures prices as an exogenous variable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07568v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaeho Lee, Eunju Hwang</dc:creator>
    </item>
    <item>
      <title>Estimating Continuous Treatment Effects in Panel Data using Machine Learning with a Climate Application</title>
      <link>https://arxiv.org/abs/2207.08789</link>
      <description>arXiv:2207.08789v3 Announce Type: replace-cross 
Abstract: Economists often estimate continuous treatment effects in panel data using linear two-way fixed effects models (TWFE). When the treatment-outcome relationship is nonlinear, TWFE is misspecifed and potentially biased for the average partial derivative (APD). We develop an automatic double/de-biased machine learning (ADML) estimator that is consistent for the population APD while allowing additive unit fixed effects, nonlinearities, and high dimensional heterogeneity. We prove asymptotic normality and add two refinements - optimization based de-biasing and analytic derivatives - that reduce bias and remove numerical approximation error. Simulations show that the proposed method outperforms high order polynomial OLS and standard ML estimators. Our estimator leads to significantly larger (by 50%), but equally precise, estimates of the effect of extreme heat on corn yield compared to standard linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.08789v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sylvia Klosin, Max Vilgalys</dc:creator>
    </item>
    <item>
      <title>Density-valued time series: Nonparametric density-on-density regression</title>
      <link>https://arxiv.org/abs/2503.22904</link>
      <description>arXiv:2503.22904v2 Announce Type: replace-cross 
Abstract: This paper is concerned with forecasting probability density functions. Density functions are nonnegative and have a constrained integral; thus, they do not constitute a vector space. Implementing unconstrained functional time-series forecasting methods is problematic for such nonlinear and constrained data. A novel forecasting method is developed based on a nonparametric function-on-function regression, where both the response and the predictor are probability density functions. Asymptotic properties of our nonparametric regression estimator are established, as well as its finite-sample performance through a series of Monte-Carlo simulation studies. Using COVID-19 data from the French department and age-specific period life tables from the United States, we assess and compare the finite-sample forecast accuracy of the proposed method with several existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22904v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Ferraty, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>A longitudinal Bayesian framework for estimating causal dose-response relationships</title>
      <link>https://arxiv.org/abs/2505.20893</link>
      <description>arXiv:2505.20893v2 Announce Type: replace-cross 
Abstract: Existing causal methods for time-varying exposure and time-varying confounding focus on estimating the average causal effect of a time-varying binary treatment on an end-of-study outcome. Methods for estimating the effects of a time-varying continuous exposure at any dose level on the outcome are limited. We introduce a scalable, non-parametric Bayesian framework for estimating longitudinal causal dose-response relationships with repeated measures.We incorporate the generalized propensity score either as a covariate or through inverse-probability weighting, formulating two Bayesian dose-response estimators. The proposed approach embeds a double non-parametric generalized Bayesian bootstrap which enables a flexible Dirichlet process specification within a generalized estimating equations structure, capturing temporal correlation while making minimal assumptions about the functional form of the continuous exposure. We applied our proposed approach to a motivating study of monthly metro-ridership data and COVID-19 case counts from major international cities, identifying causal relationships and the dynamic dose-response patterns between higher ridership and increased case counts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20893v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Luo, Kuan Liu, Ramandeep Singh, Daniel J. Graham</dc:creator>
    </item>
    <item>
      <title>Machine Learning with Multitype Protected Attributes: Intersectional Fairness through Regularisation</title>
      <link>https://arxiv.org/abs/2509.08163</link>
      <description>arXiv:2509.08163v2 Announce Type: replace-cross 
Abstract: Ensuring equitable treatment (fairness) across protected attributes (such as gender or ethnicity) is a critical issue in machine learning. Most existing literature focuses on binary classification, but achieving fairness in regression tasks-such as insurance pricing or hiring score assessments-is equally important. Moreover, anti-discrimination laws also apply to continuous attributes, such as age, for which many existing methods are not applicable. In practice, multiple protected attributes can exist simultaneously; however, methods targeting fairness across several attributes often overlook so-called "fairness gerrymandering", thereby ignoring disparities among intersectional subgroups (e.g., African-American women or Hispanic men). In this paper, we propose a distance covariance regularisation framework that mitigates the association between model predictions and protected attributes, in line with the fairness definition of demographic parity, and that captures both linear and nonlinear dependencies. To enhance applicability in the presence of multiple protected attributes, we extend our framework by incorporating two multivariate dependence measures based on distance covariance: the previously proposed joint distance covariance (JdCov) and our novel concatenated distance covariance (CCdCov), which effectively address fairness gerrymandering in both regression and classification tasks involving protected attributes of various types. We discuss and illustrate how to calibrate regularisation strength, including a method based on Jensen-Shannon divergence, which quantifies dissimilarities in prediction distributions across groups. We apply our framework to the COMPAS recidivism dataset and a large motor insurance claims dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08163v2</guid>
      <category>cs.LG</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ho Ming Lee, Katrien Antonio, Benjamin Avanzi, Lorenzo Marchi, Rui Zhou</dc:creator>
    </item>
  </channel>
</rss>
