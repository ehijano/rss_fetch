<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jul 2024 01:38:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Data Sharing for Mean Estimation Among Heterogeneous Strategic Agents</title>
      <link>https://arxiv.org/abs/2407.15881</link>
      <description>arXiv:2407.15881v1 Announce Type: new 
Abstract: We study a collaborative learning problem where $m$ agents estimate a vector $\mu\in\mathbb{R}^d$ by collecting samples from normal distributions, with each agent $i$ incurring a cost $c_{i,k} \in (0, \infty]$ to sample from the $k^{\text{th}}$ distribution $\mathcal{N}(\mu_k, \sigma^2)$. Instead of working on their own, agents can collect data that is cheap to them, and share it with others in exchange for data that is expensive or even inaccessible to them, thereby simultaneously reducing data collection costs and estimation error. However, when agents have different collection costs, we need to first decide how to fairly divide the work of data collection so as to benefit all agents. Moreover, in naive sharing protocols, strategic agents may under-collect and/or fabricate data, leading to socially undesirable outcomes. Our mechanism addresses these challenges by combining ideas from cooperative and non-cooperative game theory. We use ideas from axiomatic bargaining to divide the cost of data collection. Given such a solution, we develop a Nash incentive-compatible (NIC) mechanism to enforce truthful reporting. We achieve a $\mathcal{O}(\sqrt{m})$ approximation to the minimum social penalty (sum of agent estimation errors and data collection costs) in the worst case, and a $\mathcal{O}(1)$ approximation under favorable conditions. We complement this with a hardness result, showing that $\Omega(\sqrt{m})$ is unavoidable in any NIC mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15881v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Clinton, Yiding Chen, Xiaojin Zhu, Kirthevasan Kandasamy</dc:creator>
    </item>
    <item>
      <title>von Neumann and Newman Pokers with Finite Decks</title>
      <link>https://arxiv.org/abs/2407.16155</link>
      <description>arXiv:2407.16155v1 Announce Type: new 
Abstract: John von Neumann studied a simplified version of poker where the "deck" consists of infinitely many cards, in fact, all real numbers between $0$ and $1$. We harness the power of computation, both numeric and symbolic, to investigate analogs with finitely many cards. We also study finite analogs of a simplified poker introduced by D.J. Newman, and conclude with a thorough investigation, fully implemented in Maple, of the three-player game, doing both the finite and the infinite versions. This paper is accompanied by two Maple packages and numerous output files.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16155v1</guid>
      <category>cs.GT</category>
      <category>math.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tipaluck Krityakierne, Thotsaporn Aek Thanatipanonda, Doron Zeilberger</dc:creator>
    </item>
    <item>
      <title>Faster Optimal Coalition Structure Generation via Offline Coalition Selection and Graph-Based Search</title>
      <link>https://arxiv.org/abs/2407.16092</link>
      <description>arXiv:2407.16092v1 Announce Type: cross 
Abstract: Coalition formation is a key capability in multi-agent systems. An important problem in coalition formation is coalition structure generation: partitioning agents into coalitions to optimize the social welfare. This is a challenging problem that has been the subject of active research for the past three decades. In this paper, we present a novel algorithm, SMART, for the problem based on a hybridization of three innovative techniques. Two of these techniques are based on dynamic programming, where we show a powerful connection between the coalitions selected for evaluation and the performance of the algorithms. These algorithms use offline phases to optimize the choice of coalitions to evaluate. The third one uses branch-and-bound and integer partition graph search to explore the solution space. Our techniques bring a new way of approaching the problem and a new level of precision to the field. In experiments over several common value distributions, we show that the hybridization of these techniques in SMART is faster than the fastest prior algorithms (ODP-IP, BOSS) in generating optimal solutions across all the value distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16092v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Redha Taguelmimt, Samir Aknine, Djamila Boukredera, Narayan Changder, Tuomas Sandholm</dc:creator>
    </item>
    <item>
      <title>MOMAland: A Set of Benchmarks for Multi-Objective Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2407.16312</link>
      <description>arXiv:2407.16312v1 Announce Type: cross 
Abstract: Many challenging tasks such as managing traffic systems, electricity grids, or supply chains involve complex decision-making processes that must balance multiple conflicting objectives and coordinate the actions of various independent decision-makers (DMs). One perspective for formalising and addressing such tasks is multi-objective multi-agent reinforcement learning (MOMARL). MOMARL broadens reinforcement learning (RL) to problems with multiple agents each needing to consider multiple objectives in their learning process. In reinforcement learning research, benchmarks are crucial in facilitating progress, evaluation, and reproducibility. The significance of benchmarks is underscored by the existence of numerous benchmark frameworks developed for various RL paradigms, including single-agent RL (e.g., Gymnasium), multi-agent RL (e.g., PettingZoo), and single-agent multi-objective RL (e.g., MO-Gymnasium). To support the advancement of the MOMARL field, we introduce MOMAland, the first collection of standardised environments for multi-objective multi-agent reinforcement learning. MOMAland addresses the need for comprehensive benchmarking in this emerging field, offering over 10 diverse environments that vary in the number of agents, state representations, reward structures, and utility considerations. To provide strong baselines for future research, MOMAland also includes algorithms capable of learning policies in such settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16312v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Felten, Umut Ucak, Hicham Azmani, Gao Peng, Willem R\"opke, Hendrik Baier, Patrick Mannion, Diederik M. Roijers, Jordan K. Terry, El-Ghazali Talbi, Gr\'egoire Danoy, Ann Now\'e, Roxana R\u{a}dulescu</dc:creator>
    </item>
    <item>
      <title>Canadian Traveller Problems in Temporal Graphs</title>
      <link>https://arxiv.org/abs/2407.16491</link>
      <description>arXiv:2407.16491v1 Announce Type: cross 
Abstract: This paper formalises the Canadian Traveller problem as a positional two-player game on graphs. We consider two variants depending on whether an edge is blocked. In the locally-informed variant, the traveller learns if an edge is blocked upon reaching one of its endpoints, while in the uninformed variant, they discover this only when the edge is supposed to appear. We provide a polynomial algorithm for each shortest path variant in the uninformed case. This algorithm also solves the case of directed acyclic non-temporal graphs.
  In the locally-informed case, we prove that finding a winning strategy is PSPACE-complete. Moreover, we establish that the problem is polynomial-time solvable when $k=1$ but NP-hard for $k\geq 2$.
  Additionally, we show that the standard (non-temporal) Canadian Traveller Problem is NP-hard when there are $k\geq 4$ blocked edges, which is, to the best of our knowledge, the first hardness result for CTP for a constant number of blocked edges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16491v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Bellitto, Johanne Cohen, Bruno Escoffier, Minh-Hang Nguyen, Mikael Rabie</dc:creator>
    </item>
    <item>
      <title>Fair and Almost Truthful Mechanisms for Additive Valuations and Beyond</title>
      <link>https://arxiv.org/abs/2306.15920</link>
      <description>arXiv:2306.15920v2 Announce Type: replace 
Abstract: We study the problem of fairly allocating indivisible goods among $n$ strategic agents. It is well-known that truthfulness is incompatible with any meaningful fairness notions. We bypass the strong negative result by considering the concept of incentive ratio, a relaxation of truthfulness quantifying agents' incentive to misreport. Previous studies show that Round-Robin, which satisfies envy-freeness up to one good (EF1), achieves an incentive ratio of $2$ for additive valuations.
  In this paper, we explore the incentive ratio achievable by fair mechanisms for various classes of valuations besides additive ones. We first show that, for arbitrary $\epsilon &gt; 0$, every $(\frac{1}{2} + \epsilon)$-EF1 mechanism for additive valuations admits an incentive ratio of at least $1.5$. Then, using the above lower bound for additive valuations in a black-box manner, we show that for arbitrary $\epsilon &gt; 0$, every $\epsilon$-EF1 mechanism for cancelable valuations admits an infinite incentive ratio. Moreover, for subadditive cancelable valuations, we show that Round-Robin, which satisfies EF1, achieves an incentive ratio of $2$, and every $(\varphi - 1)$-EF1 mechanism admits an incentive ratio of at least $\varphi$ with $\varphi = (1 + \sqrt{5}) / 2 \approx 1.618$. Finally, for submodular valuations, we show that Round-Robin, which satisfies $\frac{1}{2}$-EF1, admits an incentive ratio of $n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15920v2</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Biaoshuai Tao, Mingwei Yang</dc:creator>
    </item>
    <item>
      <title>Automated Security Response through Online Learning with Adaptive Conjectures</title>
      <link>https://arxiv.org/abs/2402.12499</link>
      <description>arXiv:2402.12499v2 Announce Type: replace 
Abstract: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We present our method through an advanced persistent threat use case. Testbed evaluations show that our method produces effective security strategies that adapt to a changing environment. We also find that our method enables faster convergence than current reinforcement learning techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12499v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kim Hammar, Tao Li, Rolf Stadler, Quanyan Zhu</dc:creator>
    </item>
    <item>
      <title>Compatibility of Fairness and Nash Welfare under Subadditive Valuations</title>
      <link>https://arxiv.org/abs/2407.12461</link>
      <description>arXiv:2407.12461v2 Announce Type: replace 
Abstract: We establish a compatibility between fairness and efficiency, captured via Nash Social Welfare (NSW), under the broad class of subadditive valuations. We prove that, for subadditive valuations, there always exists a partial allocation that is envy-free up to the removal of any good (EFx) and has NSW at least half of the optimal; here, optimality is considered across all allocations, fair or otherwise. We also prove, for subadditive valuations, the universal existence of complete allocations that are envy-free up to one good (EF1) and also achieve a factor $1/2$ approximation to the optimal NSW. Our EF1 result resolves an open question posed by Garg et al. (STOC 2023).
  In addition, we develop a polynomial-time algorithm which, given an arbitrary allocation \~A as input, returns an EF1 allocation with NSW at least $1/3$ times that of \~A. Therefore, our results imply that the EF1 criterion can be attained simultaneously with a constant-factor approximation to optimal NSW in polynomial time (with demand queries), for subadditive valuations. The previously best-known approximation factor for optimal NSW, under EF1 and among $n$ agents, was $O(n)$ - we improve this bound to $O(1)$.
  It is known that EF1 and exact Pareto efficiency (PO) are incompatible with subadditive valuations. Complementary to this negative result, the current work shows that we regain compatibility by just considering a factor $1/2$ approximation: EF1 can be achieved in conjunction with $\frac{1}{2}$-PO under subadditive valuations. As such, our results serve as a general tool that can be used as a black box to convert any efficient outcome into a fair one, with only a marginal decrease in efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12461v2</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddharth Barman, Mashbat Suzuki</dc:creator>
    </item>
    <item>
      <title>Incentivizing Hidden Types in Secretary Problem</title>
      <link>https://arxiv.org/abs/2208.05897</link>
      <description>arXiv:2208.05897v2 Announce Type: replace-cross 
Abstract: We study a game between $N$ job applicants who incur a cost $c$ (relative to the job value) to reveal their type during interviews and an administrator who seeks to maximize the probability of hiring the best. We define a full learning equilibrium and prove its existence, uniqueness, and optimality. In equilibrium, the administrator accepts the current best applicant $n$ with probability $c$ if $n&lt;n^*$ and with probability 1 if $n\ge n^*$ for a threshold $n^*$ independent of $c$. In contrast to the case without cost, where the success probability converges to $1/\mathrm{e}\approx 0.37$ as $N$ tends to infinity, with cost the success probability decays like $N^{-c}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.05897v2</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Longjian Li, Alexis Akira Toda</dc:creator>
    </item>
    <item>
      <title>Static Pricing Guarantees for Queueing Systems</title>
      <link>https://arxiv.org/abs/2305.09168</link>
      <description>arXiv:2305.09168v3 Announce Type: replace-cross 
Abstract: We consider a general queueing model with price-sensitive customers in which the service provider seeks to balance two objectives, maximizing the average revenue rate and minimizing the average queue length. Customers arrive according to a Poisson process, observe an offered price, and decide to join the queue if their valuation exceeds the price. The queue is operated first-in first-out, and the service times are exponential. Our model represents applications in areas like make-to-order manufacturing, cloud computing, and food delivery.
  The optimal solution for our model is dynamic; the price changes as the state of the system changes. However, such dynamic pricing policies may be undesirable for a variety of reasons. In this work, we provide performance guarantees for a simple and natural class of static pricing policies which charge a fixed price up to a certain occupancy threshold and then allow no more customers into the system. We provide a series of results showing that such static policies can simultaneously guarantee a constant fraction of the optimal revenue with at most a constant factor increase in expected queue length. For instance, our policy for the M/M/1 setting allows bi-criteria approximations of $(0.5, 1), (0.66, 1.16), (0.75, 1.54)$ and $(0.8, 2)$ for the revenue and queue length, respectively. We also provide guarantees for settings with multiple customer classes and multiple servers, as well as the expected sojourn time objective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.09168v3</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Bergquist, Adam N. Elmachtoub</dc:creator>
    </item>
    <item>
      <title>The Last Success Problem with Samples</title>
      <link>https://arxiv.org/abs/2308.09356</link>
      <description>arXiv:2308.09356v2 Announce Type: replace-cross 
Abstract: The last success problem is an optimal stopping problem that aims to maximize the probability of stopping on the last success in a sequence of independent $n$ Bernoulli trials. In the classical setting where complete information about the distributions is available, Bruss~\cite{B00} provided an optimal stopping policy that ensures a winning probability of $1/e$. However, assuming complete knowledge of the distributions is unrealistic in many practical applications. This paper investigates a variant of the last success problem where samples from each distribution are available instead of complete knowledge of them. When a single sample from each distribution is allowed, we provide a deterministic policy that guarantees a winning probability of $1/4$. This is best possible by the upper bound provided by Nuti and Vondr\'{a}k~\cite{NV23}. Furthermore, for any positive constant $\epsilon$, we show that a constant number of samples from each distribution is sufficient to guarantee a winning probability of $1/e-\epsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09356v2</guid>
      <category>math.PR</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toru Yoshinaga, Yasushi Kawase</dc:creator>
    </item>
  </channel>
</rss>
