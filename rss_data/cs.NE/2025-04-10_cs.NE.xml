<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Apr 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Evolutionary Generation of Random Surreal Numbers for Benchmarking</title>
      <link>https://arxiv.org/abs/2504.07152</link>
      <description>arXiv:2504.07152v1 Announce Type: new 
Abstract: There are many areas of scientific endeavour where large, complex datasets are needed for benchmarking. Evolutionary computing provides a means towards creating such sets. As a case study, we consider Conway's Surreal numbers. They have largely been treated as a theoretical construct, with little effort towards empirical study, at least in part because of the difficulty of working with all but the smallest numbers. To advance this status, we need efficient algorithms, and in order to develop such we need benchmark data sets of surreal numbers. In this paper, we present a method for generating ensembles of random surreal numbers to benchmark algorithms. The approach uses an evolutionary algorithm to create the benchmark datasets where we can analyse and control features of the resulting test sets. Ultimately, the process is designed to generate networks with defined properties, and we expect this to be useful for other types of network data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07152v1</guid>
      <category>cs.NE</category>
      <category>math.CO</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3712255.3726584</arxiv:DOI>
      <arxiv:journal_reference>Genetic and Evolutionary Computation Conference (GECCO '25), July 14--18, 2025, Malaga</arxiv:journal_reference>
      <dc:creator>Matthew Roughan</dc:creator>
    </item>
    <item>
      <title>GAAPO: Genetic Algorithmic Applied to Prompt Optimization</title>
      <link>https://arxiv.org/abs/2504.07157</link>
      <description>arXiv:2504.07157v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, with their performance heavily dependent on the quality of input prompts \cite{schulhoff2025promptsurvey} \cite{sahoo2025promptengineering}. While prompt engineering has proven effective, it typically relies on manual adjustments, making it time-consuming and potentially suboptimal. This paper introduces GAAPO (Genetic Algorithm Applied to Prompt Optimization), a novel hybrid optimization framework that leverages genetic algorithm \cite{dejong1988gen} principles to evolve prompts through successive generations. Unlike traditional genetic approaches that rely solely on mutation and crossover operations, GAAPO integrates multiple specialized prompt generation strategies within its evolutionary framework. Through extensive experimentation on diverse datasets including ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important point for the future development of automatic prompt optimization methods: importance of the tradeoff between the population size and the number of generations, effect of selection methods on stability results, capacity of different LLMs and especially reasoning models to be able to automatically generate prompts from similar queries... Furthermore, we provide insights into the relative effectiveness of different prompt generation strategies and their evolution across optimization phases. These findings contribute to both the theoretical understanding of prompt optimization and practical applications in improving LLM performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07157v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xavier S\'echeresse, Jacques-Yves Guilbert--Ly, Antoine Villedieu de Torcy</dc:creator>
    </item>
    <item>
      <title>Evolutionary algorithms meet self-supervised learning: a comprehensive survey</title>
      <link>https://arxiv.org/abs/2504.07213</link>
      <description>arXiv:2504.07213v1 Announce Type: new 
Abstract: The number of studies that combine Evolutionary Machine Learning and self-supervised learning has been growing steadily in recent years. Evolutionary Machine Learning has been shown to help automate the design of machine learning algorithms and to lead to more reliable solutions. Self-supervised learning, on the other hand, has produced good results in learning useful features when labelled data is limited. This suggests that the combination of these two areas can help both in shaping evolutionary processes and in automating the design of deep neural networks, while also reducing the need for labelled data. Still, there are no detailed reviews that explain how Evolutionary Machine Learning and self-supervised learning can be used together. To help with this, we provide an overview of studies that bring these areas together. Based on this growing interest and the range of existing works, we suggest a new sub-area of research, which we call Evolutionary Self-Supervised Learning and introduce a taxonomy for it. Finally, we point out some of the main challenges and suggest directions for future research to help Evolutionary Self-Supervised Learning grow and mature as a field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07213v1</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adriano Vinhas, Jo\~ao Correia, Penousal Machado</dc:creator>
    </item>
    <item>
      <title>A Balanced Approach of Rapid Genetic Exploration and Surrogate Exploitation for Hyperparameter Optimization</title>
      <link>https://arxiv.org/abs/2504.07359</link>
      <description>arXiv:2504.07359v1 Announce Type: new 
Abstract: This paper proposes a new method for hyperparameter optimization (HPO) that balances exploration and exploitation. While evolutionary algorithms (EAs) show promise in HPO, they often struggle with effective exploitation. To address this, we integrate a linear surrogate model into a genetic algorithm (GA), allowing for smooth integration of multiple strategies. This combination improves exploitation performance, achieving an average improvement of 1.89 percent (max 6.55 percent, min -3.45 percent) over existing HPO methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07359v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2024.3508269</arxiv:DOI>
      <arxiv:journal_reference>IEEE Access, vol. 12, pp. 29590-29601, 2024</arxiv:journal_reference>
      <dc:creator>Chul Kim, Inwhee Joe</dc:creator>
    </item>
    <item>
      <title>Probability Estimation and Scheduling Optimization for Battery Swap Stations via LRU-Enhanced Genetic Algorithm and Dual-Factor Decision System</title>
      <link>https://arxiv.org/abs/2504.07453</link>
      <description>arXiv:2504.07453v1 Announce Type: new 
Abstract: To address the challenges of limited Battery Swap Stations datasets, high operational costs, and fluctuating user charging demand, this research proposes a probability estimation model based on charging pile data and constructs nine scenario-specific battery swap demand datasets. In addition, this study combines Least Recently Used strategy with Genetic Algorithm and incorporates a guided search mechanism, which effectively enhances the global optimization capability. Thus, a dual-factor decision-making based charging schedule optimization system is constructed. Experimental results show that the constructed datasets exhibit stable trend characteristics, adhering to 24-hour and 168-hour periodicity patterns, with outlier ratios consistently below 3.26%, confirming data validity. Compared to baseline, the improved algorithm achieves better fitness individuals in 80% of test regions under the same iterations. When benchmarked against immediate swap-and-charge strategy, our algorithm achieves a peak cost reduction of 13.96%. Moreover, peak user satisfaction reaches 98.57%, while the average iteration time remains below 0.6 seconds, demonstrating good computational efficiency. The complete datasets and optimization algorithm are open-sourced at https://github.com/qingshufan/GA-EVLRU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07453v1</guid>
      <category>cs.NE</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anzhen Li, Shufan Qing, Xiaochang Li, Rui Mao, Mingchen Feng</dc:creator>
    </item>
    <item>
      <title>CatCMA with Margin: Stochastic Optimization for Continuous, Integer, and Categorical Variables</title>
      <link>https://arxiv.org/abs/2504.07884</link>
      <description>arXiv:2504.07884v1 Announce Type: new 
Abstract: This study focuses on mixed-variable black-box optimization (MV-BBO), addressing continuous, integer, and categorical variables. Many real-world MV-BBO problems involve dependencies among these different types of variables, requiring efficient methods to optimize them simultaneously. Recently, stochastic optimization methods leveraging the mechanism of the covariance matrix adaptation evolution strategy have shown promising results in mixed-integer or mixed-category optimization. However, such methods cannot handle the three types of variables simultaneously. In this study, we propose CatCMA with Margin (CatCMAwM), a stochastic optimization method for MV-BBO that jointly optimizes continuous, integer, and categorical variables. CatCMAwM is developed by incorporating a novel integer handling into CatCMA, a mixed-category black-box optimization method employing a joint distribution of multivariate Gaussian and categorical distributions. The proposed integer handling is carefully designed by reviewing existing integer handlings and following the design principles of CatCMA. Even when applied to mixed-integer problems, it stabilizes the marginal probability and improves the convergence performance of continuous variables. Numerical experiments show that CatCMAwM effectively handles the three types of variables, outperforming state-of-the-art Bayesian optimization methods and baselines that simply incorporate existing integer handlings into CatCMA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07884v1</guid>
      <category>cs.NE</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3712256.3726471</arxiv:DOI>
      <dc:creator>Ryoki Hamano, Masahiro Nomura, Shota Saito, Kento Uchida, Shinichi Shirakawa</dc:creator>
    </item>
    <item>
      <title>Reservoir Computing with a Single Oscillating Gas Bubble: Emphasizing the Chaotic Regime</title>
      <link>https://arxiv.org/abs/2504.07221</link>
      <description>arXiv:2504.07221v1 Announce Type: cross 
Abstract: The rising computational and energy demands of artificial intelligence systems urge the exploration of alternative software and hardware solutions that exploit physical effects for computation. According to machine learning theory, a neural network-based computational system must exhibit nonlinearity to effectively model complex patterns and relationships. This requirement has driven extensive research into various nonlinear physical systems to enhance the performance of neural networks. In this paper, we propose and theoretically validate a reservoir computing system based on a single bubble trapped within a bulk of liquid. By applying an external acoustic pressure wave to both encode input information and excite the complex nonlinear dynamics, we showcase the ability of this single-bubble reservoir computing system to forecast complex benchmarking time series and undertake classification tasks with high accuracy. Specifically, we demonstrate that a chaotic physical regime of bubble oscillation proves to be the most effective for this kind of computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07221v1</guid>
      <category>nlin.CD</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>physics.flu-dyn</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hend Abdel-Ghani, A. H. Abbas, Ivan S. Maksymov</dc:creator>
    </item>
    <item>
      <title>Kernel Logistic Regression Learning for High-Capacity Hopfield Networks</title>
      <link>https://arxiv.org/abs/2504.07633</link>
      <description>arXiv:2504.07633v1 Announce Type: cross 
Abstract: Hebbian learning limits Hopfield network storage capacity (pattern-to-neuron ratio around 0.14). We propose Kernel Logistic Regression (KLR) learning. Unlike linear methods, KLR uses kernels to implicitly map patterns to high-dimensional feature space, enhancing separability. By learning dual variables, KLR dramatically improves storage capacity, achieving perfect recall even when pattern numbers exceed neuron numbers (up to ratio 1.5 shown), and enhances noise robustness. KLR demonstrably outperforms Hebbian and linear logistic regression approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07633v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akira Tamamori</dc:creator>
    </item>
    <item>
      <title>Abnormal Mutations: Evolution Strategies Don't Require Gaussianity</title>
      <link>https://arxiv.org/abs/2502.03148</link>
      <description>arXiv:2502.03148v2 Announce Type: replace 
Abstract: The mutation process in evolution strategies has been interlinked with the normal distribution since its inception. Many lines of reasoning have been given for this strong dependency, ranging from maximum entropy arguments to the need for isotropy. However, some theoretical results suggest that other distributions might lead to similar local convergence properties. This paper empirically shows that a wide range of evolutionary strategies, from the (1+1)-ES to CMA-ES, show comparable optimization performance when using a mutation distribution other than the standard Gaussian. Replacing it with, e.g., uniformly distributed mutations, does not deteriorate the performance of ES, when using the default adaptation mechanism for the strategy parameters. We observe that these results hold not only for the sphere model but also for a wider range of benchmark problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03148v2</guid>
      <category>cs.NE</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob de Nobel, Diederick Vermetten, Hao Wang, Anna V. Kononova, G\"unter Rudolph, Thomas B\"ack</dc:creator>
    </item>
    <item>
      <title>Cooperation Is All You Need</title>
      <link>https://arxiv.org/abs/2305.10449</link>
      <description>arXiv:2305.10449v2 Announce Type: replace-cross 
Abstract: Going beyond 'dendritic democracy', we introduce a 'democracy of local processors', termed Cooperator. Here we compare their capabilities when used in permutation invariant neural networks for reinforcement learning (RL), with machine learning algorithms based on Transformers, such as ChatGPT. Transformers are based on the long standing conception of integrate-and-fire 'point' neurons, whereas Cooperator is inspired by recent neurobiological breakthroughs suggesting that the cellular foundations of mental life depend on context-sensitive pyramidal neurons in the neocortex which have two functionally distinct points. Weshow that when used for RL, an algorithm based on Cooperator learns far quicker than that based on Transformer, even while having the same number of parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.10449v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ahsan Adeel, Junaid Muzaffar, Khubaib Ahmed, Mohsin Raza, Fahad Zia, Eamin Chaudary, Talha Bin Riaz, Ahmed Saeed</dc:creator>
    </item>
    <item>
      <title>SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via Saliency-based Spiking</title>
      <link>https://arxiv.org/abs/2407.04752</link>
      <description>arXiv:2407.04752v3 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) with billions of parameters have improved performance in various applications, but their inference processes demand significant energy and computational resources. In contrast, the human brain, with approximately 86 billion neurons, is much more energy-efficient than LLMs with similar parameters. Inspired by this, we redesign 7$\sim$70 billion parameter LLMs using bio-plausible spiking mechanisms, emulating the efficient behavior of the human brain. We propose the first spiking large language model, SpikeLLM. Coupled with the proposed model, two essential approaches are proposed to improve spike training efficiency: Generalized Integrate-and-Fire (GIF) neurons to compress spike length from $T$ to $\frac{T}{L} \log_2 L$ bits, and an Optimal Brain Spiking framework to divide outlier channels and allocate different $T$ for GIF neurons, which further compresses spike length to approximate $log_2T$ bits. The necessity of spike-driven LLM is proved by comparison with quantized LLMs with similar operations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2 perplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B W4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear layers, significantly exceeding PB-LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04752v3</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.NE</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xingrun Xing, Boyan Gao, Zheng Zhang, David A. Clifton, Shitao Xiao, Li Du, Guoqi Li, Jiajun Zhang</dc:creator>
    </item>
    <item>
      <title>AiGAS-dEVL-RC: An Adaptive Growing Neural Gas Model for Recurrently Drifting Unsupervised Data Streams</title>
      <link>https://arxiv.org/abs/2504.05761</link>
      <description>arXiv:2504.05761v2 Announce Type: replace-cross 
Abstract: Concept drift and extreme verification latency pose significant challenges in data stream learning, particularly when dealing with recurring concept changes in dynamic environments. This work introduces a novel method based on the Growing Neural Gas (GNG) algorithm, designed to effectively handle abrupt recurrent drifts while adapting to incrementally evolving data distributions (incremental drifts). Leveraging the self-organizing and topological adaptability of GNG, the proposed approach maintains a compact yet informative memory structure, allowing it to efficiently store and retrieve knowledge of past or recurring concepts, even under conditions of delayed or sparse stream supervision. Our experiments highlight the superiority of our approach over existing data stream learning methods designed to cope with incremental non-stationarities and verification latency, demonstrating its ability to quickly adapt to new drifts, robustly manage recurring patterns, and maintain high predictive accuracy with a minimal memory footprint. Unlike other techniques that fail to leverage recurring knowledge, our proposed approach is proven to be a robust and efficient online learning solution for unsupervised drifting data flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05761v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Fri, 11 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Arostegi, Miren Nekane Bilbao, Jesus L. Lobo, Javier Del Ser</dc:creator>
    </item>
  </channel>
</rss>
