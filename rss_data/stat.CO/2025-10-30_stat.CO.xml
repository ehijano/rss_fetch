<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Oct 2025 01:42:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bayesian Neural Networks vs. Mixture Density Networks: Theoretical and Empirical Insights for Uncertainty-Aware Nonlinear Modeling</title>
      <link>https://arxiv.org/abs/2510.25001</link>
      <description>arXiv:2510.25001v1 Announce Type: new 
Abstract: This paper investigates two prominent probabilistic neural modeling paradigms: Bayesian Neural Networks (BNNs) and Mixture Density Networks (MDNs) for uncertainty-aware nonlinear regression. While BNNs incorporate epistemic uncertainty by placing prior distributions over network parameters, MDNs directly model the conditional output distribution, thereby capturing multimodal and heteroscedastic data-generating mechanisms. We present a unified theoretical and empirical framework comparing these approaches. On the theoretical side, we derive convergence rates and error bounds under H\"older smoothness conditions, showing that MDNs achieve faster Kullback-Leibler (KL) divergence convergence due to their likelihood-based nature, whereas BNNs exhibit additional approximation bias induced by variational inference. Empirically, we evaluate both architectures on synthetic nonlinear datasets and a radiographic benchmark (RSNA Pediatric Bone Age Challenge). Quantitative and qualitative results demonstrate that MDNs more effectively capture multimodal responses and adaptive uncertainty, whereas BNNs provide more interpretable epistemic uncertainty under limited data. Our findings clarify the complementary strengths of posterior-based and likelihood-based probabilistic learning, offering guidance for uncertainty-aware modeling in nonlinear systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25001v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riddhi Pratim Ghosh, Ian Barnett</dc:creator>
    </item>
    <item>
      <title>TabMGP: Martingale Posterior with TabPFN</title>
      <link>https://arxiv.org/abs/2510.25154</link>
      <description>arXiv:2510.25154v1 Announce Type: cross 
Abstract: Bayesian inference provides principled uncertainty quantification but is often limited by challenges of prior elicitation, likelihood misspecification, and computational burden. The martingale posterior (MGP, Fong et al., 2023) offers an alternative, replacing prior-likelihood elicitation with a predictive rule - namely, a sequence of one-step-ahead predictive distributions - for forward data generation. The utility of MGPs depends on the choice of predictive rule, yet the literature has offered few compelling examples. Foundation transformers are well-suited here, as their autoregressive generation mirrors this forward simulation and their general-purpose design enables rich predictive modeling. We introduce TabMGP, an MGP built on TabPFN, a transformer foundation model that is currently state-of-the-art for tabular data. TabMGP produces credible sets with near-nominal coverage and often outperforms both existing MGP constructions and standard Bayes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25154v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenyon Ng, Edwin Fong, David T. Frazier, Jeremias Knoblauch, Susan Wei</dc:creator>
    </item>
    <item>
      <title>Error Bounds and Optimal Schedules for Masked Diffusions with Factorized Approximations</title>
      <link>https://arxiv.org/abs/2510.25544</link>
      <description>arXiv:2510.25544v1 Announce Type: cross 
Abstract: Recently proposed generative models for discrete data, such as Masked Diffusion Models (MDMs), exploit conditional independence approximations to reduce the computational cost of popular Auto-Regressive Models (ARMs), at the price of some bias in the sampling distribution. We study the resulting computation-vs-accuracy trade-off, providing general error bounds (in relative entropy) that depend only on the average number of tokens generated per iteration and are independent of the data dimensionality (i.e. sequence length), thus supporting the empirical success of MDMs. We then investigate the gain obtained by using non-constant schedule sizes (i.e. varying the number of unmasked tokens during the generation process) and identify the optimal schedule as a function of a so-called information profile of the data distribution, thus allowing for a principled optimization of schedule sizes. We define methods directly as sampling algorithms and do not use classical derivations as time-reversed diffusion processes, leading us to simple and transparent proofs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25544v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.CO</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Lavenant, Giacomo Zanella</dc:creator>
    </item>
    <item>
      <title>Robust variable selection for spatial point processes observed with noise</title>
      <link>https://arxiv.org/abs/2510.25550</link>
      <description>arXiv:2510.25550v1 Announce Type: cross 
Abstract: We propose a method for variable selection in the intensity function of spatial point processes that combines sparsity-promoting estimation with noise-robust model selection. As high-resolution spatial data becomes increasingly available through remote sensing and automated image analysis, identifying spatial covariates that influence the localization of events is crucial to understand the underlying mechanism. However, results from automated acquisition techniques are often noisy, for example due to measurement uncertainties or detection errors, which leads to spurious displacements and missed events. We study the impact of such noise on sparse point-process estimation across different models, including Poisson and Thomas processes. To improve noise robustness, we propose to use stability selection based on point-process subsampling and to incorporate a non-convex best-subset penalty to enhance model-selection performance. In extensive simulations, we demonstrate that such an approach reliably recovers true covariates under diverse noise scenarios and improves both selection accuracy and stability. We then apply the proposed method to a forestry data set, analyzing the distribution of trees in relation to elevation and soil nutrients in a tropical rain forest. This shows the practical utility of the method, which provides a systematic framework for robust variable selection in spatial point-process models under noise, without requiring additional knowledge of the process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25550v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominik Sturm, Ivo F. Sbalzarini</dc:creator>
    </item>
    <item>
      <title>ggtime: A Grammar of Temporal Graphics</title>
      <link>https://arxiv.org/abs/2510.25656</link>
      <description>arXiv:2510.25656v1 Announce Type: cross 
Abstract: Visualizing changes over time is fundamental to learning from the past and anticipating the future. However, temporal semantics can be complicated, and existing visualization tools often struggle to accurately represent these complexities. It is common to use bespoke plot helper functions designed to produce specific graphics, due to the absence of flexible general tools that respect temporal semantics. We address this problem by proposing a grammar of temporal graphics, and an associated software implementation, 'ggtime', that encodes temporal semantics into a declarative grammar for visualizing temporal data. The grammar introduces new composable elements that support visualization across linear, cyclical, quasi-cyclical, and other granularities; standardization of irregular durations; and alignment of time points across different granularities and time zones. It is designed for interoperability with other semantic variables, allowing navigation across the space of visualizations while preserving temporal semantics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25656v1</guid>
      <category>cs.HC</category>
      <category>stat.CO</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cynthia A. Huang, Mitchell O'Hara-Wild, Rob J. Hyndman, Matthew Kay</dc:creator>
    </item>
    <item>
      <title>Existence and optimisation of the partial correlation graphical lasso</title>
      <link>https://arxiv.org/abs/2510.25712</link>
      <description>arXiv:2510.25712v1 Announce Type: cross 
Abstract: The partial correlation graphical LASSO (PCGLASSO) is a penalised likelihood method for Gaussian graphical models which provides scale invariant sparse estimation of the precision matrix and improves upon the popular graphical LASSO method. However, the PCGLASSO suffers from computational challenges due to the non-convexity of its associated optimisation problem. This paper provides some important breakthroughs in the computation of the PCGLASSO. First, the existence of the PCGLASSO estimate is proven when the sample size is smaller than the dimension - a case in which the maximum likelihood estimate does not exist. This means that the PCGLASSO can be used with any Gaussian data. Second, a new alternating algorithm for computing the PCGLASSO is proposed and implemented in the R package PCGLASSO available at https://github.com/JackStorrorCarter/PCGLASSO. This was the first publicly available implementation of the PCGLASSO and provides competitive computation time for moderate dimension size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25712v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack Storror Carter, Cesare Molinari</dc:creator>
    </item>
    <item>
      <title>Statistical Process Monitoring based on Functional Data Analysis</title>
      <link>https://arxiv.org/abs/2510.25742</link>
      <description>arXiv:2510.25742v1 Announce Type: cross 
Abstract: In modern industrial settings, advanced acquisition systems allow for the collection of data in the form of profiles, that is, as functional relationships linking responses to explanatory variables. In this context, statistical process monitoring (SPM) aims to assess the stability of profiles over time in order to detect unexpected behavior. This review focuses on SPM methods that model profiles as functional data, i.e., smooth functions defined over a continuous domain, and apply functional data analysis (FDA) tools to address limitations of traditional monitoring techniques. A reference framework for monitoring multivariate functional data is first presented. This review then offers a focused survey of several recent FDA-based profile monitoring methods that extend this framework to address common challenges encountered in real-world applications. These include approaches that integrate additional functional covariates to enhance detection power, a robust method designed to accommodate outlying observations, a real-time monitoring technique for partially observed profiles, and two adaptive strategies that target the characteristics of the out-of-control distribution. These methods are all implemented in the R package funcharts, available on CRAN. Finally, a review of additional existing FDA-based profile monitoring methods is also presented, along with suggestions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25742v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabio Centofanti</dc:creator>
    </item>
    <item>
      <title>Knots and variance ordering of sequential Monte Carlo algorithms</title>
      <link>https://arxiv.org/abs/2510.01901</link>
      <description>arXiv:2510.01901v2 Announce Type: replace 
Abstract: Sequential Monte Carlo algorithms, or particle filters, are widely used for approximating intractable integrals, particularly those arising in Bayesian inference and state-space models. We introduce a new variance reduction technique, the knot operator, which improves the efficiency of particle filters by incorporating potential function information into part, or all, of a transition kernel. The knot operator induces a partial ordering of Feynman-Kac models that implies an order on the asymptotic variance of particle filters, offering a new approach to algorithm design. We discuss connections to existing strategies for designing efficient particle filters, including model marginalisation. Our theory generalises such techniques and provides quantitative asymptotic variance ordering results. We revisit the fully-adapted (auxiliary) particle filter using our theory of knots to show how a small modification guarantees an asymptotic variance ordering for all relevant test functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01901v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua J Bon, Anthony Lee</dc:creator>
    </item>
    <item>
      <title>Hierarchical additive interaction modelling with Gaussian process prior and its efficient implementation for multidimensional grid data</title>
      <link>https://arxiv.org/abs/2305.07073</link>
      <description>arXiv:2305.07073v4 Announce Type: replace-cross 
Abstract: Additive Gaussian process (GP) models offer flexible tools for modelling complex non-linear relationships and interaction effects among covariates. While most studies have focused on predictive performance, relatively little attention has been given to identifying the underlying interaction structure, which may be of scientific interest in many applications. In practice, the use of additive GP models in this context has been limited by the cubic computational cost and quadratic storage requirements of GP inference. This paper presents a fast hierarchical additive interaction GP model for multi-dimensional grid data. A hierarchical ANOVA decomposition kernel forms the foundation of our model, which incorporate main and interaction effects under the principle of marginality. Kernel centring ensures identifiability and provides a unique, interpretable decomposition of lower- and higher-order effects. For datasets forming a multi-dimensional grid, efficient implementation is achieved by exploiting the Kronecker product structure of the covariance matrix. Our contribution is to extend Kronecker-based computation to handle any interaction structure within the proposed class of hierarchical additive GP models, whereas previous methods were limited to separable or fully saturated cases. The benefits of the proposed approach are demonstrated through simulation studies and an application to high-frequency nitrogen dioxide concentration data in London.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.07073v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sahoko Ishida, Francesca Panero, Wicher Bergsma</dc:creator>
    </item>
  </channel>
</rss>
