<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Mar 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>WIP: Assessing the Effectiveness of ChatGPT in Preparatory Testing Activities</title>
      <link>https://arxiv.org/abs/2503.03951</link>
      <description>arXiv:2503.03951v1 Announce Type: new 
Abstract: This innovative practice WIP paper describes a research study that explores the integration of ChatGPT into the software testing curriculum and evaluates its effectiveness compared to human-generated testing artifacts. In a Capstone Project course, students were tasked with generating preparatory testing artifacts using ChatGPT prompts, which they had previously created manually. Their understanding and the effectiveness of the Artificial Intelligence generated artifacts were assessed through targeted questions. The results, drawn from this in-class assignment at a North American community college indicate that while ChatGPT can automate many testing preparation tasks, it cannot fully replace human expertise. However, students, already familiar with Information Technology at the postgraduate level, found the integration of ChatGPT into their workflow to be straightforward. The study suggests that AI can be gradually introduced into software testing education to keep pace with technological advancements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03951v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/FIE61694.2024.10893214</arxiv:DOI>
      <dc:creator>Susmita Haldar, Mary Pierce, Luiz Fernando Capretz</dc:creator>
    </item>
    <item>
      <title>AI-based Programming Assistants for Privacy-related Code Generation: The Developers' Experience</title>
      <link>https://arxiv.org/abs/2503.03988</link>
      <description>arXiv:2503.03988v1 Announce Type: new 
Abstract: With the popularising of generative AI, the existence of AI-based programming assistants for developers is no surprise. Developers increasingly use them for their work, including generating code to fulfil the data protection requirements (privacy) of the apps they build. We wanted to know if the reality is the same as expectations of AI-based programming assistants when trying to fulfil software privacy requirements, and the challenges developers face when using AI-based programming assistants and how these can be improved. To this end, we conducted a survey with 51 developers worldwide. We found that AI-based programming assistants need to be improved in order for developers to better trust them with generating code that ensures privacy. In this paper, we provide some practical recommendations for developers to consider following when using AI-based programming assistants for privacy-related code development, and some key further research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03988v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kashumi Madampe, John Grundy, Nalin Arachchilage</dc:creator>
    </item>
    <item>
      <title>Deep Learning Aided Software Vulnerability Detection: A Survey</title>
      <link>https://arxiv.org/abs/2503.04002</link>
      <description>arXiv:2503.04002v1 Announce Type: new 
Abstract: The pervasive nature of software vulnerabilities has emerged as a primary factor for the surge in cyberattacks. Traditional vulnerability detection methods, including rule-based, signature-based, manual review, static, and dynamic analysis, often exhibit limitations when encountering increasingly complex systems and a fast-evolving attack landscape. Deep learning (DL) methods excel at automatically learning and identifying complex patterns in code, enabling more effective detection of emerging vulnerabilities. This survey analyzes 34 relevant studies from high-impact journals and conferences between 2017 and 2024. This survey introduces the conceptual framework Vulnerability Detection Lifecycle for the first time to systematically analyze and compare various DL-based vulnerability detection methods and unify them into the same analysis perspective. The framework includes six phases: (1) Dataset Construction, (2) Vulnerability Granularity Definition, (3) Code Representation, (4) Model Design, (5) Model Performance Evaluation, and (6) Real-world Project Implementation. For each phase of the framework, we identify and explore key issues through in-depth analysis of existing research while also highlighting challenges that remain inadequately addressed. This survey provides guidelines for future software vulnerability detection, facilitating further implementation of deep learning techniques applications in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04002v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Nizam Uddin, Yihe Zhang, Xiali Hei</dc:creator>
    </item>
    <item>
      <title>Understanding and Detecting Compatibility Issues in Android Auto Apps</title>
      <link>https://arxiv.org/abs/2503.04003</link>
      <description>arXiv:2503.04003v1 Announce Type: new 
Abstract: Mobile platforms now power not only smartphones but also in-vehicle systems like Android Auto and CarPlay. Despite an ecosystem of over 3.5 million Android apps and more than 200 million Android Auto-compatible vehicles, only a few hundred apps have been adapted for automotive use. To better understand this gap, we studied 147 reported issues related to Android Auto and identified their root causes. We found that more than 70% of issues result from UI incompatibilities, 24% from media playback errors, and around 5% from failures in voice command handling, showing a lack of effective tools for developers. We introduce CarCompat, a static analysis framework that detects compatibility problems in Android Auto apps. CarCompat constructs a Car-Control Flow Graph (CCFG) to capture interactions among app components, lifecycle methods, and platform-specific callbacks. It applies specialized checkers to detect UI violations, media playback errors, and issues with voice command handling. We evaluated CarCompat on a dataset of 54 Android Auto apps and detected 25 new issues, 4 of which were confirmed by developers, and 2 developers have already released their fixes. The results show that CarCompat helps developers identify and fix compatibility issues, improving the in-vehicle experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04003v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moshood Fakorede, Umar Farooq</dc:creator>
    </item>
    <item>
      <title>Revisiting Abstractions for Software Architecture and Tools to Support Them</title>
      <link>https://arxiv.org/abs/2503.04008</link>
      <description>arXiv:2503.04008v1 Announce Type: new 
Abstract: The mid-1990s saw the design of programming languages for software architectures, which define the high-level aspects of software systems including how code components were composed to form full systems. Our paper "Abstractions for Software Architecture and Tools to Support Them" presented a conceptual view of software architecture based on abstractions used in practice to organize software systems, a language that supported these abstractions, and a prototype implementation of this language. By invitation, we reflect on the paper's principal ideas about system-level abstractions, place the work in a historical context of steadily increasing abstraction power in software development languages and infrastructure, and reflect on how progress since the paper's 1995 publication has been influenced, directly or indirectly, by this work. We describe current manifestations of architectural ideas and current challenges. We suggest how the strategy we used to identify and reify architectural abstractions may apply to current opportunities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04008v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TSE.2025.3533549</arxiv:DOI>
      <dc:creator>Mary Shaw, Daniel V. Klein, Theodore L. Ross</dc:creator>
    </item>
    <item>
      <title>Design Obligations for Software, with Examples from Data Abstraction and Adaptive Systems</title>
      <link>https://arxiv.org/abs/2503.04022</link>
      <description>arXiv:2503.04022v1 Announce Type: new 
Abstract: Producing a good software design involves not only writing a definition that satisfies the syntax of the chosen language or structural constraints of a design paradigm. It also involves upholding a variety of expectations about the behavior of the system: the semantic expectations. These expectations may apply not only at the code level, but also to more abstract system structures such as software architectures. Such high-level design paradigms provide a vocabulary of components or other constructs and ways to compose those constructs, but not all expressible designs are well-formed, and even well-formed designs may fail to satisfy the expectations of the paradigm. Unfortunately, these expectations are often implicit or documented only informally, so they are challenging to discover, let alone uphold. They may for example, require correct use of complex structures, internal consistency, compliance with external standards, adherence with design principles, etc. Further, the reasons for design decisions that uphold these expectations are often not explicit in the code or other representation of the system. I introduce the idea of 'design obligations', which are constraints on allowable designs within a given design paradigm that help to assure appropriate use of the paradigm. To illustrate this idea, I discuss design obligations for two paradigms: data abstraction and a class of adaptive based on feedback control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04022v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mary Shaw</dc:creator>
    </item>
    <item>
      <title>Beyond Memorization: Evaluating the True Type Inference Capabilities of LLMs for Java Code Snippets</title>
      <link>https://arxiv.org/abs/2503.04076</link>
      <description>arXiv:2503.04076v1 Announce Type: new 
Abstract: Type inference is a crucial task for reusing online code snippets, often found on platforms like StackOverflow, which frequently lack essential type information such as fully qualified names (FQNs) and required libraries. Recent studies have leveraged Large Language Models (LLMs) for type inference on code snippets, showing promising results. However, these results are potentially affected by data leakage, as the benchmark suite (StatType-SO) has been public on GitHub since 2017 (full suite in 2023). Thus, it is uncertain whether LLMs' strong performance reflects genuine code semantics understanding or a mere retrieval of ground truth from training data.
  To comprehensively assess LLMs' type inference capabilities on Java code snippets, we conducted a three-pronged evaluation. First, utilizing Thalia, a program synthesis technique, we created ThaliaType--a new, unseen dataset for type inference evaluation. On unseen snippets, LLM performance dropped significantly, with up to a 59% decrease in precision and 72% in recall. Second, we developed semantic-preserving transformations that significantly degraded LLMs' type inference performance, revealing weaknesses in understanding code semantics. Third, we used delta debugging to identify the minimal syntax elements sufficient for LLM inference. While type inference primarily involves inferring FQNs for types in the code snippet, LLMs correctly infer FQNs even when the types were absent from the snippets, suggesting a reliance on knowledge from training instead of thoroughly analyzing the snippets.
  Our findings indicate that LLMs' strong past performance likely stemmed from data leakage, rather than a genuine understanding of the semantics of code snippets. Our findings highlight the crucial need for carefully designed benchmarks using unseen code snippets to assess the true capabilities of LLMs for type inference tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04076v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yiwen Dong, Zhenyang Xu, Yongqiang Tian, Chengnian Sun</dc:creator>
    </item>
    <item>
      <title>Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination</title>
      <link>https://arxiv.org/abs/2503.04149</link>
      <description>arXiv:2503.04149v1 Announce Type: new 
Abstract: The rapid evolution of code largelanguage models underscores the need for effective and transparent benchmarking of their reasoning capabilities. However, the current benchmarking approach heavily depends on publicly available, human-created datasets. The widespread use of these fixed benchmark datasets makes the benchmarking process to be static and thus particularly susceptible to data contamination, an unavoidable consequence of the extensive data collection processes used to train Code LLMs. Existing approaches that address data contamination often suffer from human effort limitations and imbalanced problem complexity. To tackle these challenges, we propose \tool, a novel benchmarking suite for evaluating Code LLMs under potential data contamination. Given a seed programming problem, \tool employs multiple agents to extract and modify the context without altering the core logic, generating semantically equivalent variations. We introduce a dynamic data generation methods and conduct empirical studies on two seed datasets across 21 Code LLMs. Results show that \tool effectively benchmarks reasoning capabilities under contamination risks while generating diverse problem sets to ensure consistent and reliable evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04149v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simin Chen, Pranav Pusarla, Baishakhi Ray</dc:creator>
    </item>
    <item>
      <title>Extracting Fix Ingredients using Language Models</title>
      <link>https://arxiv.org/abs/2503.04214</link>
      <description>arXiv:2503.04214v1 Announce Type: new 
Abstract: Deep learning and language models are increasingly dominating automated program repair research. While previous generate-and-validate approaches were able to find and use fix ingredients on a file or even project level, neural language models are limited to the code that fits their input window. In this work we investigate how important identifier ingredients are in neural program repair and present ScanFix, an approach that leverages an additional scanner model to extract identifiers from a bug's file and potentially project-level context. We find that lack of knowledge of far-away identifiers is an important cause of failed repairs. Augmenting repair model input with scanner-extracted identifiers yields relative improvements of up to 31%. However, ScanFix is outperformed by a model with a large input window (&gt; 5k tokens). When passing ingredients from the ground-truth fix, improvements are even higher. This shows that, with refined extraction techniques, ingredient scanning, similar to fix candidate ranking, could have the potential to become an important subtask of future automated repair systems. At the same time, it also demonstrates that this idea is subject to Sutton's bitter lesson and may be rendered unnecessary by new code models with ever-increasing context windows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04214v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julian Aron Prenner, Romain Robbes</dc:creator>
    </item>
    <item>
      <title>ThrowBench: Benchmarking LLMs by Predicting Runtime Exceptions</title>
      <link>https://arxiv.org/abs/2503.04241</link>
      <description>arXiv:2503.04241v1 Announce Type: new 
Abstract: Modern Large Language Models (LLMs) have shown astounding capabilities of code understanding and synthesis. In order to assess such capabilities, several benchmarks have been devised (e.g., HumanEval). However, most benchmarks focus on code synthesis from natural language instructions. Hence, such benchmarks do not test for other forms of code understanding. Moreover, there have been concerns about contamination and leakage. That is, benchmark problems (or closely related problems) may appear in training set, strongly biasing benchmark results. In this work we investigate whether large language models can correctly predict runtime program behavior. To this end, we introduce ThrowBench, a benchmark consisting of over 2,400 short user-written programs written in four different programming languages. The majority of these programs throw an exception during runtime (due to a bug). LLMs are asked to predict whether a presented program throws an exception and, if so, which one. Evaluating our benchmark on six state-of-the-art code LLMs we see modest performance ranging from 19 to 38% (F1 score). Benchmarking a wider set of code capabilities could improve the assessment of code LLMs and help identify weak points in current models. Moreover, as ground-truth answers have been determined through program execution, leakage is not a concern. We release ThrowBench as well as all of our results together with this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04241v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julian Aron Prenner, Romain Robbes</dc:creator>
    </item>
    <item>
      <title>From Waterfallish Aerospace Certification onto Agile Certifiable Iterations</title>
      <link>https://arxiv.org/abs/2503.04265</link>
      <description>arXiv:2503.04265v1 Announce Type: new 
Abstract: Agile software development is becoming increasingly popular in the aerospace industry because of its capability to accommodate requirement changes. However, safety-critical domains require compliance with strict regulations such as the DO-178C avionics standard, which demands thorough documentation. The main challenge of this constraint is not the content itself, but rather the comprehensive traceability from system-level requirements to all sorts of testing and verification evidence, including who did what, when, and to which artifact. Currently, this is mostly a manual activity performed at the end of the project, which blocks efforts to agilize the development of software for aerospace applications. In this paper, we present a strategy and tools that support the generation of continuous documentation complying with DO-178C requirements. By iteratively creating the DO-178C documentation associated with each software component and seamlessly merging it with the previously generated documentation, we open the way to truly continuous certifiable iterations, an evolution from the current Waterfallish industry practice. The proposed mechanisms and tools were co-designed and validated with aerospace industry professionals, thereby confirming its applicability and usefulness. The generated artifacts show that document automation is feasible in the aerospace industry, opening the way for more widespread adoption of Agile practices in this highly regulated sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04265v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Eduardo Ferreira Ribeiro, M\'ario Zenha-Rela, Jo\~ao Gabriel Silva</dc:creator>
    </item>
    <item>
      <title>Simple Fault Localization using Execution Traces</title>
      <link>https://arxiv.org/abs/2503.04301</link>
      <description>arXiv:2503.04301v1 Announce Type: new 
Abstract: Traditional spectrum-based fault localization (SBFL) exploits differences in a program's coverage spectrum when run on passing and failing test cases. However, such runs can provide a wealth of additional information beyond mere coverage. Working with thousands of execution traces of short programs submitted to competitive programming contests and leveraging machine learning and additional runtime, control-flow and lexical features, we present simple ways to improve SBFL. We also propose a simple trick to integrate context information. Our approach outperforms SBFL formulae such as Ochiai on our evaluation set as well as QuixBugs and requires neither a GPU nor any form of advanced program analysis. Existing SBFL solutions could possibly be improved with reasonable effort by adopting some of the proposed ideas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04301v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julian Aron Prenner, Romain Robbes</dc:creator>
    </item>
    <item>
      <title>LONGCODEU: Benchmarking Long-Context Language Models on Long Code Understanding</title>
      <link>https://arxiv.org/abs/2503.04359</link>
      <description>arXiv:2503.04359v1 Announce Type: new 
Abstract: Current advanced long-context language models offer great potential for real-world software engineering applications. However, progress in this critical domain remains hampered by a fundamental limitation: the absence of a rigorous evaluation framework for long code understanding. To gap this obstacle, we propose a long code understanding benchmark LONGCODEU from four aspects (8 tasks) to evaluate LCLMs' long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6 general models and 3 code models). Our experimental results reveal key limitations in current LCLMs' capabilities for long code understanding. Particularly, the performance of LCLMs drops dramatically when the long code length is greater than 32K, falling far short of their claimed 128K-1M context windows. In the four aspects, inter-code unit relation understanding is the most challenging for LCLMs. Our study provides valuable insights for optimizing LCLMs and driving advancements in software engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04359v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia Li, Xuyuan Guo, Lei Li, Kechi Zhang, Ge Li, Jia Li, Zhengwei Tao, Fang Liu, Chongyang Tao, Yuqi Zhu, Zhi Jin</dc:creator>
    </item>
    <item>
      <title>Exit the Code: A Model for Understanding Career Abandonment Intention Among Software Developers</title>
      <link>https://arxiv.org/abs/2503.04460</link>
      <description>arXiv:2503.04460v1 Announce Type: new 
Abstract: Background. Career abandonment, the process in which professionals leave the activity, assuming positions in another area, among software developers involves frustration with the lost investment and emotional and financial costs, even though being beneficial for the human being, depending on personal context. Previous studies have identified work-related motivators for career abandonment, such as the threat of obsolescence, unstable requirements, and low code quality, though these factors have primarily been examined in former developers. The relationship between these motivators and the intention to abandon among currently active developers remains unexplored. Goal. This article investigates the relationship between key work-related motivators and currently active software developers intention to abandon their careers. Method. We employed a quantitative approach, surveying 221 software developers to validate a theoretical model for career abandonment intention, based on an adaptation of the Investment Model, which incorporates satisfaction with technical aspects of the profession as well as the intention to abandon. Findings. Exploratory and confirmatory factor analyses, through structural equation modeling (SEM), provided robust support for the adapted Investment Model in explaining software developers intention to abandon their careers. Moreover, career commitment significantly impacts the intention to leave the profession, being positively influenced by satisfaction with technical work-related factors and negatively influenced by career alternatives and career investment. Conclusion. The paper offers valuable insights for organizational leaders and research, potentially guiding retention strategies to better support developers, and the adoption of theoretical models to explain career abandonment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04460v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiago Massoni, Ricardo Duarte, Ruan Oliveira</dc:creator>
    </item>
    <item>
      <title>Multi-modal Summarization in Model-Based Engineering: Automotive Software Development Case Study</title>
      <link>https://arxiv.org/abs/2503.04506</link>
      <description>arXiv:2503.04506v1 Announce Type: new 
Abstract: Multimodal summarization integrating information from diverse data modalities presents a promising solution to aid the understanding of information within various processes. However, the application and advantages of multimodal summarization have not received much attention in model-based engineering (MBE), where it has become a cornerstone in the design and development of complex systems, leveraging formal models to improve understanding, validation and automation throughout the engineering lifecycle. UML and EMF diagrams in model-based engineering contain a large amount of multimodal information and intricate relational data. Hence, our study explores the application of multimodal large language models within the domain of model-based engineering to evaluate their capacity for understanding and identifying relationships, features, and functionalities embedded in UML and EMF diagrams. We aim to demonstrate the transformative potential benefits and limitations of multimodal summarization in improving productivity and accuracy in MBE practices. The proposed approach is evaluated within the context of automotive software development, while many promising state-of-art models were taken into account.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04506v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nenad Petrovic, Yurui Zhang, Moaad Maaroufi, Kuo-Yi Chao, Lukasz Mazur, Fengjunjie Pan, Vahid Zolfaghari, Alois Knoll</dc:creator>
    </item>
    <item>
      <title>Belonging Beyond Code: Queer Software Engineering and Humanities Student Experiences</title>
      <link>https://arxiv.org/abs/2503.04576</link>
      <description>arXiv:2503.04576v1 Announce Type: new 
Abstract: Queer students often encounter discrimination and a lack of belonging in their academic environments. This may be especially true in heteronormative male-dominated fields like software engineering, which already faces a diversity crisis. In contrast, disciplines like humanities have a higher proportion of queer students, suggesting a more diverse academic culture. While prior research has explored queer students' challenges in STEM fields, limited attention has been given to how experiences differ between the sociotechnical, yet highly heteronormative, field of software engineering and the socioculturally inclusive humanities. This study addresses that gap by comparing 165 queer software engineering and 119 queer humanities students experiences. Our findings reveal that queer students in software engineering are less likely to be open about their sexuality, report a significantly lower sense of belonging, and encounter more academic challenges compared to their peers in the humanities. Despite these challenges, queer software engineering students show greater determination to continue their studies. These insights suggest that software engineering could enhance inclusivity by adopting practices commonly seen in the humanities, such as integrating inclusive policies in classrooms, to create a more welcoming environment where queer students can thrive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04576v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emily Vorderw\"ulbeke, Isabella Gra{\ss}l</dc:creator>
    </item>
    <item>
      <title>The Next Frontier of LLM Applications: Open Ecosystems and Hardware Synergy</title>
      <link>https://arxiv.org/abs/2503.04596</link>
      <description>arXiv:2503.04596v1 Announce Type: new 
Abstract: Large Language Model (LLM) applications, including LLM app stores and autonomous agents, are shaping the future of AI ecosystems. However, platform silos, fragmented hardware integration, and the absence of standardized interfaces limit scalability, interoperability, and resource efficiency. While LLM app stores democratize AI, their closed ecosystems restrict modular AI reuse and cross-platform portability. Meanwhile, agent-based frameworks offer flexibility but often lack seamless integration across diverse environments. This paper envisions the future of LLM applications and proposes a three-layer decoupled architecture grounded in software engineering principles such as layered system design, service-oriented architectures, and hardware-software co-design. This architecture separates application logic, communication protocols, and hardware execution, enhancing modularity, efficiency, and cross-platform compatibility. Beyond architecture, we highlight key security and privacy challenges for safe, scalable AI deployment and outline research directions in software and security engineering. This vision aims to foster open, secure, and interoperable LLM ecosystems, guiding future advancements in AI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04596v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Hou, Yanjie Zhao, Haoyu Wang</dc:creator>
    </item>
    <item>
      <title>Trim My View: An LLM-Based Code Query System for Module Retrieval in Robotic Firmware</title>
      <link>https://arxiv.org/abs/2503.03969</link>
      <description>arXiv:2503.03969v1 Announce Type: cross 
Abstract: The software compilation process has a tendency to obscure the original design of the system and makes it difficult both to identify individual components and discern their purpose simply by examining the resulting binary code. Although decompilation techniques attempt to recover higher-level source code from the machine code in question, they are not fully able to restore the semantics of the original functions. Furthermore, binaries are often stripped of metadata, and this makes it challenging to reverse engineer complex binary software.
  In this paper we show how a combination of binary decomposition techniques, decompilation passes, and LLM-powered function summarization can be used to build an economical engine to identify modules in stripped binaries and associate them with high-level natural language descriptions. We instantiated this technique with three underlying open-source LLMs -- CodeQwen, DeepSeek-Coder and CodeStral -- and measured its effectiveness in identifying modules in robotics firmware. This experimental evaluation involved 467 modules from four devices from the ArduPilot software suite, and showed that CodeStral, the best-performing backend LLM, achieves an average F1-score of 0.68 with an online running time of just a handful of seconds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03969v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sima Arasteh, Pegah Jandaghi, Nicolaas Weideman, Dennis Perepech, Mukund Raghothaman, Christophe Hauser, Luis Garcia</dc:creator>
    </item>
    <item>
      <title>How Do Hackathons Foster Creativity? Towards AI Collaborative Evaluation of Creativity at Scale</title>
      <link>https://arxiv.org/abs/2503.04290</link>
      <description>arXiv:2503.04290v1 Announce Type: cross 
Abstract: Hackathons have become popular collaborative events for accelerating the development of creative ideas and prototypes. There are several case studies showcasing creative outcomes across domains such as industry, education, and research. However, there are no large-scale studies on creativity in hackathons which can advance theory on how hackathon formats lead to creative outcomes. We conducted a computational analysis of 193,353 hackathon projects. By operationalizing creativity through usefulness and novelty, we refined our dataset to 10,363 projects, allowing us to analyze how participant characteristics, collaboration patterns, and hackathon setups influence the development of creative projects. The contribution of our paper is twofold: We identified means for organizers to foster creativity in hackathons. We also explore the use of large language models (LLMs) to augment the evaluation of creative outcomes and discuss challenges and opportunities of doing this, which has implications for creativity research at large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04290v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeanette Falk, Yiyi Chen, Janet Rafner, Mike Zhang, Johannes Bjerva, Alexander Nolte</dc:creator>
    </item>
    <item>
      <title>No Silver Bullet: Towards Demonstrating Secure Software Development for Danish Small and Medium Enterprises in a Business-to-Business Model</title>
      <link>https://arxiv.org/abs/2503.04293</link>
      <description>arXiv:2503.04293v1 Announce Type: cross 
Abstract: Software developing small and medium enterprises (SMEs) play a crucial role as suppliers to larger corporations and public administration. It is therefore necessary for them to be able to demonstrate that their products meet certain security criteria, both to gain trust of their customers and to comply to standards that demand such a demonstration. In this study we have investigated ways for SMEs to demonstrate their security when operating in a business-to-business model, conducting semi-structured interviews (N=16) with practitioners from different SMEs in Denmark and validating our findings in a follow-up workshop (N=6). Our findings indicate five distinctive security demonstration approaches, namely: Certifications, Reports, Questionnaires, Interactive Sessions and Social Proof. We discuss the challenges, benefits, and recommendations related to these approaches, concluding that none of them is a one-size-fits all solution and that more research into relative advantages of these approaches and their combinations is needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04293v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713931</arxiv:DOI>
      <dc:creator>Raha Asadi, Bodil Biering, Vincent van Dijk, Oksana Kulyk, Elda Paja</dc:creator>
    </item>
    <item>
      <title>ToolFuzz -- Automated Agent Tool Testing</title>
      <link>https://arxiv.org/abs/2503.04479</link>
      <description>arXiv:2503.04479v1 Announce Type: cross 
Abstract: Large Language Model (LLM) Agents leverage the advanced reasoning capabilities of LLMs in real-world applications. To interface with an environment, these agents often rely on tools, such as web search or database APIs. As the agent provides the LLM with tool documentation along the user query, the completeness and correctness of this documentation is critical. However, tool documentation is often over-, under-, or ill-specified, impeding the agent's accuracy. Standard software testing approaches struggle to identify these errors as they are expressed in natural language. Thus, despite its importance, there currently exists no automated method to test the tool documentation for agents. To address this issue, we present ToolFuzz, the first method for automated testing of tool documentations. ToolFuzz is designed to discover two types of errors: (1) user queries leading to tool runtime errors and (2) user queries that lead to incorrect agent responses. ToolFuzz can generate a large and diverse set of natural inputs, effectively finding tool description errors at a low false positive rate. Further, we present two straightforward prompt-engineering approaches. We evaluate all three tool testing approaches on 32 common LangChain tools and 35 newly created custom tools and 2 novel benchmarks to further strengthen the assessment. We find that many publicly available tools suffer from underspecification. Specifically, we show that ToolFuzz identifies 20x more erroneous inputs compared to the prompt-engineering approaches, making it a key component for building reliable AI agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04479v1</guid>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ivan Milev, Mislav Balunovi\'c, Maximilian Baader, Martin Vechev</dc:creator>
    </item>
    <item>
      <title>Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market</title>
      <link>https://arxiv.org/abs/2503.04521</link>
      <description>arXiv:2503.04521v1 Announce Type: cross 
Abstract: The convergence of edge computing and AI gives rise to Edge-AI, which enables the deployment of real-time AI applications and services at the network edge. One of the fundamental research issues in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy DNN inference services by leveraging the fine-grained offloading of partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would systematically explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We investigate the multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and analyse the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties, including competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness of our auction outcomes. The extensive simulation experiments based on four representative DNN inference workloads demonstrate that our AERIA mechanism significantly outperforms several state-of-the-art approaches in revenue maximization, demonstrating the efficacy of AERIA for on-demand DNN inference in the Edge-AI market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04521v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Songyuan Li, Jia Hu, Geyong Min, Haojun Huang, Jiwei Huang</dc:creator>
    </item>
    <item>
      <title>ComplexityMeasures.jl: scalable software to unify and accelerate entropy and complexity timeseries analysis</title>
      <link>https://arxiv.org/abs/2406.05011</link>
      <description>arXiv:2406.05011v3 Announce Type: replace 
Abstract: In the nonlinear timeseries analysis literature, countless quantities have been presented as new ``entropy'' or ``complexity'' measures, often with similar roles. The ever-increasing pool of such measures makes creating a sustainable and all-encompassing software for them difficult both conceptually and pragmatically. Such a software however would be an important tool that can aid researchers make an informed decision of which measure to use and for which application, as well as accelerate novel research. Here we present {ComplexityMeasures.jl}, an easily extendable and highly performant open-source software that implements a vast selection of complexity measures. The software provides 1638 measures with 3,841 lines of source code, averaging only 2.3 lines of code per exported quantity (version 3.7). This is made possible by its mathematically rigorous composable design. In this paper we discuss the software design and demonstrate how it can accelerate complexity-related research in the future. We carefully compare it with alternative software and conclude that {ComplexityMeasures.jl} outclasses the alternatives in several objective aspects of comparison, such as computational performance, overall amount of measures, reliability, and extendability. {ComplexityMeasures.jl} is also a component of the {DynamicalSystems.jl} library for nonlinear dynamics and nonlinear timeseries analysis and follows open source development practices for creating a sustainable community of developers and contributors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05011v3</guid>
      <category>cs.SE</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>nlin.CD</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Datseris, Kristian Agas{\o}ster Haaga</dc:creator>
    </item>
    <item>
      <title>An LLM-based Agent for Reliable Docker Environment Configuration</title>
      <link>https://arxiv.org/abs/2502.13681</link>
      <description>arXiv:2502.13681v2 Announce Type: replace 
Abstract: Environment configuration is a critical yet time-consuming step in software development, especially when dealing with unfamiliar code repositories. While Large Language Models (LLMs) demonstrate the potential to accomplish software engineering tasks, existing methods for environment configuration often rely on manual efforts or fragile scripts, leading to inefficiencies and unreliable outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully automate environment configuration and generate executable Dockerfiles for arbitrary Python repositories. We address two major challenges: (1) enabling the LLM agent to configure environments within isolated Docker containers, and (2) ensuring the successful configuration process is recorded and accurately transferred to a Dockerfile without error. To achieve this, we propose atomic configuration synthesis, featuring a dual-environment architecture (internal and external environment) with a rollback mechanism to prevent environment "pollution" from failed commands, guaranteeing atomic execution (execute fully or not at all) and a Dockerfile generator to transfer successful configuration steps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark of 420 recent Python repositories with unit tests, where it achieves an 86.0% success rate, outperforming the best baseline by 63.9%. Repo2Run is available at https://github.com/bytedance/Repo2Run.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13681v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ruida Hu, Chao Peng, Xinchen Wang, Cuiyun Gao</dc:creator>
    </item>
    <item>
      <title>Bidirectionalization For The Common People</title>
      <link>https://arxiv.org/abs/2502.18954</link>
      <description>arXiv:2502.18954v2 Announce Type: replace 
Abstract: This paper presents an innovative approach to applying bidirectional transformations (BX) in practice. To introduce BX to a wider audience of technologists, engineers, and researchers, we have chosen to use C# to develop Bifrons - a library of BX lenses that replaces domain-specific programming languages (DSL) in practical use. The proposed approach simplifies the implementation effort for two-way transformations by using simple symmetric lenses as the initial design pattern. It ensures correctness within reason by providing a simple lens-testing framework. We demonstrate the usability of BX lenses in a realistic scenario by using Bifrons to perform a case study experiment synchronizing data from two structurally and technologically heterogeneous databases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18954v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juraj Don\v{c}evi\'c, Mario Br\v{c}i\'c, Danijel Mlinari\'c</dc:creator>
    </item>
    <item>
      <title>Ten simple rules for training scientists to make better software</title>
      <link>https://arxiv.org/abs/2402.04722</link>
      <description>arXiv:2402.04722v2 Announce Type: replace-cross 
Abstract: Computational methods and associated software implementations are central to every field of scientific investigation. Modern biological research, particularly within systems biology, has relied heavily on the development of software tools to process and organize increasingly large datasets, simulate complex mechanistic models, provide tools for the analysis and management of data, and visualize and organize outputs. However, developing high-quality research software requires scientists to develop a host of software development skills, and teaching these skills to students is challenging. There has been a growing importance placed on ensuring reproducibility and good development practices in computational research. However, less attention has been devoted to informing the specific teaching strategies which are effective at nurturing in researchers the complex skillset required to produce high-quality software that, increasingly, is required to underpin both academic and industrial biomedical research. Recent articles in the Ten Simple Rules collection have discussed the teaching of foundational computer science and coding techniques to biology students. We advance this discussion by describing the specific steps for effectively teaching the necessary skills scientists need to develop sustainable software packages which are fit for (re-)use in academic research or more widely. Although our advice is likely to be applicable to all students and researchers hoping to improve their software development skills, our guidelines are directed towards an audience of students that have some programming literacy but little formal training in software development or engineering, typical of early doctoral students. These practices are also applicable outside of doctoral training environments, and we believe they should form a key part of postgraduate training schemes more generally in the life sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04722v2</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1371/journal.pcbi.1012410</arxiv:DOI>
      <dc:creator>Kit Gallagher, Richard Creswell, Ben Lambert, Martin Robinson, Chon Lok Lei, Gary R. Mirams, David J. Gavaghan</dc:creator>
    </item>
    <item>
      <title>Watson: A Cognitive Observability Framework for the Reasoning of LLM-Powered Agents</title>
      <link>https://arxiv.org/abs/2411.03455</link>
      <description>arXiv:2411.03455v2 Announce Type: replace-cross 
Abstract: As foundation models (FMs) play an increasingly prominent role in complex software systems, such as agentic software, they introduce significant observability and debuggability challenges. Although recent Large Reasoning Models (LRMs) generate their thought processes as part of the output, in many scenarios fast-thinking Large Language Models (LLMs) are still preferred due to latency constraints. LLM-powered agents operate autonomously with opaque implicit reasoning, making it difficult to debug their unexpected behaviors or errors. In this paper, we introduce Watson, a novel framework that provides reasoning observability into the implicit reasoning processes of agents driven by fast-thinking LLMs, allowing the identification and localization of errors and guidance for corrections. We demonstrate the accuracy of the recovered implicit reasoning trace by Watson and its usefulness through debugging and improving the performance of LLM-powered agents in two scenarios: Massive Multitask Language Understanding (MMLU) benchmark and SWE-bench-lite. Using Watson, we were able to observe and identify the implicit reasoning errors, and automatically provide targeted corrections at runtime that improve the Pass@1 of agents on MMLU and SWE-bench-lite by 7.58 (13.45% relative improvement) and 7.76 (12.31% relative improvement) percentage points, respectively, without updates to models or the cognitive architecture of the agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03455v2</guid>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benjamin Rombaut, Sogol Masoumzadeh, Kirill Vasilevski, Dayi Lin, Ahmed E. Hassan</dc:creator>
    </item>
    <item>
      <title>Optimizing Spot Instance Reliability and Security Using Cloud-Native Data and Tools</title>
      <link>https://arxiv.org/abs/2502.01966</link>
      <description>arXiv:2502.01966v2 Announce Type: replace-cross 
Abstract: This paper represents "Cloudlab", a comprehensive, cloud - native laboratory designed to support network security research and training. Built on Google Cloud and adhering to GitOps methodologies, Cloudlab facilitates the the creation, testing, and deployment of secure, containerized workloads using Kubernetes and serverless architectures. The lab integrates tools like Palo Alto Networks firewalls, Bridgecrew for "Security as Code," and automated GitHub workflows to establish a robust Continuous Integration/Continuous Machine Learning pipeline. By providing an adaptive and scalable environment, Cloudlab supports advanced security concepts such as role-based access control, Policy as Code, and container security. This initiative enables data scientists and engineers to explore cutting-edge practices in a dynamic cloud-native ecosystem, fostering innovation and improving operational resilience in modern IT infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01966v2</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Saqib, Shubham Malhotra, Dipkumar Mehta, Jagdish Jangid, Fnu Yashu, Sachin Dixit</dc:creator>
    </item>
    <item>
      <title>Pretrained Embeddings as a Behavior Specification Mechanism</title>
      <link>https://arxiv.org/abs/2503.02012</link>
      <description>arXiv:2503.02012v2 Announce Type: replace-cross 
Abstract: We propose an approach to formally specifying the behavioral properties of systems that rely on a perception model for interactions with the physical world. The key idea is to introduce embeddings -- mathematical representations of a real-world concept -- as a first-class construct in a specification language, where properties are expressed in terms of distances between a pair of ideal and observed embeddings. To realize this approach, we propose a new type of temporal logic called Embedding Temporal Logic (ETL), and describe how it can be used to express a wider range of properties about AI-enabled systems than previously possible. We demonstrate the applicability of ETL through a preliminary evaluation involving planning tasks in robots that are driven by foundation models; the results are promising, showing that embedding-based specifications can be used to steer a system towards desirable behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02012v2</guid>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Parv Kapoor, Abigail Hammer, Ashish Kapoor, Karen Leung, Eunsuk Kang</dc:creator>
    </item>
    <item>
      <title>When Radiation Meets Linux: Analyzing Soft Errors in Linux on COTS SoCs under Proton Irradiation</title>
      <link>https://arxiv.org/abs/2503.03722</link>
      <description>arXiv:2503.03722v2 Announce Type: replace-cross 
Abstract: The increasing use of Linux on commercial off-the-shelf (COTS) system-on-chip (SoC) in spaceborne computing inherits COTS susceptibility to radiation-induced failures like soft errors. Modern SoCs exacerbate this issue as aggressive transistor scaling reduces critical charge thresholds to induce soft errors and increases radiation effects within densely packed transistors, degrading overall reliability. Linux's monolithic architecture amplifies these risks, as tightly coupled kernel subsystems propagate errors to critical components (e.g., memory management), while limited error-correcting code (ECC) offers minimal mitigation. Furthermore, the lack of public soft error data from irradiation tests on COTS SoCs running Linux hinders reliability improvements. This study evaluates proton irradiation effects (20-50 MeV) on Linux across three COTS SoC architectures: Raspberry Pi Zero 2 W (40 nm CMOS, Cortex-A53), NXP i MX 8M Plus (14 nm FinFET, Cortex-A53), and OrangeCrab (40 nm FPGA, RISC-V). Irradiation results show the 14 nm FinFET NXP SoC achieved 2-3x longer Linux uptime without ECC memory versus both 40 nm CMOS counterparts, partially due to FinFET's reduced charge collection. Additionally, this work presents the first cross-architecture analysis of soft error-prone Linux kernel components in modern SoCs to develop targeted mitigations. The findings establish foundational data on Linux's soft error sensitivity in COTS SoCs, guiding mission readiness for space applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03722v2</guid>
      <category>cs.OS</category>
      <category>cs.AR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Saad Memon, Rafal Graczyk, Tomasz Rajkowski, Jan Swakon, Damian Wrobel, Sebastian Kusyk, Mike Papadakis</dc:creator>
    </item>
  </channel>
</rss>
