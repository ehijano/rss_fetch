<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Jun 2024 04:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian Statistics: A Review and a Reminder for the Practicing Reliability Engineer</title>
      <link>https://arxiv.org/abs/2406.02751</link>
      <description>arXiv:2406.02751v1 Announce Type: new 
Abstract: This paper introduces and reviews some of the principles and methods used in Bayesian reliability. It specifically discusses methods used in the analysis of success/no-success data and then reminds the reader of a simple (yet infrequently applied) Monte Carlo algorithm that can be used to calculate the posterior distribution of a system's reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02751v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carsten H. Botts</dc:creator>
    </item>
    <item>
      <title>PriME: Privacy-aware Membership profile Estimation in networks</title>
      <link>https://arxiv.org/abs/2406.02794</link>
      <description>arXiv:2406.02794v1 Announce Type: new 
Abstract: This paper presents a novel approach to estimating community membership probabilities for network vertices generated by the Degree Corrected Mixed Membership Stochastic Block Model while preserving individual edge privacy. Operating within the $\varepsilon$-edge local differential privacy framework, we introduce an optimal private algorithm based on a symmetric edge flip mechanism and spectral clustering for accurate estimation of vertex community memberships. We conduct a comprehensive analysis of the estimation risk and establish the optimality of our procedure by providing matching lower bounds to the minimax risk under privacy constraints. To validate our approach, we demonstrate its performance through numerical simulations and its practical application to real-world data. This work represents a significant step forward in balancing accurate community membership estimation with stringent privacy preservation in network data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02794v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinav Chakraborty, Sayak Chatterjee, Sagnik Nandy</dc:creator>
    </item>
    <item>
      <title>Asymptotic inference with flexible covariate adjustment under rerandomization and stratified rerandomization</title>
      <link>https://arxiv.org/abs/2406.02834</link>
      <description>arXiv:2406.02834v1 Announce Type: new 
Abstract: Rerandomization is an effective treatment allocation procedure to control for baseline covariate imbalance. For estimating the average treatment effect, rerandomization has been previously shown to improve the precision of the unadjusted and the linearly-adjusted estimators over simple randomization without compromising consistency. However, it remains unclear whether such results apply more generally to the class of M-estimators, including the g-computation formula with generalized linear regression and doubly-robust methods, and more broadly, to efficient estimators with data-adaptive machine learners. In this paper, using a super-population framework, we develop the asymptotic theory for a more general class of covariate-adjusted estimators under rerandomization and its stratified extension. We prove that the asymptotic linearity and the influence function remain identical for any M-estimator under simple randomization and rerandomization, but rerandomization may lead to a non-Gaussian asymptotic distribution. We further explain, drawing examples from several common M-estimators, that asymptotic normality can be achieved if rerandomization variables are appropriately adjusted for in the final estimator. These results are extended to stratified rerandomization. Finally, we study the asymptotic theory for efficient estimators based on data-adaptive machine learners, and prove their efficiency optimality under rerandomization and stratified rerandomization. Our results are demonstrated via simulations and re-analyses of a cluster-randomized experiment that used stratified rerandomization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02834v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bingkai Wang, Fan Li</dc:creator>
    </item>
    <item>
      <title>Statistical inference of convex order by Wasserstein projection</title>
      <link>https://arxiv.org/abs/2406.02840</link>
      <description>arXiv:2406.02840v1 Announce Type: new 
Abstract: Ranking distributions according to a stochastic order has wide applications in diverse areas. Although stochastic dominance has received much attention,convex order, particularly in general dimensions, has yet to be investigated from a statistical point of view. This article addresses this gap by introducing a simple statistical test for convex order based on the Wasserstein projection distance. This projection distance not only encodes whether two distributions are indeed in convex order, but also quantifies the deviation from the desired convex order and produces an optimal convex order approximation. Lipschitz stability of the backward and forward Wasserstein projection distance is proved, which leads to elegant consistency results of the estimator we employ as our test statistic. Combining these with state of the art results regarding the convergence rate of empirical distributions, we also derive upper bounds for the $p$-value and type I error our test statistic, as well as upper bounds on the type II error for an appropriate class of strict alternatives. Lastly, we provide an efficient numerical scheme for our test statistic, by way of an entropic Frank-Wolfe algorithm. Some experiments based on synthetic data sets illuminates the success of our approach empirically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02840v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakwang Kim, Young-Heon Kim, Yuanlong Ruan, Andrew Warren</dc:creator>
    </item>
    <item>
      <title>Copula-based semiparametric nonnormal transformed linear model for survival data with dependent censoring</title>
      <link>https://arxiv.org/abs/2406.02948</link>
      <description>arXiv:2406.02948v1 Announce Type: new 
Abstract: Although the independent censoring assumption is commonly used in survival analysis, it can be violated when the censoring time is related to the survival time, which often happens in many practical applications. To address this issue, we propose a flexible semiparametric method for dependent censored data. Our approach involves fitting the survival time and the censoring time with a joint transformed linear model, where the transformed function is unspecified. This allows for a very general class of models that can account for possible covariate effects, while also accommodating administrative censoring. We assume that the transformed variables have a bivariate nonnormal distribution based on parametric copulas and parametric marginals, which further enhances the flexibility of our method. We demonstrate the identifiability of the proposed model and establish the consistency and asymptotic normality of the model parameters under appropriate regularity conditions and assumptions. Furthermore, we evaluate the performance of our method through extensive simulation studies, and provide a real data example for illustration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02948v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huazhen Yu, Lixin Zhang</dc:creator>
    </item>
    <item>
      <title>Sparse two-stage Bayesian meta-analysis for individualized treatments</title>
      <link>https://arxiv.org/abs/2406.03056</link>
      <description>arXiv:2406.03056v1 Announce Type: new 
Abstract: Individualized treatment rules tailor treatments to patients based on clinical, demographic, and other characteristics. Estimation of individualized treatment rules requires the identification of individuals who benefit most from the particular treatments and thus the detection of variability in treatment effects. To develop an effective individualized treatment rule, data from multisite studies may be required due to the low power provided by smaller datasets for detecting the often small treatment-covariate interactions. However, sharing of individual-level data is sometimes constrained. Furthermore, sparsity may arise in two senses: different data sites may recruit from different populations, making it infeasible to estimate identical models or all parameters of interest at all sites, and the number of non-zero parameters in the model for the treatment rule may be small. To address these issues, we adopt a two-stage Bayesian meta-analysis approach to estimate individualized treatment rules which optimize expected patient outcomes using multisite data without disclosing individual-level data beyond the sites. Simulation results demonstrate that our approach can provide consistent estimates of the parameters which fully characterize the optimal individualized treatment rule. We estimate the optimal Warfarin dose strategy using data from the International Warfarin Pharmacogenetics Consortium, where data sparsity and small treatment-covariate interaction effects pose additional statistical challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03056v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Junwei Shen, Erica E. M. Moodie, Shirin Golchi</dc:creator>
    </item>
    <item>
      <title>Ordinal Mixed-Effects Random Forest</title>
      <link>https://arxiv.org/abs/2406.03130</link>
      <description>arXiv:2406.03130v1 Announce Type: new 
Abstract: We propose an innovative statistical method, called Ordinal Mixed-Effect Random Forest (OMERF), that extends the use of random forest to the analysis of hierarchical data and ordinal responses. The model preserves the flexibility and ability of modeling complex patterns of both categorical and continuous variables, typical of tree-based ensemble methods, and, at the same time, takes into account the structure of hierarchical data, modeling the dependence structure induced by the grouping and allowing statistical inference at all data levels. A simulation study is conducted to validate the performance of the proposed method and to compare it to the one of other state-of-the art models. The application of OMERF is exemplified in a case study focusing on predicting students performances using data from the Programme for International Student Assessment (PISA) 2022. The model identifies discriminating student characteristics and estimates the school-effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03130v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giulia Bergonzoli, Lidia Rossi, Chiara Masci</dc:creator>
    </item>
    <item>
      <title>Continuous-time modeling and bootstrap for chain ladder reserving</title>
      <link>https://arxiv.org/abs/2406.03252</link>
      <description>arXiv:2406.03252v1 Announce Type: new 
Abstract: We revisit the famous Mack's model which gives an estimate for the mean square error of prediction of the chain ladder claims reserves. We introduce a stochastic differential equation driven by a Brownian motion to model accumulated total claims amount for the chain ladder method. Within this continuous-time framework, we propose a bootstrap technique for estimating the distribution of claims reserves. It turns out that our approach leads to inherently capturing asymmetry and non-negativity, eliminating the necessity for additional assumptions. We conclude with a case study and comparative analysis against alternative methodologies based on Mack's model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03252v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Baradel</dc:creator>
    </item>
    <item>
      <title>Multi-relational Network Autoregression Model with Latent Group Structures</title>
      <link>https://arxiv.org/abs/2406.03296</link>
      <description>arXiv:2406.03296v1 Announce Type: new 
Abstract: Multi-relational networks among entities are frequently observed in the era of big data. Quantifying the effects of multiple networks have attracted significant research interest recently. In this work, we model multiple network effects through an autoregressive framework for tensor-valued time series. To characterize the potential heterogeneity of the networks and handle the high dimensionality of the time series data simultaneously, we assume a separate group structure for entities in each network and estimate all group memberships in a data-driven fashion. Specifically, we propose a group tensor network autoregression (GTNAR) model, which assumes that within each network, entities in the same group share the same set of model parameters, and the parameters differ across networks. An iterative algorithm is developed to estimate the model parameters and the latent group memberships simultaneously. Theoretically, we show that the group-wise parameters and group memberships can be consistently estimated when the group numbers are correctly- or possibly over-specified. An information criterion for group number estimation of each network is also provided to consistently select the group numbers. Lastly, we implement the method on a Yelp dataset to illustrate the usefulness of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03296v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yimeng Ren, Xuening Zhu, Ganggang Xu, Yanyuan Ma</dc:creator>
    </item>
    <item>
      <title>Combining an experimental study with external data: study designs and identification strategies</title>
      <link>https://arxiv.org/abs/2406.03302</link>
      <description>arXiv:2406.03302v1 Announce Type: new 
Abstract: There is increasing interest in combining information from experimental studies, including randomized and single-group trials, with information from external experimental or observational data sources. Such efforts are usually motivated by the desire to compare treatments evaluated in different studies -- for instance, through the introduction of external treatment groups -- or to estimate treatment effects with greater precision. Proposals to combine experimental studies with external data were made at least as early as the 1970s, but in recent years have come under increasing consideration by regulatory agencies involved in drug and device evaluation, particularly with the increasing availability of rich observational data. In this paper, we describe basic templates of study designs and data structures for combining information from experimental studies with external data, and use the potential (counterfactual) outcomes framework to elaborate identification strategies for potential outcome means and average treatment effects in these designs. In formalizing designs and identification strategies for combining information from experimental studies with external data, we hope to provide a conceptual foundation to support the systematic use and evaluation of such efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03302v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lawson Ung, Guanbo Wang, Sebastien Haneuse, Miguel A. Hernan, Issa J. Dahabreh</dc:creator>
    </item>
    <item>
      <title>Decision synthesis in monetary policy</title>
      <link>https://arxiv.org/abs/2406.03321</link>
      <description>arXiv:2406.03321v1 Announce Type: new 
Abstract: The macroeconomy is a sophisticated dynamic system involving significant uncertainties that complicate modelling. In response, decision makers consider multiple models that provide different predictions and policy recommendations which are then synthesized into a policy decision. In this setting, we introduce and develop Bayesian predictive decision synthesis (BPDS) to formalize monetary policy decision processes. BPDS draws on recent developments in model combination and statistical decision theory that yield new opportunities in combining multiple models, emphasizing the integration of decision goals, expectations and outcomes into the model synthesis process. Our case study concerns central bank policy decisions about target interest rates with a focus on implications for multi-step macroeconomic forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03321v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tony Chernis, Gary Koop, Emily Tallman, Mike West</dc:creator>
    </item>
    <item>
      <title>Griddy-Gibbs sampling for Bayesian P-splines models with Poisson data</title>
      <link>https://arxiv.org/abs/2406.03336</link>
      <description>arXiv:2406.03336v1 Announce Type: new 
Abstract: P-splines are appealing for smoothing Poisson distributed counts. They provide a flexible setting for modeling nonlinear model components based on a discretized penalty structure with a relatively simple computational backbone. Under a Bayesian inferential process relying on Markov chain Monte Carlo, estimates of spline coefficients are typically obtained by means of Metropolis-type algorithms, which may suffer from convergence issues if the proposal distribution is not properly chosen. To avoid such a sensitive calibration choice, we extend the Griddy-Gibbs sampler to Bayesian P-splines models with a Poisson response variable. In this model class, conditional posterior distributions of spline components are shown to have attractive mathematical properties. Despite their non-conjugate nature, conditional posteriors of spline coefficients can be efficiently explored with a Gibbs sampling scheme by relying on grid-based approximations. The proposed Griddy-Gibbs sampler for Bayesian P-splines (GGSBPS) algorithm is an interesting calibration-free tool for density estimation and histogram smoothing that is made available in a compact and user-friendly routine. The performance of our approach is assessed in different simulation settings and the GGSBPS algorithm is illustrated on two real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03336v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oswaldo Gressani, Paul H. C. Eilers</dc:creator>
    </item>
    <item>
      <title>Bayesian Quantile Estimation and Regression with Martingale Posteriors</title>
      <link>https://arxiv.org/abs/2406.03358</link>
      <description>arXiv:2406.03358v1 Announce Type: new 
Abstract: Quantile estimation and regression within the Bayesian framework is challenging as the choice of likelihood and prior is not obvious. In this paper, we introduce a novel Bayesian nonparametric method for quantile estimation and regression based on the recently introduced martingale posterior (MP) framework. The core idea of the MP is that posterior sampling is equivalent to predictive imputation, which allows us to break free of the stringent likelihood-prior specification. We demonstrate that a recursive estimate of a smooth quantile function, subject to a martingale condition, is entirely sufficient for full nonparametric Bayesian inference. We term the resulting posterior distribution as the quantile martingale posterior (QMP), which arises from an implicit generative predictive distribution. Associated with the QMP is an expedient, MCMC-free and parallelizable posterior computation scheme, which can be further accelerated with an asymptotic approximation based on a Gaussian process. Furthermore, the well-known issue of monotonicity in quantile estimation is naturally alleviated through increasing rearrangement due to the connections to the Bayesian bootstrap. Finally, the QMP has a particularly tractable form that allows for comprehensive theoretical study, which forms a main focus of the work. We demonstrate the ease of posterior computation in simulations and real data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03358v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edwin Fong, Andrew Yiu</dc:creator>
    </item>
    <item>
      <title>Discrete Autoregressive Switching Processes in Sparse Graphical Modeling of Multivariate Time Series Data</title>
      <link>https://arxiv.org/abs/2406.03385</link>
      <description>arXiv:2406.03385v1 Announce Type: new 
Abstract: We propose a flexible Bayesian approach for sparse Gaussian graphical modeling of multivariate time series. We account for temporal correlation in the data by assuming that observations are characterized by an underlying and unobserved hidden discrete autoregressive process. We assume multivariate Gaussian emission distributions and capture spatial dependencies by modeling the state-specific precision matrices via graphical horseshoe priors. We characterize the mixing probabilities of the hidden process via a cumulative shrinkage prior that accommodates zero-inflated parameters for non-active components, and further incorporate a sparsity-inducing Dirichlet prior to estimate the effective number of states from the data. For posterior inference, we develop a sampling procedure that allows estimation of the number of discrete autoregressive lags and the number of states, and that cleverly avoids having to deal with the changing dimensions of the parameter space. We thoroughly investigate performance of our proposed methodology through several simulation studies. We further illustrate the use of our approach for the estimation of dynamic brain connectivity based on fMRI data collected on a subject performing a task-based experiment on latent learning</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03385v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beniamino Hadj-Amar, Aaron M. Bornstein, Michele Guindani, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Non-stationary Spatio-Temporal Modeling Using the Stochastic Advection-Diffusion Equation</title>
      <link>https://arxiv.org/abs/2406.03400</link>
      <description>arXiv:2406.03400v1 Announce Type: new 
Abstract: We construct flexible spatio-temporal models through stochastic partial differential equations (SPDEs) where both diffusion and advection can be spatially varying. Computations are done through a Gaussian Markov random field approximation of the solution of the SPDE, which is constructed through a finite volume method. The new flexible non-separable model is compared to a flexible separable model both for reconstruction and forecasting and evaluated in terms of root mean square errors and continuous rank probability scores. A simulation study demonstrates that the non-separable model performs better when the data is simulated with non-separable effects such as diffusion and advection. Further, we estimate surrogate models for emulating the output of a ocean model in Trondheimsfjorden, Norway, and simulate observations of autonoumous underwater vehicles. The results show that the flexible non-separable model outperforms the flexible separable model for real-time prediction of unobserved locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03400v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Outzen Berild, Geir-Arne Fuglstad</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for scale mixtures of skew-normal linear models under the centered parameterization</title>
      <link>https://arxiv.org/abs/2406.03432</link>
      <description>arXiv:2406.03432v1 Announce Type: new 
Abstract: In many situations we are interested in modeling real data where the response distribution, even conditionally on the covariates, presents asymmetry and/or heavy/light tails. In these situations, it is more suitable to consider models based on the skewed and/or heavy/light tailed distributions, such as the class of scale mixtures of skew-normal distributions. The classical parameterization of this distributions may not be good due to the some inferential issues when the skewness parameter is in a neighborhood of 0, then, the centered parameterization becomes more appropriate. In this paper, we developed a class of scale mixtures of skew-normal distributions under the centered parameterization, also a linear regression model based on them was proposed. We explore a hierarchical representation and set up a MCMC scheme for parameter estimation. Furthermore, we developed residuals and influence analysis tools. A Monte Carlo experiment is conducted to evaluate the performance of the MCMC algorithm and the behavior of the residual distribution. The methodology is illustrated with the analysis of a real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03432v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jo\~ao Victor B. de Freitas, Caio L. N. Azevedo</dc:creator>
    </item>
    <item>
      <title>Gaussian Copula Models for Nonignorable Missing Data Using Auxiliary Marginal Quantiles</title>
      <link>https://arxiv.org/abs/2406.03463</link>
      <description>arXiv:2406.03463v1 Announce Type: new 
Abstract: We present an approach for modeling and imputation of nonignorable missing data under Gaussian copulas. The analyst posits a set of quantiles of the marginal distributions of the study variables, for example, reflecting information from external data sources or elicited expert opinion. When these quantiles are accurately specified, we prove it is possible to consistently estimate the copula correlation and perform multiple imputation in the presence of nonignorable missing data. We develop algorithms for estimation and imputation that are computationally efficient, which we evaluate in simulation studies of multiple imputation inferences. We apply the model to analyze associations between lead exposure levels and end-of-grade test scores for 170,000 students in North Carolina. These measurements are not missing at random, as children deemed at-risk for high lead exposure are more likely to be measured. We construct plausible marginal quantiles for lead exposure using national statistics provided by the Centers for Disease Control and Prevention. Complete cases and missing at random analyses appear to underestimate the relationships between certain variables and end-of-grade test scores, while multiple imputation inferences under our model support stronger adverse associations between lead exposure and educational outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03463v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Feldman, Jerome P. Reiter, Daniel R. Kowal</dc:creator>
    </item>
    <item>
      <title>Planetary Causal Inference: Implications for the Geography of Poverty</title>
      <link>https://arxiv.org/abs/2406.02584</link>
      <description>arXiv:2406.02584v1 Announce Type: cross 
Abstract: Earth observation data such as satellite imagery can, when combined with machine learning, have profound impacts on our understanding of the geography of poverty through the prediction of living conditions, especially where government-derived economic indicators are either unavailable or potentially untrustworthy. Recent work has progressed in using EO data not only to predict spatial economic outcomes, but also to explore cause and effect, an understanding which is critical for downstream policy analysis. In this review, we first document the growth of interest in EO-ML analyses in the causal space. We then trace the relationship between spatial statistics and EO-ML methods before discussing the four ways in which EO data has been used in causal ML pipelines -- (1.) poverty outcome imputation for downstream causal analysis, (2.) EO image deconfounding, (3.) EO-based treatment effect heterogeneity, and (4.) EO-based transportability analysis. We conclude by providing a workflow for how researchers can incorporate EO data in causal ML analysis going forward.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02584v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kazuki Sakamoto, Connor T. Jerzak, Adel Daoud</dc:creator>
    </item>
    <item>
      <title>Tackling GenAI Copyright Issues: Originality Estimation and Genericization</title>
      <link>https://arxiv.org/abs/2406.03341</link>
      <description>arXiv:2406.03341v1 Announce Type: cross 
Abstract: The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While some studies explore methods to mitigate copyright risks by steering the outputs of generative models away from those resembling copyrighted data, little attention has been paid to the question of how much of a resemblance is undesirable; more original or unique data are afforded stronger protection, and the threshold level of resemblance for constituting infringement correspondingly lower. Here, leveraging this principle, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03341v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroaki Chiba-Okabe, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Test and Measure for Partial Mean Dependence Based on Machine Learning Methods</title>
      <link>https://arxiv.org/abs/2212.12874</link>
      <description>arXiv:2212.12874v2 Announce Type: replace 
Abstract: It is of importance to investigate the significance of a subset of covariates $W$ for the response $Y$ given covariates $Z$ in regression modeling. To this end, we propose a significance test for the partial mean independence problem based on machine learning methods and data splitting. The test statistic converges to the standard chi-squared distribution under the null hypothesis while it converges to a normal distribution under the fixed alternative hypothesis. Power enhancement and algorithm stability are also discussed. If the null hypothesis is rejected, we propose a partial Generalized Measure of Correlation (pGMC) to measure the partial mean dependence of $Y$ given $W$ after controlling for the nonlinear effect of $Z$. We present the appealing theoretical properties of the pGMC and establish the asymptotic normality of its estimator with the optimal root-$N$ convergence rate. Furthermore, the valid confidence interval for the pGMC is also derived. As an important special case when there are no conditional covariates $Z$, we introduce a new test of overall significance of covariates for the response in a model-free setting. Numerical studies and real data analysis are also conducted to compare with existing approaches and to demonstrate the validity and flexibility of our proposed procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12874v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leheng Cai, Xu Guo, Wei Zhong</dc:creator>
    </item>
    <item>
      <title>Accounting for multiplicity in machine learning benchmark performance</title>
      <link>https://arxiv.org/abs/2303.07272</link>
      <description>arXiv:2303.07272v4 Announce Type: replace 
Abstract: Machine learning methods are commonly evaluated and compared by their performance on data sets from public repositories. This allows for multiple methods, oftentimes several thousands, to be evaluated under identical conditions and across time. The highest ranked performance on a problem is referred to as state-of-the-art (SOTA) performance, and is used, among other things, as a reference point for publication of new methods. Using the highest-ranked performance as an estimate for SOTA is a biased estimator, giving overly optimistic results. The mechanisms at play are those of multiplicity, a topic that is well-studied in the context of multiple comparisons and multiple testing, but has, as far as the authors are aware of, been nearly absent from the discussion regarding SOTA estimates. The optimistic state-of-the-art estimate is used as a standard for evaluating new methods, and methods with substantial inferior results are easily overlooked. In this article, we provide a probability distribution for the case of multiple classifiers so that known analyses methods can be engaged and a better SOTA estimate can be provided. We demonstrate the impact of multiplicity through a simulated example with independent classifiers. We show how classifier dependency impacts the variance, but also that the impact is limited when the accuracy is high. Finally, we discuss three real-world examples; Kaggle competitions that demonstrate various aspects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07272v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kajsa M{\o}llersen, Einar Holsb{\o}</dc:creator>
    </item>
    <item>
      <title>Calibrated and Conformal Propensity Scores for Causal Effect Estimation</title>
      <link>https://arxiv.org/abs/2306.00382</link>
      <description>arXiv:2306.00382v2 Announce Type: replace 
Abstract: Propensity scores are commonly used to estimate treatment effects from observational data. We argue that the probabilistic output of a learned propensity score model should be calibrated -- i.e., a predictive treatment probability of 90% should correspond to 90% of individuals being assigned the treatment group -- and we propose simple recalibration techniques to ensure this property. We prove that calibration is a necessary condition for unbiased treatment effect estimation when using popular inverse propensity weighted and doubly robust estimators. We derive error bounds on causal effect estimates that directly relate to the quality of uncertainties provided by the probabilistic propensity score model and show that calibration strictly improves this error bound while also avoiding extreme propensity weights. We demonstrate improved causal effect estimation with calibrated propensity scores in several tasks including high-dimensional image covariates and genome-wide association studies (GWASs). Calibrated propensity scores improve the speed of GWAS analysis by more than two-fold by enabling the use of simpler models that are faster to train.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00382v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shachi Deshpande, Volodymyr Kuleshov</dc:creator>
    </item>
    <item>
      <title>Exact Inference for Random Effects Meta-Analyses with Small, Sparse Data</title>
      <link>https://arxiv.org/abs/2306.11907</link>
      <description>arXiv:2306.11907v2 Announce Type: replace 
Abstract: Meta-analysis aggregates information across related studies to provide more reliable statistical inference and has been a vital tool for assessing the safety and efficacy of many high profile pharmaceutical products. A key challenge in conducting a meta-analysis is that the number of related studies is typically small. Applying classical methods that are asymptotic in the number of studies can compromise the validity of inference, particularly when heterogeneity across studies is present. Moreover, serious adverse events are often rare and can result in one or more studies with no events in at least one study arm. Practitioners often apply arbitrary continuity corrections or remove zero-event studies to stabilize or define effect estimates in such settings, which can further invalidate subsequent inference. To address these significant practical issues, we introduce an exact inference method for comparing event rates in two treatment arms under a random effects framework, which we coin "XRRmeta". In contrast to existing methods, the coverage of the confidence interval from XRRmeta is guaranteed to be at or above the nominal level (up to Monte Carlo error) when the event rates, number of studies, and/or the within-study sample sizes are small. Extensive numerical studies indicate that XRRmeta does not yield overly conservative inference and we apply our proposed method to two real-data examples using our open source R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11907v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jessica Gronsbell, Zachary R McCaw, Timothy Regis, Lu Tian</dc:creator>
    </item>
    <item>
      <title>ARK: Robust Knockoffs Inference with Coupling</title>
      <link>https://arxiv.org/abs/2307.04400</link>
      <description>arXiv:2307.04400v2 Announce Type: replace 
Abstract: We investigate the robustness of the model-X knockoffs framework with respect to the misspecified or estimated feature distribution. We achieve such a goal by theoretically studying the feature selection performance of a practically implemented knockoffs algorithm, which we name as the approximate knockoffs (ARK) procedure, under the measures of the false discovery rate (FDR) and $k$-familywise error rate ($k$-FWER). The approximate knockoffs procedure differs from the model-X knockoffs procedure only in that the former uses the misspecified or estimated feature distribution. A key technique in our theoretical analyses is to couple the approximate knockoffs procedure with the model-X knockoffs procedure so that random variables in these two procedures can be close in realizations. We prove that if such coupled model-X knockoffs procedure exists, the approximate knockoffs procedure can achieve the asymptotic FDR or $k$-FWER control at the target level. We showcase three specific constructions of such coupled model-X knockoff variables, verifying their existence and justifying the robustness of the model-X knockoffs framework. Additionally, we formally connect our concept of knockoff variable coupling to a type of Wasserstein distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04400v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Fan, Lan Gao, Jinchi Lv</dc:creator>
    </item>
    <item>
      <title>Spatial data fusion adjusting for preferential sampling using INLA and SPDE</title>
      <link>https://arxiv.org/abs/2309.03316</link>
      <description>arXiv:2309.03316v3 Announce Type: replace 
Abstract: Spatially misaligned data can be fused by using a Bayesian melding model that assumes that underlying all observations there is a spatially continuous Gaussian random field process. This model can be used, for example, to predict air pollution levels by combining point data from monitoring stations and areal data from satellite imagery.
  However, if the data presents preferential sampling, that is, if the observed point locations are not independent of the underlying spatial process, the inference obtained from models that ignore such a dependence structure might not be valid.
  In this paper, we present a Bayesian spatial model for the fusion of point and areal data that takes into account preferential sampling. The model combines the Bayesian melding specification and a model for the stochastically dependent sampling and underlying spatial processes.
  Fast Bayesian inference is performed using the integrated nested Laplace approximation (INLA) and the stochastic partial differential equation (SPDE) approaches. The performance of the model is assessed using simulated data in a range of scenarios and sampling strategies that can appear in real settings. The model is also applied to predict air pollution in the USA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03316v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiman Zhong, Andr\'e Victor Ribeiro Amaral, Paula Moraga</dc:creator>
    </item>
    <item>
      <title>Regression-Based Proximal Causal Inference</title>
      <link>https://arxiv.org/abs/2402.00335</link>
      <description>arXiv:2402.00335v3 Announce Type: replace 
Abstract: Negative controls are increasingly used to evaluate the presence of potential unmeasured confounding in observational studies. Beyond the use of negative controls to detect the presence of residual confounding, proximal causal inference (PCI) was recently proposed to de-bias confounded causal effect estimates, by leveraging a pair of treatment and outcome negative control or confounding proxy variables. While formal methods for statistical inference have been developed for PCI, these methods can be challenging to implement as they involve solving complex integral equations that are typically ill-posed. We develop a regression-based PCI approach, employing two-stage generalized linear regression models (GLMs) to implement PCI, which obviates the need to solve difficult integral equations. The proposed approach has merit in that (i) it is applicable to continuous, count, and binary outcomes cases, making it relevant to a wide range of real-world applications, and (ii) it is easy to implement using off-the-shelf software for GLMs. We establish the statistical properties of regression-based PCI and illustrate their performance in both synthetic and real-world empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00335v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiewen Liu, Chan Park, Kendrick Li, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>A Bayesian shrinkage estimator for transfer learning</title>
      <link>https://arxiv.org/abs/2403.17321</link>
      <description>arXiv:2403.17321v2 Announce Type: replace 
Abstract: Transfer learning (TL) has emerged as a powerful tool to supplement data collected for a target task with data collected for a related source task. The Bayesian framework is natural for TL because information from the source data can be incorporated in the prior distribution for the target data analysis. In this paper, we propose and study Bayesian TL methods for the normal-means problem and multiple linear regression. We propose two classes of prior distributions. The first class assumes the difference in the parameters for the source and target tasks is sparse, i.e., many parameters are shared across tasks. The second assumes that none of the parameters are shared across tasks, but the differences are bounded in $\ell_2$-norm. For the sparse case, we propose a Bayes shrinkage estimator with theoretical guarantees under mild assumptions. The proposed methodology is tested on synthetic data and outperforms state-of-the-art TL methods. We then use this method to fine-tune the last layer of a neural network model to predict the molecular gap property in a material science application. We report improved performance compared to classical fine tuning and methods using only the target data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17321v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed A. Abba, Jonathan P. Williams, Brian J. Reich</dc:creator>
    </item>
    <item>
      <title>The Impact of Stocks on Correlations between Crop Yields and Prices and on Revenue Insurance Premiums using Semiparametric Quantile Regression</title>
      <link>https://arxiv.org/abs/2308.11805</link>
      <description>arXiv:2308.11805v2 Announce Type: replace-cross 
Abstract: Crop yields and harvest prices are often considered to be negatively correlated, thus acting as a natural risk management hedge through stabilizing revenues. Storage theory gives reason to believe that the correlation is an increasing function of stocks carried over from previous years. Stock-conditioned second moments have implications for price movements during shortages and for hedging needs, while spatially varying yield-price correlation structures have implications for who benefits from commodity support policies. In this paper, we propose to use semi-parametric quantile regression (SQR) with penalized B-splines to estimate a stock-conditioned joint distribution of yield and price. The proposed method, validated through a comprehensive simulation study, enables sampling from the true joint distribution using SQR. Then it is applied to approximate stock-conditioned correlation and revenue insurance premium for both corn and soybeans in the United States. For both crops, Cornbelt core regions have more negative correlations than do peripheral regions. We find strong evidence that correlation becomes less negative as stocks increase. We also show that conditioning on stocks is important when calculating actuarially fair revenue insurance premiums. In particular, revenue insurance premiums in the Cornbelt core will be biased upward if the model for calculating premiums does not allow correlation to vary with stocks available. The stock-dependent correlation can be viewed as a form of tail dependence that, if unacknowledged, leads to mispricing of revenue insurance products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11805v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matthew Stuart, Cindy Yu, David A. Hennessy</dc:creator>
    </item>
    <item>
      <title>Inferring the Long-Term Causal Effects of Long-Term Treatments from Short-Term Experiments</title>
      <link>https://arxiv.org/abs/2311.08527</link>
      <description>arXiv:2311.08527v3 Announce Type: replace-cross 
Abstract: We study inference on the long-term causal effect of a continual exposure to a novel intervention, which we term a long-term treatment, based on an experiment involving only short-term observations. Key examples include the long-term health effects of regularly-taken medicine or of environmental hazards and the long-term effects on users of changes to an online platform. This stands in contrast to short-term treatments or "shocks," whose long-term effect can reasonably be mediated by short-term observations, enabling the use of surrogate methods. Long-term treatments by definition have direct effects on long-term outcomes via continual exposure, so surrogacy conditions cannot reasonably hold. We connect the problem with offline reinforcement learning, leveraging doubly-robust estimators to estimate long-term causal effects for long-term treatments and construct confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08527v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allen Tran, Aur\'elien Bibaut, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Is Knowledge All Large Language Models Needed for Causal Reasoning?</title>
      <link>https://arxiv.org/abs/2401.00139</link>
      <description>arXiv:2401.00139v2 Announce Type: replace-cross 
Abstract: This paper explores the causal reasoning of large language models (LLMs) to enhance their interpretability and reliability in advancing artificial intelligence. Despite the proficiency of LLMs in a range of tasks, their potential for understanding causality requires further exploration. We propose a novel causal attribution model that utilizes ``do-operators" for constructing counterfactual scenarios, allowing us to systematically quantify the influence of input numerical data and LLMs' pre-existing knowledge on their causal reasoning processes. Our newly developed experimental setup assesses LLMs' reliance on contextual information and inherent knowledge across various domains. Our evaluation reveals that LLMs' causal reasoning ability mainly depends on the context and domain-specific knowledge provided. In the absence of such knowledge, LLMs can still maintain a degree of causal reasoning using the available numerical data, albeit with limitations in the calculations. This motivates the proposed fine-tuned LLM for pairwise causal discovery, effectively leveraging both knowledge and numerical information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00139v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengrui Cai, Shengjie Liu, Rui Song</dc:creator>
    </item>
    <item>
      <title>Efficient Exploration for LLMs</title>
      <link>https://arxiv.org/abs/2402.00396</link>
      <description>arXiv:2402.00396v2 Announce Type: replace-cross 
Abstract: We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00396v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, Benjamin Van Roy</dc:creator>
    </item>
    <item>
      <title>Random matrix theory improved Fr\'echet mean of symmetric positive definite matrices</title>
      <link>https://arxiv.org/abs/2405.06558</link>
      <description>arXiv:2405.06558v2 Announce Type: replace-cross 
Abstract: In this study, we consider the realm of covariance matrices in machine learning, particularly focusing on computing Fr\'echet means on the manifold of symmetric positive definite matrices, commonly referred to as Karcher or geometric means. Such means are leveraged in numerous machine-learning tasks. Relying on advanced statistical tools, we introduce a random matrix theory-based method that estimates Fr\'echet means, which is particularly beneficial when dealing with low sample support and a high number of matrices to average. Our experimental evaluation, involving both synthetic and real-world EEG and hyperspectral datasets, shows that we largely outperform state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06558v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florent Bouchard, Ammar Mian, Malik Tiomoko, Guillaume Ginolhac, Fr\'ed\'eric Pascal</dc:creator>
    </item>
    <item>
      <title>Profiled Transfer Learning for High Dimensional Linear Model</title>
      <link>https://arxiv.org/abs/2406.00701</link>
      <description>arXiv:2406.00701v2 Announce Type: replace-cross 
Abstract: We develop here a novel transfer learning methodology called Profiled Transfer Learning (PTL). The method is based on the \textit{approximate-linear} assumption between the source and target parameters. Compared with the commonly assumed \textit{vanishing-difference} assumption and \textit{low-rank} assumption in the literature, the \textit{approximate-linear} assumption is more flexible and less stringent. Specifically, the PTL estimator is constructed by two major steps. Firstly, we regress the response on the transferred feature, leading to the profiled responses. Subsequently, we learn the regression relationship between profiled responses and the covariates on the target data. The final estimator is then assembled based on the \textit{approximate-linear} relationship. To theoretically support the PTL estimator, we derive the non-asymptotic upper bound and minimax lower bound. We find that the PTL estimator is minimax optimal under appropriate regularity conditions. Extensive simulation studies are presented to demonstrate the finite sample performance of the new method. A real data example about sentence prediction is also presented with very encouraging results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00701v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziqian Lin, Junlong Zhao, Fang Wang, Hansheng Wang</dc:creator>
    </item>
  </channel>
</rss>
