<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Mar 2024 04:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Inference of Monosynaptic Connections from Parallel Spike Trains: A Review</title>
      <link>https://arxiv.org/abs/2403.10993</link>
      <description>arXiv:2403.10993v1 Announce Type: new 
Abstract: This article presents a mini-review about the progress in inferring monosynaptic connections from spike trains of multiple neurons over the past twenty years. First, we explain a variety of meanings of ``neuronal connectivity'' in different research areas of neuroscience, such as structural connectivity, monosynaptic connectivity, and functional connectivity. Among these, we focus on the methods used to infer the monosynaptic connectivity from spike data. We then summarize the inference methods based on two main approaches, i.e., correlation-based and model-based approaches. Finally, we describe available source codes for connectivity inference and future challenges. Although inference will never be perfect, the accuracy of identifying the monosynaptic connections has improved dramatically in recent years due to continuous efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10993v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryota Kobayashi, Shigeru Shinomoto</dc:creator>
    </item>
    <item>
      <title>Towards understanding the nature of direct functional connectivity in visual brain network</title>
      <link>https://arxiv.org/abs/2403.11480</link>
      <description>arXiv:2403.11480v1 Announce Type: new 
Abstract: Recent advances in neuroimaging have enabled studies in functional connectivity (FC) of human brain, alongside investigation of the neuronal basis of cognition. One important FC study is the representation of vision in human brain. The release of publicly available dataset BOLD5000 has made it possible to study the brain dynamics during visual tasks in greater detail. In this paper, a comprehensive analysis of fMRI time series (TS) has been performed to explore different types of visual brain networks (VBN). The novelty of this work lies in (1) constructing VBN with consistently significant direct connectivity using both marginal and partial correlation, which is further analyzed using graph theoretic measures, (2) classification of VBNs as formed by image complexity-specific TS, using graphical features. In image complexity-specific VBN classification, XGBoost yields average accuracy in the range of 86.5% to 91.5% for positively correlated VBN, which is 2% greater than that using negative correlation. This result not only reflects the distinguishing graphical characteristics of each image complexity-specific VBN, but also highlights the importance of studying both positively correlated and negatively correlated VBN to understand the how differently brain functions while viewing different complexities of real-world images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11480v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Debanjali Bhattacharya, Neelam Sinha</dc:creator>
    </item>
    <item>
      <title>Perceptual learning in contour detection transfer across changes in contour path and orientation</title>
      <link>https://arxiv.org/abs/2403.11516</link>
      <description>arXiv:2403.11516v1 Announce Type: new 
Abstract: The integration of local elements into shape contours is critical for target detection and identification in cluttered scenes. Previous studies have shown that observers can learn to use image regularities for contour integration and target identification. However, we still know little about the generalization of perceptual learning in contour integration. Specifically, whether training in contour detection task could transfer to untrained contour type, path or orientation is still unclear. In a series of four experiments, human perceptual learning in contour detection was studied using psychophysical methods. We trained participants to detect contours in cluttered scenes over several days, which resulted in a significant improvement in sensitivity to trained contour type. This improved sensitivity was highly specific to contour type, but transfer across changes in contour path and contour orientation. These results suggest that short-term training improves the ability to integrate specific types of contours by optimizing the ability of the visual system to extract specific image regularities. The differential specificity and generalization across different stimulus features may support the involvement of both low-level and higher-level visual areas in perceptual learning in contour detection. These findings provide further insights into understanding the nature and the brain plasticity mechanism of contour integration learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11516v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Ding, Hongqiao Shi, Shuang Song, Yonghui Wang, Ya Li</dc:creator>
    </item>
    <item>
      <title>Inter-individual and inter-site neural code conversion and image reconstruction without shared stimuli</title>
      <link>https://arxiv.org/abs/2403.11517</link>
      <description>arXiv:2403.11517v1 Announce Type: new 
Abstract: The human brain demonstrates substantial inter-individual variability in fine-grained functional topography, posing challenges in identifying common neural representations across individuals. Functional alignment has the potential to harmonize these individual differences. However, it typically requires an identical set of stimuli presented to different individuals, which is often unavailable. To address this, we propose a content loss-based neural code converter, designed to convert brain activity from one subject to another representing the same content. The converter is optimized so that the source subject's converted brain activity is decoded into a latent image representation that closely resembles that of the stimulus given to the source subject. We show that converters optimized using hierarchical image representations achieve conversion accuracy comparable to those optimized by paired brain activity as in conventional methods. The brain activity converted from a different individual and even from a different site sharing no stimuli produced reconstructions that approached the quality of within-individual reconstructions. The converted brain activity had a generalizable representation that can be read out by different decoding schemes. The converter required much fewer training samples than that typically required for decoder training to produce recognizable reconstructions. These results demonstrate that our method can effectively combine image representations to convert brain activity across individuals without the need for shared stimuli, providing a promising tool for flexibly aligning data from complex cognitive tasks and a basis for brain-to-brain communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11517v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haibao Wang, Jun Kai Ho, Fan L. Cheng, Shuntaro C. Aoki, Yusuke Muraki, Misato Tanaka, Yukiyasu Kamitani</dc:creator>
    </item>
    <item>
      <title>A psychophysical evaluation of techniques for Mooney image generation</title>
      <link>https://arxiv.org/abs/2403.11867</link>
      <description>arXiv:2403.11867v1 Announce Type: new 
Abstract: Mooney images can contribute to our understanding of the processes involved in visual perception, because they allow a dissociation between image content and image understanding. Mooney images are generated by first smoothing and subsequently thresholding an image. In most previous studies this was performed manually, using subjective criteria for generation. This manual process could eventually be avoided by using automatic generation techniques. The field of computer image processing offers numerous techniques for image thresholding, but these are only rarely used to create Mooney images. Furthermore, there is little research on the perceptual effects of smoothing and thresholding. Therefore, in this study we investigated how the choice of different thresholding techniques and amount of smoothing affects the interpretability of Mooney images for human participants. We generated Mooney images using four different thresholding techniques and, in a second experiment, parametrically varied the level of smoothing. Participants identified the concepts shown in Mooney images and rated their interpretability. Although the techniques generate physically-different Mooney images, identification performance and subjective ratings were similar across the different techniques. This indicates that finding the perfect threshold in the process of generating Mooney images is not critical for Mooney image interpretability, at least for globally-applied thresholds. The degree of smoothing applied before thresholding, on the other hand, requires more tuning depending on the noise of the original image and the desired interpretability of the resulting Mooney image. Future work in automatic Mooney image generation should pursue local thresholding techniques, where different thresholds are applied to image regions depending on the local image content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11867v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lars C. Reining, Thomas S. A. Wallis</dc:creator>
    </item>
    <item>
      <title>PTSD-MDNN : Fusion tardive de r\'eseaux de neurones profonds multimodaux pour la d\'etection du trouble de stress post-traumatique</title>
      <link>https://arxiv.org/abs/2403.10565</link>
      <description>arXiv:2403.10565v1 Announce Type: cross 
Abstract: In order to provide a more objective and quicker way to diagnose post-traumatic stress disorder (PTSD), we present PTSD-MDNN which merges two unimodal convolutional neural networks and which gives low detection error rate. By taking only videos and audios as inputs, the model could be used in the configuration of teleconsultation sessions, in the optimization of patient journeys or for human-robot interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10565v1</guid>
      <category>eess.AS</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.SD</category>
      <category>eess.IV</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Long Nguyen-Phuoc, Renald Gaboriau, Dimitri Delacroix, Laurent Navarro</dc:creator>
    </item>
    <item>
      <title>Neural Erosion: Emulating Controlled Neurodegeneration and Aging in AI Systems</title>
      <link>https://arxiv.org/abs/2403.10596</link>
      <description>arXiv:2403.10596v1 Announce Type: cross 
Abstract: Creating controlled methods to simulate neurodegeneration in artificial intelligence (AI) is crucial for applications that emulate brain function decline and cognitive disorders. We use IQ tests performed by Large Language Models (LLMs) and, more specifically, the LLaMA 2 to introduce the concept of ``neural erosion." This deliberate erosion involves ablating synapses or neurons, or adding Gaussian noise during or after training, resulting in a controlled progressive decline in the LLMs' performance. We are able to describe the neurodegeneration in the IQ tests and show that the LLM first loses its mathematical abilities and then its linguistic abilities, while further losing its ability to understand the questions. To the best of our knowledge, this is the first work that models neurodegeneration with text data, compared to other works that operate in the computer vision domain. Finally, we draw similarities between our study and cognitive decline clinical studies involving test subjects. We find that with the application of neurodegenerative methods, LLMs lose abstract thinking abilities, followed by mathematical degradation, and ultimately, a loss in linguistic ability, responding to prompts incoherently. These findings are in accordance with human studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10596v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonios Alexos, Yu-Dai Tsai, Ian Domingo, Maryam Pishgar, Pierre Baldi</dc:creator>
    </item>
    <item>
      <title>MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data</title>
      <link>https://arxiv.org/abs/2403.11207</link>
      <description>arXiv:2403.11207v1 Announce Type: cross 
Abstract: Reconstructions of visual perception from brain activity have improved tremendously, but the practical utility of such methods has been limited. This is because such models are trained independently per subject where each subject requires dozens of hours of expensive fMRI training data to attain high-quality results. The present work showcases high-quality reconstructions using only 1 hour of fMRI training data. We pretrain our model across 7 subjects and then fine-tune on minimal data from a new subject. Our novel functional alignment procedure linearly maps all brain data to a shared-subject latent space, followed by a shared non-linear mapping to CLIP image space. We then map from CLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP latents as inputs instead of text. This approach improves out-of-subject generalization with limited training data and also attains state-of-the-art image retrieval and reconstruction metrics compared to single-subject approaches. MindEye2 demonstrates how accurate reconstructions of perception are possible from a single visit to the MRI facility. All code is available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11207v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul S. Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth A. Norman, Tanishq Mathew Abraham</dc:creator>
    </item>
    <item>
      <title>Comparison and Analysis of Cognitive Load under 2D/3D Visual Stimuli</title>
      <link>https://arxiv.org/abs/2302.12968</link>
      <description>arXiv:2302.12968v3 Announce Type: replace 
Abstract: With the increasing prevalence of 3D videos, understanding the distinctions between 2D and 3D video experiences has become significantly important. In our study, we explored the cognitive load imposed by 2D and 3D video stimuli under various cognitive tasks using electroencephalogram (EEG) data. To assess the cognitive load, we introduced the Cognitive Load Index (CLI), a measure that combines the oscillations of theta and alpha oscillations. Our experimental setup included four video stimuli, each associated with typical cognitive tasks. Participants were exposed to both 2D and 3D video stimuli, during which their EEG data were recorded. Then, we analyzed the power within the 0.5-45Hz frequency of EEG data, and CLI were used to evaluate brain activity. According to our experiments and analysis, videos requiring simple observational tasks (P &lt; 0.05) consistently induce a higher cognitive load in participants when watching 3D videos. However, for videos that involve calculation tasks (P &gt; 0.05), the difference in cognitive load induced by 2D and 3D stimuli is not as pronounced. Thus, we conclude that 3D videos generally induce a higher cognitive load, but the extent of this difference also depends on the content of the video stimuli.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12968v3</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yu Liu, Chen Song, Yunpeng Yin, Herui Shi, Jinglin Sun, Han Wang, Peiguang Jing</dc:creator>
    </item>
    <item>
      <title>Perceptual Scales Predicted by Fisher Information Metrics</title>
      <link>https://arxiv.org/abs/2310.11759</link>
      <description>arXiv:2310.11759v2 Announce Type: replace 
Abstract: Perception is often viewed as a process that transforms physical variables, external to an observer, into internal psychological variables. Such a process can be modeled by a function coined perceptual scale. The perceptual scale can be deduced from psychophysical measurements that consist in comparing the relative differences between stimuli (i.e. difference scaling experiments). However, this approach is often overlooked by the modeling and experimentation communities. Here, we demonstrate the value of measuring the perceptual scale of classical (spatial frequency, orientation) and less classical physical variables (interpolation between textures) by embedding it in recent probabilistic modeling of perception. First, we show that the assumption that an observer has an internal representation of univariate parameters such as spatial frequency or orientation while stimuli are high-dimensional does not lead to contradictory predictions when following the theoretical framework. Second, we show that the measured perceptual scale corresponds to the transduction function hypothesized in this framework. In particular, we demonstrate that it is related to the Fisher information of the generative model that underlies perception and we test the predictions given by the generative model of different stimuli in a set a of difference scaling experiments. Our main conclusion is that the perceptual scale is mostly driven by the stimulus power spectrum. Finally, we propose that this measure of perceptual scale is a way to push further the notion of perceptual distances by estimating the perceptual geometry of images i.e. the path between images instead of simply the distance between those.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11759v2</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>The Twelfth International Conference on Learning Representations. 2024</arxiv:journal_reference>
      <dc:creator>Jonathan Vacher, Pascal Mamassian</dc:creator>
    </item>
    <item>
      <title>Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data</title>
      <link>https://arxiv.org/abs/2311.00136</link>
      <description>arXiv:2311.00136v4 Announce Type: replace 
Abstract: State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mouse with only few-shot fine-tuning, suggesting that the model begins learning how to do so directly from the neural representations themselves, without any explicit supervision. We used an ablation study to show that joint training on neuronal responses and behavior boosted performance, highlighting the model's ability to associate behavioral and neural representations in an unsupervised manner. These findings show that Neuroformer can analyze neural datasets and their emergent properties, informing the development of models and hypotheses associated with the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00136v4</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Antonis Antoniades, Yiyi Yu, Joseph Canzano, William Wang, Spencer LaVere Smith</dc:creator>
    </item>
    <item>
      <title>Identification of Craving Maps among Marijuana Users via the Analysis of Functional Brain Networks with High-Order Attention Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2403.00033</link>
      <description>arXiv:2403.00033v3 Announce Type: replace 
Abstract: The consumption of high doses of marijuana can have significant psychological and social impacts. In this study, we propose an interpretable framework called the HOGANN (High-Order Graph Attention Neural Networks) for Marijuana addiction classification and followed by the analysis of the localized brain network communities that demonstrated abnormal brain activities among chronic marijuana users. The HOGANN integrates dynamic intrinsic functional brain networks estimated from the resting-state functional magnetic resonance imaging (rs-fMRI) using the Long Short-Term Memory (LSTM) to capture temporal network dynamics. We employed an high-order attention module for information fusion and message passing among the neighboring nodes, enhancing the network community level analysis. We validated our model on two data cohorts and the overall classification for both dataset have achieved a much higher accuracy than the comparison algorithms. In addition, we identified the most relevant subnetworks and cognitive regions which are impacted by persistent marijuana consumption, suggesting that chronic marijuana consumption can adversely affect functional brain networks, particularly within the Dorsal Attention and Frontoparietal networks. Most interestingly, we found our model performs better on the cohorts of subjects with long time dependence, which suggests longer time consumption of marijuana brings more significant changes of brain networks. The model can identify craving brain maps, and thus pinpointing brain regions that are important for analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00033v3</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun-En Ding, Shihao Yang, Anna Zilverstand, Feng Liu</dc:creator>
    </item>
    <item>
      <title>Are you sure? Modelling Drivers' Confidence Judgments in Left-Turn Gap Acceptance Decisions</title>
      <link>https://arxiv.org/abs/2403.06496</link>
      <description>arXiv:2403.06496v2 Announce Type: replace 
Abstract: When a person makes a decision, it is automatically accompanied by a subjective probability judgment of the decision being correct, in other words, a confidence judgment. A better understanding of the mechanisms responsible for these confidence judgments could provide novel insights into human behavior. However, so far confidence judgments have been mostly studied in simplistic laboratory tasks while little is known about confidence in naturalistic dynamic tasks such as driving. In this study, we made a first attempt of connecting fundamental research on confidence with naturalistic driver behavior. We investigated the confidence of drivers in left-turn gap acceptance decisions in a driver simulator experiment (N=17). We found that confidence in these decisions depends on the size of the gap to the oncoming vehicle. Specifically, confidence increased with the gap size for trials in which the gap was accepted, and decreased with the gap size for rejected gaps. Similarly to more basic tasks, confidence was negatively related to the response times and correlated with action dynamics during decision execution. Finally, we found that confidence judgments can be captured with an extended dynamic drift-diffusion model. In the model, the drift rate of the evidence accumulator as well as the decision boundaries are functions of the gap size. Furthermore, we demonstrated that allowing for post-decision evidence accumulation in the model increases its ability to describe confidence judgments in rejected gap decisions. Overall, our study confirmed that principles known from fundamental confidence research extend to confidence judgments in dynamic decisions during a naturalistic task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06496v2</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Floor Bontje, Arkady Zgonnikov</dc:creator>
    </item>
    <item>
      <title>Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection</title>
      <link>https://arxiv.org/abs/2210.06891</link>
      <description>arXiv:2210.06891v4 Announce Type: replace-cross 
Abstract: This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a user-specified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experimental Design in imaging, to identify the most informative channel-subset whilst simultaneously training a network to execute the task given the subset. Experiments demonstrate the potential of TADRED in diverse imaging applications: several clinically-relevant tasks in magnetic resonance imaging; and remote sensing and physiological applications of hyperspectral imaging. Results show substantial improvement over classical experimental design, two recent application-specific methods within the new paradigm, and state-of-the-art approaches in supervised feature selection. We anticipate further applications of our approach. Code is available: https://github.com/sbb-gh/experimental-design-multichannel</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.06891v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stefano B. Blumberg, Paddy J. Slator, Daniel C. Alexander</dc:creator>
    </item>
    <item>
      <title>Interpretable statistical representations of neural population dynamics and geometry</title>
      <link>https://arxiv.org/abs/2304.03376</link>
      <description>arXiv:2304.03376v3 Announce Type: replace-cross 
Abstract: The dynamics of neuron populations during many behavioural tasks evolve on low-dimensional manifolds. However, it remains challenging to discover latent representations from neural recordings that are interpretable and consistently decodable across individuals and conditions without explicitly relying on behavioural information. Here, we introduce MARBLE, a fully unsupervised geometric deep learning framework for the data-driven representation of non-linear dynamics based on statistical distributions of local dynamical features. Using both in silico examples from non-linear dynamical systems and recurrent neural networks and in vivo recordings from primates and rodents, we demonstrate that MARBLE can infer latent representations that are highly interpretable in terms of global system variables such as decision-thresholds, kinematics or internal states. We also show that MARBLE representations are consistent across neural networks and animals so that they can be used to compare cognitive computations or train universal decoders. Through extensive benchmarking, we show that unsupervised MARBLE provides best-in-class within- and across-animal decoding accuracy, comparable to or significantly better than current supervised approaches, yet without the need for behavioural labels. Our results suggest that using the manifold structure in conjunction with the temporal information of neural dynamics provides a common framework to develop better decoding algorithms and assimilate data across experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03376v3</guid>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Gosztolai, Robert L. Peach, Alexis Arnaudon, Mauricio Barahona, Pierre Vandergheynst</dc:creator>
    </item>
    <item>
      <title>The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks</title>
      <link>https://arxiv.org/abs/2306.16922</link>
      <description>arXiv:2306.16922v3 Announce Type: replace-cross 
Abstract: Biological cortical neurons are remarkably sophisticated computational devices, temporally integrating their vast synaptic input over an intricate dendritic tree, subject to complex, nonlinearly interacting internal biological processes. A recent study proposed to characterize this complexity by fitting accurate surrogate models to replicate the input-output relationship of a detailed biophysical cortical pyramidal neuron model and discovered it needed temporal convolutional networks (TCN) with millions of parameters. Requiring these many parameters, however, could stem from a misalignment between the inductive biases of the TCN and cortical neuron's computations. In light of this, and to explore the computational implications of leaky memory units and nonlinear dendritic processing, we introduce the Expressive Leaky Memory (ELM) neuron model, a biologically inspired phenomenological model of a cortical neuron. Remarkably, by exploiting such slowly decaying memory-like hidden states and two-layered nonlinear integration of synaptic input, our ELM neuron can accurately match the aforementioned input-output relationship with under ten thousand trainable parameters. To further assess the computational ramifications of our neuron design, we evaluate it on various tasks with demanding temporal structures, including the Long Range Arena (LRA) datasets, as well as a novel neuromorphic dataset based on the Spiking Heidelberg Digits dataset (SHD-Adding). Leveraging a larger number of memory units with sufficiently long timescales, and correspondingly sophisticated synaptic integration, the ELM neuron displays substantial long-range processing capabilities, reliably outperforming the classic Transformer or Chrono-LSTM architectures on LRA, and even solving the Pathfinder-X task with over 70% accuracy (16k context length).</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16922v3</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Spieler, Nasim Rahaman, Georg Martius, Bernhard Sch\"olkopf, Anna Levina</dc:creator>
    </item>
  </channel>
</rss>
