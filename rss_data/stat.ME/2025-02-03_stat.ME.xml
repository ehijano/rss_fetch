<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Feb 2025 04:08:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A simple multistage theory for multimorbidity</title>
      <link>https://arxiv.org/abs/2501.18742</link>
      <description>arXiv:2501.18742v1 Announce Type: new 
Abstract: It is shown how observational data of disease incidence can be used to determine whether two diseases can have a ``shared stage'' or ``step'', before either disease can occur, and how the unobserved rate of this step can be determined. The approach is in principle quite general, and illustrated with specific examples using simplified multistage models of disease. It offers a simple approach for studying multiple diseases in an individual, and for identifying the underlying causes of multiple conditions. This includes studies for two or more cancer types. The fundamental mathematical model is analysed to compare key statistical properties such as the expectation and variance with those of independent diseases, and its properties are illustrated with specific examples. Despite the complex nature of models that study multiple conditions, the most important general results do not need an understanding of the underlying mathematics and can be appreciated by a non-expert.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18742v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anthony J. Webster</dc:creator>
    </item>
    <item>
      <title>Low-Rank Approaches to Graphon Learning in Networks</title>
      <link>https://arxiv.org/abs/2501.18785</link>
      <description>arXiv:2501.18785v1 Announce Type: new 
Abstract: The graphon is a powerful framework for modeling large-scale networks, but its estimation remains a significant challenge. In this paper, we propose a novel approach that directly leverages a low-rank representation of the graphon for parsimonious modeling. This representation naturally yields both a low-rank connection probability matrix and a low-rank graphon -- two tasks that are often infeasible in existing literature -- while also addressing the well-known identification issues in graphon estimation. By exploiting the additive structure of this representation, we develop an efficient sequential learning algorithm that estimates the low-rank connection matrix using subgraph counts and reconstructs the graphon function through interpolation. We establish the consistency of the proposed method and demonstrate its computational efficiency and estimation accuracy through extensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18785v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Fan, Feiyan Ma, Chenlei Leng, Weichi Wu</dc:creator>
    </item>
    <item>
      <title>Targeted Data Fusion for Causal Survival Analysis Under Distribution Shift</title>
      <link>https://arxiv.org/abs/2501.18798</link>
      <description>arXiv:2501.18798v1 Announce Type: new 
Abstract: Causal inference across multiple data sources has the potential to improve the generalizability, transportability, and replicability of scientific findings. However, data integration methods for time-to-event outcomes -- common in medical contexts such as clinical trials -- remain underdeveloped. Existing data fusion methods focus on binary or continuous outcomes, neglecting the distinct challenges of survival analysis, including right-censoring and the unification of discrete and continuous time frameworks. To address these gaps, we propose two novel approaches for multi-source causal survival analysis. First, considering a target site-specific causal effect, we introduce a semiparametric efficient estimator for scenarios where data-sharing is feasible. Second, we develop a federated learning framework tailored to privacy-constrained environments. This framework dynamically adjusts source site-specific contributions, downweighting biased sources and upweighting less biased ones relative to the target population. Both approaches incorporate nonparametric machine learning models to enhance robustness and efficiency, with theoretical guarantees applicable to both continuous and discrete time-to-event outcomes. We demonstrate the practical utility of our methods through extensive simulations and an application to two randomized trials of a monoclonal neutralizing antibody for HIV-1 prevention: HVTN 704/HPTN 085 (cisgender men and transgender persons in the Americas and Switzerland) and HVTN 703/HPTN 081 (women in sub-Saharan Africa). The results highlight the potential of our approaches to efficiently estimate causal effects while addressing heterogeneity across data sources and adhering to privacy and robustness constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18798v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Liu, Alexander W. Levis, Ke Zhu, Shu Yang, Peter B. Gilbert, Larry Han</dc:creator>
    </item>
    <item>
      <title>Recent advances in doubly-robust weighted ordinary least squares techniques for dynamic treatment regime estimation</title>
      <link>https://arxiv.org/abs/2501.18819</link>
      <description>arXiv:2501.18819v1 Announce Type: new 
Abstract: A dynamic treatment regime (DTR) is an approach to delivering precision medicine that uses patient characteristics to guide treatment decisions for optimal health outcomes. Numerous methods have been proposed for DTR estimation, including dynamic weighted ordinary least squares (dWOLS), a regression-based approach that affords double robustness to model misspecification within an easy to implement analytical framework. Initially, the dWOLS approach was developed under the assumptions of continuous outcomes and binary treatment decisions. Motivated by clinical research, subsequent theoretical advancements have extended the dWOLS framework to address binary, continuous and multicategory treatments across various outcome types, including binary, continuous, and survival-type. However, certain scenarios remain unexplored. This paper summarizes the last ten years of extension and application of the dWOLS method, providing a comprehensive and detailed review of the original dWOLS method and its extensions, as well as highlighting its diverse practical applications. We also explore studies that have addressed challenges associated with dWOLS implementation, such as model validation, variable selection, and handling measurement errors. Using simulated data, we present numerical illustrations along with step-by-step implementations in the \texttt{R} environment to facilitate a deeper understanding of dWOLS-based DTR estimation methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18819v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adel Ahmadi Nadi, Michael Wallace</dc:creator>
    </item>
    <item>
      <title>Bayesian mixture modeling using a mixture of finite mixtures with normalized inverse Gaussian weights</title>
      <link>https://arxiv.org/abs/2501.18854</link>
      <description>arXiv:2501.18854v1 Announce Type: new 
Abstract: In Bayesian inference for mixture models with an unknown number of components, a finite mixture model is usually employed that assumes prior distributions for mixing weights and the number of components. This model is called a mixture of finite mixtures (MFM). As a prior distribution for the weights, a (symmetric) Dirichlet distribution is widely used for conjugacy and computational simplicity, while the selection of the concentration parameter influences the estimate of the number of components. In this paper, we focus on estimating the number of components. As a robust alternative Dirichlet weights, we present a method based on a mixture of finite mixtures with normalized inverse Gaussian weights. The motivation is similar to the use of normalized inverse Gaussian processes instead of Dirichlet processes for infinite mixture modeling. Introducing latent variables, the posterior computation is carried out using block Gibbs sampling without using the reversible jump algorithm. The performance of the proposed method is illustrated through some numerical experiments and real data examples, including clustering, density estimation, and community detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18854v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fumiya Iwashige, Shintaro Hashimoto</dc:creator>
    </item>
    <item>
      <title>Measuring Inaccuracies in the Proportional Hazard Rate Model based on Extropy using a Length-Biased Weighted Residual approach</title>
      <link>https://arxiv.org/abs/2501.18888</link>
      <description>arXiv:2501.18888v1 Announce Type: new 
Abstract: In this paper, we consider the concept of the residual inaccuracy measure and extend it to its weighted version based on extropy. Properties of this measure are studied and the discrimination principle is applied in the class of proportional hazard rate (PHR) models. A characterization problem for the proposed weighted extropy-inaccuracy measure is studied. We propose some alternative expressions of weighted residual measure of inaccuracy. Additionally, we establish upper and lower limits and various inequalities related to the weighted residual inaccuracy measure using extropy. Non-parametric estimators based on the kernel density estimation method and empirical distribution function for the proposed measure are obtained and the performance of the estimators are also discussed using some simulation studies. Finally, a real dataset is applied for illustrating our new proposed measure. In general, our study highlights the potential of the weighted residual inaccuracy measure based on extropy as a powerful tool for improving the quality and reliability of data analysis and modelling across various disciplines. Researchers and practitioners can benefit from incorporating this measure into their analytical toolkit to enhance the accuracy and effectiveness of their work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18888v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>M. Hashempour, M. R. Kazemi</dc:creator>
    </item>
    <item>
      <title>Estimands for single arm dose optimization trials in oncology</title>
      <link>https://arxiv.org/abs/2501.18930</link>
      <description>arXiv:2501.18930v1 Announce Type: new 
Abstract: Phase I dose escalation trials in oncology generally aim to find the maximum tolerated dose (MTD). However, with the advent of molecular targeted therapies and antibody drug conjugates, dose limiting toxicities are less frequently observed, giving rise to the concept of optimal biological dose (OBD), which considers both efficacy and toxicity. The Estimand framework presented in the addendum of the ICH E9(R1) guidelines strengthens the dialogue between different stakeholders by bringing in greater clarity in the clinical trial objectives and by providing alignment between the targeted estimand under consideration and the statistical analysis methods. However, there lacks clarity in implementing this framework in early phase dose optimization studies. This manuscript aims at discussing the Estimand framework for dose optimization trials in oncology considering efficacy and toxicity through utility functions. Such trials should include Pharmacokinetics (PK) data, toxicity data, and efficacy data. Based on these data, the analysis methods used to identify the optimized dose/s are also described. Focusing on optimizing the utility function to estimate the OBD, the population-level summary measure should reflect only the properties used for the estimating this utility function. A detailed strategy recommendation for intercurrent events has been provided using a real-life oncology case study. Key recommendations regarding the estimand attributes include that in a seamless Phase I/II dose optimization trial, the treatment attribute should start when the subject receives the first dose. We argue that such a framework brings in additional clarity to dose optimization trial objectives and strengthens the understanding of the drug under consideration that would enable the correct dose to move to Phase II of clinical development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18930v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayon Mukherjee, Jonathan L. Moscovici, Zheng Liu</dc:creator>
    </item>
    <item>
      <title>Bayesianize Fuzziness in the Statistical Analysis of Fuzzy Data</title>
      <link>https://arxiv.org/abs/2501.18974</link>
      <description>arXiv:2501.18974v1 Announce Type: new 
Abstract: Fuzzy data, prevalent in social sciences and other fields, capture uncertainties arising from subjective evaluations and measurement imprecision. Despite significant advancements in fuzzy statistics, a unified inferential regression-based framework remains undeveloped. Hence, we propose a novel approach for analyzing bounded fuzzy variables within a regression framework. Building on the premise that fuzzy data result from a process analogous to statistical coarsening, we introduce a conditional probabilistic approach that links observed fuzzy statistics (e.g., mode, spread) to the underlying, unobserved statistical model, which depends on external covariates. The inferential problem is addressed using Approximate Bayesian methods, mainly through a Gibbs sampler incorporating a quadratic approximation of the posterior distribution. Simulation studies and applications involving external validations are employed to evaluate the effectiveness of the proposed approach for fuzzy data analysis. By reintegrating fuzzy data analysis into a more traditional statistical framework, this work provides a significant step toward enhancing the interpretability and applicability of fuzzy statistical methods in many applicative contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18974v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Calcagn\`i, Przemys{\l}aw Grzegorzewski, Maciej Romaniuk</dc:creator>
    </item>
    <item>
      <title>Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)</title>
      <link>https://arxiv.org/abs/2501.19047</link>
      <description>arXiv:2501.19047v2 Announce Type: new 
Abstract: To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19047v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maja Pavlovic</dc:creator>
    </item>
    <item>
      <title>The Case for Time in Causal DAGs</title>
      <link>https://arxiv.org/abs/2501.19311</link>
      <description>arXiv:2501.19311v1 Announce Type: new 
Abstract: We make the case for incorporating time explicitly into the definition of variables in causal directed acyclic graphs (DAGs). Causality requires that causes precede effects in time, meaning that the causal relationships between variables in one time order may not be the same in another. Therefore, any causal model requires temporal qualification. We formalize a notion of time for causal variables and argue that this resolves existing ambiguity in causal DAGs and is essential to assessing the validity of the acyclicity assumption. If variables are separated in time, their causal relationship is necessarily acyclic. Otherwise, acyclicity depends on the absence of any causal cycles permitted by the time order. We introduce a formal distinction between these two conditions and lay out their respective implications. We outline connections of our contribution with different strands of the broader causality literature and discuss the ramifications of considering time for the interpretation and applicability of DAGs as causal models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19311v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander G. Reisach, Alberto Su\'arez, Sebastian Weichwald, Antoine Chambaz</dc:creator>
    </item>
    <item>
      <title>Multi-Frame Blind Manifold Deconvolution for Rotating Synthetic Aperture Imaging</title>
      <link>https://arxiv.org/abs/2501.19386</link>
      <description>arXiv:2501.19386v1 Announce Type: new 
Abstract: Rotating synthetic aperture (RSA) imaging system captures images of the target scene at different rotation angles by rotating a rectangular aperture. Deblurring acquired RSA images plays a critical role in reconstructing a latent sharp image underlying the scene. In the past decade, the emergence of blind convolution technology has revolutionised this field by its ability to model complex features from acquired images. Most of the existing methods attempt to solve the above ill-posed inverse problem through maximising a posterior.
  Despite this progress, researchers have paid limited attention to exploring low-dimensional manifold structures of the latent image within a high-dimensional ambient-space. Here, we propose a novel method to process RSA images using manifold fitting and penalisation in the content of multi-frame blind convolution. We develop fast algorithms for implementing the proposed procedure. Simulation studies demonstrate that manifold-based deconvolution can outperform conventional deconvolution algorithms in the sense that it can generate a sharper estimate of the latent image in terms of estimating pixel intensities and preserving structural details.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19386v1</guid>
      <category>stat.ME</category>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dao Lin, Jian Zhang, Martin Benning</dc:creator>
    </item>
    <item>
      <title>Transfer Learning for Nonparametric Contextual Dynamic Pricing</title>
      <link>https://arxiv.org/abs/2501.18836</link>
      <description>arXiv:2501.18836v1 Announce Type: cross 
Abstract: Dynamic pricing strategies are crucial for firms to maximize revenue by adjusting prices based on market conditions and customer characteristics. However, designing optimal pricing strategies becomes challenging when historical data are limited, as is often the case when launching new products or entering new markets. One promising approach to overcome this limitation is to leverage information from related products or markets to inform the focal pricing decisions. In this paper, we explore transfer learning for nonparametric contextual dynamic pricing under a covariate shift model, where the marginal distributions of covariates differ between source and target domains while the reward functions remain the same. We propose a novel Transfer Learning for Dynamic Pricing (TLDP) algorithm that can effectively leverage pre-collected data from a source domain to enhance pricing decisions in the target domain. The regret upper bound of TLDP is established under a simple Lipschitz condition on the reward function. To establish the optimality of TLDP, we further derive a matching minimax lower bound, which includes the target-only scenario as a special case and is presented for the first time in the literature. Extensive numerical experiments validate our approach, demonstrating its superiority over existing methods and highlighting its practical utility in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18836v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Wang, Feiyu Jiang, Zifeng Zhao, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Lightspeed Geometric Dataset Distance via Sliced Optimal Transport</title>
      <link>https://arxiv.org/abs/2501.18901</link>
      <description>arXiv:2501.18901v1 Announce Type: cross 
Abstract: We introduce sliced optimal transport dataset distance (s-OTDD), a model-agnostic, embedding-agnostic approach for dataset comparison that requires no training, is robust to variations in the number of classes, and can handle disjoint label sets. The core innovation is Moment Transform Projection (MTP), which maps a label, represented as a distribution over features, to a real number. Using MTP, we derive a data point projection that transforms datasets into one-dimensional distributions. The s-OTDD is defined as the expected Wasserstein distance between the projected distributions, with respect to random projection parameters. Leveraging the closed form solution of one-dimensional optimal transport, s-OTDD achieves (near-)linear computational complexity in the number of data points and feature dimensions and is independent of the number of classes. With its geometrically meaningful projection, s-OTDD strongly correlates with the optimal transport dataset distance while being more efficient than existing dataset discrepancy measures. Moreover, it correlates well with the performance gap in transfer learning and classification accuracy in data augmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18901v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Hai Nguyen, Tuan Pham, Nhat Ho</dc:creator>
    </item>
    <item>
      <title>PUATE: Semiparametric Efficient Average Treatment Effect Estimation from Treated (Positive) and Unlabeled Units</title>
      <link>https://arxiv.org/abs/2501.19345</link>
      <description>arXiv:2501.19345v1 Announce Type: cross 
Abstract: The estimation of average treatment effects (ATEs), defined as the difference in expected outcomes between treatment and control groups, is a central topic in causal inference. This study develops semiparametric efficient estimators for ATE estimation in a setting where only a treatment group and an unknown group-comprising units for which it is unclear whether they received the treatment or control-are observable. This scenario represents a variant of learning from positive and unlabeled data (PU learning) and can be regarded as a special case of ATE estimation with missing data. For this setting, we derive semiparametric efficiency bounds, which provide lower bounds on the asymptotic variance of regular estimators. We then propose semiparametric efficient ATE estimators whose asymptotic variance aligns with these efficiency bounds. Our findings contribute to causal inference with missing data and weakly supervised learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19345v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato, Fumiaki Kozai, Ryo Inokuchi</dc:creator>
    </item>
    <item>
      <title>Carefree multiple testing with e-processes</title>
      <link>https://arxiv.org/abs/2501.19360</link>
      <description>arXiv:2501.19360v1 Announce Type: cross 
Abstract: E-processes enable hypothesis testing with ongoing data collection while maintaining Type I error control. However, when testing multiple hypotheses simultaneously, current $e$-value based multiple testing methods such as e-BH are not invariant to the order in which data are gathered for the different $e$-processes. This can lead to undesirable situations, e.g., where a hypothesis rejected at time $t$ is no longer rejected at time $t+1$ after choosing to gather more data for one or more $e$-processes unrelated to that hypothesis. We argue that multiple testing methods should always work with suprema of $e$-processes. We provide an example to illustrate that e-BH does not control this FDR at level $\alpha$ when applied to suprema of $e$-processes. We show that adjusters can be used to ensure FDR-sup control with e-BH under arbitrary dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19360v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yury Tavyrikov, Jelle J. Goeman, Rianne de Heide</dc:creator>
    </item>
    <item>
      <title>Efficient Estimation Under Data Fusion</title>
      <link>https://arxiv.org/abs/2111.14945</link>
      <description>arXiv:2111.14945v4 Announce Type: replace 
Abstract: We aim to make inferences about a smooth, finite-dimensional parameter by fusing data from multiple sources together. Previous works have studied the estimation of a variety of parameters in similar data fusion settings, including in the estimation of the average treatment effect and average reward under a policy, with the majority of them merging one historical data source with covariates, actions, and rewards and one data source of the same covariates. In this work, we consider the general case where one or more data sources align with each part of the distribution of the target population, for example, the conditional distribution of the reward given actions and covariates. We describe potential gains in efficiency that can arise from fusing these data sources together in a single analysis, which we characterize by a reduction in the semiparametric efficiency bound. We also provide a general means to construct estimators that achieve these bounds. In numerical experiments, we illustrate marked improvements in efficiency from using our proposed estimators rather than their natural alternatives. Finally, we illustrate the magnitude of efficiency gains that can be realized in vaccine immunogenicity studies by fusing data from two HIV vaccine trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.14945v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/biomet/asad007</arxiv:DOI>
      <arxiv:journal_reference>Biometrika 110, no. 4 (2023): 1041-1054</arxiv:journal_reference>
      <dc:creator>Sijia Li, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Covariance matrix completion via auxiliary information</title>
      <link>https://arxiv.org/abs/2402.05767</link>
      <description>arXiv:2402.05767v2 Announce Type: replace 
Abstract: Covariance matrix estimation is an important task in the analysis of multivariate data in disparate scientific fields. However, modern scientific data are often incomplete due to factors beyond the control of researchers, and traditional methods may only yield incomplete covariance matrix estimates. For example, it is impossible to obtain a complete sample covariance matrix if some variable pairs have no joint observations. We propose a novel approach, AuxCov, which exploits auxiliary variables to produce complete covariance matrix estimates from structurally incomplete data. In neuroscience, an example of an auxiliary variable is the distance between neurons, which is typically inversely related to the strength of the neuronal correlation. AuxCov estimates the relationship between the observed correlations and the auxiliary variables via regression, and uses it to predict the missing correlation estimates and to regularize the observed ones. We implement AuxCov using parametric and nonparametric regression methods, and propose procedures for tuning parameter selection and uncertainty quantification. We evaluate the performance of AuxCov through simulations and in the analysis of large-scale neuroscience data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05767v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Steneman, Giuseppe Vinci</dc:creator>
    </item>
    <item>
      <title>DGP-LVM: Derivative Gaussian process latent variable models</title>
      <link>https://arxiv.org/abs/2404.04074</link>
      <description>arXiv:2404.04074v3 Announce Type: replace 
Abstract: We develop a framework for derivative Gaussian process latent variable models (DGP-LVMs) that can handle multi-dimensional output data using modified derivative covariance functions. The modifications account for complexities in the underlying data generating process such as scaled derivatives, varying information across multiple output dimensions as well as interactions between outputs. Further, our framework provides uncertainty estimates for each latent variable samples using Bayesian inference. Through extensive simulations, we demonstrate that latent variable estimation accuracy can be drastically increased by including derivative information due to our proposed covariance function modifications. The developments are motivated by a concrete biological research problem involving the estimation of the unobserved cellular ordering from single-cell RNA (scRNA) sequencing data for gene expression and its corresponding derivative information known as RNA velocity. Since the RNA velocity is only an estimate of the exact derivative information, the derivative covariance functions need to account for potential scale differences. In a real-world case study, we illustrate the application of DGP-LVMs to such scRNA sequencing data. While motivated by this biological problem, our framework is generally applicable to all kinds of latent variable estimation problems involving derivative information irrespective of the field of study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04074v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Soham Mukherjee, Manfred Claassen, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Bayesian Estimation of Hierarchical Linear Models from Incomplete Data: Cluster-Level Interaction Effects and Small Sample Sizes</title>
      <link>https://arxiv.org/abs/2405.21020</link>
      <description>arXiv:2405.21020v2 Announce Type: replace 
Abstract: We consider Bayesian estimation of a hierarchical linear model (HLM) from partially observed data, assumed to be missing at random, and small sample sizes. A vector of continuous covariates $C$ includes cluster-level partially observed covariates with interaction effects. Due to small sample sizes from 37 patient-physician encounters repeatedly measured at four time points, maximum likelihood estimation is suboptimal. Existing Gibbs samplers impute missing values of $C$ by a Metropolis algorithm using proposal densities that have constant variances while the target posterior distributions have nonconstant variances. Therefore, these samplers may not ensure compatibility with the HLM and, as a result, may not guarantee unbiased estimation of the HLM. We introduce a compatible Gibbs sampler that imputes parameters and missing values directly from the exact posterior distributions. We apply our Gibbs sampler to the longitudinal patient-physician encounter data and compare our estimators with those from existing methods by simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.21020v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongho Shin, Yongyun Shin, Nao Hagiwara</dc:creator>
    </item>
    <item>
      <title>ROC curve analysis for functional markers</title>
      <link>https://arxiv.org/abs/2407.20929</link>
      <description>arXiv:2407.20929v2 Announce Type: replace 
Abstract: Functional markers become a more frequent tool in medical diagnosis. In this paper, we aim to define an index allowing to discriminate between populations when the observations are functional data belonging to a Hilbert space. We discuss some of the problems arising when estimating optimal directions defined to maximize the area under the curve of a projection index and we construct the corresponding ROC curve. We also go one step forward and consider the case of possibly different covariance operators, for which we recommend a quadratic discrimination rule. Consistency results are derived for both linear and quadratic indexes, under mild conditions. The results of our numerical experiments allow to see the advantages of the quadratic rule when the populations have different covariance operators. We also illustrate the considered methods on a real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20929v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana M. Bianco, Graciela Boente, Juan Carlos Pardo-Fern\'andez</dc:creator>
    </item>
    <item>
      <title>Cross-Validation with Antithetic Gaussian Randomization</title>
      <link>https://arxiv.org/abs/2412.14423</link>
      <description>arXiv:2412.14423v2 Announce Type: replace 
Abstract: We introduce a new cross-validation method based on an equicorrelated Gaussian randomization scheme. The method is well-suited for problems where sample splitting is infeasible, such as when data violate the assumption of independent and identical distribution. Even when sample splitting is possible, our method offers a computationally efficient alternative for estimating the prediction error, achieving comparable or even lower error than standard cross-validation in a few train-test repetitions. Drawing inspiration from recent techniques like data-fission and data-thinning, our method constructs train-test data pairs using externally generated Gaussian randomization variables. The key innovation lies in a carefully designed correlation structure among the randomization variables, which we refer to as antithetic Gaussian randomization. In theory, we show that this correlation is crucial in ensuring that the variance of our estimator remains bounded while allowing the bias to vanish. Through simulations on various data types and loss functions, we highlight the advantages of our antithetic Gaussian randomization scheme over both independent randomization and standard cross-validation, where the bias-variance tradeoff depends heavily on the number of folds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14423v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sifan Liu, Snigdha Panigrahi, Jake A. Soloff</dc:creator>
    </item>
    <item>
      <title>Clustering of functional data prone to complex heteroscedastic measurement error</title>
      <link>https://arxiv.org/abs/2501.14919</link>
      <description>arXiv:2501.14919v2 Announce Type: replace 
Abstract: Several factors make clustering of functional data challenging, including the infinite-dimensional space to which observations belong and the lack of a defined probability density function for the functional random variable. To overcome these barriers, researchers either assume that observations belong to a finite-dimensional space spanned by basis functions or apply nonparametric smoothing methods to the functions prior to clustering. Although extensive literature describes clustering methods for functional data, few studies have explored the clustering of measurement error--prone function-valued data. In this work, we consider clustering methods for functional data prone to complex, heteroscedastic measurement errors. Two stage-based methods using mixed-effects models are first applied to adjust for measurement error bias, followed by cluster analysis of the measurement error--adjusted curves. Through simulations, we investigate how varying sample size, the magnitude of measurement error, and the presence of complex heteroscedastic measurement errors influence the cluster analysis of functional data. Our results indicate that failing to account for measurement errors and the correlation structures associated with frequently collected functional data reduces the accuracy of identifying the true latent groups or clusters. The method consistently produces better results regardless of the initial clustering values used. Moreover, it is flexible and can be applied to various clustering approaches, based on the specific distribution of the data. The developed methods are applied to two data sets: a school-based study of energy expenditure among elementary school-aged children in Texas and data from the National Health and Nutrition Examination Survey on participants' physical activity monitored by wearable devices at frequent intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14919v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andi Mai, Lan Xue, Roger Zoh, Carmen Tekwe</dc:creator>
    </item>
    <item>
      <title>SplitVAEs: Decentralized scenario generation from siloed data for stochastic optimization problems</title>
      <link>https://arxiv.org/abs/2409.12328</link>
      <description>arXiv:2409.12328v2 Announce Type: replace-cross 
Abstract: Stochastic optimization problems in large-scale multi-stakeholder networked systems (e.g., power grids and supply chains) rely on data-driven scenarios to encapsulate complex spatiotemporal interdependencies. However, centralized aggregation of stakeholder data is challenging due to the existence of data silos resulting from computational and logistical bottlenecks. In this paper, we present SplitVAEs, a decentralized scenario generation framework that leverages variational autoencoders to generate high-quality scenarios without moving stakeholder data. With the help of experiments on distributed memory systems, we demonstrate the broad applicability of SplitVAEs in a variety of domain areas that are dominated by a large number of stakeholders. Our experiments indicate that SplitVAEs can learn spatial and temporal interdependencies in large-scale networks to generate scenarios that match the joint historical distribution of stakeholder data in a decentralized manner. Our experiments show that SplitVAEs deliver robust performance compared to centralized, state-of-the-art benchmark methods while significantly reducing data transmission costs, leading to a scalable, privacy-enhancing alternative to scenario generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12328v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/BigData62323.2024.10826070</arxiv:DOI>
      <dc:creator>H M Mohaimanul Islam, Huynh Q. N. Vo, Paritosh Ramanan</dc:creator>
    </item>
    <item>
      <title>Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection</title>
      <link>https://arxiv.org/abs/2409.15844</link>
      <description>arXiv:2409.15844v2 Announce Type: replace-cross 
Abstract: We introduce adaptive learn-then-test (aLTT), an efficient hyperparameter selection procedure that provides finite-sample statistical guarantees on the population risk of AI models. Unlike the existing learn-then-test (LTT) technique, which relies on conventional p-value-based multiple hypothesis testing (MHT), aLTT implements sequential data-dependent MHT with early termination by leveraging e-processes. As a result, aLTT can reduce the number of testing rounds, making it particularly well-suited for scenarios in which testing is costly or presents safety risks. Apart from maintaining statistical validity, in applications such as online policy selection for offline reinforcement learning and prompt engineering, aLTT is shown to achieve the same performance as LTT while requiring only a fraction of the testing rounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15844v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Zecchin, Sangwoo Park, Osvaldo Simeone</dc:creator>
    </item>
    <item>
      <title>Joint Probability Estimation of Many Binary Outcomes via Localized Adversarial Lasso</title>
      <link>https://arxiv.org/abs/2410.15166</link>
      <description>arXiv:2410.15166v4 Announce Type: replace-cross 
Abstract: In this work we consider estimating the probability of many (possibly dependent) binary outcomes which is at the core of many applications, e.g., multi-level treatments in causal inference, demands for bundle of products, etc. Without further conditions, the probability distribution of an M dimensional binary vector is characterized by exponentially in M coefficients which can lead to a high-dimensional problem even without the presence of covariates. Understanding the (in)dependence structure allows us to substantially improve the estimation as it allows for an effective factorization of the probability distribution. In order to estimate the probability distribution of a M dimensional binary vector, we leverage a Bahadur representation that connects the sparsity of its coefficients with independence across the components. We propose to use regularized and adversarial regularized estimators to obtain an adaptive estimator with respect to the dependence structure which allows for rates of convergence to depend on this intrinsic (lower) dimension. These estimators are needed to handle several challenges within this setting, including estimating nuisance parameters, estimating covariates, and nonseparable moment conditions. Our main results consider the presence of (low dimensional) covariates for which we propose a locally penalized estimator. We provide pointwise rates of convergence addressing several issues in the theoretical analyses as we strive for making a computationally tractable formulation. We apply our results in the estimation of causal effects with multiple binary treatments and show how our estimators can improve the finite sample performance when compared with non-adaptive estimators that try to estimate all the probabilities directly. We also provide simulations that are consistent with our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15166v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Belloni, Yan Chen, Matthew Harding</dc:creator>
    </item>
    <item>
      <title>Multiple testing in multi-stream sequential change detection</title>
      <link>https://arxiv.org/abs/2501.04130</link>
      <description>arXiv:2501.04130v4 Announce Type: replace-cross 
Abstract: Multi-stream sequential change detection involves simultaneously monitoring many streams of data and trying to detect when their distributions change, if at all. Here, we theoretically study multiple testing issues that arise from detecting changes in many streams. We point out that any algorithm with finite average run length (ARL) must have a trivial worst-case false detection rate (FDR), family-wise error rate (FWER), per-family error rate (PFER), and global error rate (GER); thus, any attempt to control these Type I error metrics is fundamentally in conflict with the desire for a finite ARL (which is typically necessary in order to have a small detection delay). One of our contributions is to define a new class of metrics which can be controlled, called error over patience (EOP). We propose algorithms that combine the recent e-detector framework (which generalizes the Shiryaev-Roberts and CUSUM methods) with the recent e-Benjamini-Hochberg procedure and e-Bonferroni procedures. We prove that these algorithms control the EOP at any desired level under very general dependence structures on the data within and across the streams. In fact, we prove a more general error control that holds uniformly over all stopping times and provides a smooth trade-off between the conflicting metrics. Additionally, if finiteness of the ARL is forfeited, we show that our algorithms control the worst-case Type I error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04130v4</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
  </channel>
</rss>
