<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 May 2025 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Tutorial on Bayesian Functional Regression Using Stan</title>
      <link>https://arxiv.org/abs/2505.05633</link>
      <description>arXiv:2505.05633v1 Announce Type: new 
Abstract: This manuscript provides step-by-step instructions for implementing Bayesian functional regression models using Stan. Extensive simulations indicate that the inferential performance of the methods is comparable to that of state-of-the-art frequentist approaches. However, Bayesian approaches allow for more flexible modeling and provide an alternative when frequentist methods are not available or may require additional development. Methods and software are illustrated using the accelerometry data from the National Health and Nutrition Examination Survey (NHANES).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05633v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziren Jiang, Ciprian Crainiceanu, Erjia Cui</dc:creator>
    </item>
    <item>
      <title>Variational Bayesian Logistic Tensor Regression with Application to Image Recognition</title>
      <link>https://arxiv.org/abs/2505.05730</link>
      <description>arXiv:2505.05730v1 Announce Type: new 
Abstract: In recent years, image recognition method has been a research hotspot in various fields such as video surveillance, biometric identification, unmanned vehicles, human-computer interaction, and medical image recognition. Existing recognition methods often ignore structural information of image data or depend heavily on the sample size of image data. To address this issue, we develop a novel variational Bayesian method for image classification in a logistic tensor regression model with image tensor predictors by utilizing tensor decomposition to approximate tensor regression. To handle the sparsity of tensor coefficients, we introduce the multiway shrinkage priors for marginal factor vectors of tensor coefficients. In particular, we obtain a closed-form approximation to the variational posteriors for classification prediction based on the matricization of tensor decomposition. Simulation studies are conducted to investigate the performance of the proposed methodologies in terms of accuracy, precision and F1 score. Flower image data and chest X-ray image data are illustrated by the proposed methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05730v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunzhi Jin, Yanqing Zhang, Niansheng Tang</dc:creator>
    </item>
    <item>
      <title>Model-based calibration of gear-specific fish abundance survey data as a change-of-support problem</title>
      <link>https://arxiv.org/abs/2505.05767</link>
      <description>arXiv:2505.05767v1 Announce Type: new 
Abstract: In a continental-scale fish abundance study, a major challenge in deriving an absolute abundance estimate lies in the fact that regional surveys deploy different gear types, each with its unique field of view, producing gear-specific relative abundance data. Thus, data from regional surveys in the study must be converted from the gear-specific relative scale to an absolute scale before being combined to estimate a continental scale absolute abundance. In this paper, we develop a tool that takes gear-based data as input, and produces as output the required conversion, with associated uncertainty. Methodologically, this tool is operationalized from a Bayesian hierarchical model which we develop in an inferential context that is akin to the change-of-support problem often encountered in spatial studies; the actual context here is to reconcile abundance data at various gear-specific scales, some being relative, and others, absolute. We consider data from a small-scale calibration experiment in which 2 to 4 underwater video camera types, as well as an acoustic echosounder, were simultaneously deployed on each of 21 boat trips. While acoustic fish signals are recorded along transects on the absolute scale, they are subject to confounding from acoustically similar species, thus requiring an externally derived correction factor. Conversely, a camera allows visual distinction between species but records data on a gear-specific relative scale. Our statistical modeling framework reflects the relationship among all 5 gear types across the 21 trips, and the resulting model is used to derive calibration formulae to translate relative abundance data to the corrected absolute abundance scale whenever a camera is deployed alone. Cross-validation is conducted using mark-recapture abundance estimates. We also briefly discuss the case when one camera type is deployed alongside the echosounder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05767v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grace S. Chiu, Anton H. Westveld, Mark A. Albins, Kevin M. Boswell, John M. Hoenig, Sean P. Powers, S. Lynne Stokes, Allison L. White</dc:creator>
    </item>
    <item>
      <title>Statistical methods for cost-effectiveness analysis of left-truncated censored survival data with treatment delays</title>
      <link>https://arxiv.org/abs/2505.05771</link>
      <description>arXiv:2505.05771v1 Announce Type: new 
Abstract: The incremental cost-effectiveness ratio (ICER) and incremental net benefit (INB) are widely used for cost-effectiveness analysis. We develop methods for estimation and inference for the ICER and INB which use the semiparametric stratified Cox proportional hazard model, allowing for adjustment for risk factors. Since in public health settings, patients often begin treatment after they become eligible, we account for delay times in treatment initiation. Excellent finite sample properties of the proposed estimator are demonstrated in an extensive simulation study under different delay scenarios. We apply the proposed method to evaluate the cost-effectiveness of switching treatments among AIDS patients in Tanzania.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05771v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Polyna Khudyakov, Li Xu, Ce Yang, Donna Spiegelman, Molin Wang</dc:creator>
    </item>
    <item>
      <title>Parsimonious Modeling of Periodic Time Series Using Fourier and Wavelet Techniques</title>
      <link>https://arxiv.org/abs/2505.05778</link>
      <description>arXiv:2505.05778v1 Announce Type: new 
Abstract: This paper proposes Fourier-based and wavelet-based techniques for analyzing periodic financial time series. Conventional models such as the periodic autoregressive conditional heteroscedastic (PGARCH) and periodic autoregressive conditional duration (PACD) often involve many parameters. The methods put forward here resulted in more parsimonious models with increased forecast efficiency. The effectiveness of these approaches is demonstrated through simulation and data analysis studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05778v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rhea Davis, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>GLOSSA: a user-friendly R Shiny application for Bayesian machine learning analysis of marine species distribution</title>
      <link>https://arxiv.org/abs/2505.05862</link>
      <description>arXiv:2505.05862v1 Announce Type: new 
Abstract: Species distribution models (SDMs) are one of the most common statistical methods to assess species occupancy and geographic distribution patterns. With the increasing complexity of ecological data, many methodological approaches have been developed, often accessible through command-line interfaces or graphical user interfaces (GUIs). However, few species distribution modeling tools are designed to be well-documented, user-friendly, flexible, and reproducible.
  Here we introduce GLOSSA, an open-source R package and Shiny app designed for species distribution modeling using species occurrence and environmental data. GLOSSA's user-friendly interface guides users through steps including data uploading, processing, model fitting, spatial and temporal projections, and interactive visualization of results. The app also calculates variable importance, generates response curves with environmental variables, and performs cross-validation. At its core, GLOSSA modeling approach is based on Bayesian Additive Regression Trees (BART), an innovative machine learning method.
  We present the functionality and versatility of GLOSSA through three case studies, addressing a range of ecological scenarios at regional and global scales. Along with comprehensive documentation, examples, and tutorials, these case studies illustrate how an intuitive graphical interface can make species distribution modeling accessible to a broad audience.
  GLOSSA stands out as an easy-to-use tool for species distribution modeling, providing an intuitive interface, detailed documentation, flexible modeling, and interactive result exploration and export options. Additionally, its outputs can be used directly to inform marine ecosystem models (MEMs), enhancing its utility in ecological research and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05862v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Mestre-Tom\'as, A. Fuster-Alonso, J. M. Bellido, M. Coll</dc:creator>
    </item>
    <item>
      <title>Diffusion piecewise exponential models for survival extrapolation using Piecewise Deterministic Monte Carlo</title>
      <link>https://arxiv.org/abs/2505.05932</link>
      <description>arXiv:2505.05932v1 Announce Type: new 
Abstract: The piecewise exponential model is a flexible non-parametric approach for time-to-event data, but extrapolation beyond final observation times typically relies on random walk priors and deterministic knot locations, resulting in unrealistic long-term hazards. We introduce the diffusion piecewise exponential model, a prior framework consisting of a discretised diffusion for the hazard, that can encode a wide variety of information about the long-term behaviour of the hazard, time changed by a Poisson process prior for knot locations. This allows the behaviour of the hazard in the observation period to be combined with prior information to inform extrapolations. Efficient posterior sampling is achieved using Piecewise Deterministic Markov Processes, whereby we extend existing approaches using sticky dynamics from sampling spike-and-slab distributions to more general transdimensional posteriors. We focus on applications in Health Technology Assessment, where the need to compute mean survival requires hazard functions to be extrapolated beyond the observation period, showcasing performance on datasets for Colon cancer and Leukaemia patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05932v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Hardcastle, Samuel Livingstone, Gianluca Baio</dc:creator>
    </item>
    <item>
      <title>Mixtures of multivariate linear asymmetric Laplace regressions with multiple asymmetric Laplace covariates</title>
      <link>https://arxiv.org/abs/2505.05979</link>
      <description>arXiv:2505.05979v1 Announce Type: new 
Abstract: In response to the challenge of accommodating non-Gaussian behaviour in data, the shifted asymmetric Laplace (SAL) cluster-weighted model (SALCWM) is introduced as a model-based method for jointly clustering responses and random covariates that exhibit skewness. Within each cluster, the multivariate SAL distribution is assumed for both the covariates and the responses given the covariates. To mitigate the effect of possible atypical observations, a heavy-tailed extension, the contaminated SALCWM (cSALCWM), is also proposed. In addition to the SALCWM parameters, each mixture component has a parameter controlling the proportion of outliers, one controlling the proportion of leverage points, one specifying the degree of outlierness, and another specifying the degree of leverage. The cSALCWM has the added benefit that once the model parameters are estimated and the observations are assigned to components, a more refined intra-group classification in typical points, (mild) outliers, good leverage, and bad leverage points can be directly obtained. An expectation-conditional maximization algorithm is developed for efficient maximum likelihood parameter estimation under this framework. Theoretical identifiability conditions are established, and empirical results from simulation studies and validation via real-world applications demonstrate that the cSALCWM not only preserves the modelling strengths of the SALCWM but also significantly enhances outlier detection and overall inference reliability. The methodology proposed in this paper has been implemented in an \texttt{R} package, which is publicly available at https://github.com/arnootto/ALCWM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05979v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Arnoldus F. Otto (Department of Statistics, University of Pretoria, Pretoria, South Africa), Andri\"ette Bekker (Department of Statistics, University of Pretoria, Pretoria, South Africa), Antonio Punzo (Department of Economics and Business, University of Catania, Catania, Italy), Johannes T. Ferreira (School of Statistics and Actuarial Science, University of the Witwatersrand, Johannesburg, South Africa), Cristina Tortora (Department of Mathematics and Statistics, San Jos\'e State University, California, United States of America)</dc:creator>
    </item>
    <item>
      <title>Estimating Covariate-adjusted Survival Curve in Distributed Data Environment using Data Collaboration Quasi-Experiment</title>
      <link>https://arxiv.org/abs/2505.06035</link>
      <description>arXiv:2505.06035v1 Announce Type: new 
Abstract: In recent years, there has been an increasing demand for privacy-preserving survival analysis using integrated observational data from multiple institutions and data sources. In particular, estimating survival curves adjusted for covariates that account for confounding factors is essential for evaluating the effectiveness of medical treatments. While high-precision estimation of survival curves requires the collection of large amounts of individual-level data, sharing such data is challenging due to privacy concerns and, even if sharing were possible, the communication costs between institutions would be enormous. To address these challenges, this study proposes and evaluates a novel method that leverages an extended data collaboration quasi-experiment, to estimate covariate-adjusted survival curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06035v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akihiro Toyoda, Yuji Kawamata, Tomoru Nakayama, Akira Imakura, Tetsuya Sakurai, Yukihiko Okada</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference in Boundary Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2505.05670</link>
      <description>arXiv:2505.05670v1 Announce Type: cross 
Abstract: Boundary Discontinuity Designs are used to learn about treatment effects along a continuous boundary that splits units into control and treatment groups according to a bivariate score variable. These research designs are also called Multi-Score Regression Discontinuity Designs, a leading special case being Geographic Regression Discontinuity Designs. We study the statistical properties of commonly used local polynomial treatment effects estimators along the continuous treatment assignment boundary. We consider two distinct approaches: one based explicitly on the bivariate score variable for each unit, and the other based on their univariate distance to the boundary. For each approach, we present pointwise and uniform estimation and inference methods for the treatment effect function over the assignment boundary. Notably, we show that methods based on univariate distance to the boundary exhibit an irreducible large misspecification bias when the assignment boundary has kinks or other irregularities, making the distance-based approach unsuitable for empirical work in those settings. In contrast, methods based on the bivariate score variable do not suffer from that drawback. We illustrate our methods with an empirical application. Companion general-purpose software is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05670v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>Bayesian shape-constrained regression for quantifying Alzheimer's disease biomarker progression</title>
      <link>https://arxiv.org/abs/2505.05700</link>
      <description>arXiv:2505.05700v1 Announce Type: cross 
Abstract: Several biomarkers are hypothesized to indicate early stages of Alzheimer's disease, well before the cognitive symptoms manifest. Their precise relations to the disease progression, however, is poorly understood. This lack of understanding limits our ability to diagnose the disease and intervene effectively at early stages. To provide better understanding of the relation between the disease and biomarker progressions, we propose a novel modeling approach to quantify the biomarkers' trajectories as functions of age. Building on monotone regression splines, we introduce two additional shape constraints to incorporate structures informed by the current medical literature. First, we impose the regression curves to satisfy a vanishing derivative condition, reflecting the observation that changes in biomarkers generally plateau at early and late stages of the disease. Second, we enforce the regression curves to have a unique inflection point, which enhances interpretability of the estimated disease progression and facilitates assessment of temporal ordering among the biomarkers. We fit our shape-constrained regression model under Bayesian framework to take advantage of its ability to account for the heterogeneity in disease progression among individuals. When applied to the BIOCARD data, the model is able to capture asymmetry in the biomarkers' progressions while maintaining interpretability, yielding estimates of the curves with temporal ordering consistent with the existing scientific hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05700v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyuan Li, Zheyu Wang, Akihiko Nishimura</dc:creator>
    </item>
    <item>
      <title>Doublethink: simultaneous Bayesian-frequentist model-averaged hypothesis testing</title>
      <link>https://arxiv.org/abs/2312.17566</link>
      <description>arXiv:2312.17566v3 Announce Type: replace 
Abstract: Establishing the frequentist properties of Bayesian approaches widens their appeal and offers new understanding. In hypothesis testing, Bayesian model averaging addresses the problem that conclusions are sensitive to variable selection. But Bayesian false discovery rate (FDR) guarantees are sensitive to subjective prior assumptions. Here we show that Bayesian model-averaged hypothesis testing is a closed testing procedure that controls the frequentist familywise error rate (FWER) in the strong sense. To quantify the FWER, we use the theory of regular variation and likelihood asymptotics to derive a chi-squared tail approximation for the model-averaged posterior odds. Convergence is pointwise as the sample size grows and, in a simplified setting subject to a minimum effect size assumption, uniform. The 'Doublethink' method computes simultaneous posterior odds and asymptotic p-values for model-averaged hypothesis testing. We explore Doublethink through a Mendelian randomization study and simulations, comparing to approaches like LASSO, stepwise regression, the Benjamini-Hochberg procedure, the harmonic mean p-value and e-values. We consider the limitations of the approach, including finite-sample inflation, and mitigations, like testing groups of correlated variables. We discuss the benefits of Doublethink, including post-hoc variable selection, and its wider implications for the theory and practice of hypothesis testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17566v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helen R. Fryer, Nicolas Arning, Daniel J. Wilson</dc:creator>
    </item>
    <item>
      <title>Two-Stage Nuisance Function Estimation for Causal Mediation Analysis</title>
      <link>https://arxiv.org/abs/2404.00735</link>
      <description>arXiv:2404.00735v2 Announce Type: replace 
Abstract: Tchetgen Tchetgen and Shpitser (2012) introduced an efficient, debiased, and robust influence function-based estimator for the mediation functional, which is the key component in mediation analysis. This estimator relies on the treatment, mediator, and outcome mean mechanisms. However, treating these three mechanisms as nuisance functions and fitting them as accurately as possible may not be the most effective approach. Instead, it is essential to identify the specific functionals and aspects of these mechanisms that impact the estimation of the mediation functional. In this work, we propose a two-stage estimation strategy for certain nuisance functions in the influence function of the mediation functional that are based on these three mechanisms. This strategy is guided by the role those nuisance functions play in the bias structure of the influence function-based estimator for the mediation functional. In the first stage, we estimate two primary nuisance functions, namely the inverse treatment mechanism and the outcome mean mechanism. In the second stage, we leverage these primary functions to estimate two additional nuisance functions that encapsulate the needed information about the mediator mechanism. We propose a nonparametric weighted balancing estimation approach to design the estimator for one of the nuisance functions in Stage 1 and one of in Stage 2, where the weights are designed directly based on the bias of the final estimator for the mediation functional. The remaining two nuisance functions are estimated using standard parametric or nonparametric regression methods. Once all four nuisance functions are obtained, they are incorporated into the influence function-based estimator. We provide a robustness analysis of the proposed method and establish sufficient conditions for consistency and asymptotic normality of our estimator for the mediation functional.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00735v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chang Liu, AmirEmad Ghassami</dc:creator>
    </item>
    <item>
      <title>Constrained Design of a Binary Instrument in a Partially Linear Model</title>
      <link>https://arxiv.org/abs/2406.05592</link>
      <description>arXiv:2406.05592v3 Announce Type: replace 
Abstract: We study the question of how best to assign an encouragement in a randomized encouragement study. In our setting, units arrive with covariates, receive a nudge toward treatment or control, acquire one of those statuses in a way that need not align with the nudge, and finally have a response observed. The nudge can be modeled as a binary instrument if one assumes that it affects the response only via the treatment status. Our goal is to assign the nudge as a function of covariates in a way that best estimates the local average treatment effect (LATE). We assume a partially linear model, wherein the baseline model is non-parametric and the treatment term is linear in the covariates. Under this model, we outline a two-stage procedure to consistently and optimally estimate the LATE. Though the variance of the LATE is intractable, we derive a finite sample approximation and thus a design criterion to minimize. This criterion is convex, allowing for constraints that might arise for budgetary or ethical reasons. We prove conditions under which our solution asymptotically recovers the lowest true variance among all possible nudge propensities. A one-stage version of the algorithm is consistent but not necessarily optimal. We apply our method to a semi-synthetic example involving triage in an emergency department and find significant gains relative to a regression discontinuity design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05592v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Morrison, Minh Nguyen, Jonathan Chen, Michael Baiocchi, Art B. Owen</dc:creator>
    </item>
    <item>
      <title>How accurate are Bayes factor-based null hypothesis tests? A simulation study</title>
      <link>https://arxiv.org/abs/2406.08022</link>
      <description>arXiv:2406.08022v2 Announce Type: replace 
Abstract: Bayes factor null hypothesis tests provide a viable alternative to frequentist measures of evidence quantification. Bayes factors for realistic data sets in areas like psychology cannot be calculated exactly and require numerical approximations to complex integrals. Crucially, the accuracy of these approximations, i.e., whether an approximate Bayes factor corresponds to the exact Bayes factor, is unknown, and may depend on data, prior, and likelihood. We have recently developed a novel statistical procedure, namely marginal simulation-based calibration (SBC) for Bayes factors, to test whether the computed Bayes factors for a given analysis are accurate. Here, we use marginal SBC for Bayes factors and calibration plots to test for some common cognitive designs, whether Bayes factors are calculated accurately. We use the bridgesampling/brms packages in R. We run analyses for three commonly used designs in psychology and psycholinguistics: (a) a design with random effects for subjects only, (b) a Latin square design with crossed random effects for subjects and items, but a single fixed-factor, and (c) a Latin square 2x2 design with crossed random effects for subjects and items. We find that Bayes factor estimates turn out accurate in cases when the bridgesampling algorithm does not issue a warning message, but can be biased and liberal when a warning message is shown. These results support the use of brms/bridgesampling for null hypothesis Bayes factor tests in commonly used factorial designs. They also suggest that when a warning message is issued, Bayes factor results should not be trusted. The results show that it is practical to check whether Bayes factors are computed correctly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08022v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel J. Schad, Martin Modr\'ak, Shravan Vasishth</dc:creator>
    </item>
    <item>
      <title>Optimization perspective on raking</title>
      <link>https://arxiv.org/abs/2407.20520</link>
      <description>arXiv:2407.20520v3 Announce Type: replace 
Abstract: Raking is widely used in survey inference and global health models to adjust the observations in contingency tables to given marginals, in the latter case reconciling estimates between models with different granularities. We review the convex optimization foundation of raking and focus on a dual perspective that simplifies and streamlines prior raking extensions and provides new functionality, enabling a unified approach to n-dimensional raking, raking with differential weights, ensuring bounds on estimates are respected, raking to margins either as hard constraints or as aggregate observations, handling missing data, and allowing efficient uncertainty propagation. The dual perspective also enables a uniform fast and scalable matrix-free optimization approach for all of these extensions. All of the methods are implemented in an open source Python package with an intuitive user interface, installable from PyPi (https://pypi.org/project/raking/), and we illustrate the capabilities using synthetic data and real mortality estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20520v3</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ariane Ducellier (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Alexander Hsu (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA), Parkes Kendrick (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Bill Gustafson (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Laura Dwyer-Lindgren (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Christopher Murray (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Peng Zheng (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Aleksandr Aravkin (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA)</dc:creator>
    </item>
    <item>
      <title>Clarifying the Role of the Mantel-Haenszel Risk Difference Estimator in Randomized Clinical Trials</title>
      <link>https://arxiv.org/abs/2408.12541</link>
      <description>arXiv:2408.12541v2 Announce Type: replace 
Abstract: The Mantel-Haenszel (MH) risk difference estimator, commonly used in randomized clinical trials for binary outcomes, calculates a weighted average of stratum-specific risk difference estimators. Traditionally, this method requires the stringent assumption that risk differences are homogeneous across strata, also known as the common (constant) risk difference assumption. In our article, we relax this assumption and adopt a modern perspective, viewing the MH risk difference estimator as an approach for covariate adjustment in randomized clinical trials, distinguishing its use from that in meta-analysis and observational studies. We demonstrate that, under reasonable restrictions on risk difference variability, the MH risk difference estimator consistently estimates the average treatment effect within a standard super-population framework, which is often the primary interest in randomized clinical trials, in addition to estimating a weighted average of stratum-specific risk differences. We rigorously study its properties under the large-stratum and sparse-stratum asymptotic regimes, as well as under mixed-regime settings. Furthermore, for either estimand, we propose a unified robust variance estimator that improves over the popular variance estimators by Greenland and Robins (1985) and Sato et al. (1989) and has provable consistency across these asymptotic regimes, regardless of assuming common risk differences. Extensions of our theoretical results also provide new insights into the Mantel-Haenszel test, the post-stratification estimator, and settings with multiple treatments. Our findings are thoroughly validated through simulations and a clinical trial example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12541v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Qiu, Yuhan Qian, Jaehwan Yi, Jinqiu Wang, Yu Du, Yanyao Yi, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Inference on Dynamic Spatial Autoregressive Models with Change Point Detection</title>
      <link>https://arxiv.org/abs/2411.18773</link>
      <description>arXiv:2411.18773v3 Announce Type: replace 
Abstract: We analyze a varying-coefficient dynamic spatial autoregressive model with spatial fixed effects. One salient feature of the model is the incorporation of multiple spatial weight matrices through their linear combinations with varying coefficients, which help solve the problem of choosing the most ``correct'' one for applied econometricians who often face the availability of multiple expert spatial weight matrices. We estimate and make inferences on the model coefficients and coefficients in basis expansions of the varying coefficients through penalized estimations, establishing the oracle properties of the estimators and the consistency of the overall estimated spatial weight matrix, which can be time-dependent. We further consider two applications of our model in change point detections in dynamic spatial autoregressive models, providing theoretical justifications in consistent change point locations estimation and practical implementations. Simulation experiments demonstrate the performance of our proposed methodology, and real data analyses are also carried out.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18773v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen, Yudong Chen, Clifford Lam</dc:creator>
    </item>
    <item>
      <title>A Note on the Identifiability of the Degree-Corrected Stochastic Block Model</title>
      <link>https://arxiv.org/abs/2412.03833</link>
      <description>arXiv:2412.03833v3 Announce Type: replace 
Abstract: In this short note, we address the identifiability issues inherent in the Degree-Corrected Stochastic Block Model (DCSBM). We provide a rigorous proof demonstrating that the parameters of the DCSBM are identifiable up to a scaling factor and a permutation of the community labels, under a mild condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03833v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Park, Yunpeng Zhao, Ning Hao</dc:creator>
    </item>
    <item>
      <title>The Poisson tensor completion non-parametric differential entropy estimator</title>
      <link>https://arxiv.org/abs/2505.04957</link>
      <description>arXiv:2505.04957v2 Announce Type: replace-cross 
Abstract: We introduce the Poisson tensor completion (PTC) estimator, a non-parametric differential entropy estimator. The PTC estimator leverages inter-sample relationships to compute a low-rank Poisson tensor decomposition of the frequency histogram. Our crucial observation is that the histogram bins are an instance of a space partitioning of counts and thus can be identified with a spatial Poisson process. The Poisson tensor decomposition leads to a completion of the intensity measure over all bins -- including those containing few to no samples -- and leads to our proposed PTC differential entropy estimator. A Poisson tensor decomposition models the underlying distribution of the count data and guarantees non-negative estimated values and so can be safely used directly in entropy estimation. We believe our estimator is the first tensor-based estimator that exploits the underlying spatial Poisson process related to the histogram explicitly when estimating the probability density with low-rank tensor decompositions or tensor completion. Furthermore, we demonstrate that our PTC estimator is a substantial improvement over standard histogram-based estimators for sub-Gaussian probability distributions because of the concentration of norm phenomenon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04957v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel M. Dunlavy, Richard B. Lehoucq, Carolyn D. Mayer, Arvind Prasadan</dc:creator>
    </item>
  </channel>
</rss>
