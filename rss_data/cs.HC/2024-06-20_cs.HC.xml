<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Jun 2024 04:00:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Reclaiming Power over AI: Equipping Queer Teens as AI Designers for HIV Prevention</title>
      <link>https://arxiv.org/abs/2406.13018</link>
      <description>arXiv:2406.13018v1 Announce Type: new 
Abstract: In this position paper, we explore the potential of generative AI (GenAI) tools in supporting HIV prevention initiatives among LGBTQ+ adolescents. GenAI offers opportunities to bridge information gaps and enhance healthcare access, yet it also risks exacerbating existing inequities through biased AI outputs reflecting heteronormative and cisnormative values. We advocate for the importance of queer adolescent-centered interventions, contend with the promise of GenAI tools while addressing concerns of bias, and position participatory frameworks for empowering queer youth in the design and development of AI tools. Viewing LGBTQ+ adolescents as designers, we propose a community-engaged approach to enable a group of queer teens with sexual health education expertise to design their own GenAI health tools. Through this collaborative effort, we put forward participatory ways to develop processes minimizing the potential iatrogenic harms of biased AI models, while harnessing AI benefits for LGBTQ+ teens. In this workshop, we offer specialized community-engaged knowledge in designing equitable AI tools to improve LGBTQ+ well-being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13018v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Liem, Andrew Berry, Kathryn Macapagal</dc:creator>
    </item>
    <item>
      <title>Investigating the Effect of Display Refresh Rate on First-Person Shooting Games</title>
      <link>https://arxiv.org/abs/2406.13027</link>
      <description>arXiv:2406.13027v1 Announce Type: new 
Abstract: For first-person shooting game players, display refresh rate is important for a smooth experience. Multiple studies have shown that a low display refresh rate will reduce gamers' experience and performance. However, the human eye's perception of refresh rate has an upper limit, which is usually less than what high-performance monitors, for which players pay much higher prices, provide. This study assesses whether a higher refresh rate always has a positive impact on players' performance, making it worthwhile for them to invest in high-performance monitors. A within-group experimental design study was conducted using a commercial first-person shooting game platform (N = 26) to investigate players' performance at display refresh rates of 30Hz, 60Hz, 120Hz, 144Hz, and 240Hz. Player performance was assessed based on score, accuracy, and self-ratings from the players. The results show that display refresh rate only significantly affects player performance at 30Hz.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13027v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoshen Qin, Zixian Zhu</dc:creator>
    </item>
    <item>
      <title>MagicItem: Dynamic Behavior Design of Virtual Objects with Large Language Models in a Consumer Metaverse Platform</title>
      <link>https://arxiv.org/abs/2406.13242</link>
      <description>arXiv:2406.13242v1 Announce Type: new 
Abstract: To create rich experiences in virtual reality (VR) environments, it is essential to define the behavior of virtual objects through programming. However, programming in 3D spaces requires a wide range of background knowledge and programming skills. Although Large Language Models (LLMs) have provided programming support, they are still primarily aimed at programmers. In metaverse platforms, where many users inhabit VR spaces, most users are unfamiliar with programming, making it difficult for them to modify the behavior of objects in the VR environment easily. Existing LLM-based script generation methods for VR spaces require multiple lengthy iterations to implement the desired behaviors and are difficult to integrate into the operation of metaverse platforms. To address this issue, we propose a tool that generates behaviors for objects in VR spaces from natural language within Cluster, a metaverse platform with a large user base. By integrating LLMs with the Cluster Script provided by this platform, we enable users with limited programming experience to define object behaviors within the platform freely. We have also integrated our tool into a commercial metaverse platform and are conducting online experiments with 63 general users of the platform. The experiments show that even users with no programming background can successfully generate behaviors for objects in VR spaces, resulting in a highly satisfying system. Our research contributes to democratizing VR content creation by enabling non-programmers to design dynamic behaviors for virtual objects in metaverse platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13242v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryutaro Kurai, Takefumi Hiraki, Yuichi Hiroi, Yutaro Hirao, Monica Perusquia-Hernandez, Hideaki Uchiyama, Kiyoshi Kiyokawa</dc:creator>
    </item>
    <item>
      <title>What's Next? Exploring Utilization, Challenges, and Future Directions of AI-Generated Image Tools in Graphic Design</title>
      <link>https://arxiv.org/abs/2406.13436</link>
      <description>arXiv:2406.13436v1 Announce Type: new 
Abstract: Recent advancements in artificial intelligence, such as computer vision and deep learning, have led to the emergence of numerous generative AI platforms, particularly for image generation. However, the application of AI-generated image tools in graphic design has not been extensively explored. This study conducted semi-structured interviews with seven designers of varying experience levels to understand their current usage, challenges, and future functional needs for AI-generated image tools in graphic design. As our findings suggest, AI tools serve as creative partners in design, enhancing human creativity, offering strategic insights, and fostering team collaboration and communication. The findings provide guiding recommendations for the future development of AI-generated image tools, aimed at helping engineers optimize these tools to better meet the needs of graphic designers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13436v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuying Tang, Mariana Ciancia, Zhigang Wang, Ze Gao</dc:creator>
    </item>
    <item>
      <title>System Immersion of a Driving Simulator Affects the Oscillatory Brain Activity</title>
      <link>https://arxiv.org/abs/2406.13570</link>
      <description>arXiv:2406.13570v1 Announce Type: new 
Abstract: The technological properties of a system delivering simulation experience are a crucial dimension of immersion. To create a sense of presence and reproduce drivers behaviour as realistically as possible, we need reliable driving simulators that allow drivers to become highly immersed. This study investigates the impact of a system immersion of a driving simulator on the drivers' brain activity while operating a conditionally automated vehicle. Nineteen participants drove approximately 40 minutes while their brain activity was recorded using electroencephalography (EEG). We found a significant effect of the system immersion in the occipital and parietal areas, primarily in the high-Beta bandwidth. No effect was found in the Theta, Alpha, and low-Beta bandwidths. These findings suggest that the system immersion might influence the drivers' physiological arousal, consequently influencing their cognitive and emotional processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13570v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.54941/ahfe1001825</arxiv:DOI>
      <dc:creator>Nikol Figalov\'a, J\"urgen Pichen, Lewis L. Chuang, Martin Baumann, Olga Pollatos</dc:creator>
    </item>
    <item>
      <title>On AI-Inspired UI-Design</title>
      <link>https://arxiv.org/abs/2406.13631</link>
      <description>arXiv:2406.13631v1 Announce Type: new 
Abstract: Graphical User Interface (or simply UI) is a primary mean of interaction between users and their device. In this paper, we discuss three major complementary approaches on how to use Artificial Intelligence (AI) to support app designers create better, more diverse, and creative UI of mobile apps. First, designers can prompt a Large Language Model (LLM) like GPT to directly generate and adjust one or multiple UIs. Second, a Vision-Language Model (VLM) enables designers to effectively search a large screenshot dataset, e.g. from apps published in app stores. The third approach is to train a Diffusion Model (DM) specifically designed to generate app UIs as inspirational images. We discuss how AI should be used, in general, to inspire and assist creative app design rather than automating it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13631v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, G\'erard Dray, Walid Maalej</dc:creator>
    </item>
    <item>
      <title>Which One Changes More? A Novel Radial Visualization for State Change Comparison</title>
      <link>https://arxiv.org/abs/2406.13721</link>
      <description>arXiv:2406.13721v1 Announce Type: new 
Abstract: It is common to compare state changes of multiple data items and identify which data items have changed more in various applications (e.g., annual GDP growth of different countries and daily increase of new COVID-19 cases in different regions). Grouped bar charts and slope graphs can visualize both state changes and their initial and final states of multiple data items, and are thus widely used for state change comparison. But they leverage implicit bar differences or line slopes to indicate state changes, which has been proven less effective for visual comparison. Both visualizations also suffer from visual scalability issues when an increasing number of data items need to be compared. This paper fills the research gap by proposing a novel radial visualization called Intercept Graph to facilitate visual comparison of multiple state changes. It consists of inner and outer axes, and leverages the lengths of line segments intercepted by the inner axis to explicitly encode the state changes. Users can interactively adjust the inner axis to filter large changes of their interest and magnify the difference of relatively-similar state changes, enhancing its visual scalability and comparison accuracy. We extensively evaluate the Intercept Graph in comparison with baseline methods through two usage scenarios, quantitative metric evaluations, and well-designed crowdsourcing user studies with 50 participants. Our results demonstrate the usefulness and effectiveness of the Intercept Graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13721v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaolun Ruan, Yong Wang, Qiang Guan</dc:creator>
    </item>
    <item>
      <title>Exploring the Optimal Time Window for Predicting Cognitive Load Using Physiological Sensor Data</title>
      <link>https://arxiv.org/abs/2406.13793</link>
      <description>arXiv:2406.13793v1 Announce Type: new 
Abstract: Learning analytics has begun to use physiological signals because these have been linked with learners' cognitive and affective states. These signals, when interpreted through machine learning techniques, offer a nuanced understanding of the temporal dynamics of student learning experiences and processes. However, there is a lack of clear guidance on the optimal time window to use for analyzing physiological signals within predictive models. We conducted an empirical investigation of different time windows (ranging from 60 to 210 seconds) when analysing multichannel physiological sensor data for predicting cognitive load. Our results demonstrate a preference for longer time windows, with optimal window length typically exceeding 90 seconds. These findings challenge the conventional focus on immediate physiological responses, suggesting that a broader temporal scope could provide a more comprehensive understanding of cognitive processes. In addition, the variation in which time windows best supported prediction across classifiers underscores the complexity of integrating physiological measures. Our findings provide new insights for developing educational technologies that more accurately reflect and respond to the dynamic nature of learner cognitive load in complex learning environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13793v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minghao Cai, Carrie Demmans Epp</dc:creator>
    </item>
    <item>
      <title>A Graph Model and a Layout Algorithm for Knitting Patterns</title>
      <link>https://arxiv.org/abs/2406.13800</link>
      <description>arXiv:2406.13800v1 Announce Type: new 
Abstract: Knitting, an ancient fiber art, creates a structured fabric consisting of loops or stitches. Publishing hand knitting patterns involves lengthy testing periods and numerous knitters. Modeling knitting patterns with graphs can help expedite error detection and pattern validation. In this paper, we describe how to model simple knitting patterns as planar graphs. We then design, implement, and evaluate a layout algorithm to visualize knitting patterns. Knitting patterns correspond to graphs with pre-specified edge lengths (e.g., uniform lengths, two lengths, etc.). This yields a natural graph layout optimization problem: realize a planar graph with pre-specified edge lengths, while ensuring there are no edge crossings. We quantitatively evaluate our algorithm using real knitting patterns of various sizes against three others; one created for knitting patterns, one that maintains planarity and optimizes edge lengths, and a popular force-directed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13800v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kathryn Gray, Brian Bell, Stephen Kobourov</dc:creator>
    </item>
    <item>
      <title>AltGeoViz: Facilitating Accessible Geovisualization</title>
      <link>https://arxiv.org/abs/2406.13853</link>
      <description>arXiv:2406.13853v1 Announce Type: new 
Abstract: Geovisualizations are powerful tools for exploratory spatial analysis, enabling sighted users to discern patterns, trends, and relationships within geographic data. However, these visual tools have remained largely inaccessible to screen-reader users. We present AltGeoViz, a new system we designed to facilitate geovisualization exploration for these users. AltGeoViz dynamically generates alt-text descriptions based on the user's current map view, providing summaries of spatial patterns and descriptive statistics. In a study of five screen-reader users, we found that AltGeoViz enabled them to interact with geovisualizations in previously infeasible ways. Participants demonstrated a clear understanding of data summaries and their location context, and they could synthesize spatial understandings of their explorations. Moreover, we identified key areas for improvement, such as the addition of intuitive spatial navigation controls and comparative analysis features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13853v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chu Li, Rock Yuren Pang, Ather Sharif, Arnavi Chheda-Kothary, Jeffrey Heer, Jon E. Froehlich</dc:creator>
    </item>
    <item>
      <title>We Are The Clouds: Blending Interaction and Participation in Urban Media Art</title>
      <link>https://arxiv.org/abs/2406.13883</link>
      <description>arXiv:2406.13883v1 Announce Type: new 
Abstract: Since the early 2000s, cultural institutions have been instrumental in reshaping public spaces, fostering community engagement, and nurturing artistic innovation. Central to these initiatives are audience interaction and participation concepts, yet their definitions and applications in urban media art remain nebulous. This article endeavours to demystify these terms, examining the distinct characteristics and intersections of interactive and participatory art within urban contexts. A particular emphasis is placed on artworks that harmonise both elements, exploring the motivations and outcomes of this synthesis. The case study of We Are The Clouds serves as a focal point, exemplifying how strategic integration of interaction and participation can enhance community connection and reinvigorate public spaces. Through this analysis, the paper underscores the transformative power of urban media artworks in redefining neighbourhood experiences, empowering local voices, and revitalising the essence of public realms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13883v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Varvara Guljajeva, Mar Canet Sola</dc:creator>
    </item>
    <item>
      <title>Are We There Yet? Unravelling Usability Challenges and Opportunities in Collaborative Immersive Analytics for Domain Experts</title>
      <link>https://arxiv.org/abs/2406.13918</link>
      <description>arXiv:2406.13918v1 Announce Type: new 
Abstract: In the ever-evolving discipline of high-dimensional scientific data, collaborative immersive analytics (CIA) offers a promising frontier for domain experts in complex data visualization and interpretation. This research presents a comprehensive framework for conducting usability studies on the extended reality (XR) interface of ParaView, an open-source CIA system. By employing established human-computer interaction (HCI) principles, including Jakob Nielsen's Usability Heuristics, Cognitive Load Theory, NASA Task Load Index, System Usability Scale, Affordance Theory, and Gulf of Execution and Evaluation, this study aims to identify underlying usability issues and provide guidelines for enhancing user experience in scientific domains. Our findings reveal significant usability challenges in the ParaView XR interface that impede effective teamwork and collaboration. For instance, the lack of synchronous collaboration, limited communication methods, and the absence of role-based data access are critical areas that need attention. Additionally, inadequate error handling, insufficient feedback mechanisms, and limited support resources during application use require extensive improvement to fully utilize the system's potential. Our study suggests potential improvements to overcome the existing usability barriers of the collaborative immersive system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13918v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fahim Arsad Nafis, Alexander Rose, Simon Su, Songqing Chen, Bo Han</dc:creator>
    </item>
    <item>
      <title>Comparing the Effects of Visual, Haptic, and Visuohaptic Encoding on Memory Retention of Digital Objects in Virtual Reality</title>
      <link>https://arxiv.org/abs/2406.14139</link>
      <description>arXiv:2406.14139v1 Announce Type: new 
Abstract: Although Virtual Reality (VR) has undoubtedly improved human interaction with 3D data, users still face difficulties retaining important details of complex digital objects in preparation for physical tasks. To address this issue, we evaluated the potential of visuohaptic integration to improve the memorability of virtual objects in immersive visualizations. In a user study (N=20), participants performed a delayed match-to-sample task where they memorized stimuli of visual, haptic, or visuohaptic encoding conditions. We assessed performance differences between the conditions through error rates and response time. We found that visuohaptic encoding significantly improved memorization accuracy compared to unimodal visual and haptic conditions. Our analysis indicates that integrating haptics into immersive visualizations enhances the memorability of digital objects. We discuss its implications for the optimal encoding design in VR applications that assist professionals who need to memorize and recall virtual objects in their daily work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14139v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Siqueira Rodrigues, Timo Torsten Schmidt, John Nyakatura, Stefan Zachow, Johann Habakuk Israel, Thomas Kosch</dc:creator>
    </item>
    <item>
      <title>Discrete Virtual Rotation in Pointing vs. Leaning-Directed Steering Interfaces: A Uni vs. Bimanual Perspective</title>
      <link>https://arxiv.org/abs/2406.14212</link>
      <description>arXiv:2406.14212v1 Announce Type: new 
Abstract: In this work, we explore the integration of discontinuous Orientation Selection into steering interfaces intending to preserve the seamless sensation of real-world movement, while mitigating the risk of inducing cybersickness. Our implementation encounters conflicts in standard input mappings, prompting us to adopt bimanual interaction as a solution. Recognizing the complexity that may arise from this step, we also develop unimanual alternatives, e.g., utilizing a Human-Joystick, commonly referred to as Leaning interface. The outcomes of an empirical study centered around a primed search task yield unexpected findings. We observed a sample of users spanning multiple levels of gaming experience and a balanced gender distribution exhibit no significant difficulties with the bimanual, asymmetric interfaces. Remarkably, the performance of Orientation Selection is, as in prior work, at least on par with Snap Rotation. Moreover, through a subsequent exploratory analysis, we uncover indications that Pointing-Directed Steering outperforms embodied interfaces in usability and task load in the given setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14212v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Zielasko, Maximilian Sp\"ath, Matthias W\"olwer</dc:creator>
    </item>
    <item>
      <title>Jupyter Scatter: Interactive Exploration of Large-Scale Datasets</title>
      <link>https://arxiv.org/abs/2406.14397</link>
      <description>arXiv:2406.14397v1 Announce Type: new 
Abstract: Jupyter Scatter is a scalable, interactive, and interlinked scatterplot widget for exploring datasets in Jupyter Notebook/Lab, Colab, and VS Code. Its goal is to simplify the visual exploration, analysis, and comparison of large-scale bivariate datasets. Jupyter Scatter can render up to twenty million points, supports fast point selections, integrates with Pandas DataFrame and Matplotlib, uses perceptually-effective default settings, and offers a user-friendly API.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14397v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fritz Lekschas Trevor Manz</dc:creator>
    </item>
    <item>
      <title>Science in a Blink: Supporting Ensemble Perception in Scalar Fields</title>
      <link>https://arxiv.org/abs/2406.14452</link>
      <description>arXiv:2406.14452v1 Announce Type: new 
Abstract: Visualizations support rapid analysis of scientific datasets, allowing viewers to glean aggregate information (e.g., the mean) within split-seconds. While prior research has explored this ability in conventional charts, it is unclear if spatial visualizations used by computational scientists afford a similar ensemble perception capacity. We investigate people's ability to estimate two summary statistics, mean and variance, from pseudocolor scalar fields. In a crowdsourced experiment, we find that participants can reliably characterize both statistics, although variance discrimination requires a much stronger signal. Multi-hue and diverging colormaps outperformed monochromatic, luminance ramps in aiding this extraction. Analysis of qualitative responses suggests that participants often estimate the distribution of hotspots and valleys as visual proxies for data statistics. These findings suggest that people's summary interpretation of spatial datasets is likely driven by the appearance of discrete color segments, rather than assessments of overall luminance. Implicit color segmentation in quantitative displays could thus prove more useful than previously assumed by facilitating quick, gist-level judgments about color-coded visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14452v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor A. Mateevitsi, Michael E. Papka, Khairi Reda</dc:creator>
    </item>
    <item>
      <title>Current state of LLM Risks and AI Guardrails</title>
      <link>https://arxiv.org/abs/2406.12934</link>
      <description>arXiv:2406.12934v1 Announce Type: cross 
Abstract: Large language models (LLMs) have become increasingly sophisticated, leading to widespread deployment in sensitive applications where safety and reliability are paramount. However, LLMs have inherent risks accompanying them, including bias, potential for unsafe actions, dataset poisoning, lack of explainability, hallucinations, and non-reproducibility. These risks necessitate the development of "guardrails" to align LLMs with desired behaviors and mitigate potential harm.
  This work explores the risks associated with deploying LLMs and evaluates current approaches to implementing guardrails and model alignment techniques. We examine intrinsic and extrinsic bias evaluation methods and discuss the importance of fairness metrics for responsible AI development. The safety and reliability of agentic LLMs (those capable of real-world actions) are explored, emphasizing the need for testability, fail-safes, and situational awareness.
  Technical strategies for securing LLMs are presented, including a layered protection model operating at external, secondary, and internal levels. System prompts, Retrieval-Augmented Generation (RAG) architectures, and techniques to minimize bias and protect privacy are highlighted.
  Effective guardrail design requires a deep understanding of the LLM's intended use case, relevant regulations, and ethical considerations. Striking a balance between competing requirements, such as accuracy and privacy, remains an ongoing challenge. This work underscores the importance of continuous research and development to ensure the safe and responsible use of LLMs in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12934v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suriya Ganesh Ayyamperumal, Limin Ge</dc:creator>
    </item>
    <item>
      <title>Pattern or Artifact? Interactively Exploring Embedding Quality with TRACE</title>
      <link>https://arxiv.org/abs/2406.12953</link>
      <description>arXiv:2406.12953v1 Announce Type: cross 
Abstract: This paper presents TRACE, a tool to analyze the quality of 2D embeddings generated through dimensionality reduction techniques. Dimensionality reduction methods often prioritize preserving either local neighborhoods or global distances, but insights from visual structures can be misleading if the objective has not been achieved uniformly. TRACE addresses this challenge by providing a scalable and extensible pipeline for computing both local and global quality measures. The interactive browser-based interface allows users to explore various embeddings while visually assessing the pointwise embedding quality. The interface also facilitates in-depth analysis by highlighting high-dimensional nearest neighbors for any group of points and displaying high-dimensional distances between points. TRACE enables analysts to make informed decisions regarding the most suitable dimensionality reduction method for their specific use case, by showing the degree and location where structure is preserved in the reduced space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12953v1</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edith Heiter, Liesbet Martens, Ruth Seurinck, Martin Guilliams, Tijl De Bie, Yvan Saeys, Jefrey Lijffijt</dc:creator>
    </item>
    <item>
      <title>As Advertised? Understanding the Impact of Influencer VPN Ads</title>
      <link>https://arxiv.org/abs/2406.13017</link>
      <description>arXiv:2406.13017v1 Announce Type: cross 
Abstract: Influencer VPN ads (sponsored segments) on YouTube often disseminate misleading information about both VPNs, and security &amp; privacy more broadly. However, it remains unclear how (or whether) these ads affect users' perceptions and knowledge about VPNs. In this work, we explore the relationship between YouTube VPN ad exposure and users' mental models of VPNs, security, and privacy. We use a novel VPN ad detection model to calculate the ad exposure of 217 participants via their YouTube watch histories, and we develop scales to characterize their mental models in relation to claims commonly made in VPN ads. Through (pre-registered) regression-based analysis, we find that exposure to VPN ads is significantly correlated with familiarity with VPN brands and increased belief in (hyperbolic) threats. While not specific to VPNs, these threats are often discussed in VPN ads. In contrast, although many participants agree with both factual and misleading mental models of VPNs that often appear in ads, we find no significant correlation between exposure to VPN ads and these mental models. These findings suggest that, if VPN ads do impact mental models, then it is predominantly emotional (i.e., threat perceptions) rather than technical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13017v1</guid>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Omer Akgul, Richard Roberts, Emma Shroyer, Dave Levin, Michelle L. Mazurek</dc:creator>
    </item>
    <item>
      <title>RITA: A Real-time Interactive Talking Avatars Framework</title>
      <link>https://arxiv.org/abs/2406.13093</link>
      <description>arXiv:2406.13093v1 Announce Type: cross 
Abstract: RITA presents a high-quality real-time interactive framework built upon generative models, designed with practical applications in mind. Our framework enables the transformation of user-uploaded photos into digital avatars that can engage in real-time dialogue interactions. By leveraging the latest advancements in generative modeling, we have developed a versatile platform that not only enhances the user experience through dynamic conversational avatars but also opens new avenues for applications in virtual reality, online education, and interactive gaming. This work showcases the potential of integrating computer vision and natural language processing technologies to create immersive and interactive digital personas, pushing the boundaries of how we interact with digital content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13093v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wuxinlin Cheng, Cheng Wan, Yupeng Cao, Sihan Chen</dc:creator>
    </item>
    <item>
      <title>Investigating Low-Cost LLM Annotation for~Spoken Dialogue Understanding Datasets</title>
      <link>https://arxiv.org/abs/2406.13269</link>
      <description>arXiv:2406.13269v1 Announce Type: cross 
Abstract: In spoken Task-Oriented Dialogue (TOD) systems, the choice of the semantic representation describing the users' requests is key to a smooth interaction. Indeed, the system uses this representation to reason over a database and its domain knowledge to choose its next action. The dialogue course thus depends on the information provided by this semantic representation. While textual datasets provide fine-grained semantic representations, spoken dialogue datasets fall behind. This paper provides insights into automatic enhancement of spoken dialogue datasets' semantic representations. Our contributions are three fold: (1) assess the relevance of Large Language Model fine-tuning, (2) evaluate the knowledge captured by the produced annotations and (3) highlight semi-automatic annotation implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13269v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>27th International Conference on Text, Speech and Dialogue, Sep 2024, Brno (R{\'e}p. Tch{\`e}que), Czech Republic</arxiv:journal_reference>
      <dc:creator>Lucas Druart (LIA), Valentin Vielzeuf (LIA), Yannick Est\`eve (LIA)</dc:creator>
    </item>
    <item>
      <title>Standardness Fogs Meaning: A Position Regarding the Informed Usage of Standard Datasets</title>
      <link>https://arxiv.org/abs/2406.13552</link>
      <description>arXiv:2406.13552v1 Announce Type: cross 
Abstract: Standard datasets are frequently used to train and evaluate Machine Learning models. However, the assumed standardness of these datasets leads to a lack of in-depth discussion on how their labels match the derived categories for the respective use case. In other words, the standardness of the datasets seems to fog coherency and applicability, thus impeding the trust in Machine Learning models. We propose to adopt Grounded Theory and Hypotheses Testing through Visualization as methods to evaluate the match between use case, derived categories, and labels of standard datasets. To showcase the approach, we apply it to the 20 Newsgroups dataset and the MNIST dataset. For the 20 Newsgroups dataset, we demonstrate that the labels are imprecise. Therefore, we argue that neither a Machine Learning model can learn a meaningful abstraction of derived categories nor one can draw conclusions from achieving high accuracy. For the MNIST dataset, we demonstrate how the labels can be confirmed to be defined well. We conclude that a concept of standardness of a dataset implies that there is a match between use case, derived categories, and class labels, as in the case of the MNIST dataset. We argue that this is necessary to learn a meaningful abstraction and, thus, improve trust in the Machine Learning model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13552v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tim Cech, Ole Wegen, Daniel Atzberger, Rico Richter, Willy Scheibel, J\"urgen D\"ollner</dc:creator>
    </item>
    <item>
      <title>Imagining In-distribution States: How Predictable Robot Behavior Can Enable User Control Over Learned Policies</title>
      <link>https://arxiv.org/abs/2406.13711</link>
      <description>arXiv:2406.13711v1 Announce Type: cross 
Abstract: It is crucial that users are empowered to take advantage of the functionality of a robot and use their understanding of that functionality to perform novel and creative tasks. Given a robot trained with Reinforcement Learning (RL), a user may wish to leverage that autonomy along with their familiarity of how they expect the robot to behave to collaborate with the robot. One technique is for the user to take control of some of the robot's action space through teleoperation, allowing the RL policy to simultaneously control the rest. We formalize this type of shared control as Partitioned Control (PC). However, this may not be possible using an out-of-the-box RL policy. For example, a user's control may bring the robot into a failure state from the policy's perspective, causing it to act unexpectedly and hindering the success of the user's desired task. In this work, we formalize this problem and present Imaginary Out-of-Distribution Actions, IODA, an initial algorithm which empowers users to leverage their expectations of a robot's behavior to accomplish new tasks. We deploy IODA in a user study with a real robot and find that IODA leads to both better task performance and a higher degree of alignment between robot behavior and user expectation. We also show that in PC, there is a strong and significant correlation between task performance and the robot's ability to meet user expectations, highlighting the need for approaches like IODA. Code is available at https://github.com/AABL-Lab/ioda_roman_2024</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13711v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Isaac Sheidlower, Emma Bethel, Douglas Lilly, Reuben M. Aronson, Elaine Schaertl Short</dc:creator>
    </item>
    <item>
      <title>The Efficacy of Conversational Artificial Intelligence in Rectifying the Theory of Mind and Autonomy Biases: Comparative Analysis</title>
      <link>https://arxiv.org/abs/2406.13813</link>
      <description>arXiv:2406.13813v1 Announce Type: cross 
Abstract: The study evaluates the efficacy of Conversational Artificial Intelligence (CAI) in rectifying cognitive biases and recognizing affect in human-AI interactions, which is crucial for digital mental health interventions. Cognitive biases (systematic deviations from normative thinking) affect mental health, intensifying conditions like depression and anxiety. Therapeutic chatbots can make cognitive-behavioral therapy (CBT) more accessible and affordable, offering scalable and immediate support. The research employs a structured methodology with clinical-based virtual case scenarios simulating typical user-bot interactions. Performance and affect recognition were assessed across two categories of cognitive biases: theory of mind biases (anthropomorphization of AI, overtrust in AI, attribution to AI) and autonomy biases (illusion of control, fundamental attribution error, just-world hypothesis). A qualitative feedback mechanism was used with an ordinal scale to quantify responses based on accuracy, therapeutic quality, and adherence to CBT principles. Therapeutic bots (Wysa, Youper) and general-use LLMs (GTP 3.5, GTP 4, Gemini Pro) were evaluated through scripted interactions, double-reviewed by cognitive scientists and a clinical psychologist. Statistical analysis showed therapeutic bots were consistently outperformed by non-therapeutic bots in bias rectification and in 4 out of 6 biases in affect recognition. The data suggests that non-therapeutic chatbots are more effective in addressing some cognitive biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13813v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marcin Rz\k{a}deczka, Anna Sterna, Julia Stoli\'nska, Paulina Kaczy\'nska, Marcin Moskalewicz</dc:creator>
    </item>
    <item>
      <title>Enhancing the LLM-Based Robot Manipulation Through Human-Robot Collaboration</title>
      <link>https://arxiv.org/abs/2406.14097</link>
      <description>arXiv:2406.14097v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are gaining popularity in the field of robotics. However, LLM-based robots are limited to simple, repetitive motions due to the poor integration between language models, robots, and the environment. This paper proposes a novel approach to enhance the performance of LLM-based autonomous manipulation through Human-Robot Collaboration (HRC). The approach involves using a prompted GPT-4 language model to decompose high-level language commands into sequences of motions that can be executed by the robot. The system also employs a YOLO-based perception algorithm, providing visual cues to the LLM, which aids in planning feasible motions within the specific environment. Additionally, an HRC method is proposed by combining teleoperation and Dynamic Movement Primitives (DMP), allowing the LLM-based robot to learn from human guidance. Real-world experiments have been conducted using the Toyota Human Support Robot for manipulation tasks. The outcomes indicate that tasks requiring complex trajectory planning and reasoning over environments can be efficiently accomplished through the incorporation of human demonstrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14097v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/LRA.2024.3415931</arxiv:DOI>
      <dc:creator>Haokun Liu, Yaonan Zhu, Kenji Kato, Atsushi Tsukahara, Izumi Kondo, Tadayoshi Aoyama, Yasuhisa Hasegawa</dc:creator>
    </item>
    <item>
      <title>Watching the Watchers: A Comparative Fairness Audit of Cloud-based Content Moderation Services</title>
      <link>https://arxiv.org/abs/2406.14154</link>
      <description>arXiv:2406.14154v1 Announce Type: cross 
Abstract: Online platforms face the challenge of moderating an ever-increasing volume of content, including harmful hate speech. In the absence of clear legal definitions and a lack of transparency regarding the role of algorithms in shaping decisions on content moderation, there is a critical need for external accountability. Our study contributes to filling this gap by systematically evaluating four leading cloud-based content moderation services through a third-party audit, highlighting issues such as biases against minorities and vulnerable groups that may arise through over-reliance on these services. Using a black-box audit approach and four benchmark data sets, we measure performance in explicit and implicit hate speech detection as well as counterfactual fairness through perturbation sensitivity analysis and present disparities in performance for certain target identity groups and data sets. Our analysis reveals that all services had difficulties detecting implicit hate speech, which relies on more subtle and codified messages. Moreover, our results point to the need to remove group-specific bias. It seems that biases towards some groups, such as Women, have been mostly rectified, while biases towards other groups, such as LGBTQ+ and PoC remain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14154v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Hartmann, Amin Oueslati, Dimitri Staufer</dc:creator>
    </item>
    <item>
      <title>Zero field active shielding</title>
      <link>https://arxiv.org/abs/2406.14234</link>
      <description>arXiv:2406.14234v1 Announce Type: cross 
Abstract: Ambient field suppression is critical for accurate magnetic field measurements, and a requirement for certain low-field sensors to operate. The difference in magnitude between noise and signal (up to 10$^9$) makes the problem challenging, and solutions such as passive shielding, post-hoc processing, and most active shielding designs do not address it completely. Zero field active shielding (ZFS) achieves accurate field suppression with a feed-forward structure in which correction coils are fed by reference sensors via a matrix found using data-driven methods. Requirements are a sufficient number of correction coils and reference sensors to span the ambient field at the sensors, and to zero out the coil-to-reference sensor coupling. The solution assumes instantaneous propagation and mixing, but it can be extended to handle convolutional effects. Precise calculations based on sensor and coil geometries are not necessary, other than to improve efficiency and usability. The solution is simulated here but not implemented in hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14234v1</guid>
      <category>physics.med-ph</category>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <category>physics.ins-det</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain de Cheveign\'e</dc:creator>
    </item>
    <item>
      <title>E-ANT: A Large-Scale Dataset for Efficient Automatic GUI NavigaTion</title>
      <link>https://arxiv.org/abs/2406.14250</link>
      <description>arXiv:2406.14250v1 Announce Type: cross 
Abstract: Online GUI navigation on mobile devices has driven a lot of attention recent years since it contributes to many real-world applications. With the rapid development of large language models (LLM), multimodal large language models (MLLM) have tremendous potential on this task. However, existing MLLMs need high quality data to improve its abilities of making the correct navigation decisions according to the human user inputs. In this paper, we developed a novel and highly valuable dataset, named \textbf{E-ANT}, as the first Chinese GUI navigation dataset that contains real human behaviour and high quality screenshots with annotations, containing nearly 40,000 real human traces over 5000+ different tinyAPPs. Furthermore, we evaluate various powerful MLLMs on E-ANT and show their experiments results with sufficient ablations. We believe that our proposed dataset will be beneficial for both the evaluation and development of GUI navigation and LLM/MLLM decision-making capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14250v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Wang, Tianyu Xia, Zhangxuan Gu, Yi Zhao, Shuheng Shen, Changhua Meng, Weiqiang Wang, Ke Xu</dc:creator>
    </item>
    <item>
      <title>Intelligent Interface: Enhancing Lecture Engagement with Didactic Activity Summaries</title>
      <link>https://arxiv.org/abs/2406.14266</link>
      <description>arXiv:2406.14266v1 Announce Type: cross 
Abstract: Recently, multiple applications of machine learning have been introduced. They include various possibilities arising when image analysis methods are applied to, broadly understood, video streams. In this context, a novel tool, developed for academic educators to enhance the teaching process by automating, summarizing, and offering prompt feedback on conducting lectures, has been developed. The implemented prototype utilizes machine learning-based techniques to recognise selected didactic and behavioural teachers' features within lecture video recordings.
  Specifically, users (teachers) can upload their lecture videos, which are preprocessed and analysed using machine learning models. Next, users can view summaries of recognized didactic features through interactive charts and tables. Additionally, stored ML-based prediction results support comparisons between lectures based on their didactic content. In the developed application text-based models trained on lecture transcriptions, with enhancements to the transcription quality, by adopting an automatic speech recognition solution are applied. Furthermore, the system offers flexibility for (future) integration of new/additional machine-learning models and software modules for image and video analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14266v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Wr\'oblewska, Marcel Witas, Kinga Fra\'nczak, Arkadiusz Knia\'z, Siew Ann Cheong, Tan Seng Chee, Janusz Ho{\l}yst, Marcin Paprzycki</dc:creator>
    </item>
    <item>
      <title>Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory</title>
      <link>https://arxiv.org/abs/2406.14373</link>
      <description>arXiv:2406.14373v1 Announce Type: cross 
Abstract: The emergence of Large Language Models (LLMs) and advancements in Artificial Intelligence (AI) offer an opportunity for computational social science research at scale. Building upon prior explorations of LLM agent design, our work introduces a simulated agent society where complex social relationships dynamically form and evolve over time. Agents are imbued with psychological drives and placed in a sandbox survival environment. We conduct an evaluation of the agent society through the lens of Thomas Hobbes's seminal Social Contract Theory (SCT). We analyze whether, as the theory postulates, agents seek to escape a brutish "state of nature" by surrendering rights to an absolute sovereign in exchange for order and security. Our experiments unveil an alignment: Initially, agents engage in unrestrained conflict, mirroring Hobbes's depiction of the state of nature. However, as the simulation progresses, social contracts emerge, leading to the authorization of an absolute sovereign and the establishment of a peaceful commonwealth founded on mutual cooperation. This congruence between our LLM agent society's evolutionary trajectory and Hobbes's theoretical account indicates LLMs' capability to model intricate social dynamics and potentially replicate forces that shape human societies. By enabling such insights into group behavior and emergent societal phenomena, LLM-driven multi-agent simulations, while unable to simulate all the nuances of human behavior, may hold potential for advancing our understanding of social structures, group dynamics, and complex human systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14373v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gordon Dai, Weijia Zhang, Jinhan Li, Siqi Yang, Chidera Onochie lbe, Srihas Rao, Arthur Caetano, Misha Sra</dc:creator>
    </item>
    <item>
      <title>Proceedings of The second international workshop on eXplainable AI for the Arts (XAIxArts)</title>
      <link>https://arxiv.org/abs/2406.14485</link>
      <description>arXiv:2406.14485v1 Announce Type: cross 
Abstract: This second international workshop on explainable AI for the Arts (XAIxArts) brought together a community of researchers in HCI, Interaction Design, AI, explainable AI (XAI), and digital arts to explore the role of XAI for the Arts. Workshop held at the 16th ACM Conference on Creativity and Cognition (C&amp;C 2024), Chicago, USA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14485v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Bryan-Kinns, Corey Ford, Shuoyang Zheng, Helen Kennedy, Alan Chamberlain, Makayla Lewis, Drew Hemment, Zijin Li, Qiong Wu, Lanxi Xiao, Gus Xia, Jeba Rezwana, Michael Clemens, Gabriel Vigliensoni</dc:creator>
    </item>
    <item>
      <title>Evidence of a log scaling law for political persuasion with large language models</title>
      <link>https://arxiv.org/abs/2406.14508</link>
      <description>arXiv:2406.14508v1 Announce Type: cross 
Abstract: Large language models can now generate political messages as persuasive as those written by humans, raising concerns about how far this persuasiveness may continue to increase with model size. Here, we generate 720 persuasive messages on 10 U.S. political issues from 24 language models spanning several orders of magnitude in size. We then deploy these messages in a large-scale randomized survey experiment (N = 25,982) to estimate the persuasive capability of each model. Our findings are twofold. First, we find evidence of a log scaling law: model persuasiveness is characterized by sharply diminishing returns, such that current frontier models are barely more persuasive than models smaller in size by an order of magnitude or more. Second, mere task completion (coherence, staying on topic) appears to account for larger models' persuasive advantage. These findings suggest that further scaling model size will not much increase the persuasiveness of static LLM-generated messages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14508v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kobi Hackenburg, Ben M. Tappin, Paul R\"ottger, Scott Hale, Jonathan Bright, Helen Margetts</dc:creator>
    </item>
    <item>
      <title>XAI in Automated Fact-Checking? The Benefits Are Modest and There's No One-Explanation-Fits-All</title>
      <link>https://arxiv.org/abs/2308.03372</link>
      <description>arXiv:2308.03372v2 Announce Type: replace 
Abstract: The massive volume of online information along with the issue of misinformation has spurred active research in the automation of fact-checking. Like fact-checking by human experts, it is not enough for an automated fact-checker to just be accurate, but also be able to inform and convince the user of the validity of its predictions. This becomes viable with explainable artificial intelligence (XAI). In this work, we conduct a study of XAI fact-checkers involving 180 participants to determine how users' actions towards news and their attitudes towards explanations are affected by the XAI. Our results suggest that XAI has limited effects on users' agreement with the veracity prediction of the automated fact-checker and on their intent to share news. However, XAI nudges users towards forming uniform judgments of news veracity, thereby signaling their reliance on the explanations. We also found polarizing preferences towards XAI and raise several design considerations on them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.03372v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3638380.3638388</arxiv:DOI>
      <arxiv:journal_reference>OzCHI 2023</arxiv:journal_reference>
      <dc:creator>Gionnieve Lim, Simon T. Perrault</dc:creator>
    </item>
    <item>
      <title>Understanding Users' Dissatisfaction with ChatGPT Responses: Types, Resolving Tactics, and the Effect of Knowledge Level</title>
      <link>https://arxiv.org/abs/2311.07434</link>
      <description>arXiv:2311.07434v3 Announce Type: replace 
Abstract: Large language models (LLMs) with chat-based capabilities, such as ChatGPT, are widely used in various workflows. However, due to a limited understanding of these large-scale models, users struggle to use this technology and experience different kinds of dissatisfaction. Researchers have introduced several methods, such as prompt engineering, to improve model responses. However, they focus on enhancing the model's performance in specific tasks, and little has been investigated on how to deal with the user dissatisfaction resulting from the model's responses. Therefore, with ChatGPT as the case study, we examine users' dissatisfaction along with their strategies to address the dissatisfaction. After organizing users' dissatisfaction with LLM into seven categories based on a literature review, we collected 511 instances of dissatisfactory ChatGPT responses from 107 users and their detailed recollections of dissatisfactory experiences, which we released as a publicly accessible dataset. Our analysis reveals that users most frequently experience dissatisfaction when ChatGPT fails to grasp their intentions, while they rate the severity of dissatisfaction related to accuracy the highest. We also identified four tactics users employ to address their dissatisfaction and their effectiveness. We found that users often do not use any tactics to address their dissatisfaction, and even when using tactics, 72% of dissatisfaction remained unresolved. Moreover, we found that users with low knowledge of LLMs tend to face more dissatisfaction on accuracy while they often put minimal effort in addressing dissatisfaction. Based on these findings, we propose design implications for minimizing user dissatisfaction and enhancing the usability of chat-based LLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07434v3</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3640543.3645148</arxiv:DOI>
      <dc:creator>Yoonsu Kim, Jueon Lee, Seoyoung Kim, Jaehyuk Park, Juho Kim</dc:creator>
    </item>
    <item>
      <title>Embedding Large Language Models into Extended Reality: Opportunities and Challenges for Inclusion, Engagement, and Privacy</title>
      <link>https://arxiv.org/abs/2402.03907</link>
      <description>arXiv:2402.03907v2 Announce Type: replace 
Abstract: Advances in artificial intelligence and human-computer interaction will likely lead to extended reality (XR) becoming pervasive. While XR can provide users with interactive, engaging, and immersive experiences, non-player characters are often utilized in pre-scripted and conventional ways. This paper argues for using large language models (LLMs) in XR by embedding them in avatars or as narratives to facilitate inclusion through prompt engineering and fine-tuning the LLMs. We argue that this inclusion will promote diversity for XR use. Furthermore, the versatile conversational capabilities of LLMs will likely increase engagement in XR, helping XR become ubiquitous. Lastly, we speculate that combining the information provided to LLM-powered spaces by users and the biometric data obtained might lead to novel privacy invasions. While exploring potential privacy breaches, examining user privacy concerns and preferences is also essential. Therefore, despite challenges, LLM-powered XR is a promising area with several opportunities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03907v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3640794.3665563</arxiv:DOI>
      <dc:creator>Efe Bozkir, S\"uleyman \"Ozdel, Ka Hei Carrie Lau, Mengdi Wang, Hong Gao, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>Scrolly2Reel: Retargeting Graphics for Social Media Using Narrative Beats</title>
      <link>https://arxiv.org/abs/2403.18111</link>
      <description>arXiv:2403.18111v2 Announce Type: replace 
Abstract: Content retargeting is crucial for social media creators. Once great content is created, it is important to reach as broad an audience as possible. This is particularly important in journalism where younger audiences are shifting away from print and towards short-video platforms. Many newspapers already create rich graphics for the web that they want to be able to reuse for social media. One example is scrollytelling sequences or "scrollies" -- immersive articles with graphics like animation, charts, and 3D visualizations that appear as a user scrolls. We present a system that helps transform scrollies into social media videos. By using the scriptwriting concept of narrative beats to extract fundamental storytelling units, we can create videos that are more aligned with narration, and allow for better pacing and stylistic changes. Narrative beats are thus an important primitive to retargeting content that matches the style of a new medium while maintaining the cohesiveness of the original content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18111v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Duy K. Nguyen, Jenny Ma, Pedro Alejandro Perez, Lydia B. Chilton</dc:creator>
    </item>
    <item>
      <title>EEG-DBNet: A Dual-Branch Network for Temporal-Spectral Decoding in Motor-Imagery Brain-Computer Interfaces</title>
      <link>https://arxiv.org/abs/2405.16090</link>
      <description>arXiv:2405.16090v3 Announce Type: replace 
Abstract: Motor imagery electroencephalogram (EEG)-based brain-computer interfaces (BCIs) offer significant advantages for individuals with restricted limb mobility. However, challenges such as low signal-to-noise ratio and limited spatial resolution impede accurate feature extraction from EEG signals, thereby affecting the classification accuracy of different actions. To address these challenges, this study proposes an end-to-end dual-branch network (EEG-DBNet) that decodes the temporal and spectral sequences of EEG signals in parallel through two distinct network branches. Each branch comprises a local convolutional block and a global convolutional block. The local convolutional block transforms the source signal from the temporal-spatial domain to the temporal-spectral domain. By varying the number of filters and convolution kernel sizes, the local convolutional blocks in different branches adjust the length of their respective dimension sequences. Different types of pooling layers are then employed to emphasize the features of various dimension sequences, setting the stage for subsequent global feature extraction. The global convolution block splits and reconstructs the feature of the signal sequence processed by the local convolution block in the same branch and further extracts features through the dilated causal convolutional neural networks. Finally, the outputs from the two branches are concatenated, and signal classification is completed via a fully connected layer. Our proposed method achieves classification accuracies of 85.84% and 91.60% on the BCI Competition 4-2a and BCI Competition 4-2b datasets, respectively, surpassing existing state-of-the-art models. The source code is available at https://github.com/xicheng105/EEG-DBNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16090v3</guid>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xicheng Lou, Xinwei Li, Hongying Meng, Jun Hu, Meili Xu, Yue Zhao, Jiazhang Yang, Zhangyong Li</dc:creator>
    </item>
    <item>
      <title>"It answers questions that I didn't know I had": Ph.D. Students' Evaluation of an Information Sharing Knowledge Graph</title>
      <link>https://arxiv.org/abs/2406.07730</link>
      <description>arXiv:2406.07730v2 Announce Type: replace 
Abstract: Interdisciplinary PhD programs can be challenging as the vital information needed by students may not be readily available, it is scattered across university's websites, while tacit knowledge can be obtained only by interacting with people. Hence, there is a need to develop a knowledge management model to create, query, and maintain a knowledge repository for interdisciplinary students. We propose a knowledge graph containing information on critical categories and their relationships, extracted from multiple sources, essential for interdisciplinary PhD students. This study evaluates the usability of a participatory designed knowledge graph intended to facilitate information exchange and decision-making. The usability findings demonstrate that interaction with this knowledge graph benefits PhD students by notably reducing uncertainty and academic stress, particularly among newcomers. Knowledge graph supported them in decision making, especially when choosing collaborators in an interdisciplinary setting. Key helpful features are related to exploring student faculty networks, milestones tracking, rapid access to aggregated data, and insights into crowdsourced fellow students' activities. The knowledge graph provides a solution to meet the personalized needs of doctoral researchers and has the potential to improve the information discovery and decision-making process substantially. It also includes the tacit knowledge exchange support missing from most current approaches, which is critical for this population and establishing interdisciplinary collaborations. This approach can be applied to other interdisciplinary programs and domains globally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07730v2</guid>
      <category>cs.HC</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1108/DLP-02-2024-0025</arxiv:DOI>
      <dc:creator>Stanislava Gardasevic, Manika Lamba</dc:creator>
    </item>
    <item>
      <title>Why is plausibility surprisingly problematic as an XAI criterion?</title>
      <link>https://arxiv.org/abs/2303.17707</link>
      <description>arXiv:2303.17707v3 Announce Type: replace-cross 
Abstract: Explainable artificial intelligence (XAI) is motivated by the problem of making AI predictions understandable, transparent, and responsible, as AI becomes increasingly impactful in society and high-stakes domains. XAI algorithms are designed to explain AI decisions in human-understandable ways. The evaluation and optimization criteria of XAI are gatekeepers for XAI algorithms to achieve their expected goals and should withstand rigorous inspection. To improve the scientific rigor of XAI, we conduct the first critical examination of a common XAI criterion: plausibility. It measures how convincing the AI explanation is to humans, and is usually quantified by metrics on feature localization or correlation of feature attribution. Our examination shows, although plausible explanations can improve users' understanding and local trust in an AI decision, doing so is at the cost of abandoning other possible approaches of enhancing understandability, increasing misleading explanations that manipulate users, being unable to achieve complementary human-AI task performance, and deteriorating users' global trust in the overall AI system. Because the flaws outweigh the benefits, we do not recommend using plausibility as a criterion to evaluate or optimize XAI algorithms. We also identify new directions to improve XAI on understandability and utility to users including complementary human-AI task performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.17707v3</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Weina Jin, Xiaoxiao Li, Ghassan Hamarneh</dc:creator>
    </item>
    <item>
      <title>DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors</title>
      <link>https://arxiv.org/abs/2305.05738</link>
      <description>arXiv:2305.05738v5 Announce Type: replace-cross 
Abstract: Modern advances in machine learning (ML) and wearable medical sensors (WMSs) in edge devices have enabled ML-driven disease detection for smart healthcare. Conventional ML-driven methods for disease detection rely on customizing individual models for each disease and its corresponding WMS data. However, such methods lack adaptability to distribution shifts and new task classification classes. In addition, they need to be rearchitected and retrained from scratch for each new disease. Moreover, installing multiple ML models in an edge device consumes excessive memory, drains the battery faster, and complicates the detection process. To address these challenges, we propose DOCTOR, a multi-disease detection continual learning (CL) framework based on WMSs. It employs a multi-headed deep neural network (DNN) and a replay-style CL algorithm. The CL algorithm enables the framework to continually learn new missions where different data distributions, classification classes, and disease detection tasks are introduced sequentially. It counteracts catastrophic forgetting with a data preservation method and a synthetic data generation (SDG) module. The data preservation method preserves the most informative subset of real training data from previous missions for exemplar replay. The SDG module models the probability distribution of the real training data and generates synthetic data for generative replay while retaining data privacy. The multi-headed DNN enables DOCTOR to detect multiple diseases simultaneously based on user WMS data. We demonstrate DOCTOR's efficacy in maintaining high disease classification accuracy with a single DNN model in various CL experiments. In complex scenarios, DOCTOR achieves 1.43 times better average test accuracy, 1.25 times better F1-score, and 0.41 higher backward transfer than the naive fine-tuning framework with a small model size of less than 350KB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.05738v5</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chia-Hao Li, Niraj K. Jha</dc:creator>
    </item>
    <item>
      <title>RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models</title>
      <link>https://arxiv.org/abs/2311.09641</link>
      <description>arXiv:2311.09641v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Human Feedback (RLHF) is a methodology designed to align Large Language Models (LLMs) with human preferences, playing an important role in LLMs alignment. Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially. To assess the red-teaming of RLHF against human preference data poisoning, we propose RankPoison, a poisoning attack method on candidates' selection of preference rank flipping to reach certain malicious behaviors (e.g., generating longer sequences, which can increase the computational cost). With poisoned dataset generated by RankPoison, we can perform poisoning attacks on LLMs to generate longer tokens without hurting the original safety alignment performance. Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word. Our findings highlight critical security challenges in RLHF, underscoring the necessity for more robust alignment methods for LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09641v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei Xiao</dc:creator>
    </item>
    <item>
      <title>Virtual academic conferencing: a scoping review of 1984-2021 literature. Novel modalities vs. long standing challenges in scholarly communication</title>
      <link>https://arxiv.org/abs/2402.00370</link>
      <description>arXiv:2402.00370v2 Announce Type: replace-cross 
Abstract: This study reviews the literature on virtual academic conferences, which have gained significant attention due to the COVID-19 pandemic. We conducted a scoping review, analyzing 147 documents available up to October 5th, 2021. We categorized this literature, identified main themes, examined theoretical approaches, evaluated empirical findings, and synthesized the advantages and disadvantages of virtual academic conferences. We find that the existing literature on virtual academic conferences is mainly descriptive and lacks a solid theoretical framework for studying the phenomenon. Despite the rapid growth of the literature documenting and discussing virtual conferencing induced by the pandemic, the understanding of the phenomenon is limited. We provide recommendations for future research on academic virtual conferences: their impact on research productivity, quality, and collaboration; relations to social, economic, and geopolitical inequalities in science; and their environmental aspects. We stress the need for further research encompassing the development of a theoretical framework that will guide empirical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00370v2</guid>
      <category>cs.DL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.47909/ijsmc.93</arxiv:DOI>
      <arxiv:journal_reference>Iberoamerican Journal of Science Measurement and Communication, Vol. 4 No. 1 (2024)</arxiv:journal_reference>
      <dc:creator>Agnieszka Olechnicka, Adam Ploszaj, Ewa Zegler-Poleska</dc:creator>
    </item>
    <item>
      <title>Toward Human-AI Alignment in Large-Scale Multi-Player Games</title>
      <link>https://arxiv.org/abs/2402.03575</link>
      <description>arXiv:2402.03575v2 Announce Type: replace-cross 
Abstract: Achieving human-AI alignment in complex multi-agent games is crucial for creating trustworthy AI agents that enhance gameplay. We propose a method to evaluate this alignment using an interpretable task-sets framework, focusing on high-level behavioral tasks instead of low-level policies. Our approach has three components. First, we analyze extensive human gameplay data from Xbox's Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task space. This task space serves as a basis set for a behavior manifold capturing interpretable axes: fight-flight, explore-exploit, and solo-multi-agent. Second, we train an AI agent to play Bleeding Edge using a Generative Pretrained Causal Transformer and measure its behavior. Third, we project human and AI gameplay to the proposed behavior manifold to compare and contrast. This allows us to interpret differences in policy as higher-level behavioral concepts, e.g., we find that while human players exhibit variability in fight-flight and explore-exploit behavior, AI players tend towards uniformity. Furthermore, AI agents predominantly engage in solo play, while humans often engage in cooperative and competitive multi-agent patterns. These stark differences underscore the need for interpretable evaluation, design, and integration of AI in human-aligned applications. Our study advances the alignment discussion in AI and especially generative AI research, offering a measurable framework for interpretable human-agent alignment in multiplayer gaming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03575v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sugandha Sharma, Guy Davidson, Khimya Khetarpal, Anssi Kanervisto, Udit Arora, Katja Hofmann, Ida Momennejad</dc:creator>
    </item>
    <item>
      <title>Joint Demonstration and Preference Learning Improves Policy Alignment with Human Feedback</title>
      <link>https://arxiv.org/abs/2406.06874</link>
      <description>arXiv:2406.06874v2 Announce Type: replace-cross 
Abstract: Aligning human preference and value is an important requirement for building contemporary foundation models and embodied AI. However, popular approaches such as reinforcement learning with human feedback (RLHF) break down the task into successive stages, such as supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL), each performing one specific learning task. Such a sequential approach results in serious issues such as significant under-utilization of data and distribution mismatch between the learned reward model and generated policy, which eventually lead to poor alignment performance. We develop a single stage approach named Alignment with Integrated Human Feedback (AIHF), capable of integrating both human preference and demonstration to train reward models and the policy. The proposed approach admits a suite of efficient algorithms, which can easily reduce to, and leverage, popular alignment algorithms such as RLHF and Directly Policy Optimization (DPO), and only requires minor changes to the existing alignment pipelines. We demonstrate the efficiency of the proposed solutions with extensive experiments involving alignment problems in LLMs and robotic control problems in MuJoCo. We observe that the proposed solutions outperform the existing alignment algorithms such as RLHF and DPO by large margins, especially when the amount of high-quality preference data is relatively limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06874v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenliang Li, Siliang Zeng, Zeyi Liao, Jiaxiang Li, Dongyeop Kang, Alfredo Garcia, Mingyi Hong</dc:creator>
    </item>
    <item>
      <title>Rideshare Transparency: Translating Gig Worker Insights on AI Platform Design to Policy</title>
      <link>https://arxiv.org/abs/2406.10768</link>
      <description>arXiv:2406.10768v2 Announce Type: replace-cross 
Abstract: Rideshare platforms exert significant control over workers through algorithmic systems that can result in financial, emotional, and physical harm. What steps can platforms, designers, and practitioners take to mitigate these negative impacts and meet worker needs? In this paper, through a novel mixed methods study combining a LLM-based analysis of over 1 million comments posted to online platform worker communities with semi-structured interviews of workers, we thickly characterize transparency-related harms, mitigation strategies, and worker needs while validating and contextualizing our findings within the broader worker community. Our findings expose a transparency gap between existing platform designs and the information drivers need, particularly concerning promotions, fares, routes, and task allocation. Our analysis suggests that rideshare workers need key pieces of information, which we refer to as indicators, to make informed work decisions. These indicators include details about rides, driver statistics, algorithmic implementation details, and platform policy information. We argue that instead of relying on platforms to include such information in their designs, new regulations that require platforms to publish public transparency reports may be a more effective solution to improve worker well-being. We offer recommendations for implementing such a policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10768v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Varun Nagaraj Rao, Samantha Dalal, Eesha Agarwal, Dana Calacci, Andr\'es Monroy-Hern\'andez</dc:creator>
    </item>
    <item>
      <title>Decoding the Digital Fine Print: Navigating the potholes in Terms of service/ use of GenAI tools against the emerging need for Transparent and Trustworthy Tech Futures</title>
      <link>https://arxiv.org/abs/2406.11845</link>
      <description>arXiv:2406.11845v2 Announce Type: replace-cross 
Abstract: The research investigates the crucial role of clear and intelligible terms of service in cultivating user trust and facilitating informed decision-making in the context of AI, in specific GenAI. It highlights the obstacles presented by complex legal terminology and detailed fine print, which impede genuine user consent and recourse, particularly during instances of algorithmic malfunctions, hazards, damages, or inequities, while stressing the necessity of employing machine-readable terms for effective service licensing. The increasing reliance on General Artificial Intelligence (GenAI) tools necessitates transparent, comprehensible, and standardized terms of use, which facilitate informed decision-making while fostering trust among stakeholders. Despite recent efforts promoting transparency via system and model cards, existing documentation frequently falls short of providing adequate disclosures, leaving users ill-equipped to evaluate potential risks and harms. To address this gap, this research examines key considerations necessary in terms of use or terms of service for Generative AI tools, drawing insights from multiple studies. Subsequently, this research evaluates whether the terms of use or terms of service of prominent Generative AI tools against the identified considerations. Findings indicate inconsistencies and variability in document quality, signaling a pressing demand for uniformity in disclosure practices. Consequently, this study advocates for robust, enforceable standards ensuring complete and intelligible disclosures prior to the release of GenAI tools, thereby empowering end-users to make well-informed choices and enhancing overall accountability in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11845v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sundaraparipurnan Narayanan</dc:creator>
    </item>
  </channel>
</rss>
