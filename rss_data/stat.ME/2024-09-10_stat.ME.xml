<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Sep 2024 04:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Nonparametric Inference for Balance in Signed Networks</title>
      <link>https://arxiv.org/abs/2409.06172</link>
      <description>arXiv:2409.06172v1 Announce Type: new 
Abstract: In many real-world networks, relationships often go beyond simple dyadic presence or absence; they can be positive, like friendship, alliance, and mutualism, or negative, characterized by enmity, disputes, and competition. To understand the formation mechanism of such signed networks, the social balance theory sheds light on the dynamics of positive and negative connections. In particular, it characterizes the proverbs, "a friend of my friend is my friend" and "an enemy of my enemy is my friend". In this work, we propose a nonparametric inference approach for assessing empirical evidence for the balance theory in real-world signed networks. We first characterize the generating process of signed networks with node exchangeability and propose a nonparametric sparse signed graphon model. Under this model, we construct confidence intervals for the population parameters associated with balance theory and establish their theoretical validity. Our inference procedure is as computationally efficient as a simple normal approximation but offers higher-order accuracy. By applying our method, we find strong real-world evidence for balance theory in signed networks across various domains, extending its applicability beyond social psychology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06172v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuyang Chen, Yinjie Wang, Weijing Tang</dc:creator>
    </item>
    <item>
      <title>Optimizing Sample Size for Supervised Machine Learning with Bulk Transcriptomic Sequencing: A Learning Curve Approach</title>
      <link>https://arxiv.org/abs/2409.06180</link>
      <description>arXiv:2409.06180v1 Announce Type: new 
Abstract: Accurate sample classification using transcriptomics data is crucial for advancing personalized medicine. Achieving this goal necessitates determining a suitable sample size that ensures adequate statistical power without undue resource allocation. Current sample size calculation methods rely on assumptions and algorithms that may not align with supervised machine learning techniques for sample classification. Addressing this critical methodological gap, we present a novel computational approach that establishes the power-versus-sample-size relationship by employing a data augmentation strategy followed by fitting a learning curve. We comprehensively evaluated its performance for microRNA and RNA sequencing data, considering diverse data characteristics and algorithm configurations, based on a spectrum of evaluation metrics. To foster accessibility and reproducibility, the Python and R code for implementing our approach is available on GitHub. Its deployment will significantly facilitate the adoption of machine learning in transcriptomics studies and accelerate their translation into clinically useful classifiers for personalized treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06180v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yunhui Qi, Xinyi Wang, Li-Xuan Qin</dc:creator>
    </item>
    <item>
      <title>Ensemble Doubly Robust Bayesian Inference via Regression Synthesis</title>
      <link>https://arxiv.org/abs/2409.06288</link>
      <description>arXiv:2409.06288v1 Announce Type: new 
Abstract: The doubly robust estimator, which models both the propensity score and outcomes, is a popular approach to estimate the average treatment effect in the potential outcome setting. The primary appeal of this estimator is its theoretical property, wherein the estimator achieves consistency as long as either the propensity score or outcomes is correctly specified. In most applications, however, both are misspecified, leading to considerable bias that cannot be checked. In this paper, we propose a Bayesian ensemble approach that synthesizes multiple models for both the propensity score and outcomes, which we call doubly robust Bayesian regression synthesis. Our approach applies Bayesian updating to the ensemble model weights that adapt at the unit level, incorporating data heterogeneity, to significantly mitigate misspecification bias. Theoretically, we show that our proposed approach is consistent regarding the estimation of both the propensity score and outcomes, ensuring that the doubly robust estimator is consistent, even if no single model is correctly specified. An efficient algorithm for posterior computation facilitates the characterization of uncertainty regarding the treatment effect. Our proposed approach is compared against standard and state-of-the-art methods through two comprehensive simulation studies, where we find that our approach is superior in all cases. An empirical study on the impact of maternal smoking on birth weight highlights the practical applicability of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06288v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaoru Babasaki, Shonosuke Sugasawa, Kosaku Takanashi, Kenichiro McAlinn</dc:creator>
    </item>
    <item>
      <title>This is not normal! (Re-) Evaluating the lower $n$ guildelines for regression analysis</title>
      <link>https://arxiv.org/abs/2409.06413</link>
      <description>arXiv:2409.06413v1 Announce Type: new 
Abstract: The commonly cited rule of thumb for regression analysis, which suggests that a sample size of $n \geq 30$ is sufficient to ensure valid inferences, is frequently referenced but rarely scrutinized. This research note evaluates the lower bound for the number of observations required for regression analysis by exploring how different distributional characteristics, such as skewness and kurtosis, influence the convergence of t-values to the t-distribution in linear regression models. Through an extensive simulation study involving over 22 billion regression models, this paper examines a range of symmetric, platykurtic, and skewed distributions, testing sample sizes from 4 to 10,000. The results reveal that it is sufficient that either the dependent or independent variable follow a symmetric distribution for the t-values to converge to the t-distribution at much smaller sample sizes than $n=30$. This is contrary to previous guidance which suggests that the error term needs to be normally distributed for this convergence to happen at low $n$. On the other hand, if both dependent and independent variables are highly skewed the required sample size is substantially higher. In cases of extreme skewness, even sample sizes of 10,000 do not ensure convergence. These findings suggest that the $n\geq30$ rule is too permissive in certain cases but overly conservative in others, depending on the underlying distributional characteristics. This study offers revised guidelines for determining the minimum sample size necessary for valid regression analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06413v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Randahl</dc:creator>
    </item>
    <item>
      <title>Sequential stratified inference for the mean</title>
      <link>https://arxiv.org/abs/2409.06680</link>
      <description>arXiv:2409.06680v1 Announce Type: new 
Abstract: We develop conservative tests for the mean of a bounded population using data from a stratified sample. The sample may be drawn sequentially, with or without replacement. The tests are "anytime valid," allowing optional stopping and continuation in each stratum. We call this combination of properties sequential, finite-sample, nonparametric validity. The methods express a hypothesis about the population mean as a union of intersection hypotheses describing within-stratum means. They test each intersection hypothesis using independent test supermartingales (TSMs) combined across strata by multiplication. The $P$-value of the global null hypothesis is then the maximum $P$-value of any intersection hypothesis in the union. This approach has three primary moving parts: (i) the rule for deciding which stratum to draw from next to test each intersection null, given the sample so far; (ii) the form of the TSM for each null in each stratum; and (iii) the method of combining evidence across strata. These choices interact. We examine the performance of a variety of rules with differing computational complexity. Approximately optimal methods have a prohibitive computational cost, while naive rules may be inconsistent -- they will never reject for some alternative populations, no matter how large the sample. We present a method that is statistically comparable to optimal methods in examples where optimal methods are computable, but computationally tractable for arbitrarily many strata. In numerical examples its expected sample size is substantially smaller than that of previous methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06680v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jacob V. Spertus, Mayuri Sridhar, Philip B. Stark</dc:creator>
    </item>
    <item>
      <title>Predicting Electricity Consumption with Random Walks on Gaussian Processes</title>
      <link>https://arxiv.org/abs/2409.05934</link>
      <description>arXiv:2409.05934v1 Announce Type: cross 
Abstract: We consider time-series forecasting problems where data is scarce, difficult to gather, or induces a prohibitive computational cost. As a first attempt, we focus on short-term electricity consumption in France, which is of strategic importance for energy suppliers and public stakeholders. The complexity of this problem and the many levels of geospatial granularity motivate the use of an ensemble of Gaussian Processes (GPs). Whilst GPs are remarkable predictors, they are computationally expensive to train, which calls for a frugal few-shot learning approach. By taking into account performance on GPs trained on a dataset and designing a random walk on these, we mitigate the training cost of our entire Bayesian decision-making procedure. We introduce our algorithm called \textsc{Domino} (ranDOM walk on gaussIaN prOcesses) and present numerical experiments to support its merits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05934v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chlo\'e Hashimoto-Cullen, Benjamin Guedj</dc:creator>
    </item>
    <item>
      <title>Causal Analysis of Shapley Values: Conditional vs. Marginal</title>
      <link>https://arxiv.org/abs/2409.06157</link>
      <description>arXiv:2409.06157v1 Announce Type: cross 
Abstract: Shapley values, a game theoretic concept, has been one of the most popular tools for explaining Machine Learning (ML) models in recent years. Unfortunately, the two most common approaches, conditional and marginal, to calculating Shapley values can lead to different results along with some undesirable side effects when features are correlated. This in turn has led to the situation in the literature where contradictory recommendations regarding choice of an approach are provided by different authors. In this paper we aim to resolve this controversy through the use of causal arguments. We show that the differences arise from the implicit assumptions that are made within each method to deal with missing causal information. We also demonstrate that the conditional approach is fundamentally unsound from a causal perspective. This, together with previous work in [1], leads to the conclusion that the marginal approach should be preferred over the conditional one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06157v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilya Rozenfeld</dc:creator>
    </item>
    <item>
      <title>A new paradigm for global sensitivity analysis</title>
      <link>https://arxiv.org/abs/2409.06271</link>
      <description>arXiv:2409.06271v1 Announce Type: cross 
Abstract: &lt;div&gt;&lt;p&gt;Current theory of global sensitivity analysis, based on a nonlinear functional ANOVA decomposition of the random output, is limited in scope-for instance, the analysis is limited to the output's variance and the inputs have to be mutually independent-and leads to sensitivity indices the interpretation of which is not fully clear, especially interaction effects. Alternatively, sensitivity indices built for arbitrary user-defined importance measures have been proposed but a theory to define interactions in a systematic fashion and/or establish a decomposition of the total importance measure is still missing. It is shown that these important problems are solved all at once by adopting a new paradigm. By partitioning the inputs into those causing the change in the output and those which do not, arbitrary user-defined variability measures are identified with the outcomes of a factorial experiment at two levels, leading to all factorial effects without assuming any functional decomposition. To link various well-known sensitivity indices of the literature (Sobol indices and Shapley effects), weighted factorial effects are studied and utilized.&lt;/p&gt;&lt;/div&gt;</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06271v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gildas Mazo (MaIAGE)</dc:creator>
    </item>
    <item>
      <title>Enzyme kinetic reactions as interacting particle systems: Stochastic averaging and parameter inference</title>
      <link>https://arxiv.org/abs/2409.06565</link>
      <description>arXiv:2409.06565v1 Announce Type: cross 
Abstract: We consider a stochastic model of multistage Michaelis--Menten (MM) type enzyme kinetic reactions describing the conversion of substrate molecules to a product through several intermediate species. The high-dimensional, multiscale nature of these reaction networks presents significant computational challenges, especially in statistical estimation of reaction rates. This difficulty is amplified when direct data on system states are unavailable, and one only has access to a random sample of product formation times. To address this, we proceed in two stages. First, under certain technical assumptions akin to those made in the Quasi-steady-state approximation (QSSA) literature, we prove two asymptotic results: a stochastic averaging principle that yields a lower-dimensional model, and a functional central limit theorem that quantifies the associated fluctuations. Next, for statistical inference of the parameters of the original MM reaction network, we develop a mathematical framework involving an interacting particle system (IPS) and prove a propagation of chaos result that allows us to write a product-form likelihood function. The novelty of the IPS-based inference method is that it does not require information about the state of the system and works with only a random sample of product formation times. We provide numerical examples to illustrate the efficacy of the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06565v1</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnab Ganguly, Wasiur R. KhudaBukhsh</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference for Causal Functions with Multiway Clustered Data</title>
      <link>https://arxiv.org/abs/2409.06654</link>
      <description>arXiv:2409.06654v1 Announce Type: cross 
Abstract: This paper proposes methods of estimation and uniform inference for a general class of causal functions, such as the conditional average treatment effects and the continuous treatment effects, under multiway clustering. The causal function is identified as a conditional expectation of an adjusted (Neyman-orthogonal) signal that depends on high-dimensional nuisance parameters. We propose a two-step procedure where the first step uses machine learning to estimate the high-dimensional nuisance parameters. The second step projects the estimated Neyman-orthogonal signal onto a dictionary of basis functions whose dimension grows with the sample size. For this two-step procedure, we propose both the full-sample and the multiway cross-fitting estimation approaches. A functional limit theory is derived for these estimators. To construct the uniform confidence bands, we develop a novel resampling procedure, called the multiway cluster-robust sieve score bootstrap, that extends the sieve score bootstrap (Chen and Christensen, 2018) to the novel setting with multiway clustering. Extensive numerical simulations showcase that our methods achieve desirable finite-sample behaviors. We apply the proposed methods to analyze the causal relationship between mistrust levels in Africa and the historical slave trade. Our analysis rejects the null hypothesis of uniformly zero effects and reveals heterogeneous treatment effects, with significant impacts at higher levels of trade volumes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06654v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nan Liu, Yanbo Liu, Yuya Sasaki</dc:creator>
    </item>
    <item>
      <title>Estimation of conditional average treatment effects on distributed confidential data</title>
      <link>https://arxiv.org/abs/2402.02672</link>
      <description>arXiv:2402.02672v3 Announce Type: replace 
Abstract: Estimation of conditional average treatment effects (CATEs) is an important topic in sciences. CATEs can be estimated with high accuracy if distributed data across multiple parties can be centralized. However, it is difficult to aggregate such data owing to confidential or privacy concerns. To address this issue, we proposed data collaboration double machine learning, a method that can estimate CATE models from privacy-preserving fusion data constructed from distributed data, and evaluated our method through simulations. Our contributions are summarized in the following three points. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data. Our semi-parametric CATE method enable estimation and testing that is more robust to model mis-specification than parametric methods. Second, our method enables collaborative estimation between multiple time points and different parties through the accumulation of a knowledge base. Third, our method performed equally or better than other methods in simulations using synthetic, semi-synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02672v3</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuji Kawamata, Ryoki Motai, Yukihiko Okada, Akira Imakura, Tetsuya Sakurai</dc:creator>
    </item>
    <item>
      <title>JEL ratio test for independence between a continuous and a categorical random variable</title>
      <link>https://arxiv.org/abs/2402.18105</link>
      <description>arXiv:2402.18105v2 Announce Type: replace 
Abstract: The categorical Gini covariance is a dependence measure between a numerical variable and a categorical variable. The Gini covariance measures dependence by quantifying the difference between the conditional and unconditional distributional functions. The categorical Gini covariance equals zero if and only if the numerical variable and the categorical variable are independent. We propose a non-parametric test for testing the independence between a numerical and categorical variable using a modified categorical Gini covariance. We used the theory of U-statistics to find the test statistics and study the properties. The test has an asymptotic normal distribution. Since the implementation of a normal-based test is difficult, we develop a jackknife empirical likelihood (JEL) ratio test for testing independence. Extensive Monte Carlo simulation studies are carried out to validate the performance of the proposed JEL ratio test. We illustrate the test procedure using Iris flower data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18105v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saparya Suresh, Sudheesh K. Kattumannil</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate Control for Confounder Selection Using Mirror Statistics</title>
      <link>https://arxiv.org/abs/2402.18904</link>
      <description>arXiv:2402.18904v3 Announce Type: replace 
Abstract: While data-driven confounder selection requires careful consideration, it is frequently employed in observational studies. Widely recognized criteria for confounder selection include the minimal-set approach, which involves selecting variables relevant to both treatment and outcome, and the union-set approach, which involves selecting variables associated with either treatment or outcome. These approaches are often implemented using heuristics and off-the-shelf statistical methods, where the degree of uncertainty may not be clear. In this paper, we focus on the false discovery rate (FDR) to measure uncertainty in confounder selection. We define the FDR specific to confounder selection and propose methods based on the mirror statistic, a recently developed approach for FDR control that does not rely on p-values. The proposed methods are p-value-free and require only the assumption of some symmetry in the distribution of the mirror statistic. It can be combined with sparse estimation and other methods that involve difficulties in deriving p-values. The properties of the proposed methods are investigated through exhaustive numerical experiments. Particularly in high-dimensional data scenarios, the proposed methods effectively control FDR and perform better than the p-value-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18904v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazuharu Harada, Masataka Taguri</dc:creator>
    </item>
    <item>
      <title>On Neighbourhood Cross Validation</title>
      <link>https://arxiv.org/abs/2404.16490</link>
      <description>arXiv:2404.16490v2 Announce Type: replace 
Abstract: Many varieties of cross validation would be statistically appealing for the estimation of smoothing and other penalized regression hyperparameters, were it not for the high cost of evaluating such criteria. Here it is shown how to efficiently and accurately compute and optimize a broad variety of cross validation criteria for a wide range of models estimated by minimizing a quadratically penalized loss. The leading order computational cost of hyperparameter estimation is made comparable to the cost of a single model fit given hyperparameters. In many cases this represents an $O(n)$ computational saving when modelling $n$ data. This development makes if feasible, for the first time, to use leave-out-neighbourhood cross validation to deal with the wide spread problem of un-modelled short range autocorrelation which otherwise leads to underestimation of smoothing parameters. It is also shown how to accurately quantifying uncertainty in this case, despite the un-modelled autocorrelation. Practical examples are provided including smooth quantile regression, generalized additive models for location scale and shape, and focussing particularly on dealing with un-modelled autocorrelation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16490v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon N. Wood</dc:creator>
    </item>
    <item>
      <title>Process-based Inference for Spatial Energetics Using Bayesian Predictive Stacking</title>
      <link>https://arxiv.org/abs/2405.09906</link>
      <description>arXiv:2405.09906v3 Announce Type: replace 
Abstract: Rapid developments in streaming data technologies have enabled real-time monitoring of human activity that can deliver high-resolution data on health variables over trajectories or paths carved out by subjects as they conduct their daily physical activities. Wearable devices, such as wrist-worn sensors that monitor gross motor activity, have become prevalent and have kindled the emerging field of "spatial energetics" in environmental health sciences. We devise a Bayesian inferential framework for analyzing such data while accounting for information available on specific spatial coordinates comprising a trajectory or path using a Global Positioning System (GPS) device embedded within the wearable device. We offer full probabilistic inference with uncertainty quantification using spatial-temporal process models adapted for data generated from "actigraph" units as the subject traverses a path or trajectory in their daily routine. Anticipating the need for fast inference for mobile health data, we pursue exact inference using conjugate Bayesian models and employ predictive stacking to assimilate inference across these individual models. This circumvents issues with iterative estimation algorithms such as Markov chain Monte Carlo. We devise Bayesian predictive stacking in this context for models that treat time as discrete epochs and that treat time as continuous. We illustrate our methods with simulation experiments and analysis of data from the Physical Activity through Sustainable Transport Approaches (PASTA-LA) study conducted by the Fielding School of Public Health at the University of California, Los Angeles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09906v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Preregistration does not improve the transparent evaluation of severity in Popper's philosophy of science or when deviations are allowed</title>
      <link>https://arxiv.org/abs/2408.12347</link>
      <description>arXiv:2408.12347v5 Announce Type: replace 
Abstract: One justification for preregistering research hypotheses, methods, and analyses is that it improves the transparent evaluation of the severity of hypothesis tests. In this article, I consider two cases in which preregistration does not improve this evaluation. First, I argue that, although preregistration can facilitate the transparent evaluation of severity in Mayo's error statistical philosophy of science, it does not facilitate this evaluation in Popper's theory-centric approach. To illustrate, I show that associated concerns about Type I error rate inflation are only relevant in the error statistical approach and not in a theory-centric approach. Second, I argue that a preregistered test procedure that allows deviations in its implementation does not provide a more transparent evaluation of Mayoian severity than a non-preregistered procedure. In particular, I argue that sample-based validity-enhancing deviations cause an unknown inflation of the test procedure's Type I (familywise) error rate and, consequently, an unknown reduction in its capability to license inferences severely. I conclude that preregistration does not improve the transparent evaluation of severity in Popper's philosophy of science or when deviations are allowed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12347v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Rubin</dc:creator>
    </item>
    <item>
      <title>Testing Whether Reported Treatment Effects are Unduly Dependent on the Specific Outcome Measure Used</title>
      <link>https://arxiv.org/abs/2409.03502</link>
      <description>arXiv:2409.03502v2 Announce Type: replace 
Abstract: This paper addresses the situation in which treatment effects are reported using educational or psychological outcome measures comprised of multiple questions or "items." A distinction is made between a treatment effect on the construct being measured, which is referred to as impact, and item-specific treatment effects that are not due to impact, which are referred to as differential item functioning (DIF). By definition, impact generalizes to other measures of the same construct (i.e., measures that use different items), while DIF is dependent upon the specific items that make up the outcome measure. To distinguish these two cases, two estimators of impact are compared: an estimator that naively aggregates over items, and a less efficient one that is highly robust to DIF. The null hypothesis that both are consistent estimators of the true treatment impact leads to a Hausman-like specification test of whether the naive estimate is affected by item-level variation that would not be expected to generalize beyond the specific outcome measure used. The performance of the test is illustrated with simulation studies and a re-analysis of 34 item-level datasets from 22 randomized evaluations of educational interventions. In the empirical example, the dependence of reported effect sizes on the type of outcome measure (researcher-developed or independently developed) was substantially reduced after accounting for DIF. Implications for the ongoing debate about the role of researcher-developed assessments in education sciences are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03502v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Peter Halpin, Joshua Gilbert</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: I. Mean</title>
      <link>https://arxiv.org/abs/2403.12110</link>
      <description>arXiv:2403.12110v4 Announce Type: replace-cross 
Abstract: As the most fundamental problem in statistics, robust location estimation has many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges Lehmann estimator, Huber M estimator, and median of means. Recent studies suggest that their maximum biases concerning the mean can be quite different, but the underlying mechanisms largely remain unclear. This study exploited a semiparametric method to classify distributions by the asymptotic orderliness of quantile combinations with varying breakdown points, showing their interrelations and connections to parametric distributions. Further deductions explain why the Winsorized mean typically has smaller biases compared to the trimmed mean; two sequences of semiparametric robust mean estimators emerge, particularly highlighting the superiority of the median Hodges Lehmann mean. This article sheds light on the understanding of the common nature of probability distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12110v4</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
  </channel>
</rss>
