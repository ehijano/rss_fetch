<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Aug 2024 04:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>WASP: Voting-based ex Ante method for Selecting joint Prediction strategy</title>
      <link>https://arxiv.org/abs/2408.13393</link>
      <description>arXiv:2408.13393v1 Announce Type: new 
Abstract: This paper addresses the topic of choosing a prediction strategy when using parametric or nonparametric regression models. It emphasizes the importance of ex ante prediction accuracy, ensemble approaches, and forecasting not only the values of the dependent variable but also a function of these values, such as total income or median loss. It proposes a method for selecting a strategy for predicting the vector of functions of the dependent variable using various ex ante accuracy measures. The final decision is made through voting, where the candidates are prediction strategies and the voters are diverse prediction models with their respective prediction errors. Because the method is based on a Monte Carlo simulation, it allows for new scenarios, not previously observed, to be considered. The first part of the article provides a detailed theoretical description of the proposed method, while the second part presents its practical use in managing a portfolio of communication insurance. The example uses data from the Polish insurance market. All calculations are performed using the R programme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13393v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alicja Wolny-Dominiak, Tomasz \.Z\k{a}d{\l}o</dc:creator>
    </item>
    <item>
      <title>Leveraging external data in the analysis of randomized controlled trials: a comparative analysis</title>
      <link>https://arxiv.org/abs/2408.13409</link>
      <description>arXiv:2408.13409v1 Announce Type: new 
Abstract: The use of patient-level information from previous studies, registries, and other external datasets can support the analysis of single-arm and randomized clinical trials to evaluate and test experimental treatments. However, the integration of external data in the analysis of clinical trials can also compromise the scientific validity of the results due to selection bias, study-to-study differences, unmeasured confounding, and other distortion mechanisms. Therefore, leveraging external data in the analysis of a clinical trial requires the use of appropriate methods that can detect, prevent or mitigate the risks of bias and potential distortion mechanisms. We review several methods that have been previously proposed to leverage external datasets, such as matching procedures or random effects modeling. Different methods present distinct trade-offs between risks and efficiencies. We conduct a comparative analysis of statistical methods to leverage external data and analyze randomized clinical trials. Multiple operating characteristics are discussed, such as the control of false positive results, power, and the bias of the treatment effect estimates, across candidate statistical methods. We compare the statistical methods through a broad set of simulation scenarios. We then compare the methods using a collection of datasets with individual patient-level information from several glioblastoma studies in order to provide recommendations for future glioblastoma trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13409v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gopal Kotecha, Daniel E. Schwartz, Steffen Ventz, Lorenzo Trippa</dc:creator>
    </item>
    <item>
      <title>Estimating the Effective Sample Size for an inverse problem in subsurface flows</title>
      <link>https://arxiv.org/abs/2408.13411</link>
      <description>arXiv:2408.13411v1 Announce Type: new 
Abstract: The Effective Sample Size (ESS) and Integrated Autocorrelation Time (IACT) are two popular criteria for comparing Markov Chain Monte Carlo (MCMC) algorithms and detecting their convergence. Our goal is to assess those two quantities in the context of an inverse problem in subsurface flows. We begin by presenting a review of some popular methods for their estimation, and then simulate their sample distributions on AR(1) sequences for which the exact values were known. We find that those ESS estimators may not be statistically consistent, because their variance grows linearly in the number of sample values of the MCMC. Next, we analyze the output of two distinct MCMC algorithms for the Bayesian approach to the simulation of an elliptic inverse problem. Here, the estimators cannot even agree about the order of magnitude of the ESS. Our conclusion is that the ESS has major limitations and should not be used on MCMC outputs of complex models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13411v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lucas Seiffert, Felipe Pereira</dc:creator>
    </item>
    <item>
      <title>Epistemically robust selection of fitted models</title>
      <link>https://arxiv.org/abs/2408.13414</link>
      <description>arXiv:2408.13414v1 Announce Type: new 
Abstract: Fitting models to data is an important part of the practice of science, made almost ubiquitous by advances in machine learning. Very often however, fitted solutions are not unique, but form an ensemble of candidate models -- qualitatively different, yet with comparable quantitative performance. One then needs a criterion which can select the best candidate models, or at least falsify (reject) the worst ones. Because standard statistical approaches to model selection rely on assumptions which are usually invalid in scientific contexts, they tend to be overconfident, rejecting models based on little more than statistical noise. The ideal objective for fitting models is generally considered to be the risk: this is the theoretical average loss of a model (assuming unlimited data). In this work we develop a nonparametric method for estimating, for each candidate model, the epistemic uncertainty on its risk: in other words we associate to each model a distribution of scores which accounts for expected modelling errors. We then propose that a model falsification criterion should mirror established experimental practice: a falsification result should be accepted only if it is reproducible across experimental variations. The strength of this approach is illustrated using examples from physics and neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13414v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Ren\'e, Andr\'e Longtin</dc:creator>
    </item>
    <item>
      <title>Unifying design-based and model-based sampling theory -- some suggestions to clear the cobwebs</title>
      <link>https://arxiv.org/abs/2408.13453</link>
      <description>arXiv:2408.13453v1 Announce Type: new 
Abstract: This paper gives a holistic overview of both the design-based and model-based paradigms for sampling theory. Both methods are presented within a unified framework with a simple consistent notation, and the differences in the two paradigms are explained within this common framework. We examine the different definitions of the "population variance" within the two paradigms and examine the use of Bessel's correction for a population variance. We critique some messy aspects of the presentation of the design-based paradigm and implore readers to avoid the standard presentation of this framework in favour of a more explicit presentation that includes explicit conditioning in probability statements. We also discuss a number of confusions that arise from the standard presentation of the design-based paradigm and argue that Bessel's correction should be applied to the population variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13453v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ben O'Neill</dc:creator>
    </item>
    <item>
      <title>Ridge, lasso, and elastic-net estimations of the modified Poisson and least-squares regressions for binary outcome data</title>
      <link>https://arxiv.org/abs/2408.13474</link>
      <description>arXiv:2408.13474v1 Announce Type: new 
Abstract: Logistic regression is a standard method in multivariate analysis for binary outcome data in epidemiological and clinical studies; however, the resultant odds-ratio estimates fail to provide directly interpretable effect measures. The modified Poisson and least-squares regressions are alternative standard methods that can provide risk-ratio and risk difference estimates without computational problems. However, the bias and invalid inference problems of these regression analyses under small or sparse data conditions (i.e.,the "separation" problem) have been insufficiently investigated. We show that the separation problem can adversely affect the inferences of the modified Poisson and least squares regressions, and to address these issues, we apply the ridge, lasso, and elastic-net estimating approaches to the two regression methods. As the methods are not founded on the maximum likelihood principle, we propose regularized quasi-likelihood approaches based on the estimating equations for these generalized linear models. The methods provide stable shrinkage estimates of risk ratios and risk differences even under separation conditions, and the lasso and elastic-net approaches enable simultaneous variable selection. We provide a bootstrap method to calculate the confidence intervals on the basis of the regularized quasi-likelihood estimation. The proposed methods are applied to a hematopoietic stem cell transplantation cohort study and the National Child Development Survey. We also provide an R package, regconfint, to implement these methods with simple commands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13474v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takahiro Kitano, Hisashi Noma</dc:creator>
    </item>
    <item>
      <title>Cross Sectional Regression with Cluster Dependence: Inference based on Averaging</title>
      <link>https://arxiv.org/abs/2408.13514</link>
      <description>arXiv:2408.13514v1 Announce Type: new 
Abstract: We re-investigate the asymptotic properties of the traditional OLS (pooled) estimator, $\hat{\beta} _P$, with cluster dependence. The present study considers various scenarios under various restrictions on the cluster sizes and number of clusters. It is shown that $\hat{\beta}_P$ could be inconsistent in many realistic situations. We propose a simple estimator, $\hat{\beta}_A$ based on data averaging. The asymptotic properties of $\hat{\beta}_A$ is studied. It is shown that $\hat{\beta}_A$ is consistent even when $\hat{\beta}_P$ is inconsistent. It is further shown that the proposed estimator $\hat{\beta}_A$ could be more efficient as compared to $\hat{\beta}_P$ in many practical scenarios. As a consequence of averaging, we show that $\hat{\beta}_A$ retains consistency, asymptotic normality under classical measurement error problem circumventing the use of IV. A detailed simulation study shows the efficacy of $\hat{\beta}_A$. It is also seen that $\hat{\beta}_A$ yields better goodness of fit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13514v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhodeep Dey, Gopal K. Basak, Samarjit Das</dc:creator>
    </item>
    <item>
      <title>Local statistical moments to capture Kramers-Moyal coefficients</title>
      <link>https://arxiv.org/abs/2408.13555</link>
      <description>arXiv:2408.13555v1 Announce Type: new 
Abstract: This study introduces an innovative local statistical moment approach for estimating Kramers-Moyal coefficients, effectively bridging the gap between nonparametric and parametric methodologies. These coefficients play a crucial role in characterizing stochastic processes. Our proposed approach provides a versatile framework for localized coefficient estimation, combining the flexibility of nonparametric methods with the interpretability of global parametric approaches. We showcase the efficacy of our approach through use cases involving both stationary and non-stationary time series analysis. Additionally, we demonstrate its applicability to real-world complex systems, specifically in the energy conversion process analysis of a wind turbine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13555v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Wiedemann, Matthias W\"achter, Jan A. Freund, Joachim Peinke</dc:creator>
    </item>
    <item>
      <title>Robust Principal Components by Casewise and Cellwise Weighting</title>
      <link>https://arxiv.org/abs/2408.13596</link>
      <description>arXiv:2408.13596v1 Announce Type: new 
Abstract: Principal component analysis (PCA) is a fundamental tool for analyzing multivariate data. Here the focus is on dimension reduction to the principal subspace, characterized by its projection matrix. The classical principal subspace can be strongly affected by the presence of outliers. Traditional robust approaches consider casewise outliers, that is, cases generated by an unspecified outlier distribution that differs from that of the clean cases. But there may also be cellwise outliers, which are suspicious entries that can occur anywhere in the data matrix. Another common issue is that some cells may be missing. This paper proposes a new robust PCA method, called cellPCA, that can simultaneously deal with casewise outliers, cellwise outliers, and missing cells. Its single objective function combines two robust loss functions, that together mitigate the effect of casewise and cellwise outliers. The objective function is minimized by an iteratively reweighted least squares (IRLS) algorithm. Residual cellmaps and enhanced outlier maps are proposed for outlier detection. The casewise and cellwise influence functions of the principal subspace are derived, and its asymptotic distribution is obtained. Extensive simulations and two real data examples illustrate the performance of cellPCA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13596v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Centofanti, Mia Hubert, Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Influence Networks: Bayesian Modeling and Diffusion</title>
      <link>https://arxiv.org/abs/2408.13606</link>
      <description>arXiv:2408.13606v1 Announce Type: new 
Abstract: In this article, we make an innovative adaptation of a Bayesian latent space model based on projections in a novel way to analyze influence networks. By appropriately reparameterizing the model, we establish a formal metric for quantifying each individual's influencing capacity and estimating their latent position embedded in a social space. This modeling approach introduces a novel mechanism for fully characterizing the diffusion of an idea based on the estimated latent characteristics. It assumes that each individual takes the following states: Unknown, undecided, supporting, or rejecting an idea. This approach is demonstrated using a influence network from Twitter (now $\mathbb{X}$) related to the 2022 Tax Reform in Colombia. An exhaustive simulation exercise is also performed to evaluate the proposed diffusion process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13606v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel S\'anchez-Guti\'errez, Juan Sosa, Carolina Luque</dc:creator>
    </item>
    <item>
      <title>Tree-structured Markov random fields with Poisson marginal distributions</title>
      <link>https://arxiv.org/abs/2408.13649</link>
      <description>arXiv:2408.13649v1 Announce Type: new 
Abstract: A new family of tree-structured Markov random fields for a vector of discrete counting random variables is introduced. According to the characteristics of the family, the marginal distributions of the Markov random fields are all Poisson with the same mean, and are untied from the strength or structure of their built-in dependence. This key feature is uncommon for Markov random fields and most convenient for applications purposes. The specific properties of this new family confer a straightforward sampling procedure and analytic expressions for the joint probability mass function and the joint probability generating function of the vector of counting random variables, thus granting computational methods that scale well to vectors of high dimension. We study the distribution of the sum of random variables constituting a Markov random field from the proposed family, analyze a random variable's individual contribution to that sum through expected allocations, and establish stochastic orderings to assess a wide understanding of their behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13649v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin C\^ot\'e, H\'el\`ene Cossette, Etienne Marceau</dc:creator>
    </item>
    <item>
      <title>Robust likelihood ratio tests for composite nulls and alternatives</title>
      <link>https://arxiv.org/abs/2408.14015</link>
      <description>arXiv:2408.14015v1 Announce Type: new 
Abstract: We propose an e-value based framework for testing composite nulls against composite alternatives when an $\epsilon$ fraction of the data can be arbitrarily corrupted. Our tests are inherently sequential, being valid at arbitrary data-dependent stopping times, but they are new even for fixed sample sizes, giving type-I error control without any regularity conditions. We achieve this by modifying and extending a proposal by Huber (1965) in the point null versus point alternative case. Our test statistic is a nonnegative supermartingale under the null, even with a sequentially adaptive contamination model where the conditional distribution of each observation given the past data lies within an $\epsilon$ (total variation) ball of the null. The test is powerful within an $\epsilon$ ball of the alternative. As a consequence, one obtains anytime-valid p-values that enable continuous monitoring of the data, and adaptive stopping. We analyze the growth rate of our test supermartingale and demonstrate that as $\epsilon\to 0$, it approaches a certain Kullback-Leibler divergence between the null and alternative, which is the optimal non-robust growth rate. A key step is the derivation of a robust Reverse Information Projection (RIPr). Simulations validate the theory and demonstrate excellent practical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14015v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aytijhya Saha, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Robust subgroup-classifier learning and testing in change-plane regressions</title>
      <link>https://arxiv.org/abs/2408.14036</link>
      <description>arXiv:2408.14036v1 Announce Type: new 
Abstract: Considered here are robust subgroup-classifier learning and testing in change-plane regressions with heavy-tailed errors, which can identify subgroups as a basis for making optimal recommendations for individualized treatment. A new subgroup classifier is proposed by smoothing the indicator function, which is learned by minimizing the smoothed Huber loss. Nonasymptotic properties and the Bahadur representation of estimators are established, in which the proposed estimators of the grouping difference parameter and baseline parameter achieve sub-Gaussian tails. The hypothesis test considered here belongs to the class of test problems for which some parameters are not identifiable under the null hypothesis. The classic supremum of the squared score test statistic may lose power in practice when the dimension of the grouping parameter is large, so to overcome this drawback and make full use of the data's heavy-tailed error distribution, a robust weighted average of the squared score test statistic is proposed, which achieves a closed form when an appropriate weight is chosen. Asymptotic distributions of the proposed robust test statistic are derived under the null and alternative hypotheses. The proposed robust subgroup classifier and test statistic perform well on finite samples, and their performances are shown further by applying them to a medical dataset. The proposed procedure leads to the immediate application of recommending optimal individualized treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14036v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xu Liu, Jian Huang, Yong Zhou, Xiao Zhang</dc:creator>
    </item>
    <item>
      <title>Jackknife Empirical Likelihood Method for U Statistics Based on Multivariate Samples and its Applications</title>
      <link>https://arxiv.org/abs/2408.14038</link>
      <description>arXiv:2408.14038v1 Announce Type: new 
Abstract: Empirical likelihood (EL) and its extension via the jackknife empirical likelihood (JEL) method provide robust alternatives to parametric approaches, in the contexts with uncertain data distributions. This paper explores the theoretical foundations and practical applications of JEL in the context of multivariate sample-based U-statistics. In this study we develop the JEL method for multivariate U-statistics with three (or more) samples. This study enhance the JEL methods capability to handle complex data structures while preserving the computation efficiency of the empirical likelihood method. To demonstrate the applications of the JEL method, we compute confidence intervals for differences in VUS measurements which have potential applications in classification problems. Monte Carlo simulation studies are conducted to evaluate the efficiency of the JEL, Normal approximation and Kernel based confidence intervals. These studies validate the superior performance of the JEL approach in terms of coverage probability and computational efficiency compared to other two methods. Additionally, a real data application illustrates the practical utility of the approach. The JEL method developed here has potential applications in dealing with complex data structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14038v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naresh Garg, Litty Mathew, Isha Dewan, Sudheesh Kumar Kattumannil</dc:creator>
    </item>
    <item>
      <title>A quasi-Bayesian sequential approach to deconvolution density estimation</title>
      <link>https://arxiv.org/abs/2408.14402</link>
      <description>arXiv:2408.14402v1 Announce Type: new 
Abstract: Density deconvolution addresses the estimation of the unknown (probability) density function $f$ of a random signal from data that are observed with an independent additive random noise. This is a classical problem in statistics, for which frequentist and Bayesian nonparametric approaches are available to deal with static or batch data. In this paper, we consider the problem of density deconvolution in a streaming or online setting where noisy data arrive progressively, with no predetermined sample size, and we develop a sequential nonparametric approach to estimate $f$. By relying on a quasi-Bayesian sequential approach, often referred to as Newton's algorithm, we obtain estimates of $f$ that are of easy evaluation, computationally efficient, and with a computational cost that remains constant as the amount of data increases, which is critical in the streaming setting. Large sample asymptotic properties of the proposed estimates are studied, yielding provable guarantees with respect to the estimation of $f$ at a point (local) and on an interval (uniform). In particular, we establish local and uniform central limit theorems, providing corresponding asymptotic credible intervals and bands. We validate empirically our methods on synthetic and real data, by considering the common setting of Laplace and Gaussian noise distributions, and make a comparison with respect to the kernel-based approach and a Bayesian nonparametric approach with a Dirichlet process mixture prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14402v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano Favaro, Sandra Fortini</dc:creator>
    </item>
    <item>
      <title>Generalized Bayesian nonparametric clustering framework for high-dimensional spatial omics data</title>
      <link>https://arxiv.org/abs/2408.14410</link>
      <description>arXiv:2408.14410v1 Announce Type: new 
Abstract: The advent of next-generation sequencing-based spatially resolved transcriptomics (SRT) techniques has transformed genomic research by enabling high-throughput gene expression profiling while preserving spatial context. Identifying spatial domains within SRT data is a critical task, with numerous computational approaches currently available. However, most existing methods rely on a multi-stage process that involves ad-hoc dimension reduction techniques to manage the high dimensionality of SRT data. These low-dimensional embeddings are then subjected to model-based or distance-based clustering methods. Additionally, many approaches depend on arbitrarily specifying the number of clusters (i.e., spatial domains), which can result in information loss and suboptimal downstream analysis. To address these limitations, we propose a novel Bayesian nonparametric mixture of factor analysis (BNPMFA) model, which incorporates a Markov random field-constrained Gibbs-type prior for partitioning high-dimensional spatial omics data. This new prior effectively integrates the spatial constraints inherent in SRT data while simultaneously inferring cluster membership and determining the optimal number of spatial domains. We have established the theoretical identifiability of cluster membership within this framework. The efficacy of our proposed approach is demonstrated through realistic simulations and applications to two SRT datasets. Our results show that the BNPMFA model not only surpasses state-of-the-art methods in clustering accuracy and estimating the number of clusters but also offers novel insights for identifying cellular regions within tissue samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14410v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bencong Zhu, Guanyu Hu, Xiaodan Fan, Qiwei Li</dc:creator>
    </item>
    <item>
      <title>Invariant Causal Prediction with Locally Linear Models</title>
      <link>https://arxiv.org/abs/2401.05218</link>
      <description>arXiv:2401.05218v1 Announce Type: cross 
Abstract: We consider the task of identifying the causal parents of a target variable among a set of candidate variables from observational data. Our main assumption is that the candidate variables are observed in different environments which may, for example, correspond to different settings of a machine or different time intervals in a dynamical process. Under certain assumptions different environments can be regarded as interventions on the observed system. We assume a linear relationship between target and covariates, which can be different in each environment with the only restriction that the causal structure is invariant across environments. This is an extension of the ICP ($\textbf{I}$nvariant $\textbf{C}$ausal $\textbf{P}$rediction) principle by Peters et al. [2016], who assumed a fixed linear relationship across all environments. Within our proposed setting we provide sufficient conditions for identifiability of the causal parents and introduce a practical method called LoLICaP ($\textbf{Lo}$cally $\textbf{L}$inear $\textbf{I}$nvariant $\textbf{Ca}$usal $\textbf{P}$rediction), which is based on a hypothesis test for parent identification using a ratio of minimum and maximum statistics. We then show in a simplified setting that the statistical power of LoLICaP converges exponentially fast in the sample size, and finally we analyze the behavior of LoLICaP experimentally in more general settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05218v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Mey, Rui Manuel Castro</dc:creator>
    </item>
    <item>
      <title>Cross-sectional Dependence in Idiosyncratic Volatility</title>
      <link>https://arxiv.org/abs/2408.13437</link>
      <description>arXiv:2408.13437v1 Announce Type: cross 
Abstract: This paper introduces an econometric framework for analyzing cross-sectional dependence in the idiosyncratic volatilities of assets using high frequency data. We first consider the estimation of standard measures of dependence in the idiosyncratic volatilities such as covariances and correlations. Naive estimators of these measures are biased due to the use of the error-laden estimates of idiosyncratic volatilities. We provide bias-corrected estimators and the relevant asymptotic theory. Next, we introduce an idiosyncratic volatility factor model, in which we decompose the variation in idiosyncratic volatilities into two parts: the variation related to the systematic factors such as the market volatility, and the residual variation. Again, naive estimators of the decomposition are biased, and we provide bias-corrected estimators. We also provide the asymptotic theory that allows us to test whether the residual (non-systematic) components of the idiosyncratic volatilities exhibit cross-sectional dependence. We apply our methodology to the S&amp;P 100 index constituents, and document strong cross-sectional dependence in their idiosyncratic volatilities. We consider two different sets of idiosyncratic volatility factors, and find that neither can fully account for the cross-sectional dependence in idiosyncratic volatilities. For each model, we map out the network of dependencies in residual (non-systematic) idiosyncratic volatilities across all stocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13437v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilze Kalnina, Kokouvi Tewou</dc:creator>
    </item>
    <item>
      <title>Efficient Reinforced DAG Learning without Acyclicity Constraints</title>
      <link>https://arxiv.org/abs/2408.13448</link>
      <description>arXiv:2408.13448v1 Announce Type: cross 
Abstract: Unraveling cause-effect structures embedded in mere observational data is of great scientific interest, owning to the wealth of knowledge that can benefit from such structures. Recently, reinforcement learning (RL) has emerged as the enhancement for classical techniques to search for the most probable causal explanation in the form of a directed acyclic graph (DAG). Yet, effectively exploring the DAG space is challenging due to the vast number of candidates and the intricate constraint of acyclicity. In this study, we present REACT (REinforced DAG learning without acyclicity ConstrainTs)-a novel causal discovery approach fueled by the RL machinery with an efficient DAG generation policy. Through a novel parametrization of DAGs, which allows for directly mapping a real-valued vector to an adjacency matrix representing a valid DAG in a single step without enforcing any acyclicity constraint, we are able to navigate the search space much more effectively with policy gradient methods. In addition, our comprehensive numerical evaluations on a diverse set of both synthetic and real data confirm the effectiveness of our method compared with state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13448v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bao Duong, Hung Le, Thin Nguyen</dc:creator>
    </item>
    <item>
      <title>What if? Causal Machine Learning in Supply Chain Risk Management</title>
      <link>https://arxiv.org/abs/2408.13556</link>
      <description>arXiv:2408.13556v1 Announce Type: cross 
Abstract: The penultimate goal for developing machine learning models in supply chain management is to make optimal interventions. However, most machine learning models identify correlations in data rather than inferring causation, making it difficult to systematically plan for better outcomes. In this article, we propose and evaluate the use of causal machine learning for developing supply chain risk intervention models, and demonstrate its use with a case study in supply chain risk management in the maritime engineering sector. Our findings highlight that causal machine learning enhances decision-making processes by identifying changes that can be achieved under different supply chain interventions, allowing "what-if" scenario planning. We therefore propose different machine learning developmental pathways for for predicting risk, and planning for interventions to minimise risk and outline key steps for supply chain researchers to explore causal machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13556v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mateusz Wyrembek, George Baryannis, Alexandra Brintrup</dc:creator>
    </item>
    <item>
      <title>Change Point Detection in Pairwise Comparison Data with Covariates</title>
      <link>https://arxiv.org/abs/2408.13642</link>
      <description>arXiv:2408.13642v1 Announce Type: cross 
Abstract: This paper introduces the novel piecewise stationary covariate-assisted ranking estimation (PS-CARE) model for analyzing time-evolving pairwise comparison data, enhancing item ranking accuracy through the integration of covariate information. By partitioning the data into distinct, stationary segments, the PS-CARE model adeptly detects temporal shifts in item rankings, known as change points, whose number and positions are initially unknown. Leveraging the minimum description length (MDL) principle, this paper establishes a statistically consistent model selection criterion to estimate these unknowns. The practical optimization of this MDL criterion is done with the pruned exact linear time (PELT) algorithm. Empirical evaluations reveal the method's promising performance in accurately locating change points across various simulated scenarios. An application to an NBA dataset yielded meaningful insights that aligned with significant historical events, highlighting the method's practical utility and the MDL criterion's effectiveness in capturing temporal ranking changes. To the best of the authors' knowledge, this research pioneers change point detection in pairwise comparison data with covariate information, representing a significant leap forward in the field of dynamic ranking analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13642v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Han, Thomas C. M. Lee</dc:creator>
    </item>
    <item>
      <title>Inference on Consensus Ranking of Distributions</title>
      <link>https://arxiv.org/abs/2408.13949</link>
      <description>arXiv:2408.13949v1 Announce Type: cross 
Abstract: Instead of testing for unanimous agreement, I propose learning how broad of a consensus favors one distribution over another (of earnings, productivity, asset returns, test scores, etc.). Specifically, given a sample from each of two distributions, I propose statistical inference methods to learn about the set of utility functions for which the first distribution has higher expected utility than the second distribution. With high probability, an "inner" confidence set is contained within this true set, while an "outer" confidence set contains the true set. Such confidence sets can be formed by inverting a proposed multiple testing procedure that controls the familywise error rate. Theoretical justification comes from empirical process results, given that very large classes of utility functions are generally Donsker (subject to finite moments). The theory additionally justifies a uniform (over utility functions) confidence band of expected utility differences, as well as tests with a utility-based "restricted stochastic dominance" as either the null or alternative hypothesis. Simulated and empirical examples illustrate the methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13949v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/07350015.2023.2252040</arxiv:DOI>
      <arxiv:journal_reference>Journal of Business &amp; Economic Statistics 42 (2024) 839-850</arxiv:journal_reference>
      <dc:creator>David M. Kaplan</dc:creator>
    </item>
    <item>
      <title>Bayesian Cointegrated Panels in Digital Marketing</title>
      <link>https://arxiv.org/abs/2408.14012</link>
      <description>arXiv:2408.14012v1 Announce Type: cross 
Abstract: In this paper, we fully develop and apply a novel extension of Bayesian cointegrated panels modeling in digital marketing, particularly in modeling of a system where key ROI metrics such as clicks or impressions of a given digital campaign considered. Thus, in this context our goal is evaluating how the system reacts to investment perturbations due to changes in the investment strategy and its impact on the visibility of specific campaigns. To do so, we fit the model using a set of real marketing data with different investment campaigns over the same geographic territory. By employing forecast error variance decomposition, our findings indicate that clicks and impressions have a significant impact on session generation. Also, we evaluate our approach through a comprehensive simulation study that considers different processes. The results indicate that our proposal has substantial capabilities in terms of estimability and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14012v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan David Carranza-S\'anchez, Juan Sosa</dc:creator>
    </item>
    <item>
      <title>Score-based change point detection via tracking the best of infinitely many experts</title>
      <link>https://arxiv.org/abs/2408.14073</link>
      <description>arXiv:2408.14073v1 Announce Type: cross 
Abstract: We suggest a novel algorithm for online change point detection based on sequential score function estimation and tracking the best expert approach. The core of the procedure is a version of the fixed share forecaster for the case of infinite number of experts and quadratic loss functions. The algorithm shows a promising performance in numerical experiments on artificial and real-world data sets. We also derive new upper bounds on the dynamic regret of the fixed share forecaster with varying parameter, which are of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14073v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Markovich, Nikita Puchkin</dc:creator>
    </item>
    <item>
      <title>Consistent diffusion matrix estimation from population time series</title>
      <link>https://arxiv.org/abs/2408.14408</link>
      <description>arXiv:2408.14408v1 Announce Type: cross 
Abstract: Progress on modern scientific questions regularly depends on using large-scale datasets to understand complex dynamical systems. An especially challenging case that has grown to prominence with advances in single-cell sequencing technologies is learning the behavior of individuals from population snapshots. In the absence of individual-level time series, standard stochastic differential equation models are often nonidentifiable because intrinsic diffusion cannot be distinguished from measurement noise. Despite the difficulty, accurately recovering diffusion terms is required to answer even basic questions about the system's behavior. We show how to combine population-level time series with velocity measurements to build a provably consistent estimator of the diffusion matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14408v1</guid>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aden Forrow</dc:creator>
    </item>
    <item>
      <title>Markov-Restricted Analysis of Randomized Trials with Non-Monotone Missing Binary Outcomes</title>
      <link>https://arxiv.org/abs/2105.08868</link>
      <description>arXiv:2105.08868v3 Announce Type: replace 
Abstract: Scharfstein et al. (2021) developed a sensitivity analysis model for analyzing randomized trials with repeatedly measured binary outcomes that are subject to nonmonotone missingness. Their approach becomes computationally intractable when the number of measurements is large (e.g., greater than 15). In this paper, we repair this problem by introducing mth-order Markovian restrictions. We establish identification results for the joint distribution of the binary outcomes by representing the model as a directed acyclic graph (DAG). We develop a novel estimation strategy for a smooth functional of the joint distribution. We illustrate our methodology in the context of a randomized trial designed to evaluate a web-delivered psychosocial intervention to reduce substance use, assessed by evaluating abstinence twice weekly for 12 weeks, among patients entering outpatient addiction treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.08868v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaron J. R. Lee, Agatha S. Mallett, Ilya Shpitser, Aimee Campbell, Edward Nunes, Daniel O. Scharfstein</dc:creator>
    </item>
    <item>
      <title>Asymptotically Normal Estimation of Local Latent Network Curvature</title>
      <link>https://arxiv.org/abs/2211.11673</link>
      <description>arXiv:2211.11673v4 Announce Type: replace 
Abstract: Network data, commonly used throughout the physical, social, and biological sciences, consist of nodes (individuals) and the edges (interactions) between them. One way to represent network data's complex, high-dimensional structure is to embed the graph into a low-dimensional geometric space. The curvature of this space, in particular, provides insights about the structure in the graph, such as the propensity to form triangles or present tree-like structures. We derive an estimating function for curvature based on triangle side lengths and the length of the midpoint of a side to the opposing corner. We construct an estimator where the only input is a distance matrix and also establish asymptotic normality. We next introduce a novel latent distance matrix estimator for networks and an efficient algorithm to compute the estimate via solving iterative quadratic programs. We apply this method to the Los Alamos National Laboratory Unified Network and Host dataset and show how curvature estimates can be used to detect a red-team attack faster than naive methods, as well as discover non-constant latent curvature in co-authorship networks in physics. The code for this paper is available at https://github.com/SteveJWR/netcurve, and the methods are implemented in the R package https://github.com/SteveJWR/lolaR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11673v4</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Wilkins-Reeves, Tyler McCormick</dc:creator>
    </item>
    <item>
      <title>$\ell_1$-penalized Multinomial Regression: Estimation, inference, and prediction, with an application to risk factor identification for different dementia subtypes</title>
      <link>https://arxiv.org/abs/2302.02310</link>
      <description>arXiv:2302.02310v2 Announce Type: replace 
Abstract: High-dimensional multinomial regression models are very useful in practice but have received less research attention than logistic regression models, especially from the perspective of statistical inference. In this work, we analyze the estimation and prediction error of the contrast-based $\ell_1$-penalized multinomial regression model and extend the debiasing method to the multinomial case, providing a valid confidence interval for each coefficient and $p$-value of the individual hypothesis test. We also examine cases of model misspecification and non-identically distributed data to demonstrate the robustness of our method when some assumptions are violated. We apply the debiasing method to identify important predictors in the progression into dementia of different subtypes. Results from extensive simulations show the superiority of the debiasing method compared to other inference methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.02310v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Tian, Henry Rusinek, Arjun V. Masurkar, Yang Feng</dc:creator>
    </item>
    <item>
      <title>Inferring Covariance Structure from Multiple Data Sources via Subspace Factor Analysis</title>
      <link>https://arxiv.org/abs/2305.04113</link>
      <description>arXiv:2305.04113v3 Announce Type: replace 
Abstract: Factor analysis provides a canonical framework for imposing lower-dimensional structure such as sparse covariance in high-dimensional data. High-dimensional data on the same set of variables are often collected under different conditions, for instance in reproducing studies across research groups. In such cases, it is natural to seek to learn the shared versus condition-specific structure. Existing hierarchical extensions of factor analysis have been proposed, but face practical issues including identifiability problems. To address these shortcomings, we propose a class of SUbspace Factor Analysis (SUFA) models, which characterize variation across groups at the level of a lower-dimensional subspace. We prove that the proposed class of SUFA models lead to identifiability of the shared versus group-specific components of the covariance, and study their posterior contraction properties. Taking a Bayesian approach, these contributions are developed alongside efficient posterior computation algorithms. Our sampler fully integrates out latent variables, is easily parallelizable and has complexity that does not depend on sample size. We illustrate the methods through application to integration of multiple gene expression datasets relevant to immunology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04113v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noirrit Kiran Chandra, David B. Dunson, Jason Xu</dc:creator>
    </item>
    <item>
      <title>Collective Outlier Detection and Enumeration with Conformalized Closed Testing</title>
      <link>https://arxiv.org/abs/2308.05534</link>
      <description>arXiv:2308.05534v2 Announce Type: replace 
Abstract: This paper develops a flexible distribution-free method for collective outlier detection and enumeration, designed for situations in which the presence of outliers can be detected powerfully even though their precise identification may be challenging due to the sparsity, weakness, or elusiveness of their signals. This method builds upon recent developments in conformal inference and integrates classical ideas from other areas, including multiple testing, rank tests, and non-parametric large-sample asymptotics. The key innovation lies in developing a principled and effective approach for automatically choosing the most appropriate machine learning classifier and two-sample testing procedure for a given data set. The performance of our method is investigated through extensive empirical demonstrations, including an analysis of the LHCO high-energy particle collision data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05534v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara G. Magnani, Matteo Sesia, Aldo Solari</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification using Simulation Output: Batching as an Inferential Device</title>
      <link>https://arxiv.org/abs/2311.04159</link>
      <description>arXiv:2311.04159v2 Announce Type: replace 
Abstract: We present batching as an omnibus device for uncertainty quantification using simulation output. We consider the classical context of a simulationist performing uncertainty quantification on an estimator $\theta_n$ (of an unknown fixed quantity $\theta$) using only the output data $(Y_1,Y_2,\ldots,Y_n)$ gathered from a simulation. By uncertainty quantification, we mean approximating the sampling distribution of the error $\theta_n-\theta$ toward: (A) estimating an assessment functional $\psi$, e.g., bias, variance, or quantile; or (B) constructing a $(1-\alpha)$-confidence region on $\theta$. We argue that batching is a remarkably simple and effective device for this purpose, and is especially suited for handling dependent output data such as what one frequently encounters in simulation contexts. We demonstrate that if the number of batches and the extent of their overlap are chosen appropriately, batching retains bootstrap's attractive theoretical properties of strong consistency and higher-order accuracy. For constructing confidence regions, we characterize two limiting distributions associated with a Studentized statistic. Our extensive numerical experience confirms theoretical insight, especially about the effects of batch size and batch overlap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04159v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongseok Jeon, Yi Chu, Raghu Pasupathy, Sara Shashaani</dc:creator>
    </item>
    <item>
      <title>When accurate prediction models yield harmful self-fulfilling prophecies</title>
      <link>https://arxiv.org/abs/2312.01210</link>
      <description>arXiv:2312.01210v4 Announce Type: replace 
Abstract: Prediction models are popular in medical research and practice. By predicting an outcome of interest for specific patients, these models may help inform difficult treatment decisions, and are often hailed as the poster children for personalized, data-driven healthcare. We show however, that using prediction models for decision making can lead to harmful decisions, even when the predictions exhibit good discrimination after deployment. These models are harmful self-fulfilling prophecies: their deployment harms a group of patients but the worse outcome of these patients does not invalidate the predictive power of the model. Our main result is a formal characterization of a set of such prediction models. Next we show that models that are well calibrated before and after deployment are useless for decision making as they made no change in the data distribution. These results point to the need to revise standard practices for validation, deployment and evaluation of prediction models that are used in medical decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01210v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wouter A. C. van Amsterdam, Nan van Geloven, Jesse H. Krijthe, Rajesh Ranganath, Giovanni Cin\'a</dc:creator>
    </item>
    <item>
      <title>Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population</title>
      <link>https://arxiv.org/abs/2401.14512</link>
      <description>arXiv:2401.14512v4 Announce Type: replace 
Abstract: Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We apply our methodology to extend inferences from the Starting Treatment with Agonist Replacement Therapies (START) trial -- investigating the effectiveness of medication for opioid use disorder -- to the real-world population represented by the Treatment Episode Dataset: Admissions (TEDS-A). By refining target populations using ROOT, our framework offers a systematic approach to enhance decision-making accuracy and inform future trials in diverse populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14512v4</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Harsh Parikh, Rachael Ross, Elizabeth Stuart, Kara Rudolph</dc:creator>
    </item>
    <item>
      <title>Extract Mechanisms from Heterogeneous Effects: Identification Strategy for Mediation Analysis</title>
      <link>https://arxiv.org/abs/2403.04131</link>
      <description>arXiv:2403.04131v3 Announce Type: replace 
Abstract: Understanding causal mechanisms is crucial for explaining and generalizing empirical phenomena. Causal mediation analysis offers statistical techniques to quantify mediation effects. However, current methods often require multiple ignorability assumptions or sophisticated research designs. In this paper, we introduce a novel identification strategy that enables the simultaneous identification and estimation of treatment and mediation effects. This strategy is based on a new decomposition of total treatment effects and explores heterogeneous treatment effects. Monte Carlo simulations demonstrate that the method is more accurate and precise across various scenarios. To illustrate the efficiency and efficacy of our method, we apply it to estimate the causal mediation effects in two studies with distinct data structures, focusing on common pool resource governance and voting information. Additionally, we have developed statistical software to facilitate the implementation of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04131v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu</dc:creator>
    </item>
    <item>
      <title>Bridging Binarization: Causal Inference with Dichotomized Continuous Exposures</title>
      <link>https://arxiv.org/abs/2405.07109</link>
      <description>arXiv:2405.07109v3 Announce Type: replace 
Abstract: The average treatment effect (ATE) is a common parameter estimated in causal inference literature, but it is only defined for binary treatments. Thus, despite concerns raised by some researchers, many studies seeking to estimate the causal effect of a continuous treatment create a new binary treatment variable by dichotomizing the continuous values into two categories. In this paper, we affirm binarization as a statistically valid method for answering causal questions about continuous treatments by showing the equivalence between the binarized ATE and the difference in the average outcomes of two specific modified treatment policies. These policies impose cut-offs corresponding to the binarized treatment variable and assume preservation of relative self-selection. Relative self-selection is the ratio of the probability density of an individual having an exposure equal to one value of the continuous treatment variable versus another. The policies assume that, for any two values of the treatment variable with non-zero probability density after the cut-off, this ratio will remain unchanged. Through this equivalence, we clarify the assumptions underlying binarization and discuss how to properly interpret the resulting estimator. Additionally, we introduce a new target parameter that can be computed after binarization that considers the status-quo world. We argue that this parameter addresses more relevant causal questions than the traditional binarized ATE parameter. Finally, we present a simulation study to illustrate the implications of these assumptions when analyzing data and to demonstrate how to correctly implement estimators of the parameters discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07109v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaitlyn J. Lee, Alan Hubbard, Alejandro Schuler</dc:creator>
    </item>
    <item>
      <title>Bayesian Rank-Clustering</title>
      <link>https://arxiv.org/abs/2406.19563</link>
      <description>arXiv:2406.19563v2 Announce Type: replace 
Abstract: Traditional statistical inference on ordinal comparison data results in an overall ranking of objects, e.g., from best to worst, with each object having a unique rank. However, ranks of some objects may not be statistically distinguishable. This could happen due to insufficient data or to the true underlying object qualities being equal. Because uncertainty communication in estimates of overall rankings is notoriously difficult, we take a different approach and allow groups of objects to have equal ranks or be $\textit{rank-clustered}$ in our model. Existing models related to rank-clustering are limited by their inability to handle a variety of ordinal data types, to quantify uncertainty, or by the need to pre-specify the number and size of potential rank-clusters. We solve these limitations through our proposed Bayesian $\textit{Rank-Clustered Bradley-Terry-Luce}$ model. We accommodate rank-clustering via parameter fusion by imposing a novel spike-and-slab prior on object-specific worth parameters in Bradley-Terry-Luce family of distributions for ordinal comparisons. We demonstrate rank-clustering on simulated and real datasets in surveys, elections, and sports analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19563v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Pearce, Elena A. Erosheva</dc:creator>
    </item>
    <item>
      <title>Factorial Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2407.11937</link>
      <description>arXiv:2407.11937v2 Announce Type: replace 
Abstract: In many social science applications, researchers use the difference-in-differences (DID) estimator to establish causal relationships, exploiting cross-sectional variation in a baseline factor and temporal variation in exposure to an event that presumably may affect all units. This approach, which we term factorial DID (FDID), differs from canonical DID in that it lacks a clean control group unexposed to the event after the event occurs. In this paper, we clarify FDID as a research design in terms of its data structure, feasible estimands, and identifying assumptions that allow the DID estimator to recover these estimands. We frame FDID as a factorial design with two factors: the baseline factor, denoted by $G$, and the exposure level to the event, denoted by $Z$, and define the effect modification and causal interaction as the associative and causal effects of $G$ on the effect of $Z$, respectively. We show that under the canonical no anticipation and parallel trends assumptions, the DID estimator identifies only the effect modification of $G$ in FDID, and propose an additional factorial parallel trends assumption to identify the causal interaction. Moreover, we show that the canonical DID research design can be reframed as a special case of the FDID research design with an additional exclusion restriction assumption, thereby reconciling the two approaches. We extend this framework to allow conditionally valid parallel trends assumptions and multiple time periods, and clarify assumptions required to justify regression analysis under FDID. We illustrate these findings with empirical examples from economics and political science, and provide recommendations for improving practice and interpretation under FDID.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11937v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yiqing Xu, Anqi Zhao, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Multimodal data integration and cross-modal querying via orchestrated approximate message passing</title>
      <link>https://arxiv.org/abs/2407.19030</link>
      <description>arXiv:2407.19030v3 Announce Type: replace 
Abstract: The need for multimodal data integration arises naturally when multiple complementary sets of features are measured on the same sample. Under a dependent multifactor model, we develop a fully data-driven orchestrated approximate message passing algorithm for integrating information across these feature sets to achieve statistically optimal signal recovery. In practice, these reference data sets are often queried later by new subjects that are only partially observed. Leveraging on asymptotic normality of estimates generated by our data integration method, we further develop an asymptotically valid prediction set for the latent representation of any such query subject. We demonstrate the prowess of both the data integration and the prediction set construction algorithms on a tri-modal single-cell dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19030v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sagnik Nandy, Zongming Ma</dc:creator>
    </item>
    <item>
      <title>Experimental Design For Causal Inference Through An Optimization Lens</title>
      <link>https://arxiv.org/abs/2408.09607</link>
      <description>arXiv:2408.09607v2 Announce Type: replace 
Abstract: The study of experimental design offers tremendous benefits for answering causal questions across a wide range of applications, including agricultural experiments, clinical trials, industrial experiments, social experiments, and digital experiments. Although valuable in such applications, the costs of experiments often drive experimenters to seek more efficient designs. Recently, experimenters have started to examine such efficiency questions from an optimization perspective, as experimental design problems are fundamentally decision-making problems. This perspective offers a lot of flexibility in leveraging various existing optimization tools to study experimental design problems. This manuscript thus aims to examine the foundations of experimental design problems in the context of causal inference as viewed through an optimization lens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09607v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>Preregistration does not improve the transparent evaluation of severity in Popper's philosophy of science or when deviations are allowed</title>
      <link>https://arxiv.org/abs/2408.12347</link>
      <description>arXiv:2408.12347v2 Announce Type: replace 
Abstract: One justification for preregistering research hypotheses, methods, and analyses is that it improves the transparent evaluation of the severity of hypothesis tests. In this article, I consider two cases in which preregistration does not improve this evaluation. First, I argue that, although preregistration can facilitate the transparent evaluation of severity in Mayo's error statistical philosophy of science, it does not facilitate this evaluation in Popper's theory-centric approach. To illustrate, I show that associated concerns about Type I error rate inflation are only relevant in the error statistical approach and not in a theory-centric approach. Second, I argue that a preregistered test procedure that allows deviations in its implementation does not provide a more transparent evaluation of Mayoian severity than a non-preregistered procedure. In particular, I argue that sample-based validity-enhancing deviations cause an unknown inflation of the test procedure's Type I (familywise) error rate and, consequently, an unknown reduction in its capability to license inferences severely. I conclude that preregistration does not improve the transparent evaluation of severity in Popper's philosophy of science or when deviations are allowed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12347v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Rubin</dc:creator>
    </item>
    <item>
      <title>Linear multidimensional regression with interactive fixed-effects</title>
      <link>https://arxiv.org/abs/2209.11691</link>
      <description>arXiv:2209.11691v4 Announce Type: replace-cross 
Abstract: This paper studies a linear and additively separable model for multidimensional panel data of three or more dimensions with unobserved interactive fixed effects. Two approaches are considered to account for these unobserved interactive fixed-effects when estimating coefficients on the observed covariates. First, the model is embedded within the standard two dimensional panel framework and restrictions are formed under which the factor structure methods in Bai (2009) lead to consistent estimation of model parameters, but at slow rates of convergence. The second approach develops a kernel weighted fixed-effects method that is more robust to the multidimensional nature of the problem and can achieve the parametric rate of consistency under certain conditions. Theoretical results and simulations show some benefits to standard two-dimensional panel methods when the structure of the interactive fixed-effect term is known, but also highlight how the kernel weighted method performs well without knowledge of this structure. The methods are implemented to estimate the demand elasticity for beer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.11691v4</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Freeman</dc:creator>
    </item>
    <item>
      <title>Causal Estimation of Exposure Shifts with Neural Networks</title>
      <link>https://arxiv.org/abs/2302.02560</link>
      <description>arXiv:2302.02560v4 Announce Type: replace-cross 
Abstract: A fundamental task in causal inference is estimating the effect of distribution shift in the treatment variable. We refer to this problem as shift-response function (SRF) estimation. Existing neural network methods for causal inference lack theoretical guarantees and practical implementations for SRF estimation. In this paper, we introduce Targeted Regularization for Exposure Shifts with Neural Networks (TRESNET), a method to estimate SRFs with robustness and efficiency guarantees. Our contributions are twofold. First, we propose a targeted regularization loss for neural networks with theoretical properties that ensure double robustness and asymptotic efficiency specific to SRF estimation. Second, we extend targeted regularization to support loss functions from the exponential family to accommodate non-continuous outcome distributions (e.g., discrete counts). We conduct benchmark experiments demonstrating TRESNET's broad applicability and competitiveness. We then apply our method to a key policy question in public health to estimate the causal effect of revising the US National Ambient Air Quality Standards (NAAQS) for PM 2.5 from 12 ${\mu}g/m^3$ to 9 ${\mu}g/m^3$. This change has been recently proposed by the US Environmental Protection Agency (EPA). Our goal is to estimate the reduction in deaths that would result from this anticipated revision using data consisting of 68 million individuals across the U.S.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.02560v4</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ACM KDD 2024</arxiv:journal_reference>
      <dc:creator>Mauricio Tec, Kevin Josey, Oladimeji Mudele, Francesca Dominici</dc:creator>
    </item>
    <item>
      <title>On the Efficiency of Finely Stratified Experiments</title>
      <link>https://arxiv.org/abs/2307.15181</link>
      <description>arXiv:2307.15181v4 Announce Type: replace-cross 
Abstract: This paper studies the use of finely stratified designs for the efficient estimation of a large class of treatment effect parameters that arise in the analysis of experiments. By a "finely stratified" design, we mean experiments in which units are divided into groups of a fixed size and a proportion within each group is assigned to a binary treatment uniformly at random. The class of parameters considered are those that can be expressed as the solution to a set of moment conditions constructed using a known function of the observed data. They include, among other things, average treatment effects, quantile treatment effects, and local average treatment effects as well as the counterparts to these quantities in experiments in which the unit is itself a cluster. In this setting, we establish two results. First, we show that under a finely stratified design, the na\"ive method of moments estimator achieves the same asymptotic variance as what could typically be attained under alternative treatment assignment schemes only through ex post covariate adjustment. Second, we argue that in fact the na\"ive method of moments estimator under a finely stratified design is asymptotically efficient by deriving a lower bound on the asymptotic variance of "regular" estimators of the parameter of interest in the form of a convolution theorem. This result accommodates a large class of possible treatment assignment schemes that are used routinely throughout the sciences, such as stratified block randomization and matched pairs. In this sense, "finely stratified" experiments are attractive because they lead to efficient estimators of treatment effect parameters "by design" rather than through ex post covariate adjustment and thereby remain "hands above the table."</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15181v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Jizhou Liu, Azeem M. Shaikh, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>Estimating Lagged (Cross-)Covariance Operators of $L^p$-$m$-approximable Processes in Cartesian Product Hilbert Spaces</title>
      <link>https://arxiv.org/abs/2402.08110</link>
      <description>arXiv:2402.08110v5 Announce Type: replace-cross 
Abstract: Estimating parameters of functional ARMA, GARCH and invertible processes requires estimating lagged covariance and cross-covariance operators of Cartesian product Hilbert space-valued processes. Asymptotic results have been derived in recent years, either less generally or under a strict condition. This article derives upper bounds of the estimation errors for such operators based on the mild condition Lp-m-approximability for each lag, Cartesian power(s) and sample size, where the two processes can take values in different spaces in the context of lagged cross-covariance operators. Implications of our results on eigenelements, parameters in functional AR(MA) models and other general situations are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08110v5</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian K\"uhnert</dc:creator>
    </item>
    <item>
      <title>The Causal Chambers: Real Physical Systems as a Testbed for AI Methodology</title>
      <link>https://arxiv.org/abs/2404.11341</link>
      <description>arXiv:2404.11341v2 Announce Type: replace-cross 
Abstract: In some fields of AI, machine learning and statistics, the validation of new methods and algorithms is often hindered by the scarcity of suitable real-world datasets. Researchers must often turn to simulated data, which yields limited information about the applicability of the proposed methods to real problems. As a step forward, we have constructed two devices that allow us to quickly and inexpensively produce large datasets from non-trivial but well-understood physical systems. The devices, which we call causal chambers, are computer-controlled laboratories that allow us to manipulate and measure an array of variables from these physical systems, providing a rich testbed for algorithms from a variety of fields. We illustrate potential applications through a series of case studies in fields such as causal discovery, out-of-distribution generalization, change point detection, independent component analysis, and symbolic regression. For applications to causal inference, the chambers allow us to carefully perform interventions. We also provide and empirically validate a causal model of each chamber, which can be used as ground truth for different tasks. All hardware and software is made open source, and the datasets are publicly available at causalchamber.org or through the Python package causalchamber.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11341v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan L. Gamella, Jonas Peters, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion Probabilistic Models</title>
      <link>https://arxiv.org/abs/2404.17735</link>
      <description>arXiv:2404.17735v3 Announce Type: replace-cross 
Abstract: Diffusion probabilistic models (DPMs) have become the state-of-the-art in high-quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant research effort to improve image sample quality, there is little work on representation-controlled generation using diffusion models. Specifically, causal modeling and controllable counterfactual generation using DPMs is an underexplored area. In this work, we propose CausalDiffAE, a diffusion-based causal representation learning framework to enable counterfactual generation according to a specified causal model. Our key idea is to use an encoder to extract high-level semantically meaningful causal variables from high-dimensional data and model stochastic variation using reverse diffusion. We propose a causal encoding mechanism that maps high-dimensional data to causally related latent factors and parameterize the causal mechanisms among latent factors using neural networks. To enforce the disentanglement of causal variables, we formulate a variational objective and leverage auxiliary label information in a prior to regularize the latent space. We propose a DDIM-based counterfactual generation procedure subject to do-interventions. Finally, to address the limited label supervision scenario, we also study the application of CausalDiffAE when a part of the training data is unlabeled, which also enables granular control over the strength of interventions in generating counterfactuals during inference. We empirically show that CausalDiffAE learns a disentangled latent space and is capable of generating high-quality counterfactual images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17735v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aneesh Komanduri, Chen Zhao, Feng Chen, Xintao Wu</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data: Insights from Item Response Theory</title>
      <link>https://arxiv.org/abs/2405.00161</link>
      <description>arXiv:2405.00161v3 Announce Type: replace-cross 
Abstract: Analyses of heterogeneous treatment effects (HTE) are common in applied causal inference research. However, when outcomes are latent variables assessed via psychometric instruments such as educational tests, standard methods ignore the potential HTE that may exist among the individual items of the outcome measure. Failing to account for ``item-level'' HTE (IL-HTE) can lead to both estimated standard errors that are too small and identification challenges in the estimation of treatment-by-covariate interaction effects. We demonstrate how Item Response Theory (IRT) models that estimate a treatment effect for each assessment item can both address these challenges and provide new insights into HTE generally. This study articulates the theoretical rationale for the IL-HTE model and demonstrates its practical value using 73 data sets from 46 randomized controlled trials containing 5.8 million item responses in economics, education, and health research. Our results show that the IL-HTE model reveals item-level variation masked by single-number scores, provides more meaningful standard errors in many settings, allows for estimates of the generalizability of causal effects to untested items, resolves identification problems in the estimation of interaction effects, and provides estimates of standardized treatment effect sizes corrected for attenuation due to measurement error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00161v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua B. Gilbert, Zachary Himmelsbach, James Soland, Mridul Joshi, Benjamin W. Domingue</dc:creator>
    </item>
    <item>
      <title>Tackling GenAI Copyright Issues: Originality Estimation and Genericization</title>
      <link>https://arxiv.org/abs/2406.03341</link>
      <description>arXiv:2406.03341v4 Announce Type: replace-cross 
Abstract: The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While various techniques for mitigating copyright issues have been studied, significant risks remain. Here, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process. As a practical implementation, we introduce PREGen, which combines our genericization method with an existing mitigation technique. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images. Compared to the existing method, PREGen reduces the likelihood of generating copyrighted characters by more than half when the names of copyrighted characters are used as the prompt, dramatically improving the performance. Additionally, while generative models can produce copyrighted characters even when their names are not directly mentioned in the prompt, PREGen almost entirely prevents the generation of such characters in these cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03341v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroaki Chiba-Okabe, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Visual Analysis of Multi-outcome Causal Graphs</title>
      <link>https://arxiv.org/abs/2408.02679</link>
      <description>arXiv:2408.02679v2 Announce Type: replace-cross 
Abstract: We introduce a visual analysis method for multiple causal graphs with different outcome variables, namely, multi-outcome causal graphs. Multi-outcome causal graphs are important in healthcare for understanding multimorbidity and comorbidity. To support the visual analysis, we collaborated with medical experts to devise two comparative visualization techniques at different stages of the analysis process. First, a progressive visualization method is proposed for comparing multiple state-of-the-art causal discovery algorithms. The method can handle mixed-type datasets comprising both continuous and categorical variables and assist in the creation of a fine-tuned causal graph of a single outcome. Second, a comparative graph layout technique and specialized visual encodings are devised for the quick comparison of multiple causal graphs. In our visual analysis approach, analysts start by building individual causal graphs for each outcome variable, and then, multi-outcome causal graphs are generated and visualized with our comparative technique for analyzing differences and commonalities of these causal graphs. Evaluation includes quantitative measurements on benchmark datasets, a case study with a medical expert, and expert user studies with real-world health research data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02679v2</guid>
      <category>cs.LG</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengjie Fan, Jinlu Yu, Daniel Weiskopf, Nan Cao, Huai-Yu Wang, Liang Zhou</dc:creator>
    </item>
  </channel>
</rss>
