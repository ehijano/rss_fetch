<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Jun 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Generative AI Adoption in Classroom in Context of Technology Acceptance Model (TAM) and the Innovation Diffusion Theory (IDT)</title>
      <link>https://arxiv.org/abs/2406.15360</link>
      <description>arXiv:2406.15360v1 Announce Type: new 
Abstract: The burgeoning development of generative artificial intelligence (GenAI) and the widespread adoption of large language models (LLMs) in educational settings have sparked considerable debate regarding their efficacy and acceptability.Despite the potential benefits, the assimilation of these cutting-edge technologies among educators exhibits a broad spectrum of attitudes, from enthusiastic advocacy to profound skepticism.This study aims to dissect the underlying factors influencing educators' perceptions and acceptance of GenAI and LLMs.We conducted a survey among educators and analyzed the data through the frameworks of the Technology Acceptance Model (TAM) and Innovation Diffusion Theory (IDT). Our investigation reveals a strong positive correlation between the perceived usefulness of GenAI tools and their acceptance, underscoring the importance of demonstrating tangible benefits to educators. Additionally, the perceived ease of use emerged as a significant factor, though to a lesser extent, influencing acceptance. Our findings also show that the knowledge and acceptance of these tools is not uniform, suggesting that targeted strategies are required to address the specific needs and concerns of each adopter category to facilitate broader integration of AI tools.in education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15360v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aashish Ghimire, John Edwards</dc:creator>
    </item>
    <item>
      <title>Affirmative safety: An approach to risk management for high-risk AI</title>
      <link>https://arxiv.org/abs/2406.15371</link>
      <description>arXiv:2406.15371v1 Announce Type: new 
Abstract: Prominent AI experts have suggested that companies developing high-risk AI systems should be required to show that such systems are safe before they can be developed or deployed. The goal of this paper is to expand on this idea and explore its implications for risk management. We argue that entities developing or deploying high-risk AI systems should be required to present evidence of affirmative safety: a proactive case that their activities keep risks below acceptable thresholds. We begin the paper by highlighting global security risks from AI that have been acknowledged by AI experts and world governments. Next, we briefly describe principles of risk management from other high-risk fields (e.g., nuclear safety). Then, we propose a risk management approach for advanced AI in which model developers must provide evidence that their activities keep certain risks below regulator-set thresholds. As a first step toward understanding what affirmative safety cases should include, we illustrate how certain kinds of technical evidence and operational evidence can support an affirmative safety case. In the technical section, we discuss behavioral evidence (evidence about model outputs), cognitive evidence (evidence about model internals), and developmental evidence (evidence about the training process). In the operational section, we offer examples of organizational practices that could contribute to affirmative safety cases: information security practices, safety culture, and emergency response capacity. Finally, we briefly compare our approach to the NIST AI Risk Management Framework. Overall, we hope our work contributes to ongoing discussions about national and global security risks posed by AI and regulatory approaches to address these risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15371v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akash R. Wasil, Joshua Clymer, David Krueger, Emily Dardaman, Simeon Campos, Evan R. Murphy</dc:creator>
    </item>
    <item>
      <title>The TikToking troll and weaponization of conscience: A systems perspective case study</title>
      <link>https://arxiv.org/abs/2406.15372</link>
      <description>arXiv:2406.15372v1 Announce Type: new 
Abstract: Cybercrime is a pervasive threat that impacts every facet of society. Its reach transcends geographic borders and extends far beyond the digital realm, often serving as the catalyst for offline crimes. As modern conflicts become increasingly intertwined with cyber warfare, the need for interdisciplinary cooperation to grasp and combat this escalating threat is paramount. This case study centers around a controversial TikToker, highlighting how the weaponization of conscience can be leveraged to manipulate multiple actors within a propagandist's target population. Weaponization of conscience is a tactic used by fraudsters to camouflage their activity, deceive their victims, and extend the effectiveness of their modi operandi. Research shows that 95 percent of cybersecurity incidents are the result of human error and 90 percent begin with a phishing attempt. Honing the capacity to identify and dissect strategies employed by fraudsters along with how individual reactions unfold in the larger system is an essential skill for organizations and individuals to safeguard themselves. Understanding cybercrime and its many interconnected systems requires examination through the lens of complexity science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15372v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle Espinoza</dc:creator>
    </item>
    <item>
      <title>Occupation Life Cycle</title>
      <link>https://arxiv.org/abs/2406.15373</link>
      <description>arXiv:2406.15373v1 Announce Type: new 
Abstract: This paper explores the evolution of occupations within the context of industry and technology life cycles, highlighting the critical yet underexplored intersection between occupational trends and broader economic dynamics. Introducing the Occupation Life Cycle (OLC) model, we delineate five stages (i.e., growth, peak, fluctuation, maturity, and decline) to systematically explore the trajectory of occupations. Utilizing job posting data from one of China's largest recruitment platforms as a novel proxy, our study meticulously tracks the fluctuations and emerging trends in the labor market from 2018 to 2023. Through a detailed examination of representative roles, such as short video operators and data analysts, alongside emerging occupations within the artificial intelligence (AI) sector, our findings allocate occupations to specific life cycle stages, revealing insightful patterns of occupational development and decline. Our findings offer a unique perspective on the interplay between occupational evolution and economic factors, with a particular focus on the rapidly changing Chinese labor market. This study not only contributes to the theoretical understanding of OLC but also provides practical insights for policymakers, educators, and industry leaders facing the challenges of workforce planning and development in the face of technological advancement and market shifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15373v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lan Chen, Yufei Ji, Xichen Yao, Hengshu Zhu</dc:creator>
    </item>
    <item>
      <title>Hybrid Intelligence for Digital Humanities</title>
      <link>https://arxiv.org/abs/2406.15374</link>
      <description>arXiv:2406.15374v1 Announce Type: new 
Abstract: In this paper, we explore the synergies between Digital Humanities (DH) as a discipline and Hybrid Intelligence (HI) as a research paradigm. In DH research, the use of digital methods and specifically that of Artificial Intelligence is subject to a set of requirements and constraints. We argue that these are well-supported by the capabilities and goals of HI. Our contribution includes the identification of five such DH requirements: Successful AI systems need to be able to 1) collaborate with the (human) scholar; 2) support data criticism; 3) support tool criticism; 4) be aware of and cater to various perspectives and 5) support distant and close reading. We take the CARE principles of Hybrid Intelligence (collaborative, adaptive, responsible and explainable) as theoretical framework and map these to the DH requirements. In this mapping, we include example research projects. We finally address how insights from DH can be applied to HI and discuss open challenges for the combination of the two disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15374v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor de Boer, Lise Stork</dc:creator>
    </item>
    <item>
      <title>Barriers facing e-service adopting and implementation at local environment level in Nigeria</title>
      <link>https://arxiv.org/abs/2406.15375</link>
      <description>arXiv:2406.15375v1 Announce Type: new 
Abstract: E-Government services offer a great deal of potential to improve government activities and citizen support. However, there is a lack of research covering E-Government services at the local government level, particularly in developing countries. However, implementing successful E-Service technology in this part of the world will not come without its barriers considering the unstable and fragile economies in most developing countries. The research aim is to identify the barriers facing E-Service adoption and implementation at a local environment level, using Nigeria as a case example.
  This thesis adopts an interpretive paradigm and uses action research. It consists of a large field study in Nigeria (interviews), an online survey of government officials, online focus groups, and analyses government documents and E-Service initiatives. A structured literature review method consisted of sifting through 3,245 papers. The main theoretical tools used in this thesis are the diffusion of innovation (DOI) theory and the theory of change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15375v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kazeem Oluwakemi Oseni</dc:creator>
    </item>
    <item>
      <title>Crossing the principle-practice gap in AI ethics with ethical problem-solving</title>
      <link>https://arxiv.org/abs/2406.15376</link>
      <description>arXiv:2406.15376v1 Announce Type: new 
Abstract: The past years have presented a surge in (AI) development, fueled by breakthroughs in deep learning, increased computational power, and substantial investments in the field. Given the generative capabilities of more recent AI systems, the era of large-scale AI models has transformed various domains that intersect our daily lives. However, this progress raises concerns about the balance between technological advancement, ethical considerations, safety measures, and financial interests. Moreover, using such systems in sensitive areas amplifies our general ethical awareness, prompting a reemergence of debates on governance, regulation, and human values. However, amidst this landscape, how to bridge the principle-practice gap separating ethical discourse from the technical side of AI development remains an open problem. In response to this challenge, the present work proposes a framework to help shorten this gap: ethical problem-solving (EPS). EPS is a methodology promoting responsible, human-centric, and value-oriented AI development. The framework's core resides in translating principles into practical implementations using impact assessment surveys and a differential recommendation methodology. We utilize EPS as a blueprint to propose the implementation of Ethics as a Service Platform, which is currently available as a simple demonstration. We released all framework components openly and with a permissive license, hoping the community would adopt and extend our efforts into other contexts. Available in https://github.com/Nkluge\-correa/ethical\-problem\-solving</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15376v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s43681-024-00469-8</arxiv:DOI>
      <dc:creator>Nicholas Kluge Corr\^ea, James William Santos, Camila Galv\~ao, Marcelo Pasetti, Dieine Schiavon, Faizah Naqvi, Robayet Hossain, Nythamar De Oliveira</dc:creator>
    </item>
    <item>
      <title>Model Callers for Transforming Predictive and Generative AI Applications</title>
      <link>https://arxiv.org/abs/2406.15377</link>
      <description>arXiv:2406.15377v1 Announce Type: new 
Abstract: We introduce a novel software abstraction termed "model caller," acting as an intermediary for AI and ML model calling, advocating its transformative utility beyond existing model-serving frameworks. This abstraction offers multiple advantages: enhanced accuracy and reduced latency in model predictions, superior monitoring and observability of models, more streamlined AI system architectures, simplified AI development and management processes, and improved collaboration and accountability across AI/ML/Data Science, software, data, and operations teams. Model callers are valuable for both creators and users of models within both predictive and generative AI applications. Additionally, we have developed and released a prototype Python library for model callers, accessible for installation via pip or for download from GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15377v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mukesh Dalal</dc:creator>
    </item>
    <item>
      <title>CS1-LLM: Integrating LLMs into CS1 Instruction</title>
      <link>https://arxiv.org/abs/2406.15379</link>
      <description>arXiv:2406.15379v1 Announce Type: new 
Abstract: The recent, widespread availability of Large Language Models (LLMs) like ChatGPT and GitHub Copilot may impact introductory programming courses (CS1) both in terms of what should be taught and how to teach it. Indeed, recent research has shown that LLMs are capable of solving the majority of the assignments and exams we previously used in CS1. In addition, professional software engineers are often using these tools, raising the question of whether we should be training our students in their use as well. This experience report describes a CS1 course at a large research-intensive university that fully embraces the use of LLMs from the beginning of the course. To incorporate the LLMs, the course was intentionally altered to reduce emphasis on syntax and writing code from scratch. Instead, the course now emphasizes skills needed to successfully produce software with an LLM. This includes explaining code, testing code, and decomposing large problems into small functions that are solvable by an LLM. In addition to frequent, formative assessments of these skills, students were given three large, open-ended projects in three separate domains (data science, image processing, and game design) that allowed them to showcase their creativity in topics of their choosing. In an end-of-term survey, students reported that they appreciated learning with the assistance of the LLM and that they interacted with the LLM in a variety of ways when writing code. We provide lessons learned for instructors who may wish to incorporate LLMs into their course.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15379v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annapurna Vadaparty, Daniel Zingaro, David H. Smith IV, Mounika Padala, Christine Alvarado, Jamie Gorson Benario, Leo Porter</dc:creator>
    </item>
    <item>
      <title>Actions francophones autour des normes e-learning \`a l'ISO</title>
      <link>https://arxiv.org/abs/2406.15381</link>
      <description>arXiv:2406.15381v1 Announce Type: new 
Abstract: The future of e-Learning is on the way to be constructed within ICT standardization international instances. The sub-committee 36 of ISO, which is responsible for standardizing educational technologies, is certainly the most prominent of all. The authors of this paper, who are official delegates of the Agency of French Speaking Universities (AUF) with this structure, highlight the strategic importance of active monitoring of e-Learning standards for preserving cultural diversity, linguistic and equal access to education for all.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15381v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mokhtar Ben Henda (MICA, ISD, GRESIC, ISIC, Chaire Unesco-ITEN), Henri Hudrisier (MSHPN)</dc:creator>
    </item>
    <item>
      <title>Enhancing Educational Efficiency: Generative AI Chatbots and DevOps in Education 4.0</title>
      <link>https://arxiv.org/abs/2406.15382</link>
      <description>arXiv:2406.15382v1 Announce Type: new 
Abstract: This research paper will bring forth the innovative pedagogical approach in computer science education, which uses a combination of methodologies borrowed from Artificial Intelligence (AI) and DevOps to enhance the learning experience in Content Management Systems (CMS) Development. It has been done over three academic years, comparing the traditional way of teaching with the lately introduced AI-supported techniques. This had three structured sprints, each one of them covering the major parts of the sprint: object-oriented PHP, theme development, and plugin development. In each sprint, the student deals with part of the theoretical content and part of the practical task, using ChatGPT as an auxiliary tool. In that sprint, the model will provide solutions in code debugging and extensions of complex problems. The course includes practical examples like code replication with PHP, functionality expansion of the CMS, even development of custom plugins, and themes. The course practice includes versions' control with Git repositories. Efficiency will touch the theme and plugin output rates during development and mobile/web application development. Comparative analysis indicates that there is a marked increase in efficiency and shows effectiveness with the proposed AI- and DevOps-supported methodology. The study is very informative since education in computer science and its landscape change embodies an emerging technology that could have transformation impacts on amplifying the potential for scalable and adaptive learning approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15382v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edis Meki\'c, Mihailo Jovanovi\'c, Kristijan Kuk, Bojan Prlin\v{c}evi\'c, Ana Savi\'c</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence, VR, AR and Metaverse Technologies for Human Resources Management</title>
      <link>https://arxiv.org/abs/2406.15383</link>
      <description>arXiv:2406.15383v1 Announce Type: new 
Abstract: Human Resources (HR) technology solutions encompass software and hardware tools designed to automate HR processes, gather, process, and analyze data, utilize it for strategic decision-making, and execute HR professionals' tasks while prioritizing security and privacy considerations. As with numerous other domains, Digital Transformation and emerging technologies have commenced integration into HR processes. These technologies are utilized by HR professionals and various stakeholders involved in HR operations. This study evaluates the utilization of Artificial Intelligence (AI), Virtual Reality (VR), Augmented Reality (VR), and the Metaverse within HR management, focusing on current trends and potential opportunities. A survey was conducted to gauge HR professionals' perceptions and critiques regarding these technologies. Participants were the HR department officers, academicians who specialized in HR and staff who had courses at diverse levels about HR. The acquired results were subjected to comparative analysis within this article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15383v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omer Aydin, Enis Karaarslan, Nida Gokce Narin</dc:creator>
    </item>
    <item>
      <title>U Can't Gen This? A Survey of Intellectual Property Protection Methods for Data in Generative AI</title>
      <link>https://arxiv.org/abs/2406.15386</link>
      <description>arXiv:2406.15386v1 Announce Type: new 
Abstract: Large Generative AI (GAI) models have the unparalleled ability to generate text, images, audio, and other forms of media that are increasingly indistinguishable from human-generated content. As these models often train on publicly available data, including copyrighted materials, art and other creative works, they inadvertently risk violating copyright and misappropriation of intellectual property (IP). Due to the rapid development of generative AI technology and pressing ethical considerations from stakeholders, protective mechanisms and techniques are emerging at a high pace but lack systematisation.
  In this paper, we study the concerns regarding the intellectual property rights of training data and specifically focus on the properties of generative models that enable misuse leading to potential IP violations. Then we propose a taxonomy that leads to a systematic review of technical solutions for safeguarding the data from intellectual property violations in GAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15386v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tanja \v{S}ar\v{c}evi\'c (SBA Research), Alicja Karlowicz (SBA Research), Rudolf Mayer (SBA Research), Ricardo Baeza-Yates (EAI, Northeastern University), Andreas Rauber (TU Wien)</dc:creator>
    </item>
    <item>
      <title>Examining the Legal Status of Digital Assets as Property: A Comparative Analysis of Jurisdictional Approaches</title>
      <link>https://arxiv.org/abs/2406.15391</link>
      <description>arXiv:2406.15391v1 Announce Type: new 
Abstract: This paper examines the complex legal landscape surrounding digital assets, analysing how they are defined and regulated as property across various jurisdictions. As digital assets such as cryptocurrencies and non-fungible tokens (NFTs) increasingly integrate with global economies, their intangible nature presents unique challenges to traditional property law concepts, necessitating a re-evaluation of legal definitions and ownership frameworks. This research presents a comparative analysis, reviewing how different legal systems classify and manage digital assets within property law, highlighting the variations in regulatory approaches and their implications on ownership, transfer, and inheritance rights. By examining seminal cases and regulatory developments in major jurisdictions, including the United States, the European Union, and Singapore, this paper explores the emerging trends and potential legal evolutions that could influence the global handling of digital assets. The study aims to contribute to the scholarly discourse by proposing a harmonized approach to digital asset regulation, seeking to balance innovation with legal certainty and consumer protection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15391v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2139/ssrn.4807135</arxiv:DOI>
      <dc:creator>Luke Lee</dc:creator>
    </item>
    <item>
      <title>An Automated SQL Query Grading System Using An Attention-Based Convolutional Neural Network</title>
      <link>https://arxiv.org/abs/2406.15936</link>
      <description>arXiv:2406.15936v1 Announce Type: new 
Abstract: Grading SQL queries can be a time-consuming, tedious and challenging task, especially as the number of student submissions increases. Several systems have been introduced in an attempt to mitigate these challenges, but those systems have their own limitations. This paper describes our novel approach to automating the process of grading SQL queries. Unlike previous approaches, we employ a unique convolutional neural network architecture that employs a parameter-sharing approach for different machine learning tasks that enables the architecture to induce different knowledge representations of the data to increase its potential for understanding SQL statements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15936v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Donald R. Schwartz, Pablo Rivas</dc:creator>
    </item>
    <item>
      <title>TikTok Engagement Traces Over Time and Health Risky Behaviors: Combining Data Linkage and Computational Methods</title>
      <link>https://arxiv.org/abs/2406.15991</link>
      <description>arXiv:2406.15991v1 Announce Type: new 
Abstract: Digital technologies and social algorithms are revolutionizing the media landscape, altering how we select and consume health information. Extending the selectivity paradigm with research on social media engagement, the convergence perspective, and algorithmic impact, this study investigates how individuals' liked TikTok videos on various health-risk topics are associated with their vaping and drinking behaviors. Methodologically, we relied on data linkage to objectively measure selective engagement on social media, which involves combining survey self-reports with digital traces from TikTok interactions for the consented respondents (n = 166). A computational analysis of 13,724 health-related videos liked by these respondents from 2020 to 2023 was conducted. Our findings indicate that users who initially liked drinking-related content on TikTok are inclined to favor more of such videos over time, with their likes on smoking, drinking, and fruit and vegetable videos influencing their self-reported vaping and drinking behaviors. Our study highlights the methodological value of combining digital traces, computational analysis, and self-reported data for a more objective examination of social media consumption and engagement, as well as a more ecologically valid understanding of social media's behavioral impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15991v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinyan Zhao, Chau-Wai Wong</dc:creator>
    </item>
    <item>
      <title>Pervasive Technology-Enabled Care and Support for People with Dementia: The State of Art and Research Issues</title>
      <link>https://arxiv.org/abs/2406.16138</link>
      <description>arXiv:2406.16138v1 Announce Type: new 
Abstract: Dementia is a mental illness that people live with all across the world. No one is immune. Nothing can predict its onset. The true story of dementia remains unknown globally, partly due to the denial of dementia symptoms and partly due to the social stigma attached to the disease. In recent years, dementia as a mental illness has received a lot of attention from the scientific community and healthcare providers. This paper presents a state of art survey of pervasive technology enabled care and support for people suffering from Alzheimers dementia. We identify three areas of pervasive technology support for dementia patients, focusing on care, wellness and active living. A critical analysis of existing research is presented here, exploring how pervasive computing, artificial intelligence (AI) and the Internet of Things (IoT) are already supporting and providing comfort to dementia patients, particularly those living alone in the community. The work discusses key challenges and limitations of technology-enabled support owing to reasons like lack of accessibility, availability, usability and affordability of technology, limited holistic care approach, and lack of education and information. Future research directions focusing on how pervasive and connected healthcare can better support the well being and mental health impacts of Alzheimers dementia are also highlighted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16138v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayan Kumar Ray, Geri Harris, Akbar Hossain, NZ Jhanjhi</dc:creator>
    </item>
    <item>
      <title>Thinking beyond Bias: Analyzing Multifaceted Impacts and Implications of AI on Gendered Labour</title>
      <link>https://arxiv.org/abs/2406.16207</link>
      <description>arXiv:2406.16207v1 Announce Type: new 
Abstract: Artificial Intelligence with its multifaceted technologies and integral role in global production significantly impacts gender dynamics particularly in gendered labor. This paper emphasizes the need to explore AIs broader impacts on gendered labor beyond its current emphasis on the generation and perpetuation of epistemic biases. We draw attention to how the AI industry as an integral component of the larger economic structure is transforming the nature of work. It is expanding the prevalence of platform based work models and exacerbating job insecurity particularly for women. Of critical concern is the increasing exclusion of women from meaningful engagement in the digital labor force. This issue often overlooked demands urgent attention from the AI research community. Understanding AIs multifaceted role in gendered labor requires a nuanced examination of economic transformation and its implications for gender equity. By shedding light on these intersections this paper aims to stimulate in depth discussions and catalyze targeted actions aimed at mitigating the gender disparities accentuated by AI driven transformations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16207v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satyam Mohla, Bishnupriya Bagh, Anupam Guha</dc:creator>
    </item>
    <item>
      <title>Public Constitutional AI</title>
      <link>https://arxiv.org/abs/2406.16696</link>
      <description>arXiv:2406.16696v1 Announce Type: new 
Abstract: We are increasingly subjected to the power of AI authorities. As AI decisions become inescapable, entering domains such as healthcare, education, and law, we must confront a vital question: how can we ensure AI systems have the legitimacy necessary for effective governance? This essay argues that to secure AI legitimacy, we need methods that engage the public in designing and constraining AI systems, ensuring these technologies reflect the community's shared values. Constitutional AI, proposed by Anthropic, represents a step towards this goal, offering a model for democratic control of AI. However, while Constitutional AI's commitment to hardcoding explicit principles into AI models enhances transparency and accountability, it falls short in two crucial aspects: addressing the opacity of individual AI decisions and fostering genuine democratic legitimacy. To overcome these limitations, this essay proposes "Public Constitutional AI." This approach envisions a participatory process where diverse stakeholders, including ordinary citizens, deliberate on the principles guiding AI development. The resulting "AI Constitution" would carry the legitimacy of popular authorship, grounding AI governance in the public will. Furthermore, the essay proposes "AI Courts" to develop "AI case law," providing concrete examples for operationalizing constitutional principles in AI training. This evolving combination of constitutional principles and case law aims to make AI governance more responsive to public values. By grounding AI governance in deliberative democratic processes, Public Constitutional AI offers a path to imbue automated authorities with genuine democratic legitimacy, addressing the unique challenges posed by increasingly powerful AI systems while ensuring their alignment with the public interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16696v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gilad Abiri</dc:creator>
    </item>
    <item>
      <title>A Carrying Capacity Calculator for Pedestrians Using OpenStreetMap Data: Application to Urban Tourism and Public Spaces</title>
      <link>https://arxiv.org/abs/2406.16781</link>
      <description>arXiv:2406.16781v1 Announce Type: new 
Abstract: Determining the carrying capacity of urban tourism destinations and public spaces is essential for sustainable management. This paper presents an online tool that calculates pedestrian carrying capacities for user-defined areas based on OpenStreetMap (OSM) data. The tool considers physical, real, and effective carrying capacities by incorporating parameters such as area per pedestrian, rotation factor, corrective factors, and management capacity. The carrying capacity calculator aids in balancing environmental, economic, social, and experiential factors to prevent overcrowding and preserve the quality of life for residents and visitors. This tool is particularly useful for tourism destination management, urban planning, and event management, ensuring positive visitor experiences and sustainable infrastructure development. We detail the implementation of the calculator, its underlying algorithm, and its application to the Santa Maria Maior parish in Lisbon, highlighting its effectiveness in managing urban tourism and public spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16781v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Duarte Sampaio de Almeida, Rodrigo Sim\~oes, Fernando Brito e Abreu, Adriano Lopes, In\^es Boavida-Portugal</dc:creator>
    </item>
    <item>
      <title>Blockchain for Academic Integrity: Developing the Blockchain Academic Credential Interoperability Protocol (BACIP)</title>
      <link>https://arxiv.org/abs/2406.15482</link>
      <description>arXiv:2406.15482v1 Announce Type: cross 
Abstract: This research introduces the Blockchain Academic Credential Interoperability Protocol (BACIP), designed to significantly enhance the security, privacy, and interoperability of verifying academic credentials globally, addressing the widespread issue of academic fraud. BACIP integrates dual blockchain architecture, smart contracts, and zero-knowledge proofs to offer a scalable and transparent framework aimed at reducing fraud and improving the mobility and opportunities for students and professionals worldwide. The research methodology adopts a mixed-methods approach, involving a rigorous review of pertinent literature and systematic integration of advanced technological components. This includes both qualitative and quantitative analyses that underpin the development of a universally compatible system. Preliminary evaluations suggest that BACIP could enhance verification efficiency and bolster security against tampering and unauthorized access. While the theoretical framework and practical implementations have laid a solid foundation, the protocol's real-world efficacy awaits empirical validation in a production environment. Future research will focus on deploying a prototype, establishing robust validation policies, and defining precise testing parameters. This critical phase is indispensable for a thorough assessment of BACIP's operational robustness and its compliance with international educational standards. This work contributes significantly to the academic field by proposing a robust model for managing and safeguarding academic credentials, thus laying a strong foundation for further innovation in credential verification using blockchain technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15482v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan A. Berrios Moya</dc:creator>
    </item>
    <item>
      <title>JobFair: A Framework for Benchmarking Gender Hiring Bias in Large Language Models</title>
      <link>https://arxiv.org/abs/2406.15484</link>
      <description>arXiv:2406.15484v1 Announce Type: cross 
Abstract: This paper presents a novel framework for benchmarking hierarchical gender hiring bias in Large Language Models (LLMs) for resume scoring, revealing significant issues of reverse bias and overdebiasing. Our contributions are fourfold: First, we introduce a framework using a real, anonymized resume dataset from the Healthcare, Finance, and Construction industries, meticulously used to avoid confounding factors. It evaluates gender hiring biases across hierarchical levels, including Level bias, Spread bias, Taste-based bias, and Statistical bias. This framework can be generalized to other social traits and tasks easily. Second, we propose novel statistical and computational hiring bias metrics based on a counterfactual approach, including Rank After Scoring (RAS), Rank-based Impact Ratio, Permutation Test-Based Metrics, and Fixed Effects Model-based Metrics. These metrics, rooted in labor economics, NLP, and law, enable holistic evaluation of hiring biases. Third, we analyze hiring biases in ten state-of-the-art LLMs. Six out of ten LLMs show significant biases against males in healthcare and finance. An industry-effect regression reveals that the healthcare industry is the most biased against males. GPT-4o and GPT-3.5 are the most biased models, showing significant bias in all three industries. Conversely, Gemini-1.5-Pro, Llama3-8b-Instruct, and Llama3-70b-Instruct are the least biased. The hiring bias of all LLMs, except for Llama3-8b-Instruct and Claude-3-Sonnet, remains consistent regardless of random expansion or reduction of resume content. Finally, we offer a user-friendly demo to facilitate adoption and practical application of the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15484v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ze Wang, Zekun Wu, Xin Guan, Michael Thaler, Adriano Koshiyama, Skylar Lu, Sachin Beepath, Ediz Ertekin Jr., Maria Perez-Ortiz</dc:creator>
    </item>
    <item>
      <title>Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods</title>
      <link>https://arxiv.org/abs/2406.15583</link>
      <description>arXiv:2406.15583v1 Announce Type: cross 
Abstract: Large language models (LLMs) have advanced to a point that even humans have difficulty discerning whether a text was generated by another human, or by a computer. However, knowing whether a text was produced by human or artificial intelligence (AI) is important to determining its trustworthiness, and has applications in many domains including detecting fraud and academic dishonesty, as well as combating the spread of misinformation and political propaganda. The task of AI-generated text (AIGT) detection is therefore both very challenging, and highly critical. In this survey, we summarize state-of-the art approaches to AIGT detection, including watermarking, statistical and stylistic analysis, and machine learning classification. We also provide information about existing datasets for this task. Synthesizing the research findings, we aim to provide insight into the salient factors that combine to determine how "detectable" AIGT text is under different scenarios, and to make practical recommendations for future work towards this significant technical and societal challenge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15583v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kathleen C. Fraser, Hillary Dawkins, Svetlana Kiritchenko</dc:creator>
    </item>
    <item>
      <title>Privacy Requirements and Realities of Digital Public Goods</title>
      <link>https://arxiv.org/abs/2406.15842</link>
      <description>arXiv:2406.15842v1 Announce Type: cross 
Abstract: In the international development community, the term "digital public goods" is used to describe open-source digital products (e.g., software, datasets) that aim to address the United Nations (UN) Sustainable Development Goals. DPGs are increasingly being used to deliver government services around the world (e.g., ID management, healthcare registration). Because DPGs may handle sensitive data, the UN has established user privacy as a first-order requirement for DPGs. The privacy risks of DPGs are currently managed in part by the DPG standard, which includes a prerequisite questionnaire with questions designed to evaluate a DPG's privacy posture.
  This study examines the effectiveness of the current DPG standard for ensuring adequate privacy protections. We present a systematic assessment of responses from DPGs regarding their protections of users' privacy. We also present in-depth case studies from three widely-used DPGs to identify privacy threats and compare this to their responses to the DPG standard. Our findings reveal limitations in the current DPG standard's evaluation approach. We conclude by presenting preliminary recommendations and suggestions for strengthening the DPG standard as it relates to privacy. Additionally, we hope this study encourages more usable privacy research on communicating privacy, not only to end users but also third-party adopters of user-facing technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15842v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geetika Gopi, Aadyaa Maddi, Omkhar Arasaratnam, Giulia Fanti</dc:creator>
    </item>
    <item>
      <title>Fair Clustering: Critique, Caveats, and Future Directions</title>
      <link>https://arxiv.org/abs/2406.15960</link>
      <description>arXiv:2406.15960v1 Announce Type: cross 
Abstract: Clustering is a fundamental problem in machine learning and operations research. Therefore, given the fact that fairness considerations have become of paramount importance in algorithm design, fairness in clustering has received significant attention from the research community. The literature on fair clustering has resulted in a collection of interesting fairness notions and elaborate algorithms. In this paper, we take a critical view of fair clustering, identifying a collection of ignored issues such as the lack of a clear utility characterization and the difficulty in accounting for the downstream effects of a fair clustering algorithm in machine learning settings. In some cases, we demonstrate examples where the application of a fair clustering algorithm can have significant negative impacts on social welfare. We end by identifying a collection of steps that would lead towards more impactful research in fair clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15960v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Dickerson, Seyed A. Esmaeili, Jamie Morgenstern, Claire Jie Zhang</dc:creator>
    </item>
    <item>
      <title>The Persistence of Contrarianism on Twitter: Mapping users' sharing habits for the Ukraine war, COVID-19 vaccination, and the 2020 Midterm Elections</title>
      <link>https://arxiv.org/abs/2406.16175</link>
      <description>arXiv:2406.16175v1 Announce Type: cross 
Abstract: Empirical studies of online disinformation emphasize matters of public concern such as the COVID-19 pandemic, foreign election interference, and the Russo-Ukraine war, largely in studies that treat the topics separately. Comparatively fewer studies attempt to relate such disparate topics and address the extent to which they share behaviors. In this study, we compare three samples of Twitter data on COVID-19 vaccination, the Ukraine war and the 2020 midterm elections, to ascertain how distinct ideological stances of users across the three samples might be related. Our results indicate the emergence of a broad contrarian stance that is defined by its opposition to public health narratives/policies along with the Biden administration's foreign policy stances. Sharing activity within the contrarian position falls on a spectrum with outright conspiratorial content on one end. We confirm the existence of ideologically coherent cross-subject stances among Twitter users, but in a manner not squarely aligned with right-left political orientations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16175v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Axelrod, Sangyeon Kim, John Paolillo</dc:creator>
    </item>
    <item>
      <title>Semi-Variance Reduction for Fair Federated Learning</title>
      <link>https://arxiv.org/abs/2406.16193</link>
      <description>arXiv:2406.16193v1 Announce Type: cross 
Abstract: Ensuring fairness in a Federated Learning (FL) system, i.e., a satisfactory performance for all of the participating diverse clients, is an important and challenging problem. There are multiple fair FL algorithms in the literature, which have been relatively successful in providing fairness. However, these algorithms mostly emphasize on the loss functions of worst-off clients to improve their performance, which often results in the suppression of well-performing ones. As a consequence, they usually sacrifice the system's overall average performance for achieving fairness. Motivated by this and inspired by two well-known risk modeling methods in Finance, Mean-Variance and Mean-Semi-Variance, we propose and study two new fair FL algorithms, Variance Reduction (VRed) and Semi-Variance Reduction (SemiVRed). VRed encourages equality between clients' loss functions by penalizing their variance. In contrast, SemiVRed penalizes the discrepancy of only the worst-off clients' loss functions from the average loss. Through extensive experiments on multiple vision and language datasets, we show that, SemiVRed achieves SoTA performance in scenarios with heterogeneous data distributions and improves both fairness and system overall average performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16193v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saber Malekmohammadi</dc:creator>
    </item>
    <item>
      <title>Does Cross-Cultural Alignment Change the Commonsense Morality of Language Models?</title>
      <link>https://arxiv.org/abs/2406.16316</link>
      <description>arXiv:2406.16316v1 Announce Type: cross 
Abstract: Alignment of the language model with human preferences is a common approach to making a language model useful to end users. However, most alignment work is done in English, and human preference datasets are dominated by English, reflecting only the preferences of English-speaking annotators. Nevertheless, it is common practice to use the English preference data, either directly or by translating it into the target language, when aligning a multilingual language model. The question is whether such an alignment strategy marginalizes the preference of non-English speaking users. To this end, we investigate the effect of aligning Japanese language models with (mostly) English resources. In particular, we focus on evaluating whether the commonsense morality of the resulting fine-tuned models is aligned with Japanese culture using the JCommonsenseMorality (JCM) and ETHICS datasets. The experimental results show that the fine-tuned model outperforms the SFT model. However, it does not demonstrate the same level of improvement as a model fine-tuned using the JCM, suggesting that while some aspects of commonsense morality are transferable, others may not be.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16316v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuu Jinnai</dc:creator>
    </item>
    <item>
      <title>SyROCCo: Enhancing Systematic Reviews using Machine Learning</title>
      <link>https://arxiv.org/abs/2406.16527</link>
      <description>arXiv:2406.16527v1 Announce Type: cross 
Abstract: The sheer number of research outputs published every year makes systematic reviewing increasingly time- and resource-intensive. This paper explores the use of machine learning techniques to help navigate the systematic review process. ML has previously been used to reliably 'screen' articles for review - that is, identify relevant articles based on reviewers' inclusion criteria. The application of ML techniques to subsequent stages of a review, however, such as data extraction and evidence mapping, is in its infancy. We therefore set out to develop a series of tools that would assist in the profiling and analysis of 1,952 publications on the theme of 'outcomes-based contracting'. Tools were developed for the following tasks: assign publications into 'policy area' categories; identify and extract key information for evidence mapping, such as organisations, laws, and geographical information; connect the evidence base to an existing dataset on the same topic; and identify subgroups of articles that may share thematic content. An interactive tool using these techniques and a public dataset with their outputs have been released. Our results demonstrate the utility of ML techniques to enhance evidence accessibility and analysis within the systematic review processes. These efforts show promise in potentially yielding substantial efficiencies for future systematic reviewing and for broadening their analytical scope. Our work suggests that there may be implications for the ease with which policymakers and practitioners can access evidence. While ML techniques seem poised to play a significant role in bridging the gap between research and policy by offering innovative ways of gathering, accessing, and analysing data from systematic reviews, we also highlight their current limitations and the need to exercise caution in their application, particularly given the potential for errors and biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16527v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Fang, Miguel Arana-Catania, Felix-Anselm van Lier, Juliana Outes Velarde, Harry Bregazzi, Mara Airoldi, Eleanor Carter, Rob Procter</dc:creator>
    </item>
    <item>
      <title>Efficient k-means with Individual Fairness via Exponential Tilting</title>
      <link>https://arxiv.org/abs/2406.16557</link>
      <description>arXiv:2406.16557v1 Announce Type: cross 
Abstract: In location-based resource allocation scenarios, the distances between each individual and the facility are desired to be approximately equal, thereby ensuring fairness. Individually fair clustering is often employed to achieve the principle of treating all points equally, which can be applied in these scenarios. This paper proposes a novel algorithm, tilted k-means (TKM), aiming to achieve individual fairness in clustering. We integrate the exponential tilting into the sum of squared errors (SSE) to formulate a novel objective function called tilted SSE. We demonstrate that the tilted SSE can generalize to SSE and employ the coordinate descent and first-order gradient method for optimization. We propose a novel fairness metric, the variance of the distances within each cluster, which can alleviate the Matthew Effect typically caused by existing fairness metrics. Our theoretical analysis demonstrates that the well-known k-means++ incurs a multiplicative error of O(k log k), and we establish the convergence of TKM under mild conditions. In terms of fairness, we prove that the variance generated by TKM decreases with a scaled hyperparameter. In terms of efficiency, we demonstrate the time complexity is linear with the dataset size. Our experiments demonstrate that TKM outperforms state-of-the-art methods in effectiveness, fairness, and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16557v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shengkun Zhu, Jinshan Zeng, Yuan Sun, Sheng Wang, Xiaodong Li, Zhiyong Peng</dc:creator>
    </item>
    <item>
      <title>Measuring the Recyclability of Electronic Components to Assist Automatic Disassembly and Sorting Waste Printed Circuit Boards</title>
      <link>https://arxiv.org/abs/2406.16593</link>
      <description>arXiv:2406.16593v1 Announce Type: cross 
Abstract: The waste of electrical and electronic equipment has been increased due to the fast evolution of technology products and competition of many IT sectors. Every year millions of tons of electronic waste are thrown into the environment which causes high consequences for human health. Therefore, it is crucial to control this waste flow using technology, especially using Artificial Intelligence but also reclamation of critical raw materials for new production processes. In this paper, we focused on the measurement of recyclability of waste electronic components (WECs) from waste printed circuit boards (WPCBs) using mathematical innovation model. This innovative approach evaluates both the recyclability and recycling difficulties of WECs, integrating an AI model for improved disassembly and sorting. Assessing the recyclability of individual electronic components present on WPCBs provides insight into the recovery potential of valuable materials and indicates the level of complexity involved in recycling in terms of economic worth and production utility. This novel measurement approach helps AI models in accurately determining the number of classes to be identified and sorted during the automated disassembly of discarded PCBs. It also facilitates the model in iterative training and validation of individual electronic components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16593v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Mohsin, Xianlai Zeng, Stefano Rovetta, Francesco Masulli</dc:creator>
    </item>
    <item>
      <title>Cherry on the Cake: Fairness is NOT an Optimization Problem</title>
      <link>https://arxiv.org/abs/2406.16606</link>
      <description>arXiv:2406.16606v1 Announce Type: cross 
Abstract: Fair cake-cutting is a mathematical subfield that studies the problem of fairly dividing a resource among a number of participants. The so-called ``cake,'' as an object, represents any resource that can be distributed among players. This concept is connected to supervised multi-label classification: any dataset can be thought of as a cake that needs to be distributed, where each label is a player that receives its share of the dataset. In particular, any efficient cake-cutting solution for the dataset is equivalent to an optimal decision function. Although we are not the first to demonstrate this connection, the important ramifications of this parallel seem to have been partially forgotten. We revisit these classical results and demonstrate how this connection can be prolifically used for fairness in machine learning problems. Understanding the set of achievable fair decisions is a fundamental step in finding optimal fair solutions and satisfying fairness requirements. By employing the tools of cake-cutting theory, we have been able to describe the behavior of optimal fair decisions, which, counterintuitively, often exhibit quite unfair properties. Specifically, in order to satisfy fairness constraints, it is sometimes preferable, in the name of optimality, to purposefully make mistakes and deny giving the positive label to deserving individuals in a community in favor of less worthy individuals within the same community. This practice is known in the literature as cherry-picking and has been described as ``blatantly unfair.''</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16606v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Favier, Toon Calders</dc:creator>
    </item>
    <item>
      <title>Learning Interpretable Fair Representations</title>
      <link>https://arxiv.org/abs/2406.16698</link>
      <description>arXiv:2406.16698v1 Announce Type: cross 
Abstract: Numerous approaches have been recently proposed for learning fair representations that mitigate unfair outcomes in prediction tasks. A key motivation for these methods is that the representations can be used by third parties with unknown objectives. However, because current fair representations are generally not interpretable, the third party cannot use these fair representations for exploration, or to obtain any additional insights, besides the pre-contracted prediction tasks. Thus, to increase data utility beyond prediction tasks, we argue that the representations need to be fair, yet interpretable. We propose a general framework for learning interpretable fair representations by introducing an interpretable "prior knowledge" during the representation learning process. We implement this idea and conduct experiments with ColorMNIST and Dsprite datasets. The results indicate that in addition to being interpretable, our representations attain slightly higher accuracy and fairer outcomes in a downstream classification task compared to state-of-the-art fair representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16698v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianhao Wang, Zana Bu\c{c}inca, Zilin Ma</dc:creator>
    </item>
    <item>
      <title>Inducing Group Fairness in LLM-Based Decisions</title>
      <link>https://arxiv.org/abs/2406.16738</link>
      <description>arXiv:2406.16738v1 Announce Type: cross 
Abstract: Prompting Large Language Models (LLMs) has created new and interesting means for classifying textual data. While evaluating and remediating group fairness is a well-studied problem in classifier fairness literature, some classical approaches (e.g., regularization) do not carry over, and some new opportunities arise (e.g., prompt-based remediation). We measure fairness of LLM-based classifiers on a toxicity classification task, and empirically show that prompt-based classifiers may lead to unfair decisions. We introduce several remediation techniques and benchmark their fairness and performance trade-offs. We hope our work encourages more research on group fairness in LLM-based classifiers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16738v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James Atwood, Preethi Lahoti, Ananth Balashankar, Flavien Prost, Ahmad Beirami</dc:creator>
    </item>
    <item>
      <title>Addressing Polarization and Unfairness in Performative Prediction</title>
      <link>https://arxiv.org/abs/2406.16756</link>
      <description>arXiv:2406.16756v1 Announce Type: cross 
Abstract: When machine learning (ML) models are used in applications that involve humans (e.g., online recommendation, school admission, hiring, lending), the model itself may trigger changes in the distribution of targeted data it aims to predict. Performative prediction (PP) is a framework that explicitly considers such model-dependent distribution shifts when learning ML models. While significant efforts have been devoted to finding performative stable (PS) solutions in PP for system robustness, their societal implications are less explored and it is unclear whether PS solutions are aligned with social norms such as fairness. In this paper, we set out to examine the fairness property of PS solutions in performative prediction. We first show that PS solutions can incur severe polarization effects and group-wise loss disparity. Although existing fairness mechanisms commonly used in literature can help mitigate unfairness, they may fail and disrupt the stability under model-dependent distribution shifts. We thus propose novel fairness intervention mechanisms that can simultaneously achieve both stability and fairness in PP settings. Both theoretical analysis and experiments are provided to validate the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16756v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kun Jin, Tian Xie, Yang Liu, Xueru Zhang</dc:creator>
    </item>
    <item>
      <title>Data Debiasing with Datamodels (D3M): Improving Subgroup Robustness via Data Selection</title>
      <link>https://arxiv.org/abs/2406.16846</link>
      <description>arXiv:2406.16846v1 Announce Type: cross 
Abstract: Machine learning models can fail on subgroups that are underrepresented during training. While techniques such as dataset balancing can improve performance on underperforming groups, they require access to training group annotations and can end up removing large portions of the dataset. In this paper, we introduce Data Debiasing with Datamodels (D3M), a debiasing approach which isolates and removes specific training examples that drive the model's failures on minority groups. Our approach enables us to efficiently train debiased classifiers while removing only a small number of examples, and does not require training group annotations or additional hyperparameter tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16846v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saachi Jain, Kimia Hamidieh, Kristian Georgiev, Andrew Ilyas, Marzyeh Ghassemi, Aleksander Madry</dc:creator>
    </item>
    <item>
      <title>Potential Societal Biases of ChatGPT in Higher Education: A Scoping Review</title>
      <link>https://arxiv.org/abs/2311.14381</link>
      <description>arXiv:2311.14381v2 Announce Type: replace 
Abstract: Purpose:Generative Artificial Intelligence (GAI) models, such as ChatGPT, may inherit or amplify societal biases due to their training on extensive datasets. With the increasing usage of GAI by students, faculty, and staff in higher education institutions (HEIs), it is urgent to examine the ethical issues and potential biases associated with these technologies. Design/Approach/Methods:This scoping review aims to elucidate how biases related to GAI in HEIs have been researched and discussed in recent academic publications. We categorized the potential societal biases that GAI might cause in the field of higher education. Our review includes articles written in English, Chinese, and Japanese across four main databases, focusing on GAI usage in higher education and bias. Findings:Our findings reveal that while there is meaningful scholarly discussion around bias and discrimination concerning LLMs in the AI field, most articles addressing higher education approach the issue superficially. Few articles identify specific types of bias under different circumstances, and there is a notable lack of empirical research. Most papers in our review focus primarily on educational and research fields related to medicine and engineering, with some addressing English education. However, there is almost no discussion regarding the humanities and social sciences. Additionally, a significant portion of the current discourse is in English and primarily addresses English-speaking contexts. Originality/Value:To the best of our knowledge, our study is the first to summarize the potential societal biases in higher education. This review highlights the need for more in-depth studies and empirical work to understand the specific biases that GAI might introduce or amplify in educational settings, guiding the development of more ethical AI applications in higher education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14381v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ming Li, Ariunaa Enkhtur, Beverley Anne Yamamoto, Fei Cheng</dc:creator>
    </item>
    <item>
      <title>The Efficacy of Conversational Artificial Intelligence in Rectifying the Theory of Mind and Autonomy Biases: Comparative Analysis</title>
      <link>https://arxiv.org/abs/2406.13813</link>
      <description>arXiv:2406.13813v2 Announce Type: replace 
Abstract: The study evaluates the efficacy of Conversational Artificial Intelligence (CAI) in rectifying cognitive biases and recognizing affect in human-AI interactions, which is crucial for digital mental health interventions. Cognitive biases (systematic deviations from normative thinking) affect mental health, intensifying conditions like depression and anxiety. Therapeutic chatbots can make cognitive-behavioral therapy (CBT) more accessible and affordable, offering scalable and immediate support. The research employs a structured methodology with clinical-based virtual case scenarios simulating typical user-bot interactions. Performance and affect recognition were assessed across two categories of cognitive biases: theory of mind biases (anthropomorphization of AI, overtrust in AI, attribution to AI) and autonomy biases (illusion of control, fundamental attribution error, just-world hypothesis). A qualitative feedback mechanism was used with an ordinal scale to quantify responses based on accuracy, therapeutic quality, and adherence to CBT principles. Therapeutic bots (Wysa, Youper) and general-use LLMs (GTP 3.5, GTP 4, Gemini Pro) were evaluated through scripted interactions, double-reviewed by cognitive scientists and a clinical psychologist. Statistical analysis showed therapeutic bots were consistently outperformed by non-therapeutic bots in bias rectification and in 4 out of 6 biases in affect recognition. The data suggests that non-therapeutic chatbots are more effective in addressing some cognitive biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13813v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marcin Rz\k{a}deczka, Anna Sterna, Julia Stoli\'nska, Paulina Kaczy\'nska, Marcin Moskalewicz</dc:creator>
    </item>
    <item>
      <title>Causal Fair Machine Learning via Rank-Preserving Interventional Distributions</title>
      <link>https://arxiv.org/abs/2307.12797</link>
      <description>arXiv:2307.12797v2 Announce Type: replace-cross 
Abstract: A decision can be defined as fair if equal individuals are treated equally and unequals unequally. Adopting this definition, the task of designing machine learning (ML) models that mitigate unfairness in automated decision-making systems must include causal thinking when introducing protected attributes: Following a recent proposal, we define individuals as being normatively equal if they are equal in a fictitious, normatively desired (FiND) world, where the protected attributes have no (direct or indirect) causal effect on the target. We propose rank-preserving interventional distributions to define a specific FiND world in which this holds and a warping method for estimation. Evaluation criteria for both the method and the resulting ML model are presented and validated through simulations. Experiments on empirical data showcase the practical application of our method and compare results with "fairadapt" (Ple\v{c}ko and Meinshausen, 2020), a different approach for mitigating unfairness by causally preprocessing data that uses quantile regression forests. With this, we show that our warping approach effectively identifies the most discriminated individuals and mitigates unfairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12797v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 1st Workshop on Fairness and Bias in AI co-located with 26th European Conference on Artificial Intelligence (ECAI 2023), CEUR Workshop Proceedings, https://ceur-ws.org/Vol-3523/</arxiv:journal_reference>
      <dc:creator>Ludwig Bothmann, Susanne Dandl, Michael Schomaker</dc:creator>
    </item>
    <item>
      <title>Open Models, Closed Minds? On Agents Capabilities in Mimicking Human Personalities through Open Large Language Models</title>
      <link>https://arxiv.org/abs/2401.07115</link>
      <description>arXiv:2401.07115v2 Announce Type: replace-cross 
Abstract: The emergence of unveiling human-like behaviors in Large Language Models (LLMs) has led to a closer connection between NLP and human psychology. Scholars have been studying the inherent personalities exhibited by LLMs and attempting to incorporate human traits and behaviors into them. However, these efforts have primarily focused on commercially-licensed LLMs, neglecting the widespread use and notable advancements seen in Open LLMs. This work aims to address this gap by employing a set of 12 LLM Agents based on the most representative Open models and subject them to a series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test and the Big Five Inventory (BFI) test. Our approach involves evaluating the intrinsic personality traits of Open LLM agents and determining the extent to which these agents can mimic human personalities when conditioned by specific personalities and roles. Our findings unveil that $(i)$ each Open LLM agent showcases distinct human personalities; $(ii)$ personality-conditioned prompting produces varying effects on the agents, with only few successfully mirroring the imposed personality, while most of them being ``closed-minded'' (i.e., they retain their intrinsic traits); and $(iii)$ combining role and personality conditioning can enhance the agents' ability to mimic human personalities. Our work represents a step up in understanding the dense relationship between NLP and human psychology through the lens of Open LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07115v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucio La Cava, Andrea Tagarelli</dc:creator>
    </item>
    <item>
      <title>LLM-Assisted Content Conditional Debiasing for Fair Text Embedding</title>
      <link>https://arxiv.org/abs/2402.14208</link>
      <description>arXiv:2402.14208v3 Announce Type: replace-cross 
Abstract: Mitigating biases in machine learning models has become an increasing concern in Natural Language Processing (NLP), particularly in developing fair text embeddings, which are crucial yet challenging for real-world applications like search engines. In response, this paper proposes a novel method for learning fair text embeddings. First, we define a novel content-conditional equal distance (CCED) fairness for text embeddings, ensuring content-conditional independence between sensitive attributes and text embeddings. Building on CCED, we introduce a content-conditional debiasing (CCD) loss to ensure that embeddings of texts with different sensitive attributes but identical content maintain the same distance from the embedding of their corresponding neutral text. Additionally, we tackle the issue of insufficient training data by using Large Language Models (LLMs) with instructions to fairly augment texts into different sensitive groups. Our extensive evaluations show that our approach effectively enhances fairness while maintaining the utility of embeddings. Furthermore, our augmented dataset, combined with the CCED metric, serves as an new benchmark for evaluating fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14208v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenlong Deng, Blair Chen, Beidi Zhao, Chiyu Zhang, Xiaoxiao Li, Christos Thrampoulidis</dc:creator>
    </item>
    <item>
      <title>ClaimVer: Explainable Claim-Level Verification and Evidence Attribution of Text Through Knowledge Graphs</title>
      <link>https://arxiv.org/abs/2403.09724</link>
      <description>arXiv:2403.09724v2 Announce Type: replace-cross 
Abstract: In the midst of widespread misinformation and disinformation through social media and the proliferation of AI-generated texts, it has become increasingly difficult for people to validate and trust information they encounter. Many fact-checking approaches and tools have been developed, but they often lack appropriate explainability or granularity to be useful in various contexts. A text validation method that is easy to use, accessible, and can perform fine-grained evidence attribution has become crucial. More importantly, building user trust in such a method requires presenting the rationale behind each prediction, as research shows this significantly influences people's belief in automated systems. Localizing and bringing users' attention to the specific problematic content is also paramount, instead of providing simple blanket labels. In this paper, we present ClaimVer, a human-centric framework tailored to meet users' informational and verification needs by generating rich annotations and thereby reducing cognitive load. Designed to deliver comprehensive evaluations of texts, it highlights each claim, verifies it against a trusted knowledge graph (KG), presents the evidence, and provides succinct, clear explanations for each claim prediction. Finally, our framework introduces an attribution score, enhancing applicability across a wide range of downstream tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09724v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Preetam Prabhu Srikar Dammu, Himanshu Naidu, Mouly Dewan, YoungMin Kim, Tanya Roosta, Aman Chadha, Chirag Shah</dc:creator>
    </item>
    <item>
      <title>Accurately Classifying Out-Of-Distribution Data in Facial Recognition</title>
      <link>https://arxiv.org/abs/2404.03876</link>
      <description>arXiv:2404.03876v2 Announce Type: replace-cross 
Abstract: Standard classification theory assumes that the distribution of images in the test and training sets are identical. Unfortunately, real-life scenarios typically feature unseen data ("out-of-distribution data") which is different from data in the training distribution("in-distribution"). This issue is most prevalent in social justice problems where data from under-represented groups may appear in the test data without representing an equal proportion of the training data. This may result in a model returning confidently wrong decisions and predictions. We are interested in the following question: Can the performance of a neural network improve on facial images of out-of-distribution data when it is trained simultaneously on multiple datasets of in-distribution data? We approach this problem by incorporating the Outlier Exposure model and investigate how the model's performance changes when other datasets of facial images were implemented. We observe that the accuracy and other metrics of the model can be increased by applying Outlier Exposure, incorporating a trainable weight parameter to increase the machine's emphasis on outlier images, and by re-weighting the importance of different class labels. We also experimented with whether sorting the images and determining outliers via image features would have more of an effect on the metrics than sorting by average pixel value. Our goal was to make models not only more accurate but also more fair by scanning a more expanded range of images. We also tested the datasets in reverse order to see whether a more fair dataset with balanced features has an effect on the model's accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03876v2</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianluca Barone, Aashrit Cunchala, Rudy Nunez</dc:creator>
    </item>
    <item>
      <title>ALERT: A Comprehensive Benchmark for Assessing Large Language Models' Safety through Red Teaming</title>
      <link>https://arxiv.org/abs/2404.08676</link>
      <description>arXiv:2404.08676v3 Announce Type: replace-cross 
Abstract: When building Large Language Models (LLMs), it is paramount to bear safety in mind and protect them with guardrails. Indeed, LLMs should never generate content promoting or normalizing harmful, illegal, or unethical behavior that may contribute to harm to individuals or society. This principle applies to both normal and adversarial use. In response, we introduce ALERT, a large-scale benchmark to assess safety based on a novel fine-grained risk taxonomy. It is designed to evaluate the safety of LLMs through red teaming methodologies and consists of more than 45k instructions categorized using our novel taxonomy. By subjecting LLMs to adversarial testing scenarios, ALERT aims to identify vulnerabilities, inform improvements, and enhance the overall safety of the language models. Furthermore, the fine-grained taxonomy enables researchers to perform an in-depth evaluation that also helps one to assess the alignment with various policies. In our experiments, we extensively evaluate 10 popular open- and closed-source LLMs and demonstrate that many of them still struggle to attain reasonable levels of safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08676v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Simone Tedeschi, Felix Friedrich, Patrick Schramowski, Kristian Kersting, Roberto Navigli, Huu Nguyen, Bo Li</dc:creator>
    </item>
    <item>
      <title>Mapping the Potential of Explainable Artificial Intelligence (XAI) for Fairness Along the AI Lifecycle</title>
      <link>https://arxiv.org/abs/2404.18736</link>
      <description>arXiv:2404.18736v3 Announce Type: replace-cross 
Abstract: The widespread use of artificial intelligence (AI) systems across various domains is increasingly highlighting issues related to algorithmic fairness, especially in high-stakes scenarios. Thus, critical considerations of how fairness in AI systems might be improved, and what measures are available to aid this process, are overdue. Many researchers and policymakers see explainable AI (XAI) as a promising way to increase fairness in AI systems. However, there is a wide variety of XAI methods and fairness conceptions expressing different desiderata, and the precise connections between XAI and fairness remain largely nebulous. Besides, different measures to increase algorithmic fairness might be applicable at different points throughout an AI system's lifecycle. Yet, there currently is no coherent mapping of fairness desiderata along the AI lifecycle. In this paper, we set out to bridge both these gaps: We distill eight fairness desiderata, map them along the AI lifecycle, and discuss how XAI could help address each of them. We hope to provide orientation for practical applications and to inspire XAI research specifically focused on these fairness desiderata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18736v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luca Deck, Astrid Schom\"acker, Timo Speith, Jakob Sch\"offer, Lena K\"astner, Niklas K\"uhl</dc:creator>
    </item>
    <item>
      <title>Human-AI Safety: A Descendant of Generative AI and Control Systems Safety</title>
      <link>https://arxiv.org/abs/2405.09794</link>
      <description>arXiv:2405.09794v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) is interacting with people at an unprecedented scale, offering new avenues for immense positive impact, but also raising widespread concerns around the potential for individual and societal harm. Today, the predominant paradigm for human--AI safety focuses on fine-tuning the generative model's outputs to better agree with human-provided examples or feedback. In reality, however, the consequences of an AI model's outputs cannot be determined in isolation: they are tightly entangled with the responses and behavior of human users over time. In this paper, we distill key complementary lessons from AI safety and control systems safety, highlighting open challenges as well as key synergies between both fields. We then argue that meaningful safety assurances for advanced AI technologies require reasoning about how the feedback loop formed by AI outputs and human behavior may drive the interaction towards different outcomes. To this end, we introduce a unifying formalism to capture dynamic, safety-critical human--AI interactions and propose a concrete technical roadmap towards next-generation human-centered AI safety.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09794v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Bajcsy, Jaime F. Fisac</dc:creator>
    </item>
    <item>
      <title>A survey on fairness of large language models in e-commerce: progress, application, and challenge</title>
      <link>https://arxiv.org/abs/2405.13025</link>
      <description>arXiv:2405.13025v2 Announce Type: replace-cross 
Abstract: This survey explores the fairness of large language models (LLMs) in e-commerce, examining their progress, applications, and the challenges they face. LLMs have become pivotal in the e-commerce domain, offering innovative solutions and enhancing customer experiences. This work presents a comprehensive survey on the applications and challenges of LLMs in e-commerce. The paper begins by introducing the key principles underlying the use of LLMs in e-commerce, detailing the processes of pretraining, fine-tuning, and prompting that tailor these models to specific needs. It then explores the varied applications of LLMs in e-commerce, including product reviews, where they synthesize and analyze customer feedback; product recommendations, where they leverage consumer data to suggest relevant items; product information translation, enhancing global accessibility; and product question and answer sections, where they automate customer support. The paper critically addresses the fairness challenges in e-commerce, highlighting how biases in training data and algorithms can lead to unfair outcomes, such as reinforcing stereotypes or discriminating against certain groups. These issues not only undermine consumer trust, but also raise ethical and legal concerns. Finally, the work outlines future research directions, emphasizing the need for more equitable and transparent LLMs in e-commerce. It advocates for ongoing efforts to mitigate biases and improve the fairness of these systems, ensuring they serve diverse global markets effectively and ethically. Through this comprehensive analysis, the survey provides a holistic view of the current landscape of LLMs in e-commerce, offering insights into their potential and limitations, and guiding future endeavors in creating fairer and more inclusive e-commerce environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13025v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingyang Ren, Zilin Jiang, Jinghan Cao, Sijia Li, Chiqu Li, Yiyang Liu, Shuning Huo, Tiange He, Yuan Chen</dc:creator>
    </item>
    <item>
      <title>What Do Privacy Advertisements Communicate to Consumers?</title>
      <link>https://arxiv.org/abs/2405.13857</link>
      <description>arXiv:2405.13857v2 Announce Type: replace-cross 
Abstract: When companies release marketing materials aimed at promoting their privacy practices or highlighting specific privacy features, what do they actually communicate to consumers? In this paper, we explore the impact of privacy marketing on: (1) consumers' attitudes toward the organizations providing the campaigns, (2) overall privacy awareness, and (3) the actionability of suggested privacy advice. To this end, we investigated the impact of four privacy advertising videos and one privacy game published by five different technology companies. We conducted 24 semi-structured interviews with participants randomly assigned to view one or two of the videos or play the game. Our findings suggest that awareness of privacy features can contribute to positive perceptions of a company or its products. The ads we tested were more successful in communicating the advertised privacy features than the game we tested. We observed that advertising a single privacy feature using a single metaphor in a short ad increased awareness of the advertised feature. The game failed to communicate privacy features or motivate study participants to use the features. Our results also suggest that privacy campaigns can be useful for raising awareness about privacy features and improving brand image, but may not be the most effective way to teach viewers how to use privacy features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13857v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoxin Shen, Eman Alashwali, Lorrie Faith Cranor</dc:creator>
    </item>
    <item>
      <title>Harmful Speech Detection by Language Models Exhibits Gender-Queer Dialect Bias</title>
      <link>https://arxiv.org/abs/2406.00020</link>
      <description>arXiv:2406.00020v2 Announce Type: replace-cross 
Abstract: Content moderation on social media platforms shapes the dynamics of online discourse, influencing whose voices are amplified and whose are suppressed. Recent studies have raised concerns about the fairness of content moderation practices, particularly for aggressively flagging posts from transgender and non-binary individuals as toxic. In this study, we investigate the presence of bias in harmful speech classification of gender-queer dialect online, focusing specifically on the treatment of reclaimed slurs. We introduce a novel dataset, QueerReclaimLex, based on 109 curated templates exemplifying non-derogatory uses of LGBTQ+ slurs. Dataset instances are scored by gender-queer annotators for potential harm depending on additional context about speaker identity. We systematically evaluate the performance of five off-the-shelf language models in assessing the harm of these texts and explore the effectiveness of chain-of-thought prompting to teach large language models (LLMs) to leverage author identity context. We reveal a tendency for these models to inaccurately flag texts authored by gender-queer individuals as harmful. Strikingly, across all LLMs the performance is poorest for texts that show signs of being written by individuals targeted by the featured slur (F1 &lt;= 0.24). We highlight an urgent need for fairness and inclusivity in content moderation systems. By uncovering these biases, this work aims to inform the development of more equitable content moderation practices and contribute to the creation of inclusive online spaces for all users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00020v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebecca Dorn, Lee Kezar, Fred Morstatter, Kristina Lerman</dc:creator>
    </item>
  </channel>
</rss>
