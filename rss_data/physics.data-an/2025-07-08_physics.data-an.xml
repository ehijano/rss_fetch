<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Jul 2025 01:44:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Information-theoretic Quantification of High-order Feature Effects in Classification Problems</title>
      <link>https://arxiv.org/abs/2507.04362</link>
      <description>arXiv:2507.04362v1 Announce Type: cross 
Abstract: Understanding the contribution of individual features in predictive models remains a central goal in interpretable machine learning, and while many model-agnostic methods exist to estimate feature importance, they often fall short in capturing high-order interactions and disentangling overlapping contributions. In this work, we present an information-theoretic extension of the High-order interactions for Feature importance (Hi-Fi) method, leveraging Conditional Mutual Information (CMI) estimated via a k-Nearest Neighbor (kNN) approach working on mixed discrete and continuous random variables. Our framework decomposes feature contributions into unique, synergistic, and redundant components, offering a richer, model-independent understanding of their predictive roles. We validate the method using synthetic datasets with known Gaussian structures, where ground truth interaction patterns are analytically derived, and further test it on non-Gaussian and real-world gene expression data from TCGA-BRCA. Results indicate that the proposed estimator accurately recovers theoretical and expected findings, providing a potential use case for developing feature selection algorithms or model development based on interaction analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04362v1</guid>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ivan Lazic, Chiara Bar\`a, Marta Iovino, Sebastiano Stramaglia, Niksa Jakovljevic, Luca Faes</dc:creator>
    </item>
    <item>
      <title>The Hitchhiker's Guide to Differential Dynamic Microscopy</title>
      <link>https://arxiv.org/abs/2507.05058</link>
      <description>arXiv:2507.05058v1 Announce Type: cross 
Abstract: Over nearly two decades, Differential Dynamic Microscopy (DDM) has become a standard technique for extracting dynamic correlation functions from time-lapse microscopy data, with applications spanning colloidal suspensions, polymer solutions, active fluids, and biological systems. In its most common implementation, DDM analyzes image sequences acquired with a conventional microscope equipped with a digital camera, yielding time- and wavevector-resolved information analogous to that obtained in multi-angle Dynamic Light Scattering (DLS). With a widening array of applications and a growing, heterogeneous user base, lowering the technical barrier to performing DDM has become a central objective. In this tutorial article, we provide a step-by-step guide to conducting DDM experiments -- from planning and acquisition to data analysis -- and introduce the open-source software package fastDDM, designed to efficiently process large image datasets. fastDDM employs optimized, parallel algorithms that reduce analysis times by up to four orders of magnitude on typical datasets (e.g., 10,000 frames), thereby enabling high-throughput workflows and making DDM more broadly accessible across disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05058v1</guid>
      <category>cond-mat.soft</category>
      <category>physics.bio-ph</category>
      <category>physics.data-an</category>
      <category>physics.optics</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enrico Lattuada, Fabian Krautgasser, Maxime Lavaud, Fabio Giavazzi, Roberto Cerbino</dc:creator>
    </item>
    <item>
      <title>A COMPASS to Model Comparison and Simulation-Based Inference in Galactic Chemical Evolution</title>
      <link>https://arxiv.org/abs/2507.05060</link>
      <description>arXiv:2507.05060v2 Announce Type: cross 
Abstract: We present COMPASS, a novel simulation-based inference framework that combines score-based diffusion models with transformer architectures to jointly perform parameter estimation and Bayesian model comparison across competing Galactic Chemical Evolution (GCE) models. COMPASS handles high-dimensional, incomplete, and variable-size stellar abundance datasets. Applied to high-precision elemental abundance measurements, COMPASS evaluates 40 combinations of nucleosynthetic yield tables. The model strongly favours Asymptotic Giant Branch yields from NuGrid and core-collapse SN yields used in the IllustrisTNG simulation, achieving near-unity cumulative posterior probability. Using the preferred model, we infer a steep high-mass IMF slope and an elevated Supernova Ia normalization, consistent with prior solar neighbourhood studies but now derived from fully amortized Bayesian inference. Our results demonstrate that modern SBI methods can robustly constrain uncertain physics in astrophysical simulators and enable principled model selection when analysing complex, simulation-based data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05060v2</guid>
      <category>astro-ph.GA</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Berkay Gunes, Sven Buder, Tobias Buck</dc:creator>
    </item>
    <item>
      <title>Physics-Guided Dual Implicit Neural Representations for Source Separation</title>
      <link>https://arxiv.org/abs/2507.05249</link>
      <description>arXiv:2507.05249v1 Announce Type: cross 
Abstract: Significant challenges exist in efficient data analysis of most advanced experimental and observational techniques because the collected signals often include unwanted contributions--such as background and signal distortions--that can obscure the physically relevant information of interest. To address this, we have developed a self-supervised machine-learning approach for source separation using a dual implicit neural representation framework that jointly trains two neural networks: one for approximating distortions of the physical signal of interest and the other for learning the effective background contribution. Our method learns directly from the raw data by minimizing a reconstruction-based loss function without requiring labeled data or pre-defined dictionaries. We demonstrate the effectiveness of our framework by considering a challenging case study involving large-scale simulated as well as experimental momentum-energy-dependent inelastic neutron scattering data in a four-dimensional parameter space, characterized by heterogeneous background contributions and unknown distortions to the target signal. The method is found to successfully separate physically meaningful signals from a complex or structured background even when the signal characteristics vary across all four dimensions of the parameter space. An analytical approach that informs the choice of the regularization parameter is presented. Our method offers a versatile framework for addressing source separation problems across diverse domains, ranging from superimposed signals in astronomical measurements to structural features in biomedical image reconstructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05249v1</guid>
      <category>cs.CV</category>
      <category>cond-mat.str-el</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Ni, Zhantao Chen, Alexander N. Petsch, Edmund Xu, Cheng Peng, Alexander I. Kolesnikov, Sugata Chowdhury, Arun Bansil, Jana B. Thayer, Joshua J. Turner</dc:creator>
    </item>
    <item>
      <title>Self-consistent gravity model for inferring node mass in flow networks</title>
      <link>https://arxiv.org/abs/2106.10025</link>
      <description>arXiv:2106.10025v4 Announce Type: replace 
Abstract: The gravity model, inspired by Newton's law of universal gravitation, has long served as a primary tool for interpreting trade flows between countries, using a country's economic `mass' as a key determinant. Despite its wide application, the definition of `mass' within this model remains ambiguous. It is often approximated using indicators like GDP, which may not accurately reflect a country's true trade potential. Here, we introduce a data-driven, self-consistent numerical approach that redefines `mass' from a static proxy to a dynamic attribute inferred directly from flow data. We infer mass distribution and interaction nature through our method, mirroring Newton's approach to understanding gravity. Our methodology accurately identifies predefined embeddings and reconstructs system attributes when applied to synthetic flow data, demonstrating its strong predictive power and adaptability. Further application to real-world trade networks yields critical insights, revealing the spatial spectrum of trade flows and the economic mass of countries, two key features unexplored in depth by existing models. Our methodology not only enables accurate reconstruction of the original flow but also allows for a deep understanding of the unique capabilities of each node within the network. This study marks a significant shift in the understanding and application of the gravity model, providing a more comprehensive tool for analyzing complex systems and uncovering new insights into various fields, including global trade, traffic engineering, epidemic disease prevention, and infrastructure design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.10025v4</guid>
      <category>physics.data-an</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s41598-025-03664-7</arxiv:DOI>
      <dc:creator>Daekyung Lee, Wonguk Cho, Heetae Kim, Gunn Kim, Hyeong-Chai Jeong, Beom Jun Kim</dc:creator>
    </item>
    <item>
      <title>Scalable Multi-Task Learning for Particle Collision Event Reconstruction with Heterogeneous Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2504.21844</link>
      <description>arXiv:2504.21844v2 Announce Type: replace 
Abstract: The growing luminosity frontier at the Large Hadron Collider is challenging the reconstruction and analysis of particle collision events. Increased particle multiplicities are straining latency and storage requirements at the data acquisition stage, while new complications are emerging, including higher background levels and more frequent particle vertex misassociations. This in turn necessitates the development of more holistic and scalable reconstruction methods that take advantage of recent advances in machine learning. We propose a novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique representations for diverse particle collision relationships and integrated graph pruning layers for scalability. Trained with a multi-task paradigm in an environment mimicking the LHCb experiment, this HGNN significantly improves beauty hadron reconstruction performance. Notably, it concurrently performs particle vertex association and graph pruning within a single framework. We quantify reconstruction and pruning performance, demonstrate enhanced inference time scaling with event complexity, and mitigate potential performance loss using a weighted message passing scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21844v2</guid>
      <category>physics.data-an</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Sutcliffe, Marta Calvi, Simone Capelli, Jonas Eschle, Juli\'an Garc\'ia Pardi\~nas, Abhijit Mathad, Azusa Uzuki, Nicola Serra</dc:creator>
    </item>
    <item>
      <title>Investigating the diversity and stylization of contemporary user generated visual arts in the complexity entropy plane</title>
      <link>https://arxiv.org/abs/2408.10356</link>
      <description>arXiv:2408.10356v3 Announce Type: replace-cross 
Abstract: The advent of computational and numerical methods in recent times has provided new avenues for analyzing art historiographical narratives and tracing the evolution of art styles therein. Here, we investigate an evolutionary process underpinning the emergence and stylization of contemporary user-generated visual art styles using the complexity-entropy (C-H) plane, which quantifies local structures in paintings. Informatizing 149,780 images curated in DeviantArt and Behance platforms from 2010 to 2020, we analyze the relationship between local information of the C-H space and multi-level image features generated by a deep neural network and a feature extraction algorithm. The results reveal significant statistical relationships between the C-H information of visual artistic styles and the dissimilarities of the multi-level image features over time within groups of artworks. By disclosing a particular C-H region where the diversity of image representations is noticeably manifested, our analyses reveal an empirical condition of emerging styles that are both novel in the C-H plane and characterized by greater stylistic diversity. Our research shows that visual art analyses combined with physics-inspired methodologies and machine learning, can provide macroscopic insights into quantitatively mapping relevant characteristics of an evolutionary process underpinning the creative stylization of uncharted visual arts of given groups and time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10356v3</guid>
      <category>cs.CV</category>
      <category>physics.data-an</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41598-025-04448-9</arxiv:DOI>
      <dc:creator>Seunghwan Kim, Byunghwee Lee, Wonjae Lee</dc:creator>
    </item>
  </channel>
</rss>
