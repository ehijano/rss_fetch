<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Dec 2024 05:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Strong Gaussian approximations with random multipliers</title>
      <link>https://arxiv.org/abs/2412.14346</link>
      <description>arXiv:2412.14346v1 Announce Type: new 
Abstract: One reason why standard formulations of the central limit theorems are not applicable in high-dimensional and non-stationary regimes is the lack of a suitable limit object. Instead, suitable distributional approximations can be used, where the approximating object is not constant, but a sequence as well. We extend Gaussian approximation results for the partial sum process by allowing each summand to be multiplied by a data-dependent matrix. The results allow for serial dependence of the data, and for high-dimensionality of both the data and the multipliers. In the finite-dimensional and locally-stationary setting, we obtain a functional central limit theorem as a direct consequence. An application to sequential testing in non-stationary environments is described.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14346v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabian Mies</dc:creator>
    </item>
    <item>
      <title>Nonparametric Regression in Dirichlet Spaces: A Random Obstacle Approach</title>
      <link>https://arxiv.org/abs/2412.14357</link>
      <description>arXiv:2412.14357v1 Announce Type: new 
Abstract: In this paper, we consider nonparametric estimation over general Dirichlet metric measure spaces. Unlike the more commonly studied reproducing kernel Hilbert space, whose elements may be defined pointwise, a Dirichlet space typically only contain equivalence classes, i.e. its elements are only unique almost everywhere. This lack of pointwise definition presents significant challenges in the context of nonparametric estimation, for example the classical ridge regression problem is ill-posed. In this paper, we develop a new technique for renormalizing the ridge loss by replacing pointwise evaluations with certain \textit{local means} around the boundaries of obstacles centered at each data point. The resulting renormalized empirical risk functional is well-posed and even admits a representer theorem in terms of certain equilibrium potentials, which are truncated versions of the associated Green function, cut-off at a data-driven threshold. We study the global, out-of-sample consistency of the sample minimizer, and derive an adaptive upper bound on its convergence rate that highlights the interplay of the analytic, geometric, and probabilistic properties of the Dirichlet form. We also construct a simple regressogram type estimator that achieves the minimax optimal estimation rate over certain $L^p$ subsets of a Dirichlet ball with some knowledge of the geometry of the metric measure space. Our framework notably does not require the smoothness of the underlying space, and is applicable to both manifold and fractal settings. To the best of our knowledge, this is the first paper to obtain out-of-sample convergence guarantees in the framework of general metric measure Dirichlet spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14357v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prem Talwai, David Simchi-Levi</dc:creator>
    </item>
    <item>
      <title>Asymptotic Equivalence for Nonparametric Regression</title>
      <link>https://arxiv.org/abs/2412.14800</link>
      <description>arXiv:2412.14800v1 Announce Type: new 
Abstract: We consider a nonparametric model $\mathcal{E}^{n},$ generated by independent observations $X_{i},$ $i=1,...,n,$ with densities $p(x,\theta_{i}),$ $i=1,...,n,$ the parameters of which $\theta _{i}=f(i/n)\in \Theta $ are driven by the values of an unknown function $f:[0,1]\rightarrow \Theta $ in a smoothness class. The main result of the paper is that, under regularity assumptions, this model can be approximated, in the sense of the Le Cam deficiency pseudodistance, by a nonparametric Gaussian shift model $Y_{i}=\Gamma (f(i/n))+\varepsilon _{i},$ where $\varepsilon_{1},...,\varepsilon _{n}$ are i.i.d. standard normal r.v.'s, the function $\Gamma (\theta ):\Theta \rightarrow \mathrm{R}$ satisfies $\Gamma ^{\prime}(\theta )=\sqrt{I(\theta )}$ and $I(\theta )$ is the Fisher information corresponding to the density $p(x,\theta ).$</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14800v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:journal_reference>Mathematical Methods of Statistics, 2002, Vol. 11, No 1, pp. 1-36</arxiv:journal_reference>
      <dc:creator>Ion Grama, Michael Nussbaum</dc:creator>
    </item>
    <item>
      <title>Asymptotic Equivalence for Nonparametric Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2412.15057</link>
      <description>arXiv:2412.15057v1 Announce Type: new 
Abstract: We establish that a non-Gaussian nonparametric regression model is asymptotically equivalent to a regression model with Gaussian noise. The approximation is in the sense of Le Cam's deficiency distance $\Delta $; the models are then asymptotically equivalent for all purposes of statistical decision with bounded loss. Our result concerns a sequence of independent but not identically distributed observations with each distribution in the same real-indexed exponential family. The canonical parameter is a value $f(t_i)$ of a regression function $f$ at a grid point $t_i$ (nonparametric GLM). When $f$ is in a H\"{o}lder ball with exponent $\beta &gt;\frac 12 ,$ we establish global asymptotic equivalence to observations of a signal $\Gamma (f(t))$ in Gaussian white noise, where $\Gamma $ is related to a variance stabilizing transformation in the exponential family. The result is a regression analog of the recently established Gaussian approximation for the i.i.d. model. The proof is based on a functional version of the Hungarian construction for the partial sum process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15057v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:journal_reference>Probab. Theory Relat. Fields 111, 167-214 (1998)</arxiv:journal_reference>
      <dc:creator>Ion Grama, Michael Nussbaum</dc:creator>
    </item>
    <item>
      <title>Randomization Tests for Conditional Group Symmetry</title>
      <link>https://arxiv.org/abs/2412.14391</link>
      <description>arXiv:2412.14391v1 Announce Type: cross 
Abstract: Symmetry plays a central role in the sciences, machine learning, and statistics. While statistical tests for the presence of distributional invariance with respect to groups have a long history, tests for conditional symmetry in the form of equivariance or conditional invariance are absent from the literature. This work initiates the study of nonparametric randomization tests for symmetry (invariance or equivariance) of a conditional distribution under the action of a specified locally compact group. We develop a general framework for randomization tests with finite-sample Type I error control and, using kernel methods, implement tests with finite-sample power lower bounds. We also describe and implement approximate versions of the tests, which are asymptotically consistent. We study their properties empirically on synthetic examples, and on applications to testing for symmetry in two problems from high-energy particle physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14391v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenny Chiu, Alex Sharp, Benjamin Bloem-Reddy</dc:creator>
    </item>
    <item>
      <title>Cross-Validation with Antithetic Gaussian Randomization</title>
      <link>https://arxiv.org/abs/2412.14423</link>
      <description>arXiv:2412.14423v1 Announce Type: cross 
Abstract: We introduce a method for performing cross-validation without sample splitting. The method is well-suited for problems where traditional sample splitting is infeasible, such as when data are not assumed to be independently and identically distributed. Even in scenarios where sample splitting is possible, our method offers a computationally efficient alternative for estimating prediction error, achieving comparable or even lower error than standard cross-validation at a significantly reduced computational cost.
  Our approach constructs train-test data pairs using externally generated Gaussian randomization variables, drawing inspiration from recent randomization techniques such as data-fission and data-thinning. The key innovation lies in a carefully designed correlation structure among these randomization variables, referred to as antithetic Gaussian randomization. This correlation is crucial in maintaining a bounded variance while allowing the bias to vanish, offering an additional advantage over standard cross-validation, whose performance depends heavily on the bias-variance tradeoff dictated by the number of folds. We provide a theoretical analysis of the mean squared error of the proposed estimator, proving that as the level of randomization decreases to zero, the bias converges to zero, while the variance remains bounded and decays linearly with the number of repetitions. This analysis highlights the benefits of the antithetic Gaussian randomization over independent randomization. Simulation studies corroborate our theoretical findings, illustrating the robust performance of our cross-validated estimator across various data types and loss functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14423v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sifan Liu, Snigdha Panigrahi, Jake A. Soloff</dc:creator>
    </item>
    <item>
      <title>Recovering semipermeable barriers from reflected Brownian motion</title>
      <link>https://arxiv.org/abs/2412.14740</link>
      <description>arXiv:2412.14740v1 Announce Type: cross 
Abstract: We study the recovery of one-dimensional semipermeable barriers for a stochastic process in a planar domain. The considered process acts like Brownian motion when away from the barriers and is reflected upon contact until a sufficient but random amount of interaction has occurred, determined by the permeability, after which it passes through. Given a sequence of samples, we wonder when one can determine the location and shape of the barriers.
  This paper identifies several different recovery regimes, determined by the available observation period and the time between samples, with qualitatively different behavior. The observation period $T$ dictates if the full barriers or only certain pieces can be recovered, and the sampling rate significantly influences the convergence rate as $T\to \infty$. This rate turns out polynomial for fixed-frequency data, but exponentially fast in a high-frequency regime.
  Further, the environment's impact on the difficulty of the problem is quantified using interpretable parameters in the recovery guarantees, and is found to also be regime-dependent. For instance, the curvature of the barriers affects the convergence rate for fixed-frequency data, but becomes irrelevant when $T\to \infty$ with high-frequency data.
  The results are accompanied by explicit algorithms, and we conclude by illustrating the application to real-life data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14740v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Van Werde, Jaron Sanders</dc:creator>
    </item>
    <item>
      <title>A linear regression model for quantile function data applied to paired pulmonary 3d CT scans</title>
      <link>https://arxiv.org/abs/2412.15049</link>
      <description>arXiv:2412.15049v1 Announce Type: cross 
Abstract: This paper introduces a new objective measure for assessing treatment response in asthmatic patients using computed tomography (CT) imaging data. For each patient, CT scans were obtained before and after one year of monoclonal antibody treatment. Following image segmentation, the Hounsfield unit (HU) values of the voxels were encoded through quantile functions. It is hypothesized that patients with improved conditions after treatment will exhibit better expiration, reflected in higher HU values and an upward shift in the quantile curve. To objectively measure treatment response, a novel linear regression model on quantile functions is developed, drawing inspiration from Verde and Irpino (2010). Unlike their framework, the proposed model is parametric and incorporates distributional assumptions on the errors, enabling statistical inference. The model allows for the explicit calculation of regression coefficient estimators and confidence intervals, similar to conventional linear regression. The corresponding data and R code are available on GitHub to facilitate the reproducibility of the analyses presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15049v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marie-F\'elicia B\'eclin, Pierre Lafaye de Micheaux, Nicolas Molinari, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Information-Theoretic Limits and Strong Consistency on Binary Non-uniform Hypergraph Stochastic Block Models</title>
      <link>https://arxiv.org/abs/2306.06845</link>
      <description>arXiv:2306.06845v2 Announce Type: replace 
Abstract: Consider the unsupervised classification problem in random hypergraphs under the non-uniform Hypergraph Stochastic Block Model (HSBM) with two equal-sized communities, where each edge appears independently with some probability depending only on the labels of its vertices. In this paper, the information-theoretic limits on the clustering accuracy and the strong consistency threshold are established, expressed in terms of the generalized Hellinger distance. Below the threshold, it is impossible to assign all vertices to their own communities, and the lower bound of the expected mismatch ratio is derived. On the other hand, the problem space is (sometimes) divided into two disjoint subspaces when above the threshold. When only the contracted adjacency matrix is given, with high probability, one-stage spectral algorithms succeed in assigning every vertex correctly in the subspace far away from the threshold but fail in the other one. Two subsequent refinement algorithms are proposed to improve the clustering accuracy, which attain the lowest possible mismatch ratio, previously derived from the information-theoretical perspective. The failure of spectral algorithms in the second subspace arises from the loss of information induced by tensor contraction. The origin of this loss and possible solutions to minimize the impact are presented. Moreover, different from uniform hypergraphs, strong consistency is achievable by aggregating information from all uniform layers, even if it is impossible when each layer is considered alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06845v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hai-Xiao Wang</dc:creator>
    </item>
    <item>
      <title>Diaconis-Ylvisaker prior penalized likelihood for $p/n \to \kappa \in (0,1)$ logistic regression</title>
      <link>https://arxiv.org/abs/2311.07419</link>
      <description>arXiv:2311.07419v2 Announce Type: replace 
Abstract: We characterise the behaviour of the maximum Diaconis-Ylvisaker prior penalized likelihood estimator in high-dimensional logistic regression, where the number of covariates is a fraction $\kappa \in (0,1)$ of the number of observations $n$, as $n \to \infty$. We derive the estimator's aggregate asymptotic behaviour under this proportional asymptotic regime, when covariates are independent normal random variables with mean zero and the linear predictor has asymptotic variance $\gamma^2$. From this foundation, we devise adjusted $Z$-statistics, penalized likelihood ratio statistics, and aggregate asymptotic results with arbitrary covariate covariance. While the maximum likelihood estimate asymptotically exists only for a narrow range of $(\kappa, \gamma)$ values, the maximum Diaconis-Ylvisaker prior penalized likelihood estimate not only exists always but is also directly computable using maximum likelihood routines. Thus, our asymptotic results also hold for $(\kappa, \gamma)$ values where results for maximum likelihood are not attainable, with no overhead in implementation or computation. We study the estimator's shrinkage properties, compare it to alternative estimation methods that can operate with proportional asymptotics, and present procedures for the estimation of unknown constants that describe the asymptotic behaviour of our estimator. We also provide a conjecture about the behaviour of our estimator when an intercept parameter is present in the model. We present results from extensive numerical studies to demonstrate the theoretical advances and strong evidence to support the conjecture, and illustrate the methodology we put forward through the analysis of a real-world data set on digit recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07419v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Sterzinger, Ioannis Kosmidis</dc:creator>
    </item>
    <item>
      <title>Log-concave Density Estimation with Independent Components</title>
      <link>https://arxiv.org/abs/2401.01500</link>
      <description>arXiv:2401.01500v2 Announce Type: replace 
Abstract: We propose a method for estimating a log-concave density on $\mathbb R^d$ from samples, under the assumption that there exists an orthogonal transformation that makes the components of the random vector independent. While log-concave density estimation is hard both computationally and statistically, the independent components assumption alleviates both issues, while still maintaining a large non-parametric class. We prove that under mild conditions, at most $\tilde{\mathcal{O}}(\epsilon^{-4})$ samples (suppressing constants and log factors) suffice for our proposed estimator to be within $\epsilon$ of the original density in squared Hellinger distance. On the computational front, while the usual log-concave maximum likelihood estimate can be obtained via a finite-dimensional convex program, it is slow to compute -- especially in higher dimensions. We demonstrate through numerical experiments that our estimator can be computed efficiently, making it more practical to use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01500v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sharvaj Kubal, Christian Campbell, Elina Robeva</dc:creator>
    </item>
    <item>
      <title>Langevin dynamics for high-dimensional optimization: the case of multi-spiked tensor PCA</title>
      <link>https://arxiv.org/abs/2408.06401</link>
      <description>arXiv:2408.06401v2 Announce Type: replace-cross 
Abstract: We study nonconvex optimization in high dimensions through Langevin dynamics, focusing on the multi-spiked tensor PCA problem. This tensor estimation problem involves recovering $r$ hidden signal vectors (spikes) from noisy Gaussian tensor observations using maximum likelihood estimation. We study the number of samples required for Langevin dynamics to efficiently recover the spikes and determine the necessary separation condition on the signal-to-noise ratios (SNRs) for exact recovery, distinguishing the cases $p \ge 3$ and $p=2$, where $p$ denotes the order of the tensor. In particular, we show that the sample complexity required for recovering the spike associated with the largest SNR matches the well-known algorithmic threshold for the single-spike case, while this threshold degrades when recovering all $r$ spikes. As a key step, we provide a detailed characterization of the trajectory and interactions of low-dimensional projections that capture the high-dimensional dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06401v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G\'erard Ben Arous, C\'edric Gerbelot, Vanessa Piccolo</dc:creator>
    </item>
  </channel>
</rss>
