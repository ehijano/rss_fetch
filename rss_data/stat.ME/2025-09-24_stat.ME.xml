<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Particle Filtering for Non-Deterministic Electrocardiographic Imaging</title>
      <link>https://arxiv.org/abs/2509.19404</link>
      <description>arXiv:2509.19404v1 Announce Type: new 
Abstract: Electrocardiographic imaging (ECGI) aims to non-invasively reconstruct activation maps of the heart from temporal body surface potentials. While most existing approaches rely on inverse and optimization techniques that may yield satisfactory reconstructions, they typically provide a single deterministic solution, overlooking the inherent uncertainty of the problem stemming from its very ill-posed nature, the poor knowledge of biophysical features and the unavoidable presence of noise in the measurements. The Bayesian framework, which naturally incorporates uncertainty while also accounting for temporal correlations across time steps, can be used to address this limitation. In this work, we propose a low-dimensional representation of the activation sequence that enables the use of particle filtering, a Bayesian filtering method that does not rely on predefined assumptions regarding the shape of the posterior distribution, in contrast to approaches like the Kalman filter. This allows to produce not only activation maps but also probabilistic maps indicating the likelihood of activation at each point on the heart over time, as well as pseudo-probability maps reflecting the likelihood of a point being part of an earliest activation site. Additionally, we introduce a method to estimate the probability of the presence of a conduction lines of block on the heart surface. Combined with classical reconstruction techniques, this could help discriminate artificial from true lines of block in activation maps. We support our approach with a numerical study based on simulated data, demonstrating the potential of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19404v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emma Lagracie (UB), Luc de Montella</dc:creator>
    </item>
    <item>
      <title>Forecasting High Dimensional Time Series with Dynamic Dimension Reduction</title>
      <link>https://arxiv.org/abs/2509.19418</link>
      <description>arXiv:2509.19418v1 Announce Type: new 
Abstract: Many dimension reduction techniques have been developed for independent data, and most have also been extended to time series. However, these methods often fail to account for the dynamic dependencies both within and across series. In this work, we propose a general framework for forecasting high-dimensional time series that integrates dynamic dimension reduction with regularization techniques. The effectiveness of the proposed approach is illustrated through a simulated example and a forecasting application using an economic dataset. We show that several specific methods are encompassed within this framework, including Dynamic Principal Components and Reduced Rank Autoregressive Models. Furthermore, time-domain formulations of Dynamic Canonical Correlation and Dynamic Redundancy Analysis are introduced here for the first time as particular instances of the proposed methodology. All of these techniques are analyzed as special cases of a unified procedure, enabling a coherent derivation and interpretation across methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19418v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Pe\~na, Victor J. Yohai</dc:creator>
    </item>
    <item>
      <title>Some Simplifications for the Expectation-Maximization (EM) Algorithm: The Linear Regression Model Case</title>
      <link>https://arxiv.org/abs/2509.19461</link>
      <description>arXiv:2509.19461v1 Announce Type: new 
Abstract: The EM algorithm is a generic tool that offers maximum likelihood solutions when datasets are incomplete with data values missing at random or completely at random. At least for its simplest form, the algorithm can be rewritten in terms of an ANCOVA regression specification. This formulation allows several analytical results to be derived that permit the EM algorithm solution to be expressed in terms of new observation predictions and their variances. Implementations can be made with a linear regression or a nonlinear regression model routine, allowing missing value imputations, even when they must satisfy constraints. Fourteen example datasets gleaned from the EM algorithm literature are reanalyzed. Imputation results have been verified with SAS PROC MI. Six theorems are proved that broadly contextualize imputation findings in terms of the theory, methodology, and practice of statistical science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19461v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel A. Griffith</dc:creator>
    </item>
    <item>
      <title>Chiseling: Powerful and Valid Subgroup Selection via Interactive Machine Learning</title>
      <link>https://arxiv.org/abs/2509.19490</link>
      <description>arXiv:2509.19490v1 Announce Type: new 
Abstract: In regression and causal inference, controlled subgroup selection aims to identify, with inferential guarantees, a subgroup (defined as a subset of the covariate space) on which the average response or treatment effect is above a given threshold. E.g., in a clinical trial, it may be of interest to find a subgroup with a positive average treatment effect. However, existing methods either lack inferential guarantees, heavily restrict the search for the subgroup, or sacrifice efficiency by naive data splitting. We propose a novel framework called chiseling that allows the analyst to interactively refine and test a candidate subgroup by iteratively shrinking it. The sole restriction is that the shrinkage direction only depends on the points outside the current subgroup, but otherwise the analyst may leverage any prior information or machine learning algorithm. Despite this flexibility, chiseling controls the probability that the discovered subgroup is null (e.g., has a non-positive average treatment effect) under minimal assumptions: for example, in randomized experiments, this inferential validity guarantee holds under only bounded moment conditions. When applied to a variety of simulated datasets and a real survey experiment, chiseling identifies substantially better subgroups than existing methods with inferential guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19490v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Cheng, Asher Spector, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian Operator-induced Symbolic Regression Trees for Structural Learning of Scientific Expressions</title>
      <link>https://arxiv.org/abs/2509.19710</link>
      <description>arXiv:2509.19710v1 Announce Type: new 
Abstract: The advent of Scientific Machine Learning has heralded a transformative era in scientific discovery, driving progress across diverse domains. Central to this progress is uncovering scientific laws from experimental data through symbolic regression. However, existing approaches are dominated by heuristic algorithms or data-hungry black-box methods, which often demand low-noise settings and lack principled uncertainty quantification. Motivated by interpretable Statistical Artificial Intelligence, we develop a hierarchical Bayesian framework for symbolic regression that represents scientific laws as ensembles of tree-structured symbolic expressions endowed with a regularized tree prior. This coherent probabilistic formulation enables full posterior inference via an efficient Markov chain Monte Carlo algorithm, yielding a balance between predictive accuracy and structural parsimony. To guide symbolic model selection, we develop a marginal posterior-based criterion adhering to the Occam's window principle and further quantify structural fidelity to ground truth through a tailored expression-distance metric. On the theoretical front, we establish near-minimax rate of Bayesian posterior concentration, providing the first rigorous guarantee in context of symbolic regression. Empirical evaluation demonstrates robust performance of our proposed methodology against state-of-the-art competing modules on a simulated example, a suite of canonical Feynman equations, and single-atom catalysis dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19710v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Somjit Roy, Pritam Dey, Debdeep Pati, Bani K. Mallick</dc:creator>
    </item>
    <item>
      <title>Generalized Bayesian Inference for Dynamic Random Dot Product Graphs</title>
      <link>https://arxiv.org/abs/2509.19748</link>
      <description>arXiv:2509.19748v1 Announce Type: new 
Abstract: The random dot product graph is a popular model for network data with extensions that accommodate dynamic (time-varying) networks. However, two significant deficiencies exist in the dynamic random dot product graph literature: (1) no coherent Bayesian way to update one's prior beliefs about the latent positions in dynamic random dot product graphs due to their complicated constraints, and (2) no approach to forecast future networks with meaningful uncertainty quantification. This work proposes a generalized Bayesian framework that addresses these needs using a Gibbs posterior that represents a coherent updating of Bayesian beliefs based on a least-squares loss function. We establish the consistency and contraction rate of this Gibbs posterior under commonly adopted Gaussian random walk priors. For estimation, we develop a fast Gibbs sampler with a time complexity for sampling the latent positions that is linear in the observed edges in the dynamic network, which is substantially faster than existing exact samplers. Simulations and an application to forecasting international conflicts show that the proposed method's in-sample and forecasting performance outperforms competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19748v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Daniel Loyal</dc:creator>
    </item>
    <item>
      <title>Causal Inference under Threshold Manipulation: Bayesian Mixture Modeling and Heterogeneous Treatment Effects</title>
      <link>https://arxiv.org/abs/2509.19814</link>
      <description>arXiv:2509.19814v1 Announce Type: new 
Abstract: Many marketing applications, including credit card incentive programs, offer rewards to customers who exceed specific spending thresholds to encourage increased consumption. Quantifying the causal effect of these thresholds on customers is crucial for effective marketing strategy design. Although regression discontinuity design is a standard method for such causal inference tasks, its assumptions can be violated when customers, aware of the thresholds, strategically manipulate their spending to qualify for the rewards. To address this issue, we propose a novel framework for estimating the causal effect under threshold manipulation. The main idea is to model the observed spending distribution as a mixture of two distributions: one representing customers strategically affected by the threshold, and the other representing those unaffected. To fit the mixture model, we adopt a two-step Bayesian approach consisting of modeling non-bunching customers and fitting a mixture model to a sample around the threshold. We show posterior contraction of the resulting posterior distribution of the causal effect under large samples. Furthermore, we extend this framework to a hierarchical Bayesian setting to estimate heterogeneous causal effects across customer subgroups, allowing for stable inference even with small subgroup sample sizes. We demonstrate the effectiveness of our proposed methods through simulation studies and illustrate their practical implications using a real-world marketing dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19814v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kohsuke Kubota, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Improving Disease Risk Estimation in Small Areas by Accounting for Spatiotemporal Local Discontinuities</title>
      <link>https://arxiv.org/abs/2509.19889</link>
      <description>arXiv:2509.19889v1 Announce Type: new 
Abstract: This work proposes a two-step method to enhance disease risk estimation in small areas by integrating spatiotemporal cluster detection within a Bayesian hierarchical spatiotemporal model. First, we introduce an efficient scan-statistic-based clustering algorithm that employs a greedy search within the scan window, enabling flexible cluster detection across large spatial domains. We then integrate these detected clusters into a Bayesian spatiotemporal model to estimate relative risks, explicitly accounting for identified risk discontinuities. We apply this methodology to large-scale cancer mortality data at the municipality level across continental Spain. Our results show our method offers superior cluster detection accuracy compared to SaTScan. Furthermore, integrating cluster information into a Bayesian spatiotemporal model significantly improves model fit and risk estimate performance, as evidenced by better DIC, WAIC, and logarithmic scores than SaTScan-based or standard BYM2 models. This methodology provides a powerful tool for epidemiological analysis, offering a more precise identification of high- and low-risk areas and enhancing the accuracy of risk estimation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19889v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Santaf\'e,  G.,  Adin,  A.,  Ugarte, M. L</dc:creator>
    </item>
    <item>
      <title>Extending finite mixture models with skew-normal distributions and hidden Markov models for time series</title>
      <link>https://arxiv.org/abs/2509.19920</link>
      <description>arXiv:2509.19920v1 Announce Type: new 
Abstract: We introduce an extension of finite mixture models by incorporating skew-normal distributions within a Hidden Markov Model framework. By assuming a constant transition probability matrix and allowing emission distributions to vary according to hidden states, the proposed model effectively captures dynamic dependencies between variables. Through the estimation of state-specific parameters, including location, scale, and skewness, the proposed model enables the detection of structural changes, such as shifts in the observed data distribution, while addressing challenges such as overfitting and computational inefficiencies inherent in Gaussian mixtures. Both simulation studies and real data analysis demonstrate the robustness and flexibility of the approach, highlighting its ability to accurately model asymmetric data and detect regime transitions. This methodological advancement broadens the applicability of a finite mixture of hidden Markov models across various fields, including demography, economics, finance, and environmental studies, offering a powerful tool for understanding complex temporal dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19920v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrea Nigri, Marco Forti, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Multi-state Models For Modeling Disease Histories Based On Longitudinal Data</title>
      <link>https://arxiv.org/abs/2509.19956</link>
      <description>arXiv:2509.19956v1 Announce Type: new 
Abstract: Multi-stage disease histories derived from longitudinal data are becoming increasingly available as registry data and biobanks expand. Multi-state models are suitable to investigate transitions between different disease stages in presence of competing risks. In this context, however their estimation is complicated by dependent left-truncation, multiple time scales, index event bias, and interval-censoring. In this work, we investigate the extension of piecewise exponential additive models (PAMs) to this setting and their applicability given the above challenges. In simulation studies we show that PAMs can handle dependent left-truncation and accommodate multiple time scales. Compared to a stratified single time scale model, a multiple time scales model is found to be less robust to the data generating process. We also quantify the extent of index event bias in multiple settings, demonstrating its dependence on the completeness of covariate adjustment. In general, PAMs recover baseline and fixed effects well in most settings, except for baseline hazards in interval-censored data. Finally, we apply our framework to estimate multi-state transition hazards and probabilities of chronic kidney disease (CKD) onset and progression in a UK Biobank dataset (n=$142,667$). We observe CKD progression risk to be highest for individuals with early CKD onset and to further increase over age. In addition, the well-known genetic variant rs77924615 in the UMOD locus is found to be associated with CKD onset hazards, but not with risk of further CKD progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19956v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Wiegrebe, Johannes Piller, Mathias Gorski, Merle Behr, Helmut K\"uchenhoff, Iris M. Heid, Andreas Bender</dc:creator>
    </item>
    <item>
      <title>A decision-theoretic framework for uncertainty quantification in epidemiological modelling</title>
      <link>https://arxiv.org/abs/2509.20013</link>
      <description>arXiv:2509.20013v1 Announce Type: new 
Abstract: Estimating, understanding, and communicating uncertainty is fundamental to statistical epidemiology, where model-based estimates regularly inform real-world decisions. However, sources of uncertainty are rarely formalised, and existing classifications are often defined inconsistently. This lack of structure hampers interpretation, model comparison, and targeted data collection. Connecting ideas from machine learning, information theory, experimental design, and health economics, we present a first-principles decision-theoretic framework that defines uncertainty as the expected loss incurred by making an estimate based on incomplete information, arguing that this is a highly useful and practically relevant definition for epidemiology. We show how reasoning about future data leads to a notion of expected uncertainty reduction, which induces formal definitions of reducible and irreducible uncertainty. We demonstrate our approach using a case study of SARS-CoV-2 wastewater surveillance in Aotearoa New Zealand, estimating the uncertainty reduction if wastewater surveillance were expanded to the full population. We then connect our framework to relevant literature from adjacent fields, showing how it unifies and extends many of these ideas and how it allows these ideas to be applied to a wider range of models. Altogether, our framework provides a foundation for more reliable, consistent, and policy-relevant uncertainty quantification in infectious disease epidemiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20013v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nicholas Steyn, Freddie Bickford Smith, Cathal Mills, Vik Shirvaikar, Christl A Donnelly, Kris V Parag</dc:creator>
    </item>
    <item>
      <title>Identification and Semiparametric Estimation of Conditional Means from Aggregate Data</title>
      <link>https://arxiv.org/abs/2509.20194</link>
      <description>arXiv:2509.20194v1 Announce Type: new 
Abstract: We introduce a new method for estimating the mean of an outcome variable within groups when researchers only observe the average of the outcome and group indicators across a set of aggregation units, such as geographical areas. Existing methods for this problem, also known as ecological inference, implicitly make strong assumptions about the aggregation process. We first formalize weaker conditions for identification, which motivates estimators that can efficiently control for many covariates. We propose a debiased machine learning estimator that is based on nuisance functions restricted to a partially linear form. Our estimator also admits a semiparametric sensitivity analysis for violations of the key identifying assumption, as well as asymptotically valid confidence intervals for local, unit-level estimates under additional assumptions. Simulations and validation on real-world data where ground truth is available demonstrate the advantages of our approach over existing methods. Open-source software is available which implements the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20194v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cory McCartan, Shiro Kuriwaki</dc:creator>
    </item>
    <item>
      <title>Non-overlap Average Treatment Effect Bounds</title>
      <link>https://arxiv.org/abs/2509.20206</link>
      <description>arXiv:2509.20206v1 Announce Type: new 
Abstract: The average treatment effect (ATE), the mean difference in potential outcomes under treatment and control, is a canonical causal effect. Overlap, which says that all subjects have non-zero probability of either treatment status, is necessary to identify and estimate the ATE. When overlap fails, the standard solution is to change the estimand, and target a trimmed effect in a subpopulation satisfying overlap; however, this no longer addresses the original goal of estimating the ATE. When the outcome is bounded, we demonstrate that this compromise is unnecessary. We derive non-overlap bounds: partial identification bounds on the ATE that do not require overlap. They are the sum of a trimmed effect within the overlap subpopulation and worst-case bounds on the ATE in the non-overlap subpopulation. Non-overlap bounds have width proportional to the size of the non-overlap subpopulation, making them informative when overlap violations are limited -- a common scenario in practice. Since the bounds are non-smooth functionals, we derive smooth approximations of them that contain the ATE but can be estimated using debiased estimators leveraging semiparametric efficiency theory. Specifically, we propose a Targeted Minimum Loss-Based estimator that is $\sqrt{n}$-consistent and asymptotically normal under nonparametric assumptions on the propensity score and outcome regression. We then show how to obtain a uniformly valid confidence set across all trimming and smoothing parameters with the multiplier bootstrap. This allows researchers to consider many parameters, choose the tightest confidence interval, and still attain valid coverage. We demonstrate via simulations that non-overlap bound estimators can detect non-zero ATEs with higher power than traditional doubly-robust point estimators. We illustrate our method by estimating the ATE of right heart catheterization on mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20206v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Herbert P. Susmann, Alec McClean, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>Transfer Learning in Regression with Influential Points</title>
      <link>https://arxiv.org/abs/2509.20272</link>
      <description>arXiv:2509.20272v1 Announce Type: new 
Abstract: Regression prediction plays a crucial role in practical applications and strongly relies on data annotation. However, due to prohibitive annotation costs or domain-specific constraints, labeled data in the target domain is often scarce, making transfer learning a critical solution by leveraging knowledge from resource-rich source domains. In the practical target scenario, although transfer learning has been widely applied, influential points can significantly distort parameter estimation for the target domain model. This issue is further compounded when influential points are also present in source domains, leading to aggravated performance degradation and posing critical robustness challenges for existing transfer learning frameworks. In this study, we innovatively introduce a transfer learning collaborative optimization (Trans-CO) framework for influential point detection and regression model fitting. Extensive simulation experiments demonstrate that the proposed Trans-CO algorithm outperforms competing methods in terms of model fitting performance and influential point identification accuracy. Furthermore, it achieves superior predictive accuracy on real-world datasets, providing a novel solution for transfer learning in regression with influential points</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20272v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingbing Wang, Jiaqi Wang, Yu Tang</dc:creator>
    </item>
    <item>
      <title>Scale two-sample testing with arbitrarily missing data</title>
      <link>https://arxiv.org/abs/2509.20332</link>
      <description>arXiv:2509.20332v1 Announce Type: new 
Abstract: This work proposes a novel rank-based scale two-sample testing method for univariate, distinct data when a subset of the data may be missing. Our approach is based on mathematically tight bounds of the Ansari-Bradley test statistic in the presence of missing data, and rejects the null hypothesis if the test statistic is significant regardless of the missing values. This proposed scale testing method is then combined with the location testing method proposed by Zeng et al. (2024) using the Holm-Bonferroni correction for location-scale testing. We show that our methods control the Type I error regardless of the values of the missing data. Simulation results demonstrate that our methods have good statistical power, typically when less than 10% of the data are missing, while other missing data methods, such as case deletion or imputation methods, fail to control the Type I error when the data are missing not at random. We illustrate the proposed location-scale testing method on hepatitis C virus dataset where a subset of values is unobserved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20332v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yijin Zeng, Niall M. Adams, Dean A. Bodenham</dc:creator>
    </item>
    <item>
      <title>Statistical Inference Leveraging Synthetic Data with Distribution-Free Guarantees</title>
      <link>https://arxiv.org/abs/2509.20345</link>
      <description>arXiv:2509.20345v1 Announce Type: new 
Abstract: The rapid proliferation of high-quality synthetic data -- generated by advanced AI models or collected as auxiliary data from related tasks -- presents both opportunities and challenges for statistical inference. This paper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that wraps around any statistical inference procedure to safely enhance sample efficiency by combining synthetic and real data. Our framework leverages high-quality synthetic data to boost statistical power, yet adaptively defaults to the standard inference method using only real data when synthetic data is of low quality. The error of our method remains below a user-specified bound without any distributional assumptions on the synthetic data, and decreases as the quality of the synthetic data improves. This flexibility enables seamless integration with conformal prediction, risk control, hypothesis testing, and multiple testing procedures, all without modifying the base inference method. We demonstrate the benefits of our method on challenging tasks with limited labeled data, including AlphaFold protein structure prediction, and comparing large reasoning models on complex math problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20345v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meshi Bashari, Yonghoon Lee, Roy Maor Lotan, Edgar Dobriban, Yaniv Romano</dc:creator>
    </item>
    <item>
      <title>How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment</title>
      <link>https://arxiv.org/abs/2509.19329</link>
      <description>arXiv:2509.19329v1 Announce Type: cross 
Abstract: We examined how model size, temperature, and prompt style affect Large Language Models' (LLMs) alignment within itself, between models, and with human in assessing clinical reasoning skills. Model size emerged as a key factor in LLM-human score alignment. Study highlights the importance of checking alignments across multiple levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19329v1</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julie Jung, Max Lu, Sina Chole Benker, Dogus Darici</dc:creator>
    </item>
    <item>
      <title>Testing the Constancy of Type Ia Supernova Luminosities with Gaussian Process</title>
      <link>https://arxiv.org/abs/2509.19494</link>
      <description>arXiv:2509.19494v1 Announce Type: cross 
Abstract: Type Ia supernovae (SNe~Ia) are central to studies of cosmic expansion, under the assumption that their absolute magnitude $M_B$ does not evolve with redshift. Even small drifts in brightness can bias cosmological parameters such as $H_0$ and $w$. Here we test this assumption using a non-parametric Gaussian Process (GP) reconstruction of the expansion history from cosmic chronometer $H(z)$ data, which provides a model-independent baseline distance modulus, $\mu_{\rm GP}(z)$. To propagate uncertainties, we draw Monte Carlo realizations of $H(z)$ from the GP posterior and evaluate them on a Chebyshev grid, which improves numerical stability and quadrature accuracy. Supernova observations are then compared to this baseline through residuals, $\Delta M_B(z)$, and their derivatives. Applying this method to Pantheon+ (1701 SNe~Ia) and DES 5YR (435 SNe~Ia), we find that SNe~Ia are consistent with being standard candles within $1\sigma$, though both datasets exhibit localized departures: near $z \sim 1$ in Pantheon+ and at $z \sim 0.3$--$0.5$ in DES. The presence of similar features in two independent surveys suggests they are not purely statistical. Our results point toward a possible non-monotonic luminosity evolution, likely reflecting different physical drivers at different epochs, and highlight the need for a deeper astrophysical understanding of SN~Ia populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19494v1</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.GA</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Akshay Rana</dc:creator>
    </item>
    <item>
      <title>Causal Machine Learning for Surgical Interventions</title>
      <link>https://arxiv.org/abs/2509.19705</link>
      <description>arXiv:2509.19705v1 Announce Type: cross 
Abstract: Surgical decision-making is complex and requires understanding causal relationships between patient characteristics, interventions, and outcomes. In high-stakes settings like spinal fusion or scoliosis correction, accurate estimation of individualized treatment effects (ITEs) remains limited due to the reliance on traditional statistical methods that struggle with complex, heterogeneous data. In this study, we develop a multi-task meta-learning framework, X-MultiTask, for ITE estimation that models each surgical decision (e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct task while learning shared representations across tasks. To strengthen causal validity, we incorporate the inverse probability weighting (IPW) into the training objective. We evaluate our approach on two datasets: (1) a public spinal fusion dataset (1,017 patients) to assess the effect of anterior vs. posterior approaches on complication severity; and (2) a private AIS dataset (368 patients) to analyze the impact of posterior spinal fusion (PSF) vs. non-surgical management on patient-reported outcomes (PROs). Our model achieves the highest average AUC (0.84) in the anterior group and maintains competitive performance in the posterior group (0.77). It outperforms baselines in treatment effect estimation with the lowest overall $\epsilon_{\text{NN-PEHE}}$ (0.2778) and $\epsilon_{\text{ATE}}$ (0.0763). Similarly, when predicting PROs in AIS, X-MultiTask consistently shows superior performance across all domains, with $\epsilon_{\text{NN-PEHE}}$ = 0.2551 and $\epsilon_{\text{ATE}}$ = 0.0902. By providing robust, patient-specific causal estimates, X-MultiTask offers a powerful tool to advance personalized surgical care and improve patient outcomes. The code is available at https://github.com/Wizaaard/X-MultiTask.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19705v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.26599/BDMA.2025.9020093</arxiv:DOI>
      <dc:creator>J. Ben Tamo, Nishant S. Chouhan, Micky C. Nnamdi, Yining Yuan, Shreya S. Chivilkar, Wenqi Shi, Steven W. Hwang, B. Randall Brenn, May D. Wang</dc:creator>
    </item>
    <item>
      <title>Diffusion and Flow-based Copulas: Forgetting and Remembering Dependencies</title>
      <link>https://arxiv.org/abs/2509.19707</link>
      <description>arXiv:2509.19707v1 Announce Type: cross 
Abstract: Copulas are a fundamental tool for modelling multivariate dependencies in data, forming the method of choice in diverse fields and applications. However, the adoption of existing models for multimodal and high-dimensional dependencies is hindered by restrictive assumptions and poor scaling. In this work, we present methods for modelling copulas based on the principles of diffusions and flows. We design two processes that progressively forget inter-variable dependencies while leaving dimension-wise distributions unaffected, provably defining valid copulas at all times. We show how to obtain copula models by learning to remember the forgotten dependencies from each process, theoretically recovering the true copula at optimality. The first instantiation of our framework focuses on direct density estimation, while the second specialises in expedient sampling. Empirically, we demonstrate the superior performance of our proposed methods over state-of-the-art copula approaches in modelling complex and high-dimensional dependencies from scientific datasets and images. Our work enhances the representational power of copula models, empowering applications and paving the way for their adoption on larger scales and more challenging domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19707v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Huk, Theodoros Damoulas</dc:creator>
    </item>
    <item>
      <title>Roughness Analysis of Realized Volatility and VIX through Randomized Kolmogorov-Smirnov Distribution</title>
      <link>https://arxiv.org/abs/2509.20015</link>
      <description>arXiv:2509.20015v1 Announce Type: cross 
Abstract: We introduce a novel distribution-based estimator for the Hurst parameter of log-volatility, leveraging the Kolmogorov-Smirnov statistic to assess the scaling behavior of entire distributions rather than individual moments. To address the temporal dependence of financial volatility, we propose a random permutation procedure that effectively removes serial correlation while preserving marginal distributions, enabling the rigorous application of the KS framework to dependent data. We establish the asymptotic variance of the estimator, useful for inference and confidence interval construction. From a computational standpoint, we show that derivative-free optimization methods, particularly Brent's method and the Nelder-Mead simplex, achieve substantial efficiency gains relative to grid search while maintaining estimation accuracy. Empirical analysis of the CBOE VIX index and the 5-minute realized volatility of the S&amp;P 500 reveals a statistically significant hierarchy of roughness, with implied volatility smoother than realized volatility. Both measures, however, exhibit Hurst exponents well below one-half, reinforcing the rough volatility paradigm and highlighting the open challenge of disentangling local roughness from long-memory effects in fractional modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20015v1</guid>
      <category>q-fin.MF</category>
      <category>q-fin.CP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sergio Bianchi, Daniele Angelini</dc:creator>
    </item>
    <item>
      <title>Monitoring Violations of Differential Privacy over Time</title>
      <link>https://arxiv.org/abs/2509.20283</link>
      <description>arXiv:2509.20283v1 Announce Type: cross 
Abstract: Auditing differential privacy has emerged as an important area of research that supports the design of privacy-preserving mechanisms. Privacy audits help to obtain empirical estimates of the privacy parameter, to expose flawed implementations of algorithms and to compare practical with theoretical privacy guarantees. In this work, we investigate an unexplored facet of privacy auditing: the sustained auditing of a mechanism that can go through changes during its development or deployment. Monitoring the privacy of algorithms over time comes with specific challenges. Running state-of-the-art (static) auditors repeatedly requires excessive sampling efforts, while the reliability of such methods deteriorates over time without proper adjustments. To overcome these obstacles, we present a new monitoring procedure that extracts information from the entire deployment history of the algorithm. This allows us to reduce sampling efforts, while sustaining reliable outcomes of our auditor. We derive formal guarantees with regard to the soundness of our methods and evaluate their performance for important mechanisms from the literature. Our theoretical findings and experiments demonstrate the efficacy of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20283v1</guid>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"Onder Askin, Tim Kutta, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Factor and Idiosyncratic VAR Volatility Matrix Models for Heavy-Tailed High-Frequency Financial Observations</title>
      <link>https://arxiv.org/abs/2109.05227</link>
      <description>arXiv:2109.05227v3 Announce Type: replace 
Abstract: This paper introduces a novel process for both factor and idiosyncratic volatility matrices whose eigenvalues follow the vector auto-regressive (VAR) model. We call it the factor and idiosyncratic VAR (FIVAR) model. The FIVAR model accounts for the dynamics of the factor and idiosyncratic volatilities and includes many parameters. In addition, many empirical studies have shown that high-frequency stock returns and volatilities often exhibit heavy tails. To handle these two problems simultaneously, we propose a penalized optimization procedure with a truncation scheme for parameter estimation. We apply the proposed parameter estimation procedure to predicting large volatility matrices and establish its asymptotic properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.05227v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minseok Shin, Donggyu Kim, Yazhen Wang, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Time-Varying Coefficient Estimation in Diffusion Models</title>
      <link>https://arxiv.org/abs/2202.08419</link>
      <description>arXiv:2202.08419v5 Announce Type: replace 
Abstract: In this paper, we develop a novel high-dimensional time-varying coefficient estimation method, based on high-dimensional Ito diffusion processes. To account for high-dimensional time-varying coefficients, we first estimate local (or instantaneous) coefficients using a time-localized Dantzig selection scheme under a sparsity condition, which results in biased local coefficient estimators due to the regularization. To handle the bias, we propose a debiasing scheme, which provides well-performing unbiased local coefficient estimators. With the unbiased local coefficient estimators, we estimate the integrated coefficient, and to further account for the sparsity of the coefficient process, we apply thresholding schemes. We call this Thresholding dEbiased Dantzig (TED). We establish asymptotic properties of the proposed TED estimator. In the empirical analysis, we apply the TED procedure to analyzing high-dimensional factor models using high-frequency data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.08419v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Donggyu Kim, Minseog Oh, Minseok Shin</dc:creator>
    </item>
    <item>
      <title>Robust High-Dimensional Time-Varying Coefficient Estimation</title>
      <link>https://arxiv.org/abs/2302.13658</link>
      <description>arXiv:2302.13658v3 Announce Type: replace 
Abstract: In this paper, we develop a novel high-dimensional coefficient estimation procedure based on high-frequency data. Unlike usual high-dimensional regression procedures such as LASSO, we additionally handle the heavy-tailedness of high-frequency observations as well as time variations of coefficient processes. Specifically, we employ the Huber loss and a truncation scheme to handle heavy-tailed observations, while $\ell_{1}$-regularization is adopted to overcome the curse of dimensionality. To account for the time-varying coefficient, we estimate local coefficients which are biased due to the $\ell_{1}$-regularization. Thus, when estimating integrated coefficients, we propose a debiasing scheme to enjoy the law of large numbers property and employ a thresholding scheme to further accommodate the sparsity of the coefficients. We call this Robust thrEsholding Debiased LASSO (RED-LASSO) estimator. We show that the RED-LASSO estimator can achieve a near-optimal convergence rate. In the empirical study, we apply the RED-LASSO procedure to the high-dimensional integrated coefficient estimation using high-frequency trading data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13658v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minseok Shin, Donggyu Kim</dc:creator>
    </item>
    <item>
      <title>Bayesian Geostatistics Using Predictive Stacking</title>
      <link>https://arxiv.org/abs/2304.12414</link>
      <description>arXiv:2304.12414v4 Announce Type: replace 
Abstract: We develop Bayesian predictive stacking for geostatistical models, where the primary inferential objective is to provide inference on the latent spatial random field and conduct spatial predictions at arbitrary locations. We exploit analytically tractable posterior distributions for regression coefficients of predictors and the realizations of the spatial process conditional upon process parameters. We subsequently combine such inference by stacking these models across the range of values of the hyper-parameters. We devise stacking of means and posterior densities in a manner that is computationally efficient without resorting to iterative algorithms such as Markov chain Monte Carlo (MCMC) and can exploit the benefits of parallel computations. We offer novel theoretical insights into the resulting inference within an infill asymptotic paradigm and through empirical results showing that stacked inference is comparable to full sampling-based Bayesian inference at a significantly lower computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.12414v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Zhang, Wenpin Tang, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Matrix Autoregressive Model with Vector Time Series Covariates for Spatio-Temporal Data</title>
      <link>https://arxiv.org/abs/2305.15671</link>
      <description>arXiv:2305.15671v4 Announce Type: replace 
Abstract: We develop a new methodology for forecasting matrix-valued time series with historical matrix data and auxiliary vector time series data. We focus on a time series of matrices defined on a static 2-D spatial grid and an auxiliary time series of non-spatial vectors. The proposed model, Matrix AutoRegression with Auxiliary Covariates (MARAC), contains an autoregressive component for the historical matrix predictors and an additive component that maps the auxiliary vector predictors to a matrix response via tensor-vector product. The autoregressive component adopts a bi-linear transformation framework following Chen et al. (2021), significantly reducing the number of parameters. The auxiliary component posits that the tensor coefficient, which maps non-spatial predictors to a spatial response, contains slices of spatially smooth matrix coefficients that are discrete evaluations of smooth functions on a spatial grid from a Reproducing Kernel Hilbert Space (RKHS). We propose to estimate the model parameters under a penalized maximum likelihood estimation framework coupled with an alternating minimization algorithm. We establish the joint asymptotics of the autoregressive and tensor parameters under fixed and high-dimensional regimes. Extensive simulations and a geophysical application for forecasting the global Total Electron Content (TEC) are conducted to validate the performance of MARAC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.15671v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hu Sun, Zuofeng Shang, Yang Chen</dc:creator>
    </item>
    <item>
      <title>Adaptive Neyman Allocation</title>
      <link>https://arxiv.org/abs/2309.08808</link>
      <description>arXiv:2309.08808v4 Announce Type: replace 
Abstract: In the experimental design literature, Neyman allocation refers to the practice of allocating units into treated and control groups, potentially in unequal numbers proportional to their respective standard deviations, with the objective of minimizing the variance of the treatment effect estimator. This widely recognized approach increases statistical power in scenarios where the treated and control groups have different standard deviations, as is often the case in social experiments, clinical trials, marketing research, and online A/B testing. However, Neyman allocation cannot be implemented unless the standard deviations are known in advance. Fortunately, the multi-stage nature of the aforementioned applications allows the use of earlier stage observations to estimate the standard deviations, which further guide allocation decisions in later stages. In this paper, we introduce a competitive analysis framework to study this multi-stage experimental design problem. We propose a simple adaptive Neyman allocation algorithm, which almost matches the information-theoretic limit of conducting experiments. We provide theory for estimation and inference using data collected from our adaptive Neyman allocation algorithm. We demonstrate the effectiveness of our adaptive Neyman allocation algorithm using both online A/B testing data from a social media site and synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08808v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>YEAST: Yet Another Sequential Test</title>
      <link>https://arxiv.org/abs/2406.16523</link>
      <description>arXiv:2406.16523v3 Announce Type: replace 
Abstract: Online evaluation of machine learning models is typically conducted through A/B experiments. Sequential statistical tests are valuable tools for analysing these experiments, as they enable researchers to stop data collection early without increasing the risk of false discoveries. However, existing sequential tests either limit the number of interim analyses or suffer from low statistical power. In this paper, we introduce a novel sequential test designed for continuous monitoring of A/B experiments. We validate our method using semi-synthetic simulations and demonstrate that it outperforms current state-of-the-art sequential testing approaches. Our method is derived using a new technique that inverts a bound on the probability of threshold crossing, based on a classical maximal inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16523v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexey Kurennoy, Majed Dodin, Tural Gurbanov, Ana Peleteiro Ramallo</dc:creator>
    </item>
    <item>
      <title>Subsampled One-Step Estimation for Fast Statistical Inference</title>
      <link>https://arxiv.org/abs/2407.13446</link>
      <description>arXiv:2407.13446v2 Announce Type: replace 
Abstract: Subsampling is an effective approach to alleviate the computational burden associated with large-scale datasets. Nevertheless, existing subsampling estimators incur a substantial loss in estimation efficiency compared to estimators based on the full dataset. Specifically, the convergence rate of existing subsampling estimators is typically $n^{-1/2}$ rather than $N^{-1/2}$, where $n$ and $N$ denote the subsample and full data sizes, respectively. This paper proposes a subsampled one-step (SOS) method to mitigate the estimation efficiency loss utilizing the asymptotic expansions of the subsampling and full-data estimators. The resulting SOS estimator is computationally efficient and achieves a fast convergence rate of $\max\{n^{-1}, N^{-1/2}\}$ rather than $n^{-1/2}$. We establish the asymptotic distribution of the SOS estimator, which can be non-normal in general, and construct confidence intervals on top of the asymptotic distribution. Furthermore, we prove that the SOS estimator is asymptotically normal and equivalent to the full data-based estimator when $n / \sqrt{N} \to \infty$.Simulation studies and real data analyses were conducted to demonstrate the finite sample performance of the SOS estimator. Numerical results suggest that the SOS estimator is almost as computationally efficient as the uniform subsampling estimator while achieving similar estimation efficiency to the full data-based estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13446v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/sjos.70022</arxiv:DOI>
      <dc:creator>Miaomiao Su, Ruoyu Wang</dc:creator>
    </item>
    <item>
      <title>The role of assignment in defining and identifying causal effects in randomized trials</title>
      <link>https://arxiv.org/abs/2408.14710</link>
      <description>arXiv:2408.14710v2 Announce Type: replace 
Abstract: In randomized trials, the per-protocol effect, that is, the effect of being assigned a treatment strategy and receiving treatment according to the assigned strategy, is sometimes thought to reflect the effect of the treatment strategy itself, without intervention on assignment. Here, we argue by example that this is not necessarily the case. We examine a causal structure for a randomized trial where these two causal estimands -- the per-protocol effect and the effect of the treatment strategy -- are not equal, and where their corresponding identifying observed data functionals are not the same, but both require information on assignment for identification. Our example highlights the conceptual difference between the per-protocol effect and the effect of the treatment strategy, the conditions under which these causal estimands are equal, and suggests that in some cases their identification requires information on assignment, even when assignment is randomized. Furthermore, both per-protocol effects and effects of treatment may be unidentifiable without information on treatment assignment, unless one makes additional assumptions -- informally, that assignment does not affect the outcome except through treatment (i.e., an exclusion-restriction assumption), and that assignment is not a confounder of the treatment-outcome association conditional on other variables in the analysis. Our analyses suggest a need to more clearly define the role of assignment when specifying causal effects of interest in randomized trials, which has implications for identification, analysis methods, and the interpretation of trial results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14710v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Issa J. Dahabreh, Lawson Ung, Miguel A. Hern\'an, Yu-Han Chiu</dc:creator>
    </item>
    <item>
      <title>Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices</title>
      <link>https://arxiv.org/abs/2502.01512</link>
      <description>arXiv:2502.01512v4 Announce Type: replace 
Abstract: Circular and non-flat data distributions are prevalent across diverse domains of data science, yet their specific geometric structures often remain underutilized in machine learning frameworks. A principled approach to accounting for the underlying geometry of such data is pivotal, particularly when extending statistical models, like the pervasive Gaussian distribution. In this work, we tackle those issue by focusing on the manifold of symmetric positive definite (SPD) matrices, a key focus in information geometry. We introduce a non-isotropic wrapped Gaussian by leveraging the exponential map, we derive theoretical properties of this distribution and propose a maximum likelihood framework for parameter estimation. Furthermore, we reinterpret established classifiers on SPD through a probabilistic lens and introduce new classifiers based on the wrapped Gaussian model. Experiments on synthetic and real-world datasets demonstrate the robustness and flexibility of this geometry-aware distribution, underscoring its potential to advance manifold-based data analysis. This work lays the groundwork for extending classical machine learning and statistical methods to more complex and structured data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01512v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thibault de Surrel, Fabien Lotte, Sylvain Chevallier, Florian Yger</dc:creator>
    </item>
    <item>
      <title>A Restricted Latent Class Hidden Markov Model for Polytomous Responses, Polytomous Attributes, and Covariates: Identifiability and Application</title>
      <link>https://arxiv.org/abs/2503.20940</link>
      <description>arXiv:2503.20940v3 Announce Type: replace 
Abstract: We introduce a restricted latent class exploratory model for longitudinal data with ordinal attributes and respondent-specific covariates. Responses follow a time inhomogeneous hidden Markov model where the probability of a particular latent state at a time point is conditional on values at the previous time point of the respondent's covariates and latent state. We prove that the model is identifiable, state a Bayesian formulation, and demonstrate its efficacy in a variety of scenarios through two simulation studies. We apply the model to response data from a mathematics examination, comparing the results to a previously published confirmatory analysis, and also apply it to emotional state response data which was measured over a several-day period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20940v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Alan Wayman, Steven Andrew Culpepper, Jeff Douglas, Jesse Bowers</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Properties of Generalized Ridge Estimators in Nonlinear Models</title>
      <link>https://arxiv.org/abs/2504.19018</link>
      <description>arXiv:2504.19018v2 Announce Type: replace 
Abstract: This paper addresses the longstanding challenge of analyzing the mean squared error (MSE) of ridge-type estimators in nonlinear models, including duration, Poisson, and multinomial choice models, where theoretical results have been scarce. Using a finite-sample approximation technique from the econometrics literature, we derive new results showing that the generalized ridge maximum likelihood estimator (MLE) with a sufficiently small penalty achieves lower finite-sample MSE for both estimation and prediction than the conventional MLE, regardless of whether the hypotheses incorporated in the penalty are valid. A key theoretical contribution is to demonstrate that generalized ridge estimators generate a variance-bias trade-off in the first-order MSE of nonlinear likelihood-based models -- a feature absent for the conventional MLE -- which enables ridge-type estimators to attain smaller MSE when the penalty is properly selected. Extensive simulations and an empirical application to the estimation of marginal mean and quantile treatment effects further confirm the superior performance and practical relevance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19018v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masamune Iwasawa</dc:creator>
    </item>
    <item>
      <title>Assessing Bias in the Variable Bandpass Periodic Block Bootstrap Method</title>
      <link>https://arxiv.org/abs/2509.08647</link>
      <description>arXiv:2509.08647v2 Announce Type: replace 
Abstract: The Variable Bandpass Periodic Block Bootstrap(VBPBB) is an innovative method for time series with periodically correlated(PC) components. This method applies bandpass filters to extract specific PC components from datasets, effectively eliminating unwanted interference such as noise. It then bootstraps the PC components, maintaining their correlation structure while resampling and enabling a clearer analysis of the estimation of the statistical properties of periodic patterns in time series data. While its efficiency has been demonstrated in environmental and epidemiological research, the theoretical properties of VBPBB, particularly regarding its bias of the estimated sampling distributions, remain unexamined. This study investigates issues regarding biases in VBPBB, including overall mean bias and pointwise mean bias, across a range of time series models of varying complexity, all of which exhibit periodic components. Using the R programming language, we simulate various PC time series and apply VBPBB to assess its bias under different conditions. Our findings provide key insights into the validity of VBPBB for periodic time series analysis and offer practical recommendations for its implementation, as well as directions for future theoretical advancements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08647v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanan Sun, Eric Rose, Kai Zhang, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Modelling with Sensitive Variables</title>
      <link>https://arxiv.org/abs/2403.15220</link>
      <description>arXiv:2403.15220v3 Announce Type: replace-cross 
Abstract: The paper deals with models in which the dependent variable, some explanatory variables, or both represent sensitive data. We introduce a novel discretization method that preserves data privacy when working with such variables. A multiple discretization method is proposed that utilizes information from the different discretization schemes. We show convergence in distribution for the unobserved variable and derive the asymptotic properties of the OLS estimator for linear models. Monte Carlo simulation experiments presented support our theoretical findings. Finally, we contrast our method with a differential privacy method to estimate the Australian gender wage gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15220v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Chan, Laszlo Matyas, Agoston Reguly</dc:creator>
    </item>
    <item>
      <title>A Scalable Nystr\"om-Based Kernel Two-Sample Test with Permutations</title>
      <link>https://arxiv.org/abs/2502.13570</link>
      <description>arXiv:2502.13570v3 Announce Type: replace-cross 
Abstract: Two-sample hypothesis testing-determining whether two sets of data are drawn from the same distribution-is a fundamental problem in statistics and machine learning with broad scientific applications. In the context of nonparametric testing, maximum mean discrepancy (MMD) has gained popularity as a test statistic due to its flexibility and strong theoretical foundations. However, its use in large-scale scenarios is plagued by high computational costs. In this work, we use a Nystr\"om approximation of the MMD to design a computationally efficient and practical testing algorithm while preserving statistical guarantees. Our main result is a finite-sample bound on the power of the proposed test for distributions that are sufficiently separated with respect to the MMD. The derived separation rate matches the known minimax optimal rate in this setting. We support our findings with a series of numerical experiments, emphasizing applicability to realistic scientific data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13570v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Chatalic, Marco Letizia, Nicolas Schreuder, Lorenzo Rosasco</dc:creator>
    </item>
    <item>
      <title>Stein's unbiased risk estimate and Hyv\"arinen's score matching</title>
      <link>https://arxiv.org/abs/2502.20123</link>
      <description>arXiv:2502.20123v2 Announce Type: replace-cross 
Abstract: Given a collection of observed signals corrupted with Gaussian noise, how can we learn to optimally denoise them? This fundamental problem arises in both empirical Bayes and generative modeling. In empirical Bayes, the predominant approach is via nonparametric maximum likelihood estimation (NPMLE), while in generative modeling, score matching (SM) methods have proven very successful. In our setting, Hyv\"arinen's implicit SM is equivalent to another classical idea from statistics -- Stein's Unbiased Risk Estimate (SURE). Revisiting SURE minimization, we establish, for the first time, that SURE achieves nearly parametric rates of convergence of the regret in the classical empirical Bayes setting with homoscedastic noise. We also prove that SURE-training can achieve fast rates of convergence to the oracle denoiser in a commonly studied misspecified model. In contrast, the NPMLE may not even converge to the oracle denoiser under misspecification of the class of signal distributions. We show how to practically implement our method in settings involving heteroscedasticity and side-information, such as in an application to the estimation of economic mobility in the Opportunity Atlas. Our empirical results demonstrate the superior performance of SURE-training over NPMLE under misspecification. Collectively, our findings advance SURE/SM as a strong alternative to the NPMLE for empirical Bayes problems in both theory and practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20123v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sulagna Ghosh, Nikolaos Ignatiadis, Frederic Koehler, Amber Lee</dc:creator>
    </item>
    <item>
      <title>Three Distributional Approaches for PM10 Assessment in Northern Italy</title>
      <link>https://arxiv.org/abs/2509.13886</link>
      <description>arXiv:2509.13886v2 Announce Type: replace-cross 
Abstract: We propose three spatial methods for estimating the full probability distribution of PM10 concentrations, with the ultimate goal of assessing air quality in Northern Italy. Moving beyond spatial averages and simple indicators, we adopt a distributional perspective to capture the complex variability of pollutant concentrations across space. The first proposed approach predicts class-based compositions via Fixed Rank Kriging; the second estimates multiple, non-crossing quantiles through a spatial regression with differential regularization; the third directly reconstructs full probability densities leveraging on both Fixed Rank Kriging and multiple quantiles spatial regression within a Simplicial Principal Component Analysis framework. These approaches are applied to daily PM10 measurements, collected from 2018 to 2022 in Northern Italy, to estimate spatially continuous distributions and to identify regions at risk of regulatory exceedance. The three approaches exhibit localized differences, revealing how modeling assumptions may influence the prediction of fine-scale pollutant concentration patterns. Nevertheless, they consistently agree on the broader spatial patterns of pollution. This general agreement supports the robustness of a distributional approach, which offers a comprehensive and policy-relevant framework for assessing air quality and regulatory exceedance risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13886v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marco F. De Sanctis, Andrea Gilardi, Giacomo Milan, Laura M. Sangalli, Francesca Ieva, Piercesare Secchi</dc:creator>
    </item>
    <item>
      <title>A Generative Conditional Distribution Equality Testing Framework and Its Minimax Analysis</title>
      <link>https://arxiv.org/abs/2509.17729</link>
      <description>arXiv:2509.17729v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a general framework for testing the equality of the conditional distributions in a two-sample problem. This problem is most relevant to transfer learning under covariate shift. Our framework is built on neural network-based generative methods and sample splitting techniques by transforming the conditional distribution testing problem into an unconditional one. We introduce two special tests: the generative permutation-based conditional distribution equality test and the generative classification accuracy-based conditional distribution equality test. Theoretically, we establish a minimax lower bound for statistical inference in testing the equality of two conditional distributions under certain smoothness conditions. We demonstrate that the generative permutation-based conditional distribution equality test and its modified version can attain this lower bound precisely or up to some iterated logarithmic factor. Moreover, we prove the testing consistency of the generative classification accuracy-based conditional distribution equality test. We also establish the convergence rate for the learned conditional generator by deriving new results related to the recently-developed offset Rademacher complexity and approximation properties using neural networks. Empirically, we conduct numerical studies including synthetic datasets and two real-world datasets, demonstrating the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17729v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 25 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Siming Zheng, Meifang Lan, Tong Wang, Yuanyuan Lin</dc:creator>
    </item>
  </channel>
</rss>
