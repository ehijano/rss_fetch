<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Nov 2024 05:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>LLM-itation is the Sincerest Form of Data: Generating Synthetic Buggy Code Submissions for Computing Education</title>
      <link>https://arxiv.org/abs/2411.10455</link>
      <description>arXiv:2411.10455v1 Announce Type: new 
Abstract: There is a great need for data in computing education research. Data is needed to understand how students behave, to train models of student behavior to optimally support students, and to develop and validate new assessment tools and learning analytics techniques. However, relatively few computing education datasets are shared openly, often due to privacy regulations and issues in making sure the data is anonymous. Large language models (LLMs) offer a promising approach to create large-scale, privacy-preserving synthetic data, which can be used to explore various aspects of student learning, develop and test educational technologies, and support research in areas where collecting real student data may be challenging or impractical. This work explores generating synthetic buggy code submissions for introductory programming exercises using GPT-4o. We compare the distribution of test case failures between synthetic and real student data from two courses to analyze the accuracy of the synthetic data in mimicking real student data. Our findings suggest that LLMs can be used to generate synthetic incorrect submissions that are not significantly different from real student data with regard to test case failure distributions. Our research contributes to the development of reliable synthetic datasets for computing education research and teaching, potentially accelerating progress in the field while preserving student privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10455v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juho Leinonen, Paul Denny, Olli Kiljunen, Stephen MacNeil, Sami Sarsa, Arto Hellas</dc:creator>
    </item>
    <item>
      <title>Predicting the winner of the US 2024 elections using trust analytics</title>
      <link>https://arxiv.org/abs/2411.10457</link>
      <description>arXiv:2411.10457v1 Announce Type: new 
Abstract: A number of models and techniques has been proposed for predicting the outcomes of presidential elections. Some of them use information on the socio-economical status of a country, others focus on candidates' popularity measures in news media. We employ a computational social science approach, utilising public reactions in social media to real-life events that involve presidential candidates. Contrary to the popular approach, we do not analyse public emotions but ethotic references to the character of politicians which allows us to analyse how much they are (dis-)trusted by the general public, hence the name of the tool we developed: Trust Analytics (TrustAn). Similarly to major news media's polls, we observe a tight race between Harris and Trump with week to week changes in the level of trust and distrust towards the two candidates. Using the ratio between the level of trust and distrust towards them and changes of this metric in time, we predict Donald Trump as the winner of the US 2024 elections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10457v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Budzynska Katarzyna, Gajewska Ewelina</dc:creator>
    </item>
    <item>
      <title>AI Safety Frameworks Should Include Procedures for Model Access Decisions</title>
      <link>https://arxiv.org/abs/2411.10547</link>
      <description>arXiv:2411.10547v1 Announce Type: new 
Abstract: The downstream use cases, benefits, and risks of AI models depend significantly on what sort of access is provided to the model, and who it is provided to. Though existing safety frameworks and AI developer usage policies recognise that the risk posed by a given model depends on the level of access provided to a given audience, the procedures they use to make decisions about model access are ad hoc, opaque, and lacking in empirical substantiation. This paper consequently proposes that frontier AI companies build on existing safety frameworks by outlining transparent procedures for making decisions about model access, which we term Responsible Access Policies (RAPs). We recommend that, at a minimum, RAPs should include the following: i) processes for empirically evaluating model capabilities given different styles of access, ii) processes for assessing the risk profiles of different categories of user, and iii) clear and robust pre-commitments regarding when to grant or revoke specific types of access for particular groups under specified conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10547v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward Kembery, Tom Reed</dc:creator>
    </item>
    <item>
      <title>Labeled Datasets for Research on Information Operations</title>
      <link>https://arxiv.org/abs/2411.10609</link>
      <description>arXiv:2411.10609v1 Announce Type: new 
Abstract: Social media platforms have become a hub for political activities and discussions, democratizing participation in these endeavors. However, they have also become an incubator for manipulation campaigns, like information operations (IOs). Some social media platforms have released datasets related to such IOs originating from different countries. However, we lack comprehensive control data that can enable the development of IO detection methods. To bridge this gap, we present new labeled datasets about 26 campaigns, which contain both IO posts verified by a social media platform and over 13M posts by 303k accounts that discussed similar topics in the same time frames (control data). The datasets will facilitate the study of narratives, network interactions, and engagement strategies employed by coordinated accounts across various campaigns and countries. By comparing these coordinated accounts against organic ones, researchers can develop and benchmark IO detection algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10609v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ozgur Can Seckin, Manita Pote, Alexander Nwala, Lake Yin, Luca Luceri, Alessandro Flammini, Filippo Menczer</dc:creator>
    </item>
    <item>
      <title>Transforming Teacher Education in Developing Countries: The Role of Generative AI in Bridging Theory and Practice</title>
      <link>https://arxiv.org/abs/2411.10718</link>
      <description>arXiv:2411.10718v1 Announce Type: new 
Abstract: This study examines the transformative potential of Generative AI (GenAI) in teacher education within developing countries, focusing on Ghana, where challenges such as limited pedagogical modeling, performance-based assessments, and practitioner-expertise gaps hinder progress. GenAI has the capacity to address these issues by supporting content knowledge acquisition, a role that currently dominates teacher education programs. By taking on this foundational role, GenAI allows teacher educators to redirect their focus to other critical areas, including pedagogical modeling, authentic assessments, and fostering digital literacy and critical thinking. These roles are interconnected, creating a ripple effect where pre-service teachers (PSTs) are better equipped to enhance K-12 learning outcomes and align education with workforce needs. The study emphasizes that GenAI's roles are multifaceted, directly addressing resistance to change, improving resource accessibility, and supporting teacher professional development. However, it cautions against misuse, which could undermine critical thinking and creativity, essential skills nurtured through traditional teaching methods. To ensure responsible and effective integration, the study advocates a scaffolding approach to GenAI literacy. This includes educating PSTs on its supportive role, training them in ethical use and prompt engineering, and equipping them to critically assess AI-generated content for biases and validity. The study concludes by recommending empirical research to explore these roles further and develop practical steps for integrating GenAI into teacher education systems responsibly and effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10718v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Nyaaba</dc:creator>
    </item>
    <item>
      <title>Evaluating Generative AI Systems is a Social Science Measurement Challenge</title>
      <link>https://arxiv.org/abs/2411.10939</link>
      <description>arXiv:2411.10939v1 Announce Type: new 
Abstract: Across academia, industry, and government, there is an increasing awareness that the measurement tasks involved in evaluating generative AI (GenAI) systems are especially difficult. We argue that these measurement tasks are highly reminiscent of measurement tasks found throughout the social sciences. With this in mind, we present a framework, grounded in measurement theory from the social sciences, for measuring concepts related to the capabilities, impacts, opportunities, and risks of GenAI systems. The framework distinguishes between four levels: the background concept, the systematized concept, the measurement instrument(s), and the instance-level measurements themselves. This four-level approach differs from the way measurement is typically done in ML, where researchers and practitioners appear to jump straight from background concepts to measurement instruments, with little to no explicit systematization in between. As well as surfacing assumptions, thereby making it easier to understand exactly what the resulting measurements do and do not mean, this framework has two important implications for evaluating evaluations: First, it can enable stakeholders from different worlds to participate in conceptual debates, broadening the expertise involved in evaluating GenAI systems. Second, it brings rigor to operational debates by offering a set of lenses for interrogating the validity of measurement instruments and their resulting measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10939v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanna Wallach, Meera Desai, Nicholas Pangakis, A. Feder Cooper, Angelina Wang, Solon Barocas, Alexandra Chouldechova, Chad Atalla, Su Lin Blodgett, Emily Corvi, P. Alex Dow, Jean Garcia-Gathright, Alexandra Olteanu, Stefanie Reed, Emily Sheng, Dan Vann, Jennifer Wortman Vaughan, Matthew Vogel, Hannah Washington, Abigail Z. Jacobs</dc:creator>
    </item>
    <item>
      <title>From Crime to Hypercrime: Evolving Threats and Law Enforcement's New Mandate in the AI Age</title>
      <link>https://arxiv.org/abs/2411.10995</link>
      <description>arXiv:2411.10995v1 Announce Type: new 
Abstract: The paper examines the trajectory of crime, tracing its evolution from traditional forms to digital manifestations in cybercrime, and proposes "Hypercrime" as the latest frontier. Leveraging insights from Michael McGuire's "Hypercrime: The New Geometry of Harm," the study calls for a paradigm shift in law enforcement strategies to meet the challenges posed by AI-driven hypercrime. Emphasis is placed on understanding hypercrime's complexity, developing proactive policies, and embracing technological tools to mitigate risks associated with AI misuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10995v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Francesco Schiliro</dc:creator>
    </item>
    <item>
      <title>Flood Risk Assessment of the National Harbor at Maryland, United States</title>
      <link>https://arxiv.org/abs/2411.11014</link>
      <description>arXiv:2411.11014v1 Announce Type: new 
Abstract: Over the past few decades, floods have become one of the costliest natural hazards and losses have sharply escalated. Floods are an increasing problem in urban areas due to increased residential settlement along the coastline and climate change is a contributing factor to this increased frequency. In order to analyze flood risk, a model is proposed to identify the factors associated with increased flooding at a local scale. The study area includes National Harbor, MD, and the surrounding area of Fort Washington. The objective is to assess flood risk due to an increase in sea level rise for the study area of interest. The study demonstrated that coastal flood risk increased with sea level rise even though the predicted level of impact is fairly insignificant for the study area. The level of impact from increased flooding is highly dependent on the location of the properties and other topographic information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11014v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neftalem Negussie, Addis Yesserie, Chinchu Harris, Abou Keita, Huthaifa I. Ashqar</dc:creator>
    </item>
    <item>
      <title>Early Adoption of Generative Artificial Intelligence in Computing Education: Emergent Student Use Cases and Perspectives in 2023</title>
      <link>https://arxiv.org/abs/2411.11166</link>
      <description>arXiv:2411.11166v1 Announce Type: new 
Abstract: Because of the rapid development and increasing public availability of Generative Artificial Intelligence (GenAI) models and tools, educational institutions and educators must immediately reckon with the impact of students using GenAI. There is limited prior research on computing students' use and perceptions of GenAI. In anticipation of future advances and evolutions of GenAI, we capture a snapshot of student attitudes towards and uses of yet emerging GenAI, in a period of time before university policies had reacted to these technologies. We surveyed all computer science majors in a small engineering-focused R1 university in order to: (1) capture a baseline assessment of how GenAI has been immediately adopted by aspiring computer scientists; (2) describe computing students' GenAI-related needs and concerns for their education and careers; and (3) discuss GenAI influences on CS pedagogy, curriculum, culture, and policy. We present an exploratory qualitative analysis of this data and discuss the impact of our findings on the emerging conversation around GenAI and education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11166v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3649217.3653575</arxiv:DOI>
      <dc:creator>C. Estelle Smith, Kylee Shiekh, Hayden Cooreman, Sharfi Rahman, Yifei Zhu, Md Kamrul Siam, Michael Ivanitskiy, Ahmed M. Ahmed, Michael Hallinan, Alexander Grisak, Gabe Fierro</dc:creator>
    </item>
    <item>
      <title>Investigating the Use of Productive Failure as a Design Paradigm for Learning Introductory Python Programming</title>
      <link>https://arxiv.org/abs/2411.11227</link>
      <description>arXiv:2411.11227v1 Announce Type: new 
Abstract: Productive Failure (PF) is a learning approach where students initially tackle novel problems targeting concepts they have not yet learned, followed by a consolidation phase where these concepts are taught. Recent application in STEM disciplines suggests that PF can help learners develop more robust conceptual knowledge. However, empirical validation of PF for programming education remains under-explored. In this paper, we investigate the use of PF to teach Python lists to undergraduate students with limited prior programming experience. We designed a novel PF-based learning activity that incorporated the unobtrusive collection of real-time heart-rate data from consumer-grade wearable sensors. This sensor data was used both to make the learning activity engaging and to infer cognitive load. We evaluated our approach with 20 participants, half of whom were taught Python concepts using Direct Instruction (DI), and the other half with PF. We found that although there was no difference in initial learning outcomes between the groups, students who followed the PF approach showed better knowledge retention and performance on delayed but similar tasks. In addition, physiological measurements indicated that these students also exhibited a larger decrease in cognitive load during their tasks after instruction. Our findings suggest that PF-based approaches may lead to more robust learning, and that future work should investigate similar activities at scale across a range of concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11227v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3641554.3701911</arxiv:DOI>
      <dc:creator>Hussel Suriyaarachchi, Paul Denny, Suranga Nanayakkara</dc:creator>
    </item>
    <item>
      <title>Design and Development of a Localized E-Commerce Solution for Students focussing on Economical Sharing</title>
      <link>https://arxiv.org/abs/2411.11527</link>
      <description>arXiv:2411.11527v1 Announce Type: new 
Abstract: The rapid adoption of e-commerce has transformed how students access goods and resources. However, existing platforms often fail to address the specific needs of campus communities, where students face challenges such as financial constraints, lack of access to affordable goods, and inefficient resource circulation. This research proposes ShareSpace, a localized web application designed specifically for college students to facilitate the buying, and selling of mainly second-hand goods. By addressing imbalances like surplus items left behind by seniors and shortages experienced by juniors, ShareSpace promotes sustainability and affordability within the campus ecosystem. Leveraging modern technologies such as Node.js, React.js, and MongoDB, the project demonstrates the feasibility of creating a student-centric e-commerce solution. The study highlights how ShareSpace solves the challenges of economical pricing and content moderation using proposed solutions. This study also explores the limitations of existing solutions and evaluates the potential of ShareSpace to encourage sustainable consumption and resourcefulness among students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11527v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faiz Ahmed, Nitin Kumar Jha, Md Faizan</dc:creator>
    </item>
    <item>
      <title>The ethical landscape of robot-assisted surgery. A systematic review</title>
      <link>https://arxiv.org/abs/2411.11637</link>
      <description>arXiv:2411.11637v1 Announce Type: new 
Abstract: Background: Robot-assisted surgery has been widely adopted in recent years. However, compared to other health technologies operating in close proximity to patients in a vulnerable state, ethical issues of robot-assisted surgery have received less attention. Against the background of increasing automation that are expected to raise new ethical issues, this systematic review aims to map the state of the ethical debate in this field.
  Methods: A protocol was registered in the international prospective register of systematic reviews (PROSPERO CRD42023397951). Medline via PubMed, EMBASE, CINHAL, Philosophers' Index, IEEE Xplorer, Web of Science (Core Collection), Scopus and Google Scholar were searched in January 2023. Screening, extraction, and analysis were conducted independently by two authors. A qualitative narrative synthesis was performed.
  Results: Out of 1,723 records, 66 records were included in the final dataset. Seven major strands of the ethical debate emerged during analysis. These include questions of harms and benefits, responsibility and control, professional-patient relationship, ethical issues in surgical training and learning, justice, translational questions, and economic considerations.
  Discussion: The identified themes testify to a broad range of different and differing ethical issues requiring careful deliberation and integration into the surgical ethos. Looking forward, we argue that a different perspective in addressing robotic surgical devices might be helpful to consider upcoming challenges of automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11637v1</guid>
      <category>cs.CY</category>
      <category>cs.RO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joschka Haltaufderheide, Stefanie Pfisterer-Heise, Dawid Pieper, Robert Ranisch</dc:creator>
    </item>
    <item>
      <title>Coordinated Reply Attacks in Influence Operations: Characterization and Detection</title>
      <link>https://arxiv.org/abs/2410.19272</link>
      <description>arXiv:2410.19272v1 Announce Type: cross 
Abstract: Coordinated reply attacks are a tactic observed in online influence operations and other coordinated campaigns to support or harass targeted individuals, or influence them or their followers. Despite its potential to influence the public, past studies have yet to analyze or provide a methodology to detect this tactic. In this study, we characterize coordinated reply attacks in the context of influence operations on Twitter. Our analysis reveals that the primary targets of these attacks are influential people such as journalists, news media, state officials, and politicians.
  We propose two supervised machine-learning models, one to classify tweets to determine whether they are targeted by a reply attack, and one to classify accounts that reply to a targeted tweet to determine whether they are part of a coordinated attack. The classifiers achieve AUC scores of 0.88 and 0.97, respectively. These results indicate that accounts involved in reply attacks can be detected, and the targeted accounts themselves can serve as sensors for influence operation detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19272v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Manita Pote, Tu\u{g}rulcan Elmas, Alessandro Flammini, Filippo Menczer</dc:creator>
    </item>
    <item>
      <title>Gamification and AI: Enhancing User Engagement through Intelligent Systems</title>
      <link>https://arxiv.org/abs/2411.10462</link>
      <description>arXiv:2411.10462v1 Announce Type: cross 
Abstract: Gamification applies game mechanics to non-game environments to motivate and engage users. Artificial Intelligence (AI) offers powerful tools for personalizing and optimizing gamification, adapting to users' needs, preferences, and performance levels. By integrating AI with gamification, systems can dynamically adjust game mechanics, deliver personalized feedback, and predict user behavior, significantly enhancing the effectiveness of gamification efforts. This paper examines the intersection of gamification and AI, exploring AI's methods to optimize gamified experiences and proposing mathematical models for adaptive and predictive gamification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10462v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos J. Costa, Joao Tiago Aparicio, Manuela Aparicio, Sofia Aparicio</dc:creator>
    </item>
    <item>
      <title>PhDGPT: Introducing a psychometric and linguistic dataset about how large language models perceive graduate students and professors in psychology</title>
      <link>https://arxiv.org/abs/2411.10473</link>
      <description>arXiv:2411.10473v1 Announce Type: cross 
Abstract: Machine psychology aims to reconstruct the mindset of Large Language Models (LLMs), i.e. how these artificial intelligences perceive and associate ideas. This work introduces PhDGPT, a prompting framework and synthetic dataset that encapsulates the machine psychology of PhD researchers and professors as perceived by OpenAI's GPT-3.5. The dataset consists of 756,000 datapoints, counting 300 iterations repeated across 15 academic events, 2 biological genders, 2 career levels and 42 unique item responses of the Depression, Anxiety, and Stress Scale (DASS-42). PhDGPT integrates these psychometric scores with their explanations in plain language. This synergy of scores and texts offers a dual, comprehensive perspective on the emotional well-being of simulated academics, e.g. male/female PhD students or professors. By combining network psychometrics and psycholinguistic dimensions, this study identifies several similarities and distinctions between human and LLM data. The psychometric networks of simulated male professors do not differ between physical and emotional anxiety subscales, unlike humans. Other LLMs' personification can reconstruct human DASS factors with a purity up to 80%. Furthemore, LLM-generated personifications across different scenarios are found to elicit explanations lower in concreteness and imageability in items coding for anxiety, in agreement with past studies about human psychology. Our findings indicate an advanced yet incomplete ability for LLMs to reproduce the complexity of human psychometric data, unveiling convenient advantages and limitations in using LLMs to replace human participants. PhDGPT also intriguingly capture the ability for LLMs to adapt and change language patterns according to prompted mental distress contextual features, opening new quantitative opportunities for assessing the machine psychology of these artificial intelligences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10473v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Sebastiano De Duro, Enrique Taietta, Riccardo Improta, Massimo Stella</dc:creator>
    </item>
    <item>
      <title>The Future of Skill: What Is It to Be Skilled at Work?</title>
      <link>https://arxiv.org/abs/2411.10488</link>
      <description>arXiv:2411.10488v1 Announce Type: cross 
Abstract: In this short paper, we introduce work that is aiming to purposefully venture into this mesh of questions from a different starting point. Interjecting into the conversation, we want to ask: 'What is it to be skilled at work?' Building on work from scholars like Tim Ingold, and strands of longstanding research in workplace studies and CSCW, our interest is in turning the attention to the active work of 'being good', or 'being skilled', at what we as workers do. As we see it, skill provides a counterpoint to the version of intelligence that appears to be easily blackboxed in systems like Slack, and that ultimately reduces much of what people do to work well together. To put it slightly differently, skill - as we will argue below - gives us a way into thinking about work as a much more entangled endeavour, unfolding through multiple and interweaving sets of practices, places, tools and collaborations. In this vein, designing for the future of work seems to be about much more than where work is done or how we might bolt on discrete containers of intelligence. More fruitful would be attending to how we succeed in threading so many entities together to do our jobs well - in 'coming to be skilled'.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10488v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel Niklasson, Sean Rintel, Stephann Makri, Alex Taylor</dc:creator>
    </item>
    <item>
      <title>Chain of Alignment: Integrating Public Will with Expert Intelligence for Language Model Alignment</title>
      <link>https://arxiv.org/abs/2411.10534</link>
      <description>arXiv:2411.10534v1 Announce Type: cross 
Abstract: We introduce a method to measure the alignment between public will and language model (LM) behavior that can be applied to fine-tuning, online oversight, and pre-release safety checks. Our `chain of alignment' (CoA) approach produces a rule based reward (RBR) by creating model behavior $\textit{rules}$ aligned to normative $\textit{objectives}$ aligned to $\textit{public will}$. This factoring enables a nonexpert public to directly specify their will through the normative objectives, while expert intelligence is used to figure out rules entailing model behavior that best achieves those objectives. We validate our approach by applying it across three different domains of LM prompts related to mental health. We demonstrate a public input process built on collective dialogues and bridging-based ranking that reliably produces normative objectives supported by at least $96\% \pm 2\%$ of the US public. We then show that rules developed by mental health experts to achieve those objectives enable a RBR that evaluates an LM response's alignment with the objectives similarly to human experts (Pearson's $r=0.841$, $AUC=0.964$). By measuring alignment with objectives that have near unanimous public support, these CoA RBRs provide an approximate measure of alignment between LM behavior and public will.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10534v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Konya, Aviv Ovadya, Kevin Feng, Quan Ze Chen, Lisa Schirch, Colin Irwin, Amy X. Zhang</dc:creator>
    </item>
    <item>
      <title>Debias-CLR: A Contrastive Learning Based Debiasing Method for Algorithmic Fairness in Healthcare Applications</title>
      <link>https://arxiv.org/abs/2411.10544</link>
      <description>arXiv:2411.10544v1 Announce Type: cross 
Abstract: Artificial intelligence based predictive models trained on the clinical notes can be demographically biased. This could lead to adverse healthcare disparities in predicting outcomes like length of stay of the patients. Thus, it is necessary to mitigate the demographic biases within these models. We proposed an implicit in-processing debiasing method to combat disparate treatment which occurs when the machine learning model predict different outcomes for individuals based on the sensitive attributes like gender, ethnicity, race, and likewise. For this purpose, we used clinical notes of heart failure patients and used diagnostic codes, procedure reports and physiological vitals of the patients. We used Clinical BERT to obtain feature embeddings within the diagnostic codes and procedure reports, and LSTM autoencoders to obtain feature embeddings within the physiological vitals. Then, we trained two separate deep learning contrastive learning frameworks, one for gender and the other for ethnicity to obtain debiased representations within those demographic traits. We called this debiasing framework Debias-CLR. We leveraged clinical phenotypes of the patients identified in the diagnostic codes and procedure reports in the previous study to measure fairness statistically. We found that Debias-CLR was able to reduce the Single-Category Word Embedding Association Test (SC-WEAT) effect size score when debiasing for gender and ethnicity. We further found that to obtain fair representations in the embedding space using Debias-CLR, the accuracy of the predictive models on downstream tasks like predicting length of stay of the patients did not get reduced as compared to using the un-debiased counterparts for training the predictive models. Hence, we conclude that our proposed approach, Debias-CLR is fair and representative in mitigating demographic biases and can reduce health disparities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10544v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ankita Agarwal, Tanvi Banerjee, William Romine, Mia Cajita</dc:creator>
    </item>
    <item>
      <title>Being Considerate as a Pathway Towards Pluralistic Alignment for Agentic AI</title>
      <link>https://arxiv.org/abs/2411.10613</link>
      <description>arXiv:2411.10613v1 Announce Type: cross 
Abstract: Pluralistic alignment is concerned with ensuring that an AI system's objectives and behaviors are in harmony with the diversity of human values and perspectives. In this paper we study the notion of pluralistic alignment in the context of agentic AI, and in particular in the context of an agent that is trying to learn a policy in a manner that is mindful of the values and perspective of others in the environment. To this end, we show how being considerate of the future wellbeing and agency of other (human) agents can promote a form of pluralistic alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10613v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parand A. Alamdari, Toryn Q. Klassen, Rodrigo Toro Icarte, Sheila A. McIlraith</dc:creator>
    </item>
    <item>
      <title>Pluralistic Alignment Over Time</title>
      <link>https://arxiv.org/abs/2411.10654</link>
      <description>arXiv:2411.10654v1 Announce Type: cross 
Abstract: If an AI system makes decisions over time, how should we evaluate how aligned it is with a group of stakeholders (who may have conflicting values and preferences)? In this position paper, we advocate for consideration of temporal aspects including stakeholders' changing levels of satisfaction and their possibly temporally extended preferences. We suggest how a recent approach to evaluating fairness over time could be applied to a new form of pluralistic alignment: temporal pluralism, where the AI system reflects different stakeholders' values at different times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10654v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toryn Q. Klassen, Parand A. Alamdari, Sheila A. McIlraith</dc:creator>
    </item>
    <item>
      <title>A Regularized LSTM Method for Detecting Fake News Articles</title>
      <link>https://arxiv.org/abs/2411.10713</link>
      <description>arXiv:2411.10713v1 Announce Type: cross 
Abstract: Nowadays, the rapid diffusion of fake news poses a significant problem, as it can spread misinformation and confusion. This paper aims to develop an advanced machine learning solution for detecting fake news articles. Leveraging a comprehensive dataset of news articles, including 23,502 fake news articles and 21,417 accurate news articles, we implemented and evaluated three machine-learning models. Our dataset, curated from diverse sources, provides rich textual content categorized into title, text, subject, and Date features. These features are essential for training robust classification models to distinguish between fake and authentic news articles. The initial model employed a Long Short-Term Memory (LSTM) network, achieving an accuracy of 94%. The second model improved upon this by incorporating additional regularization techniques and fine-tuning hyperparameters, resulting in a 97% accuracy. The final model combined the strengths of previous architectures with advanced optimization strategies, achieving a peak accuracy of 98%. These results demonstrate the effectiveness of our approach in identifying fake news with high precision. Implementing these models showcases significant advancements in natural language processing and machine learning techniques, contributing valuable tools for combating misinformation. Our work highlights the potential for deploying such models in real-world applications, providing a reliable method for automated fake news detection and enhancing the credibility of news dissemination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10713v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanjina Sultana Camelia, Faizur Rahman Fahim, Md. Musfique Anwar</dc:creator>
    </item>
    <item>
      <title>Large Language Models (LLMs) as Traffic Control Systems at Urban Intersections: A New Paradigm</title>
      <link>https://arxiv.org/abs/2411.10869</link>
      <description>arXiv:2411.10869v1 Announce Type: cross 
Abstract: This study introduces a novel approach for traffic control systems by using Large Language Models (LLMs) as traffic controllers. The study utilizes their logical reasoning, scene understanding, and decision-making capabilities to optimize throughput and provide feedback based on traffic conditions in real-time. LLMs centralize traditionally disconnected traffic control processes and can integrate traffic data from diverse sources to provide context-aware decisions. LLMs can also deliver tailored outputs using various means such as wireless signals and visuals to drivers, infrastructures, and autonomous vehicles. To evaluate LLMs ability as traffic controllers, this study proposed a four-stage methodology. The methodology includes data creation and environment initialization, prompt engineering, conflict identification, and fine-tuning. We simulated multi-lane four-leg intersection scenarios and generates detailed datasets to enable conflict detection using LLMs and Python simulation as a ground truth. We used chain-of-thought prompts to lead LLMs in understanding the context, detecting conflicts, resolving them using traffic rules, and delivering context-sensitive traffic management solutions. We evaluated the prformance GPT-mini, Gemini, and Llama as traffic controllers. Results showed that the fine-tuned GPT-mini achieved 83% accuracy and an F1-score of 0.84. GPT-mini model exhibited a promising performance in generating actionable traffic management insights, with high ROUGE-L scores across conflict identification of 0.95, decision-making of 0.91, priority assignment of 0.94, and waiting time optimization of 0.92. We demonstrated that LLMs can offer precise recommendations to drivers in real-time including yielding, slowing, or stopping based on vehicle dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10869v1</guid>
      <category>cs.CL</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sari Masri, Huthaifa I. Ashqar, Mohammed Elhenawy</dc:creator>
    </item>
    <item>
      <title>Generating medical screening questionnaires through analysis of social media data</title>
      <link>https://arxiv.org/abs/2411.11048</link>
      <description>arXiv:2411.11048v1 Announce Type: cross 
Abstract: Screening questionnaires are used in medicine as a diagnostic aid. Creating them is a long and expensive process, which could potentially be improved through analysis of social media posts related to symptoms and behaviors prior to diagnosis. Here we show a preliminary investigation into the feasibility of generating screening questionnaires for a given medical condition from social media postings. The method first identifies a cohort of relevant users through their posts in dedicated patient groups and a control group of users who reported similar symptoms but did not report being diagnosed with the condition of interest. Posts made prior to diagnosis are used to generate decision rules to differentiate between the different groups, by clustering symptoms mentioned by these users and training a decision tree to differentiate between the two groups. We validate the generated rules by correlating them with scores given by medical doctors to matching hypothetical cases. We demonstrate the proposed method by creating questionnaires for three conditions (endometriosis, lupus, and gout) using the data of several hundreds of users from Reddit. These questionnaires were then validated by medical doctors. The average Pearson's correlation between the latter's scores and the decision rules were 0.58 (endometriosis), 0.40 (lupus) and 0.27 (gout). Our results suggest that the process of questionnaire generation can be, at least partly, automated. These questionnaires are advantageous in that they are based on real-world experience but are currently lacking in their ability to capture the context, duration, and timing of symptoms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11048v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ortal Ashkenazi, Elad Yom-Tov, Liron Vardi David</dc:creator>
    </item>
    <item>
      <title>Different Horses for Different Courses: Comparing Bias Mitigation Algorithms in ML</title>
      <link>https://arxiv.org/abs/2411.11101</link>
      <description>arXiv:2411.11101v1 Announce Type: cross 
Abstract: With fairness concerns gaining significant attention in Machine Learning (ML), several bias mitigation techniques have been proposed, often compared against each other to find the best method. These benchmarking efforts tend to use a common setup for evaluation under the assumption that providing a uniform environment ensures a fair comparison. However, bias mitigation techniques are sensitive to hyperparameter choices, random seeds, feature selection, etc., meaning that comparison on just one setting can unfairly favour certain algorithms. In this work, we show significant variance in fairness achieved by several algorithms and the influence of the learning pipeline on fairness scores. We highlight that most bias mitigation techniques can achieve comparable performance, given the freedom to perform hyperparameter optimization, suggesting that the choice of the evaluation parameters-rather than the mitigation technique itself-can sometimes create the perceived superiority of one method over another. We hope our work encourages future research on how various choices in the lifecycle of developing an algorithm impact fairness, and trends that guide the selection of appropriate algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11101v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prakhar Ganeesh, Usman Gohar, Lu Cheng, Golnoosh Farnadi</dc:creator>
    </item>
    <item>
      <title>Association between built environment characteristics and school run traffic congestion in Beijing, China</title>
      <link>https://arxiv.org/abs/2411.11390</link>
      <description>arXiv:2411.11390v1 Announce Type: cross 
Abstract: School-escorted trips are a significant contributor to traffic congestion. Existing studies mainly compare road traffic during student pick-up/drop-off hours with off-peak times, often overlooking the fact that school-run traffic congestion is unevenly distributed across areas with different built environment characteristics. We examine the relationship between the built environment and school-run traffic congestion, using Beijing, China, as a case study. First, we use multi-source geospatial data to assess the built environment characteristics around schools across five dimensions: spatial concentration, transportation infrastructure, street topology, spatial richness, and scenescapes. Second, employing a generalized ordered logit model, we analyze how traffic congestion around schools varies during peak hours on school days, regular non-school days, and national college entrance exam days. Lastly, we identify the built environment factors contributing to school-run traffic congestion through multivariable linear regression and Shapley value explanations. Our findings reveal that: (1) School runs significantly exacerbate traffic congestion around schools, reducing the likelihood of free-flow by 8.34\% during school run times; (2) School-run traffic congestion is more severe in areas with multiple schools, bus stops, and scenescapes related to business and financial functions. These insights can inform the planning of new schools and urban upgrade strategies aimed at reducing traffic congestion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11390v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaogui Kang, Xiaxin Wu, Jialei Shi, Chao Yang</dc:creator>
    </item>
    <item>
      <title>Alien Recombination: Exploring Concept Blends Beyond Human Cognitive Availability in Visual Art</title>
      <link>https://arxiv.org/abs/2411.11494</link>
      <description>arXiv:2411.11494v1 Announce Type: cross 
Abstract: While AI models have demonstrated remarkable capabilities in constrained domains like game strategy, their potential for genuine creativity in open-ended domains like art remains debated. We explore this question by examining how AI can transcend human cognitive limitations in visual art creation. Our research hypothesizes that visual art contains a vast unexplored space of conceptual combinations, constrained not by inherent incompatibility, but by cognitive limitations imposed by artists' cultural, temporal, geographical and social contexts.
  To test this hypothesis, we present the Alien Recombination method, a novel approach utilizing fine-tuned large language models to identify and generate concept combinations that lie beyond human cognitive availability. The system models and deliberately counteracts human availability bias, the tendency to rely on immediately accessible examples, to discover novel artistic combinations.
  This system not only produces combinations that have never been attempted before within our dataset but also identifies and generates combinations that are cognitively unavailable to all artists in the domain. Furthermore, we translate these combinations into visual representations, enabling the exploration of subjective perceptions of novelty. Our findings suggest that cognitive unavailability is a promising metric for optimizing artistic novelty, outperforming merely temperature scaling without additional evaluation criteria. This approach uses generative models to connect previously unconnected ideas, providing new insight into the potential of framing AI-driven creativity as a combinatorial problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11494v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Hernandez, Levin Brinkmann, Ignacio Serna, Nasim Rahaman, Hassan Abu Alhaija, Hiromu Yakura, Mar Canet Sola, Bernhard Sch\"olkopf, Iyad Rahwan</dc:creator>
    </item>
    <item>
      <title>Timescale-agnostic characterisation for collective attention events</title>
      <link>https://arxiv.org/abs/2411.11500</link>
      <description>arXiv:2411.11500v1 Announce Type: cross 
Abstract: Online communications, and in particular social media, are a key component of how society interacts with and promotes content online. Collective attention on such content can vary wildly. The majority of breaking topics quickly fade into obscurity after only a handful of interactions, while the possibility exists for content to ``go viral'', seeing sustained interaction by large audiences over long periods. In this paper we investigate the mechanisms behind such events and introduce a new representation that enables direct comparison of events over diverse time and volume scales. We find four characteristic behaviours in the usage of hashtags on Twitter that are indicative of different patterns of attention to topics. We go on to develop an agent-based model for generating collective attention events to test the factors affecting emergence of these phenomena. This model can reproduce the characteristic behaviours seen in the Twitter dataset using a small set of parameters, and reveal that three of these behaviours instead represent a continuum determined by model parameters rather than discrete categories. These insights suggest that collective attention in social systems develops in line with a set of universal principles independent of effects inherent to system scale, and the techniques we introduce here present a valuable opportunity to infer the possible mechanisms of attention flow in online communications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11500v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tristan J. B. Cann, Iain S. Weaver, Hywel T. P. Williams</dc:creator>
    </item>
    <item>
      <title>A Pre-Trained Graph-Based Model for Adaptive Sequencing of Educational Documents</title>
      <link>https://arxiv.org/abs/2411.11520</link>
      <description>arXiv:2411.11520v1 Announce Type: cross 
Abstract: Massive Open Online Courses (MOOCs) have greatly contributed to making education more accessible.However, many MOOCs maintain a rigid, one-size-fits-all structure that fails to address the diverse needs and backgrounds of individual learners.Learning path personalization aims to address this limitation, by tailoring sequences of educational content to optimize individual student learning outcomes.Existing approaches, however, often require either massive student interaction data or extensive expert annotation, limiting their broad application.In this study, we introduce a novel data-efficient framework for learning path personalization that operates without expert annotation.Our method employs a flexible recommender system pre-trained with reinforcement learning on a dataset of raw course materials.Through experiments on semi-synthetic data, we show that this pre-training stage substantially improves data-efficiency in a range of adaptive learning scenarios featuring new educational materials.This opens up new perspectives for the design of foundation models for adaptive learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11520v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean Vassoyan (CB), Anan Sch\"utt (UNIA), Jill-J\^enn Vie (SODA), Arun-Balajiee Lekshmi-Narayanan (PITT), Elisabeth Andr\'e (UNIA), Nicolas Vayatis (CB)</dc:creator>
    </item>
    <item>
      <title>An Internet Voting System Fatally Flawed in Creative New Ways</title>
      <link>https://arxiv.org/abs/2411.11796</link>
      <description>arXiv:2411.11796v1 Announce Type: cross 
Abstract: The recently published "MERGE" protocol is designed to be used in the prototype CAC-vote system. The voting kiosk and protocol transmit votes over the internet and then transmit voter-verifiable paper ballots through the mail. In the MERGE protocol, the votes transmitted over the internet are used to tabulate the results and determine the winners, but audits and recounts use the paper ballots that arrive in time. The enunciated motivation for the protocol is to allow (electronic) votes from overseas military voters to be included in preliminary results before a (paper) ballot is received from the voter. MERGE contains interesting ideas that are not inherently unsound; but to make the system trustworthy--to apply the MERGE protocol--would require major changes to the laws, practices, and technical and logistical abilities of U.S. election jurisdictions. The gap between theory and practice is large and unbridgeable for the foreseeable future. Promoters of this research project at DARPA, the agency that sponsored the research, should acknowledge that MERGE is internet voting (election results rely on votes transmitted over the internet except in the event of a full hand count) and refrain from claiming that it could be a component of trustworthy elections without sweeping changes to election law and election administration throughout the U.S.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11796v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew W. Appel, Philip B. Stark</dc:creator>
    </item>
    <item>
      <title>Confronting Project Conflicts into Success: a Complex Systems Design Approach to Resolving Stalemates</title>
      <link>https://arxiv.org/abs/2409.10549</link>
      <description>arXiv:2409.10549v2 Announce Type: replace 
Abstract: In today's complex projects development, stakeholders are often involved too late. There is also in many cases a one-sided technical focus that only focuses on the system's behaviour and does not integrate the individual stakeholder preferences. This locks stakeholders into a 'technical' conflict instead of being able to emerge from it 'socially'. Moreover, stakeholders are often involved a-posteriori in a multi-faceted development process which is untransparent, leading to stalemates or even artefacts that nobody ever wants. There is thus a need for a purely associative and a-priori design-supported approach that integrates both system's reality and stakeholder's interests within a joint agreement and technical framework. The state-of-the-art Preferendus, the computer-aided design engine embedded within the proven Open Design Systems (Odesys) methodology, is a neutral tool in confronting complexity into success. The Preferendus is deployed to co-creatively generate a best-fit-for-common-purpose solution for a number of wind farm related degrees of freedom, project constraints and given a number of stakeholder objective functions. Since, the Preferendus design potential for a stalemate depends strongly on stakeholder interest, importance and trust, in this paper an structured stakeholder judgement approach is introduced to transparently arrive at individual stakeholder weights using a choice-based conjoint analysis (CBCA) method. This method also allows for obtaining an initial estimate for the individual stakeholder preference functions. By modelling disputable exogenous factors as endogenous design parameters, it is also shown for which factors the stalemate problem is indeed both technically and socially (un)solvable, while interests and reality are conjoined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10549v2</guid>
      <category>cs.CY</category>
      <category>math.OC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>L. G. Teuber, A. R. M. Wolfert</dc:creator>
    </item>
    <item>
      <title>Code Interviews: Design and Evaluation of a More Authentic Assessment for Introductory Programming Assignments</title>
      <link>https://arxiv.org/abs/2410.01010</link>
      <description>arXiv:2410.01010v2 Announce Type: replace 
Abstract: Generative artificial intelligence poses new challenges around assessment, increasingly driving introductory programming educators to employ invigilated exams. But exams do not afford more authentic programming experiences that involve planning, implementing, and debugging programs with computer interaction. In this experience report, we describe code interviews: a more authentic assessment method for take-home programming assignments. Through action research, we experimented with varying the number and type of questions as well as whether interviews were conducted individually or with groups of students. To scale the program, we converted most of our weekly teaching assistant (TA) sections to conduct code interviews on 5 major weekly take-home programming assignments. By triangulating data from 5 sources, we identified 4 themes. Code interviews (1) pushed students to discuss their work, motivating more nuanced but sometimes repetitive insights; (2) enabled peer learning, reducing stress in some ways but increasing stress in other ways; (3) scaled with TA-led sections, replacing familiar practice with an unfamiliar assessment; (4) focused on student contributions, limiting opportunities for TAs to give guidance and feedback. We conclude by discussing the different decisions about the design of code interviews with implications for student experience, academic integrity, and teaching workload.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01010v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suhas Kannam, Yuri Yang, Aarya Dharm, Kevin Lin</dc:creator>
    </item>
    <item>
      <title>The why, what, and how of AI-based coding in scientific research</title>
      <link>https://arxiv.org/abs/2410.02156</link>
      <description>arXiv:2410.02156v2 Announce Type: replace 
Abstract: Computer programming (coding) is indispensable for researchers across disciplines, yet it remains challenging to learn and time-consuming to carry out. Generative AI, particularly large language models (LLMs), has the potential to transform coding into intuitive conversations, but best practices and effective workflows are only emerging. We dissect AI-based coding through three key lenses: the nature and role of LLMs in coding (why), six types of coding assistance they provide (what), and a five-step workflow in action with practical implementation strategies (how). Additionally, we address the limitations and future outlook of AI in coding. By offering actionable insights, this framework helps to guide researchers in effectively leveraging AI to enhance coding practices and education, accelerating scientific progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02156v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.PL</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tonghe Zhuang, Zhicheng Lin</dc:creator>
    </item>
    <item>
      <title>How Unique is Whose Web Browser? The role of demographics in browser fingerprinting among US users</title>
      <link>https://arxiv.org/abs/2410.06954</link>
      <description>arXiv:2410.06954v3 Announce Type: replace 
Abstract: Browser fingerprinting can be used to identify and track users across the Web, even without cookies, by collecting attributes from users' devices to create unique "fingerprints". This technique and resulting privacy risks have been studied for over a decade. Yet further research is limited because prior studies used data not publicly available. Additionally, data in prior studies lacked user demographics. Here we provide a first-of-its-kind dataset to enable further research. It includes browser attributes with users' demographics and survey responses, collected with informed consent from 8,400 US study participants. We use this dataset to demonstrate how fingerprinting risks differ across demographic groups. For example, we find lower income users are more at risk, and find that as users' age increases, they are both more likely to be concerned about fingerprinting and at real risk of fingerprinting. Furthermore, we demonstrate an overlooked risk: user demographics, such as gender, age, income level and race, can be inferred from browser attributes commonly used for fingerprinting, and we identify which browser attributes most contribute to this risk. Our data collection process also conducted an experiment to study what impacts users' likelihood to share browser data for open research, in order to inform future data collection efforts, with responses from 12,461 total participants. Female participants were significantly less likely to share their browser data, as were participants who were shown the browser data we asked to collect. Overall, we show the important role of user demographics in the ongoing work that intends to assess fingerprinting risks and improve user privacy, with findings to inform future privacy enhancing browser developments. The dataset and data collection tool we provide can be used to further study research questions not addressed in this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06954v3</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.56553/popets-2025-0038</arxiv:DOI>
      <dc:creator>Alex Berke, Enrico Bacis, Badih Ghazi, Pritish Kamath, Ravi Kumar, Robin Lassonde, Pasin Manurangsi, Umar Syed</dc:creator>
    </item>
    <item>
      <title>Automating Autograding: Large Language Models as Test Suite Generators for Introductory Programming</title>
      <link>https://arxiv.org/abs/2411.09261</link>
      <description>arXiv:2411.09261v2 Announce Type: replace 
Abstract: Automatically graded programming assignments provide instant feedback to students and significantly reduce manual grading time for instructors. However, creating comprehensive suites of test cases for programming problems within automatic graders can be time-consuming and complex. The effort needed to define test suites may deter some instructors from creating additional problems or lead to inadequate test coverage, potentially resulting in misleading feedback on student solutions. Such limitations may reduce student access to the well-documented benefits of timely feedback when learning programming.
  In this work, we evaluate the effectiveness of using Large Language Models (LLMs), as part of a larger workflow, to automatically generate test suites for CS1-level programming problems. Each problem's statement and reference solution are provided to GPT-4 to produce a test suite that can be used by an autograder. We evaluate our proposed approach using a sample of 26 problems, and more than 25,000 attempted solutions to those problems, submitted by students in an introductory programming course. We compare the performance of the LLM-generated test suites against the instructor-created test suites for each problem. Our findings reveal that LLM-generated test suites can correctly identify most valid solutions, and for most problems are at least as comprehensive as the instructor test suites. Additionally, the LLM-generated test suites exposed ambiguities in some problem statements, underscoring their potential to improve both autograding and instructional design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09261v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Umar Alkafaween, Ibrahim Albluwi, Paul Denny</dc:creator>
    </item>
    <item>
      <title>Virtual Reality in Teacher Education: Insights from Pre-Service Teachers in Resource-limited Regions</title>
      <link>https://arxiv.org/abs/2411.10225</link>
      <description>arXiv:2411.10225v2 Announce Type: replace 
Abstract: This study explores the perceptions, challenges, and opportunities associated with using Virtual Reality (VR) as a tool in teacher education among pre-service teachers in a resource-limited setting. Utilizing a qualitative case study design, the study draws on the experiences and reflections of 36 Ghanaian pre-service teachers who engaged with VR in a facilitated lesson for the first time. Findings reveal that initial exposure to VR generated a positive perception, with participants highlighting VR's potential as an engaging and interactive tool that can support experiential learning. Notably, many participants saw the VR-facilitated lesson as a promising alternative to synchronous online learning, particularly for its ability to simulate in-person presentations. They believe VR's immersive capabilities could enhance both teacher preparation and learner engagement in ways that traditional teaching often does not, especially noting that VR has the potential of addressing expensive educational field trips. Despite these promising perceptions, participants identified key challenges, including limited infrastructure, unreliable internet connectivity, and insufficient access to VR equipment as perceived challenges that might hinder the integration of VR in a resource-limited region like Ghana. These findings offer significant implications for educational policymakers and institutions aiming to leverage VR to enhance teacher training and professional development in similar contexts to consider addressing the perceived challenges for successful VR integration in education. We recommend further empirical research be conducted involving pre-service teachers use of VR in their classrooms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10225v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Nyaaba, Bismark Nyaaba Akanzire, Macharious Nabang</dc:creator>
    </item>
    <item>
      <title>A Fair Loss Function for Network Pruning</title>
      <link>https://arxiv.org/abs/2211.10285</link>
      <description>arXiv:2211.10285v2 Announce Type: replace-cross 
Abstract: Model pruning can enable the deployment of neural networks in environments with resource constraints. While pruning may have a small effect on the overall performance of the model, it can exacerbate existing biases into the model such that subsets of samples see significantly degraded performance. In this paper, we introduce the performance weighted loss function, a simple modified cross-entropy loss function that can be used to limit the introduction of biases during pruning. Experiments using the CelebA, Fitzpatrick17k and CIFAR-10 datasets demonstrate that the proposed method is a simple and effective tool that can enable existing pruning methods to be used in fairness sensitive contexts. Code used to produce all experiments contained in this paper can be found at https://github.com/robbiemeyer/pw_loss_pruning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.10285v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robbie Meyer, Alexander Wong</dc:creator>
    </item>
    <item>
      <title>Towards Explainable Evaluation Metrics for Machine Translation</title>
      <link>https://arxiv.org/abs/2306.13041</link>
      <description>arXiv:2306.13041v2 Announce Type: replace-cross 
Abstract: Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics for machine translation (for example, COMET or BERTScore) are based on black-box large language models. They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are more transparent. To foster more widespread acceptance of novel high-quality metrics, explainability thus becomes crucial. In this concept paper, we identify key properties as well as key goals of explainable machine translation metrics and provide a comprehensive synthesis of recent techniques, relating them to our established goals and properties. In this context, we also discuss the latest state-of-the-art approaches to explainable metrics based on generative models such as ChatGPT and GPT4. Finally, we contribute a vision of next-generation approaches, including natural language explanations. We hope that our work can help catalyze and guide future research on explainable evaluation metrics and, mediately, also contribute to better and more transparent machine translation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13041v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research (JMLR), 2024, vol. 25, no. 75, p. 1-49</arxiv:journal_reference>
      <dc:creator>Christoph Leiter, Piyawat Lertvittayakumjorn, Marina Fomicheva, Wei Zhao, Yang Gao, Steffen Eger</dc:creator>
    </item>
    <item>
      <title>Privacy and Copyright Protection in Generative AI: A Lifecycle Perspective</title>
      <link>https://arxiv.org/abs/2311.18252</link>
      <description>arXiv:2311.18252v3 Announce Type: replace-cross 
Abstract: The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18252v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3644815.3644952</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering-Software Engineering for AI. 2024. pp.92-97</arxiv:journal_reference>
      <dc:creator>Dawen Zhang, Boming Xia, Yue Liu, Xiwei Xu, Thong Hoang, Zhenchang Xing, Mark Staples, Qinghua Lu, Liming Zhu</dc:creator>
    </item>
    <item>
      <title>Enhancing Cross-Modal Contextual Congruence for Crowdfunding Success using Knowledge-infused Learning</title>
      <link>https://arxiv.org/abs/2402.03607</link>
      <description>arXiv:2402.03607v2 Announce Type: replace-cross 
Abstract: The digital landscape continually evolves with multimodality, enriching the online experience for users. Creators and marketers aim to weave subtle contextual cues from various modalities into congruent content to engage users with a harmonious message. This interplay of multimodal cues is often a crucial factor in attracting users' attention. However, this richness of multimodality presents a challenge to computational modeling, as the semantic contextual cues spanning across modalities need to be unified to capture the true holistic meaning of the multimodal content. This contextual meaning is critical in attracting user engagement as it conveys the intended message of the brand or the organization. In this work, we incorporate external commonsense knowledge from knowledge graphs to enhance the representation of multimodal data using compact Visual Language Models (VLMs) and predict the success of multi-modal crowdfunding campaigns. Our results show that external knowledge commonsense bridges the semantic gap between text and image modalities, and the enhanced knowledge-infused representations improve the predictive performance of models for campaign success upon the baselines without knowledge. Our findings highlight the significance of contextual congruence in online multimodal content for engaging and successful crowdfunding campaigns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03607v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE International Conference on Big Data 2024 (IEEE BigData 2024)</arxiv:journal_reference>
      <dc:creator>Trilok Padhi, Ugur Kursuncu, Yaman Kumar, Valerie L. Shalin, Lane Peterson Fronczek</dc:creator>
    </item>
    <item>
      <title>Taming the Long Tail in Human Mobility Prediction</title>
      <link>https://arxiv.org/abs/2410.14970</link>
      <description>arXiv:2410.14970v3 Announce Type: replace-cross 
Abstract: With the popularity of location-based services, human mobility prediction plays a key role in enhancing personalized navigation, optimizing recommendation systems, and facilitating urban mobility and planning. This involves predicting a user's next POI (point-of-interest) visit using their past visit history. However, the uneven distribution of visitations over time and space, namely the long-tail problem in spatial distribution, makes it difficult for AI models to predict those POIs that are less visited by humans. In light of this issue, we propose the Long-Tail Adjusted Next POI Prediction (LoTNext) framework for mobility prediction, combining a Long-Tailed Graph Adjustment module to reduce the impact of the long-tailed nodes in the user-POI interaction graph and a novel Long-Tailed Loss Adjustment module to adjust loss by logit score and sample weight adjustment strategy. Also, we employ the auxiliary prediction task to enhance generalization and accuracy. Our experiments with two real-world trajectory datasets demonstrate that LoTNext significantly surpasses existing state-of-the-art works. Our code is available at https://github.com/Yukayo/LoTNext.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14970v3</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaohang Xu, Renhe Jiang, Chuang Yang, Zipei Fan, Kaoru Sezaki</dc:creator>
    </item>
    <item>
      <title>Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina</title>
      <link>https://arxiv.org/abs/2410.19599</link>
      <description>arXiv:2410.19599v2 Announce Type: replace-cross 
Abstract: Recent studies suggest large language models (LLMs) can exhibit human-like reasoning, aligning with human behavior in economic experiments, surveys, and political discourse. This has led many to propose that LLMs can be used as surrogates or simulations for humans in social science research. However, LLMs differ fundamentally from humans, relying on probabilistic patterns, absent the embodied experiences or survival objectives that shape human cognition. We assess the reasoning depth of LLMs using the 11-20 money request game. Nearly all advanced approaches fail to replicate human behavior distributions across many models. Causes of failure are diverse and unpredictable, relating to input language, roles, and safeguarding. These results advise caution when using LLMs to study human behavior or as surrogates or simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19599v2</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Gao, Dokyun Lee, Gordon Burtch, Sina Fazelpour</dc:creator>
    </item>
    <item>
      <title>Introduction to AI Safety, Ethics, and Society</title>
      <link>https://arxiv.org/abs/2411.01042</link>
      <description>arXiv:2411.01042v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence is rapidly embedding itself within militaries, economies, and societies, reshaping their very foundations. Given the depth and breadth of its consequences, it has never been more pressing to understand how to ensure that AI systems are safe, ethical, and have a positive societal impact. This book aims to provide a comprehensive approach to understanding AI risk. Our primary goals include consolidating fragmented knowledge on AI risk, increasing the precision of core ideas, and reducing barriers to entry by making content simpler and more comprehensible. The book has been designed to be accessible to readers from diverse backgrounds. You do not need to have studied AI, philosophy, or other such topics. The content is skimmable and somewhat modular, so that you can choose which chapters to read. We introduce mathematical formulas in a few places to specify claims more precisely, but readers should be able to understand the main points without these.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01042v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dan Hendrycks</dc:creator>
    </item>
    <item>
      <title>The Intersectionality Problem for Algorithmic Fairness</title>
      <link>https://arxiv.org/abs/2411.02569</link>
      <description>arXiv:2411.02569v2 Announce Type: replace-cross 
Abstract: A yet unmet challenge in algorithmic fairness is the problem of intersectionality, that is, achieving fairness across the intersection of multiple groups -- and verifying that such fairness has been attained. Because intersectional groups tend to be small, verifying whether a model is fair raises statistical as well as moral-methodological challenges. This paper (1) elucidates the problem of intersectionality in algorithmic fairness, (2) develops desiderata to clarify the challenges underlying the problem and guide the search for potential solutions, (3) illustrates the desiderata and potential solutions by sketching a proposal using simple hypothesis testing, and (4) evaluates, partly empirically, this proposal against the proposed desiderata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02569v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Himmelreich, Arbie Hsu, Kristian Lum, Ellen Veomett</dc:creator>
    </item>
  </channel>
</rss>
