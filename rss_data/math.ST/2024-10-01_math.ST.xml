<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Oct 2024 20:49:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Consistency of Graphical Model-based Clustering: Robust Clustering using Bayesian Spanning Forest</title>
      <link>https://arxiv.org/abs/2409.19129</link>
      <description>arXiv:2409.19129v1 Announce Type: new 
Abstract: For statistical inference on clustering, the mixture model-based framework is very popular. On the one hand, the model-based framework is convenient for producing probabilistic estimates of cluster assignments and uncertainty. On the other hand, the specification of a mixture model is fraught with the danger of misspecification that could lead to inconsistent clustering estimates. Graphical model-based clustering takes a different model specification strategy, in which the likelihood treats the data as arising dependently from a disjoint union of component graphs. To counter the large uncertainty of the graph, recent work on Bayesian spanning forest proposes using the integrated posterior of the node partition (marginalized over the latent edge distribution) to produce probabilistic estimates for clustering. Despite the strong empirical performance, it is not yet known whether the clustering estimator is consistent, especially when the data-generating mechanism is different from the specified graphical model. This article gives a positive answer in the asymptotic regime: when the data arise from an unknown mixture distribution, under mild conditions, the posterior concentrates on the ground-truth partition, producing correct clustering estimates including the number of clusters. This theoretical result is an encouraging development for the robust clustering literature, demonstrating the use of graphical models as a robust alternative to mixture models in model-based clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19129v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yu Zheng, Leo L. Duan, Arkaprava Roy</dc:creator>
    </item>
    <item>
      <title>Detecting Change-points in Mean of Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2409.19312</link>
      <description>arXiv:2409.19312v1 Announce Type: new 
Abstract: This work delves into presenting a probabilistic method for analyzing linear process data with weakly dependent innovations, focusing on detecting change-points in the mean and estimating its spectral density. We develop a test for identifying change-points in the mean of data coming from such a model, aiming to detect shifts in the underlying distribution. Additionally, we propose a consistent estimator for the spectral density of the data, contingent upon fundamental assumptions, notably the long-run variance. By leveraging probabilistic techniques, our approach provides reliable tools for understanding temporal changes in linear process data. Through theoretical analysis and empirical evaluation, we demonstrate the efficacy and consistency of our proposed methods, offering valuable insights for practitioners in various fields dealing with time series data analysis. Finally, we implemented our method on bitcoin data for identifying the time points of significant changes in its stock price.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19312v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ramkrishna Jyoti Samanta</dc:creator>
    </item>
    <item>
      <title>The only admissible way of merging e-values</title>
      <link>https://arxiv.org/abs/2409.19888</link>
      <description>arXiv:2409.19888v1 Announce Type: new 
Abstract: We prove that the only admissible way of merging e-values is to use a weighted arithmetic average. This result completes the picture of merging methods for e-values, and generalizes the result of Vovk and Wang (2021, Annals of Statistics, 49(3), 1736--1754) that the only admissible way of symmetrically merging e-values is to use the arithmetic average combined with a constant. Although the proved statement is naturally anticipated, its proof relies on a sophisticated application of optimal transport duality and a minimax theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19888v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Diagnostic checking of periodic vector autoregressive time series models with dependent errors</title>
      <link>https://arxiv.org/abs/2409.20001</link>
      <description>arXiv:2409.20001v1 Announce Type: new 
Abstract: In this article, we study the asymptotic behaviour of the residual autocorrelations for periodic vector autoregressive time series models (PVAR henceforth) with uncorrelated but dependent innovations (i.e., weak PVAR). We then deduce the asymptotic distribution of the Ljung-Box-McLeod modified Portmanteau statistics for weak PVAR models. In Monte Carlo experiments, we illustrate that the proposed test statistics have reasonable finite sample performance. When the innovations exhibit conditional heteroscedasticity or other forms of dependence, it appears that the standard test statistics (under independent and identically distributed innovations) are generally nonreliable, overrejecting, or underrejecting severely, while the proposed test statistics offer satisfactory levels. An illustrative application on real data is also proposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20001v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yacouba Boubacar Mainassara (LMB, UFC, UPHF, INSA Hauts-De-France, CERAMATHS), Eugen Ursu (UB)</dc:creator>
    </item>
    <item>
      <title>Conditional Diffusion Models are Minimax-Optimal and Manifold-Adaptive for Conditional Distribution Estimation</title>
      <link>https://arxiv.org/abs/2409.20124</link>
      <description>arXiv:2409.20124v1 Announce Type: new 
Abstract: We consider a class of conditional forward-backward diffusion models for conditional generative modeling, that is, generating new data given a covariate (or control variable). To formally study the theoretical properties of these conditional generative models, we adopt a statistical framework of distribution regression to characterize the large sample properties of the conditional distribution estimators induced by these conditional forward-backward diffusion models. Here, the conditional distribution of data is assumed to smoothly change over the covariate. In particular, our derived convergence rate is minimax-optimal under the total variation metric within the regimes covered by the existing literature. Additionally, we extend our theory by allowing both the data and the covariate variable to potentially admit a low-dimensional manifold structure. In this scenario, we demonstrate that the conditional forward-backward diffusion model can adapt to both manifold structures, meaning that the derived estimation error bound (under the Wasserstein metric) depends only on the intrinsic dimensionalities of the data and the covariate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20124v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rong Tang, Lizhen Lin, Yun Yang</dc:creator>
    </item>
    <item>
      <title>Repetition effects in a Sequential Monte Carlo sampler</title>
      <link>https://arxiv.org/abs/2409.19017</link>
      <description>arXiv:2409.19017v1 Announce Type: cross 
Abstract: We investigate the prevalence of sample repetition in a Sequential Monte Carlo (SMC) method recently introduced for political redistricting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19017v1</guid>
      <category>math.PR</category>
      <category>cs.CY</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah Cannon, Daryl DeFord, Moon Duchin</dc:creator>
    </item>
    <item>
      <title>Propensity Score Augmentation in Matching-based Estimation of Causal Effects</title>
      <link>https://arxiv.org/abs/2409.19230</link>
      <description>arXiv:2409.19230v1 Announce Type: cross 
Abstract: When assessing the causal effect of a binary exposure using observational data, confounder imbalance across exposure arms must be addressed. Matching methods, including propensity score-based matching, can be used to deconfound the causal relationship of interest. They have been particularly popular in practice, at least in part due to their simplicity and interpretability. However, these methods can suffer from low statistical efficiency compared to many competing methods. In this work, we propose a novel matching-based estimator of the average treatment effect based on a suitably-augmented propensity score model. Our procedure can be shown to have greater statistical efficiency than traditional matching estimators whenever prognostic variables are available, and in some cases, can nearly reach the nonparametric efficiency bound. In addition to a theoretical study, we provide numerical results to illustrate our findings. Finally, we use our novel procedure to estimate the effect of circumcision on risk of HIV-1 infection using vaccine efficacy trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19230v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ernesto Ulloa-P\'erez, Marco Carone, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Group lasso based selection for high-dimensional mediation analysis</title>
      <link>https://arxiv.org/abs/2409.20036</link>
      <description>arXiv:2409.20036v1 Announce Type: cross 
Abstract: Mediation analysis aims to identify and estimate the effect of an exposure on an outcome that is mediated through one or more intermediate variables. In the presence of multiple intermediate variables, two pertinent methodological questions arise: estimating mediated effects when mediators are correlated, and performing high-dimensional mediation analysis when the number of mediators exceeds the sample size. This paper presents a two-step procedure for high-dimensional mediation analysis. The first step selects a reduced number of candidate mediators using an ad-hoc lasso penalty. The second step applies a procedure we previously developed to estimate the mediated and direct effects, accounting for the correlation structure among the retained candidate mediators. We compare the performance of the proposed two-step procedure with state-of-the-art methods using simulated data. Additionally, we demonstrate its practical application by estimating the causal role of DNA methylation in the pathway between smoking and rheumatoid arthritis using real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20036v1</guid>
      <category>q-bio.QM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allan J\'erolon (MAP5 - UMR 8145), Flora Alarcon (MAP5 - UMR 8145), Florence Pittion (TIMC), Magali Richard (TIMC), Olivier Fran\c{c}ois (TIMC), Etienne E. Birmel\'e (IRMA), Vittorio Perduca (MAP5 - UMR 8145)</dc:creator>
    </item>
    <item>
      <title>New matrix perturbation bounds via contour expansion</title>
      <link>https://arxiv.org/abs/2409.20207</link>
      <description>arXiv:2409.20207v1 Announce Type: cross 
Abstract: Matrix perturbation bounds (such as Weyl and Davis-Kahan) are frequently used in many branches of mathematics. Most of the classical results in this area are optimal, in the worst-case analysis. However, in modern applications, both the ground and the nose matrices frequently have extra structural properties. For instance, it is often assumed that the ground matrix is essentially low rank, and the nose matrix is random or pseudo-random. We aim to rebuild a part of perturbation theory, adapting to these modern assumptions. We will do this using a contour expansion argument, which enables us to exploit the skewness among the leading eigenvectors of the ground and the noise matrix (which is significant when the two are uncorrelated) to our advantage. In the current paper, we focus on the perturbation of eigenspaces. This helps us to introduce the arguments in the cleanest way, avoiding the more technical consideration of the general case. In applications, this case is also one of the most useful. More general results appear in a subsequent paper. Our method has led to several improvements, which have direct applications in central problems. Among others, we derive a sharp result for the perturbation of a low rank matrix with random perturbation, answering an open question in this area. Next, we derive new results concerning the spike model, an important model in statistics, bridging two different directions of current research. Finally, we use our results on the perturbation of eigenspaces to derive new results concerning eigenvalues of deterministic and random matrices. In particular, we obtain new results concerning the outliers in the deformed Wigner model and the least singular value of random matrices with non-zero mean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20207v1</guid>
      <category>math.SP</category>
      <category>math.CO</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Phuc Tran, Van Vu</dc:creator>
    </item>
    <item>
      <title>Alternative representation of the large deviation rate function and hyperparameter tuning schemes for Metropolis-Hastings Markov Chains</title>
      <link>https://arxiv.org/abs/2409.20337</link>
      <description>arXiv:2409.20337v1 Announce Type: cross 
Abstract: Markov chain Monte Carlo (MCMC) methods are one of the most common classes of algorithms to sample from a target probability distribution $\pi$. A rising trend in recent years consists in analyzing the convergence of MCMC algorithms using tools from the theory of large deviations. One such result is a large deviation principle for algorithms of Metropolis-Hastings (MH) type (Milinanni &amp; Nyquist, 2024), which are a broad and popular sub-class of MCMC methods.
  A central object in large deviation theory is the rate function, through which we can characterize the speed of convergence of MCMC algorithms. In this paper we consider the large deviation rate function from (Milinanni &amp; Nyquist, 2024), of which we prove an alternative representation. We also determine upper and lower bounds for the rate function, based on which we design schemes to tune algorithms of MH type.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20337v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federica Milinanni</dc:creator>
    </item>
    <item>
      <title>A Sparse Beta Regression Model for Network Analysis</title>
      <link>https://arxiv.org/abs/2010.13604</link>
      <description>arXiv:2010.13604v3 Announce Type: replace 
Abstract: For statistical analysis of network data, the $\beta$-model has emerged as a useful tool, thanks to its flexibility in incorporating nodewise heterogeneity and theoretical tractability. To generalize the $\beta$-model, this paper proposes the Sparse $\beta$-Regression Model (S$\beta$RM) that unites two research themes developed recently in modelling homophily and sparsity. In particular, we employ differential heterogeneity that assigns weights only to important nodes and propose penalized likelihood with an $\ell_1$ penalty for parameter estimation. While our estimation method is closely related to the LASSO method for logistic regression, we develop new theory emphasizing the use of our model for dealing with a parameter regime that can handle sparse networks usually seen in practice. More interestingly, the resulting inference on the homophily parameter demands no debiasing normally employed in LASSO type estimation. We provide extensive simulation and data analysis to illustrate the use of the model. As a special case of our model, we extend the Erd\H{o}s-R\'{e}nyi model by including covariates and develop the associated statistical inference for sparse networks, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.13604v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Stein, Rui Feng, Chenlei Leng</dc:creator>
    </item>
    <item>
      <title>Empirical partially Bayes multiple testing and compound $\chi^2$ decisions</title>
      <link>https://arxiv.org/abs/2303.02887</link>
      <description>arXiv:2303.02887v2 Announce Type: replace 
Abstract: A common task in high-throughput biology is to screen for associations across thousands of units of interest, e.g., genes or proteins. Often, the data for each unit are modeled as Gaussian measurements with unknown mean and variance and are summarized as per-unit sample averages and sample variances. The downstream goal is multiple testing for the means. In this domain, it is routine to "moderate" (that is, to shrink) the sample variances through parametric empirical Bayes methods before computing p-values for the means. Such an approach is asymmetric in that a prior is posited and estimated for the nuisance parameters (variances) but not the primary parameters (means). Our work initiates the formal study of this paradigm, which we term "empirical partially Bayes multiple testing." In this framework, if the prior for the variances were known, one could proceed by computing p-values conditional on the sample variances -- a strategy called partially Bayes inference by Sir David Cox. We show that these conditional p-values satisfy an Eddington/Tweedie-type formula and are approximated at nearly-parametric rates when the prior is estimated by nonparametric maximum likelihood. The estimated p-values can be used with the Benjamini-Hochberg procedure to guarantee asymptotic control of the false discovery rate. Even in the compound setting, wherein the variances are fixed, the approach retains asymptotic type-I error guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02887v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolaos Ignatiadis, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Simulation-based, Finite-sample Inference for Privatized Data</title>
      <link>https://arxiv.org/abs/2303.05328</link>
      <description>arXiv:2303.05328v5 Announce Type: replace 
Abstract: Privacy protection methods, such as differentially private mechanisms, introduce noise into resulting statistics which often produces complex and intractable sampling distributions. In this paper, we propose a simulation-based "repro sample" approach to produce statistically valid confidence intervals and hypothesis tests, which builds on the work of Xie and Wang (2022). We show that this methodology is applicable to a wide variety of private inference problems, appropriately accounts for biases introduced by privacy mechanisms (such as by clamping), and improves over other state-of-the-art inference methods such as the parametric bootstrap in terms of the coverage and type I error of the private inference. We also develop significant improvements and extensions for the repro sample methodology for general models (not necessarily related to privacy), including 1) modifying the procedure to ensure guaranteed coverage and type I errors, even accounting for Monte Carlo error, and 2) proposing efficient numerical algorithms to implement the confidence intervals and $p$-values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05328v5</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Zhanyu Wang</dc:creator>
    </item>
    <item>
      <title>Root-n consistent semiparametric learning with high-dimensional nuisance functions under minimal sparsity</title>
      <link>https://arxiv.org/abs/2305.04174</link>
      <description>arXiv:2305.04174v4 Announce Type: replace 
Abstract: Treatment effect estimation under unconfoundedness is a fundamental task in causal inference. In response to the challenge of analyzing high-dimensional datasets collected in substantive fields such as epidemiology, genetics, economics, and social sciences, various methods for treatment effect estimation with high-dimensional nuisance parameters (the outcome regression and the propensity score) have been developed in recent years. However, it is still unclear what is the necessary and sufficient sparsity condition on the nuisance parameters such that we can estimate the treatment effect at $1 / \sqrt{n}$-rate. In this paper, we propose a new Double-Calibration strategy that corrects the estimation bias of the nuisance parameter estimates computed by regularized high-dimensional techniques and demonstrate that the corresponding Doubly-Calibrated estimator achieves $1 / \sqrt{n}$-rate as long as one of the nuisance parameters is sparse with sparsity below $\sqrt{n} / \log p$, where $p$ denotes the ambient dimension of the covariates, whereas the other nuisance parameter can be arbitrarily complex and completely misspecified. The Double-Calibration strategy can also be applied to settings other than treatment effect estimation, e.g. regression coefficient estimation in the presence of a diverging number of controls in a semiparametric partially linear model, and local average treatment effect estimation with instrumental variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04174v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Liu, Xinbo Wang, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>A New Perspective On Denoising Based On Optimal Transport</title>
      <link>https://arxiv.org/abs/2312.08135</link>
      <description>arXiv:2312.08135v2 Announce Type: replace 
Abstract: In the standard formulation of the denoising problem, one is given a probabilistic model relating a latent variable $\Theta \in \Omega \subset \mathbb{R}^m \; (m\ge 1)$ and an observation $Z \in \mathbb{R}^d$ according to: $Z \mid \Theta \sim p(\cdot\mid \Theta)$ and $\Theta \sim G^*$, and the goal is to construct a map to recover the latent variable from the observation. The posterior mean, a natural candidate for estimating $\Theta$ from $Z$, attains the minimum Bayes risk (under the squared error loss) but at the expense of over-shrinking the $Z$, and in general may fail to capture the geometric features of the prior distribution $G^*$ (e.g., low dimensionality, discreteness, sparsity, etc.). To rectify these drawbacks, we take a new perspective on this denoising problem that is inspired by optimal transport (OT) theory and use it to study a different, OT-based, denoiser at the population level setting. We rigorously prove that, under general assumptions on the model, this OT-based denoiser is mathematically well-defined and unique, and is closely connected to the solution to a Monge OT problem. We then prove that, under appropriate identifiability assumptions on the model, the OT-based denoiser can be recovered solely from information of the marginal distribution of $Z$ and the posterior mean of the model, after solving a linear relaxation problem over a suitable space of couplings that is reminiscent of standard multimarginal OT problems. In particular, thanks to Tweedie's formula, when the likelihood model $\{ p(\cdot \mid \theta) \}_{\theta \in \Omega}$ is an exponential family of distributions, the OT based-denoiser can be recovered solely from the marginal distribution of $Z$. In general, our family of OT-like relaxations is of interest in its own right and for the denoising problem suggests alternative numerical methods inspired by the rich literature on computational OT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08135v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Garcia Trillos, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Measuring Evidence with a Continuous Test</title>
      <link>https://arxiv.org/abs/2409.05654</link>
      <description>arXiv:2409.05654v3 Announce Type: replace 
Abstract: Testing has developed into the fundamental statistical framework for falsifying hypotheses. Unfortunately, tests are binary in nature: a test either rejects a hypothesis or not. Such binary decisions do not reflect the reality of many scientific studies, which often aim to present the evidence against a hypothesis and do not necessarily intend to establish a definitive conclusion. To solve this, we propose the continuous generalization of a test, which we use to measure the evidence against a hypothesis. Such a continuous test can be viewed as a continuous non-randomized interpretation of the classical 'randomized test'. This offers the benefits of a randomized test, without the downsides of external randomization. Another interpretation is as a literal measure, which measures the amount of binary tests that reject the hypothesis. Our work completes the bridge between classical tests and the recently proposed $e$-values: $e$-values bounded to $[0, 1/\alpha]$ are continuously interpreted size $\alpha$ randomized tests. Taking $\alpha$ to 0 yields the regular $e$-value: a 'level 0' continuous test. Moreover, we generalize the traditional notion of power by using generalized means. This produces a unified framework that contains both classical Neyman-Pearson optimal testing and log-optimal $e$-values, as well as a continuum of other options. The traditional $p$-value appears as the reciprocal of an $e$-value, that satisfies a weaker error bound. In an illustration in a Gaussian location model, we find that optimal continuous tests are of a beautifully simple form.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05654v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning</dc:creator>
    </item>
    <item>
      <title>Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent</title>
      <link>https://arxiv.org/abs/2409.08469</link>
      <description>arXiv:2409.08469v2 Announce Type: replace 
Abstract: We provide finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm in the Kernelized Stein Discrepancy ($\mathsf{KSD}$) and Wasserstein-2 metrics. Our key insight is that the time derivative of the relative entropy between the joint density of $N$ particle locations and the $N$-fold product target measure, starting from a regular initial distribution, splits into a dominant `negative part' proportional to $N$ times the expected $\mathsf{KSD}^2$ and a smaller `positive part'. This observation leads to $\mathsf{KSD}$ rates of order $1/\sqrt{N}$, in both continuous and discrete time, providing a near optimal (in the sense of matching the corresponding i.i.d. rates) double exponential improvement over the recent result by Shi and Mackey (2024). Under mild assumptions on the kernel and potential, these bounds also grow polynomially in the dimension $d$. By adding a bilinear component to the kernel, the above approach is used to further obtain Wasserstein-2 convergence in continuous time. For the case of `bilinear + Mat\'ern' kernels, we derive Wasserstein-2 rates that exhibit a curse-of-dimensionality similar to the i.i.d. setting. We also obtain marginal convergence and long-time propagation of chaos results for the time-averaged particle laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08469v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Krishnakumar Balasubramanian, Sayan Banerjee, Promit Ghosal</dc:creator>
    </item>
    <item>
      <title>L-2 Regularized maximum likelihood for $\beta$-model in large and sparse networks</title>
      <link>https://arxiv.org/abs/2110.11856</link>
      <description>arXiv:2110.11856v4 Announce Type: replace-cross 
Abstract: The $\beta$-model is a powerful tool for modeling large and sparse networks driven by degree heterogeneity, where many network models become infeasible due to computational challenge and network sparsity. However, existing estimation algorithms for $\beta$-model do not scale up. Also, theoretical understandings remain limited to dense networks. This paper brings several significant improvements over existing results to address the urgent needs of practice. We propose a new $\ell_2$-penalized MLE algorithm that can comfortably handle sparse networks of millions of nodes with much-improved memory parsimony. We establish the first rate-optimal error bounds and high-dimensional asymptotic normality results for $\beta$-models, under much weaker network sparsity assumptions than best existing results.
  Application of our method to large COVID-19 network data sets and discover meaningful results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.11856v4</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Meijia Shao, Yu Zhang, Qiuping Wang, Yuan Zhang, Jing Luo, Ting Yan</dc:creator>
    </item>
    <item>
      <title>Reinforcing RCTs with Multiple Priors while Learning about External Validity</title>
      <link>https://arxiv.org/abs/2112.09170</link>
      <description>arXiv:2112.09170v5 Announce Type: replace-cross 
Abstract: This paper introduces a framework for incorporating prior information into the design of sequential experiments. These sources may include past experiments, expert opinions, or the experimenter's intuition. We model the problem using a multi-prior Bayesian approach, mapping each source to a Bayesian model and aggregating them based on posterior probabilities. Policies are evaluated on three criteria: learning the parameters of payoff distributions, the probability of choosing the wrong treatment, and average rewards. Our framework demonstrates several desirable properties, including robustness to sources lacking external validity, while maintaining strong finite sample performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.09170v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Frederico Finan, Demian Pouzo</dc:creator>
    </item>
    <item>
      <title>On the number and size of Markov equivalence classes of random directed acyclic graphs</title>
      <link>https://arxiv.org/abs/2209.04395</link>
      <description>arXiv:2209.04395v2 Announce Type: replace-cross 
Abstract: In causal inference on directed acyclic graphs, the orientation of edges is in general only recovered up to Markov equivalence classes. We study Markov equivalence classes of uniformly random directed acyclic graphs. Using a tower decomposition, we show that the ratio between the number of Markov equivalence classes and directed acyclic graphs approaches a positive constant when the number of sites goes to infinity. For a typical directed acyclic graph, the expected number of elements in its Markov equivalence class remains bounded. More precisely, we prove that for a uniformly chosen directed acyclic graph, the size of its Markov equivalence class has super-polynomial tails.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.04395v2</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominik Schmid, Allan Sly</dc:creator>
    </item>
    <item>
      <title>Probability Metrics for Tropical Spaces of Different Dimensions</title>
      <link>https://arxiv.org/abs/2307.02846</link>
      <description>arXiv:2307.02846v2 Announce Type: replace-cross 
Abstract: The problem of comparing probability distributions is at the heart of many tasks in statistics and machine learning. Established comparison methods treat the standard setting that the distributions are supported in the same space. Recently, a new geometric solution has been proposed to address the more challenging problem of comparing measures in Euclidean spaces of differing dimensions. Here, we study the same problem of comparing probability distributions of different dimensions in the tropical setting, which is becoming increasingly relevant in applications involving complex data structures such as phylogenetic trees. Specifically, we construct a Wasserstein distance between measures on different tropical projective tori -- the focal metric spaces in both theory and applications of tropical geometry -- via tropical mappings between probability measures. We prove equivalence of the directionality of the maps, whether mapping from a low dimensional space to a high dimensional space or vice versa. As an important practical implication, our work provides a framework for comparing probability distributions on the spaces of phylogenetic trees with different leaf sets. We demonstrate the computational feasibility of our approach using existing optimisation techniques on both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02846v2</guid>
      <category>math.MG</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>q-bio.PE</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roan Talbut, Daniele Tramontano, Yueqi Cao, Mathias Drton, Anthea Monod</dc:creator>
    </item>
    <item>
      <title>A calculus for Markov chain Monte Carlo: studying approximations in algorithms</title>
      <link>https://arxiv.org/abs/2310.03853</link>
      <description>arXiv:2310.03853v2 Announce Type: replace-cross 
Abstract: Markov chain Monte Carlo (MCMC) algorithms are based on the construction of a Markov chain with transition probabilities leaving invariant a probability distribution of interest. In this work, we look at these transition probabilities as functions of their invariant distributions, and we develop a notion of derivative in the invariant distribution of a MCMC kernel. We build around this concept a set of tools that we refer to as Markov chain Monte Carlo Calculus. This allows us to compare Markov chains with different invariant distributions within a suitable class via what we refer to as mean value inequalities. We explain how MCMC Calculus provides a natural framework to study algorithms using an approximation of an invariant distribution, and we illustrate this by using the tools developed to prove convergence of interacting and sequential MCMC algorithms. Finally, we discuss how similar ideas can be used in other frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03853v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rocco Caprio, Adam M. Johansen</dc:creator>
    </item>
    <item>
      <title>Imprecise Markov Semigroups and their Ergodicity</title>
      <link>https://arxiv.org/abs/2405.00081</link>
      <description>arXiv:2405.00081v2 Announce Type: replace-cross 
Abstract: We introduce the concept of an imprecise Markov semigroup $\mathbf{Q}$. It is a tool that allows to represent ambiguity around both the initial and the transition probabilities of a Markov process via a compact collection of plausible Markov semigroups, each associated with a (different, plausible) Markov process. We use techniques from geometry, functional analysis, and (high dimensional) probability to study the ergodic behavior of $\mathbf{Q}$. We show that, if the initial distribution of the Markov processes associated with the elements of $\mathbf{Q}$ is known and invariant, under some conditions that also involve the geometry of the state space, eventually the ambiguity around their transition probability fades. We call this property ergodicity of the imprecise Markov semigroup, and we relate it to the classical notion of ergodicity. We prove ergodicity both when the state space is Euclidean or a Riemannian manifold, and when it is an arbitrary measurable space. The importance of our findings for the fields of machine learning and computer vision is also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00081v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Caprio</dc:creator>
    </item>
    <item>
      <title>Tropical Gradient Descent</title>
      <link>https://arxiv.org/abs/2405.19551</link>
      <description>arXiv:2405.19551v2 Announce Type: replace-cross 
Abstract: We propose a gradient descent method for solving optimisation problems arising in settings of tropical geometry - a variant of algebraic geometry that has become increasingly studied in applications such as computational biology, economics, and computer science. Our approach takes advantage of the polyhedral and combinatorial structures arising in tropical geometry to propose a versatile approach for approximating local minima in tropical statistical optimisation problems - a rapidly growing body of work in recent years. Theoretical results establish global solvability for 1-sample problems and a convergence rate of $O(1/\sqrt{k})$. Numerical experiments demonstrate the method's superior performance over classical descent for tropical optimisation problems which exhibit tropical convexity but not classical convexity. Notably, tropical descent seamlessly integrates into advanced optimisation methods, such as Adam, offering improved overall performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19551v2</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roan Talbut, Anthea Monod</dc:creator>
    </item>
    <item>
      <title>A Modified Satterthwaite (1941,1946) Effective Degrees of Freedom Approximation</title>
      <link>https://arxiv.org/abs/2409.14606</link>
      <description>arXiv:2409.14606v2 Announce Type: replace-cross 
Abstract: This study introduces a correction to the approximation of effective degrees of freedom as proposed by Satterthwaite (1941, 1946), specifically addressing scenarios where component degrees of freedom are small. The correction is grounded in analytical results concerning the moments of standard normal random variables. This modification is applicable to complex variance estimates that involve both small and large degrees of freedom, offering an enhanced approximation of the higher moments required by Satterthwaite's framework. Additionally, this correction extends and partially validates the empirically derived adjustment by Johnson &amp; Rust (1992), as it is based on theoretical foundations rather than simulations used to derive empirical transformation constants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14606v2</guid>
      <category>stat.OT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matthias von Davier</dc:creator>
    </item>
  </channel>
</rss>
