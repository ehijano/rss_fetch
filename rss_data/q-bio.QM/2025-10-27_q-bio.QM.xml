<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Oct 2025 02:58:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Ultra-wideband radar to measure in vivo musculoskeletal forces</title>
      <link>https://arxiv.org/abs/2510.20866</link>
      <description>arXiv:2510.20866v1 Announce Type: new 
Abstract: Accurate measures of musculoskeletal forces are critical for clinicians, biomechanists, and engineers, yet direct measurement is highly invasive and current estimation methods remain limited in accuracy. Here, we demonstrate the application of ultra-wideband radar to non-invasively estimate musculoskeletal forces by measuring changes in the electromagnetic properties of contracting muscles, in muscles with different structural properties, during various static and dynamic conditions, and in the presence of fatigue. First, we show that ultra-wideband radar scans of muscle can reliably track isometric force in a unipennate knee extensor (vastus lateralis) and a bipennate ankle dorsiflexor (tibialis anterior). Next, we integrate radar signals within machine-learning and linear models to estimate musculoskeletal forces during fatiguing isometric, and dynamic knee extension contractions, with exceptional accuracy (test R2&gt;0.984; errors&lt;3.3%). Finally, we identify frequency-dependent effects of musculoskeletal forces on ultra-wideband radar signals, that are independent of physiological and structural features known to influence muscle force. Together, these findings establish ultra-wideband radar as a powerful, non-invasive approach for quantifying in vivo musculoskeletal forces, with transformative potential for wearable assistive technologies, biomechanics, and rehabilitation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20866v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher S. Bird, Antonio P. L. Bo, Wei Lu, Taylor J. M. Dick</dc:creator>
    </item>
    <item>
      <title>Physics-Informed Deep Learning for Improved Input Function Estimation in Motion-Blurred Dynamic [${}^{18}$F]FDG PET Images</title>
      <link>https://arxiv.org/abs/2510.21281</link>
      <description>arXiv:2510.21281v1 Announce Type: new 
Abstract: Kinetic modeling enables \textit{in vivo} quantification of tracer uptake and glucose metabolism in [${}^{18}$F]Fluorodeoxyglucose ([${}^{18}$F]FDG) dynamic positron emission tomography (dPET) imaging of mice. However, kinetic modeling requires the accurate determination of the arterial input function (AIF) during imaging, which is time-consuming and invasive. Recent studies have shown the efficacy of using deep learning to directly predict the input function, surpassing established methods such as the image-derived input function (IDIF). In this work, we trained a physics-informed deep learning-based input function prediction model (PIDLIF) to estimate the AIF directly from the PET images, incorporating a kinetic modeling loss during training. The proposed method uses a two-tissue compartment model over two regions, the myocardium and brain of the mice, and is trained on a dataset of 70 [${}^{18}$F]FDG dPET images of mice accompanied by the measured AIF during imaging. The proposed method had comparable performance to the network without a physics-informed loss, and when sudden movement causing blurring in the images was simulated, the PIDLIF model maintained high performance in severe cases of image degradation. The proposed physics-informed method exhibits an improved robustness that is promoted by physically constraining the problem, enforcing consistency for out-of-distribution samples. In conclusion, the PIDLIF model offers insight into the effects of leveraging physiological distribution mechanics in mice to guide a deep learning-based AIF prediction network in images with severe degradation as a result of blurring due to movement during imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21281v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Salomonsen, Kristoffer K. Wickstr{\o}m, Samuel Kuttner, Elisabeth Wetzer</dc:creator>
    </item>
    <item>
      <title>eMZed 3: flexible and interactive development of scalable LC-MS/MS data analysis workflows in Python</title>
      <link>https://arxiv.org/abs/2510.21484</link>
      <description>arXiv:2510.21484v1 Announce Type: new 
Abstract: Liquid chromatography-mass spectrometry (LC-MS/MS) data analysis requires adaptable software solutions to meet diverse analytical needs. We present eMZed 3, a modern Python framework for flexible and interactive analysis of LC-MS/MS data. eMZed 3 enables users to develop scalable workflows tailored to their specific requirements while leveraging Python's extensive ecosystem of libraries. Building on its predecessor, eMZed 3 is now Python 3-based and includes substantial enhancements, including support for chromatogram-based LC-MS data, a new SQLite-based backend supporting optional out-of-memory processing, and rich interactive visualization tools. Compared to the previous version, eMZed 3 is now split into three packages: emzed (core functionalities), emzed-gui (interactive data visualization), and emzed-spyder (an integrated development environment). This modular architecture allows straightforward integration of the emzed core library into headless Python environments, including computational notebooks (such as Jupyter) or high-performance computing clusters. eMZed 3 incorporates well-established libraries such as OpenMS, and is highly suited for both targeted and untargeted metabolomics. Overall, eMZed 3 supports the efficient development of scalable and reproducible LC-MS data analysis and is accessible to both novice and advanced programmers.
  Availability and Implementation: eMZed 3 and its documentation are freely available at https://emzed.ethz.ch, the source code is hosted at https://gitlab.com/groups/emzed3. An online-executable example workflow is available on Binder at: https://mybinder.org/v2/gl/emzed3%2Femzed-example-workflow/HEAD?labpath=example.ipynb.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21484v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uwe Schmitt, Jethro L. Hemmann, Nicola Zamboni, Julia A. Vorholt, Patrick Kiefer</dc:creator>
    </item>
    <item>
      <title>WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary Proposal Networks and Post-processing Optimisation</title>
      <link>https://arxiv.org/abs/2510.21280</link>
      <description>arXiv:2510.21280v2 Announce Type: cross 
Abstract: While recent sound event detection (SED) systems can identify baleen whale calls in marine audio, challenges related to false positive and minority-class detection persist. We propose the boundary proposal network (BPN), which extends an existing lightweight SED system. The BPN is inspired by work in image object detection and aims to reduce the number of false positive detections. It achieves this by using intermediate latent representations computed within the backbone classification model to gate the final output. When added to an existing SED system, the BPN achieves a 16.8 % absolute increase in precision, as well as 21.3 % and 9.4 % improvements in the F1-score for minority-class d-calls and bp-calls, respectively. We further consider two approaches to the selection of post-processing hyperparameters: a forward-search and a backward-search. By separately optimising event-level and frame-level hyperparameters, these two approaches lead to considerable performance improvements over parameters selected using empirical methods. The complete WhaleVAD-BPN system achieves a cross-validated development F1-score of 0.475, which is a 9.8 % absolute improvement over the baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21280v2</guid>
      <category>eess.AS</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Christiaan M. Geldenhuys, G\"unther Tonitz, Thomas R. Niesler</dc:creator>
    </item>
    <item>
      <title>Foundation Models in Dermatopathology: Skin Tissue Classification</title>
      <link>https://arxiv.org/abs/2510.21664</link>
      <description>arXiv:2510.21664v1 Announce Type: cross 
Abstract: The rapid generation of whole-slide images (WSIs) in dermatopathology necessitates automated methods for efficient processing and accurate classification. This study evaluates the performance of two foundation models, UNI and Virchow2, as feature extractors for classifying WSIs into three diagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-level embeddings were aggregated into slide-level features using a mean-aggregation strategy and subsequently used to train multiple machine learning classifiers, including logistic regression, gradient-boosted trees, and random forest models. Performance was assessed using precision, recall, true positive rate, false positive rate, and the area under the receiver operating characteristic curve (AUROC) on the test set. Results demonstrate that patch-level features extracted using Virchow2 outperformed those extracted via UNI across most slide-level classifiers, with logistic regression achieving the highest accuracy (90%) for Virchow2, though the difference was not statistically significant. The study also explored data augmentation techniques and image normalization to enhance model robustness and generalizability. The mean-aggregation approach provided reliable slide-level feature representations. All experimental results and metrics were tracked and visualized using WandB.ai, facilitating reproducibility and interpretability. This research highlights the potential of foundation models for automated WSI classification, providing a scalable and effective approach for dermatopathological diagnosis while paving the way for future advancements in slide-level representation learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21664v1</guid>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riya Gupta, Yiwei Zong, Dennis H. Murphree</dc:creator>
    </item>
    <item>
      <title>BioCube: A Multimodal Dataset for Biodiversity Research</title>
      <link>https://arxiv.org/abs/2505.11568</link>
      <description>arXiv:2505.11568v3 Announce Type: replace 
Abstract: Biodiversity research requires complete and detailed information to study ecosystem dynamics at different scales. Employing data-driven methods like Machine Learning is getting traction in ecology and more specific biodiversity, offering alternative modelling pathways. For these methods to deliver accurate results there is the need for large, curated and multimodal datasets that offer granular spatial and temporal resolutions. In this work, we introduce BioCube, a multimodal, fine-grained global dataset for ecology and biodiversity research. BioCube incorporates species observations through images, audio recordings and descriptions, environmental DNA, vegetation indices, agricultural, forest, land indicators, and high-resolution climate variables. All observations are geospatially aligned under the WGS84 geodetic system, spanning from 2000 to 2020. The dataset is available at https://huggingface.co/datasets/ BioDT/BioCube, the acquisition and processing code base at https://github.com/BioDT/bfm-data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11568v3</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2760/2119408</arxiv:DOI>
      <dc:creator>Stylianos Stasinos, Martino Mensio, Elena Lazovik, Athanasios Trantas</dc:creator>
    </item>
    <item>
      <title>Domain Knowledge Infused Conditional Generative Models for Accelerating Drug Discovery</title>
      <link>https://arxiv.org/abs/2510.09837</link>
      <description>arXiv:2510.09837v2 Announce Type: replace 
Abstract: The role of Artificial Intelligence (AI) is growing in every stage of drug development. Nevertheless, a major challenge in drug discovery AI remains: Drug pharmacokinetic (PK) and Drug-Target Interaction (DTI) datasets collected in different studies often exhibit limited overlap, creating data overlap sparsity. Thus, data curation becomes difficult, negatively impacting downstream research investigations in high-throughput screening, polypharmacy, and drug combination. We propose xImagand-DKI, a novel SMILES/Protein-to-Pharmacokinetic/DTI (SP2PKDTI) diffusion model capable of generating an array of PK and DTI target properties conditioned on SMILES and protein inputs that exhibit data overlap sparsity. We infuse additional molecular and genomic domain knowledge from the Gene Ontology (GO) and molecular fingerprints to further improve our model performance. We show that xImagand-DKI-generated synthetic PK data closely resemble real data univariate and bivariate distributions, and can adequately fill in gaps among PK and DTI datasets. As such, xImagand-DKI is a promising solution for data overlap sparsity and may improve performance for downstream drug discovery research tasks. Code available at: https://github.com/GenerativeDrugDiscovery/xImagand-DKI</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09837v2</guid>
      <category>q-bio.QM</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bing Hu, Jong-Hoon Park, Helen Chen, Young-Rae Cho, Anita Layton</dc:creator>
    </item>
    <item>
      <title>BASIN: Bayesian mAtrix variate normal model with Spatial and sparsIty priors in Non-negative deconvolution</title>
      <link>https://arxiv.org/abs/2510.16130</link>
      <description>arXiv:2510.16130v2 Announce Type: replace 
Abstract: Spatial transcriptomics allows researchers to visualize and analyze gene expression within the precise location of tissues or cells. It provides spatially resolved gene expression data but often lacks cellular resolution, necessitating cell type deconvolution to infer cellular composition at each spatial location. In this paper we propose BASIN for cell type deconvolution, which models deconvolution as a nonnegative matrix factorization (NMF) problem incorporating graph Laplacian prior. Rather than find a deterministic optima like other recent methods, we propose a matrix variate Bayesian NMF method with nonnegativity and sparsity priors, in which the variables are maintained in their matrix form to derive a more efficient matrix normal posterior. BASIN employs a Gibbs sampler to approximate the posterior distribution of cell type proportions and other parameters, offering a distribution of possible solutions, enhancing robustness and providing inherent uncertainty quantification. The performance of BASIN is evaluated on different spatial transcriptomics datasets and outperforms other deconvolution methods in terms of accuracy and efficiency. The results also show the effect of the incorporated priors and reflect a truncated matrix normal distribution as we expect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16130v2</guid>
      <category>q-bio.QM</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiasen Zhang, Xi Qiao, Liangliang Zhang, Weihong Guo</dc:creator>
    </item>
    <item>
      <title>Variational autoencoders stabilise TCN performance when classifying weakly labelled bioacoustics data: an interdisciplinary approach</title>
      <link>https://arxiv.org/abs/2410.17006</link>
      <description>arXiv:2410.17006v2 Announce Type: replace-cross 
Abstract: Passive acoustic monitoring (PAM) data is often weakly labelled, audited at the scale of detection presence or absence on timescales of minutes to hours. Moreover, this data exhibits great variability from one deployment to the next, due to differences in ambient noise and the signals across sources and geographies. This study proposes a two-step solution to leverage weakly annotated data for training Deep Learning (DL) detection models. Our case study involves binary classification of the presence/absence of sperm whale (\textit{Physeter macrocephalus}) click trains in 4-minute-long recordings from a dataset comprising diverse sources and deployment conditions to maximise generalisability. We tested methods for extracting acoustic features from lengthy audio segments and integrated Temporal Convolutional Networks (TCNs) trained on the extracted features for sequence classification. For feature extraction, we introduced a new approach using Variational AutoEncoders (VAEs) to extract information from both waveforms and spectrograms, which eliminates the necessity for manual threshold setting or time-consuming strong labelling. For classification, TCNs were trained separately on sequences of either VAE embeddings or handpicked acoustic features extracted from the waveform and spectrogram representations using classical methods, to compare the efficacy of the two approaches. The TCN demonstrated robust classification capabilities on a validation set, achieving accuracies exceeding 85\% when applied to 4-minute acoustic recordings. Notably, TCNs trained on handpicked acoustic features exhibited greater variability in performance across recordings from diverse deployment conditions, whereas those trained on VAEs showed a more consistent performance, highlighting the robust transferability of VAEs for feature extraction across different deployment conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17006v2</guid>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laia Garrob\'e Fonollosa, Douglas Gillespie, Lina Stankovic, Vladimir Stankovic, Luke Rendell</dc:creator>
    </item>
    <item>
      <title>A robust and versatile deep learning model for prediction of the arterial input function in dynamic small animal $\left[^{18}\text{F}\right]$FDG PET imaging</title>
      <link>https://arxiv.org/abs/2507.02367</link>
      <description>arXiv:2507.02367v2 Announce Type: replace-cross 
Abstract: Dynamic positron emission tomography (PET) and kinetic modeling are pivotal in advancing tracer development research in small animal studies. Accurate kinetic modeling requires precise input function estimation, traditionally achieved via arterial blood sampling. However, arterial cannulation in small animals like mice, involves intricate, time-consuming, and terminal procedures, precluding longitudinal studies. This work proposes a non-invasive, fully convolutional deep learning-based approach (FC-DLIF) to predict input functions directly from PET imaging, potentially eliminating the need for blood sampling in dynamic small-animal PET. The proposed FC-DLIF model includes a spatial feature extractor acting on the volumetric time frames of the PET sequence, extracting spatial features. These are subsequently further processed in a temporal feature extractor that predicts the arterial input function. The proposed approach is trained and evaluated using images and arterial blood curves from [$^{18}$F]FDG data using cross validation. Further, the model applicability is evaluated on imaging data and arterial blood curves collected using two additional radiotracers ([$^{18}$F]FDOPA, and [$^{68}$Ga]PSMA). The model was further evaluated on data truncated and shifted in time, to simulate shorter, and shifted, PET scans. The proposed FC-DLIF model reliably predicts the arterial input function with respect to mean squared error and correlation. Furthermore, the FC-DLIF model is able to predict the arterial input function even from truncated and shifted samples. The model fails to predict the AIF from samples collected using different radiotracers, as these are not represented in the training data. Our deep learning-based input function offers a non-invasive and reliable alternative to arterial blood sampling, proving robust and flexible to temporal shifts and different scan durations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02367v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Salomonsen, Luigi T Luppino, Fredrik Aspheim, Kristoffer K. Wickstr{\o}m, Elisabeth Wetzer, Michael C. Kampffmeyer, Rodrigo Berzaghi, Rune Sundset, Robert Jenssen, Samuel Kuttner</dc:creator>
    </item>
  </channel>
</rss>
