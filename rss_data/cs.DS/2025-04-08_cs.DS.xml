<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Apr 2025 01:52:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Tight analysis of the primal-dual method for edge-covering pliable set families</title>
      <link>https://arxiv.org/abs/2504.03910</link>
      <description>arXiv:2504.03910v1 Announce Type: new 
Abstract: A classic result of Williamson, Goemans, Mihail, and Vazirani [STOC 1993: 708-717] states that the problem of covering an uncrossable set family by a min-cost edge set admits approximation ratio $2$, by a primal-dual algorithm with a reverse delete phase. Bansal, Cheriyan, Grout, and Ibrahimpur [ICALP 2023: 15:1-15:19] showed that this algorithm achieves approximation ratio $16$ for a larger class of so called $\gamma$-pliable set families, that have much weaker uncrossing properties. The approximation ratio $16$ was improved to $10$ by the author [WAOA 2025: 151-166]. Recently, Bansal [arXiv:2308.15714] stated approximation ratio $8$ for $\gamma$-pliable families and an improved approximation ratio $5$ for an important particular case of the family of cuts of size $&lt;k$ of a graph, but his proof has an error. We will improve the approximation ratio to $7$ for the former case and give a simple proof of approximation ratio $6$ for the latter case. Our analysis is supplemented by examples showing that these approximation ratios are tight for the primal-dual algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03910v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeev Nutov</dc:creator>
    </item>
    <item>
      <title>Operational research approaches and mathematical models for kidney exchange: A literature survey and empirical evaluation</title>
      <link>https://arxiv.org/abs/2504.04091</link>
      <description>arXiv:2504.04091v1 Announce Type: new 
Abstract: Kidney exchange is a transplant modality that has provided new opportunities for living kidney donation in many countries around the world since 1991. It has been extensively studied from an Operational Research (OR) perspective since 2004. This article provides a comprehensive literature survey on OR approaches to fundamental computational problems associated with kidney exchange over the last two decades. We also summarise the key integer linear programming (ILP) models for kidney exchange, showing how to model optimisation problems involving only cycles and chains separately. This allows new combined ILP models, not previously presented, to be obtained by amalgamating cycle and chain models. We present a comprehensive empirical evaluation involving all combined models from this paper in addition to bespoke software packages from the literature involving advanced techniques. This focuses primarily on computation times for 49 methods applied to 4,320 problem instances of varying sizes that reflect the characteristics of real kidney exchange datasets, corresponding to over 200,000 algorithm executions. We have made our implementations of all cycle and chain models described in this paper, together with all instances used for the experiments, and a web application to visualise our experimental results, publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04091v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathijs Barkel, Rachael Colley, Maxence Delorme, David Manlove, William Pettersson</dc:creator>
    </item>
    <item>
      <title>Optimal Smoothed Analysis of the Simplex Method</title>
      <link>https://arxiv.org/abs/2504.04197</link>
      <description>arXiv:2504.04197v1 Announce Type: new 
Abstract: Smoothed analysis is a method for analyzing the performance of algorithms, used especially for those algorithms whose running time in practice is significantly better than what can be proven through worst-case analysis. Spielman and Teng (STOC '01) introduced the smoothed analysis framework of algorithm analysis and applied it to the simplex method. Given an arbitrary linear program with $d$ variables and $n$ inequality constraints, Spielman and Teng proved that the simplex method runs in time $O(\sigma^{-30} d^{55} n^{86})$, where $\sigma &gt; 0$ is the standard deviation of Gaussian distributed noise added to the original LP data. Spielman and Teng's result was simplified and strengthened over a series of works, with the current strongest upper bound being $O(\sigma^{-3/2} d^{13/4} \log(n)^{7/4})$ pivot steps due to Huiberts, Lee and Zhang (STOC '23). We prove that there exists a simplex method whose smoothed complexity is upper bounded by $O(\sigma^{-1/2} d^{11/4} \log(n)^{7/4})$ pivot steps. Furthermore, we prove a matching high-probability lower bound of $\Omega( \sigma^{-1/2} d^{1/2}\ln(4/\sigma)^{-1/4})$ on the combinatorial diameter of the feasible polyhedron after smoothing, on instances using $n = \lfloor (4/\sigma)^d \rfloor$ inequality constraints. This lower bound indicates that our algorithm has optimal noise dependence among all simplex methods, up to polylogarithmic factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04197v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleon Bach, Sophie Huiberts</dc:creator>
    </item>
    <item>
      <title>Correlation Clustering and (De)Sparsification: Graph Sketches Can Match Classical Algorithms</title>
      <link>https://arxiv.org/abs/2504.04258</link>
      <description>arXiv:2504.04258v1 Announce Type: new 
Abstract: Correlation clustering is a widely-used approach for clustering large data sets based only on pairwise similarity information. In recent years, there has been a steady stream of better and better classical algorithms for approximating this problem. Meanwhile, another line of research has focused on porting the classical advances to various sublinear algorithm models, including semi-streaming, Massively Parallel Computation (MPC), and distributed computing. Yet, these latter works typically rely on ad-hoc approaches that do not necessarily keep up with advances in approximation ratios achieved by classical algorithms.
  Hence, the motivating question for our work is this: can the gains made by classical algorithms for correlation clustering be ported over to sublinear algorithms in a \emph{black-box manner}? We answer this question in the affirmative by introducing the paradigm of graph de-sparsification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04258v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sepehr Assadi, Sanjeev Khanna, Aaron Putterman</dc:creator>
    </item>
    <item>
      <title>Efficient Rejection Sampling in the Entropy-Optimal Range</title>
      <link>https://arxiv.org/abs/2504.04267</link>
      <description>arXiv:2504.04267v1 Announce Type: new 
Abstract: The problem of generating a random variate $X$ from a finite discrete probability distribution $P$ using an entropy source of independent unbiased coin flips is considered. The Knuth and Yao complexity theory of nonuniform random number generation furnishes a family of "entropy-optimal" sampling algorithms that consume between $H(P)$ and $H(P)+2$ coin flips per generated output, where $H$ is the Shannon entropy function. However, the space complexity of entropy-optimal samplers scales exponentially with the number of bits required to encode $P$. This article introduces a family of efficient rejection samplers and characterizes their entropy, space, and time complexity. Within this family is a distinguished sampling algorithm that requires linearithmic space and preprocessing time, and whose expected entropy cost always falls in the entropy-optimal range $[H(P), H(P)+2)$. No previous sampler for discrete probability distributions is known to achieve these characteristics. Numerical experiments demonstrate performance improvements in runtime and entropy of the proposed algorithm compared to the celebrated alias method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04267v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas L. Draper, Feras A. Saad</dc:creator>
    </item>
    <item>
      <title>Binned Group Algebra Factorization for Differentially Private Continual Counting</title>
      <link>https://arxiv.org/abs/2504.04398</link>
      <description>arXiv:2504.04398v1 Announce Type: new 
Abstract: We study memory-efficient matrix factorization for differentially private counting under continual observation. While recent work by Henzinger and Upadhyay 2024 introduced a factorization method with reduced error based on group algebra, its practicality in streaming settings remains limited by computational constraints. We present new structural properties of the group algebra factorization, enabling the use of a binning technique from Andersson and Pagh (2024). By grouping similar values in rows, the binning method reduces memory usage and running time to $\tilde O(\sqrt{n})$, where $n$ is the length of the input stream, while maintaining a low error. Our work bridges the gap between theoretical improvements in factorization accuracy and practical efficiency in large-scale private learning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04398v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monika Henzinger, Nikita P. Kalinin, Jalaj Upadhyay</dc:creator>
    </item>
    <item>
      <title>Online Facility Assignments on Polygons</title>
      <link>https://arxiv.org/abs/2504.04556</link>
      <description>arXiv:2504.04556v1 Announce Type: new 
Abstract: We study the online facility assignment problem on regular polygons, where all sides are of equal length. The influence of specific geometric settings has remained mostly unexplored, even though classical online facility assignment problems have mainly dealt with linear and general metric spaces. We fill this gap by considering the following four basic geometric settings: equilateral triangles, rectangles, regular $n$-polygons, and circles. The facilities are situated at fixed positions on the boundary, and customers appear sequentially on the boundary. A customer needs to be assigned immediately without any information about future customer arrivals. We study a natural greedy algorithm. First, we study an equilateral triangle with three facilities at its corners; customers can appear anywhere on the boundary. We then analyze regular $n$-sided polygons, obtaining a competitive ratio of $2n-1$, showing that the algorithm performance degrades linearly with the number of corner points for polygons. For the circular configuration, the competitive ratio is $2n-1$ when the distance between two adjacent facilities is the same. And the competitive ratios are $n^2-n+1$ and $2^n - 1$ for varying distances linearly and exponentially respectively. Each facility has a fixed capacity proportional to the geometric configuration, and customers appear only along the boundary edges. Our results also show that simpler geometric configurations have more efficient performance bounds and that spacing facilities uniformly apart prevent worst-case scenarios. The findings have many practical implications because large networks of facilities are best partitioned into smaller and geometrically simple pieces to guarantee good overall performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04556v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sumaiya Malik, Reyan Ahmed, Md. Manzurul Hasan</dc:creator>
    </item>
    <item>
      <title>New Algorithms for Incremental Minimum Spanning Trees and Temporal Graph Applications</title>
      <link>https://arxiv.org/abs/2504.04619</link>
      <description>arXiv:2504.04619v1 Announce Type: new 
Abstract: Processing graphs with temporal information (the temporal graphs) has become increasingly important in the real world. In this paper, we study efficient solutions to temporal graph applications using new algorithms for Incremental Minimum Spanning Trees (MST). The first contribution of this work is to formally discuss how a broad set of setting-problem combinations of temporal graph processing can be solved using incremental MST, along with their theoretical guarantees. However, to give efficient solutions for incremental MST, we observe a gap between theory and practice. While many classic data structures, such as the link-cut tree, provide strong bounds for incremental MST, their performance is limited in practice. Meanwhile, existing practical solutions used in applications do not have any non-trivial theoretical guarantees. Our second and main contribution includes new algorithms for incremental MST that are efficient both in theory and in practice. Our new data structure, the AM-tree, achieves the same theoretical bound as the link-cut tree for temporal graph processing and shows strong performance in practice. In our experiments, the AM-tree has competitive or better performance than existing practical solutions due to theoretical guarantee, and can be significantly faster than the link-cut tree (7.8-11x in update and 7.7-13.7x in query).</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04619v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangyun Ding, Yan Gu, Yihan Sun</dc:creator>
    </item>
    <item>
      <title>Automating the Search for Small Hard Examples to Approximation Algorithms</title>
      <link>https://arxiv.org/abs/2504.04738</link>
      <description>arXiv:2504.04738v1 Announce Type: new 
Abstract: Given an approximation algorithm $A$, we want to find the input with the worst approximation ratio, i.e., the input for which $A$'s output's objective value is the worst possible compared to the optimal solution's objective value. Such hard examples shed light on the approximation algorithm's weaknesses, and could help us design better approximation algorithms. When the inputs are discrete (e.g., unweighted graphs), one can find hard examples for small input sizes using brute-force enumeration. However, it's not obvious how to do this when the input space is continuous, as in makespan minimization or bin packing.
  We develop a technique for finding small hard examples for a large class of approximation algorithms. Our algorithm works by constructing a decision tree representation of the approximation algorithm and then running a linear program for each leaf node of the decision tree. We implement our technique in Python, and demonstrate it on the longest-processing-time (LPT) heuristic for makespan minimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04738v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eklavya Sharma</dc:creator>
    </item>
    <item>
      <title>The Complexity of Maximal Common Subsequence Enumeration</title>
      <link>https://arxiv.org/abs/2504.04757</link>
      <description>arXiv:2504.04757v1 Announce Type: new 
Abstract: Frequent pattern mining is widely used to find ``important'' or ``interesting'' patterns in data. While it is not easy to mathematically define such patterns, maximal frequent patterns are promising candidates, as frequency is a natural indicator of relevance and maximality helps to summarize the output. As such, their mining has been studied on various data types, including itemsets, graphs, and strings. The complexity of mining maximal frequent itemsets and subtrees has been thoroughly investigated (e.g., [Boros et al., 2003], [Uno et al., 2004]) in the literature. On the other hand, while the idea of mining frequent subsequences in sequential data was already introduced in the seminal paper [Agrawal et al., 1995], the complexity of the problem is still open.
  In this paper, we investigate the complexity of the maximal common subsequence enumeration problem, which is both an important special case of maximal frequent subsequence mining and a generalization of the classic longest common subsequence (LCS) problem. We show the hardness of enumerating maximal common subsequences between multiple strings, ruling out the possibility of an \emph{output-polynomial time} enumeration algorithm under $P \neq NP$, that is, an algorithm that runs in time ${\rm poly}(|\mathcal I| + N)$, where $|\mathcal I|$ and $N$ are the size of the input and number of output solutions, respectively. To circumvent this intractability, we also investigate the parameterized complexity of the problem, and show several results when the alphabet size, the number of strings, and the length of a string are taken into account as parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04757v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3725252</arxiv:DOI>
      <dc:creator>Giovanni Buzzega, Alessio Conte, Yasuaki Kobayashi, Kazuhiro Kurita, Giulia Punzi</dc:creator>
    </item>
    <item>
      <title>Towards Optimal Distributed Delta Coloring</title>
      <link>https://arxiv.org/abs/2504.03080</link>
      <description>arXiv:2504.03080v1 Announce Type: cross 
Abstract: The $\Delta$-vertex coloring problem has become one of the prototypical problems for understanding the complexity of local distributed graph problems on constant-degree graphs. The major open problem is whether the problem can be solved deterministically in logarithmic time, which would match the lower bound [Chang et al., FOCS'16]. Despite recent progress in the design of efficient $\Delta$-coloring algorithms, there is currently a polynomial gap between the upper and lower bounds.
  In this work we present a $O(\log n)$-round deterministic $\Delta$-coloring algorithm for dense constant-degree graphs, matching the lower bound for the problem on general graphs. For general $\Delta$ the algorithms' complexity is $\min\{\widetilde{O}(\log^{5/3}n),O(\Delta+\log n)\}$. All recent distributed and sublinear graph coloring algorithms (also for coloring with more than $\Delta$ colors) decompose the graph into sparse and dense parts. Our algorithm works for the case that this decomposition has no sparse vertices. Ironically, in recent (randomized) $\Delta$-coloring algorithms, dealing with sparse parts was relatively easy and these dense parts arguably posed the major hurdle. We present a solution that addresses the dense parts and may have the potential for extension to sparse parts.
  Our approach is fundamentally different from prior deterministic algorithms and hence hopefully contributes towards designing an optimal algorithm for the general case. Additionally, we leverage our result to also obtain a randomized $\min\{\widetilde{O}(\log^{5/3}\log n), O(\Delta+\log\log n)\}$-round algorithm for $\Delta$-coloring dense graphs that also matches the lower bound for the problem on general constant-degree graphs [Brandt et al.; STOC'16].</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03080v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Jakob, Yannic Maus</dc:creator>
    </item>
    <item>
      <title>AbsInf: A Lightweight Object to Represent float('inf') in Dijkstra's Algorithm</title>
      <link>https://arxiv.org/abs/2504.04302</link>
      <description>arXiv:2504.04302v1 Announce Type: cross 
Abstract: We introduce AbsInf, a lightweight abstract object designed as a high-performance alternative to Python's native float('inf') within pathfinding algorithms. Implemented as a C-based Python extension, AbsInf bypasses IEEE-754 float coercion and dynamic type dispatch, offering constant-time dominance comparisons and arithmetic neutrality. When integrated into Dijkstra's algorithm without altering its logic, AbsInf reduces runtime by up to 17.2%, averaging 9.74% across diverse synthetic and real-world graph datasets. This optimization highlights the performance trade-offs in high-frequency algorithmic constructs, where a symbolic use of infinity permits efficient abstraction. Our findings contribute to the broader discourse on lightweight architectural enhancements for interpreted languages, particularly in performance-critical control flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04302v1</guid>
      <category>cs.PL</category>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjan Bellamkonda, Laksh Bharani, Harivatsan Selvam</dc:creator>
    </item>
    <item>
      <title>Minimum Non-Obtuse Triangulations: The CG:SHOP Challenge 2025</title>
      <link>https://arxiv.org/abs/2504.04412</link>
      <description>arXiv:2504.04412v1 Announce Type: cross 
Abstract: We give an overview of the 2025 Computational Geometry Challenge targeting the problem Minimum Non-Obtuse Triangulation: Given a planar straight-line graph G in the plane, defined by a set of points in the plane (representing vertices) and a set of non-crossing line segments connecting them (representing edges); the objective is to find a feasible non-obtuse triangulation that uses a minimum number of Steiner points. If no triangulation without obtuse triangles is found, the secondary objective is to minimize the number of obtuse triangles in the triangulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04412v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'andor P. Fekete, Phillip Keldenich, Dominik Krupke, Stefan Schirra</dc:creator>
    </item>
    <item>
      <title>Tight Low Degree Hardness for Optimizing Pure Spherical Spin Glasses</title>
      <link>https://arxiv.org/abs/2504.04632</link>
      <description>arXiv:2504.04632v1 Announce Type: cross 
Abstract: We prove constant degree polynomial algorithms cannot optimize pure spherical $p$-spin Hamiltonians beyond the algorithmic threshold $\mathsf{ALG}(p)=2\sqrt{\frac{p-1}{p}}$. The proof goes by transforming any hypothetical such algorithm into a Lipschitz one, for which hardness was shown previously by the author and B. Huang.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04632v1</guid>
      <category>math.PR</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.DS</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Sellke</dc:creator>
    </item>
    <item>
      <title>Ineffectiveness for Search and Undecidability of PCSP Meta-Problems</title>
      <link>https://arxiv.org/abs/2504.04639</link>
      <description>arXiv:2504.04639v1 Announce Type: cross 
Abstract: It is an open question whether the search and decision versions of promise CSPs are equivalent. Most known algorithms for PCSPs solve only their \emph{decision} variant, and it is unknown whether they can be adapted to solve \emph{search} as well. The main approaches, called BLP, AIP and BLP+AIP, handle a PCSP by finding a solution to a relaxation of some integer program. We prove that rounding those solutions to a proper search certificate can be as hard as any problem in the class TFNP. In other words, these algorithms are ineffective for search. Building on the algebraic approach to PCSPs, we find sufficient conditions that imply ineffectiveness for search. Our tools are tailored to algorithms that are characterized by minions in a suitable way, and can also be used to prove undecidability results for meta-problems. This way, we show that the families of templates solvable via BLP, AIP, and BLP+AIP are undecidable.
  Using the same techniques we also analyze several algebraic conditions that are known to guarantee the tractability of finite-template CSPs. We prove that several meta-problems related to cyclic polymorphims and WNUs are undecidable for PCSPs. In particular, there is no algorithm deciding whether a finite PCSP template (1) admits cyclic a polymorphism, (2) admits a WNU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04639v1</guid>
      <category>cs.CC</category>
      <category>cs.CL</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Larrauri</dc:creator>
    </item>
    <item>
      <title>A Customized SAT-based Solver for Graph Coloring</title>
      <link>https://arxiv.org/abs/2504.04821</link>
      <description>arXiv:2504.04821v1 Announce Type: cross 
Abstract: We introduce ZykovColor, a novel SAT-based algorithm to solve the graph coloring problem working on top of an encoding that mimics the Zykov tree. Our method is based on an approach of H\'ebrard and Katsirelos (2020) that employs a propagator to enforce transitivity constraints, incorporate lower bounds for search tree pruning, and enable inferred propagations. We leverage the recently introduced IPASIR-UP interface for CaDiCal to implement these techniques with a SAT solver. Furthermore, we propose new features that take advantage of the underlying SAT solver. These include modifying the integrated decision strategy with vertex domination hints and using incremental bottom-up search that allows to reuse learned clauses from previous calls. Additionally, we integrate a more efficient clique computation to improve the lower bounds during the search. We validate the effectiveness of each new feature through an experimental analysis. ZykovColor outperforms other state-of-the-art graph coloring implementations on the DIMACS benchmark set. Further experiments on random Erd\H{o}s-R\'enyi graphs show that our new approach dominates state-of-the-art SAT-based methods for both very sparse and highly dense graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04821v1</guid>
      <category>cs.DM</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timo Brand, Daniel Faber, Stephan Held, Petra Mutzel</dc:creator>
    </item>
    <item>
      <title>The Minimum Eternal Vertex Cover Problem on a Subclass of Series-Parallel Graphs</title>
      <link>https://arxiv.org/abs/2504.04897</link>
      <description>arXiv:2504.04897v1 Announce Type: cross 
Abstract: Eternal vertex cover is the following two-player game between a defender and an attacker on a graph. Initially, the defender positions k guards on k vertices of the graph; the game then proceeds in turns between the defender and the attacker, with the attacker selecting an edge and the defender responding to the attack by moving some of the guards along the edges, including the attacked one. The defender wins a game on a graph G with k guards if they have a strategy such that, in every round of the game, the vertices occupied by the guards form a vertex cover of G, and the attacker wins otherwise. The eternal vertex cover number of a graph G is the smallest number k of guards allowing the defender to win and Eternal Vertex Cover is the problem of computing the eternal vertex cover number of the given graph.
  We study this problem when restricted to the well-known class of series-parallel graphs. In particular, we prove that Eternal Vertex Cover can be solved in linear time when restricted to melon graphs, a proper subclass of series-parallel graphs. Moreover, we also conjecture that this problem is NP-hard on series-parallel graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04897v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiziana Calamoneri, Federico Cor\`o, Giacomo Paesani</dc:creator>
    </item>
    <item>
      <title>DDPM Score Matching and Distribution Learning</title>
      <link>https://arxiv.org/abs/2504.05161</link>
      <description>arXiv:2504.05161v1 Announce Type: cross 
Abstract: Score estimation is the backbone of score-based generative models (SGMs), especially denoising diffusion probabilistic models (DDPMs). A key result in this area shows that with accurate score estimates, SGMs can efficiently generate samples from any realistic data distribution (Chen et al., ICLR'23; Lee et al., ALT'23). This distribution learning result, where the learned distribution is implicitly that of the sampler's output, does not explain how score estimation relates to classical tasks of parameter and density estimation.
  This paper introduces a framework that reduces score estimation to these two tasks, with various implications for statistical and computational learning theory:
  Parameter Estimation: Koehler et al. (ICLR'23) demonstrate that a score-matching variant is statistically inefficient for the parametric estimation of multimodal densities common in practice. In contrast, we show that under mild conditions, denoising score-matching in DDPMs is asymptotically efficient.
  Density Estimation: By linking generation to score estimation, we lift existing score estimation guarantees to $(\epsilon,\delta)$-PAC density estimation, i.e., a function approximating the target log-density within $\epsilon$ on all but a $\delta$-fraction of the space. We provide (i) minimax rates for density estimation over H\"older classes and (ii) a quasi-polynomial PAC density estimation algorithm for the classical Gaussian location mixture model, building on and addressing an open problem from Gatmiry et al. (arXiv'24).
  Lower Bounds for Score Estimation: Our framework offers the first principled method to prove computational lower bounds for score estimation across general distributions. As an application, we establish cryptographic lower bounds for score estimation in general Gaussian mixture models, conceptually recovering Song's (NeurIPS'24) result and advancing his key open problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05161v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sinho Chewi, Alkis Kalavasis, Anay Mehrotra, Omar Montasser</dc:creator>
    </item>
    <item>
      <title>Infinitely Divisible Noise for Differential Privacy: Nearly Optimal Error in the High $\varepsilon$ Regime</title>
      <link>https://arxiv.org/abs/2504.05202</link>
      <description>arXiv:2504.05202v1 Announce Type: cross 
Abstract: Differential privacy (DP) can be achieved in a distributed manner, where multiple parties add independent noise such that their sum protects the overall dataset with DP. A common technique here is for each party to sample their noise from the decomposition of an infinitely divisible distribution. We analyze two mechanisms in this setting: 1) the generalized discrete Laplace (GDL) mechanism, whose distribution (which is closed under summation) follows from differences of i.i.d. negative binomial shares, and 2) the multi-scale discrete Laplace (MSDLap) mechanism, a novel mechanism following the sum of multiple i.i.d. discrete Laplace shares at different scales.
  For $\varepsilon \geq 1$, our mechanisms can be parameterized to have $O\left(\Delta^3 e^{-\varepsilon}\right)$ and $O\left(\min\left(\Delta^3 e^{-\varepsilon}, \Delta^2 e^{-2\varepsilon/3}\right)\right)$ MSE, respectively, where $\Delta$ denote the sensitivity; the latter bound matches known optimality results. We also show a transformation from the discrete setting to the continuous setting, which allows us to transform both mechanisms to the continuous setting and thereby achieve the optimal $O\left(\Delta^2 e^{-2\varepsilon / 3}\right)$ MSE. To our knowledge, these are the first infinitely divisible additive noise mechanisms that achieve order-optimal MSE under pure DP, so our work shows formally there is no separation in utility when query-independent noise adding mechanisms are restricted to infinitely divisible noise. For the continuous setting, our result improves upon the Arete mechanism from [Pagh and Stausholm, ALT 2022] which gives an MSE of $O\left(\Delta^2 e^{-\varepsilon/4}\right)$. Furthermore, we give an exact sampler tuned to efficiently implement the MSDLap mechanism, and we apply our results to improve a state of the art multi-message shuffle DP protocol in the high $\varepsilon$ regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05202v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charlie Harrison, Pasin Manurangsi</dc:creator>
    </item>
    <item>
      <title>The Competitive Ratio of Threshold Policies for Online Unit-density Knapsack Problems</title>
      <link>https://arxiv.org/abs/1907.08735</link>
      <description>arXiv:1907.08735v4 Announce Type: replace 
Abstract: We study a wholesale supply chain ordering problem. In this problem, the supplier has an initial stock, and faces an unpredictable stream of incoming orders, making real-time decisions on whether to accept or reject each order. What makes this wholesale supply chain ordering problem special is its ``knapsack constraint,'' that is, we do not allow partially accepting an order or splitting an order. The objective is to maximize the utilized stock.
  We model this wholesale supply chain ordering problem as an online unit-density knapsack problem. We study randomized threshold algorithms that accept an item as long as its size exceeds the threshold. We derive two optimal threshold distributions, the first is 0.4324-competitive relative to the optimal offline integral packing, and the second is 0.4285-competitive relative to the optimal offline fractional packing. Both results require optimizing the cumulative distribution function of the random threshold, which are challenging infinite-dimensional optimization problems. We also consider the generalization to multiple knapsacks, where an arriving item has a different size in each knapsack. We derive a 0.2142-competitive algorithm for this problem. We also show that any randomized algorithm for this problem cannot be more than 0.4605-competitive. This is the first upper bound strictly less than 0.5, which implies the intrinsic challenge of knapsack constraint.
  We show how to naturally implement our optimal threshold distributions in the warehouses of a Latin American chain department store. We run simulations on their order data, which demonstrate the efficacy of our proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:1907.08735v4</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Will Ma, David Simchi-Levi, Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>Edge-weighted Online Stochastic Matching: Beating $1-\frac1e$</title>
      <link>https://arxiv.org/abs/2210.12543</link>
      <description>arXiv:2210.12543v3 Announce Type: replace 
Abstract: We study the edge-weighted online stochastic matching problem. Since Feldman, Mehta, Mirrokni, and Muthukrishnan proposed the $(1-\frac1e)$-competitive Suggested Matching algorithm, there has been no improvement for the general edge-weighted online stochastic matching problem. In this paper, we introduce the first algorithm beating the $1-\frac1e$ barrier in this setting, achieving a competitive ratio of $0.645$. Under the LP proposed by Jaillet and Lu, we design an algorithmic preprocessing, dividing all edges into two classes. Then based on the Suggested Matching algorithm, we adjust the matching strategy to improve the performance on one class in the early stage and on another class in the late stage, while keeping the matching events of different edges highly independent. By balancing them, we finally guarantee the matched probability of every single edge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.12543v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyi Yan</dc:creator>
    </item>
    <item>
      <title>SARRIGUREN: a polynomial-time complete algorithm for random $k$-SAT with relatively dense clauses</title>
      <link>https://arxiv.org/abs/2401.09234</link>
      <description>arXiv:2401.09234v2 Announce Type: replace 
Abstract: SARRIGUREN, a new complete algorithm for SAT based on counting clauses (which is valid also for Unique-SAT and #SAT) is described, analyzed and tested. Although existing complete algorithms for SAT perform slower with clauses with many literals, that is an advantage for SARRIGUREN, because the more literals are in the clauses the bigger is the probability of overlapping among clauses, a property that makes the clause counting process more efficient. Actually, it provides a $O(m^2 \times n/k)$ time complexity for random $k$-SAT instances of $n$ variables and $m$ relatively dense clauses, where that density level is relative to the number of variables $n$, that is, clauses are relatively dense when $k\geq7\sqrt{n}$. Although theoretically there could be worst-cases with exponential complexity, the probability of those cases to happen in random $k$-SAT with relatively dense clauses is practically zero. The algorithm has been empirically tested and that polynomial time complexity maintains also for $k$-SAT instances with less dense clauses ($k\geq5\sqrt{n}$). That density could, for example, be of only 0.049 working with $n=20000$ variables and $k=989$ literals. In addition, they are presented two more complementary algorithms that provide the solutions to $k$-SAT instances and valuable information about number of solutions for each literal. Although this algorithm does not solve the NP=P problem (it is not a polynomial algorithm for 3-SAT), it broads the knowledge about that subject, because $k$-SAT with $k&gt;3$ and dense clauses is not harder than 3-SAT. Moreover, the Python implementation of the algorithms, and all the input datasets and obtained results in the experiments are made available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09234v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alfredo Go\~ni Sarriguren</dc:creator>
    </item>
    <item>
      <title>Pointwise Lipschitz Continuous Graph Algorithms via Proximal Gradient Trajectory Analysis</title>
      <link>https://arxiv.org/abs/2405.08938</link>
      <description>arXiv:2405.08938v3 Announce Type: replace 
Abstract: In many real-world applications, it is undesirable to drastically change the problem solution after a small perturbation in the input as unstable outputs can lead to costly transaction fees, privacy and security concerns, reduced user trust, and lack of replicability. Despite the widespread application of graph algorithms, many classical algorithms are not robust to small input disturbances. Towards addressing this issue, we study the Lipschitz continuity of graph algorithms, a notion of stability introduced by Kumabe and Yoshida [KY23, FOCS'23] and further studied in various settings [KY24, ICALP'24], [KY25, SODA'25].
  We give a general unifying framework for analyzing and designing pointwise Lipschitz continuous graph algorithms. In addition to being more general, our techniques obtain better bounds than can be achieved through extensions of previous work. First, we consider a natural continuous relaxation of the underlying graph problem along with a regularized objective function. Then, we develop a novel analysis of the distance between optimal solutions of the convex programs under small perturbations of the weights. Finally, we present new problem-specific rounding techniques to obtain integral solutions to several graph problems that approximately maintain the stability guarantees of the fractional solutions. We apply our framework to a number of problems including minimum $s$-$t$ cut and maximum ($b$-)matching. To complement our algorithms, we show the tightness of our framework for the case of minimum $s$-$t$ cut by establishing tight lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08938v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quanquan C. Liu, Grigoris Velegkas, Yuichi Yoshida, Felix Zhou</dc:creator>
    </item>
    <item>
      <title>Random local access for sampling k-SAT solutions</title>
      <link>https://arxiv.org/abs/2409.03951</link>
      <description>arXiv:2409.03951v2 Announce Type: replace 
Abstract: We present a sublinear time algorithm that gives random local access to the uniform distribution over satisfying assignments to an arbitrary k-CNF formula $\Phi$, at exponential clause density. Our algorithm provides memory-less query access to variable assignments, such that the output variable assignments consistently emulate a single global satisfying assignment whose law is close to the uniform distribution over satisfying assignments to $\Phi$.
  Such models were formally defined (for the more general task of locally sampling from exponentially sized sample spaces) in 2017 by Biswas, Rubinfeld, and Yodpinyanee, who studied the analogous problem for the uniform distribution over proper q-colorings. This model extends a long line of work over multiple decades that studies sublinear time algorithms for problems in theoretical computer science. Random local access and related models have been studied for a wide variety of natural Gibbs distributions and random graphical processes. Here, we establish feasiblity of random local access models for one of the most canonical such sample spaces, the set of satisfying assignments to a k-CNF formula.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03951v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dingding Dong, Nitya Mani</dc:creator>
    </item>
    <item>
      <title>Deterministic counting from coupling independence</title>
      <link>https://arxiv.org/abs/2410.23225</link>
      <description>arXiv:2410.23225v2 Announce Type: replace 
Abstract: We show that spin systems with bounded degrees and coupling independence admit fully polynomial time approximation schemes (FPTAS). We design a new recursive deterministic counting algorithm to achieve this. As applications, we give the first FPTASes for $q$-colourings on graphs of bounded maximum degree $\Delta\ge 3$, when $q\ge (11/6-\varepsilon_0)\Delta$ for some small $\varepsilon_0\approx 10^{-5}$, or when $\Delta\ge 125$ and $q\ge 1.809\Delta$, and on graphs with sufficiently large (but constant) girth, when $q\geq\Delta+3$. These bounds match the current best randomised approximate counting algorithms by Chen, Delcourt, Moitra, Perarnau, and Postle (2019), Carlson and Vigoda (2024), and Chen, Liu, Mani, and Moitra (2023), respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23225v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Chen, Weiming Feng, Heng Guo, Xinyuan Zhang, Zongrui Zou</dc:creator>
    </item>
    <item>
      <title>Multicriteria Spanners -- A New Tool for Network Design</title>
      <link>https://arxiv.org/abs/2412.05526</link>
      <description>arXiv:2412.05526v2 Announce Type: replace 
Abstract: Designing sparse directed spanners, which are subgraphs that approximately maintain distance constraints, has attracted sustained interest in TCS, especially due to their wide applicability, as well as the difficulty to obtain tight results. However, a significant drawback of the notion of spanners is that it cannot capture multiple distance-like constraints for the same demand pair. In this paper we initiate the study of directed multicriteria spanners, in which the notion of edge lengths is replaced by the notion of resource consumption vectors, where each entry corresponds to the consumption of the respective resource on that edge. The goal is to find a minimum-cost routing solution that satisfies the multiple constraints. To the best of our knowledge, we obtain the first approximation algorithms for the directed multicriteria spanners problems, under natural assumptions. Our results match the state-of-the-art approximation ratios in special cases of ours. We also establish reductions from other natural network connectivity problems to the directed multicriteria spanners problems, including Group Steiner Distances, introduced in the undirected setting by Bil\`o, Gual\`a, Leucci and Straziota (ESA 2024), and Edge-Avoiding spanners. Our reductions imply approximation algorithms for these problems and illustrate that the notion of directed multicriteria spanners is an appropriate abstraction and generalization of natural special cases from the literature. Our main technical tool is a delicate generalization of the minimum-density junction tree framework of Chekuri, Even, Gupta, and Segev (SODA 2008, TALG 2011) to the notion of minimum-density resource-constrained junction trees, which also extends ideas from Chlamt\'a\v{c}, Dinitz, Kortsarz, and Laekhanukit (SODA 2017, TALG 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05526v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Grigorescu, Nithish Kumar Kumar, Young-San Lin</dc:creator>
    </item>
    <item>
      <title>New Oracles and Labeling Schemes for Vertex Cut Queries</title>
      <link>https://arxiv.org/abs/2501.13596</link>
      <description>arXiv:2501.13596v2 Announce Type: replace 
Abstract: We study the succinct representations of vertex cuts by centralized oracles and labeling schemes. For an undirected $n$-vertex graph $G = (V,E)$ and integer parameter $f \geq 1$, the goal is supporting vertex cut queries: Given $F \subseteq V$ with $|F| \leq f$, determine if $F$ is a vertex cut in $G$. In the centralized data structure setting, it is required to preprocess $G$ into an $f$-vertex cut oracle that can answer such queries quickly, while occupying only small space. In the labeling setting, one should assign a short label to each vertex in $G$, so that a cut query $F$ can be answered by merely inspecting the labels assigned to the vertices in $F$. While the ``$st$ cut variants'' of the above problems have been extensively studied and are known to admit very efficient solutions, the basic (global) ``cut query'' setting is essentially open (particularly for $f &gt; 3$). This work achieves the first significant progress on these problems:
  [$f$-Vertex Cut Labels:] Every $n$-vertex graph admits an $f$-vertex cut labeling scheme, where the labels have length of $\tilde{O}(n^{1-1/f})$ bits (when $f$ is polylogarithmic in $n$). This nearly matches the recent lower bound given by Long, Pettie and Saranurak (SODA 2025).
  [$f$-Vertex Cut Oracles:] For $f=O(\log n)$, every $n$-vertex graph $G$ admits $f$-vertex cut oracle with $\tilde{O}(n)$ space and $\tilde{O}(2^f)$ query time. We also show that our $f$-vertex cut oracles for every $1 \leq f \leq n$ are optimal up to $n^{o(1)}$ factors (conditioned on plausible fine-grained complexity conjectures). If $G$ is $f$-connected, i.e., when one is interested in \emph{minimum} vertex cut queries, the query time improves to $\tilde{O}(f^2)$, for any $1 \leq f \leq n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13596v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonggang Jiang, Merav Parter, Asaf Petruschka</dc:creator>
    </item>
    <item>
      <title>{\epsilon}-Cost Sharding: Scaling Hypergraph-Based Static Functions and Filters to Trillions of Keys</title>
      <link>https://arxiv.org/abs/2503.18397</link>
      <description>arXiv:2503.18397v3 Announce Type: replace 
Abstract: We describe a simple and yet very scalable implementation of static functions (VFunc) and of static filters (VFilter) based on hypergraphs. We introduce the idea of {\epsilon}-cost sharding, which allows us to build structures that can manage trillions of keys, at the same time increasing memory locality in hypergraph-based constructions. Contrarily to the commonly used HEM sharding method, {\epsilon}-cost sharding does not require to store of additional information, and does not introduce dependencies in the computation chain; its only cost is that of few arithmetical instructions, and of a relative increase {\epsilon} in space usage. We apply {\epsilon}-cost sharding to the classical MWHC construction, but we obtain the best result by combining Dietzfelbinger and Walzer's fuse graphs for large shards with lazy Gaussian elimination for small shards. We obtain large structures with an overhead of 10.5% with respect to the information-theoretical lower bound and with a query time that is a few nanoseconds away from the query time of the non-sharded version, which is the fastest currently available within the same space bounds. Besides comparing our structures with a non-sharded version, we contrast its tradeoffs with bumped ribbon constructions, a space-saving alternative to hypergraph-based static functions and filters, which provide optimum space consumption but slow construction and query time (though construction can be parallelized very efficiently). We build offline a trillion-key filter using commodity hardware in just 60 ns/key.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18397v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastiano Vigna</dc:creator>
    </item>
    <item>
      <title>Long Arithmetic Progressions in Sumsets and Subset Sums: Constructive Proofs and Efficient Witnesses</title>
      <link>https://arxiv.org/abs/2503.19299</link>
      <description>arXiv:2503.19299v3 Announce Type: replace 
Abstract: Existence of long arithmetic progression in sumsets and subset sums has been studied extensively in the field of additive combinatorics. These additive combinatorics results play a central role in the recent progress of fundamental problems in theoretical computer science including Knapsack and Subset Sum. The non-constructiveness of relevant additive combinatorics results affects their application in algorithms. In particular, several additive combinatorics-based algorithms for Subset Sum work only for the decision version of the problem, but not for the search version.
  We provide constructive proofs for finite addition theorems [S\'ark\H{o}zy'89 '94], which are fundamental results in additive combinatorics concerning the existence of long arithmetic progression in sumsets and subset sums. Our constructive proofs yield a near-linear time algorithm that returns an arithmetic progression explicitly, and moreover, for each term in the arithmetic progression, it also returns its representation as the sum of elements in the base set.
  As an application, we obtain an $\tilde{O}(n)$-time algorithm for the search version of dense subset sum now. Another application of our result is Unbounded Subset Sum, where each input integer can be used an infinite number of times. A classic result on the Frobenius problem [Erd\H{o}s and Graham '72] implies that for all $t \geq 2a^2_{\max}/n$, the decision version can be solved trivially in linear time. It remains unknown whether the search version can be solved in the same time. Our result implies that for all $t \geq ca^2_{\max}/n$ for some constant $c$, a solution for Unbounded Subset Sum can be obtained in $O(n \log a_{\max})$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19299v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Chen, Yuchen Mao, Guochuan Zhang</dc:creator>
    </item>
    <item>
      <title>Ortho-Radial Drawing in Near-Linear Time</title>
      <link>https://arxiv.org/abs/2305.00425</link>
      <description>arXiv:2305.00425v3 Announce Type: replace-cross 
Abstract: An orthogonal drawing is an embedding of a plane graph into a grid. In a seminal work of Tamassia (SIAM Journal on Computing 1987), a simple combinatorial characterization of angle assignments that can be realized as bend-free orthogonal drawings was established, thereby allowing an orthogonal drawing to be described combinatorially by listing the angles of all corners. The characterization reduces the need to consider certain geometric aspects, such as edge lengths and vertex coordinates, and simplifies the task of graph drawing algorithm design.
  Barth, Niedermann, Rutter, and Wolf (SoCG 2017) established an analogous combinatorial characterization for ortho-radial drawings, which are a generalization of orthogonal drawings to cylindrical grids. The proof of the characterization is existential and does not result in an efficient algorithm. Niedermann, Rutter, and Wolf (SoCG 2019) later addressed this issue by developing quadratic-time algorithms for both testing the realizability of a given angle assignment as an ortho-radial drawing without bends and constructing such a drawing.
  In this paper, we further improve the time complexity of these tasks to near-linear time. We establish a new characterization for ortho-radial drawings based on the concept of a good sequence. Using the new characterization, we design a simple greedy algorithm for constructing ortho-radial drawings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.00425v3</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.25.11</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 4 (2025), Article 11, 1-53</arxiv:journal_reference>
      <dc:creator>Yi-Jun Chang</dc:creator>
    </item>
    <item>
      <title>The Transient Cost of Learning in Queueing Systems</title>
      <link>https://arxiv.org/abs/2308.07817</link>
      <description>arXiv:2308.07817v3 Announce Type: replace-cross 
Abstract: Queueing systems are widely applicable stochastic models with use cases in communication networks, healthcare, service systems, etc. Although their optimal control has been extensively studied, most existing approaches assume perfect knowledge of the system parameters. This assumption rarely holds in practice where there is parameter uncertainty, thus motivating a recent line of work on bandit learning for queueing systems. This nascent stream of research focuses on the asymptotic performance of the proposed algorithms but does not provide insight on the transient performance in the early stages of the learning process.
  In this paper, we propose the Transient Cost of Learning in Queueing (TCLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty. We characterize the TCLQ of a single-queue multi-server system, and then extend these results to multi-queue multi-server systems and networks of queues. In establishing our results, we propose a unified analysis framework for TCLQ that bridges Lyapunov and bandit analysis, provides guarantees for a wide range of algorithms, and could be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.07817v3</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <category>math.PR</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Freund, Thodoris Lykouris, Wentao Weng</dc:creator>
    </item>
    <item>
      <title>Improved Hardness and Approximations for Cardinality-Based Minimum $s$-$t$ Cuts Problems in Hypergraphs</title>
      <link>https://arxiv.org/abs/2409.07201</link>
      <description>arXiv:2409.07201v3 Announce Type: replace-cross 
Abstract: In hypergraphs, an edge that crosses a cut (i.e., a bipartition of nodes) can be split in several ways, depending on how many nodes are placed on each side of the cut. A cardinality-based splitting function assigns a nonnegative cost of $w_i$ for each cut hyperedge $e$ with exactly $i$ nodes on the side of the cut that contains the minority of nodes from $e$. The cardinality-based minimum $s$-$t$ cut aims to find an $s$-$t$ cut with minimum total cost. We answer a recently posed open question by proving that the problem becomes NP-hard outside the submodular region shown by~\cite{veldt2022hypergraph}. Our result also holds for $r$-uniform hypergraphs with $r \geq 4$. Specifically for $4$-uniform hypergraphs we show that the problem is NP-hard for all $w_2 &gt; 2$, and additionally prove that the No-Even-Split problem is NP-hard. We then turn our attention to approximation strategies and approximation hardness results in the non-submodular case. We design a strategy for projecting non-submodular penalties to the submodular region, which we prove gives the optimal approximation among all such projection strategies. We also show that alternative approaches are unlikely to provide improved guarantees, by showing matching approximation hardness bounds assuming the Unique Games Conjecture and asymptotically tight approximation hardness bounds assuming $\text{P} \neq \text{NP}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07201v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Adriaens, Vedangi Bengali, Iiro Kumpulainen, Nikolaj Tatti, Nate Veldt</dc:creator>
    </item>
    <item>
      <title>The Empirical Spectral Distribution of i.i.d. Random Matrices with Random Perturbations</title>
      <link>https://arxiv.org/abs/2410.21919</link>
      <description>arXiv:2410.21919v2 Announce Type: replace-cross 
Abstract: Consider a $d\times d$ random matrix $\mathbf{M} :=\frac{1}{\sqrt{d}}\mathbf{W}+\mathbf{U}\mathbf{\Lambda}\mathbf{U}^*$, where the entries of $\mathbf{W}$ are i.i.d.\ complex variables with zero mean, unit variance, and finite fourth moment; $\mathbf{\Lambda}=\operatorname{diag}(\lambda_1, \lambda_2, \ldots, \lambda_r)$ is a diagonal matrix with $\lambda_1\ge\lambda_2\ge \ldots\ge \lambda_r&gt;1$ and $\mathbf{U}$ is uniformly drawn from the Stiefel manifold $\operatorname{Stief}(d,r)$ or its complex counterpart $\operatorname{Stief}_\mathbb{C}(d,r)$. We propose three results: (i) We show that the top $r$ eigenvalues of $\mathbf{M}$ concentrate around $\lambda_1,\lambda_2,\ldots,\lambda_r$, respectively, and the magnitude of the $(r{+}1)$-th eigenvalue concentrates around $1$. (ii) For the case $r=1$ (i.e., $\mathbf{M}=\frac{1}{\sqrt{d}}\mathbf{W}+\lambda\mathbf{u}\mathbf{u}^*$), we prove that the top eigenvector (namely $\mathbf{v}_1(\mathbf{M})$) aligns with $\mathbf{u}$ in the sense that $\left|\langle\mathbf{v}_1(\mathbf{M}), \mathbf{u}\rangle\right|$ converges to a limit which depends on $\lambda$ (in probability). (iii) In applications, we present the first optimal \textit{query complexity} lower bound for approximating the top eigenvector of asymmetric matrices. We show that for every $\operatorname{gap}=\frac{\lambda-1}{\lambda}\in (0,1/2]$, then $\operatorname{gap}(\mathbf{M})=\Omega(\operatorname{gap})$ with high probability, and if a matrix-vector product algorithm can identify a vector $\hat{\mathbf{v}}$ which satisfies $\left\|\hat{\mathbf{v}}-\mathbf{v}_1(\mathbf{M}) \right\|_2^2\le \operatorname{const}\times \operatorname{gap}$, it needs at least $\mathcal{O}\left(\frac{\log d}{\operatorname{gap}}\right)$ queries of matrix-vector products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21919v2</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Chen, Zhihua Zhang</dc:creator>
    </item>
    <item>
      <title>Young domination on Hamming rectangles</title>
      <link>https://arxiv.org/abs/2501.03788</link>
      <description>arXiv:2501.03788v2 Announce Type: replace-cross 
Abstract: In the neighborhood growth dynamics on a Hamming rectangle $[0,m-1]\times[0,n-1]\subseteq \mathbb{Z}_+^2$, the decision to add a point is made by counting the currently occupied points on the horizontal and the vertical line through it, and checking whether the pair of counts lies outside a fixed Young diagram. After the initially occupied set is chosen, the synchronous rule is iterated. The Young domination number with a fixed latency $L$ is the smallest cardinality of an initial set that covers the rectangle by $L$ steps, for $L=0,1,\ldots$ We compute this number for some special cases, including $k$-domination for any $k$ when $m=n$, thereby proving a conjecture from 2009 due to Burchett, Lachniet, and Lane, and devise approximation algorithms in the general case. These results have implications in extremal graph theory, via an equivalence between the case $L = 1$ and bipartite Tur\'an numbers for families of double stars. Our approach is based on a variety of techniques including duality between Young diagrams, algebraic formulations, explicit constructions, and dynamic programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03788v2</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Janko Gravner, Matja\v{z} Krnc, Martin Milani\v{c}, Jean-Florent Raymond</dc:creator>
    </item>
    <item>
      <title>Deterministically approximating the volume of a Kostka polytope</title>
      <link>https://arxiv.org/abs/2503.06459</link>
      <description>arXiv:2503.06459v2 Announce Type: replace-cross 
Abstract: Polynomial-time deterministic approximation of volumes of polytopes, up to an approximation factor that grows at most sub-exponentially with the dimension, remains an open problem. Recent work on this question has focused on identifying interesting classes of polytopes for which such approximation algorithms can be obtained. In this paper, we focus on one such class of polytopes: the Kostka polytopes. The volumes of Kostka polytopes appear naturally in questions of random matrix theory, in the context of evaluating the probability density that a random Hermitian matrix with fixed spectrum $\lambda$ has a given diagonal $\mu$ (the so-called randomized Schur-Horn problem): the corresponding Kostka polytope is denoted $\mathrm{GT}(\lambda, \mu)$. We give a polynomial-time deterministic algorithm for approximating the volume of a ($\Omega(n^2)$ dimensional) Kostka polytope $\mathrm{GT}(\lambda, \mu)$ to within a multiplicative factor of $\exp(O(n\log n))$, when $\lambda$ is an integral partition with $n$ parts, with entries bounded above by a polynomial in $n$, and $\mu$ is an integer vector lying in the interior of the permutohedron (i.e., convex hull of all permutations) of $\lambda$. The algorithm thus gives asymptotically correct estimates of the log-volume of Kostka polytopes corresponding to such $(\lambda, \mu)$. Our approach is based on a partition function interpretation of a continuous analogue of Schur polynomials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06459v2</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hariharan Narayanan, Piyush Srivastava</dc:creator>
    </item>
    <item>
      <title>The Power of Recursive Embeddings for $\ell_p$ Metrics</title>
      <link>https://arxiv.org/abs/2503.18508</link>
      <description>arXiv:2503.18508v2 Announce Type: replace-cross 
Abstract: Metric embedding is a powerful tool used extensively in mathematics and computer science. We devise a new method of using metric embeddings recursively, which turns out to be particularly effective in $\ell_p$ spaces, $p&gt;2$, yielding state-of-the-art results for Lipschitz decomposition, for Nearest Neighbor Search, and for embedding into $\ell_2$. In a nutshell, our method composes metric embeddings by viewing them as reductions between problems, and thereby obtains a new reduction that is substantially more effective than the known reduction that employs a single embedding. We in fact apply this method recursively, oftentimes using double recursion, which further amplifies the gap from a single embedding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18508v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <category>math.MG</category>
      <pubDate>Tue, 08 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Krauthgamer, Nir Petruschka, Shay Sapir</dc:creator>
    </item>
  </channel>
</rss>
