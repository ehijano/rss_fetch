<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Jul 2025 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>New algorithms for girth and cycle detection</title>
      <link>https://arxiv.org/abs/2507.02061</link>
      <description>arXiv:2507.02061v1 Announce Type: new 
Abstract: Let $G=(V,E)$ be an unweighted undirected graph with $n$ vertices and $m$ edges. Let $g$ be the girth of $G$, that is, the length of a shortest cycle in $G$. We present a randomized algorithm with a running time of $\tilde{O}\big(\ell \cdot n^{1 + \frac{1}{\ell - \varepsilon}}\big)$ that returns a cycle of length at most $ 2\ell \left\lceil \frac{g}{2} \right\rceil - 2 \left\lfloor \varepsilon \left\lceil \frac{g}{2} \right\rceil \right\rfloor, $ where $\ell \geq 2$ is an integer and $\varepsilon \in [0,1]$, for every graph with $g = polylog(n)$.
  Our algorithm generalizes an algorithm of Kadria \etal{} [SODA'22] that computes a cycle of length at most $4\left\lceil \frac{g}{2} \right\rceil - 2\left\lfloor \varepsilon \left\lceil \frac{g}{2} \right\rceil \right\rfloor $ in $\tilde{O}\big(n^{1 + \frac{1}{2 - \varepsilon}}\big)$ time. Kadria \etal{} presented also an algorithm that finds a cycle of length at most $ 2\ell \left\lceil \frac{g}{2} \right\rceil $ in $\tilde{O}\big(n^{1 + \frac{1}{\ell}}\big)$ time, where $\ell$ must be an integer. Our algorithm generalizes this algorithm, as well, by replacing the integer parameter $\ell$ in the running time exponent with a real-valued parameter $\ell - \varepsilon$, thereby offering greater flexibility in parameter selection and enabling a broader spectrum of combinations between running times and cycle lengths.
  We also show that for sparse graphs a better tradeoff is possible, by presenting an $\tilde{O}(\ell\cdot m^{1+1/(\ell-\varepsilon)})$ time randomized algorithm that returns a cycle of length at most $2\ell(\lfloor \frac{g-1}{2}\rfloor) - 2(\lfloor \varepsilon \lfloor \frac{g-1}{2}\rfloor \rfloor+1)$, where $\ell\geq 3$ is an integer and $\varepsilon\in [0,1)$, for every graph with $g=polylog(n)$.
  To obtain our algorithms we develop several techniques and introduce a formal definition of hybrid cycle detection algorithms. [...]</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02061v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liam Roditty, Plia Trabelsi</dc:creator>
    </item>
    <item>
      <title>A Computational Proof of the Highest-Scoring Boggle Board</title>
      <link>https://arxiv.org/abs/2507.02117</link>
      <description>arXiv:2507.02117v1 Announce Type: new 
Abstract: Finding all the words on a Boggle board is a classic computer programming problem. With a fast Boggle solver, local optimization techniques such as hillclimbing and simulated annealing can be used to find particularly high-scoring boards. The sheer number of possible Boggle boards has historically prevented an exhaustive search for the global optimum board. We apply Branch and Bound and a decision diagram-like data structure to perform the first such search. We find that the highest-scoring boards found via hillclimbing are, in fact, the global optima.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02117v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Vanderkam</dc:creator>
    </item>
    <item>
      <title>On the Adversarial Robustness of Online Importance Sampling</title>
      <link>https://arxiv.org/abs/2507.02394</link>
      <description>arXiv:2507.02394v1 Announce Type: new 
Abstract: This paper studies the adversarial-robustness of importance-sampling (aka sensitivity sampling); a useful algorithmic technique that samples elements with probabilities proportional to some measure of their importance. A streaming or online algorithm is called adversarially-robust if it succeeds with high probability on input streams that may change adaptively depending on previous algorithm outputs. Unfortunately, the dependence between stream elements breaks the analysis of most randomized algorithms, and in particular that of importance-sampling algorithms. Previously, Braverman et al. [NeurIPS 2021] suggested that streaming algorithms based on importance-sampling may be adversarially-robust; however, they proved it only for well-behaved inputs.
  We focus on the adversarial-robustness of online importance-sampling, a natural variant where sampling decisions are irrevocable and made as data arrives. Our main technical result shows that, given as input an adaptive stream of elements $x_1,\ldots,x_T\in \mathbb{R}_+$, online importance-sampling maintains a $(1\pm\epsilon)$-approximation of their sum while matching (up to lower order terms) the storage guarantees of the oblivious (non-adaptive) case. We then apply this result to develop adversarially-robust online algorithms for two fundamental problems: hypergraph cut sparsification and $\ell_p$ subspace embedding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02394v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yotam Kenneth-Mordoch, Shay Sapir</dc:creator>
    </item>
    <item>
      <title>Numerical Linear Algebra in Linear Space</title>
      <link>https://arxiv.org/abs/2507.02433</link>
      <description>arXiv:2507.02433v1 Announce Type: new 
Abstract: We present a randomized linear-space solver for general linear systems $\mathbf{A} \mathbf{x} = \mathbf{b}$ with $\mathbf{A} \in \mathbb{Z}^{n \times n}$ and $\mathbf{b} \in \mathbb{Z}^n$, without any assumption on the condition number of $\mathbf{A}$. For matrices whose entries are bounded by $\mathrm{poly}(n)$, the solver returns a $(1+\epsilon)$-multiplicative entry-wise approximation to vector $\mathbf{x} \in \mathbb{Q}^{n}$ using $\widetilde{O}(n^2 \cdot \mathrm{nnz}(\mathbf{A}))$ bit operations and $O(n \log n)$ bits of working space (i.e., linear in the size of a vector), where $\mathrm{nnz}$ denotes the number of nonzero entries. Our solver works for right-hand vector $\mathbf{b}$ with entries up to $n^{O(n)}$. To our knowledge, this is the first linear-space linear system solver over the rationals that runs in $\widetilde{O}(n^2 \cdot \mathrm{nnz}(\mathbf{A}))$ time.
  We also present several applications of our solver to numerical linear algebra problems, for which we provide algorithms with efficient polynomial running time and near-linear space. In particular, we present results for linear regression, linear programming, eigenvalues and eigenvectors, and singular value decomposition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02433v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiping Liu, Hoai-An Nguyen, Junzhao Yang</dc:creator>
    </item>
    <item>
      <title>Bounded Weighted Edit Distance: Dynamic Algorithms and Matching Lower Bounds</title>
      <link>https://arxiv.org/abs/2507.02548</link>
      <description>arXiv:2507.02548v1 Announce Type: new 
Abstract: The edit distance $ed(X,Y)$ of two strings $X,Y\in \Sigma^*$ is the minimum number of character edits (insertions, deletions, and substitutions) needed to transform $X$ into $Y$. Its weighted counterpart $ed^w(X,Y)$ minimizes the total cost of edits, which are specified using a function $w$, normalized so that each edit costs at least one. The textbook dynamic-programming procedure, given strings $X,Y\in \Sigma^{\le n}$ and oracle access to $w$, computes $ed^w(X,Y)$ in $O(n^2)$ time. Nevertheless, one can achieve better running times if the computed distance, denoted $k$, is small: $O(n+k^2)$ for unit weights [Landau and Vishkin; JCSS'88] and $\tilde{O}(n+\sqrt{nk^3})$ for arbitrary weights [Cassis, Kociumaka, Wellnitz; FOCS'23].
  In this paper, we study the dynamic version of the weighted edit distance problem, where the goal is to maintain $ed^w(X,Y)$ for strings $X,Y\in \Sigma^{\le n}$ that change over time, with each update specified as an edit in $X$ or $Y$. Very recently, Gorbachev and Kociumaka [STOC'25] showed that the unweighted distance $ed(X,Y)$ can be maintained in $\tilde{O}(k)$ time per update after $\tilde{O}(n+k^2)$-time preprocessing; here, $k$ denotes the current value of $ed(X,Y)$. Their algorithm generalizes to small integer weights, but the underlying approach is incompatible with large weights.
  Our main result is a dynamic algorithm that maintains $ed^w(X,Y)$ in $\tilde{O}(k^{3-\gamma})$ time per update after $\tilde{O}(nk^\gamma)$-time preprocessing. Here, $\gamma\in [0,1]$ is a real trade-off parameter and $k\ge 1$ is an integer threshold fixed at preprocessing time, with $\infty$ returned whenever $ed^w(X,Y)&gt;k$. We complement our algorithm with conditional lower bounds showing fine-grained optimality of our trade-off for $\gamma \in [0.5,1)$ and justifying our choice to fix $k$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02548v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Itai Boneh, Egor Gorbachev, Tomasz Kociumaka</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Knapsack under Explorable Uncertainty: Hardness and Algorithms</title>
      <link>https://arxiv.org/abs/2507.02657</link>
      <description>arXiv:2507.02657v1 Announce Type: new 
Abstract: In the knapsack problem under explorable uncertainty, we are given a knapsack instance with uncertain item profits. Instead of having access to the precise profits, we are only given uncertainty intervals that are guaranteed to contain the corresponding profits. The actual item profit can be obtained via a query. The goal of the problem is to adaptively query item profits until the revealed information suffices to compute an optimal (or approximate) solution to the underlying knapsack instance. Since queries are costly, the objective is to minimize the number of queries.
  In the offline variant of this problem, we assume knowledge of the precise profits and the task is to compute a query set of minimum cardinality that a third party without access to the profits could use to identify an optimal (or approximate) knapsack solution. We show that this offline variant is complete for the second-level of the polynomial hierarchy, i.e., $\Sigma_2^p$-complete, and cannot be approximated within a non-trivial factor unless $\Sigma_2^p = \Delta_2^p$. Motivated by these strong hardness results, we consider a resource-augmented variant of the problem where the requirements on the query set computed by an algorithm are less strict than the requirements on the optimal solution we compare against. More precisely, a query set computed by the algorithm must reveal sufficient information to identify an approximate knapsack solution, while the optimal query set we compare against has to reveal sufficient information to identify an optimal solution. We show that this resource-augmented setting allows interesting non-trivial algorithmic results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02657v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jens Schl\"oter</dc:creator>
    </item>
    <item>
      <title>Faster Algorithm for Bounded Tree Edit Distance in the Low-Distance Regime</title>
      <link>https://arxiv.org/abs/2507.02701</link>
      <description>arXiv:2507.02701v1 Announce Type: new 
Abstract: The tree edit distance is a natural dissimilarity measure between rooted ordered trees whose nodes are labeled over an alphabet $\Sigma$. It is defined as the minimum number of node edits (insertions, deletions, and relabelings) required to transform one tree into the other. In the weighted variant, the edits have associated costs (depending on the involved node labels) normalized so that each cost is at least one, and the goal is to minimize the total cost of edits.
  The unweighted tree edit distance between two trees of total size $n$ can be computed in $O(n^{2.6857})$ time; in contrast, determining the weighted tree edit distance is fine-grained equivalent to the All-Pairs Shortest Paths problem and requires $n^3/2^{\Omega(\sqrt{\log n})}$ time [Nogler et al.; STOC'25]. These super-quadratic running times are unattractive for large but very similar trees, which motivates the bounded version of the problem, where the runtime is parameterized by the computed distance $k$, potentially yielding faster algorithms for $k\ll n$.
  Previous best algorithms for the bounded unweighted setting run in $O(nk^2\log n)$ time [Akmal &amp; Jin; ICALP'21] and $O(n + k^7\log k)$ time [Das et al.; STOC'23]. For the weighted variant, the only known running time has been $O(n + k^{15})$.
  We present an $O(n + k^6\log k)$-time algorithm for computing the bounded tree edit distance in both the weighted and unweighted settings. Our approach begins with an alternative $O(nk^2\log n)$-time algorithm that handles weights and is significantly easier to analyze than the existing counterpart. We then introduce a novel optimization that leverages periodic structures within the input trees. To utilize it, we modify the $O(k^5)$-size $O(n)$-time universal kernel, the central component of the prior $O(n + k^{O(1)})$-time algorithms, so that it produces instances containing these periodic structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02701v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomasz Kociumaka, Ali Shahali</dc:creator>
    </item>
    <item>
      <title>Indexing Tries within Entropy-Bounded Space</title>
      <link>https://arxiv.org/abs/2507.02728</link>
      <description>arXiv:2507.02728v1 Announce Type: new 
Abstract: We study the problem of indexing and compressing tries using a BWT-based approach. Specifically, we consider a succinct and compressed representation of the XBWT of Ferragina et al.\ [FOCS '05, JACM '09] corresponding to the analogous of the FM-index [FOCS '00, JACM '05] for tries. This representation allows to efficiently count the number of nodes reached by a given string pattern. To analyze the space complexity of the above trie index, we propose a proof for the combinatorial problem of counting the number of tries with a given symbol distribution. We use this formula to define a worst-case entropy measure for tries, as well as a notion of k-th order empirical entropy. In particular, we show that the relationships between these two entropy measures are similar to those between the corresponding well-known measures for strings. We use these measures to prove that the XBWT of a trie can be encoded within a space bounded by our k-th order empirical entropy plus a o(n) term, with n being the number of nodes in the trie. Notably, as happens for strings, this space bound can be reached for every sufficiently small k simultaneously. Finally, we compare the space complexity of the above index with that of the r-index for tries proposed by Prezza [SODA '21] and we prove that in some cases the FM-index for tries is asymptotically smaller.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02728v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Carfagna, Carlo Tosoni</dc:creator>
    </item>
    <item>
      <title>Connected k-Median with Disjoint and Non-disjoint Clusters</title>
      <link>https://arxiv.org/abs/2507.02774</link>
      <description>arXiv:2507.02774v1 Announce Type: new 
Abstract: The connected $k$-median problem is a constrained clustering problem that combines distance-based $k$-clustering with connectivity information. The problem allows to input a metric space and an unweighted undirected connectivity graph that is completely unrelated to the metric space. The goal is to compute $k$ centers and corresponding clusters such that each cluster forms a connected subgraph of $G$, and such that the $k$-median cost is minimized.
  The problem has applications in very different fields like geodesy (particularly districting), social network analysis (especially community detection), or bioinformatics. We study a version with overlapping clusters where points can be part of multiple clusters which is natural for the use case of community detection. This problem variant is $\Omega(\log n)$-hard to approximate, and our main result is an $\mathcal{O}(k^2 \log n)$-approximation algorithm for the problem. We complement it with an $\Omega(n^{1-\epsilon})$-hardness result for the case of disjoint clusters without overlap with general connectivity graphs, as well as an exact algorithm in this setting if the connectivity graph is a tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02774v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan Eube, Kelin Luo, Dorian Reineccius, Heiko R\"oglin, Melanie Schmidt</dc:creator>
    </item>
    <item>
      <title>On the Structure of Replicable Hypothesis Testers</title>
      <link>https://arxiv.org/abs/2507.02842</link>
      <description>arXiv:2507.02842v1 Announce Type: new 
Abstract: A hypothesis testing algorithm is replicable if, when run on two different samples from the same distribution, it produces the same output with high probability. This notion, defined by by Impagliazzo, Lei, Pitassi, and Sorell [STOC'22], can increase trust in testing procedures and is deeply related to algorithmic stability, generalization, and privacy. We build general tools to prove lower and upper bounds on the sample complexity of replicable testers, unifying and quantitatively improving upon existing results.
  We identify a set of canonical properties, and prove that any replicable testing algorithm can be modified to satisfy these properties without worsening accuracy or sample complexity. A canonical replicable algorithm computes a deterministic function of its input (i.e., a test statistic) and thresholds against a uniformly random value in $[0,1]$. It is invariant to the order in which the samples are received, and, if the testing problem is ``symmetric,'' then the algorithm is also invariant to the labeling of the domain elements, resolving an open question by Liu and Ye [NeurIPS'24]. We prove new lower bounds for uniformity, identity, and closeness testing by reducing to the case where the replicable algorithm satisfies these canonical properties.
  We systematize and improve upon a common strategy for replicable algorithm design based on test statistics with known expectation and bounded variance. Our framework allow testers which have been extensively analyzed in the non-replicable setting to be made replicable with minimal overhead. As direct applications of our framework, we obtain constant-factor optimal bounds for coin testing and closeness testing and get replicability for free in a large parameter regime for uniformity testing.
  We also give state-of-the-art bounds for replicable Gaussian mean testing, and, unlike prior work, our algorithm runs in polynomial time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02842v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anders Aamand, Maryam Aliakbarpour, Justin Y. Chen, Shyam Narayanan, Sandeep Silwal</dc:creator>
    </item>
    <item>
      <title>Online Conformal Prediction with Efficiency Guarantees</title>
      <link>https://arxiv.org/abs/2507.02496</link>
      <description>arXiv:2507.02496v1 Announce Type: cross 
Abstract: We study the problem of conformal prediction in a novel online framework that directly optimizes efficiency. In our problem, we are given a target miscoverage rate $\alpha &gt; 0$, and a time horizon $T$. On each day $t \le T$ an algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in [0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is, $y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while maintaining efficiency, that is, minimizing the average volume (length) of the intervals played. This problem is an online analogue to the problem of constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input sequences. For exchangeable sequences, we show that it is possible to construct intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length upper bounded by the best fixed interval that achieves coverage in hindsight. For arbitrary sequences however, we show that any algorithm that achieves a $\mu$-approximation in average length compared to the best fixed interval achieving coverage in hindsight, must make a multiplicative factor more mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and the aspect ratio of the problem. Our main algorithmic result is a matching algorithm that can recover all Pareto-optimal settings of $\mu$ and number of mistakes. Furthermore, our algorithm is deterministic and therefore robust to an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to the classical online learning problem. In fact, we show that no single algorithm can simultaneously be Pareto-optimal for arbitrary sequences and optimal for exchangeable sequences. On the algorithmic side, we give an algorithm that achieves the near-optimal tradeoff between the two cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02496v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vaidehi Srinivas</dc:creator>
    </item>
    <item>
      <title>An Easy Proof of a Weak Version of Chernoff inequality</title>
      <link>https://arxiv.org/abs/2507.02759</link>
      <description>arXiv:2507.02759v1 Announce Type: cross 
Abstract: We prove an easy but very weak version of Chernoff inequality. Namely, that the probability that in $6M$ throws of a fair coin, one gets at most $M$ heads is $\leq 1/2^M$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02759v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sariel Har-Peled</dc:creator>
    </item>
    <item>
      <title>On the Parameterized Complexity of Eulerian Strong Component Arc Deletion</title>
      <link>https://arxiv.org/abs/2408.13819</link>
      <description>arXiv:2408.13819v2 Announce Type: replace 
Abstract: In this paper, we study the Eulerian Strong Component Arc Deletion problem, where the input is a directed multigraph and the goal is to delete the minimum number of arcs to ensure every strongly connected component of the resulting digraph is Eulerian. This problem is a natural extension of the Directed Feedback Arc Set problem and is also known to be motivated by certain scenarios arising in the study of housing markets. The complexity of the problem, when parameterized by solution size (i.e., size of the deletion set), has remained unresolved and has been highlighted in several papers. In this work, we answer this question by ruling out (subject to the usual complexity assumptions) a fixed-parameter tractable (FPT) algorithm for this parameter and conduct a broad analysis of the problem with respect to other natural parameterizations. We prove both positive and negative results. Among these, we demonstrate that the problem is also hard (W[1]-hard or even para-NP-hard) when parameterized by either treewidth or maximum degree alone. Complementing our lower bounds, we establish that the problem is in XP when parameterized by treewidth and FPT when parameterized either by both treewidth and maximum degree or by both treewidth and solution size. We show that these algorithms have near-optimal asymptotic dependence on the treewidth assuming the Exponential Time Hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13819v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V\'aclav Bla\v{z}ej, Satyabrata Jana, M. S. Ramanujan, Peter Strulo</dc:creator>
    </item>
    <item>
      <title>ARRIVAL: Recursive Framework &amp; $\ell_1$-Contraction</title>
      <link>https://arxiv.org/abs/2502.06477</link>
      <description>arXiv:2502.06477v2 Announce Type: replace 
Abstract: ARRIVAL is the problem of deciding which out of two possible destinations will be reached first by a token that moves deterministically along the edges of a directed graph, according to so-called switching rules. It is known to lie in NP $\cap$ CoNP, but not known to lie in P. The state-of-the-art algorithm due to G\"artner et al. (ICALP `21) runs in time $2^{O(\sqrt{n} \log n)}$ on an $n$-vertex graph.
  We prove that ARRIVAL can be solved in time $2^{O(k \log^2 n)}$ on $n$-vertex graphs of treewidth $k$. Our algorithm is derived by adapting a simple recursive algorithm for a generalization of ARRIVAL called G-ARRIVAL. This simple recursive algorithm acts as a framework from which we can also rederive the subexponential upper bound of G\"artner et al.
  Our second result is a reduction from G-ARRIVAL to the problem of finding an approximate fixed point of an $\ell_1$-contracting function $f : [0, 1]^n \rightarrow [0, 1]^n$. Finding such fixed points is a well-studied problem in the case of the $\ell_2$-metric and the $\ell_\infty$-metric, but little is known about the $\ell_1$-case.
  Both of our results highlight parallels between ARRIVAL and the Simple Stochastic Games (SSG) problem. Concretely, Chatterjee et al. (SODA `23) gave an algorithm for SSG parameterized by treewidth that achieves a similar bound as we do for ARRIVAL, and SSG is known to reduce to $\ell_\infty$-contraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06477v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Haslebacher</dc:creator>
    </item>
    <item>
      <title>On Finding $\ell$-th Smallest Perfect Matchings</title>
      <link>https://arxiv.org/abs/2506.22619</link>
      <description>arXiv:2506.22619v2 Announce Type: replace 
Abstract: Given an undirected weighted graph $G$ and an integer $k$, Exact-Weight Perfect Matching (EWPM) is the problem of finding a perfect matching of weight exactly $k$ in $G$. In this paper, we study EWPM and its variants. The EWPM problem is famous, since in the case of unary encoded weights, Mulmuley, Vazirani, and Vazirani showed almost 40 years ago that the problem can be solved in randomized polynomial time. However, up to this date no derandomization is known.
  Our first result is a simple deterministic algorithm for EWPM that runs in time $n^{O(\ell)}$, where $\ell$ is the number of distinct weights that perfect matchings in $G$ can take. In fact, we show how to find an $\ell$-th smallest perfect matching in any weighted graph (even if the weights are encoded in binary, in which case EWPM in general is known to be NP-complete) in time $n^{O(\ell)}$ for any integer $\ell$. Similar next-to-optimal variants have also been studied recently for the shortest path problem.
  For our second result, we extend the list of problems that are known to be equivalent to EWPM. We show that EWPM is equivalent under a weight-preserving reduction to the Exact Cycle Sum problem (ECS) in undirected graphs with a conservative (i.e. no negative cycles) weight function. To the best of our knowledge, we are the first to study this problem. As a consequence, the latter problem is contained in RP if the weights are encoded in unary. Finally, we identify a special case of EWPM, called BCPM, which was recently studied by El Maalouly, Steiner and Wulf. We show that BCPM is equivalent under a weight-preserving transformation to another problem recently studied by Schlotter and Seb\H{o} as well as Geelen and Kapadia: the Shortest Odd Cycle problem (SOC) in undirected graphs with conservative weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22619v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas El Maalouly, Sebastian Haslebacher, Adrian Taubner, Lasse Wulf</dc:creator>
    </item>
    <item>
      <title>Hardness of sampling solutions from the Symmetric Binary Perceptron</title>
      <link>https://arxiv.org/abs/2407.16627</link>
      <description>arXiv:2407.16627v2 Announce Type: replace-cross 
Abstract: We show that two related classes of algorithms, stable algorithms and Boolean circuits with bounded depth, cannot produce an approximate sample from the uniform measure over the set of solutions to the symmetric binary perceptron model at any constraint-to-variable density. This result is in contrast to the question of finding \emph{a} solution to the same problem, where efficient (and stable) algorithms are known to succeed at sufficiently low density. This result suggests that the solutions found efficiently -- whenever this task is possible -- must be highly atypical, and therefore provides an example of a problem where search is efficiently possible but approximate sampling from the set of solutions is not, at least within these two classes of algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16627v2</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Random Structures &amp; Algorithms, 2025</arxiv:journal_reference>
      <dc:creator>Ahmed El Alaoui, David Gamarnik</dc:creator>
    </item>
    <item>
      <title>An average case efficient algorithm for solving two-variable linear Diophantine equations</title>
      <link>https://arxiv.org/abs/2409.14052</link>
      <description>arXiv:2409.14052v5 Announce Type: replace-cross 
Abstract: Solving two-variable linear Diophantine equations has applications in many cryptographic protocols such as RSA and Elliptic curve cryptography. The Extended Euclid's algorithm is the most widely used algorithm to solve these equations. We revisit two algorithms to solve two-variable linear Diophantine equations. We write the iterative version of one of the revisited algorithms. For another, we do a fine-grained analysis of the number of recursive calls and arrive at a periodic function that represents the number of recursive calls. We find the period and use it to derive an accurate closed-form expression for the average number of recursive calls incurred by that algorithm. We find multiple loose upper bounds on the average number of recursive calls in different cases based on whether a solution exists or not. We find that for a fixed value of $a,b$ and a varying $c$, such that the equation $ax+by=c$ (where $a &gt; b$) is solvable, we can find the solution in $O\left(\frac{\log b}{gcd(a,b)}\right)$ average number of recursions or steps. We computationally evaluate this bound as well as one more upper bound and compare them with the average number of recursive calls in Extended Euclid's algorithm on a number of random $512$-bit inputs. We observe that the average number of iterations in the analyzed algorithm decreases with an increase in $gcd(a,b)$. We propose an iterative version of the algorithm. We implement this algorithm and find that the average number of iterations by our algorithm is less than that of two existing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14052v5</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>math.NT</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mayank Deora, Pinakpani Pal</dc:creator>
    </item>
    <item>
      <title>On Characterizations for Language Generation: Interplay of Hallucinations, Breadth, and Stability</title>
      <link>https://arxiv.org/abs/2412.18530</link>
      <description>arXiv:2412.18530v2 Announce Type: replace-cross 
Abstract: We study language generation in the limit - introduced by Kleinberg and Mullainathan [KM24] - building on classical works of Gold [Gol67] and Angluin [Ang79]. [KM24]'s main result is an algorithm for generating from any countable language collection in the limit. While their algorithm eventually generates unseen strings from the target language $K$, it sacrifices coverage or breadth, i.e., its ability to generate a rich set of strings. Recent work introduces different notions of breadth and explores when generation with breadth is possible, leaving a full characterization of these notions open. Our first set of results settles this by characterizing generation for existing notions of breadth and their natural extensions. Interestingly, our lower bounds are very flexible and hold for many performance metrics beyond breadth - for instance, showing that, in general, it is impossible to train generators which achieve a higher perplexity or lower hallucination rate for $K$ compared to other languages. Next, we study language generation with breadth and stable generators - algorithms that eventually stop changing after seeing an arbitrary but finite number of strings - and prove unconditional lower bounds for such generators, strengthening the results of [KMV25] and demonstrating that generation with many existing notions of breadth becomes equally hard, when stability is required. This gives a separation for generation with approximate breadth, between stable and unstable generators, highlighting the rich interplay between breadth, stability, and consistency in language generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18530v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alkis Kalavasis, Anay Mehrotra, Grigoris Velegkas</dc:creator>
    </item>
  </channel>
</rss>
