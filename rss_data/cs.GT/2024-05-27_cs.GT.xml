<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 May 2024 04:00:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Rationalizability, Iterated Dominance, and the Theorems of Radon and Carath\'eodory</title>
      <link>https://arxiv.org/abs/2405.16050</link>
      <description>arXiv:2405.16050v1 Announce Type: new 
Abstract: The game theoretic concepts of rationalizability and iterated dominance are closely related and provide characterizations of each other. Indeed, the equivalence between them implies that in a two player finite game, the remaining set of actions available to players after iterated elimination of strictly dominated strategies coincides with the rationalizable actions. I prove a dimensionality result following from these ideas. I show that for two player games, the number of actions available to the opposing player provides a (tight) upper bound on how a player's pure strategies may be strictly dominated by mixed strategies. I provide two different frameworks and interpretations of dominance to prove this result, and in doing so relate it to Radon's Theorem and Carath\'eodory's Theorem from convex geometry. These approaches may be seen as following from point-line duality. A new proof of the classical equivalence between these solution concepts is also given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16050v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roy Long</dc:creator>
    </item>
    <item>
      <title>Mechanism Design for LLM Fine-tuning with Multiple Reward Models</title>
      <link>https://arxiv.org/abs/2405.16276</link>
      <description>arXiv:2405.16276v1 Announce Type: new 
Abstract: Recent research on fine-tuning large language models (LLMs) through the aggregation of multiple preferences has attracted considerable attention. However, the existing literature predominantly focuses on the empirical performance of aggregation algorithms, while neglecting the underlying motivation for agents to misreport their preferences. In this paper, we formalize this as a multi-parameter mechanism design problem, where an LLM provider designs both training and payment rules to achieve specific objectives and promote the truthful reporting of preferences. Firstly, we claim the necessity of a payment scheme by demonstrating that without payments, truth-telling is a strictly dominated strategy under a wide range of training rules. Then, we introduce the affine maximizer payment scheme for the social welfare maximizing training rules that are widely used in practice, which ensures both dominant-strategy incentive compatibility (DSIC) and individual rationality (IR). Furthermore, we prove that under mild conditions, any other payment rule that also implements these training rules in DSIC can be converted to the affine maximizer payment by adding a factor irrelevant to the agents' own reports. We also show that this mechanism satisfies approximate DSIC when the input of the mechanism is a biased version of the reported preferences, showcasing its robustness in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16276v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Sun, Yurong Chen, Siwei Wang, Wei Chen, Xiaotie Deng</dc:creator>
    </item>
    <item>
      <title>DePIN: A Framework for Token-Incentivized Participatory Sensing</title>
      <link>https://arxiv.org/abs/2405.16495</link>
      <description>arXiv:2405.16495v1 Announce Type: new 
Abstract: There is always demand for integrating data into microeconomic decision making. Participatory sensing deals with how real-world data may be extracted with stakeholder participation and resolves a problem of Big Data, which is concerned with monetizing data extracted from individuals without their participation. We present how Decentralized Physical Infrastructure Networks (DePINs) extend participatory sensing. We discuss the threat models of these networks and how DePIN cryptoeconomics can advance participatory sensing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16495v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael T. C. Chiu, Sachit Mahajan, Mark C. Ballandies, Uro\v{s} V. Kalabi\'c</dc:creator>
    </item>
    <item>
      <title>Adaptive Incentive Design with Learning Agents</title>
      <link>https://arxiv.org/abs/2405.16716</link>
      <description>arXiv:2405.16716v1 Announce Type: new 
Abstract: How can the system operator learn an incentive mechanism that achieves social optimality based on limited information about the agents' behavior, who are dynamically updating their strategies? To answer this question, we propose an \emph{adaptive} incentive mechanism. This mechanism updates the incentives of agents based on the feedback of each agent's externality, evaluated as the difference between the player's marginal cost and society's marginal cost at each time step. The proposed mechanism updates the incentives on a slower timescale compared to the agents' learning dynamics, resulting in a two-timescale coupled dynamical system. Notably, this mechanism is agnostic to the specific learning dynamics used by agents to update their strategies. We show that any fixed point of this adaptive incentive mechanism corresponds to the optimal incentive mechanism, ensuring that the Nash equilibrium coincides with the socially optimal strategy. Additionally, we provide sufficient conditions that guarantee the convergence of the adaptive incentive mechanism to a fixed point. Our results apply to both atomic and non-atomic games. To demonstrate the effectiveness of our proposed mechanism, we verify the convergence conditions in two practically relevant games: atomic networked quadratic aggregative games and non-atomic network routing games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16716v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chinmay Maheshwari, Kshitij Kulkarni, Manxi Wu, Shankar Sastry</dc:creator>
    </item>
    <item>
      <title>Limited-perception games</title>
      <link>https://arxiv.org/abs/2405.16735</link>
      <description>arXiv:2405.16735v1 Announce Type: new 
Abstract: We study rational agents with different perception capabilities in strategic games. We focus on a class of one-shot limited-perception games. These games extend simultaneous-move normal-form games by presenting each player with an individualized perception of all players' payoff functions. The accuracy of a player's perception is determined by the player's capability level. Capability levels are countable and totally ordered, with a higher level corresponding to a more accurate perception. We study the rational behavior of players in these games and formalize relevant equilibria conditions. In contrast to equilibria in conventional bimatrix games, which can be represented by a pair of mixed strategies, in our limited perception games a higher-order response function captures how the lower-capability player uses their (less accurate) perception of the payoff function to reason about the (more accurate) possible perceptions of the higher-capability opponent. This response function characterizes, for each possible perception of the higher-capability player (from the perspective of the lower-capability player), the best response of the higher capability player for that perception. Since the domain of the response function can be exponentially large or even infinite, finding one equilibrium may be computationally intractable or even undecidable. Nevertheless, we show that for any $\epsilon$, there exists an $\epsilon$-equilibrium with a compact, tractable representation whose size is independent of the size of the response function's domain. We further identify classes of zero-sum limited-perception games in which finding an equilibrium becomes a (typically tractable) nonsmooth convex optimization problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16735v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kai Jia, Martin Rinard</dc:creator>
    </item>
    <item>
      <title>Instability and Efficiency of Non-cooperative Games</title>
      <link>https://arxiv.org/abs/2405.17196</link>
      <description>arXiv:2405.17196v1 Announce Type: new 
Abstract: It is well known that a non-cooperative game may have multiple equilibria. In this paper we consider the efficiency of games, measured by the ratio between the aggregate payoff over all Nash equilibria and that over all admissible controls. Such efficiency operator is typically unstable with respect to small perturbation of the game. This seemingly bad property can actually be a good news in practice: it is possible that a small change of the game mechanism may improve the efficiency of the game dramatically. We shall introduce a game mediator with limited resources and investigate the mechanism designs aiming to improve the efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17196v1</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianfeng Zhang</dc:creator>
    </item>
    <item>
      <title>Serial Monopoly on Blockchains with Quasi-patient Users</title>
      <link>https://arxiv.org/abs/2405.17334</link>
      <description>arXiv:2405.17334v1 Announce Type: new 
Abstract: This paper introduces and investigates an extension of the price dynamics in serial monopoly blockchain described in Nisan [Nis23], tailored to accommodate quasi-patient users. Our model reflects users' diminishing interest in having their transactions added to the ledger over time, resulting in only a fraction $\delta$ of the current demand persisting in the subsequent round. The framework presented by Lavi et al. [LSZ22], where users are impatient and derive utility only from immediate transaction inclusion in the next block, corresponds to $\delta=0$. Fully patient users who wait forever as in [Nis23], correspond to $\delta=1$ in our model. This work provides new bounds on the price dynamics for the more interesting case $\delta\in(0,1)$, showing somewhat unexpected effects on the dynamics itself. While the dynamics for the fully patient case is essentially "oblivious" of the structure of the daily demand curve, this is no longer true for finite $\delta &lt; 1$. Moreover, the dynamics undergoes a "transition phase" where for some $\delta$ it behaves as in the fully patient setting ($\delta=1$), and for some smaller values $\delta'&lt;\delta$ it stops "oscillating" and stays at the highest ("monopolist") price. We provide quantitative bounds and analytical results that apply to different demand functions showing that the bounds for $\delta=1$ are not tight in general, for $\delta&lt;1$. These provide guarantees on the minimum ("admission") price such that transaction willing to pay that price are eventually included (and those who do not want are never included).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17334v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paolo Penna, Manvir Schneider</dc:creator>
    </item>
    <item>
      <title>STRIDE: A Tool-Assisted LLM Agent Framework for Strategic and Interactive Decision-Making</title>
      <link>https://arxiv.org/abs/2405.16376</link>
      <description>arXiv:2405.16376v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) like GPT-4 have revolutionized natural language processing, showing remarkable linguistic proficiency and reasoning capabilities. However, their application in strategic multi-agent decision-making environments is hampered by significant limitations including poor mathematical reasoning, difficulty in following instructions, and a tendency to generate incorrect information. These deficiencies hinder their performance in strategic and interactive tasks that demand adherence to nuanced game rules, long-term planning, exploration in unknown environments, and anticipation of opponents' moves. To overcome these obstacles, this paper presents a novel LLM agent framework equipped with memory and specialized tools to enhance their strategic decision-making capabilities. We deploy the tools in a number of economically important environments, in particular bilateral bargaining and multi-agent and dynamic mechanism design. We employ quantitative metrics to assess the framework's performance in various strategic decision-making problems. Our findings establish that our enhanced framework significantly improves the strategic decision-making capability of LLMs. While we highlight the inherent limitations of current LLM models, we demonstrate the improvements through targeted enhancements, suggesting a promising direction for future developments in LLM applications for interactive environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16376v1</guid>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuanhao Li, Runhan Yang, Tiankai Li, Milad Bafarassat, Kourosh Sharifi, Dirk Bergemann, Zhuoran Yang</dc:creator>
    </item>
    <item>
      <title>Attaining Human`s Desirable Outcomes in Human-AI Interaction via Structural Causal Games</title>
      <link>https://arxiv.org/abs/2405.16588</link>
      <description>arXiv:2405.16588v1 Announce Type: cross 
Abstract: In human-AI interaction, a prominent goal is to attain human`s desirable outcome with the assistance of AI agents, which can be ideally delineated as a problem of seeking the optimal Nash Equilibrium that matches the human`s desirable outcome. However, reaching the outcome is usually challenging due to the existence of multiple Nash Equilibria that are related to the assisting task but do not correspond to the human`s desirable outcome. To tackle this issue, we employ a theoretical framework called structural causal game (SCG) to formalize the human-AI interactive process. Furthermore, we introduce a strategy referred to as pre-policy intervention on the SCG to steer AI agents towards attaining the human`s desirable outcome. In more detail, a pre-policy is learned as a generalized intervention to guide the agents` policy selection, under a transparent and interpretable procedure determined by the SCG. To make the framework practical, we propose a reinforcement learning-like algorithm to search out this pre-policy. The proposed algorithm is tested in both gridworld environments and realistic dialogue scenarios with large language models, demonstrating its adaptability in a broader class of problems and potential effectiveness in real-world situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16588v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjie Liu, Jianhong Wang, Haoxuan Li, Xu Chen, Jun Wang, Samuel Kaski, Mengyue Yang</dc:creator>
    </item>
    <item>
      <title>An Evolutionary Framework for Connect-4 as Test-Bed for Comparison of Advanced Minimax, Q-Learning and MCTS</title>
      <link>https://arxiv.org/abs/2405.16595</link>
      <description>arXiv:2405.16595v1 Announce Type: cross 
Abstract: A major challenge in decision making domains with large state spaces is to effectively select actions which maximize utility. In recent years, approaches such as reinforcement learning (RL) and search algorithms have been successful to tackle this issue, despite their differences. RL defines a learning framework that an agent explores and interacts with. Search algorithms provide a formalism to search for a solution. However, it is often difficult to evaluate the performances of such approaches in a practical way. Motivated by this problem, we focus on one game domain, i.e., Connect-4, and develop a novel evolutionary framework to evaluate three classes of algorithms: RL, Minimax and Monte Carlo tree search (MCTS). The contribution of this paper is threefold: i) we implement advanced versions of these algorithms and provide a systematic comparison with their standard counterpart, ii) we develop a novel evaluation framework, which we call the Evolutionary Tournament, and iii) we conduct an extensive evaluation of the relative performance of each algorithm to compare our findings. We evaluate different metrics and show that MCTS achieves the best results in terms of win percentage, whereas Minimax and Q-Learning are ranked in second and third place, respectively, although the latter is shown to be the fastest to make a decision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16595v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.NE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henry Taylor, Leonardo Stella</dc:creator>
    </item>
    <item>
      <title>TopoLa: a novel embedding framework for understanding complex networks</title>
      <link>https://arxiv.org/abs/2405.16928</link>
      <description>arXiv:2405.16928v1 Announce Type: cross 
Abstract: Complex networks, which are the abstractions of many real-world systems, present a persistent challenge across disciplines for people to decipher their underlying information. Recently, hyperbolic geometry of latent spaces has gained traction in network analysis, due to its ability to preserve certain local intrinsic properties of the nodes. In this study, we explore the problem from a much broader perspective: understanding the impact of nodes' global topological structures on latent space placements. Our investigations reveal a direct correlation between the topological structure of nodes and their positioning within the latent space. Building on this deep and strong connection between node distance and network topology, we propose a novel embedding framework called Topology-encoded Latent Hyperbolic Geometry (TopoLa) for analyzing complex networks. With the encoded topological information in the latent space, TopoLa is capable of enhancing both conventional and low-rank networks, using the singular value gap to clarify the mathematical principles behind this enhancement. Meanwhile, we show that the equipped TopoLa distance can also help augment pivotal deep learning models encompassing knowledge distillation and contrastive learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16928v1</guid>
      <category>cs.SI</category>
      <category>cs.GT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Zheng, Qilong Feng, Yaohang Li, Qichang Zhao, Jinhui Xu, Jianxin Wang</dc:creator>
    </item>
    <item>
      <title>Solving Woeginger's Hiking Problem: Wonderful Partitions in Anonymous Hedonic Games</title>
      <link>https://arxiv.org/abs/2311.02067</link>
      <description>arXiv:2311.02067v2 Announce Type: replace 
Abstract: A decade ago, Gerhard Woeginger posed an open problem that became well-known as "Woeginger's Hiking Problem": Consider a group of $n$ people that want to go hiking; everyone expresses preferences over the size of their hiking group in the form of an interval between $1$ and $n$. Is it possible to efficiently assign the $n$ people to a set of hiking subgroups so that every person approves the size of their assigned subgroup? The problem is also known as efficiently deciding if an instance of an anonymous Hedonic Game with interval approval preferences admits a wonderful partition.
  We resolve the open problem in the affirmative by presenting an $O(n^5)$ time algorithm for Woeginger's Hiking Problem. Our solution is based on employing a dynamic programming approach for a specific rectangle stabbing problem from computational geometry. Moreover, we propose natural, more demanding extensions of the problem, e.g., maximizing the number of satisfied participants and variants with single-peaked preferences, and show that they are also efficiently solvable. Last but not least, we employ our solution to efficiently compute a partition that maximizes the egalitarian welfare for anonymous single-peaked Hedonic Games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02067v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andrei Constantinescu, Pascal Lenzner, Rebecca Reiffenh\"auser, Daniel Schmand, Giovanna Varricchio</dc:creator>
    </item>
    <item>
      <title>Policy Space Response Oracles: A Survey</title>
      <link>https://arxiv.org/abs/2403.02227</link>
      <description>arXiv:2403.02227v2 Announce Type: replace 
Abstract: Game theory provides a mathematical way to study the interaction between multiple decision makers. However, classical game-theoretic analysis is limited in scalability due to the large number of strategies, precluding direct application to more complex scenarios. This survey provides a comprehensive overview of a framework for large games, known as Policy Space Response Oracles (PSRO), which holds promise to improve scalability by focusing attention on sufficient subsets of strategies. We first motivate PSRO and provide historical context. We then focus on the strategy exploration problem for PSRO: the challenge of assembling effective subsets of strategies that still represent the original game well with minimum computational cost. We survey current research directions for enhancing the efficiency of PSRO, and explore the applications of PSRO across various domains. We conclude by discussing open questions and future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02227v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>The 33rd International Joint Conference on Artificial Intelligence, 2024</arxiv:journal_reference>
      <dc:creator>Ariyan Bighashdel, Yongzhao Wang, Stephen McAleer, Rahul Savani, Frans A. Oliehoek</dc:creator>
    </item>
    <item>
      <title>Strategizing against Q-learners: A Control-theoretical Approach</title>
      <link>https://arxiv.org/abs/2403.08906</link>
      <description>arXiv:2403.08906v2 Announce Type: replace 
Abstract: In this paper, we explore the susceptibility of the independent Q-learning algorithms (a classical and widely used multi-agent reinforcement learning method) to strategic manipulation of sophisticated opponents in normal-form games played repeatedly. We quantify how much strategically sophisticated agents can exploit naive Q-learners if they know the opponents' Q-learning algorithm. To this end, we formulate the strategic actors' interactions as a stochastic game (whose state encompasses Q-function estimates of the Q-learners) as if the Q-learning algorithms are the underlying dynamical system. We also present a quantization-based approximation scheme to tackle the continuum state space and analyze its performance for two competing strategic actors and a single strategic actor both analytically and numerically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08906v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuksel Arslantas, Ege Yuceel, Muhammed O. Sayin</dc:creator>
    </item>
    <item>
      <title>Outcome-Driven Dynamic Refugee Assignment with Allocation Balancing</title>
      <link>https://arxiv.org/abs/2007.03069</link>
      <description>arXiv:2007.03069v5 Announce Type: replace-cross 
Abstract: This study proposes two new dynamic assignment algorithms to match refugees and asylum seekers to geographic localities within a host country. The first, currently implemented in a multi-year randomized control trial in Switzerland, seeks to maximize the average predicted employment level (or any measured outcome of interest) of refugees through a minimum-discord online assignment algorithm. The performance of this algorithm is tested on real refugee resettlement data from both the US and Switzerland, where we find that it is able to achieve near-optimal expected employment compared to the hindsight-optimal solution, and is able to improve upon the status quo procedure by 40-50%. However, pure outcome maximization can result in a periodically imbalanced allocation to the localities over time, leading to implementation difficulties and an undesirable workflow for resettlement resources and agents. To address these problems, the second algorithm balances the goal of improving refugee outcomes with the desire for an even allocation over time. We find that this algorithm can achieve near-perfect balance over time with only a small loss in expected employment compared to the employment-maximizing algorithm. In addition, the allocation balancing algorithm offers a number of ancillary benefits compared to pure outcome maximization, including robustness to unknown arrival flows and greater exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.03069v5</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1287/opre.2022.0445</arxiv:DOI>
      <dc:creator>Kirk Bansak, Elisabeth Paulson</dc:creator>
    </item>
    <item>
      <title>Interpolating Item and User Fairness in Multi-Sided Recommendations</title>
      <link>https://arxiv.org/abs/2306.10050</link>
      <description>arXiv:2306.10050v3 Announce Type: replace-cross 
Abstract: Today's online platforms heavily lean on algorithmic recommendations for bolstering user engagement and driving revenue. However, these recommendations can impact multiple stakeholders simultaneously -- the platform, items (sellers), and users (customers) -- each with their unique objectives, making it difficult to find the right middle ground that accommodates all stakeholders. To address this, we introduce a novel fair recommendation framework, Problem (FAIR), that flexibly balances multi-stakeholder interests via a constrained optimization formulation. We next explore Problem (FAIR) in a dynamic online setting where data uncertainty further adds complexity, and propose a low-regret algorithm FORM that concurrently performs real-time learning and fair recommendations, two tasks that are often at odds. Via both theoretical analysis and a numerical case study on real-world data, we demonstrate the efficacy of our framework and method in maintaining platform revenue while ensuring desired levels of fairness for both items and users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10050v3</guid>
      <category>cs.IR</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinyi Chen, Jason Cheuk Nam Liang, Negin Golrezaei, Djallel Bouneffouf</dc:creator>
    </item>
    <item>
      <title>Do LLM Agents Have Regret? A Case Study in Online Learning and Games</title>
      <link>https://arxiv.org/abs/2403.16843</link>
      <description>arXiv:2403.16843v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To promote the no-regret behaviors, we propose a novel \emph{unsupervised} training loss of \emph{regret-loss}, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. We then establish the statistical guarantee of generalization bound for regret-loss minimization, followed by the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above ``regrettable'' cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16843v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, Kaiqing Zhang</dc:creator>
    </item>
  </channel>
</rss>
