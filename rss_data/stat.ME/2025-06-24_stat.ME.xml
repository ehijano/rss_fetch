<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Jun 2025 01:42:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A practical identifiability criterion leveraging weak-form parameter estimation</title>
      <link>https://arxiv.org/abs/2506.17373</link>
      <description>arXiv:2506.17373v1 Announce Type: new 
Abstract: In this work, we define a practical identifiability criterion, (e, q)-identifiability, based on a parameter e, reflecting the noise in observed variables, and a parameter q, reflecting the mean-square error of the parameter estimator. This criterion is better able to encompass changes in the quality of the parameter estimate due to increased noise in the data (compared to existing criteria based solely on average relative errors). Furthermore, we leverage a weak-form equation error-based method of parameter estimation for systems with unobserved variables to assess practical identifiability far more quickly in comparison to output error-based parameter estimation. We do so by generating weak-form input-output equations using differential algebra techniques, as previously proposed by Boulier et al [1], and then applying Weak form Estimation of Nonlinear Dynamics (WENDy) to obtain parameter estimates. This method is computationally efficient and robust to noise, as demonstrated through two classical biological modelling examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17373v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nora Heitzman-Breen, Vanja Dukic, David M. Bortz</dc:creator>
    </item>
    <item>
      <title>Projected Normal Distribution: Moment Approximations and Generalizations</title>
      <link>https://arxiv.org/abs/2506.17461</link>
      <description>arXiv:2506.17461v1 Announce Type: new 
Abstract: The projected normal distribution, also known as the angular Gaussian distribution, is obtained by dividing a multivariate normal random variable $\mathbf{x}$ by its norm $\sqrt{\mathbf{x}^T \mathbf{x}}$. The resulting random variable follows a distribution on the unit sphere. No closed-form formulas for the moments of the projected normal distribution are known, which can limit its use in some applications. In this work, we derive analytic approximations to the first and second moments of the projected normal distribution using Taylor expansions and using results from the theory of quadratic forms of Gaussian random variables. Then, motivated by applications in systems neuroscience, we present generalizations of the projected normal distribution that divide the variable $\mathbf{x}$ by a denominator of the form $\sqrt{\mathbf{x}^T \mathbf{B} \mathbf{x} + c}$, where $\mathbf{B}$ is a symmetric positive definite matrix and $c$ is a non-negative number. We derive moment approximations as well as the density function for these other projected distributions. We show that the moments approximations are accurate for a wide range of dimensionalities and distribution parameters. Furthermore, we show that the moments approximations can be used to fit these distributions to data through moment matching. These moment matching methods should be useful for analyzing data across a range of applications where the projected normal distribution is used, and for applying the projected normal distribution and its generalizations to model data in neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17461v1</guid>
      <category>stat.ME</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Herrera-Esposito, Johannes Burge</dc:creator>
    </item>
    <item>
      <title>The Zeta Tail Distribution: A Novel Event-Count Model</title>
      <link>https://arxiv.org/abs/2506.17496</link>
      <description>arXiv:2506.17496v1 Announce Type: new 
Abstract: We introduce the Zeta Tail(a) probability distribution as a new model for random damage-event counts in risk analysis. Although readily motivated through a natural relationship with the Geometric(p) distribution, Zeta Tail(a) has received little attention in the scholarly literature. In the present work, we begin by deriving various fundamental properties of this novel distribution. We then assess its usefulness as an alternative to Geometric(p) for event-count data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17496v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers</dc:creator>
    </item>
    <item>
      <title>Curse of Dimensionality in Bayesian Model Updating</title>
      <link>https://arxiv.org/abs/2506.17744</link>
      <description>arXiv:2506.17744v1 Announce Type: new 
Abstract: Bayesian approach provides a coherent framework to address the model updating problem in structural health monitoring. The current practice, however, only focuses on low-dimension model (generally no more than 20 parameters), which limits the accuracy and predictability of the updated model. This paper aims at understanding the curse of dimensionality in Bayesian model updating, and thus proposing feasible strategies to overcome it. An analytical investigation is conducted, which allows us to answer fundamental questions in Bayesian analysis, e.g., where the posterior mass locates and how large of it comparing to the prior volume. The key concept here is the distance from the prior to the posterior, which makes the parameter estimation really difficult in high-dimension problems. In this sense, not only the dimensionality matters, but also the multi-modality, the pronounced degeneracy, and other factors that influence the prior-posterior distance matter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17744v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li Binbin, Liao Zihan</dc:creator>
    </item>
    <item>
      <title>Undersmoothed LASSO Models for Propensity Score Weighting and Synthetic Negative Control Exposures for Bias Detection</title>
      <link>https://arxiv.org/abs/2506.17760</link>
      <description>arXiv:2506.17760v1 Announce Type: new 
Abstract: The propensity score (PS) is often used to control for large numbers of covariates in high-dimensional healthcare database studies. The least absolute shrinkage and selection operator (LASSO) is a data-adaptive prediction algorithm that has become the most widely used tool for large-scale PS estimation in these settings. However, recent work has shown that the use of data-adaptive algorithms for PS estimation can come at the cost of slow convergence rates, resulting in PS-based causal estimators having poor statistical properties. While this can create challenges for the use of data-driven algorithms for PS analyses, both theory and simulations have shown that LASSO PS models can converge at a fast enough rate to provide asymptotically efficient PS weighted causal estimators. In order to achieve asymptotic efficiency, however, LASSO PS weighted estimators need to be properly tuned, which requires undersmoothing the fitted LASSO model. In this paper, we discuss challenges in determining how to undersmooth LASSO models for PS weighting and consider the use of balance diagnostics to select the degree of undersmoothing. Because no tuning criteria is universally best, we propose using synthetically generated negative control exposure studies to detect bias across alternative analytic choices. Specifically, we show that synthetic negative control exposures can identify undersmoothing techniques that likely violate partial exchangeability due to lack of control for measured confounding. We use a series of numerical studies to investigate the performance of alternative balance criteria to undersmooth LASSO PS-weighted estimators, and the use of synthetic negative control exposure studies to detect biased analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17760v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard Wyss, Ben B. Hansen, Georg Hahn, Lars van der Laan, Joshua K. Lin</dc:creator>
    </item>
    <item>
      <title>Selection of functional predictors and smooth coefficient estimation for scalar-on-function regression models</title>
      <link>https://arxiv.org/abs/2506.17773</link>
      <description>arXiv:2506.17773v1 Announce Type: new 
Abstract: In the framework of scalar-on-function regression models, in which several functional variables are employed to predict a scalar response, we propose a methodology for selecting relevant functional predictors while simultaneously providing accurate smooth (or, more generally, regular) estimates of the functional coefficients. We suppose that the functional predictors belong to a real separable Hilbert space, while the functional coefficients belong to a specific subspace of this Hilbert space. Such a subspace can be a Reproducing Kernel Hilbert Space (RKHS) to ensure the desired regularity characteristics, such as smoothness or periodicity, for the coefficient estimates. Our procedure, called SOFIA (Scalar-On-Function Integrated Adaptive Lasso), is based on an adaptive penalized least squares algorithm that leverages functional subgradients to efficiently solve the minimization problem. We demonstrate that the proposed method satisfies the functional oracle property, even when the number of predictors exceeds the sample size. SOFIA's effectiveness in variable selection and coefficient estimation is evaluated through extensive simulation studies and a real-data application to GDP growth prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17773v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hedayat Fathi, Marzia A. Cremona, Federico Severino</dc:creator>
    </item>
    <item>
      <title>Personalized feature threshold estimation in joint modelling of longitudinal and time-to-event data</title>
      <link>https://arxiv.org/abs/2506.17835</link>
      <description>arXiv:2506.17835v1 Announce Type: new 
Abstract: Cardiovascular disease (CVD) cohort studies collect longitudinal data on numerous CVD risk factors including body mass index (BMI), systolic blood pressure (SBP), diastolic blood pressure (DBP), glucose, and total cholesterol. The commonly used threshold values for identifying subjects at high risk are 30 kg/$m^2$ for BMI, 120 mmHg for SBP, 80 mmHg for DBP, 126 mg/dL for glucose, and 230 mg/dL for total cholesterol. When studying the association between features of longitudinal risk factors and time to a CVD event, an important research question is whether these CVD risk factor thresholds should vary based on individual characteristics as well as the type of longitudinal feature being considered. Using data from the Atherosclerosis Risk in Communities (ARIC) Study, we develop methods to estimate risk factor thresholds in joint models with multiple features for each longitudinal risk factor. These thresholds are allowed to vary by sex, race, and type of feature. Our methods have the potential for personalized CVD prevention strategies as well as better estimates of CVD risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17835v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mirajul Islam, Michael J. Daniels, Juned Siddique</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Left-Truncated Log-Logistic Distributions for Time-to-event Data Analysis</title>
      <link>https://arxiv.org/abs/2506.17852</link>
      <description>arXiv:2506.17852v1 Announce Type: new 
Abstract: Parameter estimation is a foundational step in statistical modeling, enabling us to extract knowledge from data and apply it effectively. Bayesian estimation of parameters incorporates prior beliefs with observed data to infer distribution parameters probabilistically and robustly. Moreover, it provides full posterior distributions, allowing uncertainty quantification and regularization, especially useful in small or truncated samples. Utilizing the left-truncated log-logistic (LTLL) distribution is particularly well-suited for modeling time-to-event data where observations are subject to a known lower bound such as precipitation data and cancer survival times. In this paper, we propose a Bayesian approach for estimating the parameters of the LTLL distribution with a fixed truncation point \( x_L &gt; 0 \). Given a random variable \( X \sim LL(\alpha, \beta; x_L) \), where \( \alpha &gt; 0 \) is the scale parameter and \( \beta &gt; 0 \) is the shape parameter, the likelihood function is derived based on a truncated sample \( X_1, X_2, \dots, X_N \) with \( X_i &gt; x_L \). We assume independent prior distributions for the parameters, and the posterior inference is conducted via Markov Chain Monte Carlo sampling, specifically using the Metropolis-Hastings algorithm to obtain posterior estimates \( \hat{\alpha} \) and \( \hat{\beta} \). Through simulation studies and real-world applications, we demonstrate that Bayesian estimation provides more stable and reliable parameter estimates, particularly when the likelihood surface is irregular due to left truncation. The results highlight the advantages of Bayesian inference outperform the estimation of parameter uncertainty in truncated distributions for time to event data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17852v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fahad Mostafa, Md Rejuan Haque, Md Mostafijur Rahman, Farzana Nasrin</dc:creator>
    </item>
    <item>
      <title>Constructing prediction intervals for the age distribution of deaths</title>
      <link>https://arxiv.org/abs/2506.17953</link>
      <description>arXiv:2506.17953v1 Announce Type: new 
Abstract: We introduce a model-agnostic procedure to construct prediction intervals for the age distribution of deaths. The age distribution of deaths is an example of constrained data, which are nonnegative and have a constrained integral. A centered log-ratio transformation and a cumulative distribution function transformation are used to remove the two constraints, where the latter transformation can also handle the presence of zero counts. Our general procedure divides data samples into training, validation, and testing sets. Within the validation set, we can select an optimal tuning parameter by calibrating the empirical coverage probabilities to be close to their nominal ones. With the selected optimal tuning parameter, we then construct the pointwise prediction intervals using the same models for the holdout data in the testing set. Using Japanese age- and sex-specific life-table death counts, we assess and evaluate the interval forecast accuracy with a suite of functional time-series models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17953v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Steven Haberman</dc:creator>
    </item>
    <item>
      <title>GRASP: Grouped Regression with Adaptive Shrinkage Priors</title>
      <link>https://arxiv.org/abs/2506.18092</link>
      <description>arXiv:2506.18092v1 Announce Type: new 
Abstract: We introduce GRASP, a simple Bayesian framework for regression with grouped predictors, built on the normal beta prime (NBP) prior. The NBP prior is an adaptive generalization of the horseshoe prior with tunable hyperparameters that control tail behavior, enabling a flexible range of sparsity, from strong shrinkage to ridge-like regularization. Unlike prior work that introduced the group inverse-gamma gamma (GIGG) prior by decomposing the NBP prior into structured hierarchies, we show that directly controlling the tails is sufficient without requiring complex hierarchical constructions. Extending the non-tail adaptive grouped half-Cauchy hierarchy of Xu et al., GRASP assigns the NBP prior to both local and group shrinkage parameters allowing adaptive sparsity within and across groups. A key contribution of this work is a novel framework to explicitly quantify correlations among shrinkage parameters within a group, providing deeper insights into grouped shrinkage behavior. We also introduce an efficient Metropolis-Hastings sampler for hyperparameter estimation. Empirical results on simulated and real-world data demonstrate the robustness and versatility of GRASP across grouped regression problems with varying sparsity and signal-to-noise ratios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18092v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shu Yu Tew, Daniel F. Schmidt, Mario Boley</dc:creator>
    </item>
    <item>
      <title>Regression Discontinuity Designs for Functional Data and Random Objects in Geodesic Spaces</title>
      <link>https://arxiv.org/abs/2506.18136</link>
      <description>arXiv:2506.18136v1 Announce Type: new 
Abstract: Regression discontinuity designs have been widely used in observational studies to estimate causal effects of an intervention or treatment at a cutoff point. We propose a generalization of regression discontinuity designs to handle complex non-Euclidean outcomes, such as networks, compositional data, functional data, and other random objects residing in geodesic metric spaces. A key challenge in this setting is the absence of algebraic operations, which makes it difficult to define treatment effects using simple differences. To address this, we define the causal effect at the cutoff as a geodesic between the local Fr\'echet means of untreated and treated outcomes. This reduces to the classical average treatment effect in the scalar case. Estimation is carried out using local Fr\'echet regression, a nonparametric method for metric space-valued responses that generalizes local linear regression. We introduce a new bandwidth selection procedure tailored to regression discontinuity designs, which performs competitively even in classical scalar scenarios. The proposed geodesic regression discontinuity design method is supported by theory, including convergence rate guarantees, and is demonstrated in applications where causal inference is of interest in complex outcome spaces. These include changes in daily CO concentration curves following the introduction of the Taipei Metro, and shifts in UK voting patterns measured by vote share compositions after Conservative Party wins. We also develop an extension to fuzzy designs with non-Euclidean outcomes, broadening the scope of causal inference to settings that allow for imperfect compliance with the assignment rule.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18136v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daisuke Kurisu, Yidong Zhou, Taisuke Otsu, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Dependent Dirichlet processes via thinning</title>
      <link>https://arxiv.org/abs/2506.18223</link>
      <description>arXiv:2506.18223v1 Announce Type: new 
Abstract: When analyzing data from multiple sources, it is often convenient to strike a careful balance between two goals: capturing the heterogeneity of the samples and sharing information across them. We introduce a novel framework to model a collection of samples using dependent Dirichlet processes constructed through a thinning mechanism. The proposed approach modifies the stick-breaking representation of the Dirichlet process by thinning, that is, setting equal to zero a random subset of the beta random variables used in the original construction. This results in a collection of dependent random distributions that exhibit both shared and unique atoms, with the shared ones assigned distinct weights in each distribution. The generality of the construction allows expressing a wide variety of dependence structures among the elements of the generated random vectors. Moreover, its simplicity facilitates the characterization of several theoretical properties and the derivation of efficient computational methods for posterior inference. A simulation study illustrates how a modeling approach based on the proposed process reduces uncertainty in group-specific inferences while preventing excessive borrowing of information when the data indicate it is unnecessary. This added flexibility improves the accuracy of posterior inference, outperforming related state-of-the-art models. An application to the Collaborative Perinatal Project data highlights the model's capability to estimate group-specific densities and uncover a meaningful partition of the observations, both within and across samples, providing valuable insights into the underlying data structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18223v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura D'Angelo, Bernardo Nipoti, Andrea Ongaro</dc:creator>
    </item>
    <item>
      <title>PCA-Guided Quantile Sampling: Preserving Data Structure in Large-Scale Subsampling</title>
      <link>https://arxiv.org/abs/2506.18249</link>
      <description>arXiv:2506.18249v1 Announce Type: new 
Abstract: We introduce Principal Component Analysis guided Quantile Sampling (PCA QS), a novel sampling framework designed to preserve both the statistical and geometric structure of large scale datasets. Unlike conventional PCA, which reduces dimensionality at the cost of interpretability, PCA QS retains the original feature space while using leading principal components solely to guide a quantile based stratification scheme. This principled design ensures that sampling remains representative without distorting the underlying data semantics. We establish rigorous theoretical guarantees, deriving convergence rates for empirical quantiles, Kullback Leibler divergence, and Wasserstein distance, thus quantifying the distributional fidelity of PCA QS samples. Practical guidelines for selecting the number of principal components, quantile bins, and sampling rates are provided based on these results. Extensive empirical studies on both synthetic and real-world datasets show that PCA QS consistently outperforms simple random sampling, yielding better structure preservation and improved downstream model performance. Together, these contributions position PCA QS as a scalable, interpretable, and theoretically grounded solution for efficient data summarization in modern machine learning workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18249v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Foo Hui-Mean, Yuan-chin Ivan Chang</dc:creator>
    </item>
    <item>
      <title>Improving precision of cumulative incidence estimates in randomized controlled trials with external controls</title>
      <link>https://arxiv.org/abs/2506.18415</link>
      <description>arXiv:2506.18415v1 Announce Type: new 
Abstract: Augmenting the control arm in clinical trials with external data can improve statistical power for demonstrating treatment effects. In many time-to-event outcome trials, participants are subject to truncation by death. Direct application of methods for competing risks analysis on the joint data may introduce bias, for example, due to covariate shifts between the populations. In this work, we consider transportability of the conditional cause-specific hazard of the event of interest under the control treatment. Under this assumption, we derive semiparametric efficiency bounds of causal cumulative incidences. This allows for quantification of the theoretical efficiency gain from incorporating the external controls. We propose triply robust estimators that can achieve the efficiency bounds, where the trial controls and external controls are made comparable through time-specific weights in a martingale integral. We conducted a simulation study to show the precision gain of the proposed fusion estimators compared to their counterparts without utilizing external controls. As a real data application, we used two cardiovascular outcome trials conducted to assess the safety of glucagon-like peptide-1 agonists. Incorporating the external controls from one trial into the other, we observed a decrease in the standard error of the treatment effects on adverse non-fatal cardiovascular events with all-cause death as the competing risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18415v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zehao Su, Helene C. W. Rytgaard, Henrik Ravn, Frank Eriksson</dc:creator>
    </item>
    <item>
      <title>Leveraging specificity for causal inference in observational studies</title>
      <link>https://arxiv.org/abs/2506.18469</link>
      <description>arXiv:2506.18469v1 Announce Type: new 
Abstract: Hill's specificity criterion has been highly influential in biomedical and epidemiological research. However, it remains controversial and its application often relies on subjective and qualitative analysis without a comprehensive and rigorous causal theory. Focusing on unmeasured confounding adjustment with multiple treatments and multiple outcomes, this paper develops a formal and quantitative framework for leveraging specificity for causal inference in observational studies. The proposed framework introduces a causal specificity assumption, a quantitative measure of specificity, a hypothesis testing procedure, and identification and estimation strategies. Identification under a nonparametric outcome model is established. The causal specificity assumption concerns only the breadth of causal associations, in contrast to Hill's specificity that concerns observed associations and to existing confounding adjustment methods that rely on auxiliary variables (e.g., instrumental variables, negative controls) or independence structures (e.g., factor models). A sensitivity analysis procedure is proposed to assess robustness of the test against violations of this assumption. This framework is particularly suited to exposure- and outcome-wide studies, where joint causal discovery across multiple treatments and multiple outcomes is of interest. It also offers a potential tool for addressing other sources of bias, such as invalid instruments in Mendelian randomization and selection bias in missing data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18469v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wang Miao</dc:creator>
    </item>
    <item>
      <title>Multi-Rank Subspace Change-Point Detection for Monitoring Robotic Swarms</title>
      <link>https://arxiv.org/abs/2506.18562</link>
      <description>arXiv:2506.18562v1 Announce Type: new 
Abstract: We study the problem of real-time detection of covariance structure changes in high-dimensional streaming data, motivated by applications such as robotic swarm monitoring. Building upon the spiked covariance model, we propose the multi-rank Subspace-CUSUM procedure, which extends the classical CUSUM framework by tracking the top principal components to approximate a likelihood ratio. We provide a theoretical analysis of the proposed method by characterizing the expected detection statistics under both pre- and post-change regimes and offer principled guidance for selecting the drift and threshold parameters to control the false alarm rate. The effectiveness of our method is demonstrated through simulations and a real-world application to robotic swarm behavior data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18562v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonghyeok Lee, Yao Xie, Youngser Park, Jason Hindes, Ira Schwartz, Carey Priebe</dc:creator>
    </item>
    <item>
      <title>Tail Flexibility in the Degrees of Preferential Attachment Networks</title>
      <link>https://arxiv.org/abs/2506.18726</link>
      <description>arXiv:2506.18726v1 Announce Type: new 
Abstract: Devising the underlying generating mechanism of a real-life network is difficult as, more often than not, only its snapshots are available, but not its full evolution. One candidate for the generating mechanism is preferential attachment which, in its simplest form, results in a degree distribution that follows the power law. Consequently, the growth of real-life networks that roughly display such power-law behaviour is commonly modelled by preferential attachment. However, the validity of the power law has been challenged by the presence of alternatives with comparable performance, as well as the recent findings that the right tail of the degree distribution is often lighter than implied by the body, whilst still being heavy. In this paper, we study a modified version of the model with a flexible preference function that allows super/sub-linear behaviour whilst also guaranteeing that the limiting degree distribution has a heavy tail. We relate the distributions tail index directly to the model parameters, allowing direct inference of the parameters from the degree distribution alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18726v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Boughen, Clement Lee, Vianey Palacios Ramirez</dc:creator>
    </item>
    <item>
      <title>Likelihood Ratio test for Poisson graph</title>
      <link>https://arxiv.org/abs/2506.18778</link>
      <description>arXiv:2506.18778v1 Announce Type: new 
Abstract: Directed acyclic graphs are widely used to describe the causal effects among random variables, and the inference of those causal effects has become an popular topic in statistics and machine learning, and has wide applications in neuroinformatics, bioinformatics and so on. However, most studies focus on the estimation or inference of the directional relations among continuous random variables, those among discrete random variables have not gained much attentions. In this article we focus on the inference of directed linkages and directed pathways in a Poisson directed graphical model. We employ likelihood ratio tests subject to non-convex acyclicity constraints, and derive the asymptotic distributions of the test statistic under the null hypothesis is true in high-dimensional situations. The power analysis and simulations suggest that the tests achieve the desired objectives of inference. An analysis of a basketball statistics dataset of NBA players during 2016-2017 season illustrates the utility of the proposed method to infer directed linkages and directed pathways in player's statistics network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18778v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Shuyan, Liu Xin, Wang Shaoli</dc:creator>
    </item>
    <item>
      <title>Asymptotic theory for the likelihood-based block maxima method in time series</title>
      <link>https://arxiv.org/abs/2506.17448</link>
      <description>arXiv:2506.17448v1 Announce Type: cross 
Abstract: This paper develops a rigorous asymptotic framework for likelihood-based inference in the Block Maxima (BM) method for stationary time series. While Bayesian inference under the BM approach has been widely studied in the independence setting, no asymptotic theory currently exists for time series. Further results are needed to establish that BM method can be applied with the kind of dependent time series models relevant to applied fields. To address this gap we first establish a comprehensive likelihood theory for the misspecified Generalized Extreme Value (GEV) model under serial dependence. Our results include uniform convergence of the empirical log-likelihood process, contraction rates for the Maximum Likelihood Estimator, and a local asymptotically Gaussian expansion. Building on this foundation, we develop the asymptotic theory of Bayesian inference for the GEV parameters, the extremal index, $T$-time-horizon return levels, and extreme quantiles (Value at Risk). Under general conditions on the prior, we prove posterior consistency, $\sqrt{k}$-contraction rates, Bernstein-von Mises theorems, and asymptotic coverage properties for credible intervals. For inference on the extremal index, we propose an adjusted posterior distribution that corrects for poor coverage exhibited by a naive Bayesian approach. Simulations show excellent inferential performances for the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17448v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David L. Carl, Simone A. Padoan, Stefano Rizzelli</dc:creator>
    </item>
    <item>
      <title>Testing Separability of High-Dimensional Covariance Matrices</title>
      <link>https://arxiv.org/abs/2506.17463</link>
      <description>arXiv:2506.17463v1 Announce Type: cross 
Abstract: Due to their parsimony, separable covariance models have been popular in modeling matrix-variate data. However, the inference from such a model may be misleading if the population covariance matrix $\Sigma$ is actually non-separable, motivating the use of statistical tests of separability. Likelihood ratio tests have tractable null distributions and good power when the sample size $n$ is larger than the number of variables $p$, but are not well-defined otherwise. Other existing separability tests for the $p&gt;n$ case have low power for small sample sizes, and have null distributions that depend on unknown parameters, preventing exact error rate control. To address these issues, we propose novel invariant tests leveraging the core covariance matrix, a complementary notion to a separable covariance matrix. We show that testing separability of $\Sigma$ is equivalent to testing sphericity of its core component. With this insight, we construct test statistics that are well-defined in high-dimensional settings and have distributions that are invariant under the null hypothesis of separability, allowing for exact simulation of null distributions. We study asymptotic null distributions and prove consistency of our tests in a $p/n\rightarrow\gamma\in(0,\infty)$ asymptotic regime. The large power of our proposed tests relative to existing procedures is demonstrated numerically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17463v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bongjung Sung, Peter D. Hoff</dc:creator>
    </item>
    <item>
      <title>Choice of Scoring Rules for Indirect Elicitation of Properties with Parametric Assumptions</title>
      <link>https://arxiv.org/abs/2506.17880</link>
      <description>arXiv:2506.17880v1 Announce Type: cross 
Abstract: People are commonly interested in predicting a statistical property of a random event such as mean and variance. Proper scoring rules assess the quality of predictions and require that the expected score gets uniquely maximized at the precise prediction, in which case we call the score directly elicits the property. Previous research work has widely studied the existence and the characterization of proper scoring rules for different properties, but little literature discusses the choice of proper scoring rules for applications at hand. In this paper, we explore a novel task, the indirect elicitation of properties with parametric assumptions, where the target property is a function of several directly-elicitable sub-properties and the total score is a weighted sum of proper scoring rules for each sub-property. Because of the restriction to a parametric model class, different settings for the weights lead to different constrained optimal solutions. Our goal is to figure out how the choice of weights affects the estimation of the target property and which choice is the best. We start it with simulation studies and observe an interesting pattern: in most cases, the optimal estimation of the target property changes monotonically with the increase of each weight, and the best configuration of weights is often to set some weights as zero. To understand how it happens, we first establish the elementary theoretical framework and then provide deeper sufficient conditions for the case of two sub-properties and of more sub-properties respectively. The theory on 2-D cases perfectly interprets the experimental results. In higher-dimensional situations, we especially study the linear cases and suggest that more complex settings can be understood with locally mapping into linear situations or using linear approximations when the true values of sub-properties are close enough to the parametric space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17880v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingfang Hu (Department of Computer Science, University of Illinois at Chicago, Chicago, USA.), Ian A. Kash (Department of Computer Science, University of Illinois at Chicago, Chicago, USA.)</dc:creator>
    </item>
    <item>
      <title>One-sample survival tests for non-proportional hazards in oncology clinical trial</title>
      <link>https://arxiv.org/abs/2506.18608</link>
      <description>arXiv:2506.18608v1 Announce Type: cross 
Abstract: In oncology, well-powered time-to-event randomized clinical trials are challenging due to a limited number of patients (e.g, pediatric cancers or personalized medicine). Last decade, many one- or two-stage designs for single-arm trials (SATs) have emerged as an alternative to overcome this issue. These designs rely on the one-sample log-rank test (OSLRT) and its modified version (mOSLRT) to compare the survival curves of an experimental and an external control group under the proportional hazards (PH) assumption that may be violated. We extend Finkelstein's formulation of OSLRT as a score test under PH by using a piecewise exponential model with change-points (CPs) for early, middle and delayed treatment effects and an accelerated hazards model for crossing hazards. The restricted mean survival time (RMST) based test is adapted to SATs and we also construct a combination test procedure (max-Combo) with correction for multiple testing. The performance of the developed tests (score tests, RMST and max-Combo tests) are evaluated through a simulation study of early, middle, delayed effects and crossing hazards. Findings show that the score tests are as conservative as the OSLRT and have the highest power when the data generation matches with the model. The max-Combo test is an interesting approach when the time-dependent relative treatment effect and/or the values of CPs are unknown. Uncertainty on the survival curve estimate of the external control group and model misspecification may have a significant impact on performance. For illustration, we apply the developed tests on three real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18608v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chlo\'e Szurewsky (U1018), Guosheng Yin (DSAS), Gw\'ena\"el Le Teuff (U1018)</dc:creator>
    </item>
    <item>
      <title>Tight Generalization Error Bounds for Stochastic Gradient Descent in Non-convex Learning</title>
      <link>https://arxiv.org/abs/2506.18645</link>
      <description>arXiv:2506.18645v1 Announce Type: cross 
Abstract: Stochastic Gradient Descent (SGD) is fundamental for training deep neural networks, especially in non-convex settings. Understanding SGD's generalization properties is crucial for ensuring robust model performance on unseen data. In this paper, we analyze the generalization error bounds of SGD for non-convex learning by introducing the Type II perturbed SGD (T2pm-SGD), which accommodates both sub-Gaussian and bounded loss functions. The generalization error bound is decomposed into two components: the trajectory term and the flatness term. Our analysis improves the trajectory term to $O(n^{-1})$, significantly enhancing the previous $O((nb)^{-1/2})$ bound for bounded losses, where n is the number of training samples and b is the batch size. By selecting an optimal variance for the perturbation noise, the overall bound is further refined to $O(n^{-2/3})$. For sub-Gaussian loss functions, a tighter trajectory term is also achieved. In both cases, the flatness term remains stable across iterations and is smaller than those reported in previous literature, which increase with iterations. This stability, ensured by T2pm-SGD, leads to tighter generalization error bounds for both loss function types. Our theoretical results are validated through extensive experiments on benchmark datasets, including MNIST and CIFAR-10, demonstrating the effectiveness of T2pm-SGD in establishing tighter generalization bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18645v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenjun Xiong, Juan Ding, Xinlei Zuo, Qizhai Li</dc:creator>
    </item>
    <item>
      <title>Structural restrictions in local causal discovery: identifying direct causes of a target variable</title>
      <link>https://arxiv.org/abs/2307.16048</link>
      <description>arXiv:2307.16048v4 Announce Type: replace 
Abstract: We consider the problem of learning a set of direct causes of a target variable from an observational joint distribution. Learning directed acyclic graphs (DAGs) that represent the causal structure is a fundamental problem in science. Several results are known when the full DAG is identifiable from the distribution, such as assuming a nonlinear Gaussian data-generating process. Here, we are only interested in identifying the direct causes of one target variable (local causal structure), not the full DAG. This allows us to relax the identifiability assumptions and develop possibly faster and more robust algorithms. In contrast to the Invariance Causal Prediction framework, we only assume that we observe one environment without any interventions. We discuss different assumptions for the data-generating process of the target variable under which the set of direct causes is identifiable from the distribution. While doing so, we put essentially no assumptions on the variables other than the target variable. In addition to the novel identifiability results, we provide two practical algorithms for estimating the direct causes from a finite random sample and demonstrate their effectiveness on several benchmark and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16048v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/biomet/asaf042</arxiv:DOI>
      <arxiv:journal_reference>Biometrika (2025)</arxiv:journal_reference>
      <dc:creator>Juraj Bodik, Val\'erie Chavez-Demoulin</dc:creator>
    </item>
    <item>
      <title>Informed Random Partition Models with Temporal Dependence</title>
      <link>https://arxiv.org/abs/2311.14502</link>
      <description>arXiv:2311.14502v2 Announce Type: replace 
Abstract: Model-based clustering is a powerful tool that is often used to discover hidden structure in data by grouping observational units that exhibit similar response values. Recently, clustering methods have been developed that permit incorporating an ``initial'' partition informed by expert opinion. Then, using some similarity criteria, partitions different from the initial one are down weighted, i.e. they are assigned reduced probabilities. These methods represent an exciting new direction of method development in clustering techniques. We add to this literature a method that very flexibly permits assigning varying levels of uncertainty to any subset of the partition. This is particularly useful in practice as there is rarely clear prior information with regards to the entire partition. Our approach is not based on partition penalties but considers individual allocation probabilities for each unit (e.g., locally weighted prior information). We illustrate the gains in prior specification flexibility via simulation studies and an application to a dataset concerning spatio-temporal evolution of ${\rm PM}_{10}$ measurements in Germany.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14502v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sally Paganin, Garritt L. Page, Fernando Andr\'es Quintana</dc:creator>
    </item>
    <item>
      <title>Data-adaptive structural change-point detection via isolation</title>
      <link>https://arxiv.org/abs/2404.19344</link>
      <description>arXiv:2404.19344v3 Announce Type: replace 
Abstract: In this paper, a new data-adaptive method, called DAIS (Data Adaptive ISolation), is introduced for the estimation of the number and the location of change-points in a given data sequence. The proposed method can detect changes in various different signal structures; we focus on the examples of piecewise-constant and continuous, piecewise-linear signals. The novelty of the proposed algorithm comes from the data-adaptive nature of the methodology. At each step, and for the data under consideration, we search for the most prominent change-point in a targeted neighborhood of the data sequence that contains this change-point with high probability. Using a suitably chosen contrast function, the change-point will then get detected after being isolated in an interval. The isolation feature enhances estimation accuracy, while the data-adaptive nature of DAIS is advantageous regarding, mainly, computational complexity. The methodology can be applied to both univariate and multivariate signals. The simulation results presented indicate that DAIS is at least as accurate as state-of-the-art competitors and in many cases significantly less computationally expensive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19344v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11222-025-10643-5</arxiv:DOI>
      <arxiv:journal_reference>Stat Comput 35, 117 (2025)</arxiv:journal_reference>
      <dc:creator>Andreas Anastasiou, Sophia Loizidou</dc:creator>
    </item>
    <item>
      <title>Approximate Bayesian Computation with Deep Learning and Conformal prediction</title>
      <link>https://arxiv.org/abs/2406.04874</link>
      <description>arXiv:2406.04874v4 Announce Type: replace 
Abstract: Approximate Bayesian Computation (ABC) methods are commonly used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Classical ABC methods are based on nearest neighbor type algorithms and rely on the choice of so-called summary statistics, distances between datasets and a tolerance threshold. Recently, methods combining ABC with more complex machine learning algorithms have been proposed to mitigate the impact of these ``user-choices''. In this paper, we propose the first, to our knowledge, ABC method completely free of summary statistics, distance, and tolerance threshold.
  Moreover, in contrast with usual generalizations of the ABC method, it associates a confidence interval (having a proper frequentist marginal coverage) with the posterior mean estimation (or other moment-type estimates).
  Our method, named ABCD-Conformal, uses a neural network with Monte Carlo Dropout to provide an estimation of the posterior mean (or other moment type functionals), and conformal theory to obtain associated confidence sets. Efficient for estimating multidimensional parameters and amortized, we test this new method on four different applications and compare it with other ABC methods in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04874v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meili Baragatti, Casenave C\'eline, Bertrand Cloez, David M\'etivier, Isabelle Sanchez</dc:creator>
    </item>
    <item>
      <title>Wasserstein $k$-Centers Clustering for Distributional Data</title>
      <link>https://arxiv.org/abs/2407.08228</link>
      <description>arXiv:2407.08228v4 Announce Type: replace 
Abstract: We develop a novel clustering method for distributional data, where each data point is regarded as a probability distribution on the real line. For distributional data, it has been challenging to develop a clustering method that utilizes modes of variation of the data because the space of probability distributions lacks a vector space structure, preventing the application of existing methods devised for functional data. Our clustering method for distributional data takes account of the differences in both means and modes of variation of clusters, in the spirit of the $k$-centers clustering approach proposed for functional data. Specifically, we consider the space of distributions equipped with the Wasserstein metric and define geodesic modes of variation of distributional data using the notion of geodesic principal component analysis. Then, we utilize geodesic modes of clusters to predict the cluster membership of each distribution. We theoretically show the validity of the proposed clustering criterion by studying the probability of correct membership. Through a simulation study and real data application, we demonstrate that the proposed distributional clustering method can improve the quality of the cluster compared to conventional clustering algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08228v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryo Okano, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Bayesian estimation for novel geometric INGARCH model</title>
      <link>https://arxiv.org/abs/2410.01283</link>
      <description>arXiv:2410.01283v3 Announce Type: replace 
Abstract: This paper introduces an integer-valued generalized autoregressive conditional heteroskedasticity (INGARCH) model based on the novel geometric distribution and discusses some of its properties. The parameter estimation problem of the models are studied by conditional maximum likelihood and Bayesian approach using Hamiltonian Monte Carlo (HMC) algorithm. The results of the simulation studies and real data analysis affirm the good performance of the estimators and the model. Forecasting using the Bayesian predictive distribution has also been studied and evaluated using real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01283v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Divya Kuttenchalil Andrews, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>A Novel Unit Distribution Named As Median Based Unit Rayleigh (MBUR): Properties and Estimations</title>
      <link>https://arxiv.org/abs/2410.04132</link>
      <description>arXiv:2410.04132v3 Announce Type: replace 
Abstract: The importance of continuously emerging new distribution is a mandate to understand the world and environment surrounding us. In this paper, the author will discuss a new distribution defined on the interval (0,1) as regards the methodology of deducing its PDF, some of its properties and related functions. A simulation and real data analysis will be highlighted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04132v3</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohammed Attia</dc:creator>
    </item>
    <item>
      <title>Explainable Linear and Generalized Linear Models by the Predictions Plot</title>
      <link>https://arxiv.org/abs/2412.16980</link>
      <description>arXiv:2412.16980v2 Announce Type: replace 
Abstract: Many statistics courses cover multiple linear regression, and present students with the formula of a prediction using the regressors, slopes, and an intercept. But is it really easy to see which terms have the largest effect, or to explain why the prediction of a specific case is unusually high or low? To assist with this the so-called predictions plot is proposed. Its simplicity makes it easy to interpret, and it combines much information. Its main benefit is that it helps explainability of the prediction formula as it is, without depending on how the formula was derived. The input variables can be numerical or categorical; interaction terms are also handled, and the model can be linear or generalized linear. Another display is proposed to visualize correlations between prediction terms, in a way that is tailored for this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16980v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Robust local empirical Bayes correction for Bayesian modeling</title>
      <link>https://arxiv.org/abs/2503.06837</link>
      <description>arXiv:2503.06837v2 Announce Type: replace 
Abstract: This paper investigates a robust empirical Bayes correction for Bayesian modeling. We show the application of the model on income distribution. Income shock includes temporal and permanent shocks. We aim to eliminate temporal shock and permanent shock using two-step local empirical correction method. Our results show that only 6.7% of the observed income shocks were permanent shock, and the posterior (permanent) mean weekly income was reduced from the observed income 415 pounds to 202 pounds for the United Kingdom using the Living Costs and Food Survey in 2021-2022. Keywords: Empirical Bayes correction; Outliers; Bayesian modeling</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06837v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
    <item>
      <title>Measuring Feature-Label Dependence Using Projection Correlation Statistic</title>
      <link>https://arxiv.org/abs/2504.19180</link>
      <description>arXiv:2504.19180v2 Announce Type: replace 
Abstract: Detecting dependence between variables is a crucial issue in statistical science. In this paper, we propose a novel metric called label projection correlation to measure the dependence between numerical and categorical variables. The proposed correlation does not require any conditions on numerical variables, and it is equal to zero if and only if the two variables are independent. When the numerical variable is one-dimensional, we demonstrate that the computational cost of the correlation estimation can be reduced to $\mathcal{O}(n \log n)$, where $ n $ is the sample size. Additionally, if the one-dimensional variable is continuous, the correlation can be simplified to a concise rank-based expression. The asymptotic theorems of the estimation are also established. Two simulated experiments are presented to demonstrate the effectiveness of the proposed correlation in feature selection. Furthermore, the metric is applied to feature selection in drivers' facial images and cancer mass-spectrometric data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19180v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiao Liu, Pengjian Shang</dc:creator>
    </item>
    <item>
      <title>Neural Networks for Parameter Estimation of the Discretely Observed Hawkes Process</title>
      <link>https://arxiv.org/abs/2506.01258</link>
      <description>arXiv:2506.01258v2 Announce Type: replace 
Abstract: When the sample path of a Hawkes process is observed discretely, such that only the total event counts in disjoint time intervals are known, the likelihood function becomes intractable. To overcome the challenge of likelihood-based inference in this setting, we propose to use a likelihood-free approach to parameter estimation, where simulated data is used to train a fully connected neural network (NN) to estimate the parameters of the Hawkes process from a summary statistic of the count data. A naive imputation estimate of the parameters forms the basis of our summary statistic, which is fast to generate and requires minimal expert knowledge to design. The resulting NN estimator is comparable to the best extant approximate likelihood estimators in terms of mean-squared error but requires significantly less computational time. We also propose to use a bootstrap procedure for bias correction and variance estimation. The proposed estimation procedure is applied to weekly count data for two infectious diseases, with a time-varying background rate used to capture seasonal fluctuations in infection risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01258v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason J. Lambe, Feng Chen, Tom Stindl, Tsz-Kit Jeffrey Kwan</dc:creator>
    </item>
    <item>
      <title>Optimal Adjustment Sets for Nonparametric Estimation of Weighted Controlled Direct Effect</title>
      <link>https://arxiv.org/abs/2506.09871</link>
      <description>arXiv:2506.09871v2 Announce Type: replace 
Abstract: The weighted controlled direct effect (WCDE) generalizes the standard controlled direct effect (CDE) by averaging over the mediator distribution, providing a robust estimate when treatment effects vary across mediator levels. This makes the WCDE especially relevant in fairness analysis, where it isolates the direct effect of an exposure on an outcome, independent of mediating pathways. This work establishes three fundamental advances for WCDE in observational studies: First, we establish necessary and sufficient conditions for the unique identifiability of the WCDE, clarifying when it diverges from the CDE. Next, we consider nonparametric estimation of the WCDE and derive its influence function, focusing on the class of regular and asymptotically linear estimators. Lastly, we characterize the optimal covariate adjustment set that minimizes the asymptotic variance, demonstrating how mediator-confounder interactions introduce distinct requirements compared to average treatment effect estimation. Our results offer a principled framework for efficient estimation of direct effects in complex causal systems, with practical applications in fairness and mediation analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09871v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruiyang Lin (University of Science,Technology of China), Yongyi Guo (University of Wisconsin-Madison), Kyra Gan (Cornell Tech)</dc:creator>
    </item>
    <item>
      <title>Estimating treatment effects with a unified semi-parametric difference-in-differences approach</title>
      <link>https://arxiv.org/abs/2506.12207</link>
      <description>arXiv:2506.12207v2 Announce Type: replace 
Abstract: Difference-in-differences (DID) approaches are widely used for estimating causal effects with observational data before and after an intervention. DID traditionally estimates the average treatment effect among the treated after making a parallel trends assumption on the means of the outcome. With skewed outcomes, a transformation is often needed; however, the transformation may be difficult to choose, results may be sensitive to the choice, and parallel trends assumptions are made on the transformed scale. Recent DID methods estimate alternative treatment effects that may be preferable with skewed outcomes. However, each alternative DID estimator requires a different parallel trends assumption. We introduce a new DID method capable of estimating average, quantile, probability, and novel Mann-Whitney treatment effects among the treated with a single unifying parallel trends assumption. The proposed method uses a semi-parametric cumulative probability model (CPM). The CPM is a linear model for a latent variable on covariates, where the latent variable results from an unspecified transformation of the outcome. Our DID approach makes a universal parallel trends assumption on the expectation of the latent variable conditional on covariates. Hence, our method avoids specifying outcome transformations and does not require separate assumptions for each estimand. We introduce the method; describe identification, estimation, and inference; conduct simulations evaluating its performance; and apply it to assess the impact of Medicaid expansion on CD4 count among people with HIV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12207v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia C. Thome, Andrew J. Spieker, Peter F. Rebeiro, Chun Li, Tong Li, Bryan E. Shepherd</dc:creator>
    </item>
    <item>
      <title>A Bayesian Non-parametric Approach to Generative Models: Integrating Variational Autoencoder and Generative Adversarial Networks using Wasserstein and Maximum Mean Discrepancy</title>
      <link>https://arxiv.org/abs/2308.14048</link>
      <description>arXiv:2308.14048v2 Announce Type: replace-cross 
Abstract: We propose a novel generative model within the Bayesian non-parametric learning (BNPL) framework to address some notable failure modes in generative adversarial networks (GANs) and variational autoencoders (VAEs)--these being overfitting in the GAN case and noisy samples in the VAE case. We will demonstrate that the BNPL framework enhances training stability and provides robustness and accuracy guarantees when incorporating the Wasserstein distance and maximum mean discrepancy measure (WMMD) into our model's loss function. Moreover, we introduce a so-called ``triple model'' that combines the GAN, the VAE, and further incorporates a code-GAN (CGAN) to explore the latent space of the VAE. This triple model design generates high-quality, diverse samples, while the BNPL framework, leveraging the WMMD loss function, enhances training stability. Together, these components enable our model to achieve superior performance across various generative tasks. These claims are supported by both theoretical analyses and empirical validation on a wide variety of datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14048v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Forough Fazeli-Asl, Michael Minyi Zhang</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction for Causal Effects of Continuous Treatments</title>
      <link>https://arxiv.org/abs/2407.03094</link>
      <description>arXiv:2407.03094v3 Announce Type: replace-cross 
Abstract: Uncertainty quantification of causal effects is crucial for safety-critical applications such as personalized medicine. A powerful approach for this is conformal prediction, which has several practical benefits due to model-agnostic finite-sample guarantees. Yet, existing methods for conformal prediction of causal effects are limited to binary/discrete treatments and make highly restrictive assumptions such as known propensity scores. In this work, we provide a novel conformal prediction method for potential outcomes of continuous treatments. We account for the additional uncertainty introduced through propensity estimation so that our conformal prediction intervals are valid even if the propensity score is unknown. Our contributions are three-fold: (1) We derive finite-sample prediction intervals for potential outcomes of continuous treatments. (2) We provide an algorithm for calculating the derived intervals. (3) We demonstrate the effectiveness of the conformal prediction intervals in experiments on synthetic and real-world datasets. To the best of our knowledge, we are the first to propose conformal prediction for continuous treatments when the propensity score is unknown and must be estimated from data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03094v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maresa Schr\"oder, Dennis Frauen, Jonas Schweisthal, Konstantin He{\ss}, Valentyn Melnychuk, Stefan Feuerriegel</dc:creator>
    </item>
    <item>
      <title>Conformal changepoint localization</title>
      <link>https://arxiv.org/abs/2505.00292</link>
      <description>arXiv:2505.00292v2 Announce Type: replace-cross 
Abstract: Changepoint localization is the problem of estimating the index at which a change occurred in the data generating distribution of an ordered list of data, or declaring that no change occurred. We present the broadly applicable CONCH (CONformal CHangepoint localization) algorithm, which uses a matrix of conformal p-values to produce a confidence interval for a (single) changepoint under the mild assumption that the pre-change and post-change distributions are each exchangeable. We exemplify the CONCH algorithm on a variety of synthetic and real-world datasets, including using black-box pre-trained classifiers to detect changes in sequences of images or text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00292v2</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>$\beta$-integrated local depth and corresponding partitioned local depth representation</title>
      <link>https://arxiv.org/abs/2506.14108</link>
      <description>arXiv:2506.14108v2 Announce Type: replace-cross 
Abstract: A novel local depth definition, $\beta$-integrated local depth ($\beta$-ILD), is proposed as a generalization of the local depth introduced by Paindaveine and Van Bever \cite{paindaveine2013depth}, designed to quantify the local centrality of data points. $\beta$-ILD inherits desirable properties from global data depth and remains robust across varying locality levels. A partitioning approach for $\beta$-ILD is introduced, leading to the construction of a matrix that quantifies the contribution of one point to another's local depth, providing a new interpretable measure of local centrality. These concepts are applied to classification and outlier detection tasks, demonstrating significant improvements in the performance of depth-based algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14108v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyi Wang, Alexandre Leblanc, Paul D. McNicholas</dc:creator>
    </item>
    <item>
      <title>LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs</title>
      <link>https://arxiv.org/abs/2506.15690</link>
      <description>arXiv:2506.15690v2 Announce Type: replace-cross 
Abstract: The increasing use of synthetic data from the public Internet has enhanced data usage efficiency in large language model (LLM) training. However, the potential threat of model collapse remains insufficiently explored. Existing studies primarily examine model collapse in a single model setting or rely solely on statistical surrogates. In this work, we introduce LLM Web Dynamics (LWD), an efficient framework for investigating model collapse at the network level. By simulating the Internet with a retrieval-augmented generation (RAG) database, we analyze the convergence pattern of model outputs. Furthermore, we provide theoretical guarantees for this convergence by drawing an analogy to interacting Gaussian Mixture Models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15690v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Wang, Lingyou Pang, Akira Horiguchi, Carey E. Priebe</dc:creator>
    </item>
  </channel>
</rss>
