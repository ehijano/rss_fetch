<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.IT</link>
    <description>math.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Jul 2025 04:05:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the Existence and Nonexistence of Splitter Sets</title>
      <link>https://arxiv.org/abs/2507.06578</link>
      <description>arXiv:2507.06578v1 Announce Type: new 
Abstract: In this paper, the existence of perfect and quasi-perfect splitter sets in finite abelian groups is studied, motivated by their application in coding theory for flash memory storage. For perfect splitter sets we view them as splittings of $\mathbb{Z}_n$, and using cyclotomic polynomials we derive a general condition for the existence of such splittings under certain circumstances. We further establish a relation between $B[-k, k](q)$ and $B[-(k-1), k+1](q)$ splitter sets, and give a necessary and sufficient condition for the existence of perfect $B[-1, 5](q)$ splitter sets. Finally, two nonexistence results for quasi-perfect splitter sets are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06578v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyu Yuan, Rongquan Feng, Gennian Ge</dc:creator>
    </item>
    <item>
      <title>Hybrid Quantum Convolutional Neural Network-Aided Pilot Assignment in Cell-Free Massive MIMO Systems</title>
      <link>https://arxiv.org/abs/2507.06585</link>
      <description>arXiv:2507.06585v1 Announce Type: new 
Abstract: A sophisticated hybrid quantum convolutional neural network (HQCNN) is conceived for handling the pilot assignment task in cell-free massive MIMO systems, while maximizing the total ergodic sum throughput. The existing model-based solutions found in the literature are inefficient and/or computationally demanding. Similarly, conventional deep neural networks may struggle in the face of high-dimensional inputs, require complex architectures, and their convergence is slow due to training numerous hyperparameters. The proposed HQCNN leverages parameterized quantum circuits (PQCs) relying on superposition for enhanced feature extraction. Specifically, we exploit the same PQC across all the convolutional layers for customizing the neural network and for accelerating the convergence. Our numerical results demonstrate that the proposed HQCNN offers a total network throughput close to that of the excessive-complexity exhaustive search and outperforms the state-of-the-art benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06585v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Doan Hieu Nguyen, Xuan Tung Nguyen, Seon-Geun Jeong, Trinh Van Chien, Lajos Hanzo, Won Joo Hwang</dc:creator>
    </item>
    <item>
      <title>Soft Robotics-Inspired Flexible Antenna Arrays</title>
      <link>https://arxiv.org/abs/2507.06589</link>
      <description>arXiv:2507.06589v1 Announce Type: new 
Abstract: In this work, a novel soft continuum robot-inspired antenna array is proposed, featuring tentacle-like structures with multiple antenna elements. The proposed array achieves reconfigurability through continuous deformation of its geometry, in contrast to reconfigurable antennas which incur a per-element control. More specifically, the deformation is modeled by amplitude and spatial frequency parameters. We consider a multi-user multiple-input single-output downlink system, whereby the optimal deformation parameters are found to maximize the sum rate in the network. A successive convex approximation method is adopted to solve the problem. Numerical results show that the proposed deformable array significantly outperforms fixed geometry and per-element reconfigurable arrays in sum rate, demonstrating the benefits of structure-level flexibility for next-generation antenna arrays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06589v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Elio Faddoul, Andreas Nicolaides, Konstantinos Ntougias, Ioannis Krikidis</dc:creator>
    </item>
    <item>
      <title>On the Convergence Speed of Spatially Coupled LDPC Ensembles Under Window Decoding</title>
      <link>https://arxiv.org/abs/2507.06635</link>
      <description>arXiv:2507.06635v1 Announce Type: new 
Abstract: It is known that windowed decoding (WD) can effectively balance the performance and complexity of spatially coupled low-density parity-check (LDPC) codes. In this study, we show that information can propagate in a wave-like manner at a constant speed under WD. Additionally, we provide an upper bound for the information propagation speed on the binary erasure channel, which can assist in designing the number of iterations required within each window.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06635v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingqing Peng, Dongxu Chang, Guanghui Wang, Guiying Yan</dc:creator>
    </item>
    <item>
      <title>On the Error Exponent Distribution of Code Ensembles over Classical-Quantum Channels</title>
      <link>https://arxiv.org/abs/2507.06868</link>
      <description>arXiv:2507.06868v1 Announce Type: new 
Abstract: We show that the probability distribution of the error exponent in i.i.d. code ensembles over classical-quantum (CQ) channels with arbitrary output states accumulates above a threshold that is strictly larger than the CQ random coding exponent (RCE) at low rates, while coinciding with it at rates close to the mutual information of the channel. This result, combined with the work by Dalai [1] and the recent ones by Renes [2] and Li and Yang [3], implies that the ensemble distribution of error exponents concentrates around the CQ RCE in the high rate regime. Moreover, in the same rate regime the threshold we derive coincides with the ensemble-average of the exponent, that is, the typical random coding (TRC) exponent [4].</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06868v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Cocco, Javier Rodr\'iguez Fonollosa</dc:creator>
    </item>
    <item>
      <title>Fractional Programming for Stochastic Precoding over Generalized Fading Channels</title>
      <link>https://arxiv.org/abs/2507.06944</link>
      <description>arXiv:2507.06944v1 Announce Type: new 
Abstract: This paper seeks an efficient algorithm for stochastic precoding to maximize the long-term average weighted sum rates throughout a multiple-input multiple-output (MIMO) network. Unlike many existing works that assume a particular probability distribution model for fading channels (which is typically Gaussian), our approach merely relies on the first and second moments of fading channels. For the stochastic precoding problem, a naive idea is to directly apply the fractional programming (FP) method to the data rate inside the expectation; it does not work well because the auxiliary variables introduced by FP are then difficult to decide. To address the above issue, we propose using a lower bound to approximate the expectation of data rate. This lower bound stems from a nontrivial use of the matrix FP, and outperforms the existing lower bounds in that it accounts for generalized fading channels whose first and second moments are known. The resulting approximate problem can be efficiently solved in closed form in an iterative fashion. Furthermore, for large-scale MIMO, we improve the efficiency of the proposed algorithm by eliminating the large matrix inverse. Simulations show that the proposed stochastic precoding method outperforms the benchmark methods in both Gaussian and non-Gaussian fading channel cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06944v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenyu Wang, Kaiming Shen</dc:creator>
    </item>
    <item>
      <title>Quantum Codes with Addressable and Transversal Non-Clifford Gates</title>
      <link>https://arxiv.org/abs/2502.01864</link>
      <description>arXiv:2502.01864v3 Announce Type: cross 
Abstract: The development of quantum codes with good error correction parameters and useful sets of transversal gates is a problem of major interest in quantum error-correction. Abundant prior works have studied transversal gates which are restricted to acting on all logical qubits simultaneously. In this work, we study codes that support transversal gates which induce $\textit{addressable}$ logical gates, i.e., the logical gates act on logical qubits of our choice. As we consider scaling to high-rate codes, the study and design of low-overhead, addressable logical operations presents an important problem for both theoretical and practical purposes.
  Our primary result is the construction of an explicit qubit code for which $\textit{any}$ triple of logical qubits across one, two, or three codeblocks can be addressed with a logical $\mathsf{CCZ}$ gate via a depth-one circuit of physical $\mathsf{CCZ}$ gates, and whose parameters are asymptotically good, up to polylogarithmic factors. The result naturally generalizes to other gates including the $\mathsf{C}^{\ell} Z$ gates for $\ell \neq 2$.
  Going beyond this, we develop a formalism for constructing quantum codes with $\textit{addressable and transversal}$ gates. Our framework, called $\textit{addressable orthogonality}$, encompasses the original triorthogonality framework of Bravyi and Haah (Phys. Rev. A 2012), and extends this and other frameworks to study addressable gates. We demonstrate the power of this framework with the construction of an asymptotically good qubit code for which $\textit{pre-designed}$, pairwise disjoint triples of logical qubits within a single codeblock may be addressed with a logical $\mathsf{CCZ}$ gate via a physical depth-one circuit of $\mathsf{Z}$, $\mathsf{CZ}$ and $\mathsf{CCZ}$ gates. In an appendix, we show that our framework extends to addressable and transversal $T$ gates, up to Clifford corrections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01864v3</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyang He, Vinod Vaikuntanathan, Adam Wills, Rachel Yun Zhang</dc:creator>
    </item>
    <item>
      <title>Simple Convergence Proof of Adam From a Sign-like Descent Perspective</title>
      <link>https://arxiv.org/abs/2507.05966</link>
      <description>arXiv:2507.05966v1 Announce Type: cross 
Abstract: Adam is widely recognized as one of the most effective optimizers for training deep neural networks (DNNs). Despite its remarkable empirical success, its theoretical convergence analysis remains unsatisfactory. Existing works predominantly interpret Adam as a preconditioned stochastic gradient descent with momentum (SGDM), formulated as $\bm{x}_{t+1} = \bm{x}_t - \frac{\gamma_t}{{\sqrt{\bm{v}_t}+\epsilon}} \circ \bm{m}_t$. This perspective necessitates strong assumptions and intricate techniques, resulting in lengthy and opaque convergence proofs that are difficult to verify and extend. In contrast, we propose a novel interpretation by treating Adam as a sign-like optimizer, expressed as $\bm{x}_{t+1} = \bm{x}_t - \gamma_t \frac{|\bm{m}_t|}{{\sqrt{\bm{v}_t}+\epsilon}} \circ {\rm Sign}(\bm{m}_t)$. This reformulation significantly simplifies the convergence analysis. For the first time, with some mild conditions, we prove that Adam achieves the optimal rate of ${\cal O}(\frac{1}{T^{\sfrac{1}{4}}})$ rather than the previous ${\cal O} \left(\frac{\ln T}{T^{\sfrac{1}{4}}}\right)$ under weak assumptions of the generalized $p$-affine variance and $(L_0, L_1, q)$-smoothness, without dependence on the model dimensionality or the numerical stability parameter $\epsilon$. Additionally, our theoretical analysis provides new insights into the role of momentum as a key factor ensuring convergence and offers practical guidelines for tuning learning rates in Adam, further bridging the gap between theory and practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05966v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanyang Peng, Shuang Qin, Yue Yu, Fangqing Jiang, Hui Wang, Zhouchen Lin</dc:creator>
    </item>
    <item>
      <title>On the Hardness of Unsupervised Domain Adaptation: Optimal Learners and Information-Theoretic Perspective</title>
      <link>https://arxiv.org/abs/2507.06552</link>
      <description>arXiv:2507.06552v1 Announce Type: cross 
Abstract: This paper studies the hardness of unsupervised domain adaptation (UDA) under covariate shift. We model the uncertainty that the learner faces by a distribution $\pi$ in the ground-truth triples $(p, q, f)$ -- which we call a UDA class -- where $(p, q)$ is the source -- target distribution pair and $f$ is the classifier. We define the performance of a learner as the overall target domain risk, averaged over the randomness of the ground-truth triple. This formulation couples the source distribution, the target distribution and the classifier in the ground truth, and deviates from the classical worst-case analyses, which pessimistically emphasize the impact of hard but rare UDA instances. In this formulation, we precisely characterize the optimal learner. The performance of the optimal learner then allows us to define the learning difficulty for the UDA class and for the observed sample. To quantify this difficulty, we introduce an information-theoretic quantity -- Posterior Target Label Uncertainty (PTLU) -- along with its empirical estimate (EPTLU) from the sample , which capture the uncertainty in the prediction for the target domain. Briefly, PTLU is the entropy of the predicted label in the target domain under the posterior distribution of ground-truth classifier given the observed source and target samples. By proving that such a quantity serves to lower-bound the risk of any learner, we suggest that these quantities can be used as proxies for evaluating the hardness of UDA learning. We provide several examples to demonstrate the advantage of PTLU, relative to the existing measures, in evaluating the difficulty of UDA learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06552v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyi Dong, Zixuan Liu, Yongyi Mao</dc:creator>
    </item>
    <item>
      <title>Relaying Quantum Information</title>
      <link>https://arxiv.org/abs/2507.06770</link>
      <description>arXiv:2507.06770v1 Announce Type: cross 
Abstract: Quantum relays are central to both quantum communication and distributed quantum computing, enabling long-distance transmission and modular architectures. Unlike classical repeaters, quantum repeaters preserve coherence without amplifying quantum information, relying on entanglement swapping and quantum error correction to overcome loss and decoherence. In this work, we investigate the transmission of quantum information via quantum relay channels. Our three-terminal relay model captures the trade-off between repeater-assisted and repeaterless communication strategies. We propose a decode-forward coding scheme and analyze both entanglement-assisted and unassisted scenarios. Our framework allows for different entanglement topologies between the transmitter, the relay and the destination receiver, recovering known results on entanglement-assisted and unassisted communication. Furthermore, we discuss the interpretation of coding with quantum side information. These findings serve as a stepping stone for the design of secure, efficient, and reliable quantum networks and the practical realization of quantum repeaters and long-range quantum key distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06770v1</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yigal Ilin, Uzi Pereg</dc:creator>
    </item>
    <item>
      <title>Layer Cake Representations for Quantum Divergences</title>
      <link>https://arxiv.org/abs/2507.07065</link>
      <description>arXiv:2507.07065v1 Announce Type: cross 
Abstract: Defining suitable quantum extensions of classical divergences often poses a challenge due to the non-commutative nature of quantum information. In this work, we propose a new approach via what we call the layer cake representation. The resulting quantum R\'enyi and $f$-divergences are then proven to be equivalent to those recently defined via integral representations. Nevertheless, the approach can provide several insights. We give an alternative proof of the integral representation of the relative entropy by Frenkel and prove a conjecture regarding a trace expression for the R\'enyi divergence. Additionally, we give applications to error exponents in hypothesis testing, a new Riemann-Stieltjes type integral representation and a variational representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07065v1</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Po-Chieh Liu, Christoph Hirche, Hao-Chung Cheng</dc:creator>
    </item>
    <item>
      <title>Improved Channel Coding Performance Through Cost Variability</title>
      <link>https://arxiv.org/abs/2407.05260</link>
      <description>arXiv:2407.05260v3 Announce Type: replace 
Abstract: Channel coding for discrete memoryless channels (DMCs) with mean and variance cost constraints has been recently introduced. We show that there is an improvement in coding performance due to cost variability, both with and without feedback. We demonstrate this improvement over the traditional almost-sure (per-codeword) cost constraint that prohibits any cost variation above a fixed threshold. Our result simultaneously shows that feedback does not improve the second-order coding rate of simple-dispersion DMCs under the almost-sure cost constraint. This finding parallels similar results for unconstrained simple-dispersion DMCs, additive white Gaussian noise (AWGN) channels and parallel Gaussian channels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05260v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adeel Mahmood, Aaron B. Wagner</dc:creator>
    </item>
    <item>
      <title>Sharp Estimates for Optimal Multistage Group Partition Testing</title>
      <link>https://arxiv.org/abs/2409.10410</link>
      <description>arXiv:2409.10410v3 Announce Type: replace 
Abstract: In multistage group testing, the tests within the same stage are considered nonadaptive, while those conducted across different stages are adaptive. Specifically, when the pools within the same stage are disjoint, meaning that the entire set is divided into several disjoint subgroups, it is referred to as a multistage group partition testing problem, denoted as the (n, d, s) problem, where n, d, and s represent the total number of items, defectives, and stages respectively. This paper presents exact solutions for the (n, 1, s) and (n, d, 2) problems for the first time. Additionally, a general dynamic programming approach is developed for the (n, d, s) problem. Significantly we give the sharp upper and lower bounds estimates. If the defective number in unknown but bounded, we can provide an algorithm with an optimal competitive ratio in the asymptotic sense. While assuming the prior distribution of the defective items, we also establish a well performing upper and lower bound estimate to the expectation of optimal strategy</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10410v3</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guojiang Shao</dc:creator>
    </item>
    <item>
      <title>LLM Agent for Hyper-Parameter Optimization</title>
      <link>https://arxiv.org/abs/2506.15167</link>
      <description>arXiv:2506.15167v2 Announce Type: replace 
Abstract: Hyper-parameters are essential and critical for the performance of communication algorithms. However, current hyper-parameters optimization approaches for Warm-Start Particles Swarm Optimization with Crossover and Mutation (WS-PSO-CM) algorithm, designed for radio map-enabled unmanned aerial vehicle (UAV) trajectory and communication, are primarily heuristic-based, exhibiting low levels of automation and improvable performance. In this paper, we design an Large Language Model (LLM) agent for automatic hyper-parameters-tuning, where an iterative framework and Model Context Protocol (MCP) are applied. In particular, the LLM agent is first set up via a profile, which specifies the boundary of hyper-parameters, task objective, terminal condition, conservative or aggressive strategy of optimizing hyper-parameters, and LLM configurations. Then, the LLM agent iteratively invokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent exits the loop based on the terminal condition and returns an optimized set of hyperparameters. Our experiment results show that the minimal sum-rate achieved by hyper-parameters generated via our LLM agent is significantly higher than those by both human heuristics and random generation methods. This indicates that an LLM agent with PSO and WS-PSO-CM algorithm knowledge is useful in seeking high-performance hyper-parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15167v2</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanzhe Wang, Jianqiu Peng, Menghao Hu, Weihuang Zhong, Tong Zhang, Shuai Wang, Yixin Zhang, Mingjie Shao, Wanli Ni</dc:creator>
    </item>
    <item>
      <title>Mutual Information Bounds for Lossy Common Information</title>
      <link>https://arxiv.org/abs/2507.04209</link>
      <description>arXiv:2507.04209v2 Announce Type: replace 
Abstract: We show the mutual information between the targets in a Gray-Wyner Network as a bound that separates Wyner's lossy common information and G\'acs-K\"orner lossy common information. The results are a generalization of the lossless case presented by Wyner (1975).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04209v2</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anderson de Andrade</dc:creator>
    </item>
    <item>
      <title>Online Quantile Regression</title>
      <link>https://arxiv.org/abs/2402.04602</link>
      <description>arXiv:2402.04602v3 Announce Type: replace-cross 
Abstract: This paper addresses the challenge of integrating sequentially arriving data within the quantile regression framework, where the number of features is allowed to grow with the number of observations, the horizon is unknown, and memory is limited. We employ stochastic sub-gradient descent to minimize the empirical check loss and study its statistical properties and regret performance. In our analysis, we unveil the delicate interplay between updating iterates based on individual observations versus batches of observations, revealing distinct regularity properties in each scenario. Our method ensures long-term optimal estimation irrespective of the chosen update strategy. Importantly, our contributions go beyond prior works by achieving exponential-type concentration inequalities and attaining optimal regret and error rates that exhibit only \textsf{ short-term} sensitivity to initial errors. A key insight from our study is the delicate statistical analyses and the revelation that appropriate stepsize schemes significantly mitigate the impact of initial errors on subsequent errors and regrets. This underscores the robustness of stochastic sub-gradient descent in handling initial uncertainties, emphasizing its efficacy in scenarios where the sequential arrival of data introduces uncertainties regarding both the horizon and the total number of observations. Additionally, when the initial error rate is well-controlled, there is a trade-off between short-term error rate and long-term optimality. Due to the lack of delicate statistical analysis for squared loss, we also briefly discuss its properties and proper schemes. Extensive simulations support our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04602v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinan Shen, Dong Xia, Wen-Xin Zhou</dc:creator>
    </item>
    <item>
      <title>LAURA: LLM-Assisted UAV Routing for AoI Minimization</title>
      <link>https://arxiv.org/abs/2503.23132</link>
      <description>arXiv:2503.23132v2 Announce Type: replace-cross 
Abstract: With the rapid growth of the low-altitude economy, there is increasing demand for real-time data collection using UAV-assisted wireless sensor networks. This paper investigates the problem of minimizing the age of information (AoI) in UAV-assisted wireless sensor networks by optimizing the UAV flight routing. We formulate the AoI minimization task and propose a large language model (LLM)-assisted UAV routing algorithm (LAURA). LAURA employs an LLM as intelligent crossover operators within an evolutionary optimization framework to efficiently explore the solution space. Simulation results show that LAURA outperforms benchmark methods in reducing the maximum AoI, especially in scenarios with a large number of sensor nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23132v2</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bisheng Wei, Ruichen Zhang, Ruihong Jiang, Mugen Peng, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>Smoothed analysis in compressed sensing</title>
      <link>https://arxiv.org/abs/2505.05188</link>
      <description>arXiv:2505.05188v2 Announce Type: replace-cross 
Abstract: Arbitrary matrices $M \in \mathbb{R}^{m \times n}$, randomly perturbed in an additive manner using a random matrix $R \in \mathbb{R}^{m \times n}$, are shown to asymptotically almost surely satisfy the so-called {\sl robust null space property}. Whilst insisting on an asymptotically optimal order of magnitude for $m$ required to attain {\sl unique reconstruction} via $\ell_1$-minimisation algorithms, our results track the level of arbitrariness allowed for the fixed seed matrix $M$ as well as the degree of distributional irregularity allowed for the entries of the perturbing matrix $R$. Starting with sub-gaussian entries for $R$, our results culminate with these allowed to have substantially heavier tails than sub-exponential ones. Throughout this trajectory, two measures control the arbitrariness allowed for $M$; the first is $\|M\|_\infty$ and the second is a localised notion of the Frobenius norm of $M$ (which depends on the sparsity of the signal being reconstructed). A key tool driving our proofs is {\sl Mendelson's small-ball method} ({\em Learning without concentration}, J. ACM, Vol. $62$, $2015$).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05188v2</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elad Aigner-Horev, Dan Hefetz, Michael Trushkin</dc:creator>
    </item>
    <item>
      <title>Mixing and Merging Metric Spaces using Directed Graphs</title>
      <link>https://arxiv.org/abs/2505.06405</link>
      <description>arXiv:2505.06405v2 Announce Type: replace-cross 
Abstract: Let $(X_1,d_1),\dots, (X_N,d_N)$ be metric spaces, where $d_i: X_i \times X_i \rightarrow [0,1]$ is a distance function for $i=1,\dots,N$. Let $\mathcal{X}$ denote the set theoretic product $X_1\times \cdots \times X_N$. Let $\mathcal{G} = \left(\mathcal{V},\mathcal{E}\right)$ be a directed graph with vertex set $\mathcal{V} =\{1,\dots, N\}$, and let $\mathcal{P} = \{p_{ij}\}$ be a collection of weights, where each $p_{ij}\in (0, 1]$ is associated with the edge $(i,j) \in \mathcal{E}$. We introduce the function $d_{\mathcal{X},\mathcal{G},\mathcal{P}}: \mathcal{X}\times \mathcal{X} \to [0,1]$ defined by \begin{align*} d_{\mathcal{X},\mathcal{G},\mathcal{P}}(\mathbf{g},\mathbf{h}) := \left(1 - \frac{1}{N}\sum_{j=1}^N \prod_{i=1}^N \left[1- d_i(g_i,h_i)\right]^{\frac{1}{p_{ji}}} \right), \end{align*} for all $\mathbf{g},\mathbf{h} \in \mathcal{X}$. In this paper we show that $d_{\mathcal{X},\mathcal{G},\mathcal{P}}$ defines a metric space over $\mathcal{X}$. Then we determine how this distance behaves under various graph operations, including disjoint unions and Cartesian products. We investigate two limiting cases: (a) when $d_{\mathcal{X},\mathcal{G},\mathcal{P}}$ is defined over a finite field, leading to a broad generalization of graph-based distances commonly studied in error-correcting code theory; and (b) when the metric is extended to graphons, enabling the measurement of distances in a continuous graph limit setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06405v2</guid>
      <category>math.CO</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mahir Bilen Can, Shantanu Chakrabartty</dc:creator>
    </item>
    <item>
      <title>The end of radical concept nativism</title>
      <link>https://arxiv.org/abs/2505.18277</link>
      <description>arXiv:2505.18277v2 Announce Type: replace-cross 
Abstract: Though humans seem to be remarkable learners, arguments in cognitive science and philosophy of mind have long maintained that learning something fundamentally new is impossible. Specifically, Jerry Fodor's arguments for radical concept nativism hold that most, if not all, concepts are innate and that what many call concept learning never actually leads to the acquisition of new concepts. These arguments have deeply affected cognitive science, and many believe that the counterarguments to radical concept nativism have been either unsuccessful or only apply to a narrow class of concepts. This paper first reviews the features and limitations of prior arguments. We then identify three critical points - related to issues of expressive power, conceptual structure, and concept possession - at which the arguments in favor of radical concept nativism diverge from describing actual human cognition. We use ideas from computer science and information theory to formalize the relevant ideas in ways that are arguably more scientifically productive. We conclude that, as a result, there is an important sense in which people do indeed learn new concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18277v2</guid>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua S. Rule, Steven T. Piantadosi</dc:creator>
    </item>
    <item>
      <title>Quantum accessible information and classical entropy inequalities</title>
      <link>https://arxiv.org/abs/2506.06700</link>
      <description>arXiv:2506.06700v2 Announce Type: replace-cross 
Abstract: Computing accessible information for an ensemble of quantum states is a basic problem in quantum information theory. The optimality criterion recently obtained in [7], when applied to specific ensembles of states, leads to nontrivial tight lower bounds for the Shannon entropy that are discrete relatives of the famous log-Sobolev inequality. In this light, the hypothesis of globally information-optimal measurement for an ensemble of equiangular equiprobable states (quantum pyramids) put forward and numerically substantiated in [2] is reconsidered and the corresponding tight entropy inequalities are proposed and proved. Via the optimality criterion, this provides also the first proof of the conjecture concerning globally information-optimal observables for quantum pyramids put forward in [2].</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06700v2</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. S. Holevo, A. V. Utkin</dc:creator>
    </item>
    <item>
      <title>Fully Parallelized BP Decoding for Quantum LDPC Codes Can Outperform BP-OSD</title>
      <link>https://arxiv.org/abs/2507.00254</link>
      <description>arXiv:2507.00254v2 Announce Type: replace-cross 
Abstract: In this work, we propose a lightweight decoder based solely on belief-propagation (BP), augmented with a speculative post-processing strategy inspired by classical Chase decoding. Our method identifies unreliable bits via BP oscillation statistics, generates a set of modified test patterns, and decodes them in parallel using low-iteration BP. We demonstrate that our approach can achieve logical error rates comparable to or even better than BP-OSD, but has lower latency over its parallelization for a variety of bivariate bicycle codes, which significantly reduces decoding complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00254v2</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 10 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ming Wang, Ang Li, Frank Mueller</dc:creator>
    </item>
  </channel>
</rss>
