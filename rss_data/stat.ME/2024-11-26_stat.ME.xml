<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Nov 2024 02:56:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Scalar-on-Shape Regression Models for Functional Data Analysis</title>
      <link>https://arxiv.org/abs/2411.15326</link>
      <description>arXiv:2411.15326v1 Announce Type: new 
Abstract: Functional data contains two components: shape (or amplitude) and phase. This paper focuses on a branch of functional data analysis (FDA), namely Shape-Based FDA, that isolates and focuses on shapes of functions. Specifically, this paper focuses on Scalar-on-Shape (ScoSh) regression models that incorporate the shapes of predictor functions and discard their phases. This aspect sets ScoSh models apart from the traditional Scalar-on-Function (ScoF) regression models that incorporate full predictor functions. ScoSh is motivated by object data analysis, {\it, e.g.}, for neuro-anatomical objects, where object morphologies are relevant and their parameterizations are arbitrary. ScoSh also differs from methods that arbitrarily pre-register data and uses it in subsequent analysis. In contrast, ScoSh models perform registration during regression, using the (non-parametric) Fisher-Rao inner product and nonlinear index functions to capture complex predictor-response relationships. This formulation results in novel concepts of {\it regression phase} and {\it regression mean} of functions. Regression phases are time-warpings of predictor functions that optimize prediction errors, and regression means are optimal regression coefficients. We demonstrate practical applications of the ScoSh model using extensive simulated and real-data examples, including predicting COVID outcomes when daily rate curves are predictors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15326v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sayan Bhadra, Anuj Srivastava</dc:creator>
    </item>
    <item>
      <title>The ultimate issue error: mistaking parameters for hypotheses</title>
      <link>https://arxiv.org/abs/2411.15398</link>
      <description>arXiv:2411.15398v1 Announce Type: new 
Abstract: In a criminal investigation, an inferential error occurs when the probability that a suspect is the source of some evidence -- such as a fingerprint -- is taken as the probability of guilt. This is known as the ultimate issue error, and the same error occurs in statistical inference when the probability that a parameter equals some value is incorrectly taken to be the probability of a hypothesis. Almost all statistical inference in the social and biological sciences is subject to this error, and replacing every instance of "hypothesis testing" with "parameter testing" in these fields would more accurately describe the target of inference. The relationship between parameter values and quantities derived from them, such as p-values or Bayes factors, have no direct quantitative relationship with scientific hypotheses. Here, we describe the problem, its consequences, and suggest options for improving scientific inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15398v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanley E. Lazic</dc:creator>
    </item>
    <item>
      <title>Asymmetric Errors</title>
      <link>https://arxiv.org/abs/2411.15499</link>
      <description>arXiv:2411.15499v1 Announce Type: new 
Abstract: We present a procedure for handling asymmetric errors. Many results in particle physics are presented as values with different positive and negative errors, and there is no consistent procedure for handling them. We consider the difference between errors quoted using pdfs and using likelihoods, and the difference between the rms spread of a measurement and the 68\% central confidence region. We provide a comprehensive analysis of the possibilities, and software tools to enable their use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15499v1</guid>
      <category>stat.ME</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roger Barlow, Alessandra Brazzale, Igor Volobouev</dc:creator>
    </item>
    <item>
      <title>Forecasting with Markovian max-stable fields in space and time: An application to wind gust speeds</title>
      <link>https://arxiv.org/abs/2411.15511</link>
      <description>arXiv:2411.15511v1 Announce Type: new 
Abstract: Hourly maxima of 3-second wind gust speeds are prominent indicators of the severity of wind storms, and accurately forecasting them is thus essential for populations, civil authorities and insurance companies. Space-time max-stable models appear as natural candidates for this, but those explored so far are not suited for forecasting and, more generally, the forecasting literature for max-stable fields is limited. To fill this gap, we consider a specific space-time max-stable model, more precisely a max-autoregressive model with advection, that is well-adapted to model and forecast atmospheric variables. We apply it, as well as our related forecasting strategy, to reanalysis 3-second wind gust data for France in 1999, and show good performance compared to a competitor model. On top of demonstrating the practical relevance of our model, we meticulously study its theoretical properties and show the consistency and asymptotic normality of the space-time pairwise likelihood estimator which is used to calibrate the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15511v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Cotsakis, Erwan Koch, Christian-Yann Robert</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis on Interaction Effects of Policy-Augmented Bayesian Networks</title>
      <link>https://arxiv.org/abs/2411.15566</link>
      <description>arXiv:2411.15566v1 Announce Type: new 
Abstract: Biomanufacturing plays an important role in supporting public health and the growth of the bioeconomy. Modeling and studying the interaction effects among various input variables is very critical for obtaining a scientific understanding and process specification in biomanufacturing. In this paper, we use the ShapleyOwen indices to measure the interaction effects for the policy-augmented Bayesian network (PABN) model, which characterizes the risk- and science-based understanding of production bioprocess mechanisms. In order to facilitate efficient interaction effect quantification, we propose a sampling-based simulation estimation framework. In addition, to further improve the computational efficiency, we develop a non-nested simulation algorithm with sequential sampling, which can dynamically allocate the simulation budget to the interactions with high uncertainty and therefore estimate the interaction effects more accurately under a total fixed budget setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15566v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junkai Zhao, Jun Luo, Wei Xie, Zixuan Bai</dc:creator>
    </item>
    <item>
      <title>Canonical Correlation Analysis: review</title>
      <link>https://arxiv.org/abs/2411.15625</link>
      <description>arXiv:2411.15625v1 Announce Type: new 
Abstract: For over a century canonical correlations, variables, and related concepts have been studied across various fields, with contributions dating back to Jordan [1875] and Hotelling [1936]. This text surveys the evolution of canonical correlation analysis, a fundamental statistical tool, beginning with its foundational theorems and progressing to recent developments and open research problems. Along the way we introduce and review methods, notions, and fundamental concepts from linear algebra, random matrix theory, and high-dimensional statistics, placing particular emphasis on rigorous mathematical treatment.
  The survey is intended for technically proficient graduate students and other researchers with an interest in this area. The content is organized into five chapters, supplemented by six sets of exercises found in Chapter 6. These exercises introduce additional material, reinforce key concepts, and serve to bridge ideas across chapters. We recommend the following sequence: first, solve Problem Set 0, then proceed with Chapter 1, solve Problem Set 1, and so on through the text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15625v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Bykhovskaya, Vadim Gorin</dc:creator>
    </item>
    <item>
      <title>Data integration using covariate summaries from external sources</title>
      <link>https://arxiv.org/abs/2411.15691</link>
      <description>arXiv:2411.15691v1 Announce Type: new 
Abstract: In modern data analysis, information is frequently collected from multiple sources, often leading to challenges such as data heterogeneity and imbalanced sample sizes across datasets. Robust and efficient data integration methods are crucial for improving the generalization and transportability of statistical findings. In this work, we address scenarios where, in addition to having full access to individualized data from a primary source, supplementary covariate information from external sources is also available. While traditional data integration methods typically require individualized covariates from external sources, such requirements can be impractical due to limitations related to accessibility, privacy, storage, and cost. Instead, we propose novel data integration techniques that rely solely on external summary statistics, such as sample means and covariances, to construct robust estimators for the mean outcome under both homogeneous and heterogeneous data settings. Additionally, we extend this framework to causal inference, enabling the estimation of average treatment effects for both generalizability and transportability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15691v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Facheng Yu, Yuqian Zhang</dc:creator>
    </item>
    <item>
      <title>Bayesian High-dimensional Grouped-regression using Sparse Projection-posterior</title>
      <link>https://arxiv.org/abs/2411.15713</link>
      <description>arXiv:2411.15713v1 Announce Type: new 
Abstract: We present a novel Bayesian approach for high-dimensional grouped regression under sparsity. We leverage a sparse projection method that uses a sparsity-inducing map to derive an induced posterior on a lower-dimensional parameter space. Our method introduces three distinct projection maps based on popular penalty functions: the Group LASSO Projection Posterior, Group SCAD Projection Posterior, and Adaptive Group LASSO Projection Posterior. Each projection map is constructed to immerse dense posterior samples into a structured, sparse space, allowing for effective group selection and estimation in high-dimensional settings. We derive optimal posterior contraction rates for estimation and prediction, proving that the methods are model selection consistent. Additionally, we propose a Debiased Group LASSO Projection Map, which ensures exact coverage of credible sets. Our methodology is particularly suited for applications in nonparametric additive models, where we apply it with B-spline expansions to capture complex relationships between covariates and response. Extensive simulations validate our theoretical findings, demonstrating the robustness of our approach across different settings. Finally, we illustrate the practical utility of our method with an application to brain MRI volume data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), where our model identifies key brain regions associated with Alzheimer's progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15713v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samhita Pal, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>A flexible and general semi-supervised approach to multiple hypothesis testing</title>
      <link>https://arxiv.org/abs/2411.15771</link>
      <description>arXiv:2411.15771v1 Announce Type: new 
Abstract: Standard multiple testing procedures are designed to report a list of discoveries, or suspected false null hypotheses, given the hypotheses' p-values or test scores. Recently there has been a growing interest in enhancing such procedures by combining additional information with the primary p-value or score. Specifically, such so-called ``side information'' can be leveraged to improve the separation between true and false nulls along additional ``dimensions'' thereby increasing the overall sensitivity. In line with this idea, we develop RESET (REScoring via Estimating and Training) which uses a unique data-splitting protocol that subsequently allows any semi-supervised learning approach to factor in the available side-information while maintaining finite-sample error rate control. Our practical implementation, RESET Ensemble, selects from an ensemble of classification algorithms so that it is compatible to a range of multiple testing scenarios without the need for the user to select the appropriate one. We apply RESET to both p-value and competition based multiple testing problems and show that RESET is (1) power-wise competitive, (2) fast compared to most tools and (3) is able to uniquely achieve finite sample FDR or FDP control, depending on the user's preference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15771v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack Freestone, William Stafford Noble, Uri Keich</dc:creator>
    </item>
    <item>
      <title>A Copula-Based Approach to Modelling and Testing for Heavy-tailed Data with Bivariate Heteroscedastic Extremes</title>
      <link>https://arxiv.org/abs/2411.15819</link>
      <description>arXiv:2411.15819v1 Announce Type: new 
Abstract: Heteroscedasticity and correlated data pose challenges for extreme value analysis, particularly in two-sample testing problems for tail behaviors. In this paper, we propose a novel copula-based multivariate model for independent but not identically distributed heavy-tailed data with heterogeneous marginal distributions and a varying copula structure. The proposed model encompasses classical models with independent and identically distributed data and some models with a mixture of correlation. To understand the tail behavior, we introduce the quasi-tail copula, which integrates both marginal heteroscedasticity and the dependence structure of the varying copula, and further propose the estimation approach. We then establish the joint asymptotic properties for the Hill estimator, scedasis functions, and quasi-tail copula. In addition, a multiplier bootstrap method is applied to estimate their complex covariance. Moreover, it is of practical interest to develop four typical two-sample testing problems under the new model, which include the equivalence of the extreme value indices and scedasis functions. Finally, we conduct simulation studies to validate our tests and apply the new model to the data from the stock market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15819v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Hu, Yanxi Hou</dc:creator>
    </item>
    <item>
      <title>Semi-parametric least-area linear-circular regression through M\"obius transformation</title>
      <link>https://arxiv.org/abs/2411.15822</link>
      <description>arXiv:2411.15822v1 Announce Type: new 
Abstract: This paper introduces a new area-based regression model where the responses are angular variables and the predictors are linear. The regression curve is formulated using a generalized M\"obius transformation that maps the real axis to the circle. A novel area-based loss function is introduced for parameter estimation, utilizing the intrinsic geometry of a curved torus. The model is semi-parametric, requiring no specific distributional assumptions for the angular error. Extensive simulation studies are performed with von Mises and wrapped Cauchy distributions as angular errors. The practical utility of the model is illustrated through real data analysis of two well-known cryptocurrencies, Bitcoin and Ethereum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15822v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Surojit Biswas, Buddhananda Banerjee</dc:creator>
    </item>
    <item>
      <title>Expert-elicitation method for non-parametric joint priors using normalizing flows</title>
      <link>https://arxiv.org/abs/2411.15826</link>
      <description>arXiv:2411.15826v1 Announce Type: new 
Abstract: We propose an expert-elicitation method for learning non-parametric joint prior distributions using normalizing flows. Normalizing flows are a class of generative models that enable exact, single-step density evaluation and can capture complex density functions through specialized deep neural networks. Building on our previously introduced simulation-based framework, we adapt and extend the methodology to accommodate non-parametric joint priors. Our framework thus supports the development of elicitation methods for learning both parametric and non-parametric priors, as well as independent or joint priors for model parameters. To evaluate the performance of the proposed method, we perform four simulation studies and present an evaluation pipeline that incorporates diagnostics and additional evaluation tools to support decision-making at each stage of the elicitation process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15826v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Florence Bockting, Stefan T. Radev, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Selective Inference for Time-Varying Effect Moderation</title>
      <link>https://arxiv.org/abs/2411.15908</link>
      <description>arXiv:2411.15908v1 Announce Type: new 
Abstract: Causal effect moderation investigates how the effect of interventions (or treatments) on outcome variables changes based on observed characteristics of individuals, known as potential effect moderators. With advances in data collection, datasets containing many observed features as potential moderators have become increasingly common. High-dimensional analyses often lack interpretability, with important moderators masked by noise, while low-dimensional, marginal analyses yield many false positives due to strong correlations with true moderators. In this paper, we propose a two-step method for selective inference on time-varying causal effect moderation that addresses the limitations of both high-dimensional and marginal analyses. Our method first selects a relatively smaller, more interpretable model to estimate a linear causal effect moderation using a Gaussian randomization approach. We then condition on the selection event to construct a pivot, enabling uniformly asymptotic semi-parametric inference in the selected model. Through simulations and real data analyses, we show that our method consistently achieves valid coverage rates, even when existing conditional methods and common sample splitting techniques fail. Moreover, our method yields shorter, bounded intervals, unlike existing methods that may produce infinitely long intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15908v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soham Bakshi, Walter Dempsey, Snigdha Panigrahi</dc:creator>
    </item>
    <item>
      <title>Analysis of longitudinal data with destructive sampling using linear mixed models</title>
      <link>https://arxiv.org/abs/2411.16153</link>
      <description>arXiv:2411.16153v1 Announce Type: new 
Abstract: This paper proposes an analysis methodology for the case where there is longitudinal data with destructive sampling of observational units, which come from experimental units that are measured at all times of the analysis. A mixed linear model is proposed and compared with regression models with fixed and mixed effects, among which is a similar that is used for data called pseudo-panel, and one of multivariate analysis of variance, which are common in statistics. To compare the models, the mean square error was used, demonstrating the advantage of the proposed methodology. In addition, an application was made to real-life data that refers to the scores in the Saber 11 tests applied to students in Colombia to see the advantage of using this methodology in practical scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16153v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>C. A. Avellaneda, O. O. Melo, N. A. Cruz</dc:creator>
    </item>
    <item>
      <title>Modeling large dimensional matrix time series with partially known and latent factors</title>
      <link>https://arxiv.org/abs/2411.16192</link>
      <description>arXiv:2411.16192v1 Announce Type: new 
Abstract: This article considers to model large-dimensional matrix time series by introducing a regression term to the matrix factor model. This is an extension of classic matrix factor model to incorporate the information of known factors or useful covariates. We establish the convergence rates of coefficient matrix, loading matrices and the signal part. The theoretical results coincide with the rates in Wang et al. (2019). We conduct numerical studies to verify the performance of our estimation procedure in finite samples. Finally, we demonstrate the superiority of our proposed model using the daily returns of stocks data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16192v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongchang Hui, Yuteng Zhang, Siting Huang</dc:creator>
    </item>
    <item>
      <title>On the achievability of efficiency bounds for covariate-adjusted response-adaptive randomization</title>
      <link>https://arxiv.org/abs/2411.16220</link>
      <description>arXiv:2411.16220v1 Announce Type: new 
Abstract: In the context of precision medicine, covariate-adjusted response-adaptive randomization (CARA) has garnered much attention from both academia and industry due to its benefits in providing ethical and tailored treatment assignments based on patients' profiles while still preserving favorable statistical properties. Recent years have seen substantial progress in understanding the inference for various adaptive experimental designs. In particular, research has focused on two important perspectives: how to obtain robust inference in the presence of model misspecification, and what the smallest variance, i.e., the efficiency bound, an estimator can achieve. Notably, Armstrong (2022) derived the asymptotic efficiency bound for any randomization procedure that assigns treatments depending on covariates and accrued responses, thus including CARA, among others. However, to the best of our knowledge, no existing literature has addressed whether and how the asymptotic efficiency bound can be achieved under CARA. In this paper, by connecting two strands of literature on adaptive randomization, namely robust inference and efficiency bound, we provide a definitive answer to this question for an important practical scenario where only discrete covariates are observed and used to form stratification. We consider a specific type of CARA, i.e., a stratified version of doubly-adaptive biased coin design, and prove that the stratified difference-in-means estimator achieves Armstrong (2022)'s efficiency bound, with possible ethical constraints on treatment assignments. Our work provides new insights and demonstrates the potential for more research regarding the design and analysis of CARA that maximizes efficiency while adhering to ethical considerations. Future studies could explore how to achieve the asymptotic efficiency bound for general CARA with continuous covariates, which remains an open question.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16220v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiahui Xin, Wei Ma</dc:creator>
    </item>
    <item>
      <title>Bayesian models for missing and misclassified variables using integrated nested Laplace approximations</title>
      <link>https://arxiv.org/abs/2411.16311</link>
      <description>arXiv:2411.16311v1 Announce Type: new 
Abstract: Misclassified variables used in regression models, either as a covariate or as the response, may lead to biased estimators and incorrect inference. Even though Bayesian models to adjust for misclassification error exist, it has not been shown how these models can be implemented using integrated nested Laplace approximation (INLA), a popular framework for fitting Bayesian models due to its computational efficiency. Since INLA requires the latent field to be Gaussian, and the Bayesian models adjusting for covariate misclassification error necessarily introduce a latent categorical variable, it is not obvious how to fit these models in INLA. Here, we show how INLA can be combined with importance sampling to overcome this limitation. We also discuss how to account for a misclassified response variable using INLA directly without any additional sampling procedure. The proposed methods are illustrated through a number of simulations and applications to real-world data, and all examples are presented with detailed code in the supporting information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16311v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Skarstein, Leonardo Soares Bastos, H{\aa}vard Rue, Stefanie Muff</dc:creator>
    </item>
    <item>
      <title>Multivariate Adjustments for Average Equivalence Testing</title>
      <link>https://arxiv.org/abs/2411.16429</link>
      <description>arXiv:2411.16429v1 Announce Type: new 
Abstract: Multivariate (average) equivalence testing is widely used to assess whether the means of two conditions of interest are `equivalent' for different outcomes simultaneously. The multivariate Two One-Sided Tests (TOST) procedure is typically used in this context by checking if, outcome by outcome, the marginal $100(1-2\alpha$)\% confidence intervals for the difference in means between the two conditions of interest lie within pre-defined lower and upper equivalence limits. This procedure, known to be conservative in the univariate case, leads to a rapid power loss when the number of outcomes increases, especially when one or more outcome variances are relatively large. In this work, we propose a finite-sample adjustment for this procedure, the multivariate $\alpha$-TOST, that consists in a correction of $\alpha$, the significance level, taking the (arbitrary) dependence between the outcomes of interest into account and making it uniformly more powerful than the conventional multivariate TOST. We present an iterative algorithm allowing to efficiently define $\alpha^{\star}$, the corrected significance level, a task that proves challenging in the multivariate setting due to the inter-relationship between $\alpha^{\star}$ and the sets of values belonging to the null hypothesis space and defining the test size. We study the operating characteristics of the multivariate $\alpha$-TOST both theoretically and via an extensive simulation study considering cases relevant for real-world analyses -- i.e.,~relatively small sample sizes, unknown and heterogeneous variances, and different correlation structures -- and show the superior finite-sample properties of the multivariate $\alpha$-TOST compared to its conventional counterpart. We finally re-visit a case study on ticlopidine hydrochloride and compare both methods when simultaneously assessing bioequivalence for multiple pharmacokinetic parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16429v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Younes Boulaguiem, Luca Insolia, Maria-Pia Victoria-Feser, Dominique-Laurent Couturier, St\'ephane Guerrier</dc:creator>
    </item>
    <item>
      <title>Heavy-tailed Contamination is Easier than Adversarial Contamination</title>
      <link>https://arxiv.org/abs/2411.15306</link>
      <description>arXiv:2411.15306v1 Announce Type: cross 
Abstract: A large body of work in the statistics and computer science communities dating back to Huber (Huber, 1960) has led to statistically and computationally efficient outlier-robust estimators. Two particular outlier models have received significant attention: the adversarial and heavy-tailed models. While the former models outliers as the result of a malicious adversary manipulating the data, the latter relaxes distributional assumptions on the data allowing outliers to naturally occur as part of the data generating process. In the first setting, the goal is to develop estimators robust to the largest fraction of outliers while in the second, one seeks estimators to combat the loss of statistical efficiency, where the dependence on the failure probability is paramount.
  Despite these distinct motivations, the algorithmic approaches to both these settings have converged, prompting questions on the relationship between the models. In this paper, we investigate and provide a principled explanation for this phenomenon. First, we prove that any adversarially robust estimator is also resilient to heavy-tailed outliers for any statistical estimation problem with i.i.d data. As a corollary, optimal adversarially robust estimators for mean estimation, linear regression, and covariance estimation are also optimal heavy-tailed estimators. Conversely, for arguably the simplest high-dimensional estimation task of mean estimation, we construct heavy-tailed estimators whose application to the adversarial setting requires any black-box reduction to remove almost all the outliers in the data. Taken together, our results imply that heavy-tailed estimation is likely easier than adversarially robust estimation opening the door to novel algorithmic approaches for the heavy-tailed setting. Additionally, confidence intervals obtained for adversarially robust estimation also hold with high-probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15306v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yeshwanth Cherapanamjeri, Daniel Lee</dc:creator>
    </item>
    <item>
      <title>From Complexity to Parsimony: Integrating Latent Class Analysis to Uncover Multimodal Learning Patterns in Collaborative Learning</title>
      <link>https://arxiv.org/abs/2411.15590</link>
      <description>arXiv:2411.15590v1 Announce Type: cross 
Abstract: Multimodal Learning Analytics (MMLA) leverages advanced sensing technologies and artificial intelligence to capture complex learning processes, but integrating diverse data sources into cohesive insights remains challenging. This study introduces a novel methodology for integrating latent class analysis (LCA) within MMLA to map monomodal behavioural indicators into parsimonious multimodal ones. Using a high-fidelity healthcare simulation context, we collected positional, audio, and physiological data, deriving 17 monomodal indicators. LCA identified four distinct latent classes: Collaborative Communication, Embodied Collaboration, Distant Interaction, and Solitary Engagement, each capturing unique monomodal patterns. Epistemic network analysis compared these multimodal indicators with the original monomodal indicators and found that the multimodal approach was more parsimonious while offering higher explanatory power regarding students' task and collaboration performances. The findings highlight the potential of LCA in simplifying the analysis of complex multimodal data while capturing nuanced, cross-modality behaviours, offering actionable insights for educators and enhancing the design of collaborative learning interventions. This study proposes a pathway for advancing MMLA, making it more parsimonious and manageable, and aligning with the principles of learner-centred education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15590v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lixiang Yan, Dragan Ga\v{s}evi\'c, Linxuan Zhao, Vanessa Echeverria, Yueqiao Jin, Roberto Martinez-Maldonado</dc:creator>
    </item>
    <item>
      <title>Trans-Glasso: A Transfer Learning Approach to Precision Matrix Estimation</title>
      <link>https://arxiv.org/abs/2411.15624</link>
      <description>arXiv:2411.15624v1 Announce Type: cross 
Abstract: Precision matrix estimation is essential in various fields, yet it is challenging when samples for the target study are limited. Transfer learning can enhance estimation accuracy by leveraging data from related source studies. We propose Trans-Glasso, a two-step transfer learning method for precision matrix estimation. First, we obtain initial estimators using a multi-task learning objective that captures shared and unique features across studies. Then, we refine these estimators through differential network estimation to adjust for structural differences between the target and source precision matrices. Under the assumption that most entries of the target precision matrix are shared with source matrices, we derive non-asymptotic error bounds and show that Trans-Glasso achieves minimax optimality under certain conditions. Extensive simulations demonstrate Trans Glasso's superior performance compared to baseline methods, particularly in small-sample settings. We further validate Trans-Glasso in applications to gene networks across brain tissues and protein networks for various cancer subtypes, showcasing its effectiveness in biological contexts. Additionally, we derive the minimax optimal rate for differential network estimation, representing the first such guarantee in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15624v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boxin Zhao, Cong Ma, Mladen Kolar</dc:creator>
    </item>
    <item>
      <title>Quantile deep learning models for multi-step ahead time series prediction</title>
      <link>https://arxiv.org/abs/2411.15674</link>
      <description>arXiv:2411.15674v1 Announce Type: cross 
Abstract: Uncertainty quantification is crucial in time series prediction, and quantile regression offers a valuable mechanism for uncertainty quantification which is useful for extreme value forecasting. Although deep learning models have been prominent in multi-step ahead prediction, the development and evaluation of quantile deep learning models have been limited. We present a novel quantile regression deep learning framework for multi-step time series prediction. In this way, we elevate the capabilities of deep learning models by incorporating quantile regression, thus providing a more nuanced understanding of predictive values. We provide an implementation of prominent deep learning models for multi-step ahead time series prediction and evaluate their performance under high volatility and extreme conditions. We include multivariate and univariate modelling, strategies and provide a comparison with conventional deep learning models from the literature. Our models are tested on two cryptocurrencies: Bitcoin and Ethereum, using daily close-price data and selected benchmark time series datasets. The results show that integrating a quantile loss function with deep learning provides additional predictions for selected quantiles without a loss in the prediction accuracy when compared to the literature. Our quantile model has the ability to handle volatility more effectively and provides additional information for decision-making and uncertainty quantification through the use of quantiles when compared to conventional deep learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15674v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jimmy Cheung, Smruthi Rangarajan, Amelia Maddocks, Xizhe Chen, Rohitash Chandra</dc:creator>
    </item>
    <item>
      <title>Connections between sequential Bayesian inference and evolutionary dynamics</title>
      <link>https://arxiv.org/abs/2411.16366</link>
      <description>arXiv:2411.16366v1 Announce Type: cross 
Abstract: It has long been posited that there is a connection between the dynamical equations describing evolutionary processes in biology and sequential Bayesian learning methods. This manuscript describes new research in which this precise connection is rigorously established in the continuous time setting. Here we focus on a partial differential equation known as the Kushner-Stratonovich equation describing the evolution of the posterior density in time. Of particular importance is a piecewise smooth approximation of the observation path from which the discrete time filtering equations, which are shown to converge to a Stratonovich interpretation of the Kushner-Stratonovich equation. This smooth formulation will then be used to draw precise connections between nonlinear stochastic filtering and replicator-mutator dynamics. Additionally, gradient flow formulations will be investigated as well as a form of replicator-mutator dynamics which is shown to be beneficial for the misspecified model filtering problem. It is hoped this work will spur further research into exchanges between sequential learning and evolutionary biology and to inspire new algorithms in filtering and sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16366v1</guid>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahani Pathiraja, Philipp Wacker</dc:creator>
    </item>
    <item>
      <title>Sequential adaptive design for emulating costly computer codes</title>
      <link>https://arxiv.org/abs/2206.12113</link>
      <description>arXiv:2206.12113v4 Announce Type: replace 
Abstract: Gaussian processes (GPs) are generally regarded as the gold standard surrogate model for emulating computationally expensive computer-based simulators. However, the problem of training GPs as accurately as possible with a minimum number of model evaluations remains challenging. We address this problem by suggesting a novel adaptive sampling criterion called VIGF (variance of improvement for global fit). The improvement function at any point is a measure of the deviation of the GP emulator from the nearest observed model output. At each iteration of the proposed algorithm, a new run is performed where VIGF is the largest. Then, the new sample is added to the design and the emulator is updated accordingly. A batch version of VIGF is also proposed which can save the user time when parallel computing is available. Additionally, VIGF is extended to the multi-fidelity case where the expensive high-fidelity model is predicted with the assistance of a lower fidelity simulator. This is performed via hierarchical kriging. The applicability of our method is assessed on a bunch of test functions and its performance is compared with several sequential sampling strategies. The results suggest that our method has a superior performance in predicting the benchmark functions in most cases. An implementation of VIGF is available in the dgpsi R package, which can be found on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.12113v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hossein Mohammadi, Peter Challenor</dc:creator>
    </item>
    <item>
      <title>Improving knockoffs with conditional calibration</title>
      <link>https://arxiv.org/abs/2208.09542</link>
      <description>arXiv:2208.09542v3 Announce Type: replace 
Abstract: The knockoff filter of Barber and Candes (arXiv:1404.5609) is a flexible framework for multiple testing in supervised learning models, based on introducing synthetic predictor variables to control the false discovery rate (FDR). Using the conditional calibration framework of Fithian and Lei (arXiv:2007.10438), we introduce the calibrated knockoff procedure, a method that uniformly improves the power of any fixed-X or model-X knockoff procedure. We show theoretically and empirically that the improvement is especially notable in two contexts where knockoff methods can be nearly powerless: when the rejection set is small, and when the structure of the design matrix in fixed-X knockoffs prevents us from constructing good knockoff variables. In these contexts, calibrated knockoffs even outperform competing FDR-controlling methods like the (dependence-adjusted) procedure Benjamini-Hochberg in many scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.09542v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiang Luo, William Fithian, Lihua Lei</dc:creator>
    </item>
    <item>
      <title>Posterior risk of modular and semi-modular Bayesian inference</title>
      <link>https://arxiv.org/abs/2301.10911</link>
      <description>arXiv:2301.10911v3 Announce Type: replace 
Abstract: Modular Bayesian methods perform inference in models that are specified through a collection of coupled sub-models, known as modules. These modules often arise from modelling different data sources or from combining domain knowledge from different disciplines. ``Cutting feedback'' is a Bayesian inference method that ensures misspecification of one module does not affect inferences for parameters in other modules, and produces what is known as the cut posterior. However, choosing between the cut posterior and the standard Bayesian posterior is challenging. When misspecification is not severe, cutting feedback can greatly increase posterior uncertainty without a large reduction of estimation bias, leading to a bias-variance trade-off. This trade-off motivates semi-modular posteriors, which interpolate between standard and cut posteriors based on a tuning parameter. In this work, we provide the first precise formulation of the bias-variance trade-off that is present in cutting feedback, and we propose a new semi-modular posterior that takes advantage of it. Under general regularity conditions, we prove that this semi-modular posterior is more accurate than the cut posterior according to a notion of posterior risk. An important implication of this result is that point inferences made under the cut posterior are inadmissable. The new method is demonstrated in a number of examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.10911v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David T. Frazier, David J. Nott</dc:creator>
    </item>
    <item>
      <title>Choosing the Right Approach at the Right Time: A Comparative Analysis of Causal Effect Estimation using Confounder Adjustment and Instrumental Variables</title>
      <link>https://arxiv.org/abs/2307.11201</link>
      <description>arXiv:2307.11201v3 Announce Type: replace 
Abstract: In observational studies, potential unobserved confounding is a major barrier in isolating the average causal effect (ACE). In these scenarios, two main approaches are often used: confounder adjustment for causality (CAC) and instrumental variable analysis for causation (IVAC). Nevertheless, both are subject to untestable assumptions and, therefore, it may be unclear which assumption violation scenarios one method is superior in terms of mitigating inconsistency for the ACE. Although general guidelines exist, direct theoretical comparisons of the trade-offs between CAC and the IVAC assumptions are limited. Using ordinary least squares (OLS) for CAC and two-stage least squares (2SLS) for IVAC, we analytically compare the relative inconsistency for the ACE of each approach under a variety of assumption violation scenarios and discuss rules of thumb for practice. Additionally, a sensitivity framework is proposed to guide analysts in determining which approach may result in less inconsistency for estimating the ACE with a given dataset. We demonstrate our findings both through simulation and by revisiting Card's analysis of the effect of educational attainment on earnings, which has been the subject of previous discussion on instrument validity. The implications of our findings on causal inference practice are discussed, providing guidance for analysts to judge whether CAC or IVAC may be more appropriate for a given situation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11201v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roy S. Zawadzki, Daniel L. Gillen</dc:creator>
    </item>
    <item>
      <title>Causally Sound Priors for Binary Experiments</title>
      <link>https://arxiv.org/abs/2308.13713</link>
      <description>arXiv:2308.13713v3 Announce Type: replace 
Abstract: We introduce the BREASE framework for the Bayesian analysis of randomized controlled trials with a binary treatment and a binary outcome. Approaching the problem from a causal inference perspective, we propose parameterizing the likelihood in terms of the baselinerisk, efficacy, and adverse side effects of the treatment, along with a flexible, yet intuitive and tractable jointly independent beta prior distribution on these parameters, which we show to be a generalization of the Dirichlet prior for the joint distribution of potential outcomes. Our approach has a number of desirable characteristics when compared to current mainstream alternatives: (i) it naturally induces prior dependence between expected outcomes in the treatment and control groups; (ii) as the baseline risk, efficacy and risk of adverse side effects are quantities commonly present in the clinicians' vocabulary, the hyperparameters of the prior are directly interpretable, thus facilitating the elicitation of prior knowledge and sensitivity analysis; and (iii) we provide analytical formulae for the marginal likelihood, Bayes factor, and other posterior quantities, as well as an exact posterior sampling algorithm and an accurate and fast data-augmented Gibbs sampler in cases where traditional MCMC fails. Empirical examples demonstrate the utility of our methods for estimation, hypothesis testing, and sensitivity analysis of treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13713v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas J. Irons, Carlos Cinelli</dc:creator>
    </item>
    <item>
      <title>Causal progress with imperfect placebo treatments and outcomes</title>
      <link>https://arxiv.org/abs/2310.15266</link>
      <description>arXiv:2310.15266v3 Announce Type: replace 
Abstract: In the quest to make defensible causal claims from observational data, it is sometimes possible to leverage information from "placebo treatments" and "placebo outcomes". Existing approaches employing such information focus largely on point identification and assume (i) "perfect placebos", meaning placebo treatments have precisely zero effect on the outcome and the real treatment has precisely zero effect on a placebo outcome; and (ii) "equiconfounding", meaning that the treatment-outcome relationship where one is a placebo suffers the same amount of confounding as does the real treatment-outcome relationship, on some scale. We instead consider an omitted variable bias framework, in which users can postulate ranges of values for the degree of unequal confounding and the degree of placebo imperfection. Once postulated, these assumptions identify or bound the linear estimates of treatment effects. Our approach also does not require using both a placebo treatment and placebo outcome, as some others do. While applicable in many settings, one ubiquitous use-case for this approach is to employ pre-treatment outcomes as (perfect) placebo outcomes, as in difference-in-difference. The parallel trends assumption in this setting is identical to the equiconfounding assumption, on a particular scale, which our framework allows the user to relax. Finally, we demonstrate the use of our framework with two applications and a simulation, employing an R package that implements these approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15266v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Rohde, Chad Hazlett</dc:creator>
    </item>
    <item>
      <title>Optimal Functional Bilinear Regression with Two-way Functional Covariates via Reproducing Kernel Hilbert Space</title>
      <link>https://arxiv.org/abs/2311.12597</link>
      <description>arXiv:2311.12597v2 Announce Type: replace 
Abstract: Traditional functional linear regression usually takes a one-dimensional functional predictor as input and estimates the continuous coefficient function. Modern applications often generate two-dimensional covariates, which become matrices when observed at grid points. To avoid the inefficiency of the classical method involving estimation of a two-dimensional coefficient function, we propose a functional bilinear regression model, and introduce an innovative three-term penalty to impose roughness penalty in the estimation. The proposed estimator exhibits minimax optimal property for prediction under the framework of reproducing kernel Hilbert space. An iterative generalized cross-validation approach is developed to choose tuning parameters, which significantly improves the computational efficiency over the traditional cross-validation approach. The statistical and computational advantages of the proposed method over existing methods are further demonstrated via simulated experiments, the Canadian weather data, and a biochemical long-range infrared light detection and ranging data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12597v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Yang, Jianlong Shao, Haipeng Shen, Hongtu Zhu</dc:creator>
    </item>
    <item>
      <title>Robust Point Matching with Distance Profiles</title>
      <link>https://arxiv.org/abs/2312.12641</link>
      <description>arXiv:2312.12641v3 Announce Type: replace 
Abstract: We show the outlier robustness and noise stability of practical matching procedures based on distance profiles. Although the idea of matching points based on invariants like distance profiles has a long history in the literature, there has been little understanding of the theoretical properties of such procedures, especially in the presence of outliers and noise. We provide a theoretical analysis showing that under certain probabilistic settings, the proposed matching procedure is successful with high probability even in the presence of outliers and noise. We demonstrate the performance of the proposed method using a real data example and provide simulation studies to complement the theoretical findings. Lastly, we extend the concept of distance profiles to the abstract setting and connect the proposed matching procedure to the Gromov-Wasserstein distance and its lower bound, with a new sample complexity result derived based on the properties of distance profiles. As a result, we contribute to the literature by providing theoretical underpinnings of the matching procedures based on invariants like distance profiles, which have been widely used in practice but have rarely been analyzed theoretically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12641v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>YoonHaeng Hur, Yuehaw Khoo</dc:creator>
    </item>
    <item>
      <title>The Cox-Polya-Gamma Algorithm for Flexible Bayesian Inference of Multilevel Survival Models</title>
      <link>https://arxiv.org/abs/2402.15060</link>
      <description>arXiv:2402.15060v2 Announce Type: replace 
Abstract: Bayesian Cox semiparametric regression is an important problem in many clinical settings. Bayesian procedures provide finite-sample inference and naturally incorporate prior information if MCMC algorithms and posteriors are well behaved. Survival analysis should also be able to incorporate multilevel modeling such as case weights, frailties and smoothing splines, in a straightforward manner. To tackle these modeling challenges, we propose the Cox-Polya-Gamma (Cox-PG) algorithm for Bayesian multilevel Cox semiparametric regression and survival functions. Our novel computational procedure succinctly addresses the difficult problem of monotonicity constrained modeling of the nonparametric baseline cumulative hazard along with multilevel regression. We develop two key strategies. First, we exploit an approximation between Cox models and negative binomial processes through the Poisson process to reduce Bayesian computation to iterative Gaussian sampling. Next, we appeal to sufficient dimension reduction to address the difficult computation of nonparametric baseline cumulative hazard, allowing for the collapse of the Markov transition within the Gibbs sampler based on beta sufficient statistics. In addition, we explore conditions for uniform ergodicity of the Cox-PG algorithm. We demonstrate our multilevel modeling approach using open source data and simulations. We provide software for our Bayesian procedure in the supplement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15060v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Benny Ren, Jeffrey Morris, Ian Barnett</dc:creator>
    </item>
    <item>
      <title>Principal stratification with U-statistics under principal ignorability</title>
      <link>https://arxiv.org/abs/2403.08927</link>
      <description>arXiv:2403.08927v3 Announce Type: replace 
Abstract: Principal stratification is a popular framework for causal inference in the presence of an intermediate outcome. While the principal average treatment effects have traditionally been the default target of inference, it may not be sufficient when the interest lies in the relative favorability of one potential outcome over the other within the principal stratum. We thus introduce the principal generalized causal effect estimands, which extend the principal average causal effects to accommodate nonlinear contrast functions. Under principal ignorability, we expand the theoretical results in Jiang et al.(2022) to a much wider class of causal estimands in the presence of a binary intermediate variable. We develop identification formulas and derive the efficient influence functions of the generalized estimands for principal stratification analyses. These efficient influence functions motivate a set of multiply robust estimators and lay the ground for obtaining efficient debiased machine learning estimators via cross-fitting based on U-statistics. The proposed methods are illustrated through simulations and the analysis of a data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08927v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Fan Li</dc:creator>
    </item>
    <item>
      <title>Covariance-free Bi-fidelity Control Variates Importance Sampling for Rare Event Reliability Analysis</title>
      <link>https://arxiv.org/abs/2405.03834</link>
      <description>arXiv:2405.03834v2 Announce Type: replace 
Abstract: Multifidelity modeling has been steadily gaining attention as a tool to address the problem of exorbitant model evaluation costs that makes the estimation of failure probabilities a significant computational challenge for complex real-world problems, particularly when failure is a rare event. To implement multifidelity modeling, estimators that efficiently combine information from multiple models/sources are necessary. In past works, the variance reduction techniques of Control Variates (CV) and Importance Sampling (IS) have been leveraged for this task. In this paper, we present the CVIS framework; a creative take on a coupled CV and IS estimator for bifidelity reliability analysis. The framework addresses some of the practical challenges of the CV method by using an estimator for the control variate mean and side-stepping the need to estimate the covariance between the original estimator and the control variate through a clever choice for the tuning constant. The task of selecting an efficient IS distribution is also considered, with a view towards maximally leveraging the bifidelity structure and maintaining expressivity. Additionally, a diagnostic is provided that indicates both the efficiency of the algorithm as well as the relative predictive quality of the models utilized. Finally, the behavior and performance of the framework is explored through analytical and numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03834v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Promit Chakroborty (Dept. of Civil,Systems Engg, Johns Hopkins University), Somayajulu L. N. Dhulipala (Idaho National Laboratory), Michael D. Shields (Dept. of Civil,Systems Engg, Johns Hopkins University)</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian Model Averaging for Linear Regression Models With Heavy-Tailed Errors</title>
      <link>https://arxiv.org/abs/2407.16366</link>
      <description>arXiv:2407.16366v2 Announce Type: replace 
Abstract: Our goal is to develop a Bayesian model averaging technique in linear regression models that accommodates heavier tailed error densities than the normal distribution. Motivated by the use of the Huber loss function in the presence of outliers, the Bayesian Huberized lasso with hyperbolic errors has been proposed and recently implemented in the literature (Park and Casella (2008); Kawakami and Hashimoto (2023)). Since the Huberized lasso cannot enforce regression coefficients to be exactly zero, we propose a fully Bayesian variable selection approach with spike and slab priors to address sparsity more effectively. The shapes of the hyperbolic and the Student-t density functions are different. Furthermore, the tails of a hyperbolic distribution are less heavy compared to those of a Cauchy distribution. Thus, we propose a flexible regression model with an error distribution encompassing both the hyperbolic and the Student-t family of distributions, along with an unknown tail heaviness parameter, that is estimated based on the data. It is known that the limiting form of both the hyperbolic and the Student-t distributions is a normal distribution. We develop an efficient Gibbs sampler with Metropolis Hastings steps for posterior computation. Through simulation studies and analyses of real datasets, we show that our method is competitive with various state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16366v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shamriddha De, Joyee Ghosh</dc:creator>
    </item>
    <item>
      <title>Unified Principal Components Analysis of Functional Time Series</title>
      <link>https://arxiv.org/abs/2408.02343</link>
      <description>arXiv:2408.02343v2 Announce Type: replace 
Abstract: Functional time series (FTS) are increasingly available from diverse real-world applications such as finance, traffic, and environmental science. To analyze such data, it is common to perform dimension reduction on FTS, converting serially dependent random functions to vector time series for downstream tasks. Traditional methods like functional principal component analysis (FPCA) and dynamic FPCA (DFPCA) can be employed for the dimension reduction of FTS. However, these methods may either not be theoretically optimal or be too redundant to represent serially dependent functional data. In this article, we introduce a novel dimension reduction method for FTS based on dynamic FPCA. Through a new concept called optimal functional filters, we unify the theories of FPCA and dynamic FPCA, providing a parsimonious and optimal representation for FTS adapting to its serial dependence structure. This framework is referred to as principal analysis via dependency-adaptivity (PADA). Under a hierarchical Bayesian model, we establish an implementation procedure of PADA for dimension reduction and prediction of irregularly observed FTS. We establish the statistical consistency of PADA in achieving parsimonious and optimal dimension reduction and demonstrate its effectiveness through extensive simulation studies. Finally, we apply our method to daily PM2.5 concentration data, validating the effectiveness of PADA for analyzing FTS data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02343v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zerui Guo, Jianbin Tan, Hui Huang</dc:creator>
    </item>
    <item>
      <title>Ridge, lasso, and elastic-net estimations of the modified Poisson and least-squares regressions for binary outcome data</title>
      <link>https://arxiv.org/abs/2408.13474</link>
      <description>arXiv:2408.13474v2 Announce Type: replace 
Abstract: Logistic regression is a standard method in multivariate analysis for binary outcome data in epidemiological and clinical studies; however, the resultant odds-ratio estimates fail to provide directly interpretable effect measures. The modified Poisson and least-squares regressions are alternative standard methods that can provide risk-ratio and risk difference estimates without computational problems. However, the bias and invalid inference problems of these regression analyses under small or sparse data conditions (i.e.,the "separation" problem) have been insufficiently investigated. We show that the separation problem can adversely affect the inferences of the modified Poisson and least squares regressions, and to address these issues, we apply the ridge, lasso, and elastic-net estimating approaches to the two regression methods. As the methods are not founded on the maximum likelihood principle, we propose regularized quasi-likelihood approaches based on the estimating equations for these generalized linear models. The methods provide stable shrinkage estimates of risk ratios and risk differences even under separation conditions, and the lasso and elastic-net approaches enable simultaneous variable selection. We provide a bootstrap method to calculate the confidence intervals on the basis of the regularized quasi-likelihood estimation. The proposed methods are applied to a hematopoietic stem cell transplantation cohort study and the National Child Development Survey. We also provide an R package, regconfint, to implement these methods with simple commands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13474v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takahiro Kitano, Hisashi Noma</dc:creator>
    </item>
    <item>
      <title>Assumption-Lean Post-Integrated Inference with Negative Control Outcomes</title>
      <link>https://arxiv.org/abs/2410.04996</link>
      <description>arXiv:2410.04996v2 Announce Type: replace 
Abstract: Data integration methods aim to extract low-dimensional embeddings from high-dimensional outcomes to remove unwanted variations, such as batch effects and unmeasured covariates, across heterogeneous datasets. However, multiple hypothesis testing after integration can be biased due to data-dependent processes. We introduce a robust post-integrated inference (PII) method that adjusts for latent heterogeneity using negative control outcomes. Leveraging causal interpretations, we derive nonparametric identifiability of the direct effects, which motivates our semiparametric inference method. Our method extends to projected direct effect estimands, accounting for hidden mediators, confounders, and moderators. These estimands remain statistically meaningful under model misspecifications and with error-prone embeddings. We provide bias quantifications and finite-sample linear expansions with uniform concentration bounds. The proposed doubly robust estimators are consistent and efficient under minimal assumptions and potential misspecification, facilitating data-adaptive estimation with machine learning algorithms. Our proposal is evaluated with random forests through simulations and analysis of single-cell CRISPR perturbed datasets with potential unmeasured confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04996v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin-Hong Du, Kathryn Roeder, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Estimation of Spatiotemporal Poisson Processes with Some Missing Location Data</title>
      <link>https://arxiv.org/abs/2410.11103</link>
      <description>arXiv:2410.11103v3 Announce Type: replace 
Abstract: We consider models for spatiotemporal Poisson processes with some missing location data. We discuss four models that make provision for missing location data, and their estimation. The corresponding code is available on GitHub as an extension of LASPATED at https://github.com/vguigues/LASPATED/Missing_Data. We tested our models using the process of emergency call arrivals to an emergency medical service where the emergency reports often omit the location of the emergency. We show the difference made by using models that make provision for missing location data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11103v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Guigues, Anton Kleywegt, Victor Hugo Nascimento, Lucas Lucas Rafael de Andrade</dc:creator>
    </item>
    <item>
      <title>Sparse Causal Effect Estimation using Two-Sample Summary Statistics in the Presence of Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2410.12300</link>
      <description>arXiv:2410.12300v4 Announce Type: replace 
Abstract: Observational genome-wide association studies are now widely used for causal inference in genetic epidemiology. To maintain privacy, such data is often only publicly available as summary statistics, and often studies for the endogenous covariates and the outcome are available separately. This has necessitated methods tailored to two-sample summary statistics. Current state-of-the-art methods modify linear instrumental variable (IV) regression -- with genetic variants as instruments -- to account for unmeasured confounding. However, since the endogenous covariates can be high dimensional, standard IV assumptions are generally insufficient to identify all causal effects simultaneously. We ensure identifiability by assuming the causal effects are sparse and propose a sparse causal effect two-sample IV estimator, spaceTSIV, adapting the spaceIV estimator by Pfister and Peters (2022) for two-sample summary statistics. We provide two methods, based on L0- and L1-penalization, respectively. We prove identifiability of the sparse causal effects in the two-sample setting and consistency of spaceTSIV. The performance of spaceTSIV is compared with existing two-sample IV methods in simulations. Finally, we showcase our methods using real proteomic and gene-expression data for drug-target discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12300v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shimeng Huang, Niklas Pfister, Jack Bowden</dc:creator>
    </item>
    <item>
      <title>A Causal Transformation Model for Time-to-Event Data Affected by Unobserved Confounding</title>
      <link>https://arxiv.org/abs/2410.15968</link>
      <description>arXiv:2410.15968v4 Announce Type: replace 
Abstract: Motivated by studies investigating causal effects in survival analysis, we propose a structural bivariate transformation model to quantify the impact of a binary treatment on a time-to-event outcome. The model equations are connected through a bivariate Gaussian distribution, with the dependence parameter capturing unobserved confounding, and are specified as functions of additive predictors to flexibly account for the impacts of observed confounders. Moreover, the baseline survival function is estimated using monotonic P-splines, the effects of binary or factor instruments can be regularized through a ridge penalty, and interactions between treatment and modifier variables can be incorporated to accommodate potential variations in treatment effects across subgroups. The proposal naturally provides an intuitive causal measure, the survival average treatment effect. Parameter estimation is achieved via an efficient and stable penalized maximum likelihood estimation approach and intervals constructed using related inferential results. We revisit a dataset from the Illinois Reemployment Bonus Experiment to estimate the causal effect of a cash bonus on unemployment duration, unveiling new insights. The modeling framework is incorporated into the R package GJRM, enabling researchers and practitioners to employ the proposed model and ensuring the reproducibility of results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15968v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giampiero Marra, Rosalba Radice</dc:creator>
    </item>
    <item>
      <title>Asynchronous Jump Testing and Estimation in High Dimensions Under Complex Temporal Dynamics</title>
      <link>https://arxiv.org/abs/2410.23706</link>
      <description>arXiv:2410.23706v2 Announce Type: replace 
Abstract: Most high dimensional changepoint detection methods assume the error process is stationary and changepoints occur synchronously across dimensions. The violation of these assumptions, which in applied settings is increasingly likely as the dimensionality of the time series being analyzed grows, can dramatically curtail the sensitivity or the accuracy of these methods. We propose AJDN (Asynchronous Jump Detection under Nonstationary noise). AJDN is a high dimensional multiscale jump detection method that tests and estimates jumps in an otherwise smoothly varying mean function for high dimensional time series with nonstationary noise where the jumps across dimensions may not occur at the same time. AJDN is correct in the sense that it detects the correct number of jumps with a prescribed probability asymptotically and its accuracy in estimating the locations of the jumps is asymptotically nearly optimal under the asynchronous jump assumption. Through a simulation study we demonstrate AJDN's robustness across a wide variety of stationary and nonstationary high dimensional time series, and we show its strong performance relative to some existing high dimensional changepoint detection methods. We apply AJDN to a seismic time series to demonstrate its ability to accurately detect jumps in real-world high dimensional time series with complex temporal dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23706v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weichi Wu, David Veitch, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal Transport Metrics for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2411.14674</link>
      <description>arXiv:2411.14674v2 Announce Type: replace 
Abstract: Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing density estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures. Estimating the mixing measure implies inference on the mixture density and the random partition. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14674v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Peter Mueller</dc:creator>
    </item>
    <item>
      <title>AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression</title>
      <link>https://arxiv.org/abs/2403.13565</link>
      <description>arXiv:2403.13565v3 Announce Type: replace-cross 
Abstract: We consider the transfer learning problem in the high dimensional linear regression setting, where the feature dimension is larger than the sample size. To learn transferable information, which may vary across features or the source samples, we propose an adaptive transfer learning method that can detect and aggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable structures. We achieve this by employing a fused-penalty, coupled with weights that can adapt according to the transferable structure. To choose the weight, we propose a theoretically informed, data-driven procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while filtering out non-transferable signals, and S-AdaTrans to obtain the optimal combination of information transferred from each source sample. We show that, with appropriately chosen weights, F-AdaTrans achieves a convergence rate close to that of an oracle estimator with a known transferable structure, and S-AdaTrans recovers existing near-minimax optimal rates as a special case. The effectiveness of the proposed method is validated using both simulation and real data, demonstrating favorable performance compared to the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13565v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zelin He, Ying Sun, Jingyuan Liu, Runze Li</dc:creator>
    </item>
    <item>
      <title>A Gaussian Process Model for Ordinal Data with Applications to Chemoinformatics</title>
      <link>https://arxiv.org/abs/2405.09989</link>
      <description>arXiv:2405.09989v2 Announce Type: replace-cross 
Abstract: With the proliferation of screening tools for chemical testing, it is now possible to create vast databases of chemicals easily. However, rigorous statistical methodologies employed to analyse these databases are in their infancy, and further development to facilitate chemical discovery is imperative. In this paper, we present conditional Gaussian process models to predict ordinal outcomes from chemical experiments, where the inputs are chemical compounds. We implement the Tanimoto distance, a metric on the chemical space, within the covariance of the Gaussian processes to capture correlated effects in the chemical space. A novel aspect of our model is that the kernel contains a scaling parameter, a feature not previously examined in the literature, that controls the strength of the correlation between elements of the chemical space. Using molecular fingerprints, a numerical representation of a compound's location within the chemical space, we find that accounting for correlation amongst chemical compounds improves predictive performance over the uncorrelated model, where effects are assumed to be independent. Moreover, we present a genetic algorithm for the facilitation of chemical discovery and identification of important features to the compound's efficacy, based on two criteria derived from the proposed model. Simulation studies are conducted to demonstrate the suitability of the proposed methods. Our model is demonstrated on a hazard classification problem of organic solvents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09989v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arron Gosnell, Evangelos Evangelou</dc:creator>
    </item>
    <item>
      <title>Estimating the Cost of Informal Care with a Novel Two-Stage Approach to Individual Synthetic Control</title>
      <link>https://arxiv.org/abs/2411.10314</link>
      <description>arXiv:2411.10314v2 Announce Type: replace-cross 
Abstract: Informal carers provide the majority of care for people living with challenges related to older age, long-term illness, or disability. However, the care they provide often results in a significant income penalty for carers, a factor largely overlooked in the economics literature and policy discourse. Leveraging data from the UK Household Longitudinal Study, this paper provides the first robust causal estimates of the caring income penalty using a novel individual synthetic control based method that accounts for unit-level heterogeneity in post-treatment trajectories over time. Our baseline estimates identify an average relative income gap of up to 45%, with an average decrease of {\pounds}162 in monthly income, peaking at {\pounds}192 per month after 4 years, based on the difference between informal carers providing the highest-intensity of care and their synthetic counterparts. We find that the income penalty is more pronounced for women than for men, and varies by ethnicity and age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10314v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Petrillo, Daniel Valdenegro, Charles Rahal, Yanan Zhang, Gwilym Pryce, Matthew R. Bennett</dc:creator>
    </item>
  </channel>
</rss>
