<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MA</link>
    <description>cs.MA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Apr 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Symmetric Policy Design for Multi-Agent Dispatch Coordination in Supply Chains</title>
      <link>https://arxiv.org/abs/2504.19397</link>
      <description>arXiv:2504.19397v1 Announce Type: new 
Abstract: We study a decentralized dispatch coordination problem in a multi-agent supply chain setting with shared logistics capacity. We propose symmetric (identical) dispatch strategies for all agents, enabling efficient coordination without centralized control. Using a common information approach, we derive a dynamic programming solution that computes optimal symmetric dispatch strategies by transforming the multi-agent problem into a tractable dynamic program on the agents common information state. Simulation results demonstrate that our method significantly reduces coordination cost compared to baseline heuristics, including belief-based strategies and an always-dispatch policy. These findings highlight the benefits of combining symmetric strategy design with a common information-based dynamic programming framework for improving multi-agent coordination performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19397v1</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sagar Sudhakara</dc:creator>
    </item>
    <item>
      <title>Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study Using Different Punishment Strategies</title>
      <link>https://arxiv.org/abs/2504.19487</link>
      <description>arXiv:2504.19487v1 Announce Type: new 
Abstract: The evolution of cooperation has been extensively studied using abstract mathematical models and simulations. Recent advances in Large Language Models (LLM) and the rise of LLM agents have demonstrated their ability to perform social reasoning, thus providing an opportunity to test the emergence of norms in more realistic agent-based simulations with human-like reasoning using natural language. In this research, we investigate whether the cooperation dynamics presented in Boyd and Richerson's model persist in a more realistic simulation of the diner's dilemma using LLM agents compared to the abstract mathematical nature in the work of Boyd and Richerson. Our findings indicate that agents follow the strategies defined in the Boyd and Richerson model, and explicit punishment mechanisms drive norm emergence, reinforcing cooperative behaviour even when the agent strategy configuration varies. Our results suggest that LLM-based Multi-Agent System simulations, in fact, can replicate the evolution of cooperation predicted by the traditional mathematical models. Moreover, our simulations extend beyond the mathematical models by integrating natural language-driven reasoning and a pairwise imitation method for strategy adoption, making them a more realistic testbed for cooperative behaviour in MASs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19487v1</guid>
      <category>cs.MA</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kavindu Warnakulasuriya, Prabhash Dissanayake, Navindu De Silva, Stephen Cranefield, Bastin Tony Roy Savarimuthu, Surangika Ranathunga, Nisansa de Silva</dc:creator>
    </item>
    <item>
      <title>Diffusion Stochastic Learning Over Adaptive Competing Networks</title>
      <link>https://arxiv.org/abs/2504.19635</link>
      <description>arXiv:2504.19635v1 Announce Type: new 
Abstract: This paper studies a stochastic dynamic game between two competing teams, each consisting of a network of collaborating agents. Unlike fully cooperative settings, where all agents share a common objective, each team in this game aims to minimize its own distinct objective. In the adversarial setting, their objectives could be conflicting as in zero-sum games. Throughout the competition, agents share strategic information within their own team while simultaneously inferring and adapting to the strategies of the opposing team. We propose diffusion learning algorithms to address two important classes of this network game: i) a zero-sum game characterized by weak cross-team subgraph interactions, and ii) a general non-zero-sum game exhibiting strong cross-team subgraph interactions. We analyze the stability performance of the proposed algorithms under reasonable assumptions and illustrate the theoretical results through experiments on Cournot team competition and decentralized GAN training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19635v1</guid>
      <category>cs.MA</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yike Zhao, Haoyuan Cai, Ali H. Sayed</dc:creator>
    </item>
    <item>
      <title>PhenoAssistant: A Conversational Multi-Agent AI System for Automated Plant Phenotyping</title>
      <link>https://arxiv.org/abs/2504.19818</link>
      <description>arXiv:2504.19818v1 Announce Type: new 
Abstract: Plant phenotyping increasingly relies on (semi-)automated image-based analysis workflows to improve its accuracy and scalability. However, many existing solutions remain overly complex, difficult to reimplement and maintain, and pose high barriers for users without substantial computational expertise. To address these challenges, we introduce PhenoAssistant: a pioneering AI-driven system that streamlines plant phenotyping via intuitive natural language interaction. PhenoAssistant leverages a large language model to orchestrate a curated toolkit supporting tasks including automated phenotype extraction, data visualisation and automated model training. We validate PhenoAssistant through several representative case studies and a set of evaluation tasks. By significantly lowering technical hurdles, PhenoAssistant underscores the promise of AI-driven methodologies to democratising AI adoption in plant biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19818v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feng Chen, Ilias Stogiannidis, Andrew Wood, Danilo Bueno, Dominic Williams, Fraser Macfarlane, Bruce Grieve, Darren Wells, Jonathan A. Atkinson, Malcolm J. Hawkesford, Stephen A. Rolfe, Tracy Lawson, Tony Pridmore, Mario Valerio Giuffrida, Sotirios A. Tsaftaris</dc:creator>
    </item>
    <item>
      <title>Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach</title>
      <link>https://arxiv.org/abs/2504.18603</link>
      <description>arXiv:2504.18603v1 Announce Type: cross 
Abstract: Quantum computing education faces significant challenges due to its complexity and the limitations of current tools; this paper introduces a novel Intelligent Teaching Assistant for quantum computing education and details its evolutionary design process. The system combines a knowledge-graph-augmented architecture with two specialized Large Language Model (LLM) agents: a Teaching Agent for dynamic interaction, and a Lesson Planning Agent for lesson plan generation. The system is designed to adapt to individual student needs, with interactions meticulously tracked and stored in a knowledge graph. This graph represents student actions, learning resources, and relationships, aiming to enable reasoning about effective learning pathways. We describe the implementation of the system, highlighting the challenges encountered and the solutions implemented, including introducing a dual-agent architecture where tasks are separated, all coordinated through a central knowledge graph that maintains system awareness, and a user-facing tag system intended to mitigate LLM hallucination and improve user control. Preliminary results illustrate the system's potential to capture rich interaction data, dynamically adapt lesson plans based on student feedback via a tag system in simulation, and facilitate context-aware tutoring through the integrated knowledge graph, though systematic evaluation is required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18603v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iizalaarab Elhaimeur, Nikos Chrisochoides</dc:creator>
    </item>
    <item>
      <title>Can AI Agents Design and Implement Drug Discovery Pipelines?</title>
      <link>https://arxiv.org/abs/2504.19912</link>
      <description>arXiv:2504.19912v1 Announce Type: cross 
Abstract: The rapid advancement of artificial intelligence, particularly autonomous agentic systems based on Large Language Models (LLMs), presents new opportunities to accelerate drug discovery by improving in-silico modeling and reducing dependence on costly experimental trials. Current AI agent-based systems demonstrate proficiency in solving programming challenges and conducting research, indicating an emerging potential to develop software capable of addressing complex problems such as pharmaceutical design and drug discovery. This paper introduces DO Challenge, a benchmark designed to evaluate the decision-making abilities of AI agents in a single, complex problem resembling virtual screening scenarios. The benchmark challenges systems to independently develop, implement, and execute efficient strategies for identifying promising molecular structures from extensive datasets, while navigating chemical space, selecting models, and managing limited resources in a multi-objective context. We also discuss insights from the DO Challenge 2025, a competition based on the proposed benchmark, which showcased diverse strategies explored by human participants. Furthermore, we present the Deep Thought multi-agent system, which demonstrated strong performance on the benchmark, outperforming most human teams. Among the language models tested, Claude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles, and GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While promising, the system's performance still fell short of expert-designed solutions and showed high instability, highlighting both the potential and current limitations of AI-driven methodologies in transforming drug discovery and broader scientific research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19912v1</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Khachik Smbatyan, Tsolak Ghukasyan, Tigran Aghajanyan, Hovhannes Dabaghyan, Sergey Adamyan, Aram Bughdaryan, Vahagn Altunyan, Gagik Navasardyan, Aram Davtyan, Anush Hakobyan, Aram Gharibyan, Arman Fahradyan, Artur Hakobyan, Hasmik Mnatsakanyan, Narek Ginoyan, Garik Petrosyan</dc:creator>
    </item>
    <item>
      <title>Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking</title>
      <link>https://arxiv.org/abs/2504.19940</link>
      <description>arXiv:2504.19940v1 Announce Type: cross 
Abstract: The growing spread of online misinformation has created an urgent need for scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where non-experts evaluate claim veracity - offers a cost-effective alternative to expert verification, despite concerns about variability in quality and bias. Encouraged by promising results in certain contexts, major platforms such as X (formerly Twitter), Facebook, and Instagram have begun shifting from centralized moderation to decentralized, crowd-based approaches.
  In parallel, advances in Large Language Models (LLMs) have shown strong performance across core fact-checking tasks, including claim detection and evidence evaluation. However, their potential role in crowdsourced workflows remains unexplored. This paper investigates whether LLM-powered generative agents - autonomous entities that emulate human behavior and decision-making - can meaningfully contribute to fact-checking tasks traditionally reserved for human crowds. Using the protocol of La Barbera et al. (2024), we simulate crowds of generative agents with diverse demographic and ideological profiles. Agents retrieve evidence, assess claims along multiple quality dimensions, and issue final veracity judgments.
  Our results show that agent crowds outperform human crowds in truthfulness classification, exhibit higher internal consistency, and show reduced susceptibility to social and cognitive biases. Compared to humans, agents rely more systematically on informative criteria such as Accuracy, Precision, and Informativeness, suggesting a more structured decision-making process. Overall, our findings highlight the potential of generative agents as scalable, consistent, and less biased contributors to crowd-based fact-checking systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19940v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Luigia Costabile, Gian Marco Orlando, Valerio La Gatta, Vincenzo Moscato</dc:creator>
    </item>
    <item>
      <title>Windowed MAPF with Completeness Guarantees</title>
      <link>https://arxiv.org/abs/2410.01798</link>
      <description>arXiv:2410.01798v3 Announce Type: replace 
Abstract: Traditional multi-agent path finding (MAPF) methods try to compute entire start-goal paths which are collision free. However, computing an entire path can take too long for MAPF systems where agents need to replan fast. Methods that address this typically employ a "windowed" approach and only try to find collision free paths for a small windowed timestep horizon. This adaptation comes at the cost of incompleteness; all current windowed approaches can become stuck in deadlock or livelock. Our main contribution is to introduce our framework, WinC-MAPF, for Windowed MAPF that enables completeness. Our framework uses heuristic update insights from single-agent real-time heuristic search algorithms as well as agent independence ideas from MAPF algorithms. We also develop Single-Step CBS (SS-CBS), an instantiation of this framework using a novel modification to CBS. We show how SS-CBS, which only plans a single step and updates heuristics, can effectively solve tough scenarios where existing windowed approaches fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01798v3</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1609/aaai.v39i22.34499</arxiv:DOI>
      <dc:creator>Rishi Veerapaneni, Muhammad Suhail Saleem, Jiaoyang Li, Maxim Likhachev</dc:creator>
    </item>
    <item>
      <title>Online Dynamic Pricing for Electric Vehicle Charging Stations with Reservations</title>
      <link>https://arxiv.org/abs/2410.05538</link>
      <description>arXiv:2410.05538v3 Announce Type: replace 
Abstract: This paper introduces a novel model for online dynamic pricing of electric vehicle charging services that integrates reservation, parking, and charging into a comprehensive bundle priced as a whole. Our approach focuses on the individual high-demand, fast-charging location, employing a Poisson process as a model of charging reservation arrivals, and develops an online dynamic pricing strategy optimized through a Markov Decision Process (MDP). A key contribution is the novel analysis of discretization error introduced when incorporating the continuous-time Poisson process into the discrete MDP framework. The MDP model's feasibility is demonstrated with a heuristic dynamic pricing method based on Monte-Carlo tree search, offering a viable path for real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05538v3</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Mrkos, Anton\'in Komenda, David Fiedler, Ji\v{r}\'i Vok\v{r}\'inek</dc:creator>
    </item>
    <item>
      <title>Incentivized Symbiosis: A Paradigm for Human-Agent Coevolution</title>
      <link>https://arxiv.org/abs/2412.06855</link>
      <description>arXiv:2412.06855v4 Announce Type: replace 
Abstract: Cooperation is vital to our survival and progress. Evolutionary game theory offers a lens to understand the structures and incentives that enable cooperation to be a successful strategy. As artificial intelligence agents become integral to human systems, the dynamics of cooperation take on unprecedented significance. The convergence of human-agent teaming, contract theory, and decentralized frameworks like Web3, grounded in transparency, accountability, and trust, offers a foundation for fostering cooperation by establishing enforceable rules and incentives for humans and AI agents. We conceptualize Incentivized Symbiosis as a social contract between humans and AI, inspired by Web3 principles and encoded in blockchain technology, to define and enforce rules, incentives, and consequences for both parties. By exploring this paradigm, we aim to catalyze new research at the intersection of systems thinking in AI, Web3, and society, fostering innovative pathways for cooperative human-agent coevolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06855v4</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomer Jordi Chaffer, Justin Goldston, Gemach D. A. T. A. I</dc:creator>
    </item>
    <item>
      <title>Fast algorithm for centralized multi-agent maze exploration</title>
      <link>https://arxiv.org/abs/2310.02121</link>
      <description>arXiv:2310.02121v2 Announce Type: replace-cross 
Abstract: Recent advances in robotics have paved the way for robots to replace humans in perilous situations, such as searching for victims in burning buildings, in earthquake-damaged structures, in uncharted caves, traversing minefields or patrolling crime-ridden streets. These challenges can be generalized as problems where agents have to explore unknown mazes. We propose a cooperative multi-agent system of automated mobile agents for exploring unknown mazes and localizing stationary targets. The Heat Equation-Driven Area Coverage (HEDAC) algorithm for maze exploration employs a potential field to guide the exploration of the maze and integrates cooperative behaviors of the agents such as collision avoidance, coverage coordination, and path planning. In contrast to previous applications for continuous static domains, we adapt the HEDAC method for mazes on expanding rectilinear grids. The proposed algorithm guarantees the exploration of the entire maze and can ensure the avoidance of collisions and deadlocks. Moreover, this is the first application of the HEDAC algorithm to domains that expand over time. To cope with the dynamically changing domain, succesive over-relaxation (SOR) iterative linear solver has been adapted and implemented, which significantly reduced the computational complexity of the presented algorithm when compared to standard direct and iterative linear solvers. The results highlight significant improvements and show the applicability of the algorithm in different mazes. They confirm its robustness, adaptability, scalability and simplicity, which enables centralized parallel computation to control multiple agents/robots in the maze.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02121v2</guid>
      <category>cs.RO</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bojan Crnkovi\'c, Stefan Ivi\'c, Mila Zovko</dc:creator>
    </item>
    <item>
      <title>An extension of May's Theorem to three alternatives: axiomatizing Minimax voting</title>
      <link>https://arxiv.org/abs/2312.14256</link>
      <description>arXiv:2312.14256v4 Announce Type: replace-cross 
Abstract: May's Theorem [K. O. May, Econometrica 20 (1952) 680-684] characterizes majority voting on two alternatives as the unique preferential voting method satisfying several simple axioms. Here we show that by adding some desirable axioms to May's axioms, we can uniquely determine how to vote on three alternatives (setting aside tiebreaking). In particular, we add two axioms stating that the voting method should mitigate spoiler effects and avoid the so-called strong no show paradox. We prove a theorem stating that any preferential voting method satisfying our enlarged set of axioms, which includes some weak homogeneity and preservation axioms, must choose from among the Minimax winners in all three-alternative elections. When applied to more than three alternatives, our axioms also distinguish Minimax from other known voting methods that coincide with or refine Minimax for three alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14256v4</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wesley H. Holliday, Eric Pacuit</dc:creator>
    </item>
    <item>
      <title>Distributed Multi-Task Learning for Stochastic Bandits with Context Distribution and Stage-wise Constraints</title>
      <link>https://arxiv.org/abs/2401.11563</link>
      <description>arXiv:2401.11563v3 Announce Type: replace-cross 
Abstract: We present conservative distributed multi-task learning in stochastic linear contextual bandits with heterogeneous agents. This extends conservative linear bandits to a distributed setting where M agents tackle different but related tasks while adhering to stage-wise performance constraints. The exact context is unknown, and only a context distribution is available to the agents as in many practical applications that involve a prediction mechanism to infer context, such as stock market prediction and weather forecast. We propose a distributed upper confidence bound (UCB) algorithm, DiSC-UCB. Our algorithm constructs a pruned action set during each round to ensure the constraints are met. Additionally, it includes synchronized sharing of estimates among agents via a central server using well-structured synchronization steps. We prove the regret and communication bounds on the algorithm. We extend the problem to a setting where the agents are unaware of the baseline reward. For this setting, we provide a modified algorithm, DiSC-UCB2, and we show that the modified algorithm achieves the same regret and communication bounds. We empirically validated the performance of our algorithm on synthetic data and real-world Movielens-100K data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11563v3</guid>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiabin Lin, Shana Moothedath</dc:creator>
    </item>
    <item>
      <title>Evolution of Societies via Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2410.17466</link>
      <description>arXiv:2410.17466v4 Announce Type: replace-cross 
Abstract: The universe involves many independent co-learning agents as an ever-evolving part of our observed environment. Yet, in practice, Multi-Agent Reinforcement Learning (MARL) applications are typically constrained to small, homogeneous populations and remain computationally intensive. We propose a methodology that enables simulating populations of Reinforcement Learning agents at evolutionary scale. More specifically, we derive a fast, parallelizable implementation of Policy Gradient (PG) and Opponent-Learning Awareness (LOLA), tailored for evolutionary simulations where agents undergo random pairwise interactions in stateless normal-form games. We demonstrate our approach by simulating the evolution of very large populations made of heterogeneous co-learning agents, under both naive and advanced learning strategies. In our experiments, 200,000 PG or LOLA agents evolve in the classic games of Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors. Each game provides distinct insights into how populations evolve under both naive and advanced MARL rules, including compelling ways in which Opponent-Learning Awareness affects social evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17466v4</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>q-bio.PE</category>
      <category>q-fin.GN</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yann Bouteiller, Karthik Soma, Giovanni Beltrame</dc:creator>
    </item>
  </channel>
</rss>
