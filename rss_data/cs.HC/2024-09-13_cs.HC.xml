<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Sep 2024 04:00:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Complexity as Design Material</title>
      <link>https://arxiv.org/abs/2409.07465</link>
      <description>arXiv:2409.07465v1 Announce Type: new 
Abstract: Complexity is often seen as a inherent negative in information design, with the job of the designer being to reduce or eliminate complexity, and with principles like Tufte's "data-ink ratio" or "chartjunk" to operationalize minimalism and simplicity in visualizations. However, in this position paper, we call for a more expansive view of complexity as a design material, like color or texture or shape: an element of information design that can be used in many ways, many of which are beneficial to the goals of using data to understand the world around us. We describe complexity as a phenomenon that occurs not just in visual design but in every aspect of the sensemaking process, from data collection to interpretation. For each of these stages, we present examples of ways that these various forms of complexity can be used (or abused) in visualization design. We ultimately call on the visualization community to build a more nuanced view of complexity, to look for places to usefully integrate complexity in multiple stages of the design process, and, even when the goal is to reduce complexity, to look for the non-visual forms of complexity that may have otherwise been overlooked.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07465v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Windhager, Alfie Abduhl-Rahman, Mark-Jan Bludau, Nicole Hengesbach, Houda Lamqaddam, Isabell Meirelles, Bettina Speckmann, Michael Correll</dc:creator>
    </item>
    <item>
      <title>Multi-scale spatiotemporal representation learning for EEG-based emotion recognition</title>
      <link>https://arxiv.org/abs/2409.07589</link>
      <description>arXiv:2409.07589v1 Announce Type: new 
Abstract: EEG-based emotion recognition holds significant potential in the field of brain-computer interfaces. A key challenge lies in extracting discriminative spatiotemporal features from electroencephalogram (EEG) signals. Existing studies often rely on domain-specific time-frequency features and analyze temporal dependencies and spatial characteristics separately, neglecting the interaction between local-global relationships and spatiotemporal dynamics. To address this, we propose a novel network called Multi-Scale Inverted Mamba (MS-iMamba), which consists of Multi-Scale Temporal Blocks (MSTB) and Temporal-Spatial Fusion Blocks (TSFB). Specifically, MSTBs are designed to capture both local details and global temporal dependencies across different scale subsequences. The TSFBs, implemented with an inverted Mamba structure, focus on the interaction between dynamic temporal dependencies and spatial characteristics. The primary advantage of MS-iMamba lies in its ability to leverage reconstructed multi-scale EEG sequences, exploiting the interaction between temporal and spatial features without the need for domain-specific time-frequency feature extraction. Experimental results on the DEAP, DREAMER, and SEED datasets demonstrate that MS-iMamba achieves classification accuracies of 94.86%, 94.94%, and 91.36%, respectively, using only four-channel EEG signals, outperforming state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07589v1</guid>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Zhou, Xiaojing Peng</dc:creator>
    </item>
    <item>
      <title>Situated Visualization in Motion for Swimming</title>
      <link>https://arxiv.org/abs/2409.07695</link>
      <description>arXiv:2409.07695v1 Announce Type: new 
Abstract: Competitive sports coverage increasingly includes information on athlete or team statistics and records. Sports video coverage has traditionally embedded representations of this data in fixed locations on the screen, but more recently also attached representations to athletes or other targets in motion. These publicly used representations so far have been rather simple and systematic investigations of the research space of embedded visualizations in motion are still missing. Here we report on our preliminary research in the domain of professional and amateur swimming. We analyzed how visualizations are currently added to the coverage of Olympics swimming competitions and then plan to derive a design space for embedded data representations for swimming competitions. We are currently conducting a crowdsourced survey to explore which kind of swimming-related data general audiences are interested in, in order to identify opportunities for additional visualizations to be added to swimming competition coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07695v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Journ\'ee Visu 2022, Jun 2022, Bordeaux, France</arxiv:journal_reference>
      <dc:creator>Lijie Yao, Anastasia Bezerianos, Romain Vuillemot, Petra Isenberg</dc:creator>
    </item>
    <item>
      <title>Visualization in Motion in Video Games for Different Types of Data</title>
      <link>https://arxiv.org/abs/2409.07696</link>
      <description>arXiv:2409.07696v1 Announce Type: new 
Abstract: We contribute an analysis of situated visualizations in motion in video games for different types of data, with a focus on quantitative and categorical data representations. Video games convey a lot of data to players, to help them succeed in the game. These visualizations frequently move across the screen due to camera changes or because the game elements themselves move. Our ultimate goal is to understand how motion factors affect visualization readability in video games and subsequently the players' performance in the game. We started our work by surveying the characteristics of how motion currently influences which kind of data representations in video games. We conducted a systematic review of 160 visualizations in motion in video games and extracted patterns and considerations regarding was what, and how visualizations currently exhibit motion factors in video games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07696v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Journ\'ee Visu 2022, Jun 2022, Bordeaux, France</arxiv:journal_reference>
      <dc:creator>Federica Bucchieri, Lijie Yao, Petra Isenberg</dc:creator>
    </item>
    <item>
      <title>Eyes on the Phish(er): Towards Understanding Users' Email Processing Pattern and Mental Models in Phishing Detection</title>
      <link>https://arxiv.org/abs/2409.07717</link>
      <description>arXiv:2409.07717v1 Announce Type: new 
Abstract: Phishing emails typically masquerade themselves as reputable identities to trick people into providing sensitive information and credentials. Despite advancements in cybersecurity, attackers continuously adapt, posing ongoing threats to individuals and organisations. While email users are the last line of defence, they are not always well-prepared to detect phishing emails. This study examines how workload affects susceptibility to phishing, using eye-tracking technology to observe participants' reading patterns and interactions with tailored phishing emails. Incorporating both quantitative and qualitative analysis, we investigate users' attention to two phishing indicators, email sender and hyperlink URLs, and their reasons for assessing the trustworthiness of emails and falling for phishing emails. Our results provide concrete evidence that attention to the email sender can reduce phishing susceptibility. While we found no evidence that attention to the actual URL in the browser influences phishing detection, attention to the text masking links can increase phishing susceptibility. We also highlight how email relevance, familiarity, and visual presentation impact first impressions of email trustworthiness and phishing susceptibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07717v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3688459.3688465</arxiv:DOI>
      <dc:creator>Sijie Zhuo, Robert Biddle, Jared Daniel Recomendable, Giovanni Russello, Danielle Lottridge</dc:creator>
    </item>
    <item>
      <title>Explorations in Designing Virtual Environments for Remote Counselling</title>
      <link>https://arxiv.org/abs/2409.07765</link>
      <description>arXiv:2409.07765v1 Announce Type: new 
Abstract: The advent of technology-enhanced interventions has significantly transformed mental health services, offering new opportunities for delivering psychotherapy, particularly in remote settings. This paper reports on a pilot study exploring the use of Virtual Reality (VR) as a medium for remote counselling. The study involved four experienced psychotherapists who evaluated three different virtual environments designed to support remote counselling. Through thematic analysis of interviews and feedback, we identified key factors that could be critical for designing effective virtual environments for counselling. These include the creation of clear boundaries, customization to meet specific therapeutic needs, and the importance of aligning the environment with various therapeutic approaches. Our findings suggest that VR can enhance the sense of presence and engagement in remote therapy, potentially improving the therapeutic relationship. In the paper we also outline areas for future research based on these pilot study results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07765v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiashuo Cao, Wujie Gao, Yun Suen Pai, Simon Hoermann, Chen Li, Nilufar Baghaei, Mark Billinghurst</dc:creator>
    </item>
    <item>
      <title>More than just a Tool: People's Perception and Acceptance of Prosocial Delivery Robots as Fellow Road Users</title>
      <link>https://arxiv.org/abs/2409.07815</link>
      <description>arXiv:2409.07815v1 Announce Type: new 
Abstract: Service robots are increasingly deployed in public spaces, performing functional tasks such as making deliveries. To better integrate them into our social environment and enhance their adoption, we consider integrating social identities within delivery robots along with their functional identity. We conducted a virtual reality-based pilot study to explore people's perceptions and acceptance of delivery robots that perform prosocial behavior. Preliminary findings from thematic analysis of semi-structured interviews illustrate people's ambivalence about dual identity. We discussed the emerging themes in light of social identity theory, framing effect, and human-robot intergroup dynamics. Building on these insights, we propose that the next generation of delivery robots should use peer-based framing, an updated value proposition, and an interactive design that places greater emphasis on expressing intentionality and emotional responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07815v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivienne Bihe Chi, Elise Ulwelling, Kevin Salubre, Shashank Mehrotra, Teruhisa Misu, Kumar Akash</dc:creator>
    </item>
    <item>
      <title>Online vs Offline: A Comparative Study of First-Party and Third-Party Evaluations of Social Chatbots</title>
      <link>https://arxiv.org/abs/2409.07823</link>
      <description>arXiv:2409.07823v1 Announce Type: new 
Abstract: This paper explores the efficacy of online versus offline evaluation methods in assessing conversational chatbots, specifically comparing first-party direct interactions with third-party observational assessments. By extending a benchmarking dataset of user dialogs with empathetic chatbots with offline third-party evaluations, we present a systematic comparison between the feedback from online interactions and the more detached offline third-party evaluations. Our results reveal that offline human evaluations fail to capture the subtleties of human-chatbot interactions as effectively as online assessments. In comparison, automated third-party evaluations using a GPT-4 model offer a better approximation of first-party human judgments given detailed instructions. This study highlights the limitations of third-party evaluations in grasping the complexities of user experiences and advocates for the integration of direct interaction feedback in conversational AI evaluation to enhance system development and user satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07823v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ekaterina Svikhnushina, Pearl Pu</dc:creator>
    </item>
    <item>
      <title>Measuring the limit of perception of bond stiffness of interactive molecules in VR via a gamified psychophysics experiment</title>
      <link>https://arxiv.org/abs/2409.07836</link>
      <description>arXiv:2409.07836v1 Announce Type: new 
Abstract: Molecular dynamics (MD) simulations provide crucial insight into molecular interactions and biomolecular function. With interactive MD simulations in VR (iMD-VR), chemists can now interact with these molecular simulations in real-time. Our sense of touch is essential for exploring the properties of physical objects, but recreating this sensory experience for virtual objects poses challenges. Furthermore, employing haptics in the context of molecular simulation is especially difficult since \textit{we do not know what molecules actually feel like}. In this paper, we build upon previous work that demonstrated how VR-users can distinguish properties of molecules without haptic feedback. We present the results of a gamified two-alternative forced choice (2AFC) psychophysics user study in which we quantify the threshold at which iMD-VR users can differentiate the stiffness of molecular bonds. Our preliminary analysis suggests that participants can sense differences between buckminsterfullerene molecules with different bond stiffness parameters and that this limit may fall within the chemically relevant range. Our results highlight how iMD-VR may facilitate a more embodied way of exploring complex and dynamic molecular systems, enabling chemists to sense the properties of molecules purely by interacting with them in VR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07836v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-71707-9_13</arxiv:DOI>
      <dc:creator>Rhoslyn Roebuck Williams, Jonathan Barnoud, Luis Toledo, Till Holzapfel, David R. Glowacki</dc:creator>
    </item>
    <item>
      <title>Objection Overruled! Lay People can Distinguish Large Language Models from Lawyers, but still Favour Advice from an LLM</title>
      <link>https://arxiv.org/abs/2409.07871</link>
      <description>arXiv:2409.07871v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are seemingly infiltrating every domain, and the legal context is no exception. In this paper, we present the results of three experiments (total N=288) that investigated lay people's willingness to act upon, and their ability to discriminate between, LLM- and lawyer-generated legal advice. In Experiment 1, participants judged their willingness to act on legal advice when the source of the advice was either known or unknown. When the advice source was unknown, participants indicated that they were significantly more willing to act on the LLM-generated advice. This result was replicated in Experiment 2. Intriguingly, despite participants indicating higher willingness to act on LLM-generated advice in Experiments 1 and 2, participants discriminated between the LLM- and lawyer-generated texts significantly above chance-level in Experiment 3. Lastly, we discuss potential explanations and risks of our findings, limitations and future work, and the importance of language complexity and real-world comparability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07871v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eike Schneiders, Tina Seabrooke, Joshua Krook, Richard Hyde, Natalie Leesakul, Jeremie Clos, Joel Fischer</dc:creator>
    </item>
    <item>
      <title>Tidal MerzA: Combining affective modelling and autonomous code generation through Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2409.07918</link>
      <description>arXiv:2409.07918v1 Announce Type: new 
Abstract: This paper presents Tidal-MerzA, a novel system designed for collaborative performances between humans and a machine agent in the context of live coding, specifically focusing on the generation of musical patterns. Tidal-MerzA fuses two foundational models: ALCAA (Affective Live Coding Autonomous Agent) and Tidal Fuzz, a computational framework. By integrating affective modelling with computational generation, this system leverages reinforcement learning techniques to dynamically adapt music composition parameters within the TidalCycles framework, ensuring both affective qualities to the patterns and syntactical correctness. The development of Tidal-MerzA introduces two distinct agents: one focusing on the generation of mini-notation strings for musical expression, and another on the alignment of music with targeted affective states through reinforcement learning. This approach enhances the adaptability and creative potential of live coding practices and allows exploration of human-machine creative interactions. Tidal-MerzA advances the field of computational music generation, presenting a novel methodology for incorporating artificial intelligence into artistic practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07918v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elizabeth Wilson, Gy\"orgy Fazekas, Geraint Wiggins</dc:creator>
    </item>
    <item>
      <title>Testing the Test: Observations When Assessing Visualization Literacy of Domain Experts</title>
      <link>https://arxiv.org/abs/2409.08101</link>
      <description>arXiv:2409.08101v1 Announce Type: new 
Abstract: Various standardized tests exist that assess individuals' visualization literacy. Their use can help to draw conclusions from studies. However, it is not taken into account that the test itself can create a pressure situation where participants might fear being exposed and assessed negatively. This is especially problematic when testing domain experts in design studies. We conducted interviews with experts from different domains performing the Mini-VLAT test for visualization literacy to identify potential problems. Our participants reported that the time limit per question, ambiguities in the questions and visualizations, and missing steps in the test procedure mainly had an impact on their performance and content. We discuss possible changes to the test design to address these issues and how such assessment methods could be integrated into existing evaluation procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08101v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seyda \"Oney, Moataz Abdelaal, Kuno Kurzhals, Paul Betz, Cordula Kropp, Daniel Weiskopf</dc:creator>
    </item>
    <item>
      <title>GAZEploit: Remote Keystroke Inference Attack by Gaze Estimation from Avatar Views in VR/MR Devices</title>
      <link>https://arxiv.org/abs/2409.08122</link>
      <description>arXiv:2409.08122v1 Announce Type: new 
Abstract: The advent and growing popularity of Virtual Reality (VR) and Mixed Reality (MR) solutions have revolutionized the way we interact with digital platforms. The cutting-edge gaze-controlled typing methods, now prevalent in high-end models of these devices, e.g., Apple Vision Pro, have not only improved user experience but also mitigated traditional keystroke inference attacks that relied on hand gestures, head movements and acoustic side-channels. However, this advancement has paradoxically given birth to a new, potentially more insidious cyber threat, GAZEploit.
  In this paper, we unveil GAZEploit, a novel eye-tracking based attack specifically designed to exploit these eye-tracking information by leveraging the common use of virtual appearances in VR applications. This widespread usage significantly enhances the practicality and feasibility of our attack compared to existing methods. GAZEploit takes advantage of this vulnerability to remotely extract gaze estimations and steal sensitive keystroke information across various typing scenarios-including messages, passwords, URLs, emails, and passcodes. Our research, involving 30 participants, achieved over 80% accuracy in keystroke inference. Alarmingly, our study also identified over 15 top-rated apps in the Apple Store as vulnerable to the GAZEploit attack, emphasizing the urgent need for bolstered security measures for this state-of-the-art VR/MR text entry method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08122v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanqiu Wang, Zihao Zhan, Haoqi Shan, Siqi Dai, Max Panoff, Shuo Wang</dc:creator>
    </item>
    <item>
      <title>Co-badge: An Activity for Collaborative Engagement with Data Visualization Design Concepts</title>
      <link>https://arxiv.org/abs/2409.08175</link>
      <description>arXiv:2409.08175v1 Announce Type: new 
Abstract: As data visualization gains popularity and projects become more interdisciplinary, there is a growing need for methods that foster creative collaboration and inform diverse audiences about data visualisation. In this paper, we introduce Co-Badge, a 90-minute design activity where participants collaboratively construct visualizations by ideating and prioritizing relevant data types, mapping them to visual variables, and constructing data badges with stationery materials. We conducted three workshops in diverse settings with participants of different backgrounds. Our findings indicate that Co-badge facilitates a playful and engaging way to gain awareness about data visualization design principles without formal training while navigating the challenges of collaboration. Our work contributes to the field of data visualization education for diverse actors. We believe Co-Badge can serve as an engaging activity that introduces basic concepts of data visualization and collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08175v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3677045.3685432</arxiv:DOI>
      <dc:creator>Damla \c{C}ay, Mary Karyda, Kitti Butter</dc:creator>
    </item>
    <item>
      <title>Exploring Use and Perceptions of Generative AI Art Tools by Blind Artists</title>
      <link>https://arxiv.org/abs/2409.08226</link>
      <description>arXiv:2409.08226v1 Announce Type: new 
Abstract: The paper explores the intersection of AI art and blindness, as existing AI research has primarily focused on AI art's reception and impact, on sighted artists and consumers. To address this gap, the researcher interviewed six blind artists from various visual art mediums and levels of blindness about the generative AI image platform Midjourney. The participants shared text prompts and discussed their reactions to the generated images with the sighted researcher. The findings highlight blind artists' interest in AI images as a collaborative tool but express concerns about cultural perceptions and labeling of AI-generated art. They also underscore unique challenges, such as potential misunderstandings and stereotypes about blindness leading to exclusion. The study advocates for greater inclusion of blind individuals in AI art, emphasizing the need to address their specific needs and experiences in developing AI art technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08226v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gayatri Raman, Erin Brady</dc:creator>
    </item>
    <item>
      <title>OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering</title>
      <link>https://arxiv.org/abs/2409.08250</link>
      <description>arXiv:2409.08250v1 Announce Type: new 
Abstract: People often capture memories through photos, screenshots, and videos. While existing AI-based tools enable querying this data using natural language, they mostly only support retrieving individual pieces of information like certain objects in photos and struggle with answering more complex queries that involve interpreting interconnected memories like event sequences. We conducted a one-month diary study to collect realistic user queries and generated a taxonomy of necessary contextual information for integrating with captured memories. We then introduce OmniQuery, a novel system that is able to answer complex personal memory-related questions that require extracting and inferring contextual information. OmniQuery augments single captured memories through integrating scattered contextual information from multiple interconnected memories, retrieves relevant memories, and uses a large language model (LLM) to comprehensive answers. In human evaluations, we show the effectiveness of OmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG system, winning or tying in 74.5% of the time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08250v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiahao Nick Li (Jerry),  Zhuohao (Jerry),  Zhang, Jiaju Ma</dc:creator>
    </item>
    <item>
      <title>The Design of Informative Take-Over Requests for Semi-Autonomous Cyber-Physical Systems: Combining Spoken Language and Visual Icons in a Drone-Controller Setting</title>
      <link>https://arxiv.org/abs/2409.08253</link>
      <description>arXiv:2409.08253v1 Announce Type: new 
Abstract: The question of how cyber-physical systems should interact with human partners that can take over control or exert oversight is becoming more pressing, as these systems are deployed for an ever larger range of tasks. Drawing on the literatures on handing over control during semi-autonomous driving and human-robot interaction, we propose a design of a take-over request that combines an abstract pre-alert with an informative TOR: Relevant sensor information is highlighted on the controller's display, while a spoken message verbalizes the reason for the TOR. We conduct our study in the context of a semi-autonomous drone control scenario as our testbed. The goal of our online study is to assess in more detail what form a language-based TOR should take. Specifically, we compare a full sentence condition to shorter fragments, and test whether the visual highlighting should be done synchronously or asynchronously with the speech. Participants showed a higher accuracy in choosing the correct solution with our bi-modal TOR and felt that they were better able to recognize the critical situation. Using only fragments in the spoken message rather than full sentences did not lead to improved accuracy or faster reactions. Also, synchronizing the visual highlighting with the spoken message did not result in better accuracy and response times were even increased in this condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08253v1</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>cs.RO</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashwini Gundappa, Emilia Ellsiepen, Lukas Schmitz, Frederik Wiehr, Vera Demberg</dc:creator>
    </item>
    <item>
      <title>Cross-Cultural Communication in the Digital Age: An Analysis of Cultural Representation and Inclusivity in Emojis</title>
      <link>https://arxiv.org/abs/2409.07475</link>
      <description>arXiv:2409.07475v1 Announce Type: cross 
Abstract: Emojis have become a universal language in the digital world, enabling users to express emotions, ideas, and identities across diverse cultural contexts. As emojis incorporate more cultural symbols and diverse representations, they play a crucial role in cross-cultural communication. This research project aims to analyze the representation of different cultures in emojis, investigate how emojis facilitate cross-cultural communication and promote inclusivity, and explore the impact of emojis on understanding and interpretation in different cultural contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07475v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lingfeng Li (Southeast University, Nanjing, China), Xiangwen Zheng (Southeast University, Nanjing, China)</dc:creator>
    </item>
    <item>
      <title>FORS-EMG: A Novel sEMG Dataset for Hand Gesture Recognition Across Multiple Forearm Orientations</title>
      <link>https://arxiv.org/abs/2409.07484</link>
      <description>arXiv:2409.07484v1 Announce Type: cross 
Abstract: Surface electromyography (sEMG) signal holds great potential in the research fields of gesture recognition and the development of robust prosthetic hands. However, the sEMG signal is compromised with physiological or dynamic factors such as forearm orientations, electrode displacement, limb position, etc. The existing dataset of sEMG is limited as they often ignore these dynamic factors during recording. In this paper, we have proposed a dataset of multichannel sEMG signals to evaluate common daily living hand gestures performed with three forearm orientations. The dataset is collected from nineteen intact-limed subjects, performing twelve hand gestures with three forearm orientations: supination, rest, and pronation.Additionally, two electrode placement positions (elbow and forearm) are considered while recording the sEMG signal. The dataset is open for public access in MATLAB file format. The key purpose of the dataset is to offer an extensive resource for developing a robust machine learning classification algorithm and hand gesture recognition applications. We validated the high quality of the dataset by assessing the signal quality matrices and classification performance, utilizing popular machine learning algorithms, various feature extraction methods, and variable window size. The obtained result highlighted the significant potential of this novel sEMG dataset that can be used as a benchmark for developing hand gesture recognition systems, conducting clinical research on sEMG, and developing human-computer interaction applications. Dataset:https://www.kaggle.com/datasets/ummerummanchaity/fors-emg-a-novel-semg-dataset/data</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07484v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Umme Rumman, Arifa Ferdousi, Md. Sazzad Hossain, Md. Johirul Islam, Shamim Ahmad, Mamun Bin Ibne Reaz, Md. Rezaul Islam</dc:creator>
    </item>
    <item>
      <title>Dynamic Fairness Perceptions in Human-Robot Interaction</title>
      <link>https://arxiv.org/abs/2409.07560</link>
      <description>arXiv:2409.07560v1 Announce Type: cross 
Abstract: People deeply care about how fairly they are treated by robots. The established paradigm for probing fairness in Human-Robot Interaction (HRI) involves measuring the perception of the fairness of a robot at the conclusion of an interaction. However, such an approach is limited as interactions vary over time, potentially causing changes in fairness perceptions as well. To validate this idea, we conducted a 2x2 user study with a mixed design (N=40) where we investigated two factors: the timing of unfair robot actions (early or late in an interaction) and the beneficiary of those actions (either another robot or the participant). Our results show that fairness judgments are not static. They can shift based on the timing of unfair robot actions. Further, we explored using perceptions of three key factors (reduced welfare, conduct, and moral transgression) proposed by a Fairness Theory from Organizational Justice to predict momentary perceptions of fairness in our study. Interestingly, we found that the reduced welfare and moral transgression factors were better predictors than all factors together. Our findings reinforce the idea that unfair robot behavior can shape perceptions of group dynamics and trust towards a robot and pave the path to future research directions on moment-to-moment fairness perceptions</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07560v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Houston Claure, Kate Candon, Inyoung Shin, Marynel V\'azquez</dc:creator>
    </item>
    <item>
      <title>From Explanations to Action: A Zero-Shot, Theory-Driven LLM Framework for Student Performance Feedback</title>
      <link>https://arxiv.org/abs/2409.08027</link>
      <description>arXiv:2409.08027v1 Announce Type: cross 
Abstract: Recent advances in eXplainable AI (XAI) for education have highlighted a critical challenge: ensuring that explanations for state-of-the-art AI models are understandable for non-technical users such as educators and students. In response, we introduce iLLuMinaTE, a zero-shot, chain-of-prompts LLM-XAI pipeline inspired by Miller's cognitive model of explanation. iLLuMinaTE is designed to deliver theory-driven, actionable feedback to students in online courses. iLLuMinaTE navigates three main stages - causal connection, explanation selection, and explanation presentation - with variations drawing from eight social science theories (e.g. Abnormal Conditions, Pearl's Model of Explanation, Necessity and Robustness Selection, Contrastive Explanation). We extensively evaluate 21,915 natural language explanations of iLLuMinaTE extracted from three LLMs (GPT-4o, Gemma2-9B, Llama3-70B), with three different underlying XAI methods (LIME, Counterfactuals, MC-LIME), across students from three diverse online courses. Our evaluation involves analyses of explanation alignment to the social science theory, understandability of the explanation, and a real-world user preference study with 114 university students containing a novel actionability simulation. We find that students prefer iLLuMinaTE explanations over traditional explainers 89.52% of the time. Our work provides a robust, ready-to-use framework for effectively communicating hybrid XAI-driven insights in education, with significant generalization potential for other human-centric fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08027v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vinitra Swamy, Davide Romano, Bhargav Srinivasa Desikan, Oana-Maria Camburu, Tanja K\"aser</dc:creator>
    </item>
    <item>
      <title>Cleaning Up the Streets: Understanding Motivations, Mental Models, and Concerns of Users Flagging Social Media Posts</title>
      <link>https://arxiv.org/abs/2309.06688</link>
      <description>arXiv:2309.06688v2 Announce Type: replace 
Abstract: Social media platforms offer flagging, a technical feature that empowers users to report inappropriate posts or bad actors to reduce online harm. The deceptively simple flagging interfaces on nearly all major social media platforms disguise complex underlying interactions among users, algorithms, and moderators. Through semi-structured interviews with 25 social media users experienced in flagging inappropriate content, we examine end-users' understanding of flagging procedures, explore the factors that motivate them to flag, and surface their cognitive and privacy concerns. Our findings uncover a lack of procedural transparency in flagging mechanisms that create gaps in users' mental models. Regardless, users strongly believe that it is crucial for platforms to provide flagging options. We highlight how flags raise questions about distributing labor and responsibility between platforms and users for addressing online harm. We recommend innovations in the flagging design space that assist user comprehension, ensure privacy, and reduce cognitive burdens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06688v2</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alice Qian Zhang, Kaitlin Montague, Shagun Jhaver</dc:creator>
    </item>
    <item>
      <title>Self-centering 3-DoF feet controller for hands-free locomotion control in telepresence and virtual reality</title>
      <link>https://arxiv.org/abs/2408.02319</link>
      <description>arXiv:2408.02319v2 Announce Type: replace-cross 
Abstract: We present a novel seated feet controller for handling 3-DoF aimed to control locomotion for telepresence robotics and virtual reality environments. Tilting the feet on two axes yields in forward, backward and sideways motion. In addition, a separate rotary joint allows for rotation around the vertical axis. Attached springs on all joints self-center the controller. The HTC Vive tracker is used to translate the trackers' orientation into locomotion commands. The proposed self-centering feet controller was used successfully for the ANA Avatar XPRIZE competition, where a naive operator traversed the robot through a longer distance, surpassing obstacles while solving various interaction and manipulation tasks in between. We publicly provide the models of the mostly 3D-printed feet controller for reproduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02319v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Fri, 13 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Raphael Memmesheimer, Christian Lenz, Max Schwarz, Michael Schreiber, Sven Behnke</dc:creator>
    </item>
  </channel>
</rss>
