<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Mar 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Effect of Leaders Voice on Financial Market: An Empirical Deep Learning Expedition on NASDAQ, NSE, and Beyond</title>
      <link>https://arxiv.org/abs/2403.12161</link>
      <description>arXiv:2403.12161v1 Announce Type: new 
Abstract: Financial market like the price of stock, share, gold, oil, mutual funds are affected by the news and posts on social media. In this work deep learning based models are proposed to predict the trend of financial market based on NLP analysis of the twitter handles of leaders of different fields. There are many models available to predict financial market based on only the historical data of the financial component but combining historical data with news and posts of the social media like Twitter is the main objective of the present work. Substantial improvement is shown in the result. The main features of the present work are: a) proposing completely generalized algorithm which is able to generate models for any twitter handle and any financial component, b) predicting the time window for a tweets effect on a stock price c) analyzing the effect of multiple twitter handles for predicting the trend. A detailed survey is done to find out the latest work in recent years in the similar field, find the research gap, and collect the required data for analysis and prediction. State-of-the-art algorithm is proposed and complete implementation with environment is given. An insightful trend of the result improvement considering the NLP analysis of twitter data on financial market components is shown. The Indian and USA financial markets are explored in the present work where as other markets can be taken in future. The socio-economic impact of the present work is discussed in conclusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12161v1</guid>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>q-fin.GN</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arijit Das, Tanmoy Nandi, Prasanta Saha, Suman Das, Saronyo Mukherjee, Sudip Kumar Naskar, Diganta Saha</dc:creator>
    </item>
    <item>
      <title>Explainable AutoML (xAutoML) with adaptive modeling for yield enhancement in semiconductor smart manufacturing</title>
      <link>https://arxiv.org/abs/2403.12381</link>
      <description>arXiv:2403.12381v1 Announce Type: new 
Abstract: Enhancing yield is recognized as a paramount driver to reducing production costs in semiconductor smart manufacturing. However, optimizing and ensuring high yield rates is a highly complex and technical challenge, especially while maintaining reliable yield diagnosis and prognosis, and this shall require understanding all the confounding factors in a complex condition. This study proposes a domain-specific explainable automated machine learning technique (termed xAutoML), which autonomously self-learns the optimal models for yield prediction, with an extent of explainability, and also provides insights on key diagnosis factors. The xAutoML incorporates tailored problem-solving functionalities in an auto-optimization pipeline to address the intricacies of semiconductor yield enhancement. Firstly, to capture the key diagnosis factors, knowledge-informed feature extraction coupled with model-agnostic key feature selection is designed. Secondly, combined algorithm selection and hyperparameter tuning with adaptive loss are developed to generate optimized classifiers for better defect prediction, and adaptively evolve in response to shifting data patterns. Moreover, a suite of explainability tools is provided throughout the AutoML pipeline, enhancing user understanding and fostering trust in the automated processes. The proposed xAutoML exhibits superior performance, with domain-specific refined countermeasures, adaptive optimization capabilities, and embedded explainability. Findings exhibit that the proposed xAutoML is a compelling solution for semiconductor yield improvement, defect diagnosis, and related applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12381v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weihong Zhai, Xiupeng Shi, Yiik Diew Wong, Qing Han, Lisheng Chen</dc:creator>
    </item>
    <item>
      <title>Bayesian estimation and uncertainty quantification of a temperature-dependent thermal conductivity</title>
      <link>https://arxiv.org/abs/2403.12696</link>
      <description>arXiv:2403.12696v1 Announce Type: new 
Abstract: We consider the problem of estimating a temperature-dependent thermal conductivity model (curve) from temperature measurements. We apply a Bayesian estimation approach that takes into account measurement errors and limited prior information of system properties. The approach intertwines system simulation and Markov chain Monte Carlo (MCMC) sampling. We investigate the impact of assuming different model classes -- cubic polynomials and piecewise linear functions -- their parametrization, and different types of prior information -- ranging from uninformative to informative. Piecewise linear functions require more parameters (conductivity values) to be estimated than the four parameters (coefficients or conductivity values) needed for cubic polynomials. The former model class is more flexible, but the latter requires less MCMC samples. While parametrizing polynomials with coefficients may feel more natural, it turns out that parametrizing them using conductivity values is far more natural for the specification of prior information. Robust estimation is possible for all model classes and parametrizations, as long as the prior information is accurate or not too informative. Gaussian Markov random field priors are especially well-suited for piecewise linear functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12696v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rodrigo L. S. Silva, Clemens Verhoosel, Erik Quaeghebeur</dc:creator>
    </item>
    <item>
      <title>Scalable Path Level Thermal History Simulation of PBF process validated by Melt Pool Images</title>
      <link>https://arxiv.org/abs/2308.02473</link>
      <description>arXiv:2308.02473v2 Announce Type: replace 
Abstract: In this paper we outline the development of a scalable PBF thermal history simulation built on CAPL and based on melt pool physics and dynamics. The new approach inherits linear scalability from CAPL and has three novel ingredients. Firstly, to simulate the laser scanning on a solid surface, we discretize the entire simulation domain instead of only the manufacturing toolpath by appending fictitious paths to the manufacturing toolpath. Secondly, to simulate the scanning on overlapping toolpaths, the path-scale simulations are initialized by a Voronoi diagram for line segments discretized from the manufacturing toolpath. Lastly, we propose a modified conduction model that considers the high thermal gradient around the melt pool. We validate the simulation against melt pool images captured with the co-axial melt pool monitoring (MPM) system on the NIST Additive Manufacturing Metrology Testbed (AMMT). Excellent agreements in the length and width of melt pools are found between simulations and experiments conducted on a custom-controlled laser powder bed fusion (LPBF) testbed on a nickel-alloy (IN625) solid surface. To the authors' best knowledge, this paper is the first to validate a full path-scale thermal history with experimentally acquired melt pool images. Comparing the simulation results and the experimental data, we discuss the influence of laser power on the melt pool length on the path-scale level. We also identify the possible ways to further improve the accuracy of the CAPL simulation without sacrificing efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02473v2</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Liu, Xingchen Liu, Goldy Kumar, Paul Witherell</dc:creator>
    </item>
    <item>
      <title>DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome</title>
      <link>https://arxiv.org/abs/2306.15006</link>
      <description>arXiv:2306.15006v2 Announce Type: replace-cross 
Abstract: Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates $36$ distinct datasets across $9$ tasks, with input lengths ranging from $70$ to $10000$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with $21 \times$ fewer parameters and approximately $92 \times$ less GPU time in pre-training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15006v2</guid>
      <category>q-bio.GN</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, Han Liu</dc:creator>
    </item>
    <item>
      <title>Solving High Frequency and Multi-Scale PDEs with Gaussian Processes</title>
      <link>https://arxiv.org/abs/2311.04465</link>
      <description>arXiv:2311.04465v2 Announce Type: replace-cross 
Abstract: Machine learning based solvers have garnered much attention in physical simulation and scientific computing, with a prominent example, physics-informed neural networks (PINNs). However, PINNs often struggle to solve high-frequency and multi-scale PDEs, which can be due to spectral bias during neural network training. To address this problem, we resort to the Gaussian process (GP) framework. To flexibly capture the dominant frequencies, we model the power spectrum of the PDE solution with a student $t$ mixture or Gaussian mixture. We apply the inverse Fourier transform to obtain the covariance function (by Wiener-Khinchin theorem). The covariance derived from the Gaussian mixture spectrum corresponds to the known spectral mixture kernel. Next, we estimate the mixture weights in the log domain, which we show is equivalent to placing a Jeffreys prior. It automatically induces sparsity, prunes excessive frequencies, and adjusts the remaining toward the ground truth. Third, to enable efficient and scalable computation on massive collocation points, which are critical to capture high frequencies, we place the collocation points on a grid, and multiply our covariance function at each input dimension. We use the GP conditional mean to predict the solution and its derivatives so as to fit the boundary condition and the equation itself. As a result, we can derive a Kronecker product structure in the covariance matrix. We use Kronecker product properties and multilinear algebra to promote computational efficiency and scalability, without low-rank approximations. We show the advantage of our method in systematic experiments. The code is released at \url{https://github.com/xuangu-fang/Gaussian-Process-Slover-for-High-Freq-PDE}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04465v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The Twelfth International Conference on Learning Representations (ICLR 2024)</arxiv:journal_reference>
      <dc:creator>Shikai Fang, Madison Cooley, Da Long, Shibo Li, Robert Kirby, Shandian Zhe</dc:creator>
    </item>
    <item>
      <title>Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming</title>
      <link>https://arxiv.org/abs/2402.13224</link>
      <description>arXiv:2402.13224v3 Announce Type: replace-cross 
Abstract: This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach demonstrates robustness against early disconnections by considering a wider range of uncertainty scenarios for optimization. The algorithm prioritizing user satisfaction over electricity cost achieves a 20% and 36% improvement in two user satisfaction metrics compared to an industry-standard baseline. Additionally, the algorithm striking the best balance between cost and user satisfaction exhibits a mere 3% relative cost increase compared to the theoretically optimal baseline - for which the nonanticipativity constraint is relaxed - while attaining 94% and 84% of the user satisfaction performance in the two used satisfaction metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13224v3</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alban Puech, Tristan Rigaut, William Templier, Maud Tournoud</dc:creator>
    </item>
  </channel>
</rss>
