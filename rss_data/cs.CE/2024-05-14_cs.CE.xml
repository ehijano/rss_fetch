<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 May 2024 04:00:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>An Efficient Multimodal Learning Framework to Comprehend Consumer Preferences Using BERT and Cross-Attention</title>
      <link>https://arxiv.org/abs/2405.07435</link>
      <description>arXiv:2405.07435v1 Announce Type: new 
Abstract: Today, the acquisition of various behavioral log data has enabled deeper understanding of customer preferences and future behaviors in the marketing field. In particular, multimodal deep learning has achieved highly accurate predictions by combining multiple types of data. Many of these studies utilize with feature fusion to construct multimodal models, which combines extracted representations from each modality. However, since feature fusion treats information from each modality equally, it is difficult to perform flexible analysis such as the attention mechanism that has been used extensively in recent years. Therefore, this study proposes a context-aware multimodal deep learning model that combines Bidirectional Encoder Representations from Transformers (BERT) and cross-attention Transformer, which dynamically changes the attention of deep-contextualized word representations based on background information such as consumer demographic and lifestyle variables. We conduct a comprehensive analysis and demonstrate the effectiveness of our model by comparing it with six reference models in three categories using behavioral logs stored on an online platform. In addition, we present an efficient multimodal learning method by comparing the learning efficiency depending on the optimizers and the prediction accuracy depending on the number of tokens in the text data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07435v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junichiro Niimi</dc:creator>
    </item>
    <item>
      <title>TOPress3D: 3D topology optimization with design-dependent pressure loads in MATLAB</title>
      <link>https://arxiv.org/abs/2405.07733</link>
      <description>arXiv:2405.07733v1 Announce Type: new 
Abstract: This paper introduces ``TOPress3D," a 3D topology optimization MATLAB code for structures subjected to design-dependent pressure loads. With a primary focus on pedagogical objectives, the code provides an easy learning experience, making it a valuable tool and practical gateway for newcomers, students, and researchers towards this topic. TOPress3D uses Darcy's law with a drainage term to link the given pressure load to design variables, which is converted to consistent nodal loads. Compliance minimization subjected to volume constraint optimization problems with pressure loads are solved. Load sensitivities arising due to design-dependent nature of the loads are evaluated using the adjoint-variable approach. The method of moving asymptotes is used to update the design variables. TOPress3D is constituted by six main parts. Each is described in detail. The code is also tailored to solve different problems. The robustness and success of the code are demonstrated while designing a few pressure load-bearing structures. The code is provided in Appendix B and is available with extensions in the supplementary material and publicly at \url{https://github.com/PrabhatIn/TOPress3D}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07733v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prabhat Kumar</dc:creator>
    </item>
    <item>
      <title>Molecular Docking via Weighted Subgraph Isomorphism on Quantum Annealers</title>
      <link>https://arxiv.org/abs/2405.06657</link>
      <description>arXiv:2405.06657v1 Announce Type: cross 
Abstract: Molecular docking is an essential step in the drug discovery process involving the detection of three-dimensional poses of a ligand inside the active site of the protein. In this paper, we address the Molecular Docking search phase by formulating the problem in QUBO terms, suitable for an annealing approach. We propose a problem formulation as a weighted subgraph isomorphism between the ligand graph and the grid of the target protein pocket. In particular, we applied a graph representation to the ligand embedding all the geometrical properties of the molecule including its flexibility, and we created a weighted spatial grid to the 3D space region inside the pocket. Results and performance obtained with quantum annealers are compared with classical simulated annealing solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06657v1</guid>
      <category>q-bio.BM</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Triuzzi, Riccardo Mengoni, Domenico Bonanni, Daniele Ottaviani, Andrea Beccari, Gianluca Palermo</dc:creator>
    </item>
    <item>
      <title>Instruction-Guided Bullet Point Summarization of Long Financial Earnings Call Transcripts</title>
      <link>https://arxiv.org/abs/2405.06669</link>
      <description>arXiv:2405.06669v1 Announce Type: cross 
Abstract: While automatic summarization techniques have made significant advancements, their primary focus has been on summarizing short news articles or documents that have clear structural patterns like scientific articles or government reports. There has not been much exploration into developing efficient methods for summarizing financial documents, which often contain complex facts and figures. Here, we study the problem of bullet point summarization of long Earning Call Transcripts (ECTs) using the recently released ECTSum dataset. We leverage an unsupervised question-based extractive module followed by a parameter efficient instruction-tuned abstractive module to solve this task. Our proposed model FLAN-FinBPS achieves new state-of-the-art performances outperforming the strongest baseline with 14.88% average ROUGE score gain, and is capable of generating factually consistent bullet point summaries that capture the important facts discussed in the ECTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06669v1</guid>
      <category>cs.CL</category>
      <category>cs.CE</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhendu Khatuya, Koushiki Sinha, Niloy Ganguly, Saptarshi Ghosh, Pawan Goyal</dc:creator>
    </item>
    <item>
      <title>Parameter-Efficient Instruction Tuning of Large Language Models For Extreme Financial Numeral Labelling</title>
      <link>https://arxiv.org/abs/2405.06671</link>
      <description>arXiv:2405.06671v1 Announce Type: cross 
Abstract: We study the problem of automatically annotating relevant numerals (GAAP metrics) occurring in the financial documents with their corresponding XBRL tags. Different from prior works, we investigate the feasibility of solving this extreme classification problem using a generative paradigm through instruction tuning of Large Language Models (LLMs). To this end, we leverage metric metadata information to frame our target outputs while proposing a parameter efficient solution for the task using LoRA. We perform experiments on two recently released financial numeric labeling datasets. Our proposed model, FLAN-FinXC, achieves new state-of-the-art performances on both the datasets, outperforming several strong baselines. We explain the better scores of our proposed model by demonstrating its capability for zero-shot as well as the least frequently occurring tags. Also, even when we fail to predict the XBRL tags correctly, our generated output has substantial overlap with the ground-truth in majority of the cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06671v1</guid>
      <category>cs.CL</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhendu Khatuya, Rajdeep Mukherjee, Akash Ghosh, Manjunath Hegde, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, Pawan Goyal</dc:creator>
    </item>
    <item>
      <title>Investigate the efficiency of incompressible flow simulations on CPUs and GPUs with BSAMR</title>
      <link>https://arxiv.org/abs/2405.07148</link>
      <description>arXiv:2405.07148v1 Announce Type: cross 
Abstract: Adaptive mesh refinement (AMR) is a classical technique about local refinement in space where needed, thus effectively reducing computational costs for HPC-based physics simulations. Although AMR has been used for many years, little reproducible research discusses the impact of software-based parameters on block-structured AMR (BSAMR) efficiency and how to choose them. This article primarily does parametric studies to investigate the computational efficiency of incompressible flows on a block-structured adaptive mesh. The parameters include refining block size, refining frequency, maximum level, and cycling method. A new projection skipping (PS) method is proposed, which brings insights about when and where the projections on coarser levels are safe to be omitted. We conduct extensive tests on different CPUs/GPUs for various 2D/3D incompressible flow cases, including bubble, RT instability, Taylor Green vortex, etc. Several valuable empirical conclusions are obtained to help guide simulations with BSAMR. Codes and all profiling data are available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07148v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dewen Liu, Shuai He, Haoran Cheng, Yadong Zeng</dc:creator>
    </item>
    <item>
      <title>CaFA: Global Weather Forecasting with Factorized Attention on Sphere</title>
      <link>https://arxiv.org/abs/2405.07395</link>
      <description>arXiv:2405.07395v1 Announce Type: cross 
Abstract: Accurate weather forecasting is crucial in various sectors, impacting decision-making processes and societal events. Data-driven approaches based on machine learning models have recently emerged as a promising alternative to numerical weather prediction models given their potential to capture physics of different scales from historical data and the significantly lower computational cost during the prediction stage. Renowned for its state-of-the-art performance across diverse domains, the Transformer model has also gained popularity in machine learning weather prediction. Yet applying Transformer architectures to weather forecasting, particularly on a global scale is computationally challenging due to the quadratic complexity of attention and the quadratic increase in spatial points as resolution increases. In this work, we propose a factorized-attention-based model tailored for spherical geometries to mitigate this issue. More specifically, it utilizes multi-dimensional factorized kernels that convolve over different axes where the computational complexity of the kernel is only quadratic to the axial resolution instead of overall resolution. The deterministic forecasting accuracy of the proposed model on $1.5^\circ$ and 0-7 days' lead time is on par with state-of-the-art purely data-driven machine learning weather prediction models. We also showcase the proposed model holds great potential to push forward the Pareto front of accuracy-efficiency for Transformer weather models, where it can achieve better accuracy with less computational cost compared to Transformer based models with standard attention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07395v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zijie Li, Anthony Zhou, Saurabh Patil, Amir Barati Farimani</dc:creator>
    </item>
    <item>
      <title>The calculation of air filtration efficiency through the visual basic programming language</title>
      <link>https://arxiv.org/abs/1405.1300</link>
      <description>arXiv:1405.1300v2 Announce Type: replace 
Abstract: The present article describes the development of a software which was written in visual basic programming language. The software calculates the particle collection efficiency and penetration of a fibrous filter medium for given values of particle diameter, fiber diameter filter thickness, volume fraction etc, during the process of air filtration. The progress of the development of software is divided into two steps, the design of graphical user interface (GUI) and the code writing. The code is mainly based on the mathematical model of particle collection efficiency of fibrous filters media.</description>
      <guid isPermaLink="false">oai:arXiv.org:1405.1300v2</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giorgos Kouropoulos</dc:creator>
    </item>
    <item>
      <title>Fast multiplication of random dense matrices with fixed sparse matrices</title>
      <link>https://arxiv.org/abs/2310.15419</link>
      <description>arXiv:2310.15419v2 Announce Type: replace 
Abstract: This work focuses on accelerating the multiplication of a dense random matrix with a (fixed) sparse matrix, which is frequently used in sketching algorithms. We develop a novel scheme that takes advantage of blocking and recomputation (on-the-fly random number generation) to accelerate this operation. The techniques we propose decrease memory movement, thereby increasing the algorithm's parallel scalability in shared memory architectures. On the Intel Frontera architecture, our algorithm can achieve 2x speedups over libraries such as Eigen and Intel MKL on some examples. In addition, with 32 threads, we can obtain a parallel efficiency of up to approximately 45%. We also present a theoretical analysis for the memory movement lower bound of our algorithm, showing that under mild assumptions, it's possible to beat the data movement lower bound of general matrix-matrix multiply (GEMM) by a factor of $\sqrt M$, where $M$ is the cache size. Finally, we incorporate our sketching algorithm into a randomized least squares solver. For extremely over-determined sparse input matrices, we show that our results are competitive with SuiteSparse; in some cases, we obtain a speedup of 10x over SuiteSparse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15419v2</guid>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Liang, Riley Murray, Ayd{\i}n Bulu\c{c}, James Demmel</dc:creator>
    </item>
    <item>
      <title>Real-time topology optimization via learnable mappings</title>
      <link>https://arxiv.org/abs/2311.08473</link>
      <description>arXiv:2311.08473v2 Announce Type: replace 
Abstract: In traditional topology optimization, the computing time required to iteratively update the material distribution within a design domain strongly depends on the complexity or size of the problem, limiting its application in real engineering contexts. This work proposes a multi-stage machine learning strategy that aims to predict an optimal topology and the related stress fields of interest, either in 2D or 3D, without resorting to any iterative analysis and design process. The overall topology optimization is treated as regression task in a low-dimensional latent space, that encodes the variability of the target designs. First, a fully-connected model is employed to surrogate the functional link between the parametric input space characterizing the design problem and the latent space representation of the corresponding optimal topology. The decoder branch of an autoencoder is then exploited to reconstruct the desired optimal topology from its latent representation. The deep learning models are trained on a dataset generated through a standard method of topology optimization implementing the solid isotropic material with penalization, for varying boundary and loading conditions. The underlying hypothesis behind the proposed strategy is that optimal topologies share enough common patterns to be compressed into small latent space representations without significant information loss. Results relevant to a 2D Messerschmitt-B\"olkow-Blohm beam and a 3D bridge case demonstrate the capabilities of the proposed framework to provide accurate optimal topology predictions in a fraction of a second.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08473v2</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/nme.7502</arxiv:DOI>
      <dc:creator>Gabriel Garayalde, Matteo Torzoni, Matteo Bruggi, Alberto Corigliano</dc:creator>
    </item>
    <item>
      <title>A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty</title>
      <link>https://arxiv.org/abs/2403.08901</link>
      <description>arXiv:2403.08901v2 Announce Type: replace 
Abstract: The widespread integration of deep neural networks in developing data-driven surrogate models for high-fidelity simulations of complex physical systems highlights the critical necessity for robust uncertainty quantification techniques and credibility assessment methodologies, ensuring the reliable deployment of surrogate models in consequential decision-making. This study presents the Occam Plausibility Algorithm for surrogate models (OPAL-surrogate), providing a systematic framework to uncover predictive neural network-based surrogate models within the large space of potential models, including various neural network classes and choices of architecture and hyperparameters. The framework is grounded in hierarchical Bayesian inferences and employs model validation tests to evaluate the credibility and prediction reliability of the surrogate models under uncertainty. Leveraging these principles, OPAL-surrogate introduces a systematic and efficient strategy for balancing the trade-off between model complexity, accuracy, and prediction uncertainty. The effectiveness of OPAL-surrogate is demonstrated through two modeling problems, including the deformation of porous materials for building insulation and turbulent combustion flow for the ablation of solid fuels within hybrid rocket motors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08901v2</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pratyush Kumar Singh, Kathryn A. Farrell-Maupin, Danial Faghihi</dc:creator>
    </item>
  </channel>
</rss>
