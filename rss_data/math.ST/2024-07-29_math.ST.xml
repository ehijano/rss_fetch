<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Jul 2024 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Network sampling based inference for subgraph counts and clustering coefficient in a Stochastic Block Model framework with some extensions to a sparse case</title>
      <link>https://arxiv.org/abs/2407.19191</link>
      <description>arXiv:2407.19191v1 Announce Type: new 
Abstract: Sampling is frequently used to collect data from large networks. In this article we provide valid asymptotic prediction intervals for subgraph counts and clustering coefficient of a population network when a network sampling scheme is used to observe the population. The theory is developed under a model based framework, where it is assumed that the population network is generated by a Stochastic Block Model (SBM). We study the effects of induced and ego-centric network formation, following the initial selection of nodes by Bernoulli sampling, and establish asymptotic normality of sample based subgraph count and clustering coefficient statistic under both network formation methods. The asymptotic results are developed under a joint design and model based approach, where the effect of sampling design is not ignored. In case of the sample based clustering coefficient statistic, we find that a bias correction is required in the ego-centric case, but there is no such bias in the induced case. We also extend the asymptotic normality results for estimated subgraph counts to a mildly sparse SBM framework, where edge probabilities decay to zero at a slow rate. In this sparse setting we find that the scaling and the maximum allowable decay rate for edge probabilities depend on the choice of the target subgraph. We obtain an expression for this maximum allowable decay rate and our results suggest that the rate becomes slower if the target subgraph has more edges in a certain sense. The simulation results suggest that the proposed prediction intervals have excellent coverage, even when the node selection probability is small and unknown SBM parameters are replaced by their estimates. Finally, the proposed methodology is applied to a real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19191v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anirban Mandal, Arindam Chatterjee</dc:creator>
    </item>
    <item>
      <title>On universal inference in Gaussian mixture models</title>
      <link>https://arxiv.org/abs/2407.19361</link>
      <description>arXiv:2407.19361v1 Announce Type: new 
Abstract: Recent work on game-theoretic statistics and safe anytime-valid inference (SAVI) provides new tools for statistical inference without assuming any regularity conditions. In particular, the framework of universal inference proposed by Wasserman, Ramdas, and Balakrishnan (2020) offers new solutions by modifying the likelihood ratio test in a data-splitting scheme. In this paper, we study the performance of the resulting split likelihood ratio test under Gaussian mixture models, which are canonical examples for models in which classical regularity conditions fail to hold. We first establish that under the null hypothesis, the split likelihood ratio statistic is asymptotically normal with increasing mean and variance. Moreover, contradicting the usual belief that the flexibility of SAVI and universal methods comes at the price of a significant loss of power, we are able to prove that universal inference surprisingly achieves the same detection rate $(n^{-1}\log\log n)^{1/2}$ as the classical likelihood ratio test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19361v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Shi, Mathias Drton</dc:creator>
    </item>
    <item>
      <title>Large-scale Multiple Testing of Cross-covariance Functions with Applications to Functional Network Models</title>
      <link>https://arxiv.org/abs/2407.19399</link>
      <description>arXiv:2407.19399v1 Announce Type: new 
Abstract: The estimation of functional networks through functional covariance and graphical models have recently attracted increasing attention in settings with high dimensional functional data, where the number of functional variables p is comparable to, and maybe larger than, the number of subjects. In this paper, we first reframe the functional covariance model estimation as a tuning-free problem of simultaneously testing p(p-1)/2 hypotheses for cross-covariance functions. Our procedure begins by constructing a Hilbert-Schmidt-norm-based test statistic for each pair, and employs normal quantile transformations for all test statistics, upon which a multiple testing step is proposed. We then explore the multiple testing procedure under a general error-contamination framework and establish that our procedure can control false discoveries asymptotically. Additionally, we demonstrate that our proposed methods for two concrete examples: the functional covariance model with partial observations and, importantly, the more challenging functional graphical model, can be seamlessly integrated into the general error-contamination framework, and, with verifiable conditions, achieve theoretical guarantees on effective false discovery control. Finally, we showcase the superiority of our proposals through extensive simulations and functional connectivity analysis of two neuroimaging datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19399v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qin Fang, Qing Jiang, Xinghao Qiao</dc:creator>
    </item>
    <item>
      <title>A Functional Principal Component Analysis Approach to Conditional Copula Estimation</title>
      <link>https://arxiv.org/abs/2407.19596</link>
      <description>arXiv:2407.19596v1 Announce Type: new 
Abstract: The conditional copula model arises when the dependence between random variables is influenced by another covariate. Despite its importance in modelling complex dependence structures, there are very few fully nonparametric approaches to estimate the conditional copula function. In the bivariate setting, the only nonparametric estimator for the conditional copula is based on Sklar's Theorem and proposed by Gijbels \textit{et al.} (2011). In this paper, we propose an alternative nonparametric approach %based on functional principal component analysis. We to construct an estimator for the bivariate conditional copula from the Karhunen-Lo\`eve representation of a suitably defined conditional copula process. We establish its consistency and weak convergence to a limit Gaussian process with explicit covariance function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19596v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toihir Soulaimana Djaloud, Cheikh Tidiane Seck</dc:creator>
    </item>
    <item>
      <title>Causal effect estimation under network interference with mean-field methods</title>
      <link>https://arxiv.org/abs/2407.19613</link>
      <description>arXiv:2407.19613v1 Announce Type: new 
Abstract: We study causal effect estimation from observational data under interference. The interference pattern is captured by an observed network. We adopt the chain graph framework of Tchetgen Tchetgen et. al. (2021), which allows (i) interaction among the outcomes of distinct study units connected along the graph and (ii) long range interference, whereby the outcome of an unit may depend on the treatments assigned to distant units connected along the interference network. For ``mean-field" interaction networks, we develop a new scalable iterative algorithm to estimate the causal effects. For gaussian weighted networks, we introduce a novel causal effect estimation algorithm based on Approximate Message Passing (AMP). Our algorithms are provably consistent under a ``high-temperature" condition on the underlying model. We estimate the (unknown) parameters of the model from data using maximum pseudo-likelihood and establish $\sqrt{n}$-consistency of this estimator in all parameter regimes. Finally, we prove that the downstream estimators obtained by plugging in estimated parameters into the aforementioned algorithms are consistent at high-temperature. Our methods can accommodate dense interactions among the study units -- a setting beyond reach using existing techniques. Our algorithms originate from the study of variational inference approaches in high-dimensional statistics; overall, we demonstrate the usefulness of these ideas in the context of causal effect estimation under interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19613v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sohom Bhattacharya, Subhabrata Sen</dc:creator>
    </item>
    <item>
      <title>On the asymptotic properties of product-PCA under the high-dimensional setting</title>
      <link>https://arxiv.org/abs/2407.19725</link>
      <description>arXiv:2407.19725v1 Announce Type: new 
Abstract: Principal component analysis (PCA) is a widely used dimension reduction method, but its performance is known to be non-robust to outliers. Recently, product-PCA (PPCA) has been shown to possess the efficiency-loss free ordering-robustness property: (i) in the absence of outliers, PPCA and PCA share the same asymptotic distributions; (ii), in the presence of outliers, PPCA is more ordering-robust than PCA in estimating the leading eigenspace. PPCA is thus different from the conventional robust PCA methods, and may deserve further investigations. In this article, we study the high-dimensional statistical properties of the PPCA eigenvalues via the techniques of random matrix theory. In particular, we derive the critical value for being distant spiked eigenvalues, the limiting values of the sample spiked eigenvalues, and the limiting spectral distribution of PPCA. Similar to the case of PCA, the explicit forms of the asymptotic properties of PPCA become available under the special case of the simple spiked model. These results enable us to more clearly understand the superiorities of PPCA in comparison with PCA. Numerical studies are conducted to verify our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19725v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hung Hung, Chin-Chun Yeh, Su-Yun Huang</dc:creator>
    </item>
    <item>
      <title>Information Criterion-Based Rank Estimation Methods for Factor Analysis: A Unified Selection Consistency Theorem and Numerical Comparison</title>
      <link>https://arxiv.org/abs/2407.19959</link>
      <description>arXiv:2407.19959v1 Announce Type: new 
Abstract: Over the years, numerous rank estimators for factor models have been proposed in the literature. This article focuses on information criterion-based rank estimators and investigates their consistency in rank selection. The gap conditions serve as necessary and sufficient conditions for rank estimators to achieve selection consistency under the general assumptions of random matrix theory. We establish a unified theorem on selection consistency, presenting the gap conditions for information criterion-based rank estimators with a unified formulation. To validate the theorem's assertion that rank selection consistency is solely determined by the gap conditions, we conduct extensive numerical simulations across various settings. Additionally, we undertake supplementary simulations to explore the strengths and limitations of information criterion-based estimators by comparing them with other types of rank estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19959v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Toshinari Morimoto, Hung Hung, Su-Yun Huang</dc:creator>
    </item>
    <item>
      <title>Non-standard boundary behaviour in binary mixture models</title>
      <link>https://arxiv.org/abs/2407.20162</link>
      <description>arXiv:2407.20162v1 Announce Type: new 
Abstract: Consider a binary mixture model of the form $F_\theta = (1-\theta)F_0 + \theta F_1$, where $F_0$ is standard Gaussian and $F_1$ is a completely specified heavy-tailed distribution with the same support. For a sample of $n$ independent and identically distributed values $X_i \sim F_\theta$, the maximum likelihood estimator $\hat\theta_n$ is asymptotically normal provided that $0 &lt; \theta &lt; 1$ is an interior point. This paper investigates the large-sample behaviour for boundary points, which is entirely different and strikingly asymmetric for $\theta=0$ and $\theta=1$. The reason for the asymmetry has to do with typical choices such that $F_0$ is an extreme boundary point and $F_1$ is usually not extreme. On the right boundary, well known results on boundary parameter problems are recovered, giving $\lim \mathbb{P}_1(\hat\theta_n &lt; 1)=1/2$. On the left boundary, $\lim\mathbb{P}_0(\hat\theta_n &gt; 0)=1-1/\alpha$, where $1\leq \alpha \leq 2$ indexes the domain of attraction of the density ratio $f_1(X)/f_0(X)$ when $X\sim F_0$. For $\alpha=1$, which is the most important case in practice, we show how the tail behaviour of $F_1$ governs the rate at which $\mathbb{P}_0(\hat\theta_n &gt; 0)$ tends to zero. A new limit theorem for the joint distribution of the sample maximum and sample mean conditional on positivity establishes multiple inferential anomalies. Most notably, given $\hat\theta_n &gt; 0$, the likelihood ratio statistic has a conditional null limit distribution that is not $\chi^2_1$, but is determined by the joint limit theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20162v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heather Battey, Peter McCullagh, Daniel Xiang</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Confidence Regions in Sparse MRI</title>
      <link>https://arxiv.org/abs/2407.18964</link>
      <description>arXiv:2407.18964v1 Announce Type: cross 
Abstract: One of the most promising solutions for uncertainty quantification in high-dimensional statistics is the debiased LASSO that relies on unconstrained $\ell_1$-minimization. The initial works focused on real Gaussian designs as a toy model for this problem. However, in medical imaging applications, such as compressive sensing for MRI, the measurement system is represented by a (subsampled) complex Fourier matrix. The purpose of this work is to extend the method to the MRI case in order to construct confidence intervals for each pixel of an MR image. We show that a sufficient amount of data is $n \gtrsim \max\{ s_0\log^2 s_0\log p, s_0 \log^2 p \}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18964v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICASSP49357.2023.10096320</arxiv:DOI>
      <dc:creator>Frederik Hoppe, Felix Krahmer, Claudio Mayrink Verdun, Marion Menzel, Holger Rauhut</dc:creator>
    </item>
    <item>
      <title>Multimodal data integration and cross-modal querying via orchestrated approximate message passing</title>
      <link>https://arxiv.org/abs/2407.19030</link>
      <description>arXiv:2407.19030v1 Announce Type: cross 
Abstract: The need for multimodal data integration arises naturally when multiple complementary sets of features are measured on the same sample. Under a dependent multifactor model, we develop a fully data-driven orchestrated approximate message passing algorithm for integrating information across these feature sets to achieve statistically optimal signal recovery. In practice, these reference data sets are often queried later by new subjects that are only partially observed. Leveraging on asymptotic normality of estimates generated by our data integration method, we further develop an asymptotically valid prediction set for the latent representation of any such query subject. We demonstrate the prowess of both the data integration and the prediction set construction algorithms on a tri-modal single-cell dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19030v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sagnik Nandy, Zongming Ma</dc:creator>
    </item>
    <item>
      <title>Identification and Inference with Invalid Instruments</title>
      <link>https://arxiv.org/abs/2407.19558</link>
      <description>arXiv:2407.19558v1 Announce Type: cross 
Abstract: Instrumental variables (IVs) are widely used to study the causal effect of an exposure on an outcome in the presence of unmeasured confounding. IVs require an instrument, a variable that is (A1) associated with the exposure, (A2) has no direct effect on the outcome except through the exposure, and (A3) is not related to unmeasured confounders. Unfortunately, finding variables that satisfy conditions (A2) or (A3) can be challenging in practice. This paper reviews works where instruments may not satisfy conditions (A2) or (A3), which we refer to as invalid instruments. We review identification and inference under different violations of (A2) or (A3), specifically under linear models, non-linear models, and heteroskedatic models. We conclude with an empirical comparison of various methods by re-analyzing the effect of body mass index on systolic blood pressure from the UK Biobank.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19558v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyunseung Kang, Zijian Guo, Zhonghua Liu, Dylan Small</dc:creator>
    </item>
    <item>
      <title>Sharp Bounds for Poly-GNNs and the Effect of Graph Noise</title>
      <link>https://arxiv.org/abs/2407.19567</link>
      <description>arXiv:2407.19567v1 Announce Type: cross 
Abstract: We investigate the classification performance of graph neural networks with graph-polynomial features, poly-GNNs, on the problem of semi-supervised node classification. We analyze poly-GNNs under a general contextual stochastic block model (CSBM) by providing a sharp characterization of the rate of separation between classes in their output node representations. A question of interest is whether this rate depends on the depth of the network $k$, i.e., whether deeper networks can achieve a faster separation? We provide a negative answer to this question: for a sufficiently large graph, a depth $k &gt; 1$ poly-GNN exhibits the same rate of separation as a depth $k=1$ counterpart. Our analysis highlights and quantifies the impact of ``graph noise'' in deep GNNs and shows how noise in the graph structure can dominate other sources of signal in the graph, negating any benefit further aggregation provides. Our analysis also reveals subtle differences between even and odd-layered GNNs in how the feature noise propagates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19567v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luciano Vinas, Arash A. Amini</dc:creator>
    </item>
    <item>
      <title>Revisiting Agnostic PAC Learning</title>
      <link>https://arxiv.org/abs/2407.19777</link>
      <description>arXiv:2407.19777v1 Announce Type: cross 
Abstract: PAC learning, dating back to Valiant'84 and Vapnik and Chervonenkis'64,'74, is a classic model for studying supervised learning. In the agnostic setting, we have access to a hypothesis set $\mathcal{H}$ and a training set of labeled samples $(x_1,y_1),\dots,(x_n,y_n) \in \mathcal{X} \times \{-1,1\}$ drawn i.i.d. from an unknown distribution $\mathcal{D}$. The goal is to produce a classifier $h : \mathcal{X} \to \{-1,1\}$ that is competitive with the hypothesis $h^\star_{\mathcal{D}} \in \mathcal{H}$ having the least probability of mispredicting the label $y$ of a new sample $(x,y)\sim \mathcal{D}$.
  Empirical Risk Minimization (ERM) is a natural learning algorithm, where one simply outputs the hypothesis from $\mathcal{H}$ making the fewest mistakes on the training data. This simple algorithm is known to have an optimal error in terms of the VC-dimension of $\mathcal{H}$ and the number of samples $n$.
  In this work, we revisit agnostic PAC learning and first show that ERM is in fact sub-optimal if we treat the performance of the best hypothesis, denoted $\tau:=\Pr_{\mathcal{D}}[h^\star_{\mathcal{D}}(x) \neq y]$, as a parameter. Concretely we show that ERM, and any other proper learning algorithm, is sub-optimal by a $\sqrt{\ln(1/\tau)}$ factor. We then complement this lower bound with the first learning algorithm achieving an optimal error for nearly the full range of $\tau$. Our algorithm introduces several new ideas that we hope may find further applications in learning theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19777v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Steve Hanneke, Kasper Green Larsen, Nikita Zhivotovskiy</dc:creator>
    </item>
    <item>
      <title>Robust estimation of parameters in logistic regression via solving the Cramer-von Mises type L2 optimization problem</title>
      <link>https://arxiv.org/abs/1703.07044</link>
      <description>arXiv:1703.07044v3 Announce Type: replace 
Abstract: This paper proposes a novel method to estimate parameters in a logistic regression model. After obtaining the estimators, their asymptotic properties are rigorously investigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:1703.07044v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiwoong Kim</dc:creator>
    </item>
    <item>
      <title>General Inferential Limits Under Differential and Pufferfish Privacy</title>
      <link>https://arxiv.org/abs/2401.15491</link>
      <description>arXiv:2401.15491v5 Announce Type: replace 
Abstract: Differential privacy (DP) is a class of mathematical standards for assessing the privacy provided by a data-release mechanism. This work concerns two important flavors of DP that are related yet conceptually distinct: pure $\varepsilon$-differential privacy ($\varepsilon$-DP) and Pufferfish privacy. We restate $\varepsilon$-DP and Pufferfish privacy as Lipschitz continuity conditions and provide their formulations in terms of an object from the imprecise probability literature: the interval of measures. We use these formulations to derive limits on key quantities in frequentist hypothesis testing and in Bayesian inference using data that are sanitised according to either of these two privacy standards. Under very mild conditions, the results in this work are valid for arbitrary parameters, priors and data generating models. These bounds are weaker than those attainable when analysing specific data generating models or data-release mechanisms. However, they provide generally applicable limits on the ability to learn from differentially private data - even when the analyst's knowledge of the model or mechanism is limited. They also shed light on the semantic interpretations of the two DP flavors under examination, a subject of contention in the current literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15491v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijar.2024.109242</arxiv:DOI>
      <dc:creator>James Bailie, Ruobin Gong</dc:creator>
    </item>
    <item>
      <title>Regression graphs and sparsity-inducing reparametrizations</title>
      <link>https://arxiv.org/abs/2402.09112</link>
      <description>arXiv:2402.09112v2 Announce Type: replace 
Abstract: That parametrization and population-level sparsity are intrinsically linked raises the possibility that relevant models, not obviously sparse in their natural formulation, exhibit a population-level sparsity after reparametrization. In covariance models, positive-definiteness enforces additional constraints on how sparsity can legitimately manifest. It is therefore natural to consider reparametrization maps in which sparsity respects positive definiteness. The main purpose of this paper is to provide insight into structures on the physically-natural scale that induce and are induced by sparsity after reparametrization. In a sense the richest of the four structures initially uncovered turns out to be that of the joint-response graphs studied by Wermuth &amp; Cox (2004), while the most restrictive is that induced by sparsity on the scale of the matrix logarithm, studied by Battey (2017). This points to a class of reparametrizations for the chain-graph models (Andersson et al., 2001), with undirected and directed acyclic graphs as special cases. While much of the paper is focused on exact zeros after reparametrization, an important insight is the interpretation of approximate zeros, which explains the modelling implications of enforcing sparsity after reparameterization: in effect, the relation between two variables would be declared null if relatively direct regression effects were negligible and other effects manifested through long paths. The insights have a bearing on methodology, some aspects of which are discussed in the supplementary material where an estimator with high-dimensional statistical guarantees is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09112v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakub Rybak, Heather Battey, Karthik Bharath</dc:creator>
    </item>
    <item>
      <title>A Wasserstein perspective of Vanilla GANs</title>
      <link>https://arxiv.org/abs/2403.15312</link>
      <description>arXiv:2403.15312v2 Announce Type: replace 
Abstract: The empirical success of Generative Adversarial Networks (GANs) caused an increasing interest in theoretical research. The statistical literature is mainly focused on Wasserstein GANs and generalizations thereof, which especially allow for good dimension reduction properties. Statistical results for Vanilla GANs, the original optimization problem, are still rather limited and require assumptions such as smooth activation functions and equal dimensions of the latent space and the ambient space. To bridge this gap, we draw a connection from Vanilla GANs to the Wasserstein distance. By doing so, existing results for Wasserstein GANs can be extended to Vanilla GANs. In particular, we obtain an oracle inequality for Vanilla GANs in Wasserstein distance. The assumptions of this oracle inequality are designed to be satisfied by network architectures commonly used in practice, such as feedforward ReLU networks. By providing a quantitative result for the approximation of a Lipschitz function by a feedforward ReLU network with bounded H\"older norm, we conclude a rate of convergence for Vanilla GANs as well as Wasserstein GANs as estimators of the unknown probability distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15312v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lea Kunkel, Mathias Trabs</dc:creator>
    </item>
    <item>
      <title>High-dimensional bootstrap and asymptotic expansion</title>
      <link>https://arxiv.org/abs/2404.05006</link>
      <description>arXiv:2404.05006v3 Announce Type: replace 
Abstract: The recent seminal work of Chernozhukov, Chetverikov and Kato has shown that bootstrap approximation for the maximum of a sum of independent random vectors is justified even when the dimension is much larger than the sample size. In this context, numerical experiments suggest that third-moment match bootstrap approximations would outperform normal approximation even without studentization, but the existing theoretical results cannot explain this phenomenon. In this paper, we first show that Edgeworth expansion, if justified, can give an explanation for this phenomenon. Second, we obtain valid Edgeworth expansions in the high-dimensional setting when the random vectors have Stein kernels. Finally, we prove the second-order accuracy of a double wild bootstrap method in this setting. As a byproduct, we find an interesting blessing of dimensionality phenomenon: The single third-moment match wild bootstrap is already second-order accurate in high-dimensions if the covariance matrix has identical diagonal entries and bounded eigenvalues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05006v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Koike</dc:creator>
    </item>
    <item>
      <title>A Berry-Esseen theorem for incomplete U-statistics with Bernoulli sampling</title>
      <link>https://arxiv.org/abs/2406.05394</link>
      <description>arXiv:2406.05394v2 Announce Type: replace 
Abstract: There has been a resurgence of interest in the asymptotic normality of incomplete U-statistics that only sum over roughly as many kernel evaluations as there are data samples, due to its computational efficiency and usefulness in quantifying the uncertainty for ensemble-based predictions. In this paper, we focus on the normal convergence of one such construction, the incomplete U-statistic with Bernoulli sampling, based on a raw sample of size $n$ and a computational budget $N$. Under minimalistic moment assumptions on the kernel, we offer accompanying Berry-Esseen bounds of the natural rate $1/\sqrt{\min(N, n)}$ that characterize the normal approximating accuracy involved when $n \asymp N$, i.e. $n$ and $N$ are of the same order in such a way that $n/N$ is lower-and-upper bounded by constants. Our key techniques include Stein's method specialized for the so-called Studentized nonlinear statistics, and an exponential lower tail bound for non-negative kernel U-statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05394v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dennis Leung</dc:creator>
    </item>
    <item>
      <title>Statistical Analysis of Data Repeatability Measures</title>
      <link>https://arxiv.org/abs/2005.11911</link>
      <description>arXiv:2005.11911v4 Announce Type: replace-cross 
Abstract: The advent of modern data collection and processing techniques has seen the size, scale, and complexity of data grow exponentially. A seminal step in leveraging these rich datasets for downstream inference is understanding the characteristics of the data which are repeatable -- the aspects of the data that are able to be identified under a duplicated analysis. Conflictingly, the utility of traditional repeatability measures, such as the intraclass correlation coefficient, under these settings is limited. In recent work, novel data repeatability measures have been introduced in the context where a set of subjects are measured twice or more, including: fingerprinting, rank sums, and generalizations of the intraclass correlation coefficient. However, the relationships between, and the best practices among these measures remains largely unknown. In this manuscript, we formalize a novel repeatability measure, discriminability. We show that it is deterministically linked with the correlation coefficient under univariate random effect models, and has desired property of optimal accuracy for inferential tasks using multivariate measurements. Additionally, we overview and systematically compare repeatability statistics using both theoretical results and simulations. We show that the rank sum statistic is deterministically linked to a consistent estimator of discriminability. The power of permutation tests derived from these measures are compared numerically under Gaussian and non-Gaussian settings, with and without simulated batch effects. Motivated by both theoretical and empirical results, we provide methodological recommendations for each benchmark setting to serve as a resource for future analyses. We believe these recommendations will play an important role towards improving repeatability in fields such as functional magnetic resonance imaging, genomics, pharmacology, and more.</description>
      <guid isPermaLink="false">oai:arXiv.org:2005.11911v4</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeyi Wang, Eric Bridgeford, Shangsi Wang, Joshua T. Vogelstein, Brian Caffo</dc:creator>
    </item>
    <item>
      <title>Multitask Learning and Bandits via Robust Statistics</title>
      <link>https://arxiv.org/abs/2112.14233</link>
      <description>arXiv:2112.14233v5 Announce Type: replace-cross 
Abstract: Decision-makers often simultaneously face many related but heterogeneous learning problems. For instance, a large retailer may wish to learn product demand at different stores to solve pricing or inventory problems, making it desirable to learn jointly for stores serving similar customers; alternatively, a hospital network may wish to learn patient risk at different providers to allocate personalized interventions, making it desirable to learn jointly for hospitals serving similar patient populations. Motivated by real datasets, we study a natural setting where the unknown parameter in each learning instance can be decomposed into a shared global parameter plus a sparse instance-specific term. We propose a novel two-stage multitask learning estimator that exploits this structure in a sample-efficient way, using a unique combination of robust statistics (to learn across similar instances) and LASSO regression (to debias the results). Our estimator yields improved sample complexity bounds in the feature dimension $d$ relative to commonly-employed estimators; this improvement is exponential for "data-poor" instances, which benefit the most from multitask learning. We illustrate the utility of these results for online learning by embedding our multitask estimator within simultaneous contextual bandit algorithms. We specify a dynamic calibration of our estimator to appropriately balance the bias-variance tradeoff over time, improving the resulting regret bounds in the context dimension $d$. Finally, we illustrate the value of our approach on synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.14233v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kan Xu, Hamsa Bastani</dc:creator>
    </item>
    <item>
      <title>Optimal Pre-Analysis Plans: Statistical Decisions Subject to Implementability</title>
      <link>https://arxiv.org/abs/2208.09638</link>
      <description>arXiv:2208.09638v3 Announce Type: replace-cross 
Abstract: What is the purpose of pre-analysis plans, and how should they be designed? We model the interaction between an agent who analyzes data and a principal who makes a decision based on agent reports. The agent could be the manufacturer of a new drug, and the principal a regulator deciding whether the drug is approved. Or the agent could be a researcher submitting a research paper, and the principal an editor deciding whether it is published. The agent decides which statistics to report to the principal. The principal cannot verify whether the analyst reported selectively. Absent a pre-analysis message, if there are conflicts of interest, then many desirable decision rules cannot be implemented. Allowing the agent to send a message before seeing the data increases the set of decision rules that can be implemented, and allows the principal to leverage agent expertise. The optimal mechanisms that we characterize require pre-analysis plans. Applying these results to hypothesis testing, we show that optimal rejection rules pre-register a valid test, and make worst-case assumptions about unreported statistics. Optimal tests can be found as a solution to a linear-programming problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.09638v3</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maximilian Kasy, Jann Spiess</dc:creator>
    </item>
    <item>
      <title>An Approximation Theory Framework for Measure-Transport Sampling Algorithms</title>
      <link>https://arxiv.org/abs/2302.13965</link>
      <description>arXiv:2302.13965v3 Announce Type: replace-cross 
Abstract: This article presents a general approximation-theoretic framework to analyze measure transport algorithms for probabilistic modeling. A primary motivating application for such algorithms is sampling -- a central task in statistical inference and generative modeling. We provide a priori error estimates in the continuum limit, i.e., when the measures (or their densities) are given, but when the transport map is discretized or approximated using a finite-dimensional function space. Our analysis relies on the regularity theory of transport maps and on classical approximation theory for high-dimensional functions. A third element of our analysis, which is of independent interest, is the development of new stability estimates that relate the distance between two maps to the distance~(or divergence) between the pushforward measures they define. We present a series of applications of our framework, where quantitative convergence rates are obtained for practical problems using Wasserstein metrics, maximum mean discrepancy, and Kullback--Leibler divergence. Specialized rates for approximations of the popular triangular Kn{\"o}the-Rosenblatt maps are obtained, followed by numerical experiments that demonstrate and extend our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13965v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo Baptista, Bamdad Hosseini, Nikola B. Kovachki, Youssef M. Marzouk, Amir Sagiv</dc:creator>
    </item>
    <item>
      <title>On Consistency of Signature Using Lasso</title>
      <link>https://arxiv.org/abs/2305.10413</link>
      <description>arXiv:2305.10413v3 Announce Type: replace-cross 
Abstract: Signatures are iterated path integrals of continuous and discrete-time processes, and their universal nonlinearity linearizes the problem of feature selection in time series data analysis. This paper studies the consistency of signature using Lasso regression, both theoretically and numerically. We establish conditions under which the Lasso regression is consistent both asymptotically and in finite sample. Furthermore, we show that the Lasso regression is more consistent with the It\^o signature for time series and processes that are closer to the Brownian motion and with weaker inter-dimensional correlations, while it is more consistent with the Stratonovich signature for mean-reverting time series and processes. We demonstrate that signature can be applied to learn nonlinear functions and option prices with high accuracy, and the performance depends on properties of the underlying process and the choice of the signature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.10413v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Guo, Binnan Wang, Ruixun Zhang, Chaoyi Zhao</dc:creator>
    </item>
    <item>
      <title>Structural restrictions in local causal discovery: identifying direct causes of a target variable</title>
      <link>https://arxiv.org/abs/2307.16048</link>
      <description>arXiv:2307.16048v2 Announce Type: replace-cross 
Abstract: We consider the problem of learning a set of direct causes of a target variable from an observational joint distribution. Learning directed acyclic graphs (DAGs) that represent the causal structure is a fundamental problem in science. Several results are known when the full DAG is identifiable from the distribution, such as assuming a nonlinear Gaussian data-generating process. Here, we are only interested in identifying the direct causes of one target variable (local causal structure), not the full DAG. This allows us to relax the identifiability assumptions and develop possibly faster and more robust algorithms. In contrast to the Invariance Causal Prediction framework, we only assume that we observe one environment without any interventions. We discuss different assumptions for the data-generating process of the target variable under which the set of direct causes is identifiable from the distribution. While doing so, we put essentially no assumptions on the variables other than the target variable. In addition to the novel identifiability results, we provide two practical algorithms for estimating the direct causes from a finite random sample and demonstrate their effectiveness on several benchmark and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16048v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juraj Bodik, Val\'erie Chavez-Demoulin</dc:creator>
    </item>
    <item>
      <title>Ensemble Kalman Filters with Resampling</title>
      <link>https://arxiv.org/abs/2308.08751</link>
      <description>arXiv:2308.08751v2 Announce Type: replace-cross 
Abstract: Filtering is concerned with online estimation of the state of a dynamical system from partial and noisy observations. In applications where the state of the system is high dimensional, ensemble Kalman filters are often the method of choice. These algorithms rely on an ensemble of interacting particles to sequentially estimate the state as new observations become available. Despite the practical success of ensemble Kalman filters, theoretical understanding is hindered by the intricate dependence structure of the interacting particles. This paper investigates ensemble Kalman filters that incorporate an additional resampling step to break the dependency between particles. The new algorithm is amenable to a theoretical analysis that extends and improves upon those available for filters without resampling, while also performing well in numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.08751v2</guid>
      <category>eess.SY</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1137/23M1594935</arxiv:DOI>
      <dc:creator>Omar Al Ghattas, Jiajun Bao, Daniel Sanz-Alonso</dc:creator>
    </item>
    <item>
      <title>Symmetry Lie Algebras of Varieties with Applications to Algebraic Statistics</title>
      <link>https://arxiv.org/abs/2309.10741</link>
      <description>arXiv:2309.10741v3 Announce Type: replace-cross 
Abstract: The motivation for this paper is to detect when an irreducible projective variety V is not toric. We do this by analyzing a Lie group and a Lie algebra associated to V. If the dimension of V is strictly less than the dimension of the above mentioned objects, then V is not a toric variety. We provide an algorithm to compute the Lie algebra of an irreducible variety and use it to provide examples of non-toric statistical models in algebraic statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10741v3</guid>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aida Maraj, Arpan Pal</dc:creator>
    </item>
    <item>
      <title>Transfer learning for piecewise-constant mean estimation: Optimality, $\ell_1$- and $\ell_0$-penalisation</title>
      <link>https://arxiv.org/abs/2310.05646</link>
      <description>arXiv:2310.05646v4 Announce Type: replace-cross 
Abstract: We study transfer learning for estimating piecewise-constant signals when source data, which may be relevant but disparate, are available in addition to the target data. We first investigate transfer learning estimators that respectively employ $\ell_1$- and $\ell_0$-penalties for unisource data scenarios and then generalise these estimators to accommodate multisources. To further reduce estimation errors, especially when some sources significantly differ from the target, we introduce an informative source selection algorithm. We then examine these estimators with multisource selection and establish their minimax optimality. Unlike the common narrative in the transfer learning literature that the performance is enhanced through large source sample sizes, our approaches leverage higher observation frequencies and accommodate diverse frequencies across multiple sources. Our theoretical findings are supported by extensive numerical experiments, with the code available online, see https://github.com/chrisfanwang/transferlearning</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05646v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Wang, Yi Yu</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to functional regression: theory and computation</title>
      <link>https://arxiv.org/abs/2312.14086</link>
      <description>arXiv:2312.14086v2 Announce Type: replace-cross 
Abstract: We propose a novel Bayesian methodology for inference in functional linear and logistic regression models based on the theory of reproducing kernel Hilbert spaces (RKHS's). We introduce general models that build upon the RKHS generated by the covariance function of the underlying stochastic process, and whose formulation includes as particular cases all finite-dimensional models based on linear combinations of marginals of the process, which can collectively be seen as a dense subspace made of simple approximations. By imposing a suitable prior distribution on this dense functional space we can perform data-driven inference via standard Bayes methodology, estimating the posterior distribution through reversible jump Markov chain Monte Carlo methods. In this context, our contribution is two-fold. First, we derive a theoretical result that guarantees posterior consistency, based on an application of a classic theorem of Doob to our RKHS setting. Second, we show that several prediction strategies stemming from our Bayesian procedure are competitive against other usual alternatives in both simulations and real data sets, including a Bayesian-motivated variable selection method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14086v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jos\'e R. Berrendero, Antonio Co\'in, Antonio Cuevas</dc:creator>
    </item>
    <item>
      <title>Doublethink: simultaneous Bayesian-frequentist model-averaged hypothesis testing</title>
      <link>https://arxiv.org/abs/2312.17566</link>
      <description>arXiv:2312.17566v2 Announce Type: replace-cross 
Abstract: Establishing the frequentist properties of Bayesian approaches widens their appeal and offers new understanding. In hypothesis testing, Bayesian model averaging addresses the problem that conclusions are sensitive to variable selection. But Bayesian false discovery rate (FDR) guarantees are contingent on prior assumptions that may be disputed. Here we show that Bayesian model-averaged hypothesis testing is a closed testing procedure that controls the frequentist familywise error rate (FWER) in the strong sense. The rate converges pointwise as the sample size grows and, under some conditions, uniformly. The `Doublethink' method computes simultaneous posterior odds and asymptotic p-values for model-averaged hypothesis testing. We explore its benefits, including post-hoc variable selection, and limitations, including finite-sample inflation, through a Mendelian randomization study and simulations comparing approaches like LASSO, stepwise regression, the Benjamini-Hochberg procedure and e-values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17566v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helen R. Fryer, Nicolas Arning, Daniel J. Wilson</dc:creator>
    </item>
    <item>
      <title>Efficient combination of observational and experimental datasets under general restrictions on outcome mean functions</title>
      <link>https://arxiv.org/abs/2406.06941</link>
      <description>arXiv:2406.06941v2 Announce Type: replace-cross 
Abstract: A researcher collecting data from a randomized controlled trial (RCT) often has access to an auxiliary observational dataset that may be confounded or otherwise biased for estimating causal effects. Common modeling assumptions impose restrictions on the outcome mean function - the conditional expectation of the outcome of interest given observed covariates - in the two datasets. Running examples from the literature include settings where the observational dataset is subject to outcome-mediated selection bias or to confounding bias taking an assumed parametric form. We propose a succinct framework to derive the efficient influence function for any identifiable pathwise differentiable estimand under a general class of restrictions on the outcome mean function. This uncovers surprising results that with homoskedastic outcomes and a constant propensity score in the RCT, even strong parametric assumptions cannot improve the semiparametric lower bound for estimating various average treatment effects. We then leverage double machine learning to construct a one-step estimator that achieves the semiparametric efficiency bound even in cases when the outcome mean function and other nuisance parameters are estimated nonparametrically. The goal is to empower a researcher with custom, previously unstudied modeling restrictions on the outcome mean function to systematically construct causal estimators that maximially leverage their assumptions for variance reduction. We demonstrate the finite sample precision gains of our estimator over existing approaches in extensions of various numerical studies and data examples from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06941v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison H. Li</dc:creator>
    </item>
    <item>
      <title>Finely Stratified Rerandomization Designs</title>
      <link>https://arxiv.org/abs/2407.03279</link>
      <description>arXiv:2407.03279v2 Announce Type: replace-cross 
Abstract: We study estimation and inference on causal parameters under finely stratified rerandomization designs, which use baseline covariates to match units into groups (e.g. matched pairs), then rerandomize within-group treatment assignments until a balance criterion is satisfied. We show that finely stratified rerandomization does partially linear regression adjustment "by design," providing nonparametric control over the stratified covariates and linear control over the rerandomized covariates. We introduce several new rerandomization schemes, allowing for imbalance metrics based on nonlinear estimators. We also propose a novel minimax scheme that uses pilot data or prior information to minimize the computational cost of rerandomization, subject to a strict bound on statistical efficiency. While the asymptotic distribution of generalized method of moments (GMM) estimators under stratified rerandomization is generically non-normal, we show how to restore asymptotic normality using ex-post linear adjustment tailored to the stratification. This enables simple asymptotically exact inference on superpopulation parameters, as well as efficient conservative inference on finite population parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03279v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Max Cytrynbaum</dc:creator>
    </item>
  </channel>
</rss>
