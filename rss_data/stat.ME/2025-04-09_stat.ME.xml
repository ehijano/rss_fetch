<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Apr 2025 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Distributed Lag Interaction Model with Index Modification</title>
      <link>https://arxiv.org/abs/2504.06363</link>
      <description>arXiv:2504.06363v1 Announce Type: new 
Abstract: Epidemiological evidence supports an association between exposure to air pollution during pregnancy and birth and child health outcomes. Typically, such associations are estimated by regressing an outcome on daily or weekly measures of exposure during pregnancy using a distributed lag model. However, these associations may be modified by multiple factors. We propose a distributed lag interaction model with index modification that allows for effect modification of a functional predictor by a weighted average of multiple modifiers. Our model allows for simultaneous estimation of modifier index weights and the exposure-time-response function via a spline cross-basis in a Bayesian hierarchical framework. Through simulations, we showed that our model out-performs competing methods when there are multiple modifiers of unknown importance. We applied our proposed method to a Colorado birth cohort to estimate the association between birth weight and air pollution modified by a neighborhood-vulnerability index and to a Mexican birth cohort to estimate the association between birthing-parent cardio-metabolic endpoints and air pollution modified by a birthing-parent lifetime stress index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06363v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danielle Demateis, Sandra India Aldana, Robert O. Wright, Rosalind Wright, Andrea Baccarelli, Elena Colicino, Ander Wilson, Kayleigh P. Keller</dc:creator>
    </item>
    <item>
      <title>Sequential Change Point Detection with FDR Control in Reconfigurable Sensor Networks</title>
      <link>https://arxiv.org/abs/2504.06526</link>
      <description>arXiv:2504.06526v1 Announce Type: new 
Abstract: This paper investigates sequential change-point detection in reconfigurable sensor networks. In this problem, data from multiple sensors are observed sequentially. Each sensor can have a unique change point, and the data distribution differs before and after the change. We aim to detect these changes as quickly as possible once they have occurred while controlling the false discovery rate at all times. Our setting is more realistic than traditional settings in that (1) the set of active sensors - i.e., those from which data can be collected - can change over time through the deactivation of existing sensors and the addition of new sensors, and (2) dependencies can occur both between sensors and across time points. We propose powerful e-value-based detection procedures that control the false discovery rate uniformly over time. Numerical experiments demonstrate that, with the same false discovery rate target, our procedures achieve superior performance compared to existing methods, exhibiting lower false non-discovery rates and reduced detection delays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06526v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seungwon Lee, Yunxiao Chen, Xiaoou Li</dc:creator>
    </item>
    <item>
      <title>Interconnections of Multimorbidity-Related Clinical Outcomes: Analysis of Health Administrative Claims Data with a Dynamic Network Approach</title>
      <link>https://arxiv.org/abs/2504.06540</link>
      <description>arXiv:2504.06540v1 Announce Type: new 
Abstract: Given the rising complexity and burden of multimorbidity, it is crucial to provide evidence-based support for managing multimorbidity-related clinical outcomes. This study introduces a dynamic network approach to investigate conditional and time-varying interconnections in disease-specific clinical outcomes. Our method effectively tackles the issue of zero inflation, a frequent challenge in medical data that complicates traditional modeling techniques. The theoretical foundations of the proposed approach are rigorously developed and validated through extensive simulations. Using Taiwan's health administrative claims data from 2000 to 2013, we construct 14 yearly networks that are temporally correlated, featuring 125 nodes that represent different disease conditions. Key network properties, such as connectivity, module, and temporal variation are analyzed. To demonstrate how these networks can inform multimorbidity management, we focus on breast cancer and analyze the relevant network structures. The findings provide valuable clinical insights that enhance the current understanding of multimorbidity. The proposed methods offer promising applications in shaping treatment strategies, optimizing health resource allocation, and informing health policy development in the context of multimorbidity management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06540v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Mei, Haonan Xiao, Ben-Chang Shia, Guanzhong Qiao, Yang Li</dc:creator>
    </item>
    <item>
      <title>Data quality or data quantity? Prioritizing data collection under distribution shift with the data usefulness coefficient</title>
      <link>https://arxiv.org/abs/2504.06570</link>
      <description>arXiv:2504.06570v1 Announce Type: new 
Abstract: Researchers often have access to multiple data sources of varying quality. For example, in psychology, a researcher may decide between running an experiment on an online platform or on a representative sample of the population of interest. Collecting a representative sample will result in higher quality data but is often more expensive. This raises the question of how to optimally prioritize data collection under resource constraints. We study this question in a setting where the distribution shift arises through many independent random changes in the population. We introduce a "data usefulness coefficient" (DUC) and show that it allows us to predict how much the risk of empirical risk minimization would decrease if a specific data set were added to the training data. An advantage of our procedure is that it does not require access to any outcome data $Y$. Instead, we rely on a random shift assumption, which implies that the strength of covariate ($X$) shift is predictive of the shift in $Y \mid X$. We also derive methods for sampling under budget and size constraints. We demonstrate the benefits of data collection based on DUC and our optimal sampling strategy in several numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06570v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivy Zhang, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>Testing Multivariate Conditional Independence Using Exchangeable Sampling and Sufficient Statistics</title>
      <link>https://arxiv.org/abs/2504.06685</link>
      <description>arXiv:2504.06685v1 Announce Type: new 
Abstract: We consider testing multivariate conditional independence between a response Y and a covariate vector X given additional variables Z. We introduce the Multivariate Sufficient Statistic Conditional Randomization Test (MS-CRT), which generates exchangeable copies of X by conditioning on sufficient statistics of P(X|Z). MS-CRT requires no modelling assumption on Y and accommodates any test statistics, including those derived from complex predictive models. It relaxes the assumptions of standard conditional randomization tests by allowing more unknown parameters in P(X|Z) than the sample size. MS-CRT avoids multiplicity corrections and effectively detects joint signals, even when individual components of X have only weak effects on Y . Our method extends to group selection with false discovery rate control. We develop efficient implementations for two important cases where P(X,Z) is either multivariate normal or belongs to a graphical model. For normal models, we establish the minimax rate optimality. For graphical models, we demonstrate the superior performance of our method compared to existing methods through comprehensive simulations and real-data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06685v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaotong Lin, Jie Xie, Fangqiao Tian, Dongming Huang</dc:creator>
    </item>
    <item>
      <title>Compatibility of Missing Data Handling Methods across the Stages of Producing Clinical Prediction Models</title>
      <link>https://arxiv.org/abs/2504.06799</link>
      <description>arXiv:2504.06799v1 Announce Type: new 
Abstract: Missing data is a challenge when developing, validating and deploying clinical prediction models (CPMs). Traditionally, decisions concerning missing data handling during CPM development and validation havent accounted for whether missingness is allowed at deployment. We hypothesised that the missing data approach used during model development should optimise model performance upon deployment, whilst the approach used during model validation should yield unbiased predictive performance estimates upon deployment; we term this compatibility. We aimed to determine which combinations of missing data handling methods across the CPM life cycle are compatible. We considered scenarios where CPMs are intended to be deployed with missing data allowed or not, and we evaluated the impact of that choice on earlier modelling decisions. Through a simulation study and an empirical analysis of thoracic surgery data, we compared CPMs developed and validated using combinations of complete case analysis, mean imputation, single regression imputation, multiple imputation, and pattern sub-modelling. If planning to deploy a CPM without allowing missing data, then development and validation should use multiple imputation when required. Where missingness is allowed at deployment, the same imputation method must be used during development and validation. Commonly used combinations of missing data handling methods result in biased predictive performance estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06799v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonia Tsvetanova, Matthew Sperrin, David A. Jenkins, Niels Peek, Iain Buchan, Stephanie Hyland, Marcus Taylor, Angela Wood, Richard D. Riley, Glen P. Martin</dc:creator>
    </item>
    <item>
      <title>Network Cross-Validation and Model Selection via Subsampling</title>
      <link>https://arxiv.org/abs/2504.06903</link>
      <description>arXiv:2504.06903v1 Announce Type: new 
Abstract: Complex and larger networks are becoming increasingly prevalent in scientific applications in various domains. Although a number of models and methods exist for such networks, cross-validation on networks remains challenging due to the unique structure of network data. In this paper, we propose a general cross-validation procedure called NETCROP (NETwork CRoss-Validation using Overlapping Partitions). The key idea is to divide the original network into multiple subnetworks with a shared overlap part, producing training sets consisting of the subnetworks and a test set with the node pairs between the subnetworks. This train-test split provides the basis for a network cross-validation procedure that can be applied on a wide range of model selection and parameter tuning problems for networks. The method is computationally efficient for large networks as it uses smaller subnetworks for the training step. We provide methodological details and theoretical guarantees for several model selection and parameter tuning tasks using NETCROP. Numerical results demonstrate that NETCROP performs accurate cross-validation on a diverse set of network model selection and parameter tuning problems. The results also indicate that NETCROP is computationally much faster while being often more accurate than the existing methods for network cross-validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06903v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sayan Chakrabarty, Srijan Sengupta, Yuguo Chen</dc:creator>
    </item>
    <item>
      <title>Assessing dominance in survival functions: A test for right-censored data</title>
      <link>https://arxiv.org/abs/2504.07012</link>
      <description>arXiv:2504.07012v1 Announce Type: new 
Abstract: This paper proposes a new statistical test to assess the dominance of survival functions in the presence of right-censored data. Traditional methods, such as the log-rank test, are inadequate for determining whether one survival function consistently dominates another, especially when survival curves cross. The proposed test is based on the supremum of the difference between Kaplan-Meier estimators and allows for distinguishing between dominance and crossing survival curves. The paper presents the test's asymptotic properties, along with simulations and applications to real datasets. The results demonstrate that the test has high sensitivity for detecting crossings and dominance compared to conventional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07012v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F\'elix Belzunce, Carolina Mart\'inez-Riquelme, Jaime Valenciano</dc:creator>
    </item>
    <item>
      <title>A Metropolis-Adjusted Langevin Algorithm for Sampling Jeffreys Prior</title>
      <link>https://arxiv.org/abs/2504.06372</link>
      <description>arXiv:2504.06372v1 Announce Type: cross 
Abstract: Inference and estimation are fundamental aspects of statistics, system identification and machine learning. For most inference problems, prior knowledge is available on the system to be modeled, and Bayesian analysis is a natural framework to impose such prior information in the form of a prior distribution. However, in many situations, coming out with a fully specified prior distribution is not easy, as prior knowledge might be too vague, so practitioners prefer to use a prior distribution that is as `ignorant' or `uninformative' as possible, in the sense of not imposing subjective beliefs, while still supporting reliable statistical analysis. Jeffreys prior is an appealing uninformative prior because it offers two important benefits: (i) it is invariant under any re-parameterization of the model, (ii) it encodes the intrinsic geometric structure of the parameter space through the Fisher information matrix, which in turn enhances the diversity of parameter samples. Despite these benefits, drawing samples from Jeffreys prior is a challenging task. In this paper, we propose a general sampling scheme using the Metropolis-Adjusted Langevin Algorithm that enables sampling of parameter values from Jeffreys prior, and provide numerical illustrations of our approach through several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06372v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yibo Shi, Braghadeesh Lakshminarayanan, Cristian R. Rojas</dc:creator>
    </item>
    <item>
      <title>Using theory-driven Integrated Population Models to evaluate competitive outcomes in stage-structured systems</title>
      <link>https://arxiv.org/abs/2504.06725</link>
      <description>arXiv:2504.06725v1 Announce Type: cross 
Abstract: Predicting competitive outcomes typically requires fitting dynamical models to data, from which interaction strengths and coexistence indicators such as invasion criteria can be produced. Methods that allow to propagate parameter uncertainty are particularly indicated. These should ideally allow for competition between and within species at various life-stages, and make the best out of multiple data sources, each of which can be relatively scarce by statistical standards. Here, we embed a mathematical model of stage-structured competition between two species, producing analytical invasion criteria, into a two-species Integrated Population Model. The community-level IPM allows to combine counts, capture-recapture, and fecundity data into a single statistical framework, and the Bayesian formulation of the IPM fully propagates parameter uncertainty into invasion criteria. Model fitting demonstrates that we can correctly predict coexistence through reciprocal invasion when present, but that interaction strengths are not always estimable, depending on the prior chosen. Our competitive exclusion scenario is shown to be harder to identify, although our model allows to at least flag this scenario as uncertain rather than mistakenly present it as coexistence. Our results confirm the importance of accounting for uncertainty in the prediction of competitive outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06725v1</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthieu Paquet, Fr\'ed\'eric Barraquand</dc:creator>
    </item>
    <item>
      <title>Communicating complex statistical models to a public health audience: translating science into action with the FARSI approach</title>
      <link>https://arxiv.org/abs/2504.06787</link>
      <description>arXiv:2504.06787v1 Announce Type: cross 
Abstract: Background. Effectively communicating complex statistical model outputs is a major challenge in public health. This study introduces the FARSI approach (Fast, Accessible, Reliable, Secure, Informative) as a framework to enhance the translation of intricate statistical findings into actionable insights for policymakers and stakeholders. We apply this framework in a real-world case study on chronic disease monitoring in Italy.
  Methods. The FARSI framework outlines key principles for developing user-friendly tools that improve the translation of statistical results. We applied these principles to create an open-access web application using R Shiny, designed to communicate chronic disease prevalence estimates from a Bayesian spatio-temporal logistic model. The case study highlights the importance of an intuitive design for fast accessibility, validated data and expert feedback for reliability, aggregated data for security, and insights into prevalence population subgroups, which were previously unobservable, for informativeness.
  Results. The web application enables stakeholders to explore disease prevalence across populations and geographical area through dynamic visualizations. It facilitates public health monitoring by, for instance, identifying disparities at the local level and assessing risk factors such as smoking. Its user-friendly interface enhances accessibility, making statistical findings more actionable. Conclusions. The FARSI framework provides a structured approach to improving the communication of complex research findings. By making statistical models more accessible and interpretable, it supports evidence-based decision-making in public health and increases the societal impact of research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06787v1</guid>
      <category>stat.OT</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mattia Stival, Lorenzo Schiavon, Gaia Bertarelli, Stefano Campostrini</dc:creator>
    </item>
    <item>
      <title>A geometric ensemble method for Bayesian inference</title>
      <link>https://arxiv.org/abs/2504.07084</link>
      <description>arXiv:2504.07084v1 Announce Type: cross 
Abstract: Conventional approximations to Bayesian inference rely on either approximations by statistics such as mean and covariance or by point particles. Recent advances such as the ensemble Gaussian mixture filter have generalized these notions to sums of parameterized distributions. This work presents a new methodology for approximating Bayesian inference by sums of uniform distributions on convex polytopes. The methodology presented herein is developed from the simplest convex polytope filter that takes advantage of uniform prior and measurement uncertainty, to an operationally viable ensemble filter with Kalmanized approximations to updating convex polytopes. Numerical results on the Ikeda map show the viability of this methodology in the low-dimensional setting, and numerical results on the Lorenz '96 equations similarly show viability in the high-dimensional setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07084v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrey A Popov</dc:creator>
    </item>
    <item>
      <title>Targeted Function Balancing</title>
      <link>https://arxiv.org/abs/2203.12179</link>
      <description>arXiv:2203.12179v3 Announce Type: replace 
Abstract: This paper introduces Targeted Function Balancing (TFB), a covariate balancing weights framework for estimating the average treatment effect of a binary intervention. TFB first regresses an outcome on covariates, and then selects weights that balance functions (of the covariates) that are probabilistically near the resulting regression function. This yields balance in the regression function's predicted values and the covariates, with the regression function's estimated variance determining how much balance in the covariates is sufficient. Notably, TFB demonstrates that intentionally leaving imbalance in some covariates can increase efficiency without introducing bias, challenging traditions that warn against imbalance in any variable. Additionally, TFB is entirely defined by a regression function and its estimated variance, turning the problem of how best to balance the covariates into how best to model the outcome. Kernel regularized least squares (KRLS), the LASSO, and Bayesian Additive Regression Trees (BART) are considered as regression estimators. With KRLS, TFB contributes to the literature of kernel-based weights. As for the LASSO, TFB uses the regression function's estimated variance to prioritize balance in certain dimensions of the covariates, a feature that can be greatly exploited by choosing a sparse regression estimator. With BART, we demonstrate that TFB can apply regression estimators that do not have linear representations. The R Package tfb implements TFB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.12179v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.19068.62086</arxiv:DOI>
      <dc:creator>Leonard Wainstein, He Bai</dc:creator>
    </item>
    <item>
      <title>Beyond Conditional Averages: Estimating The Individual Causal Effect Distribution</title>
      <link>https://arxiv.org/abs/2210.16563</link>
      <description>arXiv:2210.16563v2 Announce Type: replace 
Abstract: In recent years, the field of causal inference from observational data has emerged rapidly. The literature has focused on (conditional) average causal effect estimation. When (remaining) variability of individual causal effects (ICEs) is considerable, average effects may be uninformative for an individual. The fundamental problem of causal inference precludes estimating the joint distribution of potential outcomes without making assumptions. In this work, we show that the ICE distribution is identifiable under (conditional) independence of the individual effect and the potential outcome under no exposure, in addition to the common assumptions of consistency, positivity, and conditional exchangeability. Moreover, we present a family of flexible latent variable models that can be used to study individual effect modification and estimate the ICE distribution from cross-sectional data. How such latent variable models can be applied and validated in practice is illustrated in a case study on the effect of Hepatic Steatosis on a clinical precursor to heart failure. Under the assumptions presented, we estimate that 20.6% (95% Bayesian credible interval: 8.9%, 33.6%) of the population has a harmful effect greater than twice the average causal effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.16563v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Post, Edwin van den Heuvel</dc:creator>
    </item>
    <item>
      <title>Network inference via approximate Bayesian computation. Illustration on a stochastic multi-population neural mass model</title>
      <link>https://arxiv.org/abs/2306.15787</link>
      <description>arXiv:2306.15787v3 Announce Type: replace 
Abstract: In this article, we propose an adapted sequential Monte Carlo approximate Bayesian computation (SMC-ABC) algorithm for network inference in coupled stochastic differential equations (SDEs) used for multivariate time series modeling. Our approach is motivated by neuroscience, specifically the challenge of estimating brain connectivity before and during epileptic seizures. To this end, we make four key contributions. First, we introduce a 6N-dimensional SDE to model the activity of N coupled neuronal populations, extending the (single-population) stochastic Jansen and Rit neural mass model used to describe human electroencephalography (EEG) rhythms, particularly epileptic activity. Second, we construct a reliable and efficient numerical splitting scheme for the model simulation. Third, we apply the proposed adapted SMC-ABC algorithm to the neural mass model and validate it on different types of simulated data. Compared to standard SMC-ABC, our approach significantly reduces computational cost by requiring fewer model simulations to reach the desired posterior region, thanks to the inclusion of binary parameters describing the presence or absence of coupling directions. Finally, we apply our method to real multi-channel EEG data, uncovering potential similarities in patients' brain activities across different epileptic seizures, as well as differences between pre-seizure and seizure periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15787v3</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susanne Ditlevsen, Massimiliano Tamborrino, Irene Tubikanec</dc:creator>
    </item>
    <item>
      <title>Multilevel Metamodels: Enhancing Inference, Interpretability, and Generalizability in Monte Carlo Simulation Studies</title>
      <link>https://arxiv.org/abs/2401.07294</link>
      <description>arXiv:2401.07294v4 Announce Type: replace 
Abstract: Metamodels, or the regression analysis of Monte Carlo simulation results, provide a powerful tool to summarize simulation findings. However, an underutilized approach is the multilevel metamodel (MLMM) that accounts for the dependent data structure that arises from fitting multiple models to the same simulated data set. In this study, we articulate the theoretical rationale for the MLMM and illustrate how it can improve the interpretability of simulation results, better account for complex simulation designs, and provide new insights into the generalizability of simulation findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07294v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joshua Gilbert, Luke Miratrix</dc:creator>
    </item>
    <item>
      <title>Two-stage Estimators for Spatial Confounding with Point-Referenced Data</title>
      <link>https://arxiv.org/abs/2404.09358</link>
      <description>arXiv:2404.09358v3 Announce Type: replace 
Abstract: Public health data are often spatially dependent, but standard spatial regression methods can suffer from bias and invalid inference when the independent variable is associated with spatially-correlated residuals. This could occur if, for example, there is an unmeasured environmental contaminant associated with the independent and outcome variables in a spatial regression analysis. Geoadditive structural equation modeling (gSEM), in which an estimated spatial trend is removed from both the explanatory and response variables before estimating the parameters of interest, has previously been proposed as a solution, but there has been little investigation of gSEM's properties with point-referenced data. We link gSEM to results on double machine learning and semiparametric regression based on two-stage procedures. We propose using these semiparametric estimators for spatial regression using Gaussian processes with Mat\`ern covariance to estimate the spatial trends, and term this class of estimators Double Spatial Regression (DSR). We derive regularity conditions for root-$n$ asymptotic normality and consistency and closed-form variance estimation, and show that in simulations where standard spatial regression estimators are highly biased and have poor coverage, DSR can mitigate bias more effectively than competitors and obtain nominal coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09358v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nate Wiecha, Jane A. Hoppin, Brian J. Reich</dc:creator>
    </item>
    <item>
      <title>Bayesian Deep Generative Models for Multiplex Networks with Multiscale Overlapping Clusters</title>
      <link>https://arxiv.org/abs/2405.20936</link>
      <description>arXiv:2405.20936v5 Announce Type: replace 
Abstract: Our interest is in multiplex network data with multiple network samples observed across the same set of nodes. Examples originate from a variety of fields, including brain connectivity, international trade networks, and social networks, among others. Our goal is to infer a hierarchical structure of the nodes at a population level, while performing multi-resolution clustering of the individual replicates. To accomplish this, we propose a Bayesian hierarchical model, provide theoretical support in terms of identifiability and posterior consistency, and design efficient methods for posterior computation. We provide novel technical tools for proving model identifiability, which are of independent interest. Our proposed methodology is demonstrated through numerical simulation and an application to brain connectome data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20936v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuren Zhou, Yuqi Gu, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Surface data imputation with stochastic processes</title>
      <link>https://arxiv.org/abs/2410.22824</link>
      <description>arXiv:2410.22824v2 Announce Type: replace 
Abstract: Spurious measurements frequently occur in surface data from technical components. Excluding or ignoring these spurious points may lead to incorrect surface characterization if these points inherit features of the surface. Therefore, data imputation must be applied to ensure that the estimated data points at spurious measurements do not deviate strongly from the true surface and its characteristics. Traditional surface data imputation methods rely on simple assumptions and ignore existing knowledge of the surface, resulting in suboptimal estimates. In this paper, we propose the use of stochastic processes for data imputation. This approach, which originates from surface texture simulation, allows a straightforward integration of a priori knowledge. We employ Gaussian processes with both stationary and non-stationary covariance structures to address missing values in surface data. In addition, we apply the method to a real-world scenario in which a spurious turned profile is obtained from an actual measurement. Our results demonstrate that the proposed method fills the missing values by maintaining the surface characteristics, particularly when surface features are missing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22824v2</guid>
      <category>stat.ME</category>
      <category>cond-mat.mtrl-sci</category>
      <category>eess.SP</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1361-6501/adc6a3</arxiv:DOI>
      <arxiv:journal_reference>Meas. Sci. Technol. 36 (2025) 046131</arxiv:journal_reference>
      <dc:creator>Arsalan Jawaid, Samuel Schmidt, Marvin Lotz, J\"org Seewig</dc:creator>
    </item>
    <item>
      <title>Spline Quantile Regression</title>
      <link>https://arxiv.org/abs/2501.03883</link>
      <description>arXiv:2501.03883v2 Announce Type: replace 
Abstract: Quantile regression is a powerful tool capable of offering a richer view of the data as compared to least-squares regression. Quantile regression is typically performed individually on a few quantiles or a grid of quantiles without considering the similarity of the underlying regression coefficients at nearby quantiles. When needed, an ad hoc post-processing procedure such as kernel smoothing is employed to smooth the individually estimated coefficients across quantiles and thereby improve the performance of these estimates. This paper introduces a new method, called spline quantile regression (SQR), that unifies quantile regression with quantile smoothing and jointly estimates the regression coefficients across quantiles as smoothing splines. We discuss the computation of the SQR solution as a linear program (LP) using an interior-point algorithm. We also experiment with some gradient algorithms that require less memory than the LP algorithm. The performance of the SQR method and these algorithms is evaluated using simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03883v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ta-Hsin Li, Nimrod Megiddo</dc:creator>
    </item>
    <item>
      <title>Efficient Inference for Covariate-adjusted Bradley-Terry Model with Covariate Shift</title>
      <link>https://arxiv.org/abs/2503.18256</link>
      <description>arXiv:2503.18256v2 Announce Type: replace 
Abstract: We propose a general framework for statistical inference on the overall strengths of players in pairwise comparisons, allowing for potential shifts in the covariate distribution. These covariates capture important contextual information that may impact the winning probability of each player. We measure the overall strengths of players under a target distribution through its Kullback-Leibler projection onto a class of covariate-adjusted Bradley-Terry model. Consequently, our estimands remain well-defined without requiring stringent model assumptions. We develop semiparametric efficient estimators and corresponding inferential procedures that allow for flexible estimation of the nuisance functions. When the conditional Bradley-Terry assumption holds, we propose additional estimators that do not require observing all pairwise comparisons. We demonstrate the performance of our proposed method in simulation studies and apply it to assess the alignment of large language models with human preferences in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18256v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiudi Li, Sijia Li</dc:creator>
    </item>
    <item>
      <title>Analyzing Functional Data with a Mixture of Covariance Structures Using a Curve-Based Sampling Scheme</title>
      <link>https://arxiv.org/abs/2504.01313</link>
      <description>arXiv:2504.01313v2 Announce Type: replace 
Abstract: Motivated by distinct walking patterns in real-world free-living gait data, this paper proposes an innovative curve-based sampling scheme for the analysis of functional data characterized by a mixture of covariance structures. Traditional approaches often fail to adequately capture inherent complexities arising from heterogeneous covariance patterns across distinct subsets of the data. We introduce a unified Bayesian framework that integrates a nonlinear regression function with a continuous-time hidden Markov model, enabling the identification and utilization of varying covariance structures. One of the key contributions is the development of a computationally efficient curve-based sampling scheme for hidden state estimation, addressing the sampling complexities associated with high-dimensional, conditionally dependent data. This paper details the Bayesian inference procedure, examines the asymptotic properties to ensure the structural consistency of the model, and demonstrates its effectiveness through simulated and real-world examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01313v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yian Yu, Bo Wang, Jian Qing Shi</dc:creator>
    </item>
    <item>
      <title>Low-Rank Thinning</title>
      <link>https://arxiv.org/abs/2502.12063</link>
      <description>arXiv:2502.12063v4 Announce Type: replace-cross 
Abstract: The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12063v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annabelle Michael Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Differentially Private Joint Independence Test</title>
      <link>https://arxiv.org/abs/2503.18721</link>
      <description>arXiv:2503.18721v2 Announce Type: replace-cross 
Abstract: Identification of joint dependence among more than two random vectors plays an important role in many statistical applications, where the data may contain sensitive or confidential information. In this paper, we consider the the $d$-variable Hilbert-Schmidt independence criterion (dHSIC) in the context of differential privacy. Given the limiting distribution of the empirical estimate of dHSIC is complicated Gaussian chaos, constructing tests in the non-privacy regime is typically based on permutation and bootstrap. To detect joint dependence in privacy, we propose a dHSIC-based testing procedure by employing a differentially private permutation methodology. Our method enjoys privacy guarantee, valid level and pointwise consistency, while the bootstrap counterpart suffers inconsistent power. We further investigate the uniform power of the proposed test in dHSIC metric and $L_2$ metric, indicating that the proposed test attains the minimax optimal power across different privacy regimes. As a byproduct, our results also contain the pointwise and uniform power of the non-private permutation dHSIC, addressing an unsolved question remained in Pfister et al. (2018). Both numerical simulations and real data analysis on causal inference suggest our proposed test performs well empirically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18721v2</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingwei Liu, Yuexin Chen, Wangli Xu</dc:creator>
    </item>
  </channel>
</rss>
