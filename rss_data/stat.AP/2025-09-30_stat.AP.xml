<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 Oct 2025 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Chinese vs. World Bank Development Projects: Insights from Earth Observation and Computer Vision on Wealth Gains in Africa, 2002-2013</title>
      <link>https://arxiv.org/abs/2509.25648</link>
      <description>arXiv:2509.25648v1 Announce Type: new 
Abstract: Debates about whether development projects improve living conditions persist, partly because observational estimates can be biased by incomplete adjustment and because reliable outcome data are scarce at the neighborhood level. We address both issues in a continent-scale, sector-specific evaluation of Chinese and World Bank projects across 9,899 neighborhoods in 36 African countries (2002 to 2013), representative of 88% of the population. First, we use a recent dataset that measures living conditions with a machine-learned wealth index derived from contemporaneous satellite imagery, yielding a consistent panel of 6.7 km square mosaics. Second, to strengthen identification, we proxy officials' map-based placement criteria using pre-treatment daytime satellite images and fuse these with rich tabular covariates to estimate funder- and sector-specific ATEs via inverse-probability weighting. Incorporating imagery systematically shrinks effects relative to tabular-only models, indicating prior work likely overstated benefits. On average, both donors raise wealth, with larger gains for China; sector extremes in our sample include Trade and Tourism for the World Bank (+6.27 IWI points), and Emergency Response for China (+14.32). Assignment-mechanism analyses show World Bank placement is generally more predictable from imagery alone, as well as from tabular covariates. This suggests that Chinese project placements are more driven by non-visible, political, or event-driven factors than World Bank placements. To probe residual concerns about selection on observables, we also estimate within-neighborhood (unit) fixed-effects models at a spatial resolution about 450 times finer than prior fixed effects analyses, leveraging the computer-vision-imputed IWI panels; these deliver smaller but directionally consistent effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25648v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adel Daoud, Cindy Conlin, Connor T. Jerzak</dc:creator>
    </item>
    <item>
      <title>DNA shotgun sequencing evidence: sample-specific and unknown genotyping error probabilities</title>
      <link>https://arxiv.org/abs/2509.26112</link>
      <description>arXiv:2509.26112v1 Announce Type: new 
Abstract: DNA shotgun sequencing evidence is starting to gain a lot of attraction in forensic genetics. Methods to correctly interpret such evidence, including properly accounting for sequencing errors, are needed. This paper extends the wgsLR model by Andersen et. al. (2025) from only allowing for the same, known genotyping error probability for the two samples (trace sample from unknown donor and reference sample from person of interest), to allowing for different genotyping error probabilities (e.g., from trace hair sample and buccal swab reference sample). The model was also extended to be able to integrate out unknown genotyping error probabilities if only a prior probability is known. The sensitivity of the model against overdispersion was also investigated and it was found that it is very robust against overdispersion in estimating the genotyping error probability. It was also found that integrating out unknown genotyping error probability of the trace sample gave concordant weight of evidence under both the hypotheses (first hypothesis that the same individual was the donor of both trace and reference sample as well as the second hypothesis that two different individuals were the donors for the trace and reference sample). It was found that it is more consevative to use prior distributions with a too small mean rather than a too high mean. The extension of the model is implemented in the R package wgsLR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26112v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikkel Meyer Andersen</dc:creator>
    </item>
    <item>
      <title>Confidence Intervals for Conditional Covariances of Natural Frequencies</title>
      <link>https://arxiv.org/abs/2509.26348</link>
      <description>arXiv:2509.26348v1 Announce Type: new 
Abstract: In structural health monitoring (SHM), sensor measurements are collected, and damage-sensitive features such as natural frequencies are extracted for damage detection. However, these features depend not only on damage but are also influenced by various confounding factors, including environmental conditions and operational parameters. These factors must be identified, and their effects must be removed before further analysis. However, it has been shown that confounding variables may influence the mean and the covariance of the extracted features. This is particularly significant since the covariance is an essential building block in many damage detection tools. To account for the complex relationships resulting from the confounding factors, a nonparametric kernel approach can be used to estimate a conditional covariance matrix. By doing so, the covariance matrix is allowed to change depending on the identified confounding factor, thus providing a clearer understanding of how, for example, temperature influences the extracted features. This paper presents two bootstrap-based methods for obtaining confidence intervals for the conditional covariances, providing a way to quantify the uncertainty associated with the conditional covariance estimator. A proof-of-concept Monte Carlo study compares the two bootstrap versions proposed and evaluates their effectiveness. Finally, the methods are applied to the natural frequency data of the KW51 railway bridge near Leuven, Belgium. This real-world application highlights the practical implications of the findings. It underscores the importance of accurately accounting for confounding factors to generate more reliable diagnostic values with fewer false alarms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26348v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 11th International Operational Modal Analysis Conference (IOMAC 2025)</arxiv:journal_reference>
      <dc:creator>Lizzie Neumann, Philipp Wittenberg, Jan Gertheiss</dc:creator>
    </item>
    <item>
      <title>Evaluating treatment effects on longitudinal outcomes with attrition due to death: Methods for a two-dimentional estimand with a case study in Quality of Life</title>
      <link>https://arxiv.org/abs/2509.25548</link>
      <description>arXiv:2509.25548v1 Announce Type: cross 
Abstract: When longitudinal outcomes are evaluated in mortal populations, their non-existence after death complicates the analysis and its causal interpretation. Where popular methods often merge longitudinal outcome and survival into one scale or otherwise try to circumvent the problem of mortality, some highly relevant questions require survival to be acknowledged as a unique condition. "\textit{What are my chances of survival}" and "\textit{What can I expect for my condition while still alive}" reflect the intrinsically two-dimensional outcome of survival and longitudinal outcome while-alive. We define a two-dimensional causal while-alive estimand for a point exposure and compare two methods for estimation in an observational setting. Regression-Standardization models survival and the observed longitudinal outcome before standardizing the latter to a target population weighted by its estimated survival. Alternatively, Inverse Probability of Treatment and Censoring Weighting weights the observed outcomes twice, to account for censoring and differences in baseline-case-mix. Both approaches rely on the same causal identification assumptions, but require different models to be correctly specified. With its potential to extrapolate, Regression-Standardization is more efficient when all assumptions are met. We show finite sample performance in a simulation study and apply the methods to a case study on quality of life in oncology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25548v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dries Reynders, Doranne Thomassen, Satrajit Roychoudhury, Cecilie Delphin Amdal, Jammbe Z. Musoro, Willi Sauerbrei, Saskia le Cessie, Els Goetghebeur</dc:creator>
    </item>
    <item>
      <title>PPD-CPP: Pointwise predictive density calibrated-power prior in dynamically borrowing historical information</title>
      <link>https://arxiv.org/abs/2509.25688</link>
      <description>arXiv:2509.25688v1 Announce Type: cross 
Abstract: Incorporating historical or real-world data into analyses of treatment effects for rare diseases has become increasingly popular. A major challenge, however, lies in determining the appropriate degree of congruence between historical and current data. In this study, we devote ourselves to the capacity of historical data in replicating the current data, and propose a new congruence measure/estimand $p_{CM}$. $p_{CM}$ quantifies the heterogeneity between two datasets following the idea of the marginal posterior predictive $p$-value, and its asymptotic properties were derived. Building upon $p_{CM}$, we develop the pointwise predictive density calibrated-power prior (PPD-CPP) to dynamically leverage historical information. PPD-CPP achieves the borrowing consistency and allows modeling the power parameter either as a fixed scalar or case-specific quantity informed by covariates. Simulation studies were conducted to demonstrate the performance of these methods and the methodology was illustrated using the Mother's Gift study and \textit{Ceriodaphnia dubia} toxicity test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25688v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shixuan Wang, Jing Zhang, Emily L. Kang, Bin Zhang</dc:creator>
    </item>
    <item>
      <title>Modeling Spatial Heterogeneity in Exposure Buffers and Risk: A Hierarchical Bayesian Approach</title>
      <link>https://arxiv.org/abs/2509.25708</link>
      <description>arXiv:2509.25708v1 Announce Type: cross 
Abstract: Place-based epidemiology studies often rely on circular buffers to define "exposure" to spatially distributed risk factors, where the buffer radius represents a threshold beyond which exposure does not influence the outcome of interest. This approach is popular due to its simplicity and alignment with public health policies. However, buffer radii are often chosen relatively arbitrarily and assumed constant across the spatial domain. This may result in suboptimal statistical inference if these modeling choices are incorrect. To address this, we develop SVBR (Spatially-Varying Buffer Radii), a flexible hierarchical Bayesian spatial change points approach that treats buffer radii as unknown parameters and allows both radii and exposure effects to vary spatially. Through simulations, we find that SVBR improves estimation and inference for key model parameters compared to traditional methods. We also apply SVBR to study healthcare access in Madagascar, finding that proximity to healthcare facilities generally increases antenatal care usage, with clear spatial variation in this relationship. By relaxing rigid assumptions about buffer characteristics, our method offers a flexible, data-driven approach to accurately defining exposure and quantifying its impact. The newly developed methods are available in the R package EpiBuffer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25708v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saskia Comess, Daniel E Ho, Joshua L Warren</dc:creator>
    </item>
    <item>
      <title>Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research</title>
      <link>https://arxiv.org/abs/2509.26080</link>
      <description>arXiv:2509.26080v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are being increasingly used as synthetic agents in social science, in applications ranging from augmenting survey responses to powering multi-agent simulations. Because strong prediction plus conditioning prompts, token log-probs, and repeated sampling mimic Bayesian workflows, their outputs can be misinterpreted as posterior-like evidence from a coherent model. However, prediction does not equate to probabilism, and accurate points do not imply calibrated uncertainty. This paper outlines cautions that should be taken when interpreting LLM outputs and proposes a pragmatic reframing for the social sciences in which LLMs are used as high-capacity pattern matchers for quasi-predictive interpolation under explicit scope conditions and not as substitutes for probabilistic inference. Practical guardrails such as independent draws, preregistered human baselines, reliability-aware validation, and subgroup calibration, are introduced so that researchers may engage in useful prototyping and forecasting while avoiding category errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26080v1</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Rose Madden</dc:creator>
    </item>
    <item>
      <title>Parameter estimation of the four-parameter Harris extended Weibull distribution with applications to real-life data</title>
      <link>https://arxiv.org/abs/2509.26162</link>
      <description>arXiv:2509.26162v1 Announce Type: cross 
Abstract: This paper explores the extension of the classical two-parameter Weibull distribution to a four-parameter Harris extended Weibull (HEW) distribution. The flexibility of this probability distribution is illustrated by the varying shapes of HEW density function. Estimation of HEW parameters is explored using estimation methods such as the least-squares, maximum product of spacings, and minimum distance method. We provide Bayesian inference on the random parameters of the HEW distribution using Metropolis-Hastings algorithm to sample from the joint posterior distribution. Performance of the estimation methods is assessed using extensive simulations. The applicability of the distribution is demonstrated against three variants of the Weibull distribution on three real-life datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26162v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Prithul Chaturvedi, Himanshu Pokhriyal</dc:creator>
    </item>
    <item>
      <title>EnScale: Temporally-consistent multivariate generative downscaling via proper scoring rules</title>
      <link>https://arxiv.org/abs/2509.26258</link>
      <description>arXiv:2509.26258v1 Announce Type: cross 
Abstract: The practical use of future climate projections from global circulation models (GCMs) is often limited by their coarse spatial resolution, requiring downscaling to generate high-resolution data. Regional climate models (RCMs) provide this refinement, but are computationally expensive. To address this issue, machine learning models can learn the downscaling function, mapping coarse GCM outputs to high-resolution fields. Among these, generative approaches aim to capture the full conditional distribution of RCM data given coarse-scale GCM data, which is characterized by large variability and thus challenging to model accurately. We introduce EnScale, a generative machine learning framework that emulates the full GCM-to-RCM map by training on multiple pairs of GCM and corresponding RCM data. It first adjusts large-scale mismatches between GCM and coarsened RCM data, followed by a super-resolution step to generate high-resolution fields. Both steps employ generative models optimized with the energy score, a proper scoring rule. Compared to state-of-the-art ML downscaling approaches, our setup reduces computational cost by about one order of magnitude. EnScale jointly emulates multiple variables -- temperature, precipitation, solar radiation, and wind -- spatially consistent over an area in Central Europe. In addition, we propose a variant EnScale-t that enables temporally consistent downscaling. We establish a comprehensive evaluation framework across various categories including calibration, spatial structure, extremes, and multivariate dependencies. Comparison with diverse benchmarks demonstrates EnScale's strong performance and computational efficiency. EnScale offers a promising approach for accurate and temporally consistent RCM emulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26258v1</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Maybritt Schillinger, Maxim Samarin, Xinwei Shen, Reto Knutti, Nicolai Meinshausen</dc:creator>
    </item>
    <item>
      <title>"Rich-Get-Richer"? Analyzing Content Creator Earnings Across Large Social Media Platforms</title>
      <link>https://arxiv.org/abs/2509.26523</link>
      <description>arXiv:2509.26523v1 Announce Type: cross 
Abstract: This paper examines whether monthly content creator earnings follow a power law distribution, driven by compounding 'rich-get-richer' dynamics (Barabasi and Albert 1999). Patreon creator earnings data for 2018, 2021, and 2024 for Instagram, Twitch, YouTube, Twitter, Facebook, and Patreon exhibit a power law exponent around $\alpha = 2$. This suggests that algorithmic systems generate unequalizing returns closer to highly concentrated capital income and wealth, rather than labor income. Platforms governed by powerful and compounding recommendation systems, such as Instagram and YouTube, exhibit both a stronger power law relation (lower $\alpha$) and lower mean, median, and interquartile earnings, indicating algorithms that disproportionately favor top earners at the expense of a 'middle class' of creators. In contrast, Twitter and Patreon have a more moderate $\alpha$, with less earnings inequality and higher middle class earnings. Policies which incentivize the algorithmic promotion of longer-tail content (to explore more and exploit less) may help creator ecosystems become more equitable and sustainable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26523v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilan Strauss, Jangho Yang, Mariana Mazzucato</dc:creator>
    </item>
    <item>
      <title>Spatio-Temporal-Network Point Processes for Modeling Crime Events with Landmarks</title>
      <link>https://arxiv.org/abs/2409.10882</link>
      <description>arXiv:2409.10882v2 Announce Type: replace 
Abstract: Self-exciting point processes are widely used to model the contagious effects of crime events living within continuous geographic space, using their occurrence time and locations. However, in urban environments, most events are naturally constrained within the city's street network structure, and the contagious effects of crime are governed by such a network geography. Meanwhile, the complex distribution of urban infrastructures also plays an important role in shaping crime patterns across space. We introduce a novel spatio-temporal-network point process framework for crime modeling that integrates these urban environmental characteristics by incorporating self-attention graph neural networks. Our framework incorporates the street network structure as the underlying event space, where crime events can occur at random locations on the network edges. To realistically capture criminal movement patterns, distances between events are measured using street network distances. We then propose a new mark for a crime event by concatenating the event's crime category with the type of its nearby landmark, aiming to capture how the urban design influences the mixing structures of various crime types. A graph attention network architecture is adopted to learn the existence of mark-to-mark interactions. Extensive experiments on crime data from Valencia, Spain, demonstrate the effectiveness of our framework in understanding the crime landscape and forecasting crime risks across regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10882v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheng Dong, Jorge Mateu, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Modeling zero-inflated precipitation extremes</title>
      <link>https://arxiv.org/abs/2504.11058</link>
      <description>arXiv:2504.11058v2 Announce Type: replace 
Abstract: Accurate modeling of daily rainfall, encompassing both dry and wet days as well as extreme precipitation events, is critical for robust hydrological and climatological analyses. This study proposes a zero-inflated extended generalized Pareto distribution model that unifies the modeling of dry days, low, moderate, and extreme rainfall within a single framework. Unlike traditional approaches that rely on prespecified threshold selection to identify extremes, our proposed model captures tail behavior intrinsically through a tail index that aligns with the generalized Pareto distribution. The model also accommodates covariate effects via generalized additive modeling, allowing for the representation of complex climatic variability. The current implementation is limited to a univariate setting, modeling daily rainfall independently of covariates. Model estimation is carried out using both maximum likelihood and Bayesian approaches. Simulation studies and empirical applications demonstrate the models flexibility in capturing zero inflation and heavy-tailed behavior characteristics of daily rainfall distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11058v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aamar Abbas, Touqeer Ahmad, Ishfaq Ahmad</dc:creator>
    </item>
    <item>
      <title>Intraday FX Volatility-Curve Forecasting with Functional GARCH Approaches</title>
      <link>https://arxiv.org/abs/2311.18477</link>
      <description>arXiv:2311.18477v3 Announce Type: replace-cross 
Abstract: This paper seeks to forecast intraday volatility curves for major foreign exchange (FX) currencies using functional GARCH models. Intraday return curves are observed at a daily frequency, yet preserve the full high-frequency trading structure, enabling volatility analysis at the intraday level. We demonstrate that the USD/EUR, USD/GBP, and USD/JPY intraday return curves exhibit strong cross-dependence, while individually they are serially uncorrelated but display long-range conditional heteroskedasticity. Embedding cross-currency dependence via multi-level functional principal component analysis and adding intraday bid-ask spread curves as exogenous drivers significantly improves intraday and day-ahead volatility forecasts relative to functional and realised-volatility baselines. The precise volatility forecasts motivate the construction of intraday Value-at-Risk (VaR). An intraday risk management application highlights that predicted intraday VaR curves can help mitigate dramatic losses in intraday trading strategies, showcasing their practical economic benefits in FX markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18477v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fearghal Kearney, Han Lin Shang, Yuqian Zhao</dc:creator>
    </item>
    <item>
      <title>Conformal inference for cell type annotation with graph-structured constraints</title>
      <link>https://arxiv.org/abs/2410.23786</link>
      <description>arXiv:2410.23786v2 Announce Type: replace-cross 
Abstract: Conformal inference is a method that provides prediction sets for machine learning models, operating independently of the underlying distributional assumptions and relying solely on the exchangeability of training and test data. Despite its wide applicability and popularity, its application in graph-structured problems remains underexplored. This paper addresses this gap by developing an approach that leverages the rich information encoded in the graph structure of predicted classes to enhance the interpretability of conformal sets. Using a motivating example from genomics, specifically imaging-based spatial transcriptomics data and single-cell RNA sequencing data, we demonstrate how incorporating graph-structured constraints can improve the interpretation of cell type predictions. This approach aims to generate more coherent conformal sets that align with the inherent relationships among classes, facilitating clearer and more intuitive interpretations of model predictions. Additionally, we provide a technique to address non-exchangeability, particularly when the distribution of the response variable changes between training and test datasets. We implemented our method in the open-source R package scConform, available at https://github.com/ccb-hms/scConform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23786v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniela Corbetta, Livio Finos, Ludwig Geistlinger, Davide Risso</dc:creator>
    </item>
    <item>
      <title>Fast Likelihood-Free Parameter Estimation for L\'evy Processes</title>
      <link>https://arxiv.org/abs/2505.01639</link>
      <description>arXiv:2505.01639v2 Announce Type: replace-cross 
Abstract: L\'evy processes are widely used in financial modeling due to their ability to capture discontinuities and heavy tails, which are common in high-frequency asset return data. However, parameter estimation remains a challenge when associated likelihoods are unavailable or costly to compute. We propose a fast and accurate method for L\'evy parameter estimation using the neural Bayes estimation (NBE) framework -- a simulation-based, likelihood-free approach that leverages permutation-invariant neural networks to approximate Bayes estimators. We contribute new theoretical results, showing that NBE results in consistent estimators whose risk converges to the Bayes estimator under mild conditions. Moreover, through extensive simulations across several L\'evy models, we show that NBE outperforms traditional methods in both accuracy and runtime, while also enabling two complementary approaches to uncertainty quantification. We illustrate our approach on a challenging high-frequency cryptocurrency return dataset, where the method captures evolving parameter dynamics and delivers reliable and interpretable inference at a fraction of the computational cost of traditional methods. NBE provides a scalable and practical solution for inference in complex financial models, enabling parameter estimation and uncertainty quantification over an entire year of data in just seconds. We additionally investigate nearly a decade of high-frequency Bitcoin returns, requiring less than one minute to estimate parameters under the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01639v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Coloma, William Kleiber</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework for Co-Mediators of Zero-Inflated Single-Cell RNA-Seq Data</title>
      <link>https://arxiv.org/abs/2507.06113</link>
      <description>arXiv:2507.06113v3 Announce Type: replace-cross 
Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized the study of cellular heterogeneity, enabling detailed molecular profiling at the individual cell level. However, integrating high-dimensional single-cell data into causal mediation analysis remains challenging due to zero inflation and complex mediator structures. We propose a novel mediation framework leveraging zero-inflated negative binomial models to characterize cell-level mediator distributions and beta regression for zero-inflation proportions. The model can identify expression level as well as expressed proportion that could mediate disease-leading causal pathway. Extensive simulation studies demonstrate improved power and controlled false discovery rates. We further illustrate the utility of this approach through application to ROSMAP single-cell transcriptomic data, uncovering biologically meaningful mediation effects that enhance understanding of disease mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06113v3</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seungjun Ahn, Li Chen, Maaike van Gerwen, Panos Roussos, Zhigang Li</dc:creator>
    </item>
    <item>
      <title>Mapping beyond diseases: Controlled variable selection for secondary phenotypes using tilted knockoffs</title>
      <link>https://arxiv.org/abs/2508.18548</link>
      <description>arXiv:2508.18548v2 Announce Type: replace-cross 
Abstract: Researchers in biomedical studies often work with samples that are not selected uniformly at random from the population of interest, a major example being a case-control study. While these designs are motivated by specific scientific questions, it is often of interest to use the data collected to pursue secondary lines of investigations. In these cases, ignoring the fact that observations are not sampled uniformly at random can lead to spurious results. For example, in a case-control study, one might identify a spurious association between an exposure and a secondary phenotype when both affect the case-control status. This phenomenon is known as collider bias in the causal inference literature. While tests of independence under biased sampling are available, these methods typically do not apply when the number of variables is large.
  Here, we are interested in using the biased sample to select important exposures among a multitude of possible variables with replicability guarantees. While the model-X knockoff framework has been developed to test conditional independence hypotheses with False Discovery Rate (FDR) control, we show that its naive application fails to control FDR in the presence of biased sampling. We show how tilting the population distribution with the selection probability and constructing knockoff variables according to this tilted distribution instead leads to selection with FDR control. We study the FDR and power of the tilted knockoff method using simulated examples, and apply it to identify genetic underpinning of endophenotypes in a case-control study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18548v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qian Zhao, Susan Service, Carrie E. Bearden, Carlos Lopez-Jaramillo, Nelson Freimer, Chiara Sabatti</dc:creator>
    </item>
  </channel>
</rss>
