<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2024 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Average sensitivity of the Knapsack Problem</title>
      <link>https://arxiv.org/abs/2405.13343</link>
      <description>arXiv:2405.13343v1 Announce Type: new 
Abstract: In resource allocation, we often require that the output allocation of an algorithm is stable against input perturbation because frequent reallocation is costly and untrustworthy. Varma and Yoshida (SODA'21) formalized this requirement for algorithms as the notion of average sensitivity. Here, the average sensitivity of an algorithm on an input instance is, roughly speaking, the average size of the symmetric difference of the output for the instance and that for the instance with one item deleted, where the average is taken over the deleted item.
  In this work, we consider the average sensitivity of the knapsack problem, a representative example of a resource allocation problem. We first show a $(1-\epsilon)$-approximation algorithm for the knapsack problem with average sensitivity $O(\epsilon^{-1}\log \epsilon^{-1})$. Then, we complement this result by showing that any $(1-\epsilon)$-approximation algorithm has average sensitivity $\Omega(\epsilon^{-1})$. As an application of our algorithm, we consider the incremental knapsack problem in the random-order setting, where the goal is to maintain a good solution while items arrive one by one in a random order. Specifically, we show that for any $\epsilon &gt; 0$, there exists a $(1-\epsilon)$-approximation algorithm with amortized recourse $O(\epsilon^{-1}\log \epsilon^{-1})$ and amortized update time $O(\log n+f_\epsilon)$, where $n$ is the total number of items and $f_\epsilon&gt;0$ is a value depending on $\epsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13343v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soh Kumabe, Yuichi Yoshida</dc:creator>
    </item>
    <item>
      <title>Faster Vizing and Near-Vizing Edge Coloring Algorithms</title>
      <link>https://arxiv.org/abs/2405.13371</link>
      <description>arXiv:2405.13371v1 Announce Type: new 
Abstract: Vizing's celebrated theorem states that every simple graph with maximum degree $\Delta$ admits a $(\Delta+1)$ edge coloring which can be found in $O(m \cdot n)$ time on $n$-vertex $m$-edge graphs. This is just one color more than the trivial lower bound of $\Delta$ colors needed in any proper edge coloring. After a series of simplifications and variations, this running time was eventually improved by Gabow, Nishizeki, Kariv, Leven, and Terada in 1985 to $O(m\sqrt{n\log{n}})$ time. This has effectively remained the state-of-the-art modulo an $O(\sqrt{\log{n}})$-factor improvement by Sinnamon in 2019.
  As our main result, we present a novel randomized algorithm that computes a $\Delta+O(\log{n})$ coloring of any given simple graph in $O(m\log{\Delta})$ expected time; in other words, a near-linear time randomized algorithm for a ``near''-Vizing's coloring.
  As a corollary of this algorithm, we also obtain the following results:
  * A randomized algorithm for $(\Delta+1)$ edge coloring in $O(n^2\log{n})$ expected time. This is near-linear in the input size for dense graphs and presents the first polynomial time improvement over the longstanding bounds of Gabow et.al. for Vizing's theorem in almost four decades.
  * A randomized algorithm for $(1+\varepsilon) \Delta$ edge coloring in $O(m\log{(1/\varepsilon)})$ expected time for any $\varepsilon = \omega(\log{n}/\Delta)$. The dependence on $\varepsilon$ exponentially improves upon a series of recent results that obtain algorithms with runtime of $\Omega(m/\varepsilon)$ for this problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13371v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sepehr Assadi</dc:creator>
    </item>
    <item>
      <title>Cascading-Tree Algorithm for the 0-1 Knapsack Problem (In Memory of Heiner M{\"u}ller-Merbach, a Former President of IFORS)</title>
      <link>https://arxiv.org/abs/2405.13450</link>
      <description>arXiv:2405.13450v1 Announce Type: new 
Abstract: In operations research, the Knapsack Problem (KP) is one of the classical optimization problems that has been widely studied. The KP has several variants and, in this paper, we address the binary KP, where for a given knapsack (with limited capacity) as well as a number of items, each of them has its own weight (volume or cost) and value, the objective consists in finding a selection of items such that the total value of the selected items is maximized and the capacity limit of the knapsack is respected. In this paper, in memorial of Prof. Dr. Heiner M{\"u}ller-Merbach, a former president of IFORS, we address the binary KP and revisit a classical algorithm, named cascading-tree branch-and-bound algorithm, that was originally introduced by him in 1978. However, the algorithm is surprisingly absent from the scientific literature because the paper was published in a German journal. We carried out computational experiments in order to compare the algorithm versus some classic methods. The numerical results show the effectiveness of the interesting idea used in the cascading-tree algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13450v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahdi Moeini (ENSIIE), Daniel Schermer (TU Kaiserslautern), Oliver Wendt (TU Kaiserslautern)</dc:creator>
    </item>
    <item>
      <title>Enumerating Graphlets with Amortized Time Complexity Independent of Graph Size</title>
      <link>https://arxiv.org/abs/2405.13613</link>
      <description>arXiv:2405.13613v1 Announce Type: new 
Abstract: Graphlets of order $k$ in a graph $G$ are connected subgraphs induced by $k$ nodes (called $k$-graphlets) or by $k$ edges (called edge $k$-graphlets). They are among the interesting subgraphs in network analysis to get insights on both the local and global structure of a network. While several algorithms exist for discovering and enumerating graphlets, the cost per solution of such algorithms typically depends on the size of the graph $G$, or its maximum degree. In real networks, even the latter can be in the order of millions, whereas $k$ is typically required to be a small value. In this paper we provide the first algorithm to list all graphlets of order $k$ in a graph $G=(V,E)$ with an amortized cost per solution depending \emph{solely} on the order $k$, contrarily to previous approaches where the cost depends \emph{also} on the size of $G$ or its maximum degree. Specifically, we show that it is possible to list $k$-graphlets in $O(k^2)$ time per solution, and to list edge $k$-graphlets in $O(k)$ time per solution. Furthermore we show that, if the input graph has bounded degree, then the cost per solution for listing $k$-graphlets is reduced to $O(k)$. Whenever $k = O(1)$, as it is often the case in practical settings, these algorithms are the first to achieve constant time per solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13613v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alessio Conte, Roberto Grossi, Yasuaki Kobayashi, Kazuhiro Kurita, Davide Rucci, Takeaki Uno, Kunihiro Wasa</dc:creator>
    </item>
    <item>
      <title>Path-Reporting Distance Oracles with Linear Size</title>
      <link>https://arxiv.org/abs/2405.14254</link>
      <description>arXiv:2405.14254v1 Announce Type: new 
Abstract: Given an undirected weighted graph, an (approximate) distance oracle is a data structure that can (approximately) answer distance queries. A {\em Path-Reporting Distance Oracle}, or {\em PRDO}, is a distance oracle that must also return a path between the queried vertices. Given a graph on $n$ vertices and an integer parameter $k\ge 1$, Thorup and Zwick \cite{TZ01} showed a PRDO with stretch $2k-1$, size $O(k\cdot n^{1+1/k})$ and query time $O(k)$ (for the query time of PRDOs, we omit the time needed to report the path itself). Subsequent works \cite{MN06,C14,C15} improved the size to $O(n^{1+1/k})$ and the query time to $O(1)$. However, these improvements produce distance oracles which are not path-reporting. Several other works \cite{ENW16,EP15} focused on small size PRDO for general graphs, but all known results on distance oracles with linear size suffer from polynomial stretch, polynomial query time, or not being path-reporting.
  In this paper we devise the first linear size PRDO with poly-logarithmic stretch and low query time $O(\log\log n)$. More generally, for any integer $k\ge 1$, we obtain a PRDO with stretch at most $O(k^{4.82})$, size $O(n^{1+1/k})$, and query time $O(\log k)$. In addition, we can make the size of our PRDO as small as $n+o(n)$, at the cost of increasing the query time to poly-logarithmic. For unweighted graphs, we improve the stretch to $O(k^2)$.
  We also consider {\em pairwise PRDO}, which is a PRDO that is only required to answer queries from a given set of pairs ${\cal P}$. An exact PRDO of size $O(n+|{\cal P}|^2)$ and constant query time was provided in \cite{EP15}. In this work we dramatically improve the size, at the cost of slightly increasing the stretch. Specifically, given any $\epsilon&gt;0$, we devise a pairwise PRDO with stretch $1+\epsilon$, constant query time, and near optimal size $n^{o(1)}\cdot (n+|{\cal P}|)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14254v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ofer Neiman, Idan Shabat</dc:creator>
    </item>
    <item>
      <title>Polynomial Pass Semi-Streaming Lower Bounds for K-Cores and Degeneracy</title>
      <link>https://arxiv.org/abs/2405.14835</link>
      <description>arXiv:2405.14835v1 Announce Type: new 
Abstract: The following question arises naturally in the study of graph streaming algorithms:
  "Is there any graph problem which is "not too hard", in that it can be solved efficiently with total communication (nearly) linear in the number $n$ of vertices, and for which, nonetheless, any streaming algorithm with $\tilde{O}(n)$ space (i.e., a semi-streaming algorithm) needs a polynomial $n^{\Omega(1)}$ number of passes?"
  Assadi, Chen, and Khanna [STOC 2019] were the first to prove that this is indeed the case. However, the lower bounds that they obtained are for rather non-standard graph problems.
  Our first main contribution is to present the first polynomial-pass lower bounds for natural "not too hard" graph problems studied previously in the streaming model: $k$-cores and degeneracy. We devise a novel communication protocol for both problems with near-linear communication, thus showing that $k$-cores and degeneracy are natural examples of "not too hard" problems. Indeed, previous work have developed single-pass semi-streaming algorithms for approximating these problems. In contrast, we prove that any semi-streaming algorithm for exactly solving these problems requires (almost) $\Omega(n^{1/3})$ passes.
  Our second main contribution is improved round-communication lower bounds for the underlying communication problems at the basis of these reductions:
  * We improve the previous lower bound of Assadi, Chen, and Khanna for hidden pointer chasing (HPC) to achieve optimal bounds.
  * We observe that all current reductions from HPC can also work with a generalized version of this problem that we call MultiHPC, and prove an even stronger and optimal lower bound for this generalization.
  These two results collectively allow us to improve the resulting pass lower bounds for semi-streaming algorithms by a polynomial factor, namely, from $n^{1/5}$ to $n^{1/3}$ passes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14835v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepehr Assadi, Prantar Ghosh, Bruno Loff, Parth Mittal, Sagnik Mukhopadhyay</dc:creator>
    </item>
    <item>
      <title>Dequantizability from inputs</title>
      <link>https://arxiv.org/abs/2405.13273</link>
      <description>arXiv:2405.13273v1 Announce Type: cross 
Abstract: By comparing constructions of block encoding given by [1-4], we propose a way to extract dequantizability from advancements in dequantization techniques that have been led by Tang, as in [5]. Then we apply this notion to the sparse-access input model that is known to be BQP-complete in general, thereby conceived to be un-dequantizable. Our goal is to break down this belief by examining the sparse-access input model's instances, particularly their input matrices. In conclusion, this paper forms a dequantizability-verifying scheme that can be applied whenever an input is given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13273v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tae-Won Kim, Byung-Soo Choi</dc:creator>
    </item>
    <item>
      <title>Quantum (Inspired) $D^2$-sampling with Applications</title>
      <link>https://arxiv.org/abs/2405.13351</link>
      <description>arXiv:2405.13351v1 Announce Type: cross 
Abstract: $D^2$-sampling is a fundamental component of sampling-based clustering algorithms such as $k$-means++. Given a dataset $V \subset \mathbb{R}^d$ with $N$ points and a center set $C \subset \mathbb{R}^d$, $D^2$-sampling refers to picking a point from $V$ where the sampling probability of a point is proportional to its squared distance from the nearest center in $C$. Starting with empty $C$ and iteratively $D^2$-sampling and updating $C$ in $k$ rounds is precisely $k$-means++ seeding that runs in $O(Nkd)$ time and gives $O(\log{k})$-approximation in expectation for the $k$-means problem. We give a quantum algorithm for (approximate) $D^2$-sampling in the QRAM model that results in a quantum implementation of $k$-means++ that runs in time $\tilde{O}(\zeta^2 k^2)$. Here $\zeta$ is the aspect ratio (i.e., largest to smallest interpoint distance), and $\tilde{O}$ hides polylogarithmic factors in $N, d, k$. It can be shown through a robust approximation analysis of $k$-means++ that the quantum version preserves its $O(\log{k})$ approximation guarantee. Further, we show that our quantum algorithm for $D^2$-sampling can be 'dequantized' using the sample-query access model of Tang (PhD Thesis, Ewin Tang, University of Washington, 2023). This results in a fast quantum-inspired classical implementation of $k$-means++, which we call QI-$k$-means++, with a running time $O(Nd) + \tilde{O}(\zeta^2k^2d)$, where the $O(Nd)$ term is for setting up the sample-query access data structure. Experimental investigations show promising results for QI-$k$-means++ on large datasets with bounded aspect ratio. Finally, we use our quantum $D^2$-sampling with the known $ D^2$-sampling-based classical approximation scheme (i.e., $(1+\varepsilon)$-approximation for any given $\varepsilon&gt;0$) to obtain the first quantum approximation scheme for the $k$-means problem with polylogarithmic running time dependence on $N$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13351v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ragesh Jaiswal, Poojan Shah</dc:creator>
    </item>
    <item>
      <title>Recovering short generators via negative moments of Dirichlet $L$-functions</title>
      <link>https://arxiv.org/abs/2405.13420</link>
      <description>arXiv:2405.13420v1 Announce Type: cross 
Abstract: In 2016, Cramer, Ducas, Peikert and, Regev proposed an efficient algorithm for recovering short generators of principal ideals in $q$-th cyclotomic fields with $q$ being a prime power. In this paper, we improve their analysis of the dual basis of the log-cyclotomic-unit lattice under the Generalised Riemann Hypothesis and in the case that $q$ is a prime number by the negative square moment of Dirichlet $L$-functions at $s=1$. As an implication, we obtain a better lower bound on the success probability for the algorithm in this special case. In order to prove our main result, we also give an analysis of the behaviour of negative $2k$-th moments of Dirichlet $L$-functions at $s=1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13420v1</guid>
      <category>math.NT</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iu-Iong Ng, Yuichiro Toma</dc:creator>
    </item>
    <item>
      <title>Sparse Induced Subgraphs of Large Treewidth</title>
      <link>https://arxiv.org/abs/2405.13797</link>
      <description>arXiv:2405.13797v1 Announce Type: cross 
Abstract: Motivated by an induced counterpart of treewidth sparsifiers (i.e., sparse subgraphs keeping the treewidth large) provided by the celebrated Grid Minor theorem of Robertson and Seymour [JCTB '86] or by a classic result of Chekuri and Chuzhoy [SODA '15], we show that for any natural numbers $t$ and $w$, and real $\varepsilon &gt; 0$, there is an integer $W := W(t,w,\varepsilon)$ such that every graph with treewidth at least $W$ and no $K_{t,t}$ subgraph admits a 2-connected $n$-vertex induced subgraph with treewidth at least $w$ and at most $(1+\varepsilon)n$ edges. The induced subgraph is either a subdivided wall, or its line graph, or a spanning supergraph of a subdivided biclique. This in particular extends a result of Weissauer [JCTB '19] that graphs of large treewidth have a large biclique subgraph or a long induced cycle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13797v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Edouard Bonnet</dc:creator>
    </item>
    <item>
      <title>On the Inapproximability of Finding Minimum Monitoring Edge-Geodetic Sets</title>
      <link>https://arxiv.org/abs/2405.13875</link>
      <description>arXiv:2405.13875v1 Announce Type: cross 
Abstract: Given an undirected connected graph $G = (V(G), E(G))$ on $n$ vertices, the minimum Monitoring Edge-Geodetic Set (MEG-set) problem asks to find a subset $M \subseteq V(G)$ of minimum cardinality such that, for every edge $e \in E(G)$, there exist $x,y \in M$ for which all shortest paths between $x$ and $y$ in $G$ traverse $e$.
  We show that, for any constant $c &lt; \frac{1}{2}$, no polynomial-time $(c \log n)$-approximation algorithm for the minimum MEG-set problem exists, unless $\mathsf{P} = \mathsf{NP}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13875v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Bil\`o, Giordano Colli, Luca Forlizzi, Stefano Leucci</dc:creator>
    </item>
    <item>
      <title>On connections between k-coloring and Euclidean k-means</title>
      <link>https://arxiv.org/abs/2405.13877</link>
      <description>arXiv:2405.13877v1 Announce Type: cross 
Abstract: In the Euclidean $k$-means problems we are given as input a set of $n$ points in $\mathbb{R}^d$ and the goal is to find a set of $k$ points $C\subseteq \mathbb{R}^d$, so as to minimize the sum of the squared Euclidean distances from each point in $P$ to its closest center in $C$. In this paper, we formally explore connections between the $k$-coloring problem on graphs and the Euclidean $k$-means problem. Our results are as follows:
  $\bullet$ For all $k\ge 3$, we provide a simple reduction from the $k$-coloring problem on regular graphs to the Euclidean $k$-means problem. Moreover, our technique extends to enable a reduction from a structured max-cut problem (which may be considered as a partial 2-coloring problem) to the Euclidean $2$-means problem. Thus, we have a simple and alternate proof of the NP-hardness of Euclidean 2-means problem.
  $\bullet$ In the other direction, we mimic the $O(1.7297^n)$ time algorithm of Williams [TCS'05] for the max-cut of problem on $n$ vertices to obtain an algorithm for the Euclidean 2-means problem with the same runtime, improving on the naive exhaustive search running in $2^n\cdot \text{poly}(n,d)$ time.
  $\bullet$ We prove similar results and connections as above for the Euclidean $k$-min-sum problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13877v1</guid>
      <category>cs.CG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enver Aman, Karthik C. S., Sharath Punna</dc:creator>
    </item>
    <item>
      <title>Practical $0.385$-Approximation for Submodular Maximization Subject to a Cardinality Constraint</title>
      <link>https://arxiv.org/abs/2405.13994</link>
      <description>arXiv:2405.13994v1 Announce Type: cross 
Abstract: Non-monotone constrained submodular maximization plays a crucial role in various machine learning applications. However, existing algorithms often struggle with a trade-off between approximation guarantees and practical efficiency. The current state-of-the-art is a recent $0.401$-approximation algorithm, but its computational complexity makes it highly impractical. The best practical algorithms for the problem only guarantee $1/e$-approximation. In this work, we present a novel algorithm for submodular maximization subject to a cardinality constraint that combines a guarantee of $0.385$-approximation with a low and practical query complexity of $O(n+k^2)$. Furthermore, we evaluate the empirical performance of our algorithm in experiments based on various machine learning applications, including Movie Recommendation, Image Summarization, and more. These experiments demonstrate the efficacy of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13994v1</guid>
      <category>cs.LG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Murad Tukan, Loay Mualem, Moran Feldman</dc:creator>
    </item>
    <item>
      <title>Online Classification with Predictions</title>
      <link>https://arxiv.org/abs/2405.14066</link>
      <description>arXiv:2405.14066v1 Announce Type: cross 
Abstract: We study online classification when the learner has access to predictions about future examples. We design an online learner whose expected regret is never worse than the worst-case regret, gracefully improves with the quality of the predictions, and can be significantly better than the worst-case regret when the predictions of future examples are accurate. As a corollary, we show that if the learner is always guaranteed to observe data where future examples are easily predictable, then online learning can be as easy as transductive online learning. Our results complement recent work in online algorithms with predictions and smoothed online classification, which go beyond a worse-case analysis by using machine-learned predictions and distributional assumptions respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14066v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vinod Raman, Ambuj Tewari</dc:creator>
    </item>
    <item>
      <title>Deterministic Policies for Constrained Reinforcement Learning in Polynomial-Time</title>
      <link>https://arxiv.org/abs/2405.14183</link>
      <description>arXiv:2405.14183v1 Announce Type: cross 
Abstract: We present a novel algorithm that efficiently computes near-optimal deterministic policies for constrained reinforcement learning (CRL) problems. Our approach combines three key ideas: (1) value-demand augmentation, (2) action-space approximate dynamic programming, and (3) time-space rounding. Under mild reward assumptions, our algorithm constitutes a fully polynomial-time approximation scheme (FPTAS) for a diverse class of cost criteria. This class requires that the cost of a policy can be computed recursively over both time and (state) space, which includes classical expectation, almost sure, and anytime constraints. Our work not only provides provably efficient algorithms to address real-world challenges in decision-making but also offers a unifying theory for the efficient computation of constrained deterministic policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14183v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy McMahan</dc:creator>
    </item>
    <item>
      <title>Graphlets correct for the topological information missed by random walks</title>
      <link>https://arxiv.org/abs/2405.14194</link>
      <description>arXiv:2405.14194v1 Announce Type: cross 
Abstract: Random walks are widely used for mining networks due to the computational efficiency of computing them. For instance, graph representation learning learns a d-dimensional embedding space, so that the nodes that tend to co-occur on random walks (a proxy of being in the same network neighborhood) are close in the embedding space. Specific local network topology (i.e., structure) influences the co-occurrence of nodes on random walks, so random walks of limited length capture only partial topological information, hence diminishing the performance of downstream methods. We explicitly capture all topological neighborhood information and improve performance by introducing orbit adjacencies that quantify the adjacencies of two nodes as co-occurring on a given pair of graphlet orbits, which are symmetric positions on graphlets (small, connected, non-isomorphic, induced subgraphs of a large network). Importantly, we mathematically prove that random walks on up to k nodes capture only a subset of all the possible orbit adjacencies for up to k-node graphlets. Furthermore, we enable orbit adjacency-based analysis of networks by developing an efficient GRaphlet-orbit ADjacency COunter (GRADCO), which exhaustively computes all 28 orbit adjacency matrices for up to four-node graphlets. Note that four-node graphlets suffice, because real networks are usually small-world. In large networks on around 20,000 nodes, GRADCOcomputesthe28matricesinminutes. Onsixrealnetworksfromvarious domains, we compare the performance of node-label predictors obtained by using the network embeddings based on our orbit adjacencies to those based on random walks. We find that orbit adjacencies, which include those unseen by random walks, outperform random walk-based adjacencies, demonstrating the importance of the inclusion of the topological neighborhood information that is unseen by random walks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14194v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sam F. L. Windels, Noel Malod-Dognin, Natasa Przulj</dc:creator>
    </item>
    <item>
      <title>A Quantum Speed-Up for Approximating the Top Eigenvectors of a Matrix</title>
      <link>https://arxiv.org/abs/2405.14765</link>
      <description>arXiv:2405.14765v1 Announce Type: cross 
Abstract: Finding a good approximation of the top eigenvector of a given $d\times d$ matrix $A$ is a basic and important computational problem, with many applications. We give two different quantum algorithms that, given query access to the entries of a Hermitian matrix $A$ and assuming a constant eigenvalue gap, output a classical description of a good approximation of the top eigenvector: one algorithm with time complexity $\mathcal{\tilde{O}}(d^{1.75})$ and one with time complexity $d^{1.5+o(1)}$ (the first algorithm has a slightly better dependence on the $\ell_2$-error of the approximating vector than the second, and uses different techniques of independent interest). Both of our quantum algorithms provide a polynomial speed-up over the best-possible classical algorithm, which needs $\Omega(d^2)$ queries to entries of $A$, and hence $\Omega(d^2)$ time. We extend this to a quantum algorithm that outputs a classical description of the subspace spanned by the top-$q$ eigenvectors in time $qd^{1.5+o(1)}$. We also prove a nearly-optimal lower bound of $\tilde{\Omega}(d^{1.5})$ on the quantum query complexity of approximating the top eigenvector.
  Our quantum algorithms run a version of the classical power method that is robust to certain benign kinds of errors, where we implement each matrix-vector multiplication with small and well-behaved error on a quantum computer, in different ways for the two algorithms. Our first algorithm estimates the matrix-vector product one entry at a time, using a new ``Gaussian phase estimation'' procedure. Our second algorithm uses block-encoding techniques to compute the matrix-vector product as a quantum state, from which we obtain a classical description by a new time-efficient unbiased pure-state tomography procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14765v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanlin Chen, Andr\'as Gily\'en, Ronald de Wolf</dc:creator>
    </item>
    <item>
      <title>Minimum Cost Adaptive Submodular Cover</title>
      <link>https://arxiv.org/abs/2208.08351</link>
      <description>arXiv:2208.08351v2 Announce Type: replace 
Abstract: Adaptive submodularity is a fundamental concept in stochastic optimization, with numerous applications such as sensor placement, hypothesis identification and viral marketing. We consider the problem of minimum cost cover of adaptive-submodular functions, and provide a $4(1+\ln Q)$-approximation algorithm, where $Q$ is the goal value. In fact, we consider a significantly more general objective of minimizing the $p^{th}$ moment of the coverage cost, and show that our algorithm simultaneously achieves a $(p+1)^{p+1}\cdot (\ln Q+1)^p$ approximation guarantee for all $p\ge 1$. All our approximation ratios are best possible up to constant factors (assuming $P\ne NP$). Moreover, our results also extend to the setting where one wants to cover {\em multiple} adaptive-submodular functions. Finally, we evaluate the empirical performance of our algorithm on instances of hypothesis identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.08351v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hessa Al-Thani, Yubing Cui, Viswanath Nagarajan</dc:creator>
    </item>
    <item>
      <title>Improved Approximation Bounds for Minimum Weight Cycle in the CONGEST Model</title>
      <link>https://arxiv.org/abs/2308.08670</link>
      <description>arXiv:2308.08670v3 Announce Type: replace 
Abstract: Minimum Weight Cycle (MWC) is the problem of finding a simple cycle of minimum weight in a graph $G=(V,E)$. This is a fundamental graph problem with classical sequential algorithms that run in $\tilde{O}(n^3)$ and $\tilde{O}(mn)$ time where $n=|V|$ and $m=|E|$. In recent years this problem has received significant attention in the context of fine-grained sequential complexity as well as in the design of faster sequential approximation algorithms, though not much is known in the distributed CONGEST model.
  We present sublinear-round approximation algorithms for computing MWC in directed graphs, and weighted graphs. Our algorithms use a variety of techniques in non-trivial ways, such as in our approximate directed unweighted MWC algorithm that efficiently computes BFS from all vertices restricted to certain implicitly computed neighborhoods in sublinear rounds, and in our weighted approximation algorithms that use unweighted MWC algorithms on scaled graphs combined with a fast and streamlined method for computing multiple source approximate SSSP. We present $\tilde{\Omega}(\sqrt{n})$ lower bounds for arbitrary constant factor approximation of MWC in directed graphs and undirected weighted graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.08670v3</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vignesh Manoharan, Vijaya Ramachandran</dc:creator>
    </item>
    <item>
      <title>Size-constrained Weighted Ancestors with Applications</title>
      <link>https://arxiv.org/abs/2311.15777</link>
      <description>arXiv:2311.15777v2 Announce Type: replace 
Abstract: The weighted ancestor problem on a rooted node-weighted tree $T$ is a generalization of the classic predecessor problem: construct a data structure for a set of integers that supports fast predecessor queries. Both problems are known to require $\Omega(\log\log n)$ time for queries provided $\mathcal{O}(n\text{ poly} \log n)$ space is available, where $n$ is the input size. The weighted ancestor problem has attracted a lot of attention by the combinatorial pattern matching community due to its direct application to suffix trees. In this formulation of the problem, the nodes are weighted by string depth. This research has culminated in a data structure for weighted ancestors in suffix trees with $\mathcal{O}(1)$ query time and an $\mathcal{O}(n)$-time construction algorithm [Belazzougui et al., CPM 2021]. In this paper, we consider a different version of the weighted ancestor problem, where the nodes are weighted by any function $\textsf{weight}$ that maps the nodes of $T$ to positive integers, such that $\textsf{weight}(u)\le \textsf{size}(u)$ for any node $u$ and $\textsf{weight}(u_1)\le \textsf{weight}(u_2)$ if node $u_1$ is a descendant of node $u_2$, where $\textsf{size}(u)$ is the number of nodes in the subtree rooted at $u$. In the size-constrained weighted ancestor (SWA) problem, for any node $u$ of $T$ and any integer $k$, we are asked to return the lowest ancestor $w$ of $u$ with weight at least $k$. We show that for any rooted tree with $n$ nodes, we can locate node $w$ in $\mathcal{O}(1)$ time after $\mathcal{O}(n)$-time preprocessing. In particular, this implies a data structure for the SWA problem in suffix trees with $\mathcal{O}(1)$ query time and $\mathcal{O}(n)$-time preprocessing, when the nodes are weighted by $\textsf{weight}$. We also show several string-processing applications of this result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15777v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip Bille, Yakov Nekrich, Solon P. Pissis</dc:creator>
    </item>
    <item>
      <title>Parsimonious Learning-Augmented Approximations for Dense Instances of $\mathcal{NP}$-hard Problems</title>
      <link>https://arxiv.org/abs/2402.02062</link>
      <description>arXiv:2402.02062v2 Announce Type: replace 
Abstract: The classical work of (Arora et al., 1999) provides a scheme that gives, for any $\epsilon&gt;0$, a polynomial time $1-\epsilon$ approximation algorithm for dense instances of a family of $\mathcal{NP}$-hard problems, such as Max-CUT and Max-$k$-SAT. In this paper we extend and speed up this scheme using a logarithmic number of one-bit predictions. We propose a learning augmented framework which aims at finding fast algorithms which guarantees approximation consistency, smoothness and robustness with respect to the prediction error. We provide such algorithms, which moreover use predictions parsimoniously, for dense instances of various optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02062v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evripidis Bampis, Bruno Escoffier, Michalis Xefteris</dc:creator>
    </item>
    <item>
      <title>Parallel and (Nearly) Work-Efficient Dynamic Programming</title>
      <link>https://arxiv.org/abs/2404.16314</link>
      <description>arXiv:2404.16314v2 Announce Type: replace 
Abstract: The idea of dynamic programming (DP), proposed by Bellman in the 1950s, is one of the most important algorithmic techniques. However, in parallel, many fundamental and sequentially simple problems become more challenging, and open to a (nearly) work-efficient solution (i.e., the work is off by at most a polylogarithmic factor over the best sequential solution). In fact, sequential DP algorithms employ many advanced optimizations such as decision monotonicity or special data structures, and achieve better work than straightforward solutions. Many such optimizations are inherently sequential, which creates extra challenges for a parallel algorithm to achieve the same work bound.
  The goal of this paper is to achieve (nearly) work-efficient parallel DP algorithms by parallelizing classic, highly-optimized and practical sequential algorithms. We show a general framework called the Cordon Algorithm for parallel DP algorithms, and use it to solve several classic problems. Our selection of problems includes Longest Increasing Subsequence (LIS), sparse Longest Common Subsequence (LCS), convex/concave generalized Least Weight Subsequence (LWS), Optimal Alphabetic Tree (OAT), and more. We show how the Cordon Algorithm can be used to achieve the same level of optimization as the sequential algorithms, and achieve good parallelism. Many of our algorithms are conceptually simple, and we show some experimental results as proofs-of-concept.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16314v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3626183.3659958</arxiv:DOI>
      <dc:creator>Xiangyun Ding, Yan Gu, Yihan Sun</dc:creator>
    </item>
    <item>
      <title>Discretely Beyond $1/e$: Guided Combinatorial Algorithms for Submodular Maximization</title>
      <link>https://arxiv.org/abs/2405.05202</link>
      <description>arXiv:2405.05202v2 Announce Type: replace 
Abstract: For constrained, not necessarily monotone submodular maximization, all known approximation algorithms with ratio greater than $1/e$ require continuous ideas, such as queries to the multilinear extension of a submodular function and its gradient, which are typically expensive to simulate with the original set function. For combinatorial algorithms, the best known approximation ratios for both size and matroid constraint are obtained by a simple randomized greedy algorithm of Buchbinder et al. [9]: $1/e \approx 0.367$ for size constraint and $0.281$ for the matroid constraint in $\mathcal O (kn)$ queries, where $k$ is the rank of the matroid. In this work, we develop the first combinatorial algorithms to break the $1/e$ barrier: we obtain approximation ratio of $0.385$ in $\mathcal O (kn)$ queries to the submodular set function for size constraint, and $0.305$ for a general matroid constraint. These are achieved by guiding the randomized greedy algorithm with a fast local search algorithm. Further, we develop deterministic versions of these algorithms, maintaining the same ratio and asymptotic time complexity. Finally, we develop a deterministic, nearly linear time algorithm with ratio $0.377$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05202v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixin Chen, Ankur Nath, Chunli Peng, Alan Kuhnle</dc:creator>
    </item>
    <item>
      <title>Faster Linear-Size And-Or Path and Adder Circuits</title>
      <link>https://arxiv.org/abs/2405.12765</link>
      <description>arXiv:2405.12765v2 Announce Type: replace 
Abstract: We consider the fundamental problem of constructing fast and small circuits for binary addition. We propose a new algorithm with running time $\mathcal O(n \log_2 n)$ for constructing linear-size $n$-bit adder circuits with a significantly better depth guarantee compared to previous approaches: Our circuits have a depth of at most $\log_2 n + \log_2 \log_2 n + \log_2 \log_2 \log_2 n + \text{const}$, improving upon the previously best circuits by [12] with a depth of at most $\log_2 n + 8 \sqrt{\log_2 n} + 6 \log_2 \log_2 n + \text{const}$. Hence, we decrease the gap to the lower bound of $\log_2 n + \log_2 \log_2 n + \text{const}$ by [5] significantly from $\mathcal O (\sqrt{\log_2 n})$ to $\mathcal O(\log_2 \log_2 \log_2 n)$.
  Our core routine is a new algorithm for the construction of a circuit for a single carry bit, or, more generally, for an And-Or path, i.e., a Boolean function of type $t_0 \lor ( t_1 \land (t_2 \lor ( \dots t_{m-1}) \dots ))$. We compute linear-size And-Or path circuits with a depth of at most $\log_2 m + \log_2 \log_2 m + 0.65$ in time $\mathcal O(m \log_2 m)$. These are the first And-Or path circuits known that, up to an additive constant, match the lower bound by [5] and at the same time have a linear size. The previously fastest And-Or path circuits are only by an additive constant worse in depth, but have a much higher size in the order of $\mathcal O (m \log_2 m)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12765v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ulrich Brenner, Anna Silvanus</dc:creator>
    </item>
    <item>
      <title>A Strongly Polynomial-Time Algorithm for Weighted General Factors with Three Feasible Degrees</title>
      <link>https://arxiv.org/abs/2301.11761</link>
      <description>arXiv:2301.11761v3 Announce Type: replace-cross 
Abstract: General factors are a generalization of matchings. Given a graph $G$ with a set $\pi(v)$ of feasible degrees, called a degree constraint, for each vertex $v$ of $G$, the general factor problem is to find a (spanning) subgraph $F$ of $G$ such that $\text{deg}_F(x) \in \pi(v)$ for every $v$ of $G$. When all degree constraints are symmetric $\Delta$-matroids, the problem is solvable in polynomial time. The weighted general factor problem is to find a general factor of the maximum total weight in an edge-weighted graph. In this paper, we present the first strongly polynomial-time algorithm for a type of weighted general factor problems with real-valued edge weights that is provably not reducible to the weighted matching problem by gadget constructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.11761v3</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuai Shao, Stanislav \v{Z}ivn\'y</dc:creator>
    </item>
    <item>
      <title>A Competitive Algorithm for Agnostic Active Learning</title>
      <link>https://arxiv.org/abs/2310.18786</link>
      <description>arXiv:2310.18786v3 Announce Type: replace-cross 
Abstract: For some hypothesis classes and input distributions, active agnostic learning needs exponentially fewer samples than passive learning; for other classes and distributions, it offers little to no improvement. The most popular algorithms for agnostic active learning express their performance in terms of a parameter called the disagreement coefficient, but it is known that these algorithms are inefficient on some inputs.
  We take a different approach to agnostic active learning, getting an algorithm that is competitive with the optimal algorithm for any binary hypothesis class $H$ and distribution $D_X$ over $X$. In particular, if any algorithm can use $m^*$ queries to get $O(\eta)$ error, then our algorithm uses $O(m^* \log |H|)$ queries to get $O(\eta)$ error. Our algorithm lies in the vein of the splitting-based approach of Dasgupta [2004], which gets a similar result for the realizable ($\eta = 0$) setting.
  We also show that it is NP-hard to do better than our algorithm's $O(\log |H|)$ overhead in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18786v3</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Eric Price, Yihan Zhou</dc:creator>
    </item>
    <item>
      <title>Collaborative Learning with Different Labeling Functions</title>
      <link>https://arxiv.org/abs/2402.10445</link>
      <description>arXiv:2402.10445v3 Announce Type: replace-cross 
Abstract: We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions.
  We show that, when the data distributions satisfy a weaker realizability assumption, which appeared in [Crammer and Mansour, 2012] in the context of multi-task learning, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class.
  In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is NP-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, we give learners that are both sample- and computationally-efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10445v3</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuyang Deng, Mingda Qiao</dc:creator>
    </item>
    <item>
      <title>Agent-based Leader Election, MST, and Beyond</title>
      <link>https://arxiv.org/abs/2403.13716</link>
      <description>arXiv:2403.13716v2 Announce Type: replace-cross 
Abstract: Leader election is one of the fundamental and well-studied problems in distributed computing. In this paper, we initiate the study of leader election using mobile agents. Suppose $n$ agents are positioned initially arbitrarily on the nodes of an arbitrary, anonymous, $n$-node, $m$-edge graph $G$. The agents relocate themselves autonomously on the nodes of $G$ and elect an agent as a leader such that the leader agent knows it is a leader and the other agents know they are not leaders. The objective is to minimize time and memory requirements. Following the literature, we consider the synchronous setting in which each agent performs its operations synchronously with others and hence the time complexity can be measured in rounds. The quest in this paper is to provide solutions without agents knowing any graph parameter, such as $n$, a priori. We first establish that, without agents knowing any graph parameter a priori, there exists a deterministic algorithm to elect an agent as a leader in $O(m)$ rounds with $O(n\log n)$ bits at each agent. Using this leader election result, we develop a deterministic algorithm for agents to construct a minimum spanning tree of $G$ in $O(m+n\log n)$ rounds using $O(n \log n)$ bits memory at each agent, without agents knowing any graph parameter a priori. Finally, using the same leader election result, we provide improved time/memory results for other fundamental distributed graph problems, namely, gathering, maximal independent set, and minimal dominating sets, removing the assumptions on agents knowing graph parameters a priori.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13716v2</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ajay D. Kshemkalyani, Manish Kumar, Anisur Rahaman Molla, Gokarna Sharma</dc:creator>
    </item>
    <item>
      <title>Online bipartite matching with imperfect advice</title>
      <link>https://arxiv.org/abs/2405.09784</link>
      <description>arXiv:2405.09784v3 Announce Type: replace-cross 
Abstract: We study the problem of online unweighted bipartite matching with $n$ offline vertices and $n$ online vertices where one wishes to be competitive against the optimal offline algorithm. While the classic RANKING algorithm of Karp et al. [1990] provably attains competitive ratio of $1-1/e &gt; 1/2$, we show that no learning-augmented method can be both 1-consistent and strictly better than $1/2$-robust under the adversarial arrival model. Meanwhile, under the random arrival model, we show how one can utilize methods from distribution testing to design an algorithm that takes in external advice about the online vertices and provably achieves competitive ratio interpolating between any ratio attainable by advice-free methods and the optimal ratio of 1, depending on the advice quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09784v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davin Choo, Themis Gouleakis, Chun Kai Ling, Arnab Bhattacharyya</dc:creator>
    </item>
  </channel>
</rss>
