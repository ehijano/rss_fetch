<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Nov 2025 05:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>PEARL: Power- and Energy-Aware Multicore Intermittent Computing</title>
      <link>https://arxiv.org/abs/2511.00316</link>
      <description>arXiv:2511.00316v1 Announce Type: new 
Abstract: Low-power multicore platforms are suitable for running data-intensive tasks in parallel, but they are highly inefficient for computing on intermittent power. In this work, we present PEARL (PowEr And eneRgy-aware MuLticore Intermittent Computing), a novel systems support that can make existing multicore microcontroller (MCU) platforms suitable for efficient intermittent computing. PEARL achieves this by leveraging only a three-threshold voltage tracking circuit and an external fast non-volatile memory, which multicore MCUs can smoothly interface. PEARL software runtime manages these components and performs energy- and power-aware adaptation of the multicore configuration to introduce minimal backup overheads and boost performance. Our evaluation shows that PEARL outperforms the state-of-the-art solutions by up to 30x and consumes up to 32x less energy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00316v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Khakim Akhunov, Eren Yildiz, Kasim Sinan Yildirim</dc:creator>
    </item>
    <item>
      <title>Edge-Enabled UAV Swarm Deployment for Rapid Post-Disaster Search and Rescue</title>
      <link>https://arxiv.org/abs/2511.01459</link>
      <description>arXiv:2511.01459v1 Announce Type: new 
Abstract: This paper presents an optimized Joint Radar-Communication (JRC) system utilizing multiple Unmanned Aerial Vehicles (UAVs) to simultaneously achieve sensing and communication objectives. By leveraging UAVs equipped with dual radar and communication capabilities, the proposed framework aims to maximize radar sensing performance across all UAVs in challenging environments. The proposed approach focuses on formulating and solving a UAV positioning and power allocation problem to optimize multi-UAV sensing and communications performance over multiple targets within designated zones. Due to the NP-hard and combinatorial nature of the problem, we propose a Distributed JRC-based (DJRC) solution. This solution employs an efficient reward for potential actions and consistently selects the best action that maximizes the reward while ensuring both communications and sensing performance. Simulation results demonstrate significant performance improvements of the proposed solution over state-of-the-art radar- or communication-centric trajectory planning methods, with polynomial complexity dependent on the number of UAVs and linear dependence on the iteration count.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01459v1</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alaa Awad Abdellatif, Helder Fontes, Andre Coelho, Luis M. Pessoa, Rui Campos</dc:creator>
    </item>
    <item>
      <title>Tailored robotic training improves hand function and proprioceptive processing in stroke survivors with proprioceptive deficits: A randomized controlled trial</title>
      <link>https://arxiv.org/abs/2511.00259</link>
      <description>arXiv:2511.00259v1 Announce Type: cross 
Abstract: Precision rehabilitation aims to tailor movement training to improve outcomes. We tested whether proprioceptively-tailored robotic training improves hand function and neural processing in stroke survivors. Using a robotic finger exoskeleton, we tested two proprioceptively-tailored approaches: Propriopixel Training, which uses robot-facilitated, gamified movements to enhance proprioceptive processing, and Virtual Assistance Training, which reduces robotic aid to increase reliance on self-generated feedback. In a randomized controlled trial, forty-six chronic stroke survivors completed nine 2-hour sessions of Standard, Propriopixel or Virtual training. Among participants with proprioceptive deficits, Propriopixel ((Box and Block Test: 7 +/- 4.2, p=0.002) and Virtual Assistance (4.5 +/- 4.4 , p=0.068) yielded greater gains in hand function (Standard: 0.8 +/- 2.3 blocks). Proprioceptive gains correlated with improvements in hand function. Tailored training enhanced neural sensitivity to proprioceptive cues, evidenced by a novel EEG biomarker, the proprioceptive Contingent Negative Variation. These findings support proprioceptively-tailored training as a pathway to precision neurorehabilitation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00259v1</guid>
      <category>cs.RO</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andria J. Farrens, Luis Garcia-Fernandez, Raymond Diaz Rojas, Jillian Obeso Estrada, Dylan Reinsdorf, Vicky Chan, Disha Gupta, Joel Perry, Eric Wolbrecht, An Do, Steven C. Cramer, David J. Reinkensmeyer</dc:creator>
    </item>
    <item>
      <title>Lessons Learned from the Use of Generative AI in Engineering and Quality Assurance of a WEB System for Healthcare</title>
      <link>https://arxiv.org/abs/2511.00658</link>
      <description>arXiv:2511.00658v1 Announce Type: cross 
Abstract: The advances and availability of technologies involving Generative Artificial Intelligence (AI) are evolving clearly and explicitly, driving immediate changes in various work activities. Software Engineering (SE) is no exception and stands to benefit from these new technologies, enhancing productivity and quality in its software development processes. However, although the use of Generative AI in SE practices is still in its early stages, considering the lack of conclusive results from ongoing research and the limited technological maturity, we have chosen to incorporate these technologies in the development of a web-based software system to be used in clinical trials by a thoracic diseases research group at our university. For this reason, we decided to share this experience report documenting our development team's learning journey in using Generative AI during the software development process. Project management, requirements specification, design, development, and quality assurance activities form the scope of observation. Although we do not yet have definitive technological evidence to evolve our development process significantly, the results obtained and the suggestions shared here represent valuable insights for software organizations seeking to innovate their development practices to achieve software quality with generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00658v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guilherme H. Travassos, Sabrina Rocha, Rodrigo Feitosa, Felipe Assis, Patricia Goncalves, Andre Gheventer, Larissa Galeno, Arthur Sasse, Julio Cesar Guimaraes, Carlos Brito, Joao Pedro Wieland</dc:creator>
    </item>
    <item>
      <title>Correspondence Between Ising Machines and Neural Networks</title>
      <link>https://arxiv.org/abs/2511.00746</link>
      <description>arXiv:2511.00746v1 Announce Type: cross 
Abstract: Computation with the Ising model is central to future computing technologies like quantum annealing, adiabatic quantum computing, and thermodynamic classical computing. Traditionally, computed values have been equated with ground states. This paper generalizes computation with ground states to computation with spin averages, allowing computations to take place at high temperatures. It then introduces a systematic correspondence between Ising devices and neural networks and a simple method to run trained feed-forward neural networks on Ising-type hardware. Finally, a mathematical proof is offered that these implementations are always successful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00746v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>quant-ph</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew G. Moore</dc:creator>
    </item>
    <item>
      <title>Characterizing QUBO Reformulations of the Max-k-Cut Problem for Quantum Computing</title>
      <link>https://arxiv.org/abs/2511.01108</link>
      <description>arXiv:2511.01108v1 Announce Type: cross 
Abstract: Quantum computing offers significant potential for solving NP-hard combinatorial (optimization) problems that are beyond the reach of classical computers. One way to tap into this potential is by reformulating combinatorial problems as a quadratic unconstrained binary optimization (QUBO) problem. The solution of the QUBO reformulation can then be addressed using adiabatic quantum computing devices or appropriate quantum computing algorithms on gate-based quantum computing devices. In general, QUBO reformulations of combinatorial problems can be readily obtained by properly penalizing the violation of the problem's constraints in the original problem's objective. However, characterizing tight (i.e., minimal but sufficient) penalty coefficients for this purpose is critical for enabling the solution of the resulting QUBO in current and near-term quantum computing devices. Along these lines, we here focus on the (weighted) max $k$-cut problem, a fundamental combinatorial problem with wide-ranging applications that generalizes the well-known max cut problem. We present closed-form characterizations of tight penalty coefficients for two distinct QUBO reformulations of the max $k$-cut problem whose values depend on the (weighted) degree of the vertices of the graph defining the problem. These findings contribute to the ongoing effort to make quantum computing a viable tool for solving combinatorial problems at scale. We support our theoretical results with illustrative examples. Further, we benchmark the proposed QUBO reformulations to solve the max $k$-cut problem on a quantum computer simulator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01108v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrian Harkness, Hamidreza Validi, Ramin Fakhimi, Illya V. Hicks, Tam\'as Terlaky, Luis F. Zuluaga</dc:creator>
    </item>
    <item>
      <title>MM-2FSK: Multimodal Frequency Shift Keying for Ultra-Efficient and Robust High-Resolution MIMO Radar Imaging</title>
      <link>https://arxiv.org/abs/2511.01405</link>
      <description>arXiv:2511.01405v1 Announce Type: cross 
Abstract: Accurate reconstruction of static and rapidly moving targets demands three-dimensional imaging solutions with high temporal and spatial resolution. Radar sensors are a promising sensing modality because of their fast capture rates and their independence from lighting conditions. To achieve high spatial resolution, MIMO radars with large apertures are required. Yet, they are infrequently used for dynamic scenarios due to significant limitations in signal processing algorithms. These limitations impose substantial hardware constraints due to their computational intensity and reliance on large signal bandwidths, ultimately restricting the sensor's capture rate.
  One solution of previous work is to use few frequencies only, which enables faster capture and requires less computation; however, this requires coarse knowledge of the target's position and works in a limited depth range only. To address these challenges, we extend previous work into the multimodal domain with MM-2FSK, which leverages an assistive optical depth sensing modality to obtain a depth prior, enabling high framerate capture with only few frequencies.
  We evaluate our method using various target objects with known ground truth geometry that is spatially registered to real millimeter-wave MIMO radar measurements. Our method demonstrates superior performance in terms of depth quality, being able to compete with the time- and resource-intensive measurements with many frequencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01405v1</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vanessa Wirth, Johanna Br\"aunig, Martin Vossiek, Tim Weyrich, Marc Stamminger</dc:creator>
    </item>
    <item>
      <title>Closed-Loop Long-Term Experimental Molecular Communication System</title>
      <link>https://arxiv.org/abs/2502.00831</link>
      <description>arXiv:2502.00831v2 Announce Type: replace 
Abstract: We present a fluid-based experimental molecular communication (MC) testbed which uses media modulation. Motivated by the natural human cardiovascular system, the testbed operates in a closed-loop tube system. The proposed system is designed to be biocompatible, resource-efficient, and controllable from outside the tube. As signaling molecule, the testbed employs the green fluorescent protein variant "Dreiklang" (GFPD). GFPDs can be reversibly switched via light of different wavelengths between a bright fluorescent state and a less fluorescent state. GFPDs in solution are filled into the testbed prior to the start of information transmission and remain there for an entire experiment. For information transmission, an optical transmitter (TX) and an optical eraser (EX), which are located outside the tube, are used to write and erase the information encoded in the state of the GFPDs, respectively. At the receiver (RX), the state of the GFPDs is read out by fluorescence detection. In our testbed, due to the closed-loop setup, we observe new forms of inter-symbol interferences (ISI), which do not occur in short experiments and open-loop systems. For the testbed, we developed a communication scheme, which includes blind transmission start detection, symbol-by-symbol synchronization, and adaptive threshold detection. We comprehensively analyze our MC experiments using different performance metrics. Moreover, we experimentally demonstrate the error-free transmission of 5370 bit at a data rate of 36 $\textrm{bit}\, \textrm{min}^{\boldsymbol{-1}}$ using 8-ary modulation and the error-free binary transmission of around 90000 bit at a data rate of 12 $\textrm{bit}\, \textrm{min}^{\boldsymbol{-1}}$. For the latter experiment, data was transmitted for a period of 125 hours. All signals recorded and parts of the evaluation code are publicly available on Zenodo and Github, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00831v2</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maike Scherer, Lukas Brand, Louis Wolf, Teena tom Dieck, Maximilian Sch\"afer, Sebastian Lotter, Andreas Burkovski, Heinrich Sticht, Robert Schober, Kathrin Castiglione</dc:creator>
    </item>
    <item>
      <title>Brain Organoid Computing -- an Overview</title>
      <link>https://arxiv.org/abs/2503.19770</link>
      <description>arXiv:2503.19770v2 Announce Type: replace 
Abstract: The aim of this paper is to give an overview of brain organoid computing, its characteristics, challenges, as well as possible advantages for future applications in the field of artificial intelligence. An important part is the extensive bibliography covering all relevant aspects and questions on this topic. Brain organoids - three-dimensional in vitro neural structures derived from human stem cells - have recently garnered attention not only in medical research but also as potential substrates for unconventional computing. Their biological nature allows them to exhibit learning behavior, plasticity, and parallel information processing, making them fundamentally different from traditional silicon-based systems. This opens up new perspectives on how intelligent systems might be designed in the future. Using brain organoids for computing presents a possible pathway towards more adaptive, energy-efficient, and biologically inspired forms of AI. However, challenges persist, particularly regarding lifespan, interfacing, reproducibility, and ethical concerns regarding the use of human-derived tissue. This paper aims to provide a foundational understanding for researchers exploring the convergence of human biology and computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19770v2</guid>
      <category>cs.ET</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannic Talavera, Bernd Ulmann</dc:creator>
    </item>
    <item>
      <title>Machine Olfaction and Embedded AI Are Shaping the New Global Sensing Industry</title>
      <link>https://arxiv.org/abs/2510.19660</link>
      <description>arXiv:2510.19660v2 Announce Type: replace 
Abstract: Machine olfaction is rapidly emerging as a transformative capability, with applications spanning non-invasive medical diagnostics, industrial monitoring, agriculture, and security and defense. Recent advances in stabilizing mammalian olfactory receptors and integrating them into biophotonic and bioelectronic systems have enabled detection at near single-molecule resolution thus placing machines on par with trained detection dogs. As this technology converges with multimodal AI and distributed sensor networks imbued with embedded AI, it introduces a new, biochemical layer to a sensing ecosystem currently dominated by machine vision and audition. This review and industry roadmap surveys the scientific foundations, technological frontiers, and strategic applications of machine olfaction making the case that we are currently witnessing the rise of a new industry that brings with it a global chemosensory infrastructure. We cover exemplary industrial, military and consumer applications and address some of the ethical and legal concerns arising. We find that machine olfaction is poised to bring forth a planet-wide molecular awareness tech layer with the potential of spawning vast emerging markets in health, security, and environmental sensing via scent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19660v2</guid>
      <category>cs.ET</category>
      <category>q-bio.BM</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andreas Mershin, Nikolas Stefanou, Adan Rotteveel, Matthew Kung, George Kung, Alexandru Dan, Howard Kivell, Zoia Okulova, Zoi Kountouri, Paul Pu Liang</dc:creator>
    </item>
    <item>
      <title>Controversy and consensus: common ground and best practices for life cycle assessment of emerging technologies</title>
      <link>https://arxiv.org/abs/2501.10382</link>
      <description>arXiv:2501.10382v4 Announce Type: replace-cross 
Abstract: Public and private interest in life cycle assessment (LCA) has grown as environmental disclosure norms tighten, driving demand for decision-relevant assessment early in technological development cycles. Early-stage LCA has the potential to guide design choices, steer innovation, and mitigate lock-in of adverse environmental impacts. However, many aspects of early-stage LCA practice remain unsettled. We convened experts in a series of Faraday Discussion-style workshops to address recurring debates across six key topics for emerging technologies: appropriate use of LCA, uncertainty, comparison with incumbents, standardization, scale-up, and stakeholder engagement. For each issue, we present a declarative resolution, summarize key arguments for and against it, identify points of consensus, and provide recommendations. Across topics, the research network converged on practical priorities including framing studies to the decision context; setting minimum reporting expectations for data and study quality; and explicitly stating limits of transferability for scenario-based uncertainty assessment or analytically scaled-up projections. Disagreements persisted on when to formalize standards and how extensively uncertainty can/should be treated for low-maturity technologies. Supplementing the workshop findings with examples and context from relevant literature, we synthesize outcomes into a set of shared challenges and research priorities to strengthen transparent, evidence-based, and context-informed approaches for early-stage LCA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10382v4</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachel Woods-Robinson, Amila Abeynayaka, Mik Carbajales-Dale, Hao Chen, Anthony Cheng, Gregory Cooney, Abby Kirchofer, Manish Kumar, Heather P. H. Liddell, Lisa Peterson, I. Daniel Posen, Sheikh Moni, Sylvia Sleep, Liz Wachs, Shiva Zargar, Joule Bergerson</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Wireless Semantic Communication with Large AI Models</title>
      <link>https://arxiv.org/abs/2506.03167</link>
      <description>arXiv:2506.03167v2 Announce Type: replace-cross 
Abstract: Semantic communication (SemCom) has emerged as a promising paradigm for 6G wireless systems by transmitting task-relevant information rather than raw bits, yet existing approaches remain vulnerable to dual sources of uncertainty: semantic misinterpretation arising from imperfect feature extraction and transmission-level perturbations from channel noise. Current deep learning based SemCom systems typically employ domain-specific architectures that lack robustness guarantees and fail to generalize across diverse noise conditions, adversarial attacks, and out-of-distribution data. In this paper, a novel and generalized semantic communication framework called WaSeCom is proposed to systematically address uncertainty and enhance robustness. In particular, Wasserstein distributionally robust optimization is employed to provide resilience against semantic misinterpretation and channel perturbations. A rigorous theoretical analysis is performed to establish the robust generalization guarantees of the proposed framework. Experimental results on image and text transmission demonstrate that WaSeCom achieves improved robustness under noise and adversarial perturbations. These results highlight its effectiveness in preserving semantic fidelity across varying wireless conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03167v2</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Long Tan Le, Senura Hansaja Wanasekara, Zerun Niu, Nguyen H. Tran, Phuong Vo, Walid Saad, Dusit Niyato, Zhu Han, Choong Seon Hong, H. Vincent Poor</dc:creator>
    </item>
    <item>
      <title>OrbitChain: Orchestrating In-orbit Real-time Analytics of Earth Observation Data</title>
      <link>https://arxiv.org/abs/2508.13374</link>
      <description>arXiv:2508.13374v2 Announce Type: replace-cross 
Abstract: Earth observation analytics have the potential to serve many time-sensitive applications. However, due to limited bandwidth and duration of ground-satellite connections, it takes hours or even days to download and analyze data from existing Earth observation satellites, making real-time demands like timely disaster response impossible. Toward real-time analytics, we introduce OrbitChain, a collaborative analytics framework that orchestrates computational resources across multiple satellites in an Earth observation constellation. OrbitChain decomposes analytics applications into microservices and allocates computational resources for time-constrained analysis. A traffic routing algorithm is devised to minimize the inter-satellite communication overhead. OrbitChain adopts a pipeline workflow that completes Earth observation tasks in real-time, facilitates time-sensitive applications and inter-constellation collaborations such as tip-and-cue. To evaluate OrbitChain, we implement a hardware-in-the-loop orbital computing testbed. Experiments show that our system can complete up to 60% analytics workload than existing Earth observation analytics framework while reducing the communication overhead by up to 72%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13374v2</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhouyu Li, Zhijin Yang, Huayue Gu, Xiaojian Wang, Yuchen Liu, Ruozhou Yu</dc:creator>
    </item>
  </channel>
</rss>
