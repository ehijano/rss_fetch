<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Jul 2024 10:51:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Dirichlet process mixture model based on topologically augmented signal representation for clustering infant vocalizations</title>
      <link>https://arxiv.org/abs/2407.05760</link>
      <description>arXiv:2407.05760v1 Announce Type: new 
Abstract: Based on audio recordings made once a month during the first 12 months of a child's life, we propose a new method for clustering this set of vocalizations. We use a topologically augmented representation of the vocalizations, employing two persistence diagrams for each vocalization: one computed on the surface of its spectrogram and one on the Takens' embeddings of the vocalization. A synthetic persistent variable is derived for each diagram and added to the MFCCs (Mel-frequency cepstral coefficients). Using this representation, we fit a non-parametric Bayesian mixture model with a Dirichlet process prior to model the number of components. This procedure leads to a novel data-driven categorization of vocal productions. Our findings reveal the presence of 8 clusters of vocalizations, allowing us to compare their temporal distribution and acoustic profiles in the first 12 months of life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05760v1</guid>
      <category>stat.AP</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillem Bonafos, Clara Bourot, Pierre Pudlo, Jean-Marc Freyermuth, Laurence Reboul, Samuel Tron\c{c}on, Arnaud Rey</dc:creator>
    </item>
    <item>
      <title>Normative brain mapping of 3-dimensional morphometry imaging data using skewed functional data analysis</title>
      <link>https://arxiv.org/abs/2407.05806</link>
      <description>arXiv:2407.05806v1 Announce Type: new 
Abstract: Tensor-based morphometry (TBM) aims at showing local differences in brain volumes with respect to a common template. TBM images are smooth but they exhibit (especially in diseased groups) higher values in some brain regions called lateral ventricles. More specifically, our voxelwise analysis shows both a mean-variance relationship in these areas and evidence of spatially dependent skewness. We propose a model for 3-dimensional functional data where mean, variance, and skewness functions vary smoothly across brain locations. We model the voxelwise distributions as skew-normal. The smooth effects of age and sex are estimated on a reference population of cognitively normal subjects from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and mapped across the whole brain. The three parameter functions allow to transform each TBM image (in the reference population as well as in a test set) into a Gaussian process. These subject-specific normative maps are used to derive indices of deviation from a healthy condition to assess the individual risk of pathological degeneration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05806v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Palma (for the Alzheimer's Disease Neuroimaging Initiative), Shahin Tavakoli (for the Alzheimer's Disease Neuroimaging Initiative), Julia Brettschneider (for the Alzheimer's Disease Neuroimaging Initiative), Ana-Maria Staicu (for the Alzheimer's Disease Neuroimaging Initiative), Thomas E. Nichols (for the Alzheimer's Disease Neuroimaging Initiative)</dc:creator>
    </item>
    <item>
      <title>Aortic root landmark localization with optimal transport loss for heatmap regression</title>
      <link>https://arxiv.org/abs/2407.04921</link>
      <description>arXiv:2407.04921v1 Announce Type: cross 
Abstract: Anatomical landmark localization is gaining attention to ease the burden on physicians. Focusing on aortic root landmark localization, the three hinge points of the aortic valve can reduce the burden by automatically determining the valve size required for transcatheter aortic valve implantation surgery. Existing methods for landmark prediction of the aortic root mainly use time-consuming two-step estimation methods. We propose a highly accurate one-step landmark localization method from even coarse images. The proposed method uses an optimal transport loss to break the trade-off between prediction precision and learning stability in conventional heatmap regression methods. We apply the proposed method to the 3D CT image dataset collected at Sendai Kousei Hospital and show that it significantly improves the estimation error over existing methods and other loss functions. Our code is available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04921v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tsuyoshi Ishizone, Masaki Miyasaka, Sae Ochi, Norio Tada, Kazuyuki Nakamura</dc:creator>
    </item>
    <item>
      <title>Bayesian network-guided sparse regression with flexible varying effects</title>
      <link>https://arxiv.org/abs/2407.05089</link>
      <description>arXiv:2407.05089v1 Announce Type: cross 
Abstract: In this paper, we propose Varying Effects Regression with Graph Estimation (VERGE), a novel Bayesian method for feature selection in regression. Our model has key aspects that allow it to leverage the complex structure of data sets arising from genomics or imaging studies. We distinguish between the predictors, which are the features utilized in the outcome prediction model, and the subject-level covariates, which modulate the effects of the predictors on the outcome. We construct a varying coefficients modeling framework where we infer a network among the predictor variables and utilize this network information to encourage the selection of related predictors. We employ variable selection spike-and-slab priors that enable the selection of both network-linked predictor variables and covariates that modify the predictor effects. We demonstrate through simulation studies that our method outperforms existing alternative methods in terms of both feature selection and predictive accuracy. We illustrate VERGE with an application to characterizing the influence of gut microbiome features on obesity, where we identify a set of microbial taxa and their ecological dependence relations. We allow subject-level covariates including sex and dietary intake variables to modify the coefficients of the microbiome predictors, providing additional insight into the interplay between these factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05089v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yangfan Ren, Christine B. Peterson, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Distributional stability of sparse inverse covariance matrix estimators</title>
      <link>https://arxiv.org/abs/2407.05110</link>
      <description>arXiv:2407.05110v1 Announce Type: cross 
Abstract: Finding an approximation of the inverse of the covariance matrix, also known as precision matrix, of a random vector with empirical data is widely discussed in finance and engineering. In data-driven problems, empirical data may be ``contaminated''. This raises the question as to whether the approximate precision matrix is reliable from a statistical point of view. In this paper, we concentrate on a much-noticed sparse estimator of the precision matrix and investigate the issue from the perspective of distributional stability. Specifically, we derive an explicit local Lipschitz bound for the distance between the distributions of the sparse estimator under two different distributions (regarded as the true data distribution and the distribution of ``contaminated'' data). The distance is measured by the Kantorovich metric on the set of all probability measures on a matrix space. We also present analogous results for the standard estimators of the covariance matrix and its eigenvalues. Furthermore, we discuss two applications and conduct some numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05110v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Renjie Chen, Huifu Xu, Henryk Z\"ahle</dc:creator>
    </item>
    <item>
      <title>Roughness regularization for functional data analysis with free knots spline estimation</title>
      <link>https://arxiv.org/abs/2407.05159</link>
      <description>arXiv:2407.05159v1 Announce Type: cross 
Abstract: In the era of big data, an ever-growing volume of information is recorded, either continuously over time or sporadically, at distinct time intervals. Functional Data Analysis (FDA) stands at the cutting edge of this data revolution, offering a powerful framework for handling and extracting meaningful insights from such complex datasets. The currently proposed FDA me\-thods can often encounter challenges, especially when dealing with curves of varying shapes. This can largely be attributed to the method's strong dependence on data approximation as a key aspect of the analysis process. In this work, we propose a free knots spline estimation method for functional data with two penalty terms and demonstrate its performance by comparing the results of several clustering methods on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05159v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna De Magistris (University of Campania "Luigi Vanvitelli"), Valentina De Simone (University of Campania "Luigi Vanvitelli"), Elvira Romano (University of Campania "Luigi Vanvitelli"), Gerardo Toraldo (University of Campania "Luigi Vanvitelli")</dc:creator>
    </item>
    <item>
      <title>Leveraging AI for Climate Resilience in Africa: Challenges, Opportunities, and the Need for Collaboration</title>
      <link>https://arxiv.org/abs/2407.05210</link>
      <description>arXiv:2407.05210v1 Announce Type: cross 
Abstract: As climate change issues become more pressing, their impact in Africa calls for urgent, innovative solutions tailored to the continent's unique challenges. While Artificial Intelligence (AI) emerges as a critical and valuable tool for climate change adaptation and mitigation, its effectiveness and potential are contingent upon overcoming significant challenges such as data scarcity, infrastructure gaps, and limited local AI development. This position paper explores the role of AI in climate change adaptation and mitigation in Africa. It advocates for a collaborative approach to build capacity, develop open-source data repositories, and create context-aware, robust AI-driven climate solutions that are culturally and contextually relevant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05210v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rendani Mbuvha, Yassine Yaakoubi, John Bagiliko, Santiago Hincapie Potes, Amal Nammouchi, Sabrina Amrouche</dc:creator>
    </item>
    <item>
      <title>A likelihood ratio test for circular multimodality</title>
      <link>https://arxiv.org/abs/2407.05957</link>
      <description>arXiv:2407.05957v1 Announce Type: cross 
Abstract: The modes of a statistical population are high frequency points around which most of the probability mass is accumulated. For the particular case of circular densities, we address the problem of testing if, given an observed sample of a random angle, the underlying circular distribution model is multimodal. Our work is motivated by the analysis of migration patterns of birds and the methodological proposal follows a novel approach based on likelihood ratio ideas, combined with critical bandwidths. Theoretical results support the behaviour of the test, whereas simulation examples show its finite sample performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05957v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Bol\'on, Rosa M. Crujeiras, Alberto Rodr\'iguez-Casal</dc:creator>
    </item>
    <item>
      <title>Simulation-based Benchmarking for Causal Structure Learning in Gene Perturbation Experiments</title>
      <link>https://arxiv.org/abs/2407.06015</link>
      <description>arXiv:2407.06015v1 Announce Type: cross 
Abstract: Causal structure learning (CSL) refers to the task of learning causal relationships from data. Advances in CSL now allow learning of causal graphs in diverse application domains, which has the potential to facilitate data-driven causal decision-making. Real-world CSL performance depends on a number of $\textit{context-specific}$ factors, including context-specific data distributions and non-linear dependencies, that are important in practical use-cases. However, our understanding of how to assess and select CSL methods in specific contexts remains limited. To address this gap, we present $\textit{CausalRegNet}$, a multiplicative effect structural causal model that allows for generating observational and interventional data incorporating context-specific properties, with a focus on the setting of gene perturbation experiments. Using real-world gene perturbation data, we show that CausalRegNet generates accurate distributions and scales far better than current simulation frameworks. We illustrate the use of CausalRegNet in assessing CSL methods in the context of interventional experiments in biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06015v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luka Kova\v{c}evi\'c, Izzy Newsham, Sach Mukherjee, John Whittaker</dc:creator>
    </item>
    <item>
      <title>Large Row-Constrained Supersaturated Designs for High-throughput Screening</title>
      <link>https://arxiv.org/abs/2407.06173</link>
      <description>arXiv:2407.06173v1 Announce Type: cross 
Abstract: High-throughput screening, in which multiwell plates are used to test large numbers of compounds against specific targets, is widely used across many areas of the biological sciences and most prominently in drug discovery. We propose a statistically principled approach to these screening experiments, using the machinery of supersaturated designs and the Lasso. To accommodate limitations on the number of biological entities that can be applied to a single microplate well, we present a new class of row-constrained supersaturated designs. We develop a computational procedure to construct these designs, provide some initial lower bounds on the average squared off-diagonal values of their main-effects information matrix, and study the impact of the constraint on design quality. We also show via simulation that the proposed constrained row screening method is statistically superior to existing methods and demonstrate the use of the new methodology on a real drug-discovery system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06173v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Byran J. Smucker, Stephen E. Wright, Isaac Williams, Richard C. Page, Andor J. Kiss, Surendra Bikram Silwal, Maria Weese, David J. Edwards</dc:creator>
    </item>
    <item>
      <title>Data Mining of Telematics Data: Unveiling the Hidden Patterns in Driving Behaviour</title>
      <link>https://arxiv.org/abs/2304.10591</link>
      <description>arXiv:2304.10591v2 Announce Type: replace 
Abstract: With the advancement in technology, telematics data which capture vehicle movements information are becoming available to more insurers. As these data capture the actual driving behaviour, they are expected to improve our understanding of driving risk and facilitate more accurate auto-insurance ratemaking. In this paper, we analyze an auto-insurance dataset with telematics data collected from a major European insurer. Through a detailed discussion of the telematics data structure and related data quality issues, we elaborate on practical challenges in processing and incorporating telematics information in loss modelling and ratemaking. Then, with an exploratory data analysis, we demonstrate the existence of heterogeneity in individual driving behaviour, even within the groups of policyholders with and without claims, which supports the study of telematics data. Our regression analysis reiterates the importance of telematics data in claims modelling; in particular, we propose a speed transition matrix that describes discretely recorded speed time series and produces statistically significant predictors for claim counts. We conclude that large speed transitions, together with higher maximum speed attained, nighttime driving and increased harsh braking, are associated with increased claim counts. Moreover, we empirically illustrate the learning effects in driving behaviour: we show that both severe harsh events detected at a high threshold and expected claim counts are not directly proportional with driving time or distance, but they increase at a decreasing rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.10591v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Weng Chan, Spark C. Tseung, Andrei L. Badescu, X. Sheldon Lin</dc:creator>
    </item>
    <item>
      <title>Community Correlations and Testing Independence Between Binary Graphs</title>
      <link>https://arxiv.org/abs/1906.03661</link>
      <description>arXiv:1906.03661v3 Announce Type: replace-cross 
Abstract: Graph data has a unique structure that deviates from standard data assumptions, often necessitating modifications to existing methods or the development of new ones to ensure valid statistical analysis. In this paper, we explore the notion of correlation and dependence between two binary graphs. Given vertex communities, we propose community correlations to measure the edge association, which equals zero if and only if the two graphs are conditionally independent within a specific pair of communities. The set of community correlations naturally leads to the maximum community correlation, indicating conditional independence on all possible pairs of communities, and to the overall graph correlation, which equals zero if and only if the two binary graphs are unconditionally independent. We then compute the sample community correlations via graph encoder embedding, proving they converge to their respective population versions, and derive the asymptotic null distribution to enable a fast, valid, and consistent test for conditional or unconditional independence between two binary graphs. The theoretical results are validated through comprehensive simulations, and we provide two real-data examples: one using Enron email networks and another using mouse connectome graphs, to demonstrate the utility of the proposed correlation measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:1906.03661v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cencheng Shen, Jes\"us Arroyo, Junhao Xiong, Joshua T. Vogelstein</dc:creator>
    </item>
    <item>
      <title>Towards Improving Unit Commitment Economics: An Add-On Tailor for Renewable Energy and Reserve Predictions</title>
      <link>https://arxiv.org/abs/2208.13065</link>
      <description>arXiv:2208.13065v4 Announce Type: replace-cross 
Abstract: Generally, day-ahead unit commitment (UC) is conducted in a predict-then-optimize process: it starts by predicting the renewable energy source (RES) availability and system reserve requirements; given the predictions, the UC model is then optimized to determine the economic operation plans. In fact, predictions within the process are raw. In other words, if the predictions are further tailored to assist UC in making the economic operation plans against realizations of the RES and reserve requirements, UC economics will benefit significantly. To this end, this paper presents a cost-oriented tailor of RES-and-reserve predictions for UC, deployed as an add-on to the predict-then-optimize process. The RES-and-reserve tailor is trained by solving a bi-level mixed-integer programming model: the upper level trains the tailor based on its induced operating cost; the lower level, given tailored predictions, mimics the system operation process and feeds the induced operating cost back to the upper level; finally, the upper level evaluates the training quality according to the fed-back cost. Through this training, the tailor learns to customize the raw predictions into cost-oriented predictions. Moreover, the tailor can be embedded into the existing predict-then-optimize process as an add-on, improving the UC economics. Lastly, the presented method is compared to traditional, binary-relaxation, neural network-based, stochastic, and robust methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.13065v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xianbang Chen, Yikui Liu, Lei Wu</dc:creator>
    </item>
    <item>
      <title>Predicting the volatility of major energy commodity prices: the dynamic persistence model</title>
      <link>https://arxiv.org/abs/2402.01354</link>
      <description>arXiv:2402.01354v2 Announce Type: replace-cross 
Abstract: Time variation and persistence are crucial properties of volatility that are often studied separately in energy volatility forecasting models. Here, we propose a novel approach that allows shocks with heterogeneous persistence to vary smoothly over time, and thus model the two together. We argue that this is important because such dynamics arise naturally from the dynamic nature of shocks in energy commodities. We identify such dynamics from the data using localised regressions and build a model that significantly improves volatility forecasts. Such forecasting models, based on a rich persistence structure that varies smoothly over time, outperform state-of-the-art benchmark models and are particularly useful for forecasting over longer horizons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01354v2</guid>
      <category>q-fin.GN</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jozef Barunik, Lukas Vacha</dc:creator>
    </item>
    <item>
      <title>Correlations versus noise in the NFT market</title>
      <link>https://arxiv.org/abs/2404.15495</link>
      <description>arXiv:2404.15495v2 Announce Type: replace-cross 
Abstract: The non-fungible token (NFT) market emerges as a recent trading innovation leveraging blockchain technology, mirroring the dynamics of the cryptocurrency market. The current study is based on the capitalization changes and transaction volumes across a large number of token collections on the Ethereum platform. In order to deepen the understanding of the market dynamics, the collection-collection dependencies are examined by using the multivariate formalism of detrended correlation coefficient and correlation matrix. It appears that correlation strength is lower here than that observed in previously studied markets. Consequently, the eigenvalue spectra of the correlation matrix more closely follow the Marchenko-Pastur distribution, still, some departures indicating the existence of correlations remain. The comparison of results obtained from the correlation matrix built from the Pearson coefficients and, independently, from the detrended cross-correlation coefficients suggests that the global correlations in the NFT market arise from higher frequency fluctuations. Corresponding minimal spanning trees (MSTs) for capitalization variability exhibit a scale-free character while, for the number of transactions, they are somewhat more decentralized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15495v2</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <category>econ.EM</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0214399</arxiv:DOI>
      <arxiv:journal_reference>Chaos 34, 073112 (2024)</arxiv:journal_reference>
      <dc:creator>Marcin W\k{a}torek, Pawe{\l} Szyd{\l}o, Jaros{\l}aw Kwapie\'n, Stanis{\l}aw Dro\.zd\.z</dc:creator>
    </item>
    <item>
      <title>Categorization of 31 computational methods to detect spatially variable genes from spatially resolved transcriptomics data</title>
      <link>https://arxiv.org/abs/2405.18779</link>
      <description>arXiv:2405.18779v2 Announce Type: replace-cross 
Abstract: In the analysis of spatially resolved transcriptomics data, detecting spatially variable genes (SVGs) is crucial. Numerous computational methods exist, but varying SVG definitions and methodologies lead to incomparable results. We review 31 state-of-the-art methods, categorizing SVGs into three types: overall, cell-type-specific, and spatial-domain-marker SVGs. Our review explains the intuitions underlying these methods, summarizes their applications, and categorizes the hypothesis tests they use in the trade-off between generality and specificity for SVG detection. We discuss challenges in SVG detection and propose future directions for improvement. Our review offers insights for method developers and users, advocating for category-specific benchmarking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18779v2</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guanao Yan, Shuo Harper Hua, Jingyi Jessica Li</dc:creator>
    </item>
    <item>
      <title>Analysis of Linked Files: A Missing Data Perspective</title>
      <link>https://arxiv.org/abs/2406.14717</link>
      <description>arXiv:2406.14717v3 Announce Type: replace-cross 
Abstract: In many applications, researchers seek to identify overlapping entities across multiple data files. Record linkage algorithms facilitate this task, in the absence of unique identifiers. As these algorithms rely on semi-identifying information, they may miss records that represent the same entity, or incorrectly link records that do not represent the same entity. Analysis of linked files commonly ignores such linkage errors, resulting in biased, or overly precise estimates of the associations of interest. We view record linkage as a missing data problem, and delineate the linkage mechanisms that underpin analysis methods with linked files. Following the missing data literature, we group these methods under three categories: likelihood and Bayesian methods, imputation methods, and weighting methods. We summarize the assumptions and limitations of the methods, and evaluate their performance in a wide range of simulation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14717v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gauri Kamat, Roee Gutman</dc:creator>
    </item>
    <item>
      <title>Lithium-Ion Battery System Health Monitoring and Fault Analysis from Field Data Using Gaussian Processes</title>
      <link>https://arxiv.org/abs/2406.19015</link>
      <description>arXiv:2406.19015v2 Announce Type: replace-cross 
Abstract: Health monitoring, fault analysis, and detection are critical for the safe and sustainable operation of battery systems. We apply Gaussian process resistance models on lithium iron phosphate battery field data to effectively separate the time-dependent and operating point-dependent resistance. The data set contains 29 battery systems returned to the manufacturer for warranty, each with eight cells in series, totaling 232 cells and 131 million data rows. We develop probabilistic fault detection rules using recursive spatiotemporal Gaussian processes. These processes allow the quick processing of over a million data points, enabling advanced online monitoring and furthering the understanding of battery pack failure in the field. The analysis underlines that often, only a single cell shows abnormal behavior or a knee point, consistent with weakest-link failure for cells connected in series, amplified by local resistive heating. The results further the understanding of how batteries degrade and fail in the field and demonstrate the potential of efficient online monitoring based on data. We open-source the code and publish the large data set upon completion of the review of this article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19015v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joachim Schaeffer, Eric Lenz, Duncan Gulla, Martin Z. Bazant, Richard D. Braatz, Rolf Findeisen</dc:creator>
    </item>
  </channel>
</rss>
