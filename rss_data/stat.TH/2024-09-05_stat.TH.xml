<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Sep 2024 04:02:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Error bounds of Median-of-means estimators with VC-dimension</title>
      <link>https://arxiv.org/abs/2409.03410</link>
      <description>arXiv:2409.03410v1 Announce Type: new 
Abstract: We obtain the upper error bounds of robust estimators for mean vector, using the median-of-means (MOM) method. The method is designed to handle data with heavy tails and contamination, with only a finite second moment, which is weaker than many others, relying on the VC dimension rather than the Rademacher complexity to measure statistical complexity. This allows us to implement MOM in covariance estimation, without imposing conditions such as $L$-sub-Gaussian or $L_{4}-L_{2}$ norm equivalence. In particular, we derive a new robust estimator, the MOM version of the halfspace depth, along with error bounds for mean estimation in any norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03410v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxuan Wang, Yiming Chen, Hanchao Wang, Lixin Zhang</dc:creator>
    </item>
    <item>
      <title>Convergence Rates for the Maximum A Posteriori Estimator in PDE-Regression Models with Random Design</title>
      <link>https://arxiv.org/abs/2409.03417</link>
      <description>arXiv:2409.03417v1 Announce Type: new 
Abstract: We consider the statistical inverse problem of recovering a parameter $\theta\in H^\alpha$ from data arising from the Gaussian regression problem \begin{equation*}
  Y = \mathscr{G}(\theta)(Z)+\varepsilon \end{equation*} with nonlinear forward map $\mathscr{G}:\mathbb{L}^2\to\mathbb{L}^2$, random design points $Z$ and Gaussian noise $\varepsilon$. The estimation strategy is based on a least squares approach under $\Vert\cdot\Vert_{H^\alpha}$-constraints. We establish the existence of a least squares estimator $\hat{\theta}$ as a maximizer for a given functional under Lipschitz-type assumptions on the forward map $\mathscr{G}$. A general concentration result is shown, which is used to prove consistency and upper bounds for the prediction error. The corresponding rates of convergence reflect not only the smoothness of the parameter of interest but also the ill-posedness of the underlying inverse problem. We apply the general model to the Darcy problem, where the recovery of an unknown coefficient function $f$ of a PDE is of interest. For this example, we also provide corresponding rates of convergence for the prediction and estimation errors. Additionally, we briefly discuss the applicability of the general model to other problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03417v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maximilian Siebel</dc:creator>
    </item>
    <item>
      <title>The Geometry and Well-Posedness of Sparse Regularized Linear Regression</title>
      <link>https://arxiv.org/abs/2409.03461</link>
      <description>arXiv:2409.03461v1 Announce Type: new 
Abstract: In this work, we study the well-posedness of certain sparse regularized linear regression problems, i.e., the existence, uniqueness and continuity of the solution map with respect to the data. We focus on regularization functions that are convex piecewise linear, i.e., whose epigraph is polyhedral. This includes total variation on graphs and polyhedral constraints. We provide a geometric framework for these functions based on their connection to polyhedral sets and apply this to the study of the well-posedness of the corresponding sparse regularized linear regression problem. Particularly, we provide geometric conditions for well-posedness of the regression problem, compare these conditions to those for smooth regularization, and show the computational difficulty of verifying these conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03461v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jasper Marijn Everink, Yiqiu Dong, Martin Skovgaard Andersen</dc:creator>
    </item>
    <item>
      <title>Likelihood Geometry of the Squared Grassmannian</title>
      <link>https://arxiv.org/abs/2409.03730</link>
      <description>arXiv:2409.03730v1 Announce Type: new 
Abstract: We study projection determinantal point processes and their connection to the squared Grassmannian. We prove that the log-likelihood function of this statistical model has $(n - 1)!/2$ critical points, all of which are real and positive, thereby settling a conjecture of Devriendt, Friedman, Reinke, and Sturmfels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03730v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Friedman</dc:creator>
    </item>
    <item>
      <title>On the edge eigenvalues of the precision matrices of nonstationary autoregressive processes</title>
      <link>https://arxiv.org/abs/2109.02204</link>
      <description>arXiv:2109.02204v4 Announce Type: replace-cross 
Abstract: This paper investigates structural changes in the parameters of first-order autoregressive models by analyzing the edge eigenvalues of the precision matrices. Specifically, edge eigenvalues in the precision matrix are observed if and only if there is a structural change in the autoregressive coefficients. We show that these edge eigenvalues correspond to the zeros of a determinantal equation. Additionally, we propose a consistent estimator for detecting outliers within the panel time series framework, supported by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.02204v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junho Yang</dc:creator>
    </item>
    <item>
      <title>Boosting the Power of Kernel Two-Sample Tests</title>
      <link>https://arxiv.org/abs/2302.10687</link>
      <description>arXiv:2302.10687v2 Announce Type: replace-cross 
Abstract: The kernel two-sample test based on the maximum mean discrepancy (MMD) is one of the most popular methods for detecting differences between two distributions over general metric spaces. In this paper we propose a method to boost the power of the kernel test by combining MMD estimates over multiple kernels using their Mahalanobis distance. We derive the asymptotic null distribution of the proposed test statistic and use a multiplier bootstrap approach to efficiently compute the rejection region. The resulting test is universally consistent and, since it is obtained by aggregating over a collection of kernels/bandwidths, is more powerful in detecting a wide range of alternatives in finite samples. We also derive the distribution of the test statistic for both fixed and local contiguous alternatives. The latter, in particular, implies that the proposed test is statistically efficient, that is, it has non-trivial asymptotic (Pitman) efficiency. The consistency properties of the Mahalanobis and other natural aggregation methods are also explored when the number of kernels is allowed to grow with the sample size. Extensive numerical experiments are performed on both synthetic and real-world datasets to illustrate the efficacy of the proposed method over single kernel tests. The computational complexity of the proposed method is also studied, both theoretically and in simulations. Our asymptotic results rely on deriving the joint distribution of MMD estimates using the framework of multiple stochastic integrals, which is more broadly useful, specifically, in understanding the efficiency properties of recently proposed adaptive MMD tests based on kernel aggregation and also in developing more computationally efficient (linear time) tests that combine multiple kernels. We conclude with an application of the Mahalanobis aggregation method for kernels with diverging scaling parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10687v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirban Chatterjee, Bhaswar B. Bhattacharya</dc:creator>
    </item>
    <item>
      <title>LASPATED: a Library for the Analysis of SPAtio-TEmporal Discrete data</title>
      <link>https://arxiv.org/abs/2401.04156</link>
      <description>arXiv:2401.04156v2 Announce Type: replace-cross 
Abstract: We describe methods, tools, and a software library called LASPATED, available on GitHub (at https://github.com/vguigues/) to fit models using spatio-temporal data and space-time discretization. A video tutorial for this library is available on YouTube. We consider two types of methods to estimate a non-homogeneous Poisson process in space and time. The methods approximate the arrival intensity function of the Poisson process by discretizing space and time, and estimating arrival intensity as a function of subregion and time interval. With such methods, it is typical that the dimension of the estimator is large relative to the amount of data, and therefore the performance of the estimator can be improved by using additional data. The first method uses additional data to add a regularization term to the likelihood function for calibrating the intensity of the Poisson process. The second method uses additional data to estimate arrival intensity as a function of covariates. We describe a Python package to perform various types of space and time discretization. We also describe two packages for the calibration of the models, one in Matlab and one in C++. We demonstrate the advantages of our methods compared to basic maximum likelihood estimation with simulated and real data. The experiments with real data calibrate models of the arrival process of emergencies to be handled by the Rio de Janeiro emergency medical service.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04156v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Guigues, Anton Kleywegt, Giovanni Amorim, Andr\'e Mazal Krauss, Victor Hugo Nascimento</dc:creator>
    </item>
    <item>
      <title>Negative Probability</title>
      <link>https://arxiv.org/abs/2405.03043</link>
      <description>arXiv:2405.03043v2 Announce Type: replace-cross 
Abstract: Negative probabilities arise primarily in physics, statistical quantum mechanics and quantum computing. Negative probabilities arise as mixing distributions of unobserved latent variables in Bayesian modeling. Our goal is to provide a link between these two viewpoints. Bartlett provides a definition of negative probabilities based on extraordinary random variables and properties of their characteristic function. A version of Bayes rule is given with negative mixing weights. The classic half coin distribution and Polya-Gamma mixing is discussed. Heisenberg's principle of uncertainty and the duality of scale mixtures of Normals is also discussed. A number of examples of dual densities with negative mixing measures are provided including the Linnik and Wigner distributions. Finally, we conclude with directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03043v2</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Polson, Vadim Sokolov</dc:creator>
    </item>
  </channel>
</rss>
