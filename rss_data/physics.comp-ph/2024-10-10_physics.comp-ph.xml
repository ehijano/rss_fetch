<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.comp-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.comp-ph</link>
    <description>physics.comp-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.comp-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 11 Oct 2024 04:01:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Uniform accuracy of implicit-explicit Runge-Kutta methods for linear hyperbolic relaxation systems</title>
      <link>https://arxiv.org/abs/2410.07254</link>
      <description>arXiv:2410.07254v1 Announce Type: cross 
Abstract: In this paper, we study the uniform accuracy of implicit-explicit (IMEX) Runge-Kutta (RK) schemes for general linear hyperbolic relaxation systems satisfying the structural stability condition proposed in \cite{yong_singular_1999}. We establish the uniform stability and accuracy of a class of IMEX-RK schemes with spatial discretization using a Fourier spectral method. Our results demonstrate that the accuracy of the fully discretized schemes is independent of the relaxation time across all regimes. Numerical experiments on applications in traffic flows and kinetic theory verify our theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07254v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiting Ma, Juntao Huang</dc:creator>
    </item>
    <item>
      <title>Collective variables of neural networks: empirical time evolution and scaling laws</title>
      <link>https://arxiv.org/abs/2410.07451</link>
      <description>arXiv:2410.07451v1 Announce Type: cross 
Abstract: This work presents a novel means for understanding learning dynamics and scaling relations in neural networks. We show that certain measures on the spectrum of the empirical neural tangent kernel, specifically entropy and trace, yield insight into the representations learned by a neural network and how these can be improved through architecture scaling. These results are demonstrated first on test cases before being shown on more complex networks, including transformers, auto-encoders, graph neural networks, and reinforcement learning studies. In testing on a wide range of architectures, we highlight the universal nature of training dynamics and further discuss how it can be used to understand the mechanisms behind learning in neural networks. We identify two such dominant mechanisms present throughout machine learning training. The first, information compression, is seen through a reduction in the entropy of the NTK spectrum during training, and occurs predominantly in small neural networks. The second, coined structure formation, is seen through an increasing entropy and thus, the creation of structure in the neural network representations beyond the prior established by the network at initialization. Due to the ubiquity of the latter in deep neural network architectures and its flexibility in the creation of feature-rich representations, we argue that this form of evolution of the network's entropy be considered the onset of a deep learning regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07451v1</guid>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Tovey, Sven Krippendorf, Michael Spannowsky, Konstantin Nikolaou, Christian Holm</dc:creator>
    </item>
    <item>
      <title>Methods for Few-View CT Image Reconstruction</title>
      <link>https://arxiv.org/abs/2410.07552</link>
      <description>arXiv:2410.07552v1 Announce Type: cross 
Abstract: Computed Tomography (CT) is an essential non-destructive three dimensional imaging modality used in medicine, security screening, and inspection of manufactured components. Typical CT data acquisition entails the collection of a thousand or more projections through the object under investigation through a range of angles covering one hundred eighty degrees or more. It may be desirable or required that the number of projections angles be reduced by one or two orders of magnitude for reasons such as acquisition time or dose. Unless specialized reconstruction algorithms are applied, reconstructing with fewer views will result in streak artifacts and failure to resolve object boundaries at certain orientations. These artifacts may substantially diminish the usefulness of the reconstructed CT volumes.
  Here we develop constrained and regularized numerical optimization methods to reconstruct CT volumes from 4-28 projections. These methods entail utilization of novel data fidelity and convex and non-convex regularization terms. In addition, the methods outlined here are usually carried out by a sequence of two or three numerical optimization methods in sequence.
  The efficacy of our methods is demonstrated on four measured and three simulated few-view CT data sets. We show that these methods outperform other state of the art few-view numerical optimization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07552v1</guid>
      <category>physics.med-ph</category>
      <category>cs.MS</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle M. Champley, Michael B. Zellner, Joseph W. Tringe, Harry E. Martz Jr</dc:creator>
    </item>
    <item>
      <title>Learning Equivariant Non-Local Electron Density Functionals</title>
      <link>https://arxiv.org/abs/2410.07972</link>
      <description>arXiv:2410.07972v1 Announce Type: cross 
Abstract: The accuracy of density functional theory hinges on the approximation of non-local contributions to the exchange-correlation (XC) functional. To date, machine-learned and human-designed approximations suffer from insufficient accuracy, limited scalability, or dependence on costly reference data. To address these issues, we introduce Equivariant Graph Exchange Correlation (EG-XC), a novel non-local XC functional based on equivariant graph neural networks. EG-XC combines semi-local functionals with a non-local feature density parametrized by an equivariant nuclei-centered point cloud representation of the electron density to capture long-range interactions. By differentiating through a self-consistent field solver, we train EG-XC requiring only energy targets. In our empirical evaluation, we find EG-XC to accurately reconstruct `gold-standard' CCSD(T) energies on MD17. On out-of-distribution conformations of 3BPA, EG-XC reduces the relative MAE by 35% to 50%. Remarkably, EG-XC excels in data efficiency and molecular size extrapolation on QM9, matching force fields trained on 5 times more and larger molecules. On identical training sets, EG-XC yields on average 51% lower MAEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07972v1</guid>
      <category>cs.LG</category>
      <category>physics.chem-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Gao, Eike Eberhard, Stephan G\"unnemann</dc:creator>
    </item>
    <item>
      <title>Differentiability in Unrolled Training of Neural Physics Simulators on Transient Dynamics</title>
      <link>https://arxiv.org/abs/2402.12971</link>
      <description>arXiv:2402.12971v2 Announce Type: replace 
Abstract: Unrolling training trajectories over time strongly influences the inference accuracy of neural network-augmented physics simulators. We analyze this in three variants of training neural time-steppers. In addition to one-step setups and fully differentiable unrolling, we include a third, less widely used variant: unrolling without temporal gradients. Comparing networks trained with these three modalities disentangles the two dominant effects of unrolling, training distribution shift and long-term gradients. We present detailed study across physical systems, network sizes and architectures, training setups, and test scenarios. It also encompasses two simulation modes: In prediction setups, we rely solely on neural networks to compute a trajectory. In contrast, correction setups include a numerical solver that is supported by a neural network. Spanning these variations, our study provides the empirical basis for our main findings: Non-differentiable but unrolled training with a numerical solver in a correction setup can yield substantial improvements over a fully differentiable prediction setup not utilizing this solver. The accuracy of models trained in a fully differentiable setup differs compared to their non-differentiable counterparts. Differentiable ones perform best in a comparison among correction networks as well as among prediction setups. For both, the accuracy of non-differentiable unrolling comes close. Furthermore, we show that these behaviors are invariant to the physical system, the network architecture and size, and the numerical scheme. These results motivate integrating non-differentiable numerical simulators into training setups even if full differentiability is unavailable. We show the convergence rate of common architectures to be low compared to numerical algorithms. This motivates correction setups combining neural and numerical parts which utilize benefits of both.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12971v2</guid>
      <category>physics.comp-ph</category>
      <category>cs.LG</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bjoern List, Li-Wei Chen, Kartik Bali, Nils Thuerey</dc:creator>
    </item>
    <item>
      <title>Neural Operators Learn the Local Physics of Magnetohydrodynamics</title>
      <link>https://arxiv.org/abs/2404.16015</link>
      <description>arXiv:2404.16015v2 Announce Type: replace 
Abstract: Magnetohydrodynamics (MHD) plays a pivotal role in describing the dynamics of plasma and conductive fluids, essential for understanding phenomena such as the structure and evolution of stars and galaxies, and in nuclear fusion for plasma motion through ideal MHD equations. Solving these hyperbolic PDEs requires sophisticated numerical methods, presenting computational challenges due to complex structures and high costs. Recent advances introduce neural operators like the Fourier Neural Operator (FNO) as surrogate models for traditional numerical analyses. This study explores a modified Flux Fourier neural operator model to approximate the numerical flux of ideal MHD, offering a novel approach that outperforms existing neural operator models by enabling continuous inference, generalization outside sampled distributions, and faster computation compared to classical numerical schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16015v2</guid>
      <category>physics.comp-ph</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taeyoung Kim, Youngsoo Ha, Myungjoo Kang</dc:creator>
    </item>
    <item>
      <title>Hyperbolic Machine Learning Moment Closures for the BGK Equations</title>
      <link>https://arxiv.org/abs/2401.04783</link>
      <description>arXiv:2401.04783v2 Announce Type: replace-cross 
Abstract: We introduce a hyperbolic closure for the Grad moment expansion of the Bhatnagar-Gross-Krook's (BGK) kinetic model using a neural network (NN) trained on BGK's moment data. This closure is motivated by the exact closure for the free streaming limit that we derived in our paper on closures in transport \cite{Huang2022-RTE1}. The exact closure relates the gradient of the highest moment to the gradient of four lower moments. As with our past work, the model presented here learns the gradient of the highest moment in terms of the coefficients of gradients for all lower ones. By necessity, this means that the resulting hyperbolic system is not conservative in the highest moment. For stability, the output layers of the NN are designed to enforce hyperbolicity and Galilean invariance. This ensures the model can be run outside of the training window of the NN. Unlike our previous work on radiation transport that dealt with linear models, the BGK model's nonlinearity demanded advanced training tools. These comprised an optimal learning rate discovery, one cycle training, batch normalization in each neural layer, and the use of the \texttt{AdamW} optimizer. To address the non-conservative structure of the hyperbolic model, we adopt the FORCE numerical method to achieve robust solutions. This results in a comprehensive computing model combining learned closures with methods for solving hyperbolic models. The proposed model can capture accurate moment solutions across a broad spectrum of Knudsen numbers. Our paper details the multi-scale model construction and is run on a range of test problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04783v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew J. Christlieb, Mingchang Ding, Juntao Huang, Nicholas A. Krupansky</dc:creator>
    </item>
    <item>
      <title>Stability-Aware Training of Machine Learning Force Fields with Differentiable Boltzmann Estimators</title>
      <link>https://arxiv.org/abs/2402.13984</link>
      <description>arXiv:2402.13984v2 Announce Type: replace-cross 
Abstract: Machine learning force fields (MLFFs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations. However, they can produce unstable simulations, limiting their ability to model phenomena occurring over longer timescales and compromising the quality of estimated observables. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which leverages joint supervision from reference quantum-mechanical calculations and system observables. StABlE Training iteratively runs many MD simulations in parallel to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. We achieve efficient end-to-end automatic differentiation through MD simulations using our Boltzmann Estimator, a generalization of implicit differentiation techniques to a broader class of stochastic algorithms. Unlike existing techniques based on active learning, our approach requires no additional ab-initio energy and forces calculations to correct instabilities. We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, using three modern MLFF architectures. StABlE-trained models achieve significant improvements in simulation stability, data efficiency, and agreement with reference observables. By incorporating observables into the training process alongside first-principles calculations, StABlE Training can be viewed as a general semi-empirical framework applicable across MLFF architectures and systems. This makes it a powerful tool for training stable and accurate MLFFs, particularly in the absence of large reference datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13984v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.chem-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanjeev Raja, Ishan Amin, Fabian Pedregosa, Aditi S. Krishnapriyan</dc:creator>
    </item>
    <item>
      <title>Amoeba Monte Carlo algorithms for random trees with controlled branching activity: efficient trial move generation and universal dynamics</title>
      <link>https://arxiv.org/abs/2406.19547</link>
      <description>arXiv:2406.19547v2 Announce Type: replace-cross 
Abstract: The reptation Monte Carlo algorithm is a simple, physically motivated and efficient method for equilibrating semi-dilute solutions of linear polymers. Here we propose two simple generalizations for the analogue {\it Amoeba} algorithm for randomly branching chains, which allow to efficiently deal with random trees with controlled branching activity. We analyse the rich relaxation dynamics of Amoeba algorithms and demonstrate the existence of an unexpected scaling regime for the tree relaxation. In particular, our results suggests that the equilibration time for Amoeba algorithms scales in general like $N^2 \langle n_{\rm lin}\rangle^\Delta$, where $N$ denotes the number of tree nodes, $\langle n_{\rm lin}\rangle$ the mean number of linear segments the trees are composed of and $\Delta \simeq 0.4$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19547v2</guid>
      <category>cond-mat.soft</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pieter H. W. van der Hoek, Angelo Rosa, Ralf Everaers</dc:creator>
    </item>
    <item>
      <title>GPU Acceleration of Numerical Atomic Orbitals-Based Density Functional Theory Algorithms within the ABACUS package</title>
      <link>https://arxiv.org/abs/2409.09399</link>
      <description>arXiv:2409.09399v3 Announce Type: replace-cross 
Abstract: With the fast developments of high-performance computing, first-principles methods based on quantum mechanics play a significant role in materials research, serving as fundamental tools for predicting and analyzing various properties of materials. However, the inherent complexity and substantial computational demands of first-principles algorithms, such as density functional theory, limit their use in larger systems. The rapid development of heterogeneous computing, particularly General-Purpose Graphics Processing Units (GPGPUs), has heralded new prospects for enhancing the performance and cost-effectiveness of first-principles algorithms. We utilize GPGPUs to accelerate the electronic structure algorithms in Atomic-orbital Based Ab-initio Computation at USTC (ABACUS), a first-principles computational package based on the linear combination of atomic orbitals (LCAO) basis set. We design algorithms on GPGPU to efficiently construct and diagonalize the Hamiltonian of a given system, including the related force and stress calculations. The effectiveness of this computational acceleration has been demonstrated through calculations on twisted bilayer graphene with the system size up to 10,444 atoms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09399v3</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haochong Zhang, Zichao Deng, Yu Liu, Tao Liu, Mohan Chen, Shi Yin, Lixin He</dc:creator>
    </item>
    <item>
      <title>Quantum many-body solver using artificial neural networks and its applications to strongly correlated electron systems</title>
      <link>https://arxiv.org/abs/2410.02633</link>
      <description>arXiv:2410.02633v2 Announce Type: replace-cross 
Abstract: With the evolution of numerical methods, we are now aiming at not only qualitative understanding but also quantitative prediction and design of quantum many-body phenomena. As a novel numerical approach, machine learning techniques have been introduced in 2017 to analyze quantum many-body problems. Since then, proposed various novel approaches have opened a new era, in which challenging and fundamental problems in physics can be solved by machine learning methods. Especially, quantitative and accurate estimates of material-dependent physical properties of strongly correlated matter have now become realized by combining first-principles calculations with highly accurate quantum many-body solvers developed with the help of machine learning methods. Thus developed quantitative description of electron correlations will constitute a key element of materials science in the next generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02633v2</guid>
      <category>cond-mat.str-el</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.supr-con</category>
      <category>physics.comp-ph</category>
      <category>quant-ph</category>
      <pubDate>Fri, 11 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yusuke Nomura, Masatoshi Imada</dc:creator>
    </item>
  </channel>
</rss>
