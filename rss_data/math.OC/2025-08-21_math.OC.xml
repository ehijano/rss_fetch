<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 04:01:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Basis pursuit by inconsistent alternating projections</title>
      <link>https://arxiv.org/abs/2508.15026</link>
      <description>arXiv:2508.15026v1 Announce Type: new 
Abstract: Basis pursuit is the problem of finding a vector with smallest $\ell_1$-norm among the solutions of a given linear system of equations. It is a famous convex relaxation of what the literature refers to as sparse affine feasibility problem, in which sparse solutions to underdetermined systems are sought. In addition to enjoying convexity, the basis pursuit can be trivially rewritten as a linear program, and thus standard tools in linear programming apply for solving it.
  In turn, we tackle the basis pursuit in its very original shape with a scheme that uses alternating projections in its subproblems. These subproblems are designed to be inconsistent in the sense that they relate to two non-intersecting sets. Quite recently, inconsistency coming from infeasibility has been seen to work in favor of alternating projections and correspondent convergence rates. In our work, this feature is now suitably enforced in a new and numerically competitive method for solving the basis pursuit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15026v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roger Behling, Yunier Bello-Cruz, Luiz-Rafael Santos, Paulo J. S. Silva</dc:creator>
    </item>
    <item>
      <title>Linear Quadratic Regulation for First Order Hyperbolic PDEs</title>
      <link>https://arxiv.org/abs/2508.15028</link>
      <description>arXiv:2508.15028v1 Announce Type: new 
Abstract: We consider transport processes that are modeled by first order hyperbolic partial differential equations. Our goal is to find a full state feedback that makes a given reference profile locally asymptotically stable. To accomplish this we employ Linear Quadratic Regulation (LQR) with finite dimensional patch or point control actuation. We derive the Riccati partial differential equation whose solution is the kernel of the optimal cost. The optimal state feedback is also found. The derivation is accomplished by elementary techniques such as integration by parts and completing the square. We apply this theory to two examples that have appeared in the literature and that were solved by a modification of LQR. The first example deals with a model of a fixed-bed chemical reactor and the second example deals with traffic congestion on a stretch of freeway.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15028v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arthur J. Krener</dc:creator>
    </item>
    <item>
      <title>Laguerre geometry for optimization of gridshell with specified force distribution</title>
      <link>https://arxiv.org/abs/2508.15179</link>
      <description>arXiv:2508.15179v1 Announce Type: new 
Abstract: For the design of gridshells consisting of continuous beams in two directions and quadrilateral faces to cover a large space of architecture, it is important to arrange each row and column of beams along a planar curve and ensure planar faces for constructability and cross-sectional compatibility at joints. It is also important that the gridshell is in equilibrium mainly with axial forces against the design loads; i.e., bending deformation should be avoided. In this study, we first find a continuous shell surface where the principal curvature lines coincide with the principal directions of membrane forces. For this purpose, we utilize the L-isothermic surface, which is a kind of membrane O-surface. Specifically, the generalized Dupin cyclide is used as the reference surface, which has an explicit form of membrane forces with a single arbitrary parameter against normal loads. Various force distributions are obtained as the parameter is adjusted without changing the surface shape. Next, the shell is discretized into a gridshell, and target axial forces are obtained from the section length of the covering region of each node. The axial forces are adjusted by optimizing the cross-sectional radii of the beams with pipe sections to realize the specified target force distribution. Since the Laguerre transformation preserves geometric and static properties of the L-isothermic surface, the force can be adjusted by a simple process without re-optimization to obtain an approximate optimal solution of the gridshell after transformation. The ratio of out-of-plane shear force to the normal load at the node is also evaluated to investigate the effect of deformation on the force distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15179v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kohei Kabaki, Kentaro Hayakawa, Makoto Ohsaki</dc:creator>
    </item>
    <item>
      <title>Differential Stochastic Variational Inequalities with Parametric Optimization</title>
      <link>https://arxiv.org/abs/2508.15241</link>
      <description>arXiv:2508.15241v1 Announce Type: new 
Abstract: The differential stochastic variational inequality with parametric convex optimization (DSVI-O) is an ordinary differential equation whose right-hand side involves a stochastic variational inequality and solutions of several dynamic and random parametric convex optimization problems. We consider that the distribution of the random variable is time-dependent and assume that the involved functions are continuous and the expectation is well-defined. We show that the DSVI-O has a weak solution with integrable and measurable solutions of the parametric optimization problems. Moreover, we propose a discrete scheme of DSVI-O by using a time-stepping approximation and the sample average approximation and prove the convergence of the discrete scheme. We illustrate our theoretical results of DSVI-O with applications in an embodied intelligence system for the elderly health by synthetic health care data generated by Multimodal Large Language Models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15241v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaojun Chen, Jian Guo, Guan Wang</dc:creator>
    </item>
    <item>
      <title>Estimating profitable price bounds for prescriptive price optimization</title>
      <link>https://arxiv.org/abs/2508.15248</link>
      <description>arXiv:2508.15248v1 Announce Type: new 
Abstract: Pricing of products and services, which has a significant impact on consumer demand, is one of the most important factors in maximizing business profits. Prescriptive price optimization is a prominent data-driven pricing methodology consisting of two phases: demand forecasting and price optimization. In the practice of prescriptive price optimization, the price of each item is typically set within a predetermined range defined by lower and upper bounds. Narrow price ranges can lead to missed opportunities, while wide price ranges run the risk of proposing unrealistic prices; therefore, determining profitable price bounds while maintaining the reliability of the suggested prices is a critical challenge that directly affects the effectiveness of prescriptive price optimization. We propose two methods for estimating price bounds in prescriptive price optimization so that future total revenue derived from the optimized prices will be maximized. Our first method for price bounds estimation uses the bootstrap procedure to estimate confidence intervals for optimal prices. Our second method uses the Nelder--Mead simplex method for black-box price bounds optimization that maximizes total revenue estimated through $K$-fold cross-validation. Experimental results with synthetic price--demand datasets demonstrate that our methods successfully narrowed down the price range while maintaining high revenues, particularly when the number of items was small or the demand noise level was low. Moreover, as more data accumulated, the comparative advantage of our methods further increased.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15248v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masato Inokuma, Shunnosuke Ikeda, Yuichi Takano</dc:creator>
    </item>
    <item>
      <title>Integrated Take-off Management and Trajectory Optimization for Merging Control in Urban Air Mobility Corridors</title>
      <link>https://arxiv.org/abs/2508.15395</link>
      <description>arXiv:2508.15395v1 Announce Type: new 
Abstract: Urban Air Mobility (UAM) has the potential to revolutionize daily transportation, offering rapid and efficient aerial mobility services. Take-off and merging phases are critical for air corridor operations, requiring the coordination of take-off aircraft and corridor traffic while ensuring safety and seamless transition. This paper proposes an integrated take-off management and trajectory optimization for merging control in UAM corridors. We first introduce a novel take-off airspace design. To our knowledge, this paper is one of the first to propose a structured design for take-off airspace. Based on the take-off airspace design, we devise a hierarchical coordinated take-off and merging management (HCTMM) strategy. To be specific, the take-off airspace design can simplify aircraft dynamics and thus reduce the dimensionality of the trajectory optimization problem whilst mitigating obstacle avoidance complexities. The HCTMM strategy strictly ensures safety and improves the efficiency of take-off and merging operations. At the tactical level, a scheduling algorithm coordinates aircraft take-off times and selects dynamic merging points to reduce conflicts and ensure smooth take-off and merging processes. At the operational level, a trajectory optimization strategy ensures that each aircraft reaches the dynamic merging point efficiently while satisfying safety constraints. Simulation results show that, compared to representative strategies with fixed or dynamic merging points, the HCTMM strategy significantly improves operational efficiency and reduces computational burden, while ensuring safety under various corridor traffic conditions. Further results confirm the scalability of the HCTMM strategy and the computational efficiency enabled by the proposed take-off airspace design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15395v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingqi Liu, Tianlu Pan, Jingjun Tan, Renxin Zhong, Can Chen</dc:creator>
    </item>
    <item>
      <title>A smoothed proximal trust-region algorithm for nonconvex optimization problems with $L^p$-regularization, $p\in (0,1)$</title>
      <link>https://arxiv.org/abs/2508.15446</link>
      <description>arXiv:2508.15446v1 Announce Type: new 
Abstract: We investigate a trust-region algorithm to solve a nonconvex optimization problem with $L^p$-regularization for $p\in(0,1)$. The algorithm relies on descent properties of a so-called generalized Cauchy point that can be obtained efficiently by a line search along a suitable proximal path. To handle the nonconvexity and nonsmoothness of the $L^p$-pseudonorm, we replace it by a smooth approximation and construct a convex upper bound of that approximation. This enables us to use results of a trust-region method for composite problems with a convex nonsmooth term. We prove convergence properties of the resulting smoothed proximal trust-region algorithm and investigate its performance in some numerical examples. Furthermore, approximate subproblem solvers for the arising trust-region subproblems are considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15446v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harbir Antil, Anna Lentz</dc:creator>
    </item>
    <item>
      <title>Control-Based Online Distributed Optimization</title>
      <link>https://arxiv.org/abs/2508.15498</link>
      <description>arXiv:2508.15498v1 Announce Type: new 
Abstract: In this paper we design a novel class of online distributed optimization algorithms leveraging control theoretical techniques. We start by focusing on quadratic costs, and assuming to know an internal model of their variation. In this set-up, we formulate the algorithm design as a robust control problem, showing that it yields a fully distributed algorithm. We also provide a distributed routine to acquire the internal model. We show that the algorithm converges exactly to the sequence of optimal solutions. We empirically evaluate the performance of the algorithm for different choices of parameters. Additionally, we evaluate the performance of the algorithm for quadratic problems with inexact internal model and non-quadratic problems, and show that it outperforms alternative algorithms in both scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15498v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wouter J. A. van Weerelt, Nicola Bastianello</dc:creator>
    </item>
    <item>
      <title>Controlled Optimization of Quadratic Functions in $\mathbb{R}^n$</title>
      <link>https://arxiv.org/abs/2508.15515</link>
      <description>arXiv:2508.15515v1 Announce Type: new 
Abstract: In this work, we introduce and study the controllability of the trajectories of a linear dynamical system, which can be used to solve the minimization of a quadratic function in finite dimension. We named this dynamical system the controlled quadratic gradient flow. Finally, we introduce what we call the controlled quadratic gradient descent and the controlled proximity operator which are respectively the Euler explicit and implicit discretization of the controlled gradient flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15515v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Jacques Godeme</dc:creator>
    </item>
    <item>
      <title>Finding a Maximal Determinant Principal Submatrix via Hadamard's Inequality and Conic Relaxations</title>
      <link>https://arxiv.org/abs/2508.15608</link>
      <description>arXiv:2508.15608v1 Announce Type: new 
Abstract: An important yet challenging problem in numerical linear algebra is finding a principal submatrix with the maximum determinant. In this paper, we examine several exact and approximate approaches to this problem. We first propose an upper bound based on Hadamard's inequality, along with a projection scheme based on the Gram-Schmidt process without normalization. This scheme yields a highly effective exact algorithm for solving small- to medium-scale instances. We then study a linear programming (LP) relaxation that facilitates reliable performance evaluation when the exact method returns only near-optimal solutions, and prove that our projection scheme also strengthens the upper bound obtained from the LP relaxation. Finally, we present stronger upper bounds via semidefinite programming, further illustrating the intrinsic difficulty of determinant maximization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15608v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Hu, Stefan Sremac, Hugo J. Woerdeman, Henry Wolkowicz</dc:creator>
    </item>
    <item>
      <title>Approximation of risk-averse optimal feedback control</title>
      <link>https://arxiv.org/abs/2508.15618</link>
      <description>arXiv:2508.15618v1 Announce Type: new 
Abstract: The challenge of constructing feedback control laws for risk-averse optimal control of partial differential equations (PDEs) with random coefficients is addressed. The control objective composes a tracking-type cost with the nonlinear entropic risk measure. A sequential quadratic programming scheme is derived that iteratively solves linear quadratic subproblems obtained through second-order Taylor expansions of the objective functional, with each subproblem re-centered at the previous iterate. It is shown that this method converges locally quadratically to the unique risk-averse optimal control. This work provides the first rigorous feedback synthesis for risk-averse objectives subject to PDEs with random coefficients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15618v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp A. Guth, Karl Kunisch</dc:creator>
    </item>
    <item>
      <title>Lower Bounds on the Haraux Function</title>
      <link>https://arxiv.org/abs/2508.15735</link>
      <description>arXiv:2508.15735v1 Announce Type: new 
Abstract: The Haraux function is an important tool in monotone operator theory and its applications. One of its salient properties for maximally monotone operators is to be valued in $[0,+\infty]$ and to vanish only on the graph of the operator. Sharper lower bounds for this function were recently proposed in specific scenarios. We derive lower bounds in the general context of set-valued operators in reflexive Banach spaces. These bounds are new, even for maximally monotone operators acting on Euclidean spaces, a context in which we show that they can be better than existing ones. As a by-product, we obtain lower bounds for the Fenchel--Young function in variational analysis. Several examples are given and applications to composite monotone inclusions are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15735v1</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick L. Combettes, Julien N. Mayrand</dc:creator>
    </item>
    <item>
      <title>Non-overlapping, Schwarz-type Domain Decomposition Method for Physics and Equality Constrained Artificial Neural Networks</title>
      <link>https://arxiv.org/abs/2409.13644</link>
      <description>arXiv:2409.13644v2 Announce Type: cross 
Abstract: We present a non-overlapping, Schwarz-type domain decomposition method with a generalized interface condition, designed for physics-informed machine learning of partial differential equations (PDEs) in both forward and inverse contexts. Our approach employs physics and equality-constrained artificial neural networks (PECANN) within each subdomain. Unlike the original PECANN method, which relies solely on initial and boundary conditions to constrain PDEs, our method uses both boundary conditions and the governing PDE to constrain a unique interface loss function for each subdomain. This modification improves the learning of subdomain-specific interface parameters while reducing communication overhead by delaying information exchange between neighboring subdomains. To address the constrained optimization in each subdomain, we apply an augmented Lagrangian method with a conditionally adaptive update strategy, transforming the problem into an unconstrained dual optimization. A distinct advantage of our domain decomposition method is its ability to learn solutions to both Poisson's and Helmholtz equations, even in cases with high-wavenumber and complex-valued solutions. Through numerical experiments with up to 64 subdomains, we demonstrate that our method consistently generalizes well as the number of subdomains increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13644v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qifeng Hu, Shamsulhaq Basir, Inanc Senocak</dc:creator>
    </item>
    <item>
      <title>Generative Neural Operators of Log-Complexity Can Simultaneously Solve Infinitely Many Convex Programs</title>
      <link>https://arxiv.org/abs/2508.14995</link>
      <description>arXiv:2508.14995v1 Announce Type: cross 
Abstract: Neural operators (NOs) are a class of deep learning models designed to simultaneously solve infinitely many related problems by casting them into an infinite-dimensional space, whereon these NOs operate. A significant gap remains between theory and practice: worst-case parameter bounds from universal approximation theorems suggest that NOs may require an unrealistically large number of parameters to solve most operator learning problems, which stands in direct opposition to a slew of experimental evidence. This paper closes that gap for a specific class of {NOs}, generative {equilibrium operators} (GEOs), using (realistic) finite-dimensional deep equilibrium layers, when solving families of convex optimization problems over a separable Hilbert space $X$. Here, the inputs are smooth, convex loss functions on $X$, and outputs are the associated (approximate) solutions to the optimization problem defined by each input loss.
  We show that when the input losses lie in suitable infinite-dimensional compact sets, our GEO can uniformly approximate the corresponding solutions to arbitrary precision, with rank, depth, and width growing only logarithmically in the reciprocal of the approximation error. We then validate both our theoretical results and the trainability of GEOs on three applications: (1) nonlinear PDEs, (2) stochastic optimal control problems, and (3) hedging problems in mathematical finance under liquidity constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14995v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>q-fin.CP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasis Kratsios, Ariel Neufeld, Philipp Schmocker</dc:creator>
    </item>
    <item>
      <title>Analysis of mean field games via Fokker-Planck-Kolmogorov equations: existence of equilibria</title>
      <link>https://arxiv.org/abs/2508.15029</link>
      <description>arXiv:2508.15029v1 Announce Type: cross 
Abstract: We study mean field games with unbounded coefficients. The existence of a solution is proved. We propose a new approach based on Fokker-Planck-Kolmogorov equations, the Ambrosio-Figalli-Trevisan superposition principle and a priory estimates with Lyapunov functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15029v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanislav V. Shaposhnikov, Dmitry V. Shatilovich</dc:creator>
    </item>
    <item>
      <title>Enhancing Optimizer Stability: Momentum Adaptation of The NGN Step-size</title>
      <link>https://arxiv.org/abs/2508.15071</link>
      <description>arXiv:2508.15071v1 Announce Type: cross 
Abstract: Modern optimization algorithms that incorporate momentum and adaptive step-size offer improved performance in numerous challenging deep learning tasks. However, their effectiveness is often highly sensitive to the choice of hyperparameters, especially the step-size. Tuning these parameters is often difficult, resource-intensive, and time-consuming. Therefore, recent efforts have been directed toward enhancing the stability of optimizers across a wide range of hyperparameter choices [Schaipp et al., 2024]. In this paper, we introduce an algorithm that matches the performance of state-of-the-art optimizers while improving stability to the choice of the step-size hyperparameter through a novel adaptation of the NGN step-size method [Orvieto and Xiao, 2024]. Specifically, we propose a momentum-based version (NGN-M) that attains the standard convergence rate of $\mathcal{O}(1/\sqrt{K})$ under less restrictive assumptions, without the need for interpolation condition or assumptions of bounded stochastic gradients or iterates, in contrast to previous approaches. Additionally, we empirically demonstrate that the combination of the NGN step-size with momentum results in enhanced robustness to the choice of the step-size hyperparameter while delivering performance that is comparable to or surpasses other state-of-the-art optimizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15071v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rustem Islamov, Niccolo Ajroldi, Antonio Orvieto, Aurelien Lucchi</dc:creator>
    </item>
    <item>
      <title>Jointly Computation- and Communication-Efficient Distributed Learning</title>
      <link>https://arxiv.org/abs/2508.15509</link>
      <description>arXiv:2508.15509v1 Announce Type: cross 
Abstract: We address distributed learning problems over undirected networks. Specifically, we focus on designing a novel ADMM-based algorithm that is jointly computation- and communication-efficient. Our design guarantees computational efficiency by allowing agents to use stochastic gradients during local training. Moreover, communication efficiency is achieved as follows: i) the agents perform multiple training epochs between communication rounds, and ii) compressed transmissions are used. We prove exact linear convergence of the algorithm in the strongly convex setting. We corroborate our theoretical results by numerical comparisons with state of the art techniques on a classification task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15509v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoxing Ren, Nicola Bastianello, Karl H. Johansson, Thomas Parisini</dc:creator>
    </item>
    <item>
      <title>Integral bases, perfect matchings, and the Petersen graph</title>
      <link>https://arxiv.org/abs/2508.15602</link>
      <description>arXiv:2508.15602v1 Announce Type: cross 
Abstract: Let $G=(V,E)$ be a matching-covered graph, denote by $P$ its perfect matching polytope, and by $L$ the integer lattice generated by the integral points in $P$. In this paper, we give polyhedral proofs for two difficult results established by Lov\'{a}sz (1987), and by Carvalho, Lucchesi, and Murty (2002) in a series of three papers. More specifically, we reprove that $L$ has a lattice basis consisting solely of incidence vectors of some perfect matchings of $G$, $2x\in L$ for all $x\in \mathrm{lin}(P)\cap \mathbb{Z}^E$, and if $G$ has no Petersen brick then $L = \mathrm{lin}(P)\cap \mathbb{Z}^E$. This is achieved by studying the facial structure of $P$ and its relationship with the lattice $L$. Along the way, we give a new polyhedral characterization of the Petersen graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15602v1</guid>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ahmad Abdi, Olha Silina</dc:creator>
    </item>
    <item>
      <title>Mirror Descent for Stochastic Control Problems with Measure-valued Controls</title>
      <link>https://arxiv.org/abs/2401.01198</link>
      <description>arXiv:2401.01198v2 Announce Type: replace 
Abstract: This paper studies the convergence of the mirror descent algorithm for finite horizon stochastic control problems with measure-valued control processes. The control objective involves a convex regularisation function, denoted as $h$, with regularisation strength determined by the weight $\tau\ge 0$. The setting covers regularised relaxed control problems. Under suitable conditions, we establish the relative smoothness and convexity of the control objective with respect to the Bregman divergence of $h$, and prove linear convergence of the algorithm for $\tau=0$ and exponential convergence for $\tau&gt;0$. The results apply to common regularisers including relative entropy, $\chi^2$-divergence, and entropic Wasserstein costs. This validates recent reinforcement learning heuristics that adding regularisation accelerates the convergence of gradient methods. The proof exploits careful regularity estimates of backward stochastic differential equations in the bounded mean oscillation norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01198v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bekzhan Kerimkulov, David \v{S}i\v{s}ka, {\L}ukasz Szpruch, Yufei Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal consumption under relaxed benchmark tracking and consumption drawdown constraint</title>
      <link>https://arxiv.org/abs/2410.16611</link>
      <description>arXiv:2410.16611v3 Announce Type: replace 
Abstract: This paper studies an optimal consumption problem with both relaxed benchmark tracking and consumption drawdown constraint, leading to a stochastic control problem with dynamic state-control constraints. In our relaxed tracking formulation, it is assumed that the fund manager can strategically inject capital to the fund account such that the total capital process always outperforms the benchmark process, which is described by a geometric Brownian motion. We first transform the original regular-singular control problem with state-control constraints into an equivalent regular control problem with a reflected state process and consumption drawdown constraint. By utilizing the dual transform and the optimal consumption behavior, we then turn to study the linear dual PDE with both Neumann boundary condition and free boundary condition in a piecewise manner across different regions. Using the smoothfit principle and the super-contact condition, we derive the closed-form solution of the dual PDE, and obtain the optimal investment and consumption in feedback form. We then prove the verification theorem on optimality by some novel arguments with the aid of an auxiliary reflected dual process and some technical estimations. Some numerical examples and financial insights are also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16611v3</guid>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lijun Bo, Yijie Huang, Kaixin Yan, Xiang Yu</dc:creator>
    </item>
    <item>
      <title>Learning Large Neighborhood Search for Maritime Inventory Routing Optimization</title>
      <link>https://arxiv.org/abs/2502.15244</link>
      <description>arXiv:2502.15244v2 Announce Type: replace 
Abstract: Maritime inventory routing optimization is an important yet challenging combinatorial optimization problem. We propose a machine learning-based local search approach for finding feasible solutions of large-scale maritime inventory routing optimization problems. Given the combinatorial complexity of the problems, we integrate a graph neural network-based neighborhood selection method to enhance local search efficiency. Our approach enables a structured exploration of different neighborhoods by imitating an optimization-based expert neighborhood selection policy, improving solution quality while maintaining computational efficiency. Through extensive computational experiments on realistic instances, we demonstrate that our method outperforms direct mixed-integer programming as well as benchmark local search approaches in solution time and solution quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15244v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rui Chen, Defeng Liu, Nan Jiang, Rishabh Gupta, Mustafa Kilinc, Andrea Lodi</dc:creator>
    </item>
    <item>
      <title>A strongly polynomial-time algorithm for the general linear programming problem</title>
      <link>https://arxiv.org/abs/2503.12041</link>
      <description>arXiv:2503.12041v5 Announce Type: replace 
Abstract: This article presents a strongly polynomial-time algorithm for the general linear programming problem. This algorithm is an implicit reduction procedure that works as follows. Primal and dual problems are combined into a special system of linear equations constrained by complementarity relations and non-negative variables. Each iteration of the algorithm consists of applying a pair of complementary Gauss-Jordan pivoting operations, guided by a necessary-condition lemma. The algorithm requires no more than k+n iterations, as there are only k+n complementary pairs of columns to compare one-pair-at-a-time, where k is the number of constraints and n is the number of variables of given general linear programming problem. Numerical illustration is given that includes an instance of a classical problem of Klee and Minty and a problem of Beale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12041v5</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Awoniyi</dc:creator>
    </item>
    <item>
      <title>Polytope Volume Monitoring Problem: Formulation and Solution via Parametric Linear Program Based Control Barrier Function</title>
      <link>https://arxiv.org/abs/2503.12546</link>
      <description>arXiv:2503.12546v3 Announce Type: replace 
Abstract: Motivated by the latest research on feasible space monitoring of multiple control barrier functions (CBFs) as well as polytopic collision avoidance, this paper studies the Polytope Volume Monitoring (PVM) problem, whose goal is to design a control law for inputs of nonlinear systems to prevent the volume of some state-dependent polytope from decreasing to zero. Recent studies have explored the idea of applying Chebyshev ball method in optimization theory to solve the case study of PVM; however, the underlying difficulties caused by nonsmoothness have not been addressed. This paper continues the study on this topic, where our main contribution is to establish the relationship between nonsmooth CBF and parametric optimization theory through directional derivatives for the first time, to solve PVM problems more conveniently. In detail, inspired by Chebyshev ball approach, a parametric linear program (PLP) based nonsmooth barrier function candidate is established for PVM, and then, sufficient conditions for it to be a nonsmooth CBF are proposed, based on which a quadratic program (QP) based safety filter with guaranteed feasibility is proposed to address PVM problems. Finally, a numerical simulation example is given to show the efficiency of the proposed safety filter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12546v3</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shizhen Wu, Jinyang Dong, Xu Fang, Ning Sun, Yongchun Fang</dc:creator>
    </item>
    <item>
      <title>A Two-Stage Stochastic Model for Road-Rail Intermodal Freight Transportation Under Demand and Capacity Uncertainty</title>
      <link>https://arxiv.org/abs/2503.17510</link>
      <description>arXiv:2503.17510v2 Announce Type: replace 
Abstract: With the steady increase in global logistics and freight transport demand, the need for efficient and sustainable intermodal transport systems becomes increasingly important. This study addresses the optimization of container movement by intermodal transport with fixed train schedules. We emphasize the integration of road-rail intermodal transport amid uncertain demand and train (spot) capacities. A two-stage stochastic optimization model is developed to strategically manage the transportation of containers from multiple origins to designated intermodal hubs. By leveraging spot capacities at train stations and addressing uncertainties in demand and train capacity, the model integrates Conditional Value-at-Risk (CVaR) to balance cost efficiency and risk management, enabling robust decision-making under uncertainty. The model's objectives encompass minimizing transportation costs, mitigating carbon emissions, and enhancing the reliability of containerized freight movement across the network. A comprehensive case study using real-world data demonstrates the practical applicability of the model, highlighting its effectiveness in reducing operational costs, minimizing environmental impacts, and providing actionable insights for stakeholders to navigate the trade-offs between expected costs and risk management in dynamic intermodal transport settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17510v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremiah Gbadegoye, Mustafa C. Camur, Xueping Li</dc:creator>
    </item>
    <item>
      <title>Online Convex Optimization and Integral Quadratic Constraints: An automated approach to regret analysis</title>
      <link>https://arxiv.org/abs/2503.23600</link>
      <description>arXiv:2503.23600v3 Announce Type: replace 
Abstract: We propose a novel approach for analyzing dynamic regret of first-order constrained online convex optimization algorithms for strongly convex and Lipschitz-smooth objectives. Crucially, we provide a general analysis that is applicable to a wide range of first-order algorithms that can be expressed as an interconnection of a linear dynamical system in feedback with a first-order oracle. By leveraging Integral Quadratic Constraints (IQCs), we derive a semi-definite program which, when feasible, provides a regret guarantee for the online algorithm. For this, the concept of variational IQCs is introduced as the generalization of IQCs to time-varying monotone operators. Our bounds capture the temporal rate of change of the problem in the form of the path length of the time-varying minimizer and the objective function variation. In contrast to standard results in OCO, our results do not require nerither the assumption of gradient boundedness, nor that of a bounded feasible set. Numerical analyses showcase the ability of the approach to capture the dependence of the regret on the function class condition number.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23600v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian Jakob, Andrea Iannelli</dc:creator>
    </item>
    <item>
      <title>Linear time-and-space-invariant relaxation systems</title>
      <link>https://arxiv.org/abs/2504.06009</link>
      <description>arXiv:2504.06009v2 Announce Type: replace 
Abstract: This paper generalizes the physical property of relaxation from linear time-invariant (LTI) to linear time-and-space-invariant (LTSI) systems. It is shown that the defining features of relaxation -- complete monotonicity, passivity, and memory-based storage -- carry over seamlessly to the spatio-temporal domain. An LTSI system is shown to be of relaxation type if and only if its associated spatio-temporal Hankel operator is cyclically monotone. This implies the existence of an intrinsic quadratic storage functional defined uniquely by past inputs, independently of any state-space realization. As in the LTI case, LTSI relaxation systems are shown to be those systems for which the state-space concept of storage coincides with the input-output concept of fading memory functional.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06009v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tihol Ivanov Donchev, Brayan M. Shali, Rodolphe Sepulchre</dc:creator>
    </item>
    <item>
      <title>Source identification via pathwise gradient estimation</title>
      <link>https://arxiv.org/abs/2505.18205</link>
      <description>arXiv:2505.18205v2 Announce Type: replace 
Abstract: In the context of PDE-constrained optimization theory, source identification problems traditionally entail particles emerging from an unknown source distribution inside a domain, moving according to a prescribed stochastic process, e.g.~Brownian motion, and then exiting through the boundary of a compact domain. Given information about the flux of particles through the boundary of the domain, the challenge is to infer as much as possible about the source.
  In the PDE setting, it is usually assumed that the flux can be observed without error and at all points on the boundary. Here we consider a different, more statistical presentation of the problem, in which the data has the form of discrete counts of particles arriving at a set of disjoint detectors whose union is a strict subset of the boundary. In keeping with the primacy of the stochastic processes in the generation of the model, we present a stochastic gradient descent algorithm in which exit rates and parameter sensitivities are computed by simulations of particle paths. We present examples for both It\^o diffusion and piecewise-deterministic Markov processes, noting that the form of the sensitivities depends only on the parameterization of the source distribution and is universal among a large class of Markov processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18205v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard B. Lehoucq, Scott A. McKinley, Petr Plech\'a\v{c}</dc:creator>
    </item>
    <item>
      <title>Mean-Field Langevin Diffusions with Density-dependent Temperature</title>
      <link>https://arxiv.org/abs/2507.20958</link>
      <description>arXiv:2507.20958v2 Announce Type: replace 
Abstract: In the context of non-convex optimization, we let the temperature of a Langevin diffusion to depend on the diffusion's own density function. The rationale is that the induced density captures to some extent the landscape imposed by the non-convex function to be minimized, such that a density-dependent temperature provides location-wise random perturbation that may better react to, for instance, the location and depth of local minimizers. As the Langevin dynamics is now self-regulated by its own density, it forms a mean-field stochastic differential equation (SDE) of the Nemytskii type, distinct from the standard McKean-Vlasov equations. Relying on Wasserstein subdifferential calculus, we first show that the corresponding (nonlinear) Fokker-Planck equation has a unique solution. Next, a weak solution to the SDE is constructed from the solution to the Fokker-Planck equation, by Trevisan's superposition principle. As time goes to infinity, we further show that the induced density converges to an invariant distribution, which admits an explicit formula in terms of the Lambert $W$ function. A numerical example suggests that the density-dependent temperature can simultaneously improve the accuracy of and rate of convergence to the estimate of global minimizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20958v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu-Jui Huang, Zachariah Malik</dc:creator>
    </item>
    <item>
      <title>Federated Learning on Riemannian Manifolds: A Gradient-Free Projection-Based Approach</title>
      <link>https://arxiv.org/abs/2507.22855</link>
      <description>arXiv:2507.22855v2 Announce Type: replace 
Abstract: Federated learning (FL) has emerged as a powerful paradigm for collaborative model training across distributed clients while preserving data privacy. However, existing FL algorithms predominantly focus on unconstrained optimization problems with exact gradient information, limiting its applicability in scenarios where only noisy function evaluations are accessible or where model parameters are constrained. To address these challenges, we propose a novel zeroth-order projection-based algorithm on Riemannian manifolds for FL. By leveraging the projection operator, we introduce a computationally efficient zeroth-order Riemannian gradient estimator. Unlike existing estimators, ours requires only a simple Euclidean random perturbation, eliminating the need to sample random vectors in the tangent space, thus reducing computational cost. Theoretically, we first prove the approximation properties of the estimator and then establish the sublinear convergence of the proposed algorithm, matching the rate of its first-order counterpart. Numerically, we first assess the efficiency of our estimator using kernel principal component analysis. Furthermore, we apply the proposed algorithm to two real-world scenarios: zeroth-order attacks on deep neural networks and low-rank neural network training to validate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22855v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongye Wang, Zhaoye Pan, Chang He, Jiaxiang Li, Bo Jiang</dc:creator>
    </item>
    <item>
      <title>Augmentation Algorithms for Integer Programs with Total Variation-like Regularization</title>
      <link>https://arxiv.org/abs/2508.05822</link>
      <description>arXiv:2508.05822v2 Announce Type: replace 
Abstract: We address a class of integer optimization programs with a total variation-like regularizer and convex, separable constraints on a graph. Our approach makes use of the Graver basis, an optimality certificate for integer programs, which we characterize as corresponding to the collection of induced connected subgraphs of our graph. We demonstrate how to use this basis to craft an exact global optimization algorithm for the unconstrained problem recovering a method first shown by Kolmogorov and Shioura in 2009. We then address the problem with an additional budget constraint with a randomized heuristic algorithm that samples improving moves from the Graver basis in a randomized variant of the simplex algorithm. Through comprehensive experiments, we demonstrate that this randomized algorithm is competitive with and often outperforms state-of-the-art integer program solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05822v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominic Yang, Sven Leyffer, Miles Bakenhus</dc:creator>
    </item>
    <item>
      <title>The Optimal Smoothings of Sublinear Functions and Convex Cones</title>
      <link>https://arxiv.org/abs/2508.06681</link>
      <description>arXiv:2508.06681v2 Announce Type: replace 
Abstract: This paper considers the problem of smoothing convex functions and sets, seeking the nearest smooth convex function or set to a given one. For convex cones and sublinear functions, a full characterization of the set of all optimal smoothings is given. These provide if and only if characterizations of the set of optimal smoothings for any target level of smoothness. Optimal smoothings restricting to either inner or outer approximations also follow from our theory. Finally, we apply our theory to provide insights into smoothing amenable functions given by compositions with sublinear functions and generic convex sets by expressing them as conic sections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06681v2</guid>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thabo Samakhoana, Benjamin Grimmer</dc:creator>
    </item>
    <item>
      <title>On Connections Between Association Schemes and Analyses of Polyhedral and Positive Semidefinite Lift-and-Project Relaxations</title>
      <link>https://arxiv.org/abs/2008.08628</link>
      <description>arXiv:2008.08628v4 Announce Type: replace-cross 
Abstract: We explore some connections between association schemes and the analyses of the semidefinite programming (SDP) based convex relaxations of combinatorial optimization problems in the Lov\'{a}sz--Schrijver lift-and-project hierarchy. Our analysis of the relaxations of the stable set polytope leads to bounds on the clique and stability numbers of some regular graphs reminiscent of classical bounds by Delsarte and Hoffman, as well as the notion of deeply vertex-transitive graphs -- highly symmetric graphs that we show arise naturally from some association schemes. We also study relaxations of the hypergraph matching problem, and determine exactly or provide bounds on the lift-and-project ranks of these relaxations. Our proofs for these results also inspire the study of a homogeneous coherent configuration based on hypermatchings, which is an association scheme except it is generally non-commutative. We then illustrate the usefulness of obtaining commutative subschemes from non-commutative homogeneous coherent configurations via contraction in this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.08628v4</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Hin Au, Nathan Lindzey, Levent Tun\c{c}el</dc:creator>
    </item>
    <item>
      <title>Wasserstein Distributionally Robust Shallow Convex Neural Networks</title>
      <link>https://arxiv.org/abs/2407.16800</link>
      <description>arXiv:2407.16800v3 Announce Type: replace-cross 
Abstract: In this work, we propose Wasserstein distributionally robust shallow convex neural networks (WaDiRo-SCNNs) to provide reliable nonlinear predictions when subject to adverse and corrupted datasets. Our approach is based on the reformulation of a new convex training program for ReLU-based shallow neural networks, which allows us to cast the problem into the order-1 Wasserstein distributionally robust optimization framework. Our training procedure is conservative, has low stochasticity, is solvable with open-source solvers, and is scalable to large industrial deployments. We provide out-of-sample performance guarantees, show that hard convex physical constraints can be enforced in the training program, and propose a mixed-integer convex post-training verification program to evaluate model stability. WaDiRo-SCNN aims to make neural networks safer for critical applications, such as in the energy sector. Finally, we numerically demonstrate our model's performance through both a synthetic experiment and a real-world power system application, viz., the prediction of hourly energy consumption in non-residential buildings within the context of virtual power plants, and evaluate its stability across standard regression benchmark datasets. The experimental results are convincing and showcase the strengths of the proposed model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16800v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Pallage, Antoine Lesage-Landry</dc:creator>
    </item>
    <item>
      <title>Faster Convergence of Riemannian Stochastic Gradient Descent with Increasing Batch Size</title>
      <link>https://arxiv.org/abs/2501.18164</link>
      <description>arXiv:2501.18164v3 Announce Type: replace-cross 
Abstract: We have theoretically analyzed the use of Riemannian stochastic gradient descent (RSGD) and found that using an increasing batch size leads to faster RSGD convergence rate than using a constant batch size not only with a constant learning rate but also with a decaying learning rate, such as cosine annealing decay and polynomial decay. The convergence rate of RSGD improves from $O(\sqrt{T^{-1}+\text{const.}})$ with a constant batch size to $O(T^{-\frac{1}{2}})$ with an increasing batch size, where $T$ denotes the number of iterations. Using principal component analysis and low-rank matrix completion tasks, we investigated, both theoretically and numerically, how increasing batch size affects computational time as measured by stochastic first-order oracle (SFO) complexity. Increasing batch size reduces the SFO complexity of RSGD. Furthermore, our numerical results demonstrated that increasing batch size offers the advantages of both small and large constant batch sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18164v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kanata Oowada, Hideaki Iiduka</dc:creator>
    </item>
    <item>
      <title>Deceptive Sequential Decision-Making via Regularized Policy Optimization</title>
      <link>https://arxiv.org/abs/2501.18803</link>
      <description>arXiv:2501.18803v2 Announce Type: replace-cross 
Abstract: Autonomous systems are increasingly expected to operate in the presence of adversaries, though adversaries may infer sensitive information simply by observing a system. Therefore, present a deceptive sequential decision-making framework that not only conceals sensitive information, but actively misleads adversaries about it. We model autonomous systems as Markov decision processes, with adversaries using inverse reinforcement learning to recover reward functions. To counter them, we present three regularization strategies for policy synthesis problems that actively deceive an adversary about a system's reward. ``Diversionary deception'' leads an adversary to draw any false conclusion about the system's reward function. ``Targeted deception'' leads an adversary to draw a specific false conclusion about the system's reward function. ``Equivocal deception'' leads an adversary to infer that the real reward and a false reward both explain the system's behavior. We show how each form of deception can be implemented in policy optimization problems and analytically bound the loss in total accumulated reward induced by deception. Next, we evaluate these developments in a multi-agent setting. We show that diversionary, targeted, and equivocal deception all steer the adversary to false beliefs while still attaining a total accumulated reward that is at least 97% of its optimal, non-deceptive value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18803v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yerin Kim, Alexander Benvenuti, Bo Chen, Mustafa Karabag, Abhishek Kulkarni, Nathaniel D. Bastian, Ufuk Topcu, Matthew Hale</dc:creator>
    </item>
  </channel>
</rss>
