<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 May 2024 04:00:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>New Approaches to Old Problems? Thinking About a New Design of the AML/CFT Strategy</title>
      <link>https://arxiv.org/abs/2405.18517</link>
      <description>arXiv:2405.18517v1 Announce Type: cross 
Abstract: The entry of new technological infrastructures into the financial markets poses serious concerns about the misuse of the economic system for illicit purposes, such as money laundering and financing of terrorism. Although there are cases in which this connection has already been discovered by malicious actors, distributed ledger technologies can nevertheless represent a powerful tool at the disposal of competent authorities to trace illicit flows and to better monitor risks in financial markets. However, this possibility may go through an interdisciplinary analysis of the phenomena. The search for alternative systems to move funds, rather than the traditional financial intermediaries, such as banks, is not a new circumstance and not necessarily for criminal purposes. Nevertheless, some of the already-known value transfer systems may benefit from the use of distributed ledger technology and make their detection more difficult. The European institutions are discussing the needed legislative packages to enforce the current regulations and to extend their application to the crypto space, as well as the establishment of a new competent authority.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18517v1</guid>
      <category>econ.TH</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Ferri</dc:creator>
    </item>
    <item>
      <title>Biclustering a dataset using photonic quantum computing</title>
      <link>https://arxiv.org/abs/2405.18622</link>
      <description>arXiv:2405.18622v1 Announce Type: cross 
Abstract: Biclustering is a problem in machine learning and data mining that seeks to group together rows and columns of a dataset according to certain criteria. In this work, we highlight the natural relation that quantum computing models like boson and Gaussian boson sampling (GBS) have to this problem. We first explore the use of boson sampling to identify biclusters based on matrix permanents. We then propose a heuristic that finds clusters in a dataset using Gaussian boson sampling by (i) converting the dataset into a bipartite graph and then (ii) running GBS to find the densest sub-graph(s) within the larger bipartite graph. Our simulations for the above proposed heuristics show promising results for future exploration in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18622v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>physics.optics</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajinkya Borle, Ameya Bhave</dc:creator>
    </item>
    <item>
      <title>ChatGPT as the Marketplace of Ideas: Should Truth-Seeking Be the Goal of AI Content Governance?</title>
      <link>https://arxiv.org/abs/2405.18636</link>
      <description>arXiv:2405.18636v1 Announce Type: cross 
Abstract: As one of the most enduring metaphors within legal discourse, the marketplace of ideas has wielded considerable influence over the jurisprudential landscape for decades. A century after the inception of this theory, ChatGPT emerged as a revolutionary technological advancement in the twenty-first century. This research finds that ChatGPT effectively manifests the marketplace metaphor. It not only instantiates the promises envisaged by generations of legal scholars but also lays bare the perils discerned through sustained academic critique. Specifically, the workings of ChatGPT and the marketplace of ideas theory exhibit at least four common features: arena, means, objectives, and flaws. These shared attributes are sufficient to render ChatGPT historically the most qualified engine for actualizing the marketplace of ideas theory.
  The comparison of the marketplace theory and ChatGPT merely marks a starting point. A more meaningful undertaking entails reevaluating and reframing both internal and external AI policies by referring to the accumulated experience, insights, and suggestions researchers have raised to fix the marketplace theory. Here, a pivotal issue is: should truth-seeking be set as the goal of AI content governance? Given the unattainability of the absolute truth-seeking goal, I argue against adopting zero-risk policies. Instead, a more judicious approach would be to embrace a knowledge-based alternative wherein large language models (LLMs) are trained to generate competing and divergent viewpoints based on sufficient justifications. This research also argues that so-called AI content risks are not created by AI companies but are inherent in the entire information ecosystem. Thus, the burden of managing these risks should be distributed among different social actors, rather than being solely shouldered by chatbot companies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18636v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Stanford Law &amp; Policy Review Online 35 (2024) 11-37</arxiv:journal_reference>
      <dc:creator>Jiawei Zhang</dc:creator>
    </item>
    <item>
      <title>Few-Shot Testing: Estimating Uncertainty of Memristive Deep Neural Networks Using One Bayesian Test Vector</title>
      <link>https://arxiv.org/abs/2405.18894</link>
      <description>arXiv:2405.18894v1 Announce Type: cross 
Abstract: The performance of deep learning algorithms such as neural networks (NNs) has increased tremendously recently, and they can achieve state-of-the-art performance in many domains. However, due to memory and computation resource constraints, implementing NNs on edge devices is a challenging task. Therefore, hardware accelerators such as computation-in-memory (CIM) with memristive devices have been developed to accelerate the most common operations, i.e., matrix-vector multiplication. However, due to inherent device properties, external environmental factors such as temperature, and an immature fabrication process, memristors suffer from various non-idealities, including defects and variations occurring during manufacturing and runtime. Consequently, there is a lack of complete confidence in the predictions made by the model. To improve confidence in NN predictions made by hardware accelerators in the presence of device non-idealities, in this paper, we propose a Bayesian test vector generation framework that can estimate the model uncertainty of NNs implemented on memristor-based CIM hardware. Compared to the conventional point estimate test vector generation method, our method is more generalizable across different model dimensions and requires storing only one test Bayesian vector in the hardware. Our method is evaluated on different model dimensions, tasks, fault rates, and variation noise to show that it can consistently achieve $100\%$ coverage with only $0.024$ MB of memory overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18894v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soyed Tuhin Ahmed, Mehdi Tahoori</dc:creator>
    </item>
  </channel>
</rss>
