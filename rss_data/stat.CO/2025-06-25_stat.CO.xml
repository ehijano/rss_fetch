<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Jun 2025 01:37:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>From Minimax Optimal Importance Sampling to Uniformly Ergodic Importance-tempered MCMC</title>
      <link>https://arxiv.org/abs/2506.19186</link>
      <description>arXiv:2506.19186v1 Announce Type: new 
Abstract: We make two closely related theoretical contributions to the use of importance sampling schemes. First, for independent sampling, we prove that the minimax optimal trial distribution coincides with the target if and only if the target distribution has no atom with probability greater than $1/2$, where "minimax" means that the worst-case asymptotic variance of the self-normalized importance sampling estimator is minimized. When a large atom exists, it should be downweighted by the trial distribution. A similar phenomenon holds for a continuous target distribution concentrated on a small set. Second, we argue that it is often advantageous to run the Metropolis--Hastings algorithm with a tempered stationary distribution, $\pi(x)^\beta$, and correct for the bias by importance weighting. The dynamics of this "importance-tempered" sampling scheme can be described by a continuous-time Markov chain. We prove that for one-dimensional targets with polynomial tails, $\pi(x) \propto (1 + |x|)^{-\gamma}$, this chain is uniformly ergodic if and only if $1/\gamma &lt; \beta &lt; (\gamma - 2)/\gamma$. These results suggest that for target distributions with light or polynomial tails of order $\gamma &gt; 3$, importance tempering can improve the precision of time-average estimators and essentially eliminate the need for burn-in.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19186v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quan Zhou</dc:creator>
    </item>
    <item>
      <title>gcor: A Python Implementation of Categorical Gini Correlation and Its Inference</title>
      <link>https://arxiv.org/abs/2506.19230</link>
      <description>arXiv:2506.19230v2 Announce Type: cross 
Abstract: Categorical Gini Correlation (CGC), introduced by Dang et al. (2020), is a novel dependence measure designed to quantify the association between a numerical variable and a categorical variable. It has appealing properties compared to existing dependence measures, such as zero correlation mutually implying independence between the variables. It has also shown superior performance over existing methods when applied to feature screening for classification. This article presents a Python implementation for computing CGC, constructing confidence intervals, and performing independence tests based on it. Efficient algorithms have been implemented for all procedures, and they have been optimized using vectorization and parallelization to enhance computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19230v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sameera Hewage</dc:creator>
    </item>
    <item>
      <title>Modeling the uncertainty on the covariance matrix for probabilistic forecast reconciliation</title>
      <link>https://arxiv.org/abs/2506.19554</link>
      <description>arXiv:2506.19554v1 Announce Type: cross 
Abstract: In forecast reconciliation, the covariance matrix of the base forecasts errors plays a crucial role. Typically, this matrix is estimated, and then treated as known. In contrast, we propose a Bayesian reconciliation model that explicitly accounts for the uncertainty in the covariance matrix. We choose an Inverse-Wishart prior, which leads to a multivariate-t reconciled predictive distribution and allows a completely analytical derivation. Empirical experiments demonstrate that this approach improves the accuracy of the prediction intervals with respect to MinT, leading to more reliable probabilistic forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19554v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Carrara, Lorenzo Zambon, Dario Azzimonti, Giorgio Corani</dc:creator>
    </item>
    <item>
      <title>An accuracy-runtime trade-off comparison of scalable Gaussian process approximations for spatial data</title>
      <link>https://arxiv.org/abs/2501.11448</link>
      <description>arXiv:2501.11448v2 Announce Type: replace 
Abstract: Gaussian processes (GPs) are flexible, probabilistic, non-parametric models widely employed in various fields such as spatial statistics and machine learning. A drawback of Gaussian processes is their computational cost having $\mathcal{O}(N^3)$ time and $\mathcal{O}(N^2)$ memory complexity which makes them prohibitive for large data sets. Numerous approximation techniques have been proposed to address this limitation. In this work, we systematically compare the accuracy of different Gaussian process approximations concerning likelihood evaluation, parameter estimation, and prediction taking into account the computational time required to perform these tasks. In other words, we analyze the trade-off between accuracy and runtime on multiple simulated and large-scale real-world data sets. We find that Vecchia approximations consistently emerge as the most accurate in almost all experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11448v2</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Filippo Rambelli, Fabio Sigrist</dc:creator>
    </item>
  </channel>
</rss>
