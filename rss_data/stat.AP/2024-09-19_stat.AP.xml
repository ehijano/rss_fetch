<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Sep 2024 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Equity considerations in COVID-19 vaccine allocation modelling: a literature review</title>
      <link>https://arxiv.org/abs/2409.11462</link>
      <description>arXiv:2409.11462v1 Announce Type: new 
Abstract: We conducted a literature review of COVID-19 vaccine allocation modelling papers, specifically looking for publications that considered equity. We found that most models did not take equity into account, with the vast majority of publications presenting aggregated results and no results by any subgroup (e.g. age, race, geography, etc). We then give examples of how modelling can be useful to answer equity questions, and highlight some of the findings from the publications that did. Lastly, we describe seven considerations that seem important to consider when including equity in future vaccine allocation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11462v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eva Rumpler, Marc Lipsitch</dc:creator>
    </item>
    <item>
      <title>Forecasting age distribution of life-table death counts via {\alpha}-transformation</title>
      <link>https://arxiv.org/abs/2409.11658</link>
      <description>arXiv:2409.11658v1 Announce Type: new 
Abstract: We introduce a compositional power transformation, known as an {\alpha}-transformation, to model and forecast a time series of life-table death counts, possibly with zero counts observed at older ages. As a generalisation of the isometric log-ratio transformation (i.e., {\alpha} = 0), the {\alpha} transformation relies on the tuning parameter {\alpha}, which can be determined in a data-driven manner. Using the Australian age-specific period life-table death counts from 1921 to 2020, the {\alpha} transformation can produce more accurate short-term point and interval forecasts than the log-ratio transformation. The improved forecast accuracy of life-table death counts is of great importance to demographers and government planners for estimating survival probabilities and life expectancy and actuaries for determining annuity prices and reserves for various initial ages and maturity terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11658v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Steven Haberman</dc:creator>
    </item>
    <item>
      <title>Conformity assessment of processes and lots in the framework of JCGM 106:2012</title>
      <link>https://arxiv.org/abs/2409.11912</link>
      <description>arXiv:2409.11912v1 Announce Type: new 
Abstract: ISO/IEC 17000:2020 defines conformity assessment as an "activity to determine whether specified requirements relating to a product, process, system, person or body are fulfilled". JCGM (2012) establishes a framework for accounting for measurement uncertainty in conformity assessment. The focus of JCGM (2012) is on the conformity assessment of individual units of product based on measurements on a cardinal continuous scale. However, the scheme can also be applied to composite assessment targets like finite lots of product or manufacturing processes, and to the evaluation of characteristics in discrete cardinal or nominal scales.
  We consider the application of the JCGM scheme in the conformity assessment of finite lots or processes of discrete units subject to a dichotomous quality classification as conforming and nonconforming. A lot or process is classified as conforming if the actual proportion nonconforming does not exceed a prescribed upper tolerance limit, otherwise the lot or process is classified as nonconforming. The measurement on the lot or process is a statistical estimation of the proportion nonconforming based on attributes or variables sampling, and meassurement uncertainty is sampling uncertainty. Following JCGM (2012), we analyse the effect of measurement uncertainty (sampling uncertainty) in attributes sampling, and we calculate key conformity assessment parameters, in particular the producer's and consumer's risk. We suggest to integrate such parameters as a useful add-on into ISO acceptance sampling standards such as the ISO 2859 series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11912v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rainer G\"ob, Steffen Uhlig, Bernard Colson</dc:creator>
    </item>
    <item>
      <title>Who's the GOAT? Sports Rankings and Data-Driven Random Walks on the Symmetric Group</title>
      <link>https://arxiv.org/abs/2409.12107</link>
      <description>arXiv:2409.12107v1 Announce Type: new 
Abstract: Given a collection of historical sports rankings, can one tell which player is the greatest of all time (i.e., the GOAT)? In this work, we design a data-driven random walk on the symmetric group to obtain a stationary distribution over player rankings, spanning across different time periods in sports history. We combine this distribution with a notion of stochastic dominance to obtain a partial order over the players. We implement our methods using publicly available data from the Association of Tennis Professionals (ATP) and the Women's Tennis Association (WTA) to find the GOATs in the respective categories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12107v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gian-Gabriel P. Garcia, J. Carlos Mart\'inez Mori</dc:creator>
    </item>
    <item>
      <title>Bayesian estimation of the number of significant principal components for cultural data</title>
      <link>https://arxiv.org/abs/2409.12129</link>
      <description>arXiv:2409.12129v1 Announce Type: new 
Abstract: Principal component analysis (PCA) is often used to analyze multivariate data together with cluster analysis, which depends on the number of principal components used. It is therefore important to determine the number of significant principal components (PCs) extracted from a data set. Here we use a variational Bayesian version of classical PCA, to develop a new method for estimating the number of significant PCs in contexts where the number of samples is of a similar to or greater than the number of features. This eliminates guesswork and potential bias in manually determining the number of principal components and avoids overestimation of variance by filtering noise. This framework can be applied to datasets of different shapes (number of rows and columns), different data types (binary, ordinal, categorical, continuous), and with noisy and missing data. Therefore, it is especially useful for data with arbitrary encodings and similar numbers of rows and columns, such as cultural, ecological, morphological, and behavioral datasets. We tested our method on both synthetic data and empirical datasets and found that it may underestimate but not overestimate the number of principal components for the synthetic data. A small number of components was found for each empirical dataset. These results suggest that it is broadly applicable across the life sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12129v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joshua C. Macdonald, Javier Blanco-Portillo, Marcus W. Feldman, Yoav Ram</dc:creator>
    </item>
    <item>
      <title>Bias Reduction in Matched Observational Studies with Continuous Treatments: Calipered Non-Bipartite Matching and Bias-Corrected Estimation and Inference</title>
      <link>https://arxiv.org/abs/2409.11701</link>
      <description>arXiv:2409.11701v1 Announce Type: cross 
Abstract: Matching is a commonly used causal inference framework in observational studies. By pairing individuals with different treatment values but with the same values of covariates (i.e., exact matching), the sample average treatment effect (SATE) can be consistently estimated and inferred using the classic Neyman-type (difference-in-means) estimator and confidence interval. However, inexact matching typically exists in practice and may cause substantial bias for the downstream treatment effect estimation and inference. Many methods have been proposed to reduce bias due to inexact matching in the binary treatment case. However, to our knowledge, no existing work has systematically investigated bias due to inexact matching in the continuous treatment case. To fill this blank, we propose a general framework for reducing bias in inexactly matched observational studies with continuous treatments. In the matching stage, we propose a carefully formulated caliper that incorporates the information of both the paired covariates and treatment doses to better tailor matching for the downstream SATE estimation and inference. In the estimation and inference stage, we propose a bias-corrected Neyman estimator paired with the corresponding bias-corrected variance estimator to leverage the information on propensity density discrepancies after inexact matching to further reduce the bias due to inexact matching. We apply our proposed framework to COVID-19 social mobility data to showcase differences between classic and bias-corrected SATE estimation and inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11701v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Frazier, Siyu Heng, Wen Zhou</dc:creator>
    </item>
    <item>
      <title>Optimal Visual Search with Highly Heuristic Decision Rules</title>
      <link>https://arxiv.org/abs/2409.12124</link>
      <description>arXiv:2409.12124v1 Announce Type: cross 
Abstract: Visual search is a fundamental natural task for humans and other animals. We investigated the decision processes humans use when searching briefly presented displays having well-separated potential target-object locations. Performance was compared with the Bayesian-optimal decision process under the assumption that the information from the different potential target locations is statistically independent. Surprisingly, humans performed slightly better than optimal, despite humans' substantial loss of sensitivity in the fovea, and the implausibility of the human brain replicating the optimal computations. We show that three factors can quantitatively explain these seemingly paradoxical results. Most importantly, simple and fixed heuristic decision rules reach near optimal search performance. Secondly, foveal neglect primarily affects only the central potential target location. Finally, spatially correlated neural noise causes search performance to exceed that predicted for independent noise. These findings have far-reaching implications for understanding visual search tasks and other identification tasks in humans and other animals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12124v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anqi Zhang, Wilson S. Geisler</dc:creator>
    </item>
    <item>
      <title>A robust Bayesian latent position approach for community detection in networks with continuous attributes</title>
      <link>https://arxiv.org/abs/2301.00055</link>
      <description>arXiv:2301.00055v3 Announce Type: replace 
Abstract: The increasing prevalence of multiplex networks has spurred a critical need to take into account potential dependencies across different layers, especially when the goal is community detection, which is a fundamental learning task in network analysis. We propose a full Bayesian mixture model for community detection in both single-layer and multi-layer networks. A key feature of our model is the joint modeling of the nodal attributes that often come with the network data as a spatial process over the latent space. In addition, our model for multi-layer networks allows layers to have different strengths of dependency in the unique latent position structure and assumes that the probability of a relation between two actors (in a layer) depends on the distances between their latent positions (multiplied by a layer-specific factor) and the difference between their nodal attributes. Under our prior specifications, the actors' positions in the latent space arise from a finite mixture of Gaussian distributions, each corresponding to a cluster. Simulated examples show that our model outperforms existing benchmark models and exhibits significantly greater robustness when handling datasets with missing values. The model is also applied to a real-world three-layer network of employees in a law firm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.00055v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhumengmeng Jin, Juan Sosa, Shangchen Song, Brenda Betancourt</dc:creator>
    </item>
    <item>
      <title>Multi-time small-area estimation of oil and gas production capacity by Bayesian multilevel modeling</title>
      <link>https://arxiv.org/abs/2408.11167</link>
      <description>arXiv:2408.11167v2 Announce Type: replace 
Abstract: This paper presents a Bayesian multilevel modeling approach for estimating well-level oil and gas production capacities across multiple time periods within small geographic areas. Focusing on basins, which are economically significant drilling regions, we model the latent production capacity of wells using small-area estimation techniques. Our model accounts for well-level variations within these basins, incorporating factors such as lateral length, water usage, and sand usage. A key aspect of our methodology is the use of the Maidenhead Coordinate System to define small areas, enabling detailed regional analysis. The model was developed and validated using data from the Eagle Ford region, covering the years 2014 to 2019, and demonstrates strong predictive performance, particularly in handling small sample sizes. We expand the model to accommodate temporal dynamics by introducing time-effect parameters, allowing for the analysis of production trends over time. Additionally, we explore the impact of technological advancements by modeling water-sand intensity as a proxy for production efficiency. Our findings suggest that Bayesian multilevel modeling provides robust and flexible tools for understanding and predicting oil and gas production at a granular level, offering valuable insights for energy production forecasting and resource management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11167v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroaki Minato</dc:creator>
    </item>
    <item>
      <title>Nonlinear Causality in Brain Networks: With Application to Motor Imagery vs Execution</title>
      <link>https://arxiv.org/abs/2409.10374</link>
      <description>arXiv:2409.10374v2 Announce Type: replace 
Abstract: One fundamental challenge of data-driven analysis in neuroscience is modeling causal interactions and exploring the connectivity between nodes in a brain network. Various statistical methods, using different perspectives and data modalities, have been developed to understand the causal structures in brain dynamics. This study introduces a novel statistical approach, TAR4C, to dissect causal interactions in multichannel EEG recordings. TAR4C uses the threshold autoregressive (TAR) model to describe causal interactions between nodes in a brain network from two perspectives. The first tests whether one node controls the dynamics of another. The controlling node, named the threshold variable, implies its causative role since it operates as a switching mechanism governing the instantaneous transitions between autoregressive structures. This concept is known as threshold non-linearity. Once verified between a node pair, the next step in TAR modeling is assessing the causal node's predictive ability on the other's activity, representing causal interactions in autoregressive terms, a concept underlying Granger (G) causality. TAR4C can discover non-linear, time-dependent causal interactions while maintaining the G-causality framework. The approach's efficacy is demonstrated through EEG data from a motor execution/imagery experiment. By comparing causal interactions during motor execution and imagery, TAR4C reveals key similarities and differences in brain connectivity across subjects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10374v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sipan Aslan, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Mitigating the risk of tanking in multi-stage tournaments</title>
      <link>https://arxiv.org/abs/2211.16054</link>
      <description>arXiv:2211.16054v5 Announce Type: replace-cross 
Abstract: Multi-stage tournaments consisting of a round-robin group stage followed by a knockout phase are ubiquitous in sports. However, this format is incentive incompatible if at least two teams from a group advance to the knockout stage where the brackets are predetermined. A model is developed to quantify the risk of tanking in these contests. The suggested approach is applied to the 2022 FIFA World Cup to uncover how its design could have been improved by changing group labelling (a reform that has received no attention before) and the schedule of group matches. Scheduling is found to be a surprisingly weak intervention compared to previous results on the risk of collusion in a group. The probability of tanking, which is disturbingly high around 25\%, cannot be reduced by more than 3 percentage points via these policies. Tournament organisers need to consider more fundamental changes against tanking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16054v5</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o</dc:creator>
    </item>
    <item>
      <title>Performance of Cross-Validated Targeted Maximum Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2409.11265</link>
      <description>arXiv:2409.11265v2 Announce Type: replace-cross 
Abstract: Background: Advanced methods for causal inference, such as targeted maximum likelihood estimation (TMLE), require certain conditions for statistical inference. However, in situations where there is not differentiability due to data sparsity or near-positivity violations, the Donsker class condition is violated. In such situations, TMLE variance can suffer from inflation of the type I error and poor coverage, leading to conservative confidence intervals. Cross-validation of the TMLE algorithm (CVTMLE) has been suggested to improve on performance compared to TMLE in settings of positivity or Donsker class violations. We aim to investigate the performance of CVTMLE compared to TMLE in various settings.
  Methods: We utilised the data-generating mechanism as described in Leger et al. (2022) to run a Monte Carlo experiment under different Donsker class violations. Then, we evaluated the respective statistical performances of TMLE and CVTMLE with different super learner libraries, with and without regression tree methods.
  Results: We found that CVTMLE vastly improves confidence interval coverage without adversely affecting bias, particularly in settings with small sample sizes and near-positivity violations. Furthermore, incorporating regression trees using standard TMLE with ensemble super learner-based initial estimates increases bias and variance leading to invalid statistical inference.
  Conclusions: It has been shown that when using CVTMLE the Donsker class condition is no longer necessary to obtain valid statistical inference when using regression trees and under either data sparsity or near-positivity violations. We show through simulations that CVTMLE is much less sensitive to the choice of the super learner library and thereby provides better estimation and inference in cases where the super learner library uses more flexible candidates and is prone to overfitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11265v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew J. Smith, Rachael V. Phillips, Camille Maringe, Miguel Angel Luque-Fernandez</dc:creator>
    </item>
  </channel>
</rss>
