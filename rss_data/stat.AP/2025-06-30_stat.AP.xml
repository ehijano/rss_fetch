<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Jul 2025 04:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Learning Individual Reproductive Behavior from Aggregate Fertility Rates via Neural Posterior Estimation</title>
      <link>https://arxiv.org/abs/2506.22607</link>
      <description>arXiv:2506.22607v1 Announce Type: new 
Abstract: While age-specific fertility rates (ASFRs) provide the most extensive record of reproductive change, their aggregate nature masks the underlying behavioral mechanisms that ultimately drive fertility trends. To recover these mechanisms, we develop a likelihood-free Bayesian framework that couples an individual-level model of the reproductive process with Sequential Neural Posterior Estimation (SNPE). This allows us to infer eight behavioral and biological parameters from just two aggregate series: ASFRs and the age-profile of planned versus unplanned births. Applied to U.S. National Survey of Family Growth cohorts and to Demographic and Health Survey cohorts from Colombia, the Dominican Republic, and Peru, the method reproduces observed fertility schedules and, critically, predicts out-of-sample micro-level distributions of age at first sex, inter-birth intervals, and family-size ideals, none of which inform the estimation step. Because the fitted model yields complete synthetic life histories, it enables behaviorally explicit population forecasts and supports the construction of demographic digital twins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22607v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daniel Ciganda, Ignacio Camp\'on, I\~naki Permanyer, Jakob H Macke</dc:creator>
    </item>
    <item>
      <title>Strategic analysis of hydrogen market dynamics across collaboration models</title>
      <link>https://arxiv.org/abs/2506.22690</link>
      <description>arXiv:2506.22690v1 Announce Type: new 
Abstract: The global energy landscape is experiencing a transformative shift, with an increasing emphasis on sustainable and clean energy sources. Hydrogen remains a promising candidate for decarbonization, energy storage, and as an alternative fuel. This study explores the landscape of hydrogen pricing and demand dynamics by evaluating three collaboration scenarios: market-based pricing, cooperative integration, and coordinated decision-making. It incorporates price-sensitive demand, environmentally friendly production methods, and market penetration effects, to provide insights into maximizing market share, profitability, and sustainability within the hydrogen industry. This study contributes to understanding the complexities of collaboration by analyzing those structures and their role in a fast transition to clean hydrogen production by balancing economic viability and environmental goals. The findings reveal that the cooperative integration strategy is the most effective for sustainable growth, increasing green hydrogen's market share to 19.06 % and highlighting the potential for environmentally conscious hydrogen production. They also suggest that the coordinated decision-making approach enhances profitability through collaborative tariff contracts while balancing economic viability and environmental goals. This study also underscores the importance of strategic pricing mechanisms, policy alignment, and the role of hydrogen hubs in achieving sustainable growth in the hydrogen sector. By highlighting the uncertainties and potential barriers, this research offers actionable guidance for policymakers and industry players in shaping a competitive and sustainable energy marketplace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22690v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.rser.2024.115001</arxiv:DOI>
      <dc:creator>Mohammad Asghari, Hamid Afshari, Mohamad Y Jaber, Cory Searcy</dc:creator>
    </item>
    <item>
      <title>FuzzCoh: Robust Canonical Coherence-Based Fuzzy Clustering of Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2506.22861</link>
      <description>arXiv:2506.22861v1 Announce Type: new 
Abstract: Brain cognitive and sensory functions are often associated with electrophysiological activity at specific frequency bands. Clustering multivariate time series (MTS) data like EEGs is important for understanding brain functions but challenging due to complex non-stationary cross-dependencies, gradual transitions between cognitive states, noisy measurements, and ambiguous cluster boundaries. To address these issues, we develop a robust fuzzy clustering framework in the spectral domain. Our method leverages Kendall's tau-based canonical coherence, which extracts meaningful frequency-specific monotonic relationships between groups of channels or regions. KenCoh effectively captures dominant coherence structures while remaining robust against outliers and noise, making it suitable for real EEG datasets that typically contain artifacts. Our method first projects each MTS object onto vectors derived from the KenCoh estimates (i.e, canonical directions), which capture relevant information on the connectivity structure of oscillatory signals in predefined frequency bands. These spectral features are utilized to determine clusters of epochs using a fuzzy partitioning strategy, accommodating gradual transitions and overlapping class structure. Lastly, we demonstrate the effectiveness of our approach to EEG data where latent cognitive states such as alertness and drowsiness exhibit frequency-specific dynamics and ambiguity. Our method captures both spectral and spatial features by locating the frequency-dependent structure and brain functional connectivity. Built on the KenCoh framework for fuzzy clustering, it handles the complexity of high-dimensional time series data and is broadly applicable to domains such as neuroscience, wearable sensing, environmental monitoring, and finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22861v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziling Ma, Mara Sherlin Talento, Ying Sun, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Can LLM Improve for Expert Forecast Combination? Evidence from the European Central Bank Survey</title>
      <link>https://arxiv.org/abs/2506.23154</link>
      <description>arXiv:2506.23154v1 Announce Type: new 
Abstract: This study explores the potential of large language models (LLMs) to enhance expert forecasting through ensemble learning. Leveraging the European Central Bank's Survey of Professional Forecasters (SPF) dataset, we propose a comprehensive framework to evaluate LLM-driven ensemble predictions under varying conditions, including the intensity of expert disagreement, dynamics of herd behavior, and limitations in attention allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23154v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinuo Ren, Jue Wang</dc:creator>
    </item>
    <item>
      <title>Profiling Frailty: A parsimonious Frailty Index from health administrative data based on POSET theory</title>
      <link>https://arxiv.org/abs/2506.23158</link>
      <description>arXiv:2506.23158v1 Announce Type: new 
Abstract: Frailty assessment is crucial for stratifying populations and addressing healthcare challenges associated with ageing. This study proposes a Frailty Index based on administrative health data, with the aim of facilitating informed decision-making and resource allocation in population health management. The aim of this work is to develop a Frailty Index that 1) accurately predicts multiple adverse health outcomes, 2) comprises a parsimonious set of variables, 3) aggregates variables without predefined weights, 4) regenerates when applied to different populations, and 5) relies solely on routinely collected administrative data. Using administrative data from a local health authority in Italy, we identified two cohorts of individuals aged $\ge$65 years. A set of six adverse outcomes (death, emergency room access with highest priority, hospitalisation, disability onset, dementia onset, and femur fracture) was selected to define frailty. Variable selection was performed using logistic regression modelling and a forward approach based on partially ordered set (POSET) theory. The final Frailty Index comprised eight variables: age, disability, total number of hospitalisations, mental disorders, neurological diseases, heart failure, kidney failure, and cancer. The Frailty Index performs well or very well for all adverse outcomes (AUC range: 0.664-0.854) except hospitalisation (AUC: 0.664). The index also captured associations between frailty and chronic diseases, comorbidities, and socioeconomic deprivation. This study presents a validated, parsimonious Frailty Index based on routinely collected administrative data. The proposed approach offers a comprehensive toolkit for stratifying populations by frailty level, facilitating targeted interventions and resource allocation in population health management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23158v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Margherita Silan, Maurizio Nicolaio, Giovanna Boccuzzo</dc:creator>
    </item>
    <item>
      <title>Uncertainty Annotations for Holistic Test Description of Cyber-physical Energy Systems</title>
      <link>https://arxiv.org/abs/2506.23436</link>
      <description>arXiv:2506.23436v1 Announce Type: new 
Abstract: The complexity of experimental setups in the field of cyber-physical energy systems has motivated the development of the Holistic Test Description (HTD), a well-adopted approach for documenting and communicating test designs. Uncertainty, in its many flavours, is an important factor influencing the communication about experiment plans, execution of, and the reproducibility of experimental results. The work presented here focuses on supporting the structured analysis of experimental uncertainty aspects during planning and documenting complex energy systems tests. This paper introduces uncertainty extensions to the original HTD and an additional uncertainty analysis tool. The templates and tools are openly available and their use is exemplified in two case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23436v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Heussen, Jan S\"oren Schwarz, Eike Schulte, Zhiwang Feng, Leonard Enrique Ramos Perez, John Nikoletatos, Filip Pr\"ostl Andren</dc:creator>
    </item>
    <item>
      <title>Rothman diagrams: the geometry of association measure modification and collapsibility</title>
      <link>https://arxiv.org/abs/2506.23927</link>
      <description>arXiv:2506.23927v1 Announce Type: new 
Abstract: Here, we outline how Rothman diagrams provide a geometric perspective that can help epidemiologists understand the relationships between effect measure modification (which we call association measure modification), collapsibility, and confounding. A Rothman diagram plots the risk of disease in the unexposed on the x-axis and the risk in the exposed on the y-axis. Crude and stratum-specific risks in the two exposure groups define points in the unit square. When there is modification of a measure of association $M$ by a covariate $C$, the stratum-specific values of $M$ differ across strata defined by $C$, so the stratum-specific points are on different contour lines of $M$. We show how collapsibility can be defined in terms of standardization instead of no confounding, and we show that a measure of association is collapsible if and only if all its contour lines are straight. We illustrate these ideas using data from a study in Newcastle, United Kingdom, where the causal effect of smoking on 20-year mortality was confounded by age. From this perspective, it is clear that association measure modification and collapsibility are logically independent of confounding. This distinction can be obscured when these concepts are taught using regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23927v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eben Kenah</dc:creator>
    </item>
    <item>
      <title>Decadal Analysis of Delhi's Air Pollution Crisis: Unraveling the Contributors</title>
      <link>https://arxiv.org/abs/2506.24087</link>
      <description>arXiv:2506.24087v1 Announce Type: new 
Abstract: Recently, Delhi has become a chamber of bad air quality. This study explores the trends of probable contributors to Delhi's deteriorating air quality by analyzing data from 2014 to 2024 -- a period that has not been the central focus of previous research. The study aims to reassess the contributors in light of recent shifts. The consistently worsening air quality has forced the people of Delhi to adapt to an unhealthy environment. People breathing this polluted air are at great risk of developing several health issues such as respiratory infections, heart disease, and lung cancer. The study provides a quantified perspective on how each contributor has influenced pollution levels by identifying percentage contributions of major sources. Over the years, Delhi's air pollution has been primarily attributed to stubble burning. However, the present study discusses the decline in stubble burning cases in the current scenario and the evolving impact of contributors such as vehicular emissions, industrial activities, and population growth. Moreover, the study assesses the effectiveness of mitigation strategies like Electric Vehicles (EVs), public transport expansion, and pollution control policies. The average levels of the Air Quality Index (AQI) during October-November and November-December remained consistently high from 2018 to 2024, reaching 374 in November 2024. Based on the data-driven analysis, the study demonstrates that existing measures have fallen short and makes a strong case for implementing new long-term strategies focusing on the root causes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24087v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prachi Tewari, Shweta Jindal</dc:creator>
    </item>
    <item>
      <title>Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data</title>
      <link>https://arxiv.org/abs/2506.22499</link>
      <description>arXiv:2506.22499v1 Announce Type: cross 
Abstract: This study presents a novel integrated framework for dynamic origin-destination demand estimation (DODE) in multi-class mesoscopic network models, leveraging high-resolution satellite imagery together with conventional traffic data from local sensors. Unlike sparse local detectors, satellite imagery offers consistent, city-wide road and traffic information of both parking and moving vehicles, overcoming data availability limitations. To extract information from imagery data, we design a computer vision pipeline for class-specific vehicle detection and map matching, generating link-level traffic density observations by vehicle class. Building upon this information, we formulate a computational graph-based DODE model that calibrates dynamic network states by jointly matching observed traffic counts and travel times from local sensors with density measurements derived from satellite imagery. To assess the accuracy and scalability of the proposed framework, we conduct a series of numerical experiments using both synthetic and real-world data. The results of out-of-sample tests demonstrate that supplementing traditional data with satellite-derived density significantly improves estimation performance, especially for links without local sensors. Real-world experiments also confirm the framework's capability to handle large-scale networks, supporting its potential for practical deployment in cities of varying sizes. Sensitivity analysis further evaluates the impact of data quality related to satellite imagery data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22499v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiachao Liu, Pablo Guarda, Koichiro Niinuma, Sean Qian</dc:creator>
    </item>
    <item>
      <title>Fair Box ordinate transform for forecasts following a multivariate Gaussian law</title>
      <link>https://arxiv.org/abs/2506.22601</link>
      <description>arXiv:2506.22601v1 Announce Type: cross 
Abstract: Monte Carlo techniques are the method of choice for making probabilistic predictions of an outcome in several disciplines. Usually, the aim is to generate calibrated predictions which are statistically indistinguishable from the outcome. Developers and users of such Monte Carlo predictions are interested in evaluating the degree of calibration of the forecasts. Here, we consider predictions of $p$-dimensional outcomes sampling a multivariate Gaussian distribution and apply the Box ordinate transform (BOT) to assess calibration. However, this approach is known to fail to reliably indicate calibration when the sample size n is moderate. For some applications, the cost of obtaining Monte-Carlo estimates is significant, which can limit the sample size, for instance, in model development when the model is improved iteratively. Thus, it would be beneficial to be able to reliably assess calibration even if the sample size n is moderate. To address this need, we introduce a fair, sample size- and dimension-dependent version of the Gaussian sample BOT. In a simulation study, the fair Gaussian sample BOT is compared with alternative BOT versions for different miscalibrations and for different sample sizes. Results confirm that the fair Gaussian sample BOT is capable of correctly identifying miscalibration when the sample size is moderate in contrast to the alternative BOT versions. Subsequently, the fair Gaussian sample BOT is applied to two to 12-dimensional predictions of temperature and vector wind using operational ensemble forecasts of the European Centre for Medium-Range Weather Forecasts (ECMWF). Firstly, perfectly reliable situations are considered where the outcome is replaced by a forecast that samples the same distribution as the members in the ensemble. Secondly, the BOT is computed using estimates of the actual temperature and vector wind from ECMWF analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22601v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'andor Baran, Martin Leutbecher</dc:creator>
    </item>
    <item>
      <title>Do Electric Vehicles Induce More Motion Sickness Than Fuel Vehicles? A Survey Study in China</title>
      <link>https://arxiv.org/abs/2506.22674</link>
      <description>arXiv:2506.22674v1 Announce Type: cross 
Abstract: Electric vehicles (EVs) are a promising alternative to fuel vehicles (FVs), given some unique characteristics of EVs, for example, the low air pollution and maintenance cost. However, the increasing prevalence of EVs is accompanied by widespread complaints regarding the high likelihood of motion sickness (MS) induction, especially when compared to FVs, which has become one of the major obstacles to the acceptance and popularity of EVs. Despite the prevalence of such complaints online and among EV users, the association between vehicle type (i.e., EV versus FV) and MS prevalence and severity has not been quantified. Thus, this study aims to investigate the existence of EV-induced MS and explore the potential factors leading to it. A survey study was conducted to collect passengers' MS experience in EVs and FVs in the past one year. In total, 639 valid responses were collected from mainland China. The results show that FVs were associated with a higher frequency of MS, while EVs were found to induce more severe MS symptoms. Further, we found that passengers' MS severity was associated with individual differences (i.e., age, gender, sleep habits, susceptibility to motion-induced MS), in-vehicle activities (i.e., chatting with others and watching in-vehicle displays), and road conditions (i.e., congestion and slope), while the MS frequency was associated with the vehicle ownership and riding frequency. The results from this study can guide the directions of future empirical studies that aim to quantify the inducers of MS in EVs and FVs, as well as the optimization of EVs to reduce MS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22674v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Weiyin Xie, Chunxi Huang, Jiyao Wang, Dengbo He</dc:creator>
    </item>
    <item>
      <title>Doubly robust estimation of causal effects for random object outcomes with continuous treatments</title>
      <link>https://arxiv.org/abs/2506.22754</link>
      <description>arXiv:2506.22754v1 Announce Type: cross 
Abstract: Causal inference is central to statistics and scientific discovery, enabling researchers to identify cause-and-effect relationships beyond associations. While traditionally studied within Euclidean spaces, contemporary applications increasingly involve complex, non-Euclidean data structures that reside in abstract metric spaces, known as random objects, such as images, shapes, networks, and distributions. This paper introduces a novel framework for causal inference with continuous treatments applied to non-Euclidean data. To address the challenges posed by the lack of linear structures, we leverage Hilbert space embeddings of the metric spaces to facilitate Fr\'echet mean estimation and causal effect mapping. Motivated by a study on the impact of exposure to fine particulate matter on age-at-death distributions across U.S. counties, we propose a nonparametric, doubly-debiased causal inference approach for outcomes as random objects with continuous treatments. Our framework can accommodate moderately high-dimensional vector-valued confounders and derive efficient influence functions for estimation to ensure both robustness and interpretability. We establish rigorous asymptotic properties of the cross-fitted estimators and employ conformal inference techniques for counterfactual outcome prediction. Validated through numerical experiments and applied to real-world environmental data, our framework extends causal inference methodologies to complex data structures, broadening its applicability across scientific disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22754v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satarupa Bhattacharjee, Bing Li, Xiao Wu, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>Curious Causality-Seeking Agents Learn Meta Causal World</title>
      <link>https://arxiv.org/abs/2506.23068</link>
      <description>arXiv:2506.23068v1 Announce Type: cross 
Abstract: When building a world model, a common assumption is that the environment has a single, unchanging underlying causal rule, like applying Newton's laws to every situation. In reality, what appears as a drifting causal mechanism is often the manifestation of a fixed underlying mechanism seen through a narrow observational window. This brings about a problem that, when building a world model, even subtle shifts in policy or environment states can alter the very observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal Graph} as world models, a minimal unified representation that efficiently encodes the transformation rules governing how causal structures shift across different latent world states. A single Meta-Causal Graph is composed of multiple causal subgraphs, each triggered by meta state, which is in the latent state space. Building on this representation, we introduce a \textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta states that trigger each subgraph, (2) discover the corresponding causal relationships by agent curiosity-driven intervention policy, and (3) iteratively refine the Meta-Causal Graph through ongoing curiosity-driven exploration and agent experiences. Experiments on both synthetic tasks and a challenging robot arm manipulation task demonstrate that our method robustly captures shifts in causal dynamics and generalizes effectively to previously unseen contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23068v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyu Zhao, Haoxuan Li, Haifeng Zhang, Jun Wang, Francesco Faccio, J\"urgen Schmidhuber, Mengyue Yang</dc:creator>
    </item>
    <item>
      <title>Covariate-informed link prediction with extreme taxonomic bias</title>
      <link>https://arxiv.org/abs/2506.23370</link>
      <description>arXiv:2506.23370v1 Announce Type: cross 
Abstract: Biotic interactions provide a valuable window into the inner workings of complex ecological communities and capture the loss of ecological function often precipitated by environmental change. However, the financial and logistical challenges associated with collecting interaction data result in networks that are recorded with geographical and taxonomic bias, particularly when studies are narrowly focused. We develop an approach to reduce bias in link prediction in the common scenario in which data are derived from studies focused on a small number of species. Our Extended Covariate-Informed Link Prediction (COIL+) framework utilizes a latent factor model that flexibly borrows information between species and incorporates dependence on covariates and phylogeny, and introduces a framework for borrowing information from multiple studies to reduce bias due to uncertain species occurrence. Additionally, we propose a new trait matching procedure which permits heterogeneity in trait-interaction propensity associations at the species level. We illustrate the approach through an application to a literature compilation data set of 268 sources reporting frugivory in Afrotropical forests and compare the performance with and without correction for uncertainty in occurrence. Our method results in a substantial improvement in link prediction, revealing 5,255 likely but unobserved frugivory interactions, and increasing model discrimination under conditions of great taxonomic bias and narrow study focus. This framework generalizes to a variety of network contexts and offers a useful tool for link prediction given networks recorded with bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23370v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jennifer N. Kampe, Camille DeSisto, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Breadth, Depth, and Flux of Course-Prerequisite Networks</title>
      <link>https://arxiv.org/abs/2506.23510</link>
      <description>arXiv:2506.23510v1 Announce Type: cross 
Abstract: Course-prerequisite networks (CPNs) are directed acyclic graphs that model complex academic curricula by representing courses as nodes and dependencies between them as directed links. These networks are indispensable tools for visualizing, studying, and understanding curricula. For example, CPNs can be used to detect important courses, improve advising, guide curriculum design, analyze graduation time distributions, and quantify the strength of knowledge flow between different university departments. However, most CPN analyses to date have focused only on micro- and meso-scale properties. To fill this gap, we define and study three new global CPN measures: breadth, depth, and flux. All three measures are invariant under transitive reduction and are based on the concept of topological stratification, which generalizes topological ordering in directed acyclic graphs. These measures can be used for macro-scale comparison of different CPNs. We illustrate the new measures numerically by applying them to three real and synthetic CPNs from three universities: the Cyprus University of Technology, the California Institute of Technology, and Johns Hopkins University. The CPN data analyzed in this paper are publicly available in a GitHub repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23510v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konstantin Zuev, Pavlos Stavrinides</dc:creator>
    </item>
    <item>
      <title>Developing a Synthetic Socio-Economic Index through Autoencoders: Evidence from Florence's Suburban Areas</title>
      <link>https://arxiv.org/abs/2506.23849</link>
      <description>arXiv:2506.23849v1 Announce Type: cross 
Abstract: The interest in summarizing complex and multidimensional phenomena often related to one or more specific sectors (social, economic, environmental, political, etc.) to make them easily understandable even to non-experts is far from waning. A widely adopted approach for this purpose is the use of composite indices, statistical measures that aggregate multiple indicators into a single comprehensive measure. In this paper, we present a novel methodology called AutoSynth, designed to condense potentially extensive datasets into a single synthetic index or a hierarchy of such indices. AutoSynth leverages an Autoencoder, a neural network technique, to represent a matrix of features in a lower-dimensional space. Although this approach is not limited to the creation of a particular composite index and can be applied broadly across various sectors, the motivation behind this work arises from a real-world need. Specifically, we aim to assess the vulnerability of the Italian city of Florence at the suburban level across three dimensions: economic, demographic, and social. To demonstrate the methodology's effectiveness, it is also applied to estimate a vulnerability index using a rich, publicly available dataset on U.S. counties and validated through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23849v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giulio Grossi, Emilia Rocco</dc:creator>
    </item>
    <item>
      <title>CoMMiT: Co-informed inference of microbiome-metabolome interactions via transfer learning</title>
      <link>https://arxiv.org/abs/2506.24013</link>
      <description>arXiv:2506.24013v1 Announce Type: cross 
Abstract: Recent multi-omic microbiome studies enable integrative analysis of microbes and metabolites, uncovering their associations with various host conditions. Such analyses require multivariate models capable of accounting for the complex correlation structures between microbes and metabolites. However, existing multivariate models often suffer from low statistical power for detecting microbiome-metabolome interactions due to small sample sizes and weak biological signals. To address these challenges, we introduce CoMMiT, Co-informed inference of Microbiome-Metabolome Interactions via novel Transfer learning models. Unlike conventional transfer-learning methods that borrow information from external datasets, CoMMiT leverages similarities across metabolites within a single cohort, reducing the risk of negative transfer often caused by differences in sequencing platforms and bioinformatic pipelines across studies. CoMMiT operates under the flexible assumption that auxiliary metabolites are collectively informative for the target metabolite, without requiring individual auxiliary metabolites to be informative. CoMMiT uses a novel data-driven approach to selecting the optimal set of auxiliary metabolites. Using this optimal set, CoMMiT employs a de-biasing framework to enable efficient calculation of p-values, facilitating the identification of statistically significant microbiome-metabolome interactions. Applying CoMMiT to a feeding study reveals biologically meaningful microbiome-metabolome interactions under a low glycemic load diet, demonstrating the diet-host link through gut metabolism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24013v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leiyue Li, Chenglong Ye, Tim Randolph, Meredith Hullar, Johanna Lampe, Marian Neuhouser, Daniel Raftery, Yue Wang</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis method in the presence of a missing not at random ordinal independent variable</title>
      <link>https://arxiv.org/abs/2506.24025</link>
      <description>arXiv:2506.24025v1 Announce Type: cross 
Abstract: Data analysis often encounters missing data, which can result in inaccurate conclusions, especially when it comes to ordinal variables. In trauma data, the Glasgow Coma Scale is useful for assessing the level of consciousness. This score is often missing in patients who are intubated or under sedation upon arrival at the hospital, and those with normal reactivity without head injury, suggesting a Missing Not At Random (MNAR) mechanism. The problem with MNAR is the absence of a definitive analysis. While sensitivity analysis is often recommended, practical limitations sometimes restrict the analysis to a basic comparison between results under Missing Completely At Random (MCAR) and Missing At Random (MAR) assumptions, disregarding MNAR plausibility. Our objective is to propose a flexible and accessible sensitivity analysis method in the presence of a MNAR ordinal independent variable. The method is inspired by the sensitivity analysis approach proposed by Leurent et al. (2018) for a continuous response variable. We propose an extension for an independent ordinal variable. The method is evaluated on simulated data before being applied to Pan-Canadian trauma data from April 2013 to March 2018. The simulation shows that MNAR estimates are less biased than MAR estimates and more precise than complete case analysis (CC) estimates. The confidence intervals coverage rates are relatively better for MNAR estimates than CC and MAR estimates. In the application, it is observed that the Glasgow Coma Scale is significant under MNAR, unlike MCAR and MAR assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24025v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdoulaye Dioni, Alexandre Bureau, Lynne Moore, Aida Eslami</dc:creator>
    </item>
    <item>
      <title>Pressing Intensity: An Intuitive Measure for Pressing in Soccer</title>
      <link>https://arxiv.org/abs/2501.04712</link>
      <description>arXiv:2501.04712v2 Announce Type: replace 
Abstract: Pressing is a fundamental defensive strategy in football, characterized by applying pressure on the ball owning team to regain possession. Despite its significance, existing metrics for measuring pressing often lack precision or comprehensive consideration of positional data, player movement and speed. This research introduces an innovative framework for quantifying pressing intensity, leveraging advancements in positional tracking data and components from Spearman's Pitch Control model. Our method integrates player velocities, movement directions, and reaction times to compute the time required for a defender to intercept an attacker or the ball. This time-to-intercept measure is then transformed into probabilistic values using a logistic function, enabling dynamic and intuitive analysis of pressing situations at the individual frame level. the model captures how every player's movement influences pressure on the field, offering actionable insights for coaches, analysts, and decision-makers. By providing a robust and intepretable metric, our approach facilitates the identification of pressing strategies, advanced situational analyses, and the derivation of metrics, advancing the analytical capabilities for modern football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04712v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joris Bekkers</dc:creator>
    </item>
    <item>
      <title>Community Detection Analysis of Spatial Transcriptomics Data</title>
      <link>https://arxiv.org/abs/2503.12351</link>
      <description>arXiv:2503.12351v3 Announce Type: replace 
Abstract: The spatial transcriptomics (ST) data produced by recent biotechnologies, such as CosMx and Xenium, contain huge amount of information about cancer tissue samples, which has great potential for cancer research via detection of community: a collection of cells with distinct cell-type composition and similar neighboring patterns. But existing clustering methods do not work well for community detection of CosMx ST data, and the commonly used kNN compositional data method shows lack of informative neighboring cell patterns for huge CosMx data. In this article, we propose a novel and more informative disk compositional data (DCD) method, which identifies neighboring patterns of each cell via taking into account of ST data features from recent new technologies. After initial processing ST data into DCD matrix, a new innovative and interpretable DCD-TMHC community detection method is proposed here. Extensive simulation studies and CosMx breast cancer data analysis clearly show that our proposed DCD-TMHC method is superior to other methods. Based on the communities detected by DCD-TMHC method for CosMx breast cancer data, the logistic regression analysis results demonstrate that DCD-TMHC method is clearly interpretable and superior, especially in terms of assessment for different stages of cancer. These suggest that our proposed novel, innovative, informative and interpretable DCD-TMHC method here will be helpful and have impact to future cancer research based on ST data, which can improve cancer diagnosis and monitor cancer treatment progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12351v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles Zhao</dc:creator>
    </item>
    <item>
      <title>Adaptive Supergeo Design: A Scalable Framework for Geographic Marketing Experiments</title>
      <link>https://arxiv.org/abs/2506.20499</link>
      <description>arXiv:2506.20499v2 Announce Type: replace 
Abstract: Geographic experiments are a gold-standard for measuring incremental return on ad spend (iROAS) at scale, yet their design is challenging: the unit count is small, heterogeneity is large, and the optimal Supergeo partitioning problem is NP-hard. We introduce Adaptive Supergeo Design (ASD), a two-stage framework that renders Supergeo designs practical for thousands of markets. A bespoke graph-neural network first learns geo-embeddings and proposes a concise candidate set of 'supergeos'; a CP-SAT solver then selects a partition that balances both baseline outcomes and pre-treatment covariates believed to modify the treatment effect. We prove that ASD's objective value is within (1+epsilon) of the global optimum under mild community-structure assumptions. In simulations with up to 1,000 Designated Market Areas ASD completes in minutes on standard hardware, retains every media dollar, and cuts iROAS bias substantively relative to existing methods. ASD therefore turns geo-lift testing into a routine, scalable component of media planning while preserving statistical rigour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20499v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Shaw</dc:creator>
    </item>
    <item>
      <title>Who is driving the conversation? Analysing the nodality of British MPs and journalists on social media</title>
      <link>https://arxiv.org/abs/2402.08765</link>
      <description>arXiv:2402.08765v3 Announce Type: replace-cross 
Abstract: With the rise of social media, political conversations now take place in more diffuse environments. In this context, it is not always clear why some actors, more than others, have greater influence on how discussions are shaped. To investigate the factors behind such influence, we build on nodality, a concept in political science which describes the capacity of an actor to exchange information within discourse networks. This concept goes beyond traditional network metrics that describe the position of an actor in the network to include exogenous drivers of influence (e.g. factors relating to organisational hierarchies). We study online discourse on Twitter (now X) in the UK to measure the relative nodality of two sets of policy actors - Members of Parliament (MPs) and accredited journalists - on four policy topics. We find that influence on the platform is driven by two key factors: (i) active nodality, derived from the actor's level of topic-related engagement, and (ii) inherent nodality, which is independent of the platform discourse and reflects the actor's institutional position. These findings significantly further our understanding of the origins of influence on social media platforms and suggest in which contexts influence is transferable across topics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08765v3</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sukankana Chakraborty, Leonardo Castro-Gonzalez, Helen Margetts, Hardik Rajpal, Daniele Guariso, Jonathan Bright</dc:creator>
    </item>
    <item>
      <title>The inverse Kalman filter</title>
      <link>https://arxiv.org/abs/2407.10089</link>
      <description>arXiv:2407.10089v5 Announce Type: replace-cross 
Abstract: We introduce the inverse Kalman filter, which enables exact matrix-vector multiplication between a covariance matrix from a dynamic linear model and any real-valued vector with linear computational cost. We integrate the inverse Kalman filter with the conjugate gradient algorithm, which substantially accelerates the computation of matrix inversion for a general form of covariance matrix, where other approximation approaches may not be directly applicable. We demonstrate the scalability and efficiency of the proposed approach through applications in nonparametric estimation of particle interaction functions, using both simulations and cell trajectories from microscopy data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10089v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Fang, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Best Linear Unbiased Estimate from Privatized Contingency Tables</title>
      <link>https://arxiv.org/abs/2409.04387</link>
      <description>arXiv:2409.04387v5 Announce Type: replace-cross 
Abstract: In differential privacy (DP) mechanisms, it can be beneficial to release ``redundant'' outputs, where some quantities can be estimated in multiple ways by combining different privatized values. Indeed, the DP 2020 Decennial Census products published by the U.S. Census Bureau consist of such redundant noisy counts. When redundancy is present, the DP output can be improved by enforcing self-consistency (i.e., estimators obtained using different noisy counts result in the same value), and we show that the minimum variance processing is a linear projection. However, standard projection algorithms require excessive computation and memory, making them impractical for large-scale applications such as the Decennial Census. We propose the Scalable Efficient Algorithm for Best Linear Unbiased Estimate (SEA BLUE), based on a two-step process of aggregation and differencing that 1) enforces self-consistency through a linear and unbiased procedure, 2) is computationally and memory efficient, 3) achieves the minimum variance solution under certain structural assumptions, and 4) is empirically shown to be robust to violations of these structural assumptions. We propose three methods of calculating confidence intervals from our estimates, under various assumptions. Finally, we apply SEA BLUE to two 2010 Census demonstration products, illustrating its scalability and validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04387v5</guid>
      <category>stat.CO</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Adam Edwards, Paul Bartholomew, Andrew Sillers</dc:creator>
    </item>
    <item>
      <title>Thompson, Ulam, or Gauss? Multi-criteria recommendations for posterior probability computation methods in Bayesian response-adaptive trials</title>
      <link>https://arxiv.org/abs/2411.19871</link>
      <description>arXiv:2411.19871v2 Announce Type: replace-cross 
Abstract: Bayesian adaptive designs enable flexible clinical trials by adapting features based on accumulating data. Among these, Bayesian Response-Adaptive Randomization (BRAR) skews patient allocation towards more promising treatments based on interim data. Implementing BRAR requires the relatively quick evaluation of posterior probabilities. However, the limitations of existing closed-form solutions mean trials often rely on computationally intensive approximations which can impact accuracy and the scope of scenarios explored. While faster Gaussian approximations exist, their reliability is not guaranteed. Critically, the approximation method used is often poorly reported, and the literature lacks practical guidance for selecting and comparing these methods, particularly regarding the trade-offs between computational speed, inferential accuracy, and their implications for patient benefit.
  In this paper, we focus on BRAR trials with binary endpoints, developing a novel algorithm that efficiently and exactly computes these posterior probabilities, enabling a robust assessment of existing approximation methods in use. Leveraging these exact computations, we establish a comprehensive benchmark for evaluating approximation methods based on their computational speed, patient benefit, and inferential accuracy. Our comprehensive analysis, conducted through a range of simulations in the two-armed case and a re-analysis of the three-armed Established Status Epilepticus Treatment Trial, reveals that the exact calculation algorithm is often the fastest, even for up to 12 treatment arms. Furthermore, we demonstrate that commonly used approximation methods can lead to significant power loss and Type I error rate inflation. We conclude by providing practical guidance to aid practitioners in selecting the most appropriate computation method for various clinical trial settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19871v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Kaddaj, Lukas Pin, Stef Baas, Edwin Y. N. Tang, David S. Robertson, Sof\'ia S. Villar</dc:creator>
    </item>
    <item>
      <title>DGSAM: Domain Generalization via Individual Sharpness-Aware Minimization</title>
      <link>https://arxiv.org/abs/2503.23430</link>
      <description>arXiv:2503.23430v2 Announce Type: replace-cross 
Abstract: Domain generalization (DG) aims to learn models that perform well on unseen target domains by training on multiple source domains. Sharpness-Aware Minimization (SAM), known for finding flat minima that improve generalization, has therefore been widely adopted in DG. However, our analysis reveals that SAM in DG may converge to \textit{fake flat minima}, where the total loss surface appears flat in terms of global sharpness but remains sharp with respect to individual source domains. To understand this phenomenon more precisely, we formalize the average worst-case domain risk as the maximum loss under domain distribution shifts within a bounded divergence, and derive a generalization bound that reveals the limitations of global sharpness-aware minimization. In contrast, we show that individual sharpness provides a valid upper bound on this risk, making it a more suitable proxy for robust domain generalization. Motivated by these insights, we shift the DG paradigm toward minimizing individual sharpness across source domains. We propose \textit{Decreased-overhead Gradual SAM (DGSAM)}, which applies gradual domain-wise perturbations in a computationally efficient manner to consistently reduce individual sharpness. Extensive experiments demonstrate that DGSAM not only improves average accuracy but also reduces performance variance across domains, while incurring less computational overhead than SAM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23430v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youngjun Song, Youngsik Hwang, Jonghun Lee, Heechang Lee, Dong-Young Lim</dc:creator>
    </item>
    <item>
      <title>Online model learning with data-assimilated reservoir computers</title>
      <link>https://arxiv.org/abs/2504.16767</link>
      <description>arXiv:2504.16767v2 Announce Type: replace-cross 
Abstract: We propose an online learning framework for forecasting nonlinear spatio-temporal signals (fields). The method integrates (i) dimensionality reduction, here, a simple proper orthogonal decomposition (POD) projection; (ii) a generalized autoregressive model to forecast reduced dynamics, here, a reservoir computer; (iii) online adaptation to update the reservoir computer (the model), here, ensemble sequential data assimilation. We demonstrate the framework on a wake past a cylinder governed by the Navier-Stokes equations, exploring the assimilation of full flow fields (projected onto POD modes) and sparse sensors. Three scenarios are examined: a na\"ive physical state estimation; a two-fold estimation of physical and reservoir states; and a three-fold estimation that also adjusts the model parameters. The two-fold strategy significantly improves ensemble convergence and reduces reconstruction error compared to the na\"ive approach. The three-fold approach enables robust online training of partially-trained reservoir computers, overcoming limitations of a priori training. By unifying data-driven reduced order modelling with Bayesian data assimilation, this work opens new opportunities for scalable online model learning for nonlinear time series forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16767v2</guid>
      <category>cs.LG</category>
      <category>physics.flu-dyn</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea N\'ovoa, Luca Magri</dc:creator>
    </item>
    <item>
      <title>Discretion in the Loop: Human Expertise in Algorithm-Assisted College Advising</title>
      <link>https://arxiv.org/abs/2505.13325</link>
      <description>arXiv:2505.13325v2 Announce Type: replace-cross 
Abstract: In higher education, many institutions use algorithmic alerts to flag at-risk students and deliver advising at scale. While much research has focused on evaluating algorithmic predictions, relatively little is known about how discretionary interventions by human experts shape outcomes in algorithm-assisted settings. We study this question using rich quantitative and qualitative data from a randomized controlled trial of an algorithm-assisted advising program at Georgia State University. Taking a mixed-methods approach, we examine whether and how advisors use context unavailable to an algorithm to guide interventions and influence student success. We develop a causal graphical framework for human expertise in the interventional setting, extending prior work on discretion in purely predictive settings. We then test a necessary condition for discretionary expertise using structured advisor logs and student outcomes data, identifying several interventions that meet the criterion for statistical significance. Accordingly, we estimate that 2 out of 3 interventions taken by advisors in the treatment arm were plausibly "expertly targeted" to students using non-algorithmic context. Systematic qualitative analysis of advisor notes corroborates these findings, showing a pattern of advisors incorporating diverse forms of contextual information--such as personal circumstances, financial issues, and student engagement--into their decisions. Our results offer theoretical and practical insight into the real-world effectiveness of algorithm-supported college advising, and underscore the importance of accounting for human expertise in the design, evaluation, and implementation of algorithmic decision systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13325v2</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kara Schechtman, Benjamin Brandon, Jenise Stafford, Hannah Li, Lydia T. Liu</dc:creator>
    </item>
  </channel>
</rss>
