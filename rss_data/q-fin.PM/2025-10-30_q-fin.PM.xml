<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.PM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.PM</link>
    <description>q-fin.PM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.PM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Oct 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Learning to Manage Investment Portfolios beyond Simple Utility Functions</title>
      <link>https://arxiv.org/abs/2510.26165</link>
      <description>arXiv:2510.26165v1 Announce Type: new 
Abstract: While investment funds publicly disclose their objectives in broad terms, their managers optimize for complex combinations of competing goals that go beyond simple risk-return trade-offs. Traditional approaches attempt to model this through multi-objective utility functions, but face fundamental challenges in specification and parameterization. We propose a generative framework that learns latent representations of fund manager strategies without requiring explicit utility specification.
  Our approach directly models the conditional probability of a fund's portfolio weights, given stock characteristics, historical returns, previous weights, and a latent variable representing the fund's strategy. Unlike methods based on reinforcement learning or imitation learning, which require specified rewards or labeled expert objectives, our GAN-based architecture learns directly from the joint distribution of observed holdings and market data.
  We validate our framework on a dataset of 1436 U.S. equity mutual funds. The learned representations successfully capture known investment styles, such as "growth" and "value," while also revealing implicit manager objectives. For instance, we find that while many funds exhibit characteristics of Markowitz-like optimization, they do so with heterogeneous realizations for turnover, concentration, and latent factors.
  To analyze and interpret the end-to-end model, we develop a series of tests that explain the model, and we show that the benchmark's expert labeling are contained in our model's encoding in a linear interpretable way.
  Our framework provides a data-driven approach for characterizing investment strategies for applications in market simulation, strategy attribution, and regulatory oversight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26165v1</guid>
      <category>q-fin.PM</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3768292.3770426</arxiv:DOI>
      <dc:creator>Maarten P. Scholl, Mahmoud Mahfouz, Anisoara Calinescu, J. Doyne Farmer</dc:creator>
    </item>
    <item>
      <title>ChatGPT in Systematic Investing - Enhancing Risk-Adjusted Returns with LLMs</title>
      <link>https://arxiv.org/abs/2510.26228</link>
      <description>arXiv:2510.26228v1 Announce Type: new 
Abstract: This paper investigates whether large language models (LLMs) can improve cross-sectional momentum strategies by extracting predictive signals from firm-specific news. We combine daily U.S. equity returns for S&amp;P 500 constituents with high-frequency news data and use prompt-engineered queries to ChatGPT that inform the model when a stock is about to enter a momentum portfolio. The LLM evaluates whether recent news supports a continuation of past returns, producing scores that condition both stock selection and portfolio weights. An LLM-enhanced momentum strategy outperforms a standard long-only momentum benchmark, delivering higher Sharpe and Sortino ratios both in-sample and in a truly out-of-sample period after the model's pre-training cut-off. These gains are robust to transaction costs, prompt design, and portfolio constraints, and are strongest for concentrated, high-conviction portfolios. The results suggest that LLMs can serve as effective real-time interpreters of financial news, adding incremental value to established factor-based investment strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26228v1</guid>
      <category>q-fin.PM</category>
      <category>q-fin.PR</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nikolas Anic, Andrea Barbon, Ralf Seiz, Carlo Zarattini</dc:creator>
    </item>
    <item>
      <title>Entropy-Guided Multiplicative Updates: KL Projections for Multi-Factor Target Exposures</title>
      <link>https://arxiv.org/abs/2510.24607</link>
      <description>arXiv:2510.24607v2 Announce Type: replace 
Abstract: We introduce Entropy-Guided Multiplicative Updates (EGMU), a convex optimization framework for constructing multi-factor target-exposure portfolios by minimizing Kullback-Leibler divergence from a benchmark under linear factor constraints. We establish feasibility and uniqueness of strictly positive solutions when the benchmark and targets satisfy convex-hull conditions. We derive the dual concave formulation with explicit gradient, Hessian, and sensitivity expressions, and provide two provably convergent solvers: a damped dual Newton method with global convergence and local quadratic rate, and a KL-projection scheme based on iterative proportional fitting and Bregman-Dykstra projections. We further generalize EGMU to handle elastic targets and robust target sets, and introduce a path-following ordinary differential equation for tracing solution trajectories. Stable and scalable implementations are provided using LogSumExp stabilization, covariance regularization, and half-space KL projections. Our focus is on theory and reproducible algorithms; empirical benchmarking is optional.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24607v2</guid>
      <category>q-fin.PM</category>
      <category>math.OC</category>
      <pubDate>Fri, 31 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yimeng Qiu</dc:creator>
    </item>
  </channel>
</rss>
