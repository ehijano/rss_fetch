<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Dec 2025 05:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Improved Lower Bounds for Privacy under Continual Release</title>
      <link>https://arxiv.org/abs/2512.15981</link>
      <description>arXiv:2512.15981v1 Announce Type: new 
Abstract: We study the problem of continually releasing statistics of an evolving dataset under differential privacy. In the event-level setting, we show the first polynomial lower bounds on the additive error for insertions-only graph problems such as maximum matching, degree histogram and $k$-core. This is an exponential improvement on the polylogarithmic lower bounds of Fichtenberger et al.[ESA 2021] for the former two problems, and are the first continual release lower bounds for the latter. Our results run counter to the intuition that the difference between insertions-only vs fully dynamic updates causes the gap between polylogarithmic and polynomial additive error. We show that for maximum matching and $k$-core, allowing small multiplicative approximations is what brings the additive error down to polylogarithmic.
  Beyond graph problems, our techniques also show that polynomial additive error is unavoidable for Simultaneous Norm Estimation in the insertions-only setting. When multiplicative approximations are allowed, we circumvent this lower bound by giving the first continual mechanism with polylogarithmic additive error under $(1+\zeta)$ multiplicative approximations, for $\zeta&gt;0$, for estimating all monotone symmetric norms simultaneously.
  In the item-level setting, we show polynomial lower bounds on the product of the multiplicative and the additive error of continual mechanisms for a large range of graph problems. To the best of our knowledge, these are the first lower bounds for any differentially private continual release mechanism with multiplicative error. To obtain this, we prove a new lower bound on the product of multiplicative and additive error for 1-Way-Marginals, from which we reduce to continual graph problems. This generalizes the lower bounds of Hardt and Talwar[STOC 2010] and Bun et al.[STOC 2014] on the additive error for mechanisms with no multiplicative error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15981v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bardiya Aryanfard, Monika Henzinger, David Saulpic, A. R. Sricharan</dc:creator>
    </item>
    <item>
      <title>Instance Optimality in PageRank Centrality Estimation</title>
      <link>https://arxiv.org/abs/2512.16087</link>
      <description>arXiv:2512.16087v1 Announce Type: new 
Abstract: We study an adaptive variant of a simple, classic algorithm for estimating a vertex's PageRank centrality within a constant relative error, with constant probability. We show that this algorithm is instance-optimal up to a polylogarithmic factor for any directed graph of order $n$ whose maximal in- and out-degrees are at most a constant fraction of $n$. The instance-optimality also extends to graphs in which up to a polylogarithmic number of vertices have unbounded degree, thereby covering all sparse graphs with $\widetilde{O}(n)$ edges. Finally, we provide a counterexample showing that the algorithm is not instance-optimal for graphs with degrees mostly equal to $n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16087v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikkel Thorup, Hanzhi Wang</dc:creator>
    </item>
    <item>
      <title>Conquering the Multiverse: The River Voting Method with Efficient Parallel Universe Tiebreaking</title>
      <link>https://arxiv.org/abs/2512.16414</link>
      <description>arXiv:2512.16414v1 Announce Type: new 
Abstract: Democracy relies on making collective decisions through voting. In addition, voting procedures have further applications, for example in the training of artificial intelligence. An essential criterion for determining the winner of a fair election is that all alternatives are treated equally: this is called neutrality. The established Ranked Pairs voting method cannot simultaneously guarantee neutrality and be computationally tractable for election with ties. River, the recently introduced voting method, shares desirable properties with Ranked Pairs and has further advantages, such as a new property related to resistance against manipulation. Both Ranked Pairs and River use a weighted margin graph to model the election. Ties in the election can lead to edges of equal margin. To order the edges in such a case, a tiebreaking scheme must be employed. Many tiebreaks violate neutrality or other important properties. A tiebreaking scheme that preserves neutrality is Parallel Universe Tiebreaking (PUT). Ranked Pairs with PUT is NP-hard to compute.
  The main result of this thesis shows that River with PUT can be computed in polynomial worst-case runtime: We can check whether an alternative is a River PUT winner, by running River with a specially constructed ordering of the edges. To construct this ordering, we introduce the semi-River diagram which contains the edges that can appear in any River diagram for some arbitrary tiebreak. On this diagram we can compute the River winners, by applying a variant of Prims algorithm per alternative. Additionally, we give an algorithm improve the previous naive runtime of River from $\mathcal{O}(n^4)$ to $\mathcal{O}(n^2 \log n)$, where n is the number of alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16414v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jannes Malanowski</dc:creator>
    </item>
    <item>
      <title>Fully Dynamic Algorithms for Chamfer Distance</title>
      <link>https://arxiv.org/abs/2512.16639</link>
      <description>arXiv:2512.16639v1 Announce Type: new 
Abstract: We study the problem of computing Chamfer distance in the fully dynamic setting, where two set of points $A, B \subset \mathbb{R}^{d}$, each of size up to $n$, dynamically evolve through point insertions or deletions and the goal is to efficiently maintain an approximation to $\mathrm{dist}_{\mathrm{CH}}(A,B) = \sum_{a \in A} \min_{b \in B} \textrm{dist}(a,b)$, where $\textrm{dist}$ is a distance measure. Chamfer distance is a widely used dissimilarity metric for point clouds, with many practical applications that require repeated evaluation on dynamically changing datasets, e.g., when used as a loss function in machine learning. In this paper, we present the first dynamic algorithm for maintaining an approximation of the Chamfer distance under the $\ell_p$ norm for $p \in \{1,2 \}$. Our algorithm reduces to approximate nearest neighbor (ANN) search with little overhead. Plugging in standard ANN bounds, we obtain $(1+\epsilon)$-approximation in $\tilde{O}(\epsilon^{-d})$ update time and $O(1/\epsilon)$-approximation in $\tilde{O}(d n^{\epsilon^2} \epsilon^{-4})$ update time. We evaluate our method on real-world datasets and demonstrate that it performs competitively against natural baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16639v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gramoz Goranci, Shaofeng Jiang, Peter Kiss, Eva Szilagyi, Qiaoyuan Yang</dc:creator>
    </item>
    <item>
      <title>Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery</title>
      <link>https://arxiv.org/abs/2512.16875</link>
      <description>arXiv:2512.16875v1 Announce Type: new 
Abstract: We study the problem of finding confidence ellipsoids for an arbitrary distribution in high dimensions. Given samples from a distribution $D$ and a confidence parameter $\alpha$, the goal is to find the smallest volume ellipsoid $E$ which has probability mass $\Pr_{D}[E] \ge 1-\alpha$. Ellipsoids are a highly expressive class of confidence sets as they can capture correlations in the distribution, and can approximate any convex set. This problem has been studied in many different communities. In statistics, this is the classic minimum volume estimator introduced by Rousseeuw as a robust non-parametric estimator of location and scatter. However in high dimensions, it becomes NP-hard to obtain any non-trivial approximation factor in volume when the condition number $\beta$ of the ellipsoid (ratio of the largest to the smallest axis length) goes to $\infty$. This motivates the focus of our paper: can we efficiently find confidence ellipsoids with volume approximation guarantees when compared to ellipsoids of bounded condition number $\beta$?
  Our main result is a polynomial time algorithm that finds an ellipsoid $E$ whose volume is within a $O(\beta^{\gamma d})$ multiplicative factor of the volume of best $\beta$-conditioned ellipsoid while covering at least $1-O(\alpha/\gamma)$ probability mass for any $\gamma &lt; \alpha$. We complement this with a computational hardness result that shows that such a dependence seems necessary up to constants in the exponent. The algorithm and analysis uses the rich primal-dual structure of the minimum volume enclosing ellipsoid and the geometric Brascamp-Lieb inequality. As a consequence, we obtain the first polynomial time algorithm with approximation guarantees on worst-case instances of the robust subspace recovery problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16875v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Gao, Liren Shan, Vaidehi Srinivas, Aravindan Vijayaraghavan</dc:creator>
    </item>
    <item>
      <title>Make the most of what you have: Resource-efficient randomized algorithms for matrix computations</title>
      <link>https://arxiv.org/abs/2512.15929</link>
      <description>arXiv:2512.15929v1 Announce Type: cross 
Abstract: In recent years, randomized algorithms have established themselves as fundamental tools in computational linear algebra, with applications in scientific computing, machine learning, and quantum information science. Many randomized matrix algorithms proceed by first collecting information about a matrix and then processing that data to perform some computational task. This thesis addresses the following question: How can one design algorithms that use this information as efficiently as possible, reliably achieving the greatest possible speed and accuracy for a limited data budget?
  The first part of this thesis focuses on low-rank approximation for positive-semidefinite matrices. Here, the goal is to compute an accurate approximation to a matrix after accessing as few entries of the matrix as possible. This part of the thesis explores the randomly pivoted Cholesky (RPCholesky) algorithm for this task, which achieves a level of speed and reliability greater than other methods for the same problem.
  The second part of this thesis considers the task of estimating attributes of an implicit matrix accessible only by matrix-vector products. This thesis describes the leave-one-out approach to developing matrix attribute estimation algorithms and develops optimized trace, diagonal, and row-norm estimation algorithms.
  The third part of this thesis considers randomized algorithms for overdetermined linear least squares problems. Randomized algorithms for linear-least squares problems are asymptotically faster than any known deterministic algorithm, but recent work has raised questions about the accuracy of these methods in floating point arithmetic. This thesis shows these issues are resolvable by developing fast randomized least-squares problem achieving backward stability, the gold-standard stability guarantee for a numerical algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15929v1</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.7907/pef3-mg80</arxiv:DOI>
      <dc:creator>Ethan N. Epperly</dc:creator>
    </item>
    <item>
      <title>Provably Extracting the Features from a General Superposition</title>
      <link>https://arxiv.org/abs/2512.15987</link>
      <description>arXiv:2512.15987v1 Announce Type: cross 
Abstract: It is widely believed that complex machine learning models generally encode features through linear representations, but these features exist in superposition, making them challenging to recover. We study the following fundamental setting for learning features in superposition from black-box query access: we are given query access to a function \[ f(x)=\sum_{i=1}^n a_i\,\sigma_i(v_i^\top x), \] where each unit vector $v_i$ encodes a feature direction and $\sigma_i:\mathbb{R} \rightarrow \mathbb{R}$ is an arbitrary response function and our goal is to recover the $v_i$ and the function $f$.
  In learning-theoretic terms, superposition refers to the overcomplete regime, when the number of features is larger than the underlying dimension (i.e. $n &gt; d$), which has proven especially challenging for typical algorithmic approaches. Our main result is an efficient query algorithm that, from noisy oracle access to $f$, identifies all feature directions whose responses are non-degenerate and reconstructs the function $f$. Crucially, our algorithm works in a significantly more general setting than all related prior results -- we allow for essentially arbitrary superpositions, only requiring that $v_i, v_j$ are not nearly identical for $i \neq j$, and general response functions $\sigma_i$. At a high level, our algorithm introduces an approach for searching in Fourier space by iteratively refining the search space to locate the hidden directions $v_i$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15987v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allen Liu</dc:creator>
    </item>
    <item>
      <title>Prefix Sums via Kronecker Products</title>
      <link>https://arxiv.org/abs/2512.16309</link>
      <description>arXiv:2512.16309v1 Announce Type: cross 
Abstract: In this work, we revisit prefix sums through the lens of linear algebra. We describe an identity that decomposes triangular all-ones matrices as a sum of two Kronecker products, and apply it to design recursive prefix sum algorithms and circuits. Notably, the proposed family of circuits is the first one that achieves the following three properties simultaneously: (i) zero-deficiency, (ii) constant fan-out per-level, and (iii) depth that is asymptotically strictly smaller than $2\log(n)$ for input length n. As an application, we show how to use these circuits to design quantum adders with $1.893\log(n) + O(1)$ Toffoli depth, $O(n)$ Toffoli gates, and $O(n)$ additional qubits, improving the Toffoli depth and/or Toffoli size of existing constructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16309v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleksandros Sobczyk, Anastasios Zouzias</dc:creator>
    </item>
    <item>
      <title>FPT Parameterisations of Fractional and Generalised Hypertree Width</title>
      <link>https://arxiv.org/abs/2507.11080</link>
      <description>arXiv:2507.11080v2 Announce Type: replace 
Abstract: We present the first fixed-parameter tractable (FPT) algorithms for exact computation of generalized hypertree width (ghw) and fractional hypertree width (fhw). Our algorithms are parameterized by the target width, the rank, and the maximum degree of the input hypergraph. More generally, we show that testing f-width is in FPT for a broad class of width functions that we call manageable. This class contains the edge cover number $\rho$ and its fractional relaxation $\rho^*$, and thus covers both generalized and fractional hypertree width. We additionally extend our framework to also obtain an fpt algorithm for computing a discretized version of adaptive width. Our approach extends a recent algorithm for treewidth (Boja\'ncyk &amp; Pilipczuk, LMCS 2022) that utilises monadic second-order transductions. To extend this idea beyond treewidth we develop new combinatorial machinery around elimination forests in hypergraphs, culminating in a structural normal form for optimal witnesses that makes transduction-based optimisation applicable in the much more general context of manageable width functions. This yields the first exact FPT algorithms for these measures under any nontrivial parameterisation and provides structural tools that may enable more direct optimisation algorithms</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11080v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Lanzinger, Igor Razgon, Daniel Unterberger</dc:creator>
    </item>
    <item>
      <title>A 13/6-Approximation for Strip Packing via the Bottom-Left Algorithm</title>
      <link>https://arxiv.org/abs/2509.04654</link>
      <description>arXiv:2509.04654v3 Announce Type: replace 
Abstract: In the Strip Packing problem, we are given a vertical strip of fixed width and unbounded height, along with a set of axis-parallel rectangles. The task is to place all rectangles within the strip, without overlaps, while minimizing the height of the packing. This problem is known to be NP-hard. The Bottom-Left Algorithm is a simple and widely used heuristic for Strip Packing. Given a fixed order of the rectangles, it places them one by one, always choosing the lowest feasible position in the strip and, in case of ties, the leftmost one. Baker, Coffman, and Rivest proved in 1980 that the Bottom-Left Algorithm has approximation ratio 3 if the rectangles are sorted by decreasing width. For the past 45 years, no alternative ordering has been found that improves this bound. We introduce a new rectangle ordering and show that with this ordering the Bottom-Left Algorithm achieves a 13/6 approximation for the Strip Packing problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04654v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefan Hougardy, Bart Zondervan</dc:creator>
    </item>
    <item>
      <title>Zero-Freeness is All You Need: A Weitz-Type FPTAS for the Entire Lee-Yang Zero-Free Region</title>
      <link>https://arxiv.org/abs/2509.06623</link>
      <description>arXiv:2509.06623v2 Announce Type: replace 
Abstract: We present a Weitz-type FPTAS for the ferromagnetic Ising model across the entire Lee--Yang zero-free region, without relying on the strong spatial mixing (SSM) property. Our algorithm is Weitz-type for two reasons. First, it expresses the partition function as a telescoping product of ratios, with the key being to approximate each ratio. Second, it uses Weitz's self-avoiding walk tree, and truncates it at logarithmic depth to give a good and efficient approximation. The key difference from the standard Weitz algorithm is that we approximate a carefully designed edge-deletion ratio instead of the marginal probability of a vertex being assigned a particular spin, ensuring our algorithm does not require SSM.
  Furthermore, by establishing local dependence of coefficients (LDC), we prove a novel form of SSM for these edge-deletion ratios, which, in turn, implies the standard SSM for the random cluster model. This is the first SSM result for the random cluster model on general graphs, beyond lattices. Our proof of LDC is based on a new divisibility relation, and we show such relations hold quite universally. This leads to a broadly applicable framework for proving LDC across a variety of models, including the Potts model, the hypergraph independence polynomial, and Holant problems. Combined with existing zero-freeness results for these models, we derive new SSM results for them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06623v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuai Shao, Ke Shi</dc:creator>
    </item>
    <item>
      <title>On the Parameterized Complexity of Grundy Domination and Zero Forcing Problems</title>
      <link>https://arxiv.org/abs/2508.18104</link>
      <description>arXiv:2508.18104v3 Announce Type: replace-cross 
Abstract: We consider two different problem families that deal with domination in graphs. On the one hand, we focus on dominating sequences. In such a sequence, every vertex dominates some vertex of the graph that was not dominated by any earlier vertex in the sequence. The problem of finding the longest dominating sequence is known as $\mathsf{Grundy~Domination}$. Depending on whether the closed or the open neighborhoods are used for domination, there are three other versions of this problem: $\mathsf{Grundy~Total~Domination}$, $\mathsf{L\text{-}Grundy~Domination}$, and $\mathsf{Z\text{-}Grundy~Domination}$. We show that all four problem variants are $\mathsf{W[1]}$-complete when parameterized by the solution size.
  On the other hand, we consider the family of zero forcing problems which form the parametric duals of the Grundy domination problems. In these problems, one looks for the smallest set of vertices initially colored blue such that certain color change rules are able to color all other vertices blue. Bhyravarapu et al. [IWOCA 2025] showed that the dual of $\mathsf{Z\text{-}Grundy~Domination}$, known as $\mathsf{Zero~Forcing~Set}$, is in $\mathsf{FPT}$ when parameterized by the treewidth or the solution size. We extend their treewidth result to the other three variants of zero forcing and their respective Grundy domination problems. Our algorithm also implies an $\mathsf{FPT}$ algorithm for $\mathsf{Grundy~Domination}$ when parameterized by the number of vertices that are not in the dominating sequence. In contrast, we show that $\mathsf{L\text{-}Grundy~Domination}$ is $\mathsf{W[1]}$-hard for that parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18104v3</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Scheffler</dc:creator>
    </item>
  </channel>
</rss>
