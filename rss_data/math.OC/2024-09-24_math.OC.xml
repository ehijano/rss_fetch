<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 04:01:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>$L^q$ approximate controllability frequency criterion for linear difference delay equations with distributed delays</title>
      <link>https://arxiv.org/abs/2409.13777</link>
      <description>arXiv:2409.13777v1 Announce Type: new 
Abstract: Based on an algebraic point of view and the realization theory developed by Y. Yamamoto, the present paper states a necessary and sufficient criterion, given in the frequency domain, for the $L^q$ approximate controllability in finite time of linear difference delay equations with distributed delays. Furthermore, an upper bound for the minimal time of the $L^q$ approximate controllability is obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13777v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'ebastien Fueyo</dc:creator>
    </item>
    <item>
      <title>Convergence analysis of primal-dual augmented Lagrangian methods and duality theory</title>
      <link>https://arxiv.org/abs/2409.13974</link>
      <description>arXiv:2409.13974v1 Announce Type: new 
Abstract: We develop a unified theory of augmented Lagrangians for nonconvex optimization problems that encompasses both duality theory and convergence analysis of primal-dual augmented Lagrangian methods in the infinite dimensional setting. Our goal is to present many well-known concepts and results related to augmented Lagrangians in a unified manner and bridge a gap between existing convergence analysis of primal-dual augmented Lagrangian methods and abstract duality theory. Within our theory we specifically emphasize the role of various fundamental duality concepts (such as duality gap, optimal dual solutions, global saddle points, etc.) in convergence analysis of augmented Lagrangians methods and underline interconnections between all these concepts and convergence of primal and dual sequences generated by such methods. In particular, we prove that the zero duality gap property is a necessary condition for the boundedness of the primal sequence, while the existence of an optimal dual solution is a necessary condition for the boundedness of the sequences of multipliers and penalty parameters, irrespective of the way in which the multipliers and the penalty parameter are updated. Our theoretical results are applicable to many different augmented Lagrangians for various types of cone constrained optimization problems, including Rockafellar-Wets' augmented Lagrangian, (penalized) exponential/hyperbolic-type augmented Lagrangians, modified barrier functions, etc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13974v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. V. Dolgopolik</dc:creator>
    </item>
    <item>
      <title>Quantitative convergence for mean field control with common noise and degenerate idiosyncratic noise</title>
      <link>https://arxiv.org/abs/2409.14053</link>
      <description>arXiv:2409.14053v1 Announce Type: new 
Abstract: We consider the convergence problem in the setting of mean field control with common noise and degenerate idiosyncratic noise. Our main results establish a rate of convergence of the finite-dimensional value functions $V^N$ towards the mean field value function $U$. In the case that the idiosyncratic noise is constant (but possibly degenerate), we obtain the rate $N^{-1/(d+7)}$, which is close to the conjectured optimal rate $N^{-1/d}$, and improves on the existing literature even in the non-degenerate setting. In the case that the idiosyncratic noise can be both non-constant and degenerate, the argument is more complicated, and we instead find the rate $N^{-1/(3d + 19)}$. Our proof strategy builds on the one initiated in [Daudin, Delarue, Jackson - JFA, 2024] in the case of non-degenerate idiosyncratic noise and zero common noise, which consists of approximating $U$ by more regular functions which are almost subsolutions of the infinite-dimensional Hamilton-Jacobi equation solved by $U$. Because of the different noise structure, several new steps are necessary in order to produce an appropriate mollification scheme. In addition to our main convergence results, we investigate the case of zero idiosyncratic noise, and show that sharper results can be obtained there by purely control-theoretic arguments. We also provide examples to demonstrate that the value function is sensitive to the choice of admissible controls in the zero noise setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14053v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alekos Cecchin, Samuel Daudin, Joe Jackson, Mattia Martini</dc:creator>
    </item>
    <item>
      <title>Accelerated Stochastic ExtraGradient: Mixing Hessian and Gradient Similarity to Reduce Communication in Distributed and Federated Learning</title>
      <link>https://arxiv.org/abs/2409.14280</link>
      <description>arXiv:2409.14280v1 Announce Type: new 
Abstract: Modern realities and trends in learning require more and more generalization ability of models, which leads to an increase in both models and training sample size. It is already difficult to solve such tasks in a single device mode. This is the reason why distributed and federated learning approaches are becoming more popular every day. Distributed computing involves communication between devices, which requires solving two key problems: efficiency and privacy. One of the most well-known approaches to combat communication costs is to exploit the similarity of local data. Both Hessian similarity and homogeneous gradients have been studied in the literature, but separately. In this paper, we combine both of these assumptions in analyzing a new method that incorporates the ideas of using data similarity and clients sampling. Moreover, to address privacy concerns, we apply the technique of additional noise and analyze its impact on the convergence of the proposed method. The theory is confirmed by training on real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14280v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitry Bylinkin, Kirill Degtyarev, Aleksandr Beznosikov</dc:creator>
    </item>
    <item>
      <title>A Trust Region Method with Regularized Barzilai-Borwein Step-Size for Large-Scale Unconstrained Optimization</title>
      <link>https://arxiv.org/abs/2409.14383</link>
      <description>arXiv:2409.14383v1 Announce Type: new 
Abstract: We develop a Trust Region method with Regularized Barzilai-Borwein step-size obtained in a previous paper for solving large-scale unconstrained optimization problems. Simultaneously, the non-monotone technique is combined to formulate an efficient trust region method. The proposed method adaptively generates a suitable step-size within the trust region. The minimizer of the resulted model can be easily determined, and at the same time, the convergence of the algorithm is also maintained. Numerical results are presented to support the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14383v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Xu, Congpei An</dc:creator>
    </item>
    <item>
      <title>An Operator Learning Approach to Nonsmooth Optimal Control of Nonlinear PDEs</title>
      <link>https://arxiv.org/abs/2409.14417</link>
      <description>arXiv:2409.14417v1 Announce Type: new 
Abstract: Optimal control problems with nonsmooth objectives and nonlinear partial differential equation (PDE) constraints are challenging, mainly because of the underlying nonsmooth and nonconvex structures and the demanding computational cost for solving multiple high-dimensional and ill-conditioned systems after mesh-based discretization. To mitigate these challenges numerically, we propose an operator learning approach in combination with an effective primal-dual optimization idea which can decouple the treatment of the control and state variables so that each of the resulting iterations only requires solving two PDEs. Our main purpose is to construct neural surrogate models for the involved PDEs by operator learning, allowing the solution of a PDE to be obtained with only a forward pass of the neural network. The resulting algorithmic framework offers a hybrid approach that combines the flexibility and generalization of operator learning with the model-based nature and structure-friendly efficiency of primal-dual-based algorithms. The primal-dual-based operator learning approach offers numerical methods that are mesh-free, easy to implement, and adaptable to various optimal control problems with nonlinear PDEs. It is notable that the neural surrogate models can be reused across iterations and parameter settings, hence computational cost can be substantially alleviated. We validate the effectiveness and efficiency of the primal-dual-based operator learning approach across a range of typical optimal control problems with nonlinear PDEs, including optimal control of stationary Burgers equations, sparse bilinear control of parabolic equations, and optimal control of semilinear parabolic equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14417v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongcun Song, Xiaoming Yuan, Hangrui Yue, Tianyou Zeng</dc:creator>
    </item>
    <item>
      <title>Visualizing Shape Functionals via Sinkhorn Multidimensional Scaling</title>
      <link>https://arxiv.org/abs/2409.14687</link>
      <description>arXiv:2409.14687v1 Announce Type: new 
Abstract: In this paper, we present Sinkhorn multidimensional scaling (Sinkhorn MDS) as a method for visualizing shape functionals in shape spaces. This approach uses the Sinkhorn divergence to map these infinite-dimensional spaces into lower-dimensional Euclidean spaces. We establish error estimates for the embedding generated by Sinkhorn MDS compared to the unregularized case. Moreover, we validate the method through numerical experiments, including visualizations of the classical Dido's problem and two newly introduced shape functionals: the double-well and Sinkhorn cone-type shape functionals. Our results demonstrate that Sinkhorn MDS effectively captures and visualizes shapes of shape functionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14687v1</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toshiaki Yachimura, Jun Okamoto, Lorenzo Cavallina</dc:creator>
    </item>
    <item>
      <title>A New Crossover Algorithm for LP Inspired by the Spiral Dynamic of PDHG</title>
      <link>https://arxiv.org/abs/2409.14715</link>
      <description>arXiv:2409.14715v1 Announce Type: new 
Abstract: Motivated by large-scale applications, there is a recent trend of research on using first-order methods for solving LP. Among them, PDLP, which is based on a primal-dual hybrid gradient (PDHG) algorithm, may be the most promising one. In this paper, we present a geometric viewpoint on the behavior of PDHG for LP. We demonstrate that PDHG iterates exhibit a spiral pattern with a closed-form solution when the variable basis remains unchanged. This spiral pattern consists of two orthogonal components: rotation and forward movement, where rotation improves primal and dual feasibility, while forward movement advances the duality gap. We also characterize the different situations in which basis change events occur. Inspired by the spiral behavior of PDHG, we design a new crossover algorithm to obtain a vertex solution from any optimal LP solution. This approach differs from traditional simplex-based crossover methods. Our numerical experiments demonstrate the effectiveness of the proposed algorithm, showcasing its potential as an alternative option for crossover.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14715v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianhao Liu, Haihao Lu</dc:creator>
    </item>
    <item>
      <title>An enhanced and more realistic tank environment setup for the development of new methods for fish behavioral analysis in aquaculture</title>
      <link>https://arxiv.org/abs/2409.14730</link>
      <description>arXiv:2409.14730v1 Announce Type: new 
Abstract: The aquaculture industry is constantly making efforts to improve fish welfare while maintaining the ethically sustainable farming practises. This work presents an enhanced tank environment designed for testing and developing novel combinations of technologies for analyzing and detecting behavioral responses in fish shoals/groups. Regular cameras are combined with event cameras and a scanning sonar to comprise a sensor suite that offers a more detailed and complex way of fish observation. The modified tank environment is designed to simulate the prevailing conditions on-site at cage based farms, particularly in terms of lighting conditions, while all tank systems and sensors are hidden behind specially designed enclosures, providing a "clean" environment (open arena) less likely to impact the fish behavior. The proposed sensor suite will be tested and demonstrated in the modified tank environment to benchmark its ability in monitoring fish, after which it will be adapted for use in a more industrially relevant situation with open cages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14730v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitris Voskakis, Martin F{\o}re, Eirik Svendsen, Aleksander Perlic Liland, Sonia Rey Planellas, Harkaitz Eguiraun, Pascal Klebert</dc:creator>
    </item>
    <item>
      <title>A Bi-criterion Steiner Traveling Salesperson Problem with Time Windows for Last-Mile Electric Vehicle Logistics</title>
      <link>https://arxiv.org/abs/2409.14848</link>
      <description>arXiv:2409.14848v1 Announce Type: new 
Abstract: This paper addresses the problem of energy-efficient and safe routing of last-mile electric freight vehicles. With the rising environmental footprint of the transportation sector and the growing popularity of E-Commerce, freight companies are likely to benefit from optimal time-window-feasible tours that minimize energy usage while reducing traffic conflicts at intersections and thereby improving safety. We formulate this problem as a Bi-criterion Steiner Traveling Salesperson Problem with Time Windows (BSTSPTW) with energy consumed and the number of left turns at intersections as the two objectives while also considering regenerative braking capabilities. We first discuss an exact mixed-integer programming model with scalarization to enumerate points on the efficiency frontier for small instances. For larger networks, we develop an efficient local search-based heuristic, which uses several operators to intensify and diversify the search process. We demonstrate the utility of the proposed methods using benchmark data and real-world instances from Amazon delivery routes in Austin, US. Comparisons with state-of-the-art solvers shows that our heuristics can generate near-optimal solutions within reasonable time budgets, effectively balancing energy efficiency and safety under practical delivery constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14848v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <category>cs.DM</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prateek Agarwal, Debojjal Bagchi, Tarun Rambha, Venktesh Pandey</dc:creator>
    </item>
    <item>
      <title>Optimal state estimation: Turnpike analysis and performance results</title>
      <link>https://arxiv.org/abs/2409.14873</link>
      <description>arXiv:2409.14873v1 Announce Type: new 
Abstract: In this paper, we introduce turnpike arguments in the context of optimal state estimation. In particular, we show that the optimal solution of the state estimation problem involving all available past data serves as turnpike for the solutions of truncated problems involving only a subset of the data. We consider two different mathematical characterizations of this phenomenon and provide corresponding sufficient conditions that rely on strict dissipativity and decaying sensitivity. As second contribution, we show how a specific turnpike property can be used to establish performance guarantees when approximating the optimal solution of the full problem by a sequence of truncated problems, and we show that the resulting performance (both averaged and non-averaged) is approximately optimal with error terms that can be made arbitrarily small by an appropriate choice of the horizon length. In addition, we discuss interesting implications of these results for the practically relevant case of moving horizon estimation and illustrate our results with a numerical example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14873v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julian D. Schiller, Lars Gr\"une, Matthias A. M\"uller</dc:creator>
    </item>
    <item>
      <title>Outer Approximation Scheme for Weakly Convex Constrained Optimization Problems</title>
      <link>https://arxiv.org/abs/2409.14895</link>
      <description>arXiv:2409.14895v1 Announce Type: new 
Abstract: Outer approximation methods have long been employed to tackle a variety of optimization problems, including linear programming, in the 1960s, and continue to be effective for solving variational inequalities, general convex problems, as well as mixed-integer linear, and nonlinear programming problems.
  In this work, we introduce a novel outer approximation scheme specifically designed for solving weakly convex constrained optimization problems. The key idea lies in utilizing quadratic cuts, rather than the traditional linear cuts, and solving an outer approximation problem at each iteration in the form of a Quadratically Constrained Quadratic Programming (QCQP) problem.
  The primary result demonstrated in this work is that every convergent subsequence generated by the proposed outer approximation scheme converges to a global minimizer of the general weakly convex optimization problem under consideration. To enhance the practical implementation of this method, we also propose two variants of the algorithm.
  The efficacy of our approach is illustrated through its application to two distinct problems: the circular packing problem and the Neyman-Pearson classification problem, both of which are reformulated within the framework of weakly convex constrained optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14895v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ewa M. Bednarczuk, Giovanni Bruccola, Jean-Christophe Pesquet, Krzysztof Rutkowski</dc:creator>
    </item>
    <item>
      <title>Continuation Method for Nonsmooth Model Predictive Control Using Proximal Technique</title>
      <link>https://arxiv.org/abs/2409.14944</link>
      <description>arXiv:2409.14944v1 Announce Type: new 
Abstract: This paper presents a novel framework for the continuation method of model predictive control based on optimal control problem with a nonsmooth regularizer. Via the proximal operator, the first-order optimality inclusion relation is reformulated into an equation system, to which the continuation method is applicable. In addition, we present constraint qualifications that ensure the well-posedness of the proposed equation system. A numerical example is also presented that demonstrates the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14944v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryotaro Shima, Ryuta Moriyasu, Teruki Kato</dc:creator>
    </item>
    <item>
      <title>A single-loop proximal-conditional-gradient penalty method</title>
      <link>https://arxiv.org/abs/2409.14957</link>
      <description>arXiv:2409.14957v1 Announce Type: new 
Abstract: We consider the problem of minimizing a convex separable objective (as a separable sum of two proper closed convex functions $f$ and $g$) over a linear coupling constraint. We assume that $f$ can be decomposed as the sum of a smooth part having H\"older continuous gradient (with exponent $\mu\in(0,1]$) and a nonsmooth part that admits efficient proximal mapping computations, while $g$ can be decomposed as the sum of a smooth part having H\"older continuous gradient (with exponent $\nu\in(0,1]$) and a nonsmooth part that admits efficient linear oracles. Motivated by the recent work [41], we propose a single-loop variant of the standard penalty method, which we call a single-loop proximal-conditional-gradient penalty method ($proxCG_{1\ell}^{pen}$), for this problem. In each iteration, we successively perform one proximal-gradient step involving $f$ and one conditional-gradient step involving $g$ on the quadratic penalty function, followed by an update of the penalty parameter. We present explicit rules for updating the penalty parameter and the stepsize in the conditional-gradient step. Under a standard constraint qualification and domain boundedness assumption, we show that the objective value deviations (from the optimal value) along the sequence generated decay in the order of $t^{-\min\{\mu,\nu,1/2\}}$ with the associated feasibility violations decaying in the order of $t^{-1/2}$. Moreover, if the nonsmooth parts are indicator functions and the extended objective is a Kurdyka-Lojasiewicz function with exponent $\alpha\in [0,1)$, then the distances to the optimal solution set along the sequence generated by $proxCG_{1\ell}^{pen}$ decay asymptotically at a rate of $t^{-(1-\alpha)\min\{\mu,\nu,1/2\}}$. Finally, we illustrate numerically the convergence behavior of $proxCG_{1\ell}^{pen}$ on minimizing the $\ell_1$ norm subject to a residual error measured by $\ell_p$ norm, $p\in(1,2]$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14957v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Zhang, Liaoyuan Zeng, Ting Kei Pong</dc:creator>
    </item>
    <item>
      <title>Methods for Convex $(L_0,L_1)$-Smooth Optimization: Clipping, Acceleration, and Adaptivity</title>
      <link>https://arxiv.org/abs/2409.14989</link>
      <description>arXiv:2409.14989v1 Announce Type: new 
Abstract: Due to the non-smoothness of optimization problems in Machine Learning, generalized smoothness assumptions have been gaining a lot of attention in recent years. One of the most popular assumptions of this type is $(L_0,L_1)$-smoothness (Zhang et al., 2020). In this paper, we focus on the class of (strongly) convex $(L_0,L_1)$-smooth functions and derive new convergence guarantees for several existing methods. In particular, we derive improved convergence rates for Gradient Descent with (Smoothed) Gradient Clipping and for Gradient Descent with Polyak Stepsizes. In contrast to the existing results, our rates do not rely on the standard smoothness assumption and do not suffer from the exponential dependency from the initial distance to the solution. We also extend these results to the stochastic case under the over-parameterization assumption, propose a new accelerated method for convex $(L_0,L_1)$-smooth optimization, and derive new convergence rates for Adaptive Gradient Descent (Malitsky and Mishchenko, 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14989v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduard Gorbunov, Nazarii Tupitsa, Sayantan Choudhury, Alen Aliev, Peter Richt\'arik, Samuel Horv\'ath, Martin Tak\'a\v{c}</dc:creator>
    </item>
    <item>
      <title>Necessary conditions for the optimal control of a shape optimization problem with non-smooth PDE constraints</title>
      <link>https://arxiv.org/abs/2409.15039</link>
      <description>arXiv:2409.15039v1 Announce Type: new 
Abstract: This paper is concerned with the derivation of necessary conditions for the optimal shape of a design problem governed by a non-smooth PDE. The main particularity thereof is the lack of differentiability of the nonlinearity in the state equation, which, at the same time, is solved on an unknown domain. We follow the functional variational approach introduced in [37] where the set of admissible shapes is parametrized by a large class of continuous mappings. It has been recently established [4] that each parametrization associated to an optimal shape is the limit of a sequence of global optima of minimization problems with convex admissible set consisting of functions. Though non-smooth, these problems allow for the derivation of an optimality system equivalent with the first order necessary optimality condition [5]. In the present manuscript we let the approximation parameter vanish therein. The final necessary conditions for the non-smooth shape optimization problem consist of an adjoint equation, a limit gradient equation that features a measure concentrated on the boundary of the optimal shape and, because of the non-smoothness, an inclusion that involves its Clarke subdifferential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15039v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Livia Betz</dc:creator>
    </item>
    <item>
      <title>Learning in memristive electrical circuits</title>
      <link>https://arxiv.org/abs/2409.15136</link>
      <description>arXiv:2409.15136v1 Announce Type: new 
Abstract: Memristors are nonlinear two-terminal circuit elements whose resistance at a given time depends on past electrical stimuli. Recently, networks of memristors have received attention in neuromorphic computing since they can be used as a tool to perform linear algebraic operations, like matrix-vector multiplication, directly in hardware. In this paper, the aim is to resolve two fundamental questions pertaining to a specific, but relevant, class of memristive circuits called crossbar arrays. In particular, we show (1) how the resistance values of the memristors at a given time can be determined from external (voltage and current) measurements, and (2) how the resistances can be steered to desired values by applying suitable external voltages to the network. The results will be applied to solve a prototypical learning problem, namely linear least squares, by applying and measuring voltages and currents in a suitable memristive circuit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15136v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marieke Heidema, Henk van Waarde, Bart Besselink</dc:creator>
    </item>
    <item>
      <title>Modeling a demographic problem using the Leslie matrix</title>
      <link>https://arxiv.org/abs/2409.15147</link>
      <description>arXiv:2409.15147v1 Announce Type: new 
Abstract: The application of Leslie matrices in demographic research is considered in this paper. The Leslie matrix is first proposed in the 1940s and gained popularity in the mid-1960s, becoming fundamental tool for predicting population dynamics. The Leslie matrix allows to categorize individuals based on various attributes and calculate the expected population sizes for various demographic categories in subsequent time intervals. The universality of the Leslie matrix extends to diverse life cycles in plants and animals, making it ubiquitous tool in non-human species. In the paper is presented detailed application of Leslie matrices to the problem of the two countries, demonstrating their practical value in solving real demographic problems. In conclusion, the Leslie matrix remains a cornerstone of demographic analysis, reflecting the complexity of population dynamics and providing a robust framework for understanding the intricate interplay of factors shaping human society. Its enduring relevance and adaptability make it an essential component in the toolkit of demographers and ecologists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15147v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>O. A. Malafeyev, T. R. Nabiev, N. D. Redinskikh</dc:creator>
    </item>
    <item>
      <title>A constrained optimization approach to improve robustness of neural networks</title>
      <link>https://arxiv.org/abs/2409.13770</link>
      <description>arXiv:2409.13770v1 Announce Type: cross 
Abstract: In this paper, we present a novel nonlinear programming-based approach to fine-tune pre-trained neural networks to improve robustness against adversarial attacks while maintaining high accuracy on clean data. Our method introduces adversary-correction constraints to ensure correct classification of adversarial data and minimizes changes to the model parameters. We propose an efficient cutting-plane-based algorithm to iteratively solve the large-scale nonconvex optimization problem by approximating the feasible region through polyhedral cuts and balancing between robustness and accuracy. Computational experiments on standard datasets such as MNIST and CIFAR10 demonstrate that the proposed approach significantly improves robustness, even with a very small set of adversarial data, while maintaining minimal impact on accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13770v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shudian Zhao, Jan Kronqvist</dc:creator>
    </item>
    <item>
      <title>Optimality of a barrier strategy in a spectrally negative L\'evy model with a level-dependent intensity of bankruptcy</title>
      <link>https://arxiv.org/abs/2409.13849</link>
      <description>arXiv:2409.13849v1 Announce Type: cross 
Abstract: We consider de Finetti's stochastic control problem for a spectrally negative L\'evy process in an Omega model. In such a model, the (controlled) process is allowed to spend time under the critical level but is then subject to a level-dependent intensity of bankruptcy. First, before considering the control problem, we derive some analytical properties of the corresponding Omega scale functions. Second, we prove that exists a barrier strategy that is optimal for this control problem under a mild assumption on the L\'evy measure. Finally, we analyse numerically the impact of the bankruptcy rate function on the optimal strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13849v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dante Mata, Jean-Fran\c{c}ois Renaud</dc:creator>
    </item>
    <item>
      <title>QoS-Aware and Routing-Flexible Network Slicing for Service-Oriented Networks</title>
      <link>https://arxiv.org/abs/2409.13943</link>
      <description>arXiv:2409.13943v1 Announce Type: cross 
Abstract: In this paper, we consider the network slicing (NS) problem which attempts to map multiple customized virtual network requests (also called services) to a common shared network infrastructure and manage network resources to meet diverse quality of service (QoS) requirements. We propose a mixed-integer nonlinear programming (MINLP) formulation for the considered NS problem that can flexibly route the traffic flow of the services on multiple paths and provide end-to-end delay and reliability guarantees for all services. To overcome the computational difficulty due to the intrinsic nonlinearity in the MINLP formulation, we transform the MINLP formulation into an equivalent mixed-integer linear programming (MILP) formulation and further show that their continuous relaxations are equivalent. In sharp contrast to the continuous relaxation of the MINLP formulation which is a nonconvex nonlinear programming problem, the continuous relaxation of the MILP formulation is a polynomial-time solvable linear programming problem, which significantly facilitates the algorithmic design. Based on the newly proposed MILP formulation, we develop a customized column generation (cCG) algorithm for solving the NS problem. The proposed cCG algorithm is a decomposition-based algorithm and is particularly suitable for solving large-scale NS problems. Numerical results demonstrate the efficacy of the proposed formulations and the proposed cCG algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13943v1</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei-Kun Chen, Ya-Feng Liu, Yu-Hong Dai, Zhi-Quan Luo</dc:creator>
    </item>
    <item>
      <title>FracGM: A Fast Fractional Programming Technique for Geman-McClure Robust Estimator</title>
      <link>https://arxiv.org/abs/2409.13978</link>
      <description>arXiv:2409.13978v1 Announce Type: cross 
Abstract: Robust estimation is essential in computer vision, robotics, and navigation, aiming to minimize the impact of outlier measurements for improved accuracy. We present a fast algorithm for Geman-McClure robust estimation, FracGM, leveraging fractional programming techniques. This solver reformulates the original non-convex fractional problem to a convex dual problem and a linear equation system, iteratively solving them in an alternating optimization pattern. Compared to graduated non-convexity approaches, this strategy exhibits a faster convergence rate and better outlier rejection capability. In addition, the global optimality of the proposed solver can be guaranteed under given conditions. We demonstrate the proposed FracGM solver with Wahba's rotation problem and 3-D point-cloud registration along with relaxation pre-processing and projection post-processing. Compared to state-of-the-art algorithms, when the outlier rates increase from 20\% to 80\%, FracGM shows 53\% and 88\% lower rotation and translation increases. In real-world scenarios, FracGM achieves better results in 13 out of 18 outcomes, while having a 19.43\% improvement in the computation time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13978v1</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bang-Shien Chen, Yu-Kai Lin, Jian-Yu Chen, Chih-Wei Huang, Jann-Long Chern, Ching-Cherng Sun</dc:creator>
    </item>
    <item>
      <title>Theory for Optimal Estimation and Control under Resource Limitations and Its Applications to Biological Information Processing and Decision-Making</title>
      <link>https://arxiv.org/abs/2409.14003</link>
      <description>arXiv:2409.14003v1 Announce Type: cross 
Abstract: Despite being optimized, the information processing of biological organisms exhibits significant variability in its complexity and capability. One potential source of this diversity is the limitation of resources required for information processing. However, we lack a theoretical framework that comprehends the relationship between biological information processing and resource limitations and integrates it with decision-making conduced downstream of the information processing. In this paper, we propose a novel optimal estimation and control theory that accounts for the resource limitations inherent in biological systems. This theory explicitly formulates the memory that organisms can store and operate and obtains optimal memory dynamics using optimal control theory. This approach takes account of various resource limitations, such as memory capacity, intrinsic noise, and energy cost, and unifies state estimation and control. We apply this theory to minimal models of biological information processing and decision-making under resource limitations and find that such limitations induce discontinuous and non-monotonic phase transitions between memory-less and memory-based strategies. Therefore, this theory establishes a comprehensive framework for addressing biological information processing and decision-making under resource limitations, revealing the rich and complex behaviors that arise from resource limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14003v1</guid>
      <category>physics.bio-ph</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takehiro Tottori, Tetsuya J. Kobayashi</dc:creator>
    </item>
    <item>
      <title>An Evolutionary Algorithm For the Vehicle Routing Problem with Drones with Interceptions</title>
      <link>https://arxiv.org/abs/2409.14173</link>
      <description>arXiv:2409.14173v1 Announce Type: cross 
Abstract: The use of trucks and drones as a solution to address last-mile delivery challenges is a new and promising research direction explored in this paper. The variation of the problem where the drone can intercept the truck while in movement or at the customer location is part of an optimisation problem called the vehicle routing problem with drones with interception (VRPDi). This paper proposes an evolutionary algorithm to solve the VRPDi. In this variation of the VRPDi, multiple pairs of trucks and drones need to be scheduled. The pairs leave and return to a depot location together or separately to make deliveries to customer nodes. The drone can intercept the truck after the delivery or meet up with the truck at the following customer location. The algorithm was executed on the travelling salesman problem with drones (TSPD) datasets by Bouman et al. (2015), and the performance of the algorithm was compared by benchmarking the results of the VRPDi against the results of the VRP of the same dataset. This comparison showed improvements in total delivery time between 39% and 60%. Further detailed analysis of the algorithm results examined the total delivery time, distance, node delivery scheduling and the degree of diversity during the algorithm execution. This analysis also considered how the algorithm handled the VRPDi constraints. The results of the algorithm were then benchmarked against algorithms in Dillon et al. (2023) and Ernst (2024). The latter solved the problem with a maximum drone distance constraint added to the VRPDi. The analysis and benchmarking of the algorithm results showed that the algorithm satisfactorily solved 50 and 100-nodes problems in a reasonable amount of time, and the solutions found were better than those found by the algorithms in Dillon et al. (2023) and Ernst (2024) for the same problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14173v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Pambo, Jacomine Grobler</dc:creator>
    </item>
    <item>
      <title>Fast Local Search Strategies for Large-Scale General Quadratic Integer Programming</title>
      <link>https://arxiv.org/abs/2409.14176</link>
      <description>arXiv:2409.14176v1 Announce Type: cross 
Abstract: This study investigates the area of general quadratic integer programming (QIP), encompassing both unconstrained (UQIP) and constrained (CQIP) variants. These NP-hard problems have far-reaching applications, yet the non-convex cases have received limited attention in the literature. To address this gap, we introduce a closed-form formula for single-variable changes, establishing novel necessary and sufficient conditions for 1-Opt local improvement in UQIP and CQIP. We develop a simple local and sophisticated tabu search with an oscillation strategy tailored for large-scale problems. Experimental results on instances with up to 8000 variables demonstrate the efficiency of these strategies, producing high-quality solutions within a short time. Our approaches significantly outperform the Gurobi 11.0.2 solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14176v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haibo Wang, Bahram Alidaee</dc:creator>
    </item>
    <item>
      <title>A novel load distribution strategy for aggregators using IoT-enabled mobile devices</title>
      <link>https://arxiv.org/abs/2409.14293</link>
      <description>arXiv:2409.14293v1 Announce Type: cross 
Abstract: The rapid proliferation of Internet-of-things (IoT) as well as mobile devices such as Electric Vehicles (EVs), has led to unpredictable load at the grid. The demand to supply ratio is particularly exacerbated at a few grid aggregators (charging stations) with excessive demand due to the geographic location, peak time, etc. Existing solutions on demand response cannot achieve significant improvements based only on time-shifting the loads without considering the device properties such as charging modes and movement capabilities to enable geographic migration. Additionally, the information on the spare capacity at a few aggregators can aid in re-channeling the load from other aggregators facing excess demand to allow migration of devices. In this paper, we model these flexible properties of the devices as a mixed-integer non-linear problem (MINLP) to minimize excess load and the improve the utility (benefit) across all devices. We propose an online distributed low-complexity heuristic that prioritizes devices based on demand and deadlines to minimize the cumulative loss in utility. The proposed heuristic is tested on an exhaustive set of synthetic data and compared with solutions from a solver/optimization tool for the same runtime to show the impracticality of using a solver. A real-world EV testbed data is also tested with our proposed solution and other scheduling solutions to show the practicality of generating a feasible schedule and a loss improvement of at least 57.23%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14293v1</guid>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>10.1109/SmartGridComm51999.2021.9632317</arxiv:journal_reference>
      <dc:creator>Nitin Shivaraman, Jakob Fittler, Saravanan Ramanathan, Arvind Easwaran, Sebastian Steinhorst</dc:creator>
    </item>
    <item>
      <title>The sparseness of g-convex functions</title>
      <link>https://arxiv.org/abs/2409.14434</link>
      <description>arXiv:2409.14434v1 Announce Type: cross 
Abstract: The g-convexity of functions on manifolds is a generalization of the convexity of functions on Rn. It plays an essential role in both differential geometry and non-convex optimization theory. This paper is concerned with g-convex smooth functions on manifolds. We establish criteria for the existence of a Riemannian metric (or connection) with respect to which a given function is g-convex. Using these criteria, we obtain three sparseness results for g-convex functions: (1) The set of g-convex functions on a compact manifold is nowhere dense in the space of smooth functions. (2) Most polynomials on Rn that is g-convex with respect to some geodesically complete connection has at most one critical point. (3) The density of g-convex univariate (resp. quadratic, monomial, additively separable) polynomials asymptotically decreases to zero</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14434v1</guid>
      <category>math.DG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Wang, Ke Ye</dc:creator>
    </item>
    <item>
      <title>A Review of Scalable and Privacy-Preserving Multi-Agent Frameworks for Distributed Energy Resource Control</title>
      <link>https://arxiv.org/abs/2409.14499</link>
      <description>arXiv:2409.14499v1 Announce Type: cross 
Abstract: Distributed energy resources (DERs) are gaining prominence due to their advantages in improving energy efficiency, reducing carbon emissions, and enhancing grid resilience. Despite the increasing deployment, the potential of DERs has yet to be fully explored and exploited. A fundamental question restrains the management of numerous DERs in large-scale power systems, "How should DER data be securely processed and DER operations be efficiently optimized?" To address this question, this paper considers two critical issues, namely privacy for processing DER data and scalability in optimizing DER operations, then surveys existing and emerging solutions from a multi-agent framework perspective. In the context of scalability, this paper reviews state-of-the-art research that relies on parallel control, optimization, and learning within distributed and/or decentralized information exchange structures, while in the context of privacy, it identifies privacy preservation measures that can be synthesized into the aforementioned scalable structures. Despite research advances in these areas, challenges remain because these highly interdisciplinary studies blend a wide variety of scalable computing architectures and privacy preservation techniques from different fields, making them difficult to adapt in practice. To mitigate this issue, this paper provides a holistic review of trending strategies that orchestrate privacy and scalability for large-scale power system operations from a multi-agent perspective, particularly for DER control problems. Furthermore, this review extrapolates new approaches for future scalable, privacy-aware, and cybersecure pathways to unlock the full potential of DERs through controlling, optimizing, and learning generic multi-agent-based cyber-physical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14499v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Huo, Hao Huang, Katherine R. Davis, H. Vincent Poor, Mingxi Liu</dc:creator>
    </item>
    <item>
      <title>BDF schemes for accelerated gradient flows in projection-free approximation of nonconvex constrained variational minimization</title>
      <link>https://arxiv.org/abs/2409.14670</link>
      <description>arXiv:2409.14670v1 Announce Type: cross 
Abstract: We present a set of novel accelerated gradient flow methods for solving quadratic energy minimization problems with nonconvex constraints. Our algorithms are built on novel evolutionary equations that combine projection-free approximations for nonconvex constraints with first- and higher-order backward differentiation formulas (BDFs) for {artificial} temporal derivatives. We focus on examining the asymptotic consistency of constraints achieved by the accelerated gradient flow using the BDF schemes. Both unconditional and conditional high-order estimates for constraint violations in these schemes are established. Numerical results not only validate our theoretical findings but also demonstrate that the proposed methods outperform existing gradient flow approaches in terms of both efficiency and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14670v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guozhi Dong, Zikang Gong, Ziqing Xie, Shuo Yang</dc:creator>
    </item>
    <item>
      <title>Sparsity-Enhanced Multilayered Non-Convex Regularization with Epigraphical Relaxation for Debiased Signal Recovery</title>
      <link>https://arxiv.org/abs/2409.14768</link>
      <description>arXiv:2409.14768v1 Announce Type: cross 
Abstract: This paper proposes a precise signal recovery method with multilayered non-convex regularization, enhancing sparsity/low-rankness for high-dimensional signals including images and videos. In optimization-based signal recovery, multilayered convex regularization functions based on the L1 and nuclear-norms not only guarantee a global optimal solution but also offer more accurate estimation than single-layered ones, thanks to their faithful modeling of structured sparsity and low-rankness in high-dimensional signals. However, these functions are known to yield biased solutions (estimated with smaller amplitude values than the true ones). To address this issue, multilayered non-convex regularization functions have been considered, although they face their own challenges: 1) their closed-form proximity operators are unavailable, and 2) convergence may result in a local optimal solution. In this paper, we resolve the two issues with an approach based on epigraphical relaxation (ER). First, ER decomposes a multilayered non-convex regularization function into the outermost function and epigraph constraints for the inner functions, facilitating the computation of proximity operators. Second, the relaxed regularization functions by ER are integrated into a non-convexly regularized convex optimization model to estimate a global optimal solution with less bias. Numerical experiments demonstrate the bias reduction achieved by the proposed method in image recovery and principal component analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14768v1</guid>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akari Katsuma, Seisuke Kyochi, Shunsuke Ono, Ivan Selesnick</dc:creator>
    </item>
    <item>
      <title>A novel agent with formal goal-reaching guarantees: an experimental study with a mobile robot</title>
      <link>https://arxiv.org/abs/2409.14867</link>
      <description>arXiv:2409.14867v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has been shown to be effective and convenient for a number of tasks in robotics. However, it requires the exploration of a sufficiently large number of state-action pairs, many of which may be unsafe or unimportant. For instance, online model-free learning can be hazardous and inefficient in the absence of guarantees that a certain set of desired states will be reached during an episode. An increasingly common approach to address safety involves the addition of a shielding system that constrains the RL actions to a safe set of actions. In turn, a difficulty for such frameworks is how to effectively couple RL with the shielding system to make sure the exploration is not excessively restricted. This work presents a novel safe model-free RL agent called Critic As Lyapunov Function (CALF) and showcases how CALF can be used to improve upon control baselines in robotics in an efficient and convenient fashion while ensuring guarantees of stable goal reaching. The latter is a crucial part of safety, as seen generally. With CALF all state-action pairs remain explorable and yet reaching of desired goal states is formally guaranteed. Formal analysis is provided that shows the goal stabilization-ensuring properties of CALF and a set of real-world and numerical experiments with a non-holonomic wheeled mobile robot (WMR) TurtleBot3 Burger confirmed the superiority of CALF over such a well-established RL agent as proximal policy optimization (PPO), and a modified version of SARSA in a few-episode setting in terms of attained total cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14867v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigory Yaremenko, Dmitrii Dobriborsci, Roman Zashchitin, Ruben Contreras Maestre, Ngoc Quoc Huy Hoang, Pavel Osinenko</dc:creator>
    </item>
    <item>
      <title>A Contract Theory for Layered Control Architectures</title>
      <link>https://arxiv.org/abs/2409.14902</link>
      <description>arXiv:2409.14902v1 Announce Type: cross 
Abstract: Autonomous systems typically leverage layered control architectures with a combination of discrete and continuous models operating at different timescales. As a result, layered systems form a new class of hybrid systems composed of systems operating on a diverse set of continuous and discrete signals. This paper formalizes the notion of a layered (hierarchical) control architecture through a theory of relations between its layers. This theory enables us to formulate contracts within layered control systems -- these define interfaces between layers and isolate the design of each layer, guaranteeing that composition of contracts at each layer results in a contract capturing the desired system-wide specification. Thus, the proposed theory yields the ability to analyze layered control architectures via a compositional approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14902v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manuel Mazo Jr., Will Compton, Max H. Cohen, Aaron D. Ames</dc:creator>
    </item>
    <item>
      <title>Kinodynamic Motion Planning for Collaborative Object Transportation by Multiple Mobile Manipulators</title>
      <link>https://arxiv.org/abs/2409.14910</link>
      <description>arXiv:2409.14910v1 Announce Type: cross 
Abstract: This work proposes a kinodynamic motion planning technique for collaborative object transportation by multiple mobile manipulators in dynamic environments. A global path planner computes a linear piecewise path from start to goal. A novel algorithm detects the narrow regions between the static obstacles and aids in defining the obstacle-free region to enhance the feasibility of the global path. We then formulate a local online motion planning technique for trajectory generation that minimizes the control efforts in a receding horizon manner. It plans the trajectory for finite time horizons, considering the kinodynamic constraints and the static and dynamic obstacles. The planning technique jointly plans for the mobile bases and the arms to utilize the locomotion capability of the mobile base and the manipulation capability of the arm efficiently. We use a convex cone approach to avoid self-collision of the formation by modifying the mobile manipulators admissible state without imposing additional constraints. Numerical simulations and hardware experiments showcase the efficiency of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14910v1</guid>
      <category>cs.RO</category>
      <category>cs.MA</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keshab Patra, Arpita Sinha, Anirban Guha</dc:creator>
    </item>
    <item>
      <title>Free boundary regularity for a spectral optimal partition problem with volume and inclusion constraints</title>
      <link>https://arxiv.org/abs/2409.14916</link>
      <description>arXiv:2409.14916v1 Announce Type: cross 
Abstract: This paper is devoted to a complete characterization of the free boundary of a particular solution to the following spectral $k$-partition problem with measure and inclusion constraints: \[ \inf \left\{\sum_{i=1}^k \lambda_1(\omega_i)\; : \; \omega_i \subset \Omega \mbox{ are nonempty open sets for all } i=1,\ldots, k,\;
  \omega_i \cap \omega_j = \emptyset \: \text{for all}\: i \not=j \mbox{ and } \sum_{i=1}^{k}|\omega_i| = a \right\}, \] where $\Omega$ is a bounded domain of $\mathbb{R}^N$, $a\in (0,|\Omega|)$. In particular, we prove free boundary conditions, classify contact points, characterize the regular and singular part of the free boundary (including branching points), and describe the interaction of the partition with the fixed boundary $\partial \Omega$.
  The proof is based on a perturbed version of the problem, combined with monotonicity formulas, blowup analysis and classification of blowups, suitable deformations of optimal sets and eigenfunctions, as well as the improvement of flatness of [Russ-Trey-Velichkov, CVPDE 58, 2019] for the one-phase points, and of [De Philippis-Spolaor-Velichkov, Invent. Math. 225, 2021] at two-phase points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14916v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dario Mazzoleni, Makson S. Santos, Hugo Tavares</dc:creator>
    </item>
    <item>
      <title>FLeNS: Federated Learning with Enhanced Nesterov-Newton Sketch</title>
      <link>https://arxiv.org/abs/2409.15216</link>
      <description>arXiv:2409.15216v1 Announce Type: cross 
Abstract: Federated learning faces a critical challenge in balancing communication efficiency with rapid convergence, especially for second-order methods. While Newton-type algorithms achieve linear convergence in communication rounds, transmitting full Hessian matrices is often impractical due to quadratic complexity. We introduce Federated Learning with Enhanced Nesterov-Newton Sketch (FLeNS), a novel method that harnesses both the acceleration capabilities of Nesterov's method and the dimensionality reduction benefits of Hessian sketching. FLeNS approximates the centralized Newton's method without relying on the exact Hessian, significantly reducing communication overhead. By combining Nesterov's acceleration with adaptive Hessian sketching, FLeNS preserves crucial second-order information while preserving the rapid convergence characteristics. Our theoretical analysis, grounded in statistical learning, demonstrates that FLeNS achieves super-linear convergence rates in communication rounds - a notable advancement in federated optimization. We provide rigorous convergence guarantees and characterize tradeoffs between acceleration, sketch size, and convergence speed. Extensive empirical evaluation validates our theoretical findings, showcasing FLeNS's state-of-the-art performance with reduced communication requirements, particularly in privacy-sensitive and edge-computing scenarios. The code is available at https://github.com/sunnyinAI/FLeNS</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15216v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunny Gupta,  Mohit, Pankhi Kashyap, Pranav Jeevan, Amit Sethi</dc:creator>
    </item>
    <item>
      <title>The Minimizer of the Sum of Two Strongly Convex Functions</title>
      <link>https://arxiv.org/abs/2305.13134</link>
      <description>arXiv:2305.13134v2 Announce Type: replace 
Abstract: The optimization problem concerning the determination of the minimizer for the sum of convex functions holds significant importance in the realm of distributed and decentralized optimization. In scenarios where full knowledge of the functions is not available, limiting information to individual minimizers and convexity parameters -- either due to privacy concerns or the nature of solution analysis -- necessitates an exploration of the region encompassing potential minimizers based solely on these known quantities. The characterization of this region becomes notably intricate when dealing with multivariate strongly convex functions compared to the univariate case. This paper contributes outer and inner approximations for the region harboring the minimizer of the sum of two strongly convex functions, given a constraint on the norm of the gradient at the minimizer of the sum. Notably, we explicitly delineate the boundaries and interiors of both the outer and inner approximations. Intriguingly, the boundaries as well as the interiors turn out to be identical. Furthermore, we establish that the boundary of the region containing potential minimizers aligns with that of the outer and inner approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.13134v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/02331934.2024.2402923</arxiv:DOI>
      <arxiv:journal_reference>Optimization, 2024, 1-41</arxiv:journal_reference>
      <dc:creator>Kananart Kuwaranancharoen, Shreyas Sundaram</dc:creator>
    </item>
    <item>
      <title>Self-supervised Equality Embedded Deep Lagrange Dual for Approximate Constrained Optimization</title>
      <link>https://arxiv.org/abs/2306.06674</link>
      <description>arXiv:2306.06674v5 Announce Type: replace 
Abstract: Conventional solvers are often computationally expensive for constrained optimization, particularly in large-scale and time-critical problems. While this leads to a growing interest in using neural networks (NNs) as fast optimal solution approximators, incorporating the constraints with NNs is challenging. In this regard, we propose deep Lagrange dual with equality embedding (DeepLDE), a framework that learns to find an optimal solution without using labels. To ensure feasible solutions, we embed equality constraints into the NNs and train the NNs using the primal-dual method to impose inequality constraints. Furthermore, we prove the convergence of DeepLDE and show that the primal-dual learning method alone cannot ensure equality constraints without the help of equality embedding. Simulation results on convex, non-convex, and AC optimal power flow (AC-OPF) problems show that the proposed DeepLDE achieves the smallest optimality gap among all the NN-based approaches while always ensuring feasible solutions. Furthermore, the computation time of the proposed method is about 5 to 250 times faster than DC3 and the conventional solvers in solving constrained convex, non-convex optimization, and/or AC-OPF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06674v5</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minsoo Kim, Hongseok Kim</dc:creator>
    </item>
    <item>
      <title>Distributed nonconvex optimization for control of water networks with time-coupling constraints</title>
      <link>https://arxiv.org/abs/2311.05180</link>
      <description>arXiv:2311.05180v2 Announce Type: replace 
Abstract: In this paper, we present a new control model for optimizing pressure and water quality operations in water distribution networks. Our formulation imposes a set of time-coupling constraints to manage temporal pressure variations, which are exacerbated by the transition between pressure and water quality controls. The resulting optimization problem is a nonconvex, nonlinear program with nonseparable structure across time steps. This problem proves challenging for state-of-the-art nonlinear solvers, often precluding their direct use for near real-time control in large-scale networks. To overcome this computational burden, we investigate a distributed optimization approach based on the alternating direction method of multipliers (ADMM). In particular, we implement and evaluate two algorithms: a standard ADMM scheme and a two-level variant that provides theoretical convergence guarantees for our nonconvex problem. We use a benchmarking water network and a large-scale operational network in the UK for our numerical experiments. The results demonstrate good convergence behavior across all problem instances for the two-level algorithm, whereas the standard ADMM approach struggles to converge in some instances. With an appropriately tuned penalty parameter, however, both distributed algorithms yield good quality solutions and computational times compatible with near real-time (e.g. hourly) control requirements for large-scale water networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05180v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11269-024-03985-8</arxiv:DOI>
      <dc:creator>Bradley Jenks, Aly-Joy Ulusoy, Filippo Pecci, Ivan Stoianov</dc:creator>
    </item>
    <item>
      <title>Fast convergence of trust-regions for non-isolated minima via analysis of CG on indefinite matrices</title>
      <link>https://arxiv.org/abs/2311.07404</link>
      <description>arXiv:2311.07404v2 Announce Type: replace 
Abstract: Trust-region methods (TR) can converge quadratically to minima where the Hessian is positive definite. However, if the minima are not isolated, then the Hessian there cannot be positive definite. The weaker Polyak$\unicode{x2013}${\L}ojasiewicz (P{\L}) condition is compatible with non-isolated minima, and it is enough for many algorithms to preserve good local behavior. Yet, TR with an $\textit{exact}$ subproblem solver lacks even basic features such as a capture theorem under P{\L}.
  In practice, a popular $\textit{inexact}$ subproblem solver is the truncated conjugate gradient method (tCG). Empirically, TR-tCG exhibits super-linear convergence under P{\L}. We confirm this theoretically.
  The main mathematical obstacle is that, under P{\L}, at points arbitrarily close to minima, the Hessian has vanishingly small, possibly negative eigenvalues. Thus, tCG is applied to ill-conditioned, indefinite systems. Yet, the core theory underlying tCG is that of CG, which assumes a positive definite operator. Accordingly, we develop new tools to analyze the dynamics of CG in the presence of small eigenvalues of any sign, for the regime of interest to TR-tCG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07404v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quentin Rebjock, Nicolas Boumal</dc:creator>
    </item>
    <item>
      <title>Unboundedness of the images of set-valued mappings having closed graphs: Application to vector optimization</title>
      <link>https://arxiv.org/abs/2312.14783</link>
      <description>arXiv:2312.14783v2 Announce Type: replace 
Abstract: In this paper, we propose criteria for unboundedness of the images of set-valued mappings having closed graphs in Euclidean spaces. We focus on mappings whose domains are non-closed or whose values are connected. These criteria allow us to see structural properties of solutions in vector optimization, where solution sets can be considered as the images of solution mappings associated to specific scalarization methods. In particular, we prove that if the domain of a certain solution mapping is non-closed, then the weak Pareto solution set is unbounded. Furthermore, for a quasi-convex problem, we demonstrate two criteria to ensure that if the weak Pareto solution set is disconnected then each connected component is unbounded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14783v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vu Trung Hieu, Elisabeth Anna Sophia K\"obis, Markus Arthur K\"obis, Paul Hugo Schm\"olling</dc:creator>
    </item>
    <item>
      <title>A hierarchy of convex relaxations for the total variation distance</title>
      <link>https://arxiv.org/abs/2401.01086</link>
      <description>arXiv:2401.01086v2 Announce Type: replace 
Abstract: Given two measures $\mu$, $\nu$ on Rd that satisfy Carleman's condition, we provide a numerical scheme to approximate as closely as desired the total variation distance between $\mu$ and $\nu$. It consists of solving a sequence (hierarchy) of convex relaxations whose associated sequence of optimal values converges to the total variation distance, an additional illustration of the versatility of the Moment-SOS hierarchy. Indeed each relaxation in the hierarchy is a semidefinite program whose size increases with the number of involved moments. It has an optimal solution which is a couple of degree-2n pseudo-moments which converge, as n grows, to moments of the Hahn-Jordan decomposition of $\mu$-$\nu$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01086v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Bernard Lasserre (LAAS-POP, TSE-R)</dc:creator>
    </item>
    <item>
      <title>Norm-induced Cuts: Optimization with Lipschitzian Black-box Functions</title>
      <link>https://arxiv.org/abs/2403.11546</link>
      <description>arXiv:2403.11546v2 Announce Type: replace 
Abstract: In this paper, we consider a finite dimensional optimization problem minimizing a continuous objective on a compact domain subject to a multi-dimensional constraint function. For the latter, we only assume the availability of a Lipschitz property. In recent literature methods based on non-convex outer approximation are proposed for tackling one dimensional equality constraints on bounded polyhedral domains, which are Lipschitz with respect to the maximum norm. To the best of our knowledge, however, there exists no non-convex outer approximation method for a general problem class. We introduce a meta-level solution framework to solve such problems and tackle the underlying theoretical foundations. Considering the feasible domain without the constraint function as manageable, our method relaxes the multidimensional constraint and iteratively refines the feasible region by means of norm-induced cuts, relying on an oracle for the resulting sub-problems. We show the method's correctness and investigate the problem complexity. In order to account for discussions about functionality, limits, and extensions, we present computational examples including illustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11546v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrian G\"o{\ss}, Alexander Martin, Sebastian Pokutta, Kartikey Sharma</dc:creator>
    </item>
    <item>
      <title>Using the Empirical Attainment Function for Analyzing Single-objective Black-box Optimization Algorithms</title>
      <link>https://arxiv.org/abs/2404.02031</link>
      <description>arXiv:2404.02031v3 Announce Type: replace 
Abstract: A widely accepted way to assess the performance of iterative black-box optimizers is to analyze their empirical cumulative distribution function (ECDF) of pre-defined quality targets achieved not later than a given runtime. In this work, we consider an alternative approach, based on the empirical attainment function (EAF) and we show that the target-based ECDF is an approximation of the EAF. We argue that the EAF has several advantages over the target-based ECDF. In particular, it does not require defining a priori quality targets per function, captures performance differences more precisely, and enables the use of additional summary statistics that enrich the analysis. We also show that the average area over the convergence curves is a simpler-to-calculate, but equivalent, measure of anytime performance. To facilitate the accessibility of the EAF, we integrate a module to compute it into the IOHanalyzer platform. Finally, we illustrate the use of the EAF via synthetic examples and via the data available for the BBOB suite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02031v3</guid>
      <category>math.OC</category>
      <category>cs.NE</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TEVC.2024.3462758</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Evolutionary Computation (2024)</arxiv:journal_reference>
      <dc:creator>Manuel L\'opez-Ib\'a\~nez, Diederick Vermetten, Johann Dreo, Carola Doerr</dc:creator>
    </item>
    <item>
      <title>Distributed Computing for Huge-Scale Linear Programming</title>
      <link>https://arxiv.org/abs/2408.06204</link>
      <description>arXiv:2408.06204v3 Announce Type: replace 
Abstract: This study develops an algorithm for distributed computing of linear programming problems of huge-scales. Global consensus with single common variable, multiblocks, and augmented Lagrangian are adopted. The consensus is used to partition the constraints of equality and inequality into multi consensus blocks, the subblocks of each consensus block are employed to partition the primal variables into $M$ sets of disjoint subvectors. The block-coordinate Gauss-Seidel method, the proximal point method (via $\Vert\cdot\Vert_{1}$ and $\Vert\cdot\Vert^2$), and ADMM are used to update the primal variables. The dual variables are partitioned into group 1 and group 2; descent models are used to update group 1 to guarantee convergence of the algorithm. The algorithm converges to feasible points. How to update group 2 of the dual is to be explored which is linked to whether a feasible is an optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06204v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luoyi Tao</dc:creator>
    </item>
    <item>
      <title>Existence of Solutions for Fractional Optimal Control Problems with Superlinear-subcritical Controls</title>
      <link>https://arxiv.org/abs/2408.09586</link>
      <description>arXiv:2408.09586v2 Announce Type: replace 
Abstract: This paper gives an existence result for solutions to an elliptic optimal control problem based on a general fractional kernel, where the admissible controls come from a class satisfying both a growth bound and a superlinear-subcritical condition. Each admissible control is known to produce a nontrivial corresponding state by applying the Mountain Pass Theorem to fractional equations. The main theoretical contribution is the construction of a suitable set of admissible controls on which the the standard existence theory for control problems with linear and semi-linear state constraints can be adapted. Extra care is taken to explain what new difficulties arise for these types of control problems, to justify the limitations of this theory. For completeness, the corresponding local elliptic control problem is also studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09586v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua M. Siktar</dc:creator>
    </item>
    <item>
      <title>Modelling Global Trade with Optimal Transport</title>
      <link>https://arxiv.org/abs/2409.06554</link>
      <description>arXiv:2409.06554v2 Announce Type: replace 
Abstract: Global trade is shaped by a complex mix of factors beyond supply and demand, including tangible variables like transport costs and tariffs, as well as less quantifiable influences such as political and economic relations. Traditionally, economists model trade using gravity models, which rely on explicit covariates but often struggle to capture these subtler drivers of trade. In this work, we employ optimal transport and a deep neural network to learn a time-dependent cost function from data, without imposing a specific functional form. This approach consistently outperforms traditional gravity models in accuracy while providing natural uncertainty quantification. Applying our framework to global food and agricultural trade, we show that the global South suffered disproportionately from the war in Ukraine's impact on wheat markets. We also analyze the effects of free-trade agreements and trade disputes with China, as well as Brexit's impact on British trade with Europe, uncovering hidden patterns that trade volumes alone cannot reveal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06554v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Gaskin, Marie-Therese Wolfram, Andrew Duncan, Guven Demirel</dc:creator>
    </item>
    <item>
      <title>Addressing misspecification in contextual optimization</title>
      <link>https://arxiv.org/abs/2409.10479</link>
      <description>arXiv:2409.10479v2 Announce Type: replace 
Abstract: We study a linear contextual optimization problem where a decision maker has access to historical data and contextual features to learn a cost prediction model aimed at minimizing decision error. We adopt the predict-then-optimize framework for this analysis. Given that perfect model alignment with reality is often unrealistic in practice, we focus on scenarios where the chosen hypothesis set is misspecified. In this context, it remains unclear whether current contextual optimization approaches can effectively address such model misspecification. In this paper, we present a novel integrated learning and optimization approach designed to tackle model misspecification in contextual optimization. This approach offers theoretical generalizability, tractability, and optimality guarantees, along with strong practical performance. Our method involves minimizing a tractable surrogate loss that aligns with the performance value from cost vector predictions, regardless of whether the model is misspecified, and can be optimized in reasonable time. To our knowledge, no previous work has provided an approach with such guarantees in the context of model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10479v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Bennouna, Jiawei Zhang, Saurabh Amin, Asuman Ozdaglar</dc:creator>
    </item>
    <item>
      <title>Does DQN Learn?</title>
      <link>https://arxiv.org/abs/2205.13617</link>
      <description>arXiv:2205.13617v4 Announce Type: replace-cross 
Abstract: For a reinforcement learning method to be useful, the policy it estimates in the limit must be superior to the initial guess, at least on average. In this work, we show that the widely used Deep Q-Network (DQN) fails to meet even this basic criterion, even when it gets to see all possible states and actions infinitely often (a condition that ensures tabular Q-learning's convergence to the optimal Q-value). Our work's key highlights are as follows. First, we numerically show that DQN generally has a non-trivial probability of producing a policy worse than the initial one. Second, we give a theoretical explanation for this behavior in the context of linear DQN, wherein we replace the neural network with a linear function approximation but retain DQN's other key ideas, such as experience replay, target network, and $\epsilon$-greedy exploration. Our main result is that the tail behaviors of linear DQN are governed by invariant sets of a deterministic differential inclusion, a set-valued generalization of a differential equation. Notably, we show that these invariant sets need not align with locally optimal policies, thus explaining DQN's pathological behaviors, such as convergence to sub-optimal policies and policy oscillation. We also provide a scenario where the limiting policy is always the worst. Our work addresses a longstanding gap in understanding the behaviors of Q-learning with function approximation and $\epsilon$-greedy exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.13617v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Gopalan, Gugan Thoppe</dc:creator>
    </item>
    <item>
      <title>Welfare-Optimized Recommender Systems</title>
      <link>https://arxiv.org/abs/2206.13845</link>
      <description>arXiv:2206.13845v2 Announce Type: replace-cross 
Abstract: We present a recommender system based on the Random Utility Model. Online shoppers are modeled as rational decision makers with limited information, and the recommendation task is formulated as the problem of optimally enriching the shopper's awareness set. Notably, the price information and the shopper's Willingness-To-Pay play crucial roles. Furthermore, to better account for the commercial nature of the recommendation, we unify the retailer and shoppers' contradictory objectives into a single welfare metric, which we propose as a new recommendation goal. We test our framework on synthetic data and show its performance in a wide range of scenarios. This new framework, that was absent from the Recommender System literature, opens the door to Welfare-Optimized Recommender Systems, couponing, and price optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.13845v2</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Heymann, Flavian Vasile, David Rohde</dc:creator>
    </item>
    <item>
      <title>An Adaptive and Stability-Promoting Layerwise Training Approach for Sparse Deep Neural Network Architecture</title>
      <link>https://arxiv.org/abs/2211.06860</link>
      <description>arXiv:2211.06860v2 Announce Type: replace-cross 
Abstract: This work presents a two-stage adaptive framework for progressively developing deep neural network (DNN) architectures that generalize well for a given training data set. In the first stage, a layerwise training approach is adopted where a new layer is added each time and trained independently by freezing parameters in the previous layers. We impose desirable structures on the DNN by employing manifold regularization, sparsity regularization, and physics-informed terms. We introduce a epsilon-delta stability-promoting concept as a desirable property for a learning algorithm and show that employing manifold regularization yields a epsilon-delta stability-promoting algorithm. Further, we also derive the necessary conditions for the trainability of a newly added layer and investigate the training saturation problem. In the second stage of the algorithm (post-processing), a sequence of shallow networks is employed to extract information from the residual produced in the first stage, thereby improving the prediction accuracy. Numerical investigations on prototype regression and classification problems demonstrate that the proposed approach can outperform fully connected DNNs of the same size. Moreover, by equipping the physics-informed neural network (PINN) with the proposed adaptive architecture strategy to solve partial differential equations, we numerically show that adaptive PINNs not only are superior to standard PINNs but also produce interpretable hidden layers with provable stability. We also apply our architecture design strategy to solve inverse problems governed by elliptic partial differential equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.06860v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C G Krishnanunni, Tan Bui-Thanh</dc:creator>
    </item>
    <item>
      <title>Noise Stability Optimization for Finding Flat Minima: A Hessian-based Regularization Approach</title>
      <link>https://arxiv.org/abs/2306.08553</link>
      <description>arXiv:2306.08553v4 Announce Type: replace-cross 
Abstract: The training of over-parameterized neural networks has received much study in recent literature. An important consideration is the regularization of over-parameterized networks due to their highly nonconvex and nonlinear geometry. In this paper, we study noise injection algorithms, which can regularize the Hessian of the loss, leading to regions with flat loss surfaces. Specifically, by injecting isotropic Gaussian noise into the weight matrices of a neural network, we can obtain an approximately unbiased estimate of the trace of the Hessian. However, naively implementing the noise injection via adding noise to the weight matrices before backpropagation presents limited empirical improvements. To address this limitation, we design a two-point estimate of the Hessian penalty, which injects noise into the weight matrices along both positive and negative directions of the random noise. In particular, this two-point estimate eliminates the variance of the first-order Taylor's expansion term on the Hessian. We show a PAC-Bayes generalization bound that depends on the trace of the Hessian (and the radius of the weight space), which can be measured from data.
  We conduct a detailed experimental study to validate our approach and show that it can effectively regularize the Hessian and improve generalization. First, our algorithm can outperform prior approaches on sharpness-reduced training, delivering up to a 2.4% test accuracy increase for fine-tuning ResNets on six image classification datasets. Moreover, the trace of the Hessian reduces by 15.8%, and the largest eigenvalue is reduced by 9.7% with our approach. We also find that the regularization of the Hessian can be combined with weight decay and data augmentation, leading to stronger regularization. Second, our approach remains effective for improving generalization in pretraining multimodal CLIP models and chain-of-thought fine-tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.08553v4</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Trans. Mach. Learn. Res. 2024</arxiv:journal_reference>
      <dc:creator>Hongyang R. Zhang, Dongyue Li, Haotian Ju</dc:creator>
    </item>
    <item>
      <title>Reconstructing the kinetic chemotaxis kernel using macroscopic data: well-posedness and ill-posedness</title>
      <link>https://arxiv.org/abs/2309.05004</link>
      <description>arXiv:2309.05004v3 Announce Type: replace-cross 
Abstract: Bacterial motion is steered by external stimuli (chemotaxis), and the motion described on the mesoscopic scale is uniquely determined by a parameter $K$ that models velocity change response from the bacteria. This parameter is called chemotaxis kernel. In a practical setting, it is inferred by experimental data. We deploy a PDE-constrained optimization framework to perform this reconstruction using velocity-averaged, localized data taken in the interior of the domain. The problem can be well-posed or ill-posed depending on the data preparation and the experimental setup. In particular, we propose one specific design that guarantees numerical reconstructability and local convergence. This design is adapted to the discretization of $K$ in space and decouples the reconstruction of local values of $K$ into smaller cell problems, opening up parallelization opportunities. Numerical evidences support the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05004v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <category>q-bio.CB</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kathrin Hellmuth, Christian Klingenberg, Qin Li, Min Tang</dc:creator>
    </item>
    <item>
      <title>Quantum Optimization: Potential, Challenges, and the Path Forward</title>
      <link>https://arxiv.org/abs/2312.02279</link>
      <description>arXiv:2312.02279v2 Announce Type: replace-cross 
Abstract: Recent advances in quantum computers are demonstrating the ability to solve problems at a scale beyond brute force classical simulation. As such, a widespread interest in quantum algorithms has developed in many areas, with optimization being one of the most pronounced domains. Across computer science and physics, there are a number of different approaches for major classes of optimization problems, such as combinatorial optimization, convex optimization, non-convex optimization, and stochastic extensions. This work draws on multiple approaches to study quantum optimization. Provably exact versus heuristic settings are first explained using computational complexity theory - highlighting where quantum advantage is possible in each context. Then, the core building blocks for quantum optimization algorithms are outlined to subsequently define prominent problem classes and identify key open questions that, if answered, will advance the field. The effects of scaling relevant problems on noisy quantum devices are also outlined in detail, alongside meaningful benchmarking problems. We underscore the importance of benchmarking by proposing clear metrics to conduct appropriate comparisons with classical optimization techniques. Lastly, we highlight two domains - finance and sustainability - as rich sources of optimization problems that could be used to benchmark, and eventually validate, the potential real-world impact of quantum optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02279v2</guid>
      <category>quant-ph</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amira Abbas, Andris Ambainis, Brandon Augustino, Andreas B\"artschi, Harry Buhrman, Carleton Coffrin, Giorgio Cortiana, Vedran Dunjko, Daniel J. Egger, Bruce G. Elmegreen, Nicola Franco, Filippo Fratini, Bryce Fuller, Julien Gacon, Constantin Gonciulea, Sander Gribling, Swati Gupta, Stuart Hadfield, Raoul Heese, Gerhard Kircher, Thomas Kleinert, Thorsten Koch, Georgios Korpas, Steve Lenk, Jakub Marecek, Vanio Markov, Guglielmo Mazzola, Stefano Mensa, Naeimeh Mohseni, Giacomo Nannicini, Corey O'Meara, Elena Pe\~na Tapia, Sebastian Pokutta, Manuel Proissl, Patrick Rebentrost, Emre Sahin, Benjamin C. B. Symons, Sabine Tornow, Victor Valls, Stefan Woerner, Mira L. Wolf-Bauwens, Jon Yard, Sheir Yarkoni, Dirk Zechiel, Sergiy Zhuk, Christa Zoufal</dc:creator>
    </item>
    <item>
      <title>Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications</title>
      <link>https://arxiv.org/abs/2312.02828</link>
      <description>arXiv:2312.02828v4 Announce Type: replace-cross 
Abstract: In this paper, we study the convergence properties of the Stochastic Gradient Descent (SGD) method for finding a stationary point of a given objective function $J(\cdot)$. The objective function is not required to be convex. Rather, our results apply to a class of ``invex'' functions, which have the property that every stationary point is also a global minimizer. First, it is assumed that $J(\cdot)$ satisfies a property that is slightly weaker than the Kurdyka-Lojasiewicz (KL) condition, denoted here as (KL'). It is shown that the iterations $J(\boldsymbol{\theta}_t)$ converge almost surely to the global minimum of $J(\cdot)$. Next, the hypothesis on $J(\cdot)$ is strengthened from (KL') to the Polyak-Lojasiewicz (PL) condition. With this stronger hypothesis, we derive estimates on the rate of convergence of $J(\boldsymbol{\theta}_t)$ to its limit. Using these results, we show that for functions satisfying the PL property, the convergence rate of both the objective function and the norm of the gradient with SGD is the same as the best-possible rate for convex functions. While some results along these lines have been published in the past, our contributions contain two distinct improvements. First, the assumptions on the stochastic gradient are more general than elsewhere, and second, our convergence is almost sure, and not in expectation. We also study SGD when only function evaluations are permitted. In this setting, we determine the ``optimal'' increments or the size of the perturbations. Using the same set of ideas, we establish the global convergence of the Stochastic Approximation (SA) algorithm under more general assumptions on the measurement error, compared to the existing literature. We also derive bounds on the rate of convergence of the SA algorithm under appropriate assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02828v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajeeva L. Karandikar, M. Vidyasagar</dc:creator>
    </item>
    <item>
      <title>Theoretical and Empirical Advances in Forest Pruning</title>
      <link>https://arxiv.org/abs/2401.05535</link>
      <description>arXiv:2401.05535v3 Announce Type: replace-cross 
Abstract: Decades after their inception, regression forests continue to provide state-of-the-art accuracy, outperforming in this respect alternative machine learning models such as regression trees or even neural networks. However, being an ensemble method, the one aspect where regression forests tend to severely underperform regression trees is interpretability. In the present work, we revisit forest pruning, an approach that aims to have the best of both worlds: the accuracy of regression forests and the interpretability of regression trees. This pursuit, whose foundation lies at the core of random forest theory, has seen vast success in empirical studies. In this paper, we contribute theoretical results that support and qualify those empirical findings; namely, we prove the asymptotic advantage of a Lasso-pruned forest over its unpruned counterpart under extremely weak assumptions, as well as high-probability finite-sample generalization bounds for regression forests pruned according to the main methods, which we then validate by way of simulation. Then, we test the accuracy of pruned regression forests against their unpruned counterparts on 19 different datasets (16 synthetic, 3 real). We find that in the vast majority of scenarios tested, there is at least one forest-pruning method that yields equal or better accuracy than the original full forest (in expectation), while just using a small fraction of the trees. We show that, in some cases, the reduction in the size of the forest is so dramatic that the resulting sub-forest can be meaningfully merged into a single tree, obtaining a level of interpretability that is qualitatively superior to that of the original regression forest, which remains a black box.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05535v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Albert Dorador</dc:creator>
    </item>
    <item>
      <title>Generating Likely Counterfactuals Using Sum-Product Networks</title>
      <link>https://arxiv.org/abs/2401.14086</link>
      <description>arXiv:2401.14086v3 Announce Type: replace-cross 
Abstract: Explainability of decisions made by AI systems is driven by both recent regulation and user demand. These decisions are often explainable only \emph{post hoc}, after the fact. In counterfactual explanations, one may ask what constitutes the best counterfactual explanation. Clearly, multiple criteria must be taken into account, although "distance from the sample" is a key criterion. Recent methods that consider the plausibility of a counterfactual seem to sacrifice this original objective. Here, we present a system that provides high-likelihood explanations that are, at the same time, close and sparse. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14086v3</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiri Nemecek, Tomas Pevny, Jakub Marecek</dc:creator>
    </item>
    <item>
      <title>Projected Forward Gradient-Guided Frank-Wolfe Algorithm via Variance Reduction</title>
      <link>https://arxiv.org/abs/2403.12511</link>
      <description>arXiv:2403.12511v2 Announce Type: replace-cross 
Abstract: This paper aims to enhance the use of the Frank-Wolfe (FW) algorithm for training deep neural networks. Similar to any gradient-based optimization algorithm, FW suffers from high computational and memory costs when computing gradients for DNNs. This paper introduces the application of the recently proposed projected forward gradient (Projected-FG) method to the FW framework, offering reduced computational cost similar to backpropagation and low memory utilization akin to forward propagation. Our results show that trivial application of the Projected-FG introduces non-vanishing convergence error due to the stochastic noise that the Projected-FG method introduces in the process. This noise results in an non-vanishing variance in the Projected-FG estimated gradient. To address this, we propose a variance reduction approach by aggregating historical Projected-FG directions. We demonstrate rigorously that this approach ensures convergence to the optimal solution for convex functions and to a stationary point for non-convex functions. These convergence properties are validated through a numerical example, showcasing the approach's effectiveness and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12511v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Rostami, S. S. Kia</dc:creator>
    </item>
    <item>
      <title>Convergence rate of random scan Coordinate Ascent Variational Inference under log-concavity</title>
      <link>https://arxiv.org/abs/2406.07292</link>
      <description>arXiv:2406.07292v2 Announce Type: replace-cross 
Abstract: The Coordinate Ascent Variational Inference scheme is a popular algorithm used to compute the mean-field approximation of a probability distribution of interest. We analyze its random scan version, under log-concavity assumptions on the target density. Our approach builds on the recent work of M. Arnese and D. Lacker, \emph{Convergence of coordinate ascent variational inference for log-concave measures via optimal transport} [arXiv:2404.08792] which studies the deterministic scan version of the algorithm, phrasing it as a block-coordinate descent algorithm in the space of probability distributions endowed with the geometry of optimal transport. We obtain tight rates for the random scan version, which imply that the total number of factor updates required to converge scales linearly with the condition number and the number of blocks of the target distribution. By contrast, available bounds for the deterministic scan case scale quadratically in the same quantities, which is analogue to what happens for optimization of convex functions in Euclidean spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07292v2</guid>
      <category>stat.ML</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Lavenant, Giacomo Zanella</dc:creator>
    </item>
    <item>
      <title>Infinite-Horizon Reinforcement Learning with Multinomial Logistic Function Approximation</title>
      <link>https://arxiv.org/abs/2406.13633</link>
      <description>arXiv:2406.13633v2 Announce Type: replace-cross 
Abstract: We study model-based reinforcement learning with non-linear function approximation where the transition function of the underlying Markov decision process (MDP) is given by a multinomial logistic (MNL) model. We develop a provably efficient discounted value iteration-based algorithm that works for both infinite-horizon average-reward and discounted-reward settings. For average-reward communicating MDPs, the algorithm guarantees a regret upper bound of $\tilde{\mathcal{O}}(dD\sqrt{T})$ where $d$ is the dimension of feature mapping, $D$ is the diameter of the underlying MDP, and $T$ is the horizon. For discounted-reward MDPs, our algorithm achieves $\tilde{\mathcal{O}}(d(1-\gamma)^{-2}\sqrt{T})$ regret where $\gamma$ is the discount factor. Then we complement these upper bounds by providing several regret lower bounds. We prove a lower bound of $\Omega(d\sqrt{DT})$ for learning communicating MDPs of diameter $D$ and a lower bound of $\Omega(d(1-\gamma)^{3/2}\sqrt{T})$ for learning discounted-reward MDPs with discount factor $\gamma$. Lastly, we show a regret lower bound of $\Omega(dH^{3/2}\sqrt{K})$ for learning $H$-horizon episodic MDPs with MNL function approximation where $K$ is the number of episodes, which improves upon the best-known lower bound for the finite-horizon setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13633v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaehyun Park, Junyeop Kwon, Dabeen Lee</dc:creator>
    </item>
    <item>
      <title>Real-time Estimation of Bound Water Concentration during Lyophilization with Temperature-based State Observers</title>
      <link>https://arxiv.org/abs/2407.13844</link>
      <description>arXiv:2407.13844v2 Announce Type: replace-cross 
Abstract: Lyophilization (aka freeze drying) has been shown to provide long-term stability for many crucial biotherapeutics, e.g., mRNA vaccines for COVID-19, allowing for higher storage temperature. The final stage of lyophilization, namely secondary drying, entails bound water removal via desorption, in which accurate prediction of bound water concentration is vital to ensuring the quality of the lyophilized product. This article proposes a novel technique for real-time estimation of the residual moisture during secondary drying in lyophilization. A state observer is employed, which combines temperature measurement and mechanistic understanding of heat transfer and desorption kinetics, without requiring any online concentration measurement. Results from both simulations and experimental data show that the observer can accurately estimate the concentration of bound water in real time for all possible concentration levels, operating conditions, and measurement noise. This framework can also be applied for monitoring and control of the residual moisture in other desorption-related processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13844v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijpharm.2024.124693</arxiv:DOI>
      <dc:creator>Prakitr Srisuma, George Barbastathis, Richard Braatz</dc:creator>
    </item>
    <item>
      <title>A trustworthy blockchain-based energy trading scheme for V2G operations in distributed power grids via integrated scheduling and trading framework</title>
      <link>https://arxiv.org/abs/2407.13988</link>
      <description>arXiv:2407.13988v2 Announce Type: replace-cross 
Abstract: The rapid growth of electric vehicles (EVs) and the deployment of vehicle-to-grid (V2G) technology pose significant challenges for distributed power grids, particularly in fostering trust and ensuring effective coordination among stakeholders. In this paper, we developed an integrated scheduling and trading framework to conduct transparent and efficacious coordination in V2G operations. In blockchain implementation, we propose a cyber-physical blockchain architecture that enhances transaction efficiency and scalability by leveraging smart charging points (SCPs) for rapid transaction validation through a fast-path practical byzantine fault tolerance (fast-path PBFT) consensus mechanism. From the energy dispatching perspective, a game-theoretical pricing strategy is employed and smart contracts are utilized for autonomous decision-making between EVs and operators, aiming to optimize the trading process and maximize economic benefits. Numerical evaluation of blockchain consensus shows the effect of the fast-path PBFT consensus in improving systems scalability with a balanced trade-off in robustness. A case study, utilizing real-world data from the Southern University of Science and Technology (SUSTech), demonstrates significant reductions in EV charging costs and the framework potential to support auxiliary grid services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13988v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunwang Chen, Xiang Lei, Songyan Niu, Linni Jian</dc:creator>
    </item>
    <item>
      <title>Bi-level regularization via iterative mesh refinement for aeroacoustics</title>
      <link>https://arxiv.org/abs/2409.06854</link>
      <description>arXiv:2409.06854v2 Announce Type: replace-cross 
Abstract: In this work, we illustrate the connection between adaptive mesh refinement for finite element discretized PDEs and the recently developed \emph{bi-level regularization algorithm}. By adaptive mesh refinement according to data noise, regularization effect and convergence are immediate consequences. We moreover demonstrate its numerical advantages to the classical Landweber algorithm in term of time and reconstruction quality for the example of the Helmholtz equation in an aeroacoustic setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06854v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Aarset, Tram Thi Ngoc Nguyen</dc:creator>
    </item>
    <item>
      <title>Increasing Both Batch Size and Learning Rate Accelerates Stochastic Gradient Descent</title>
      <link>https://arxiv.org/abs/2409.08770</link>
      <description>arXiv:2409.08770v2 Announce Type: replace-cross 
Abstract: The performance of mini-batch stochastic gradient descent (SGD) strongly depends on setting the batch size and learning rate to minimize the empirical loss in training the deep neural network. In this paper, we present theoretical analyses of mini-batch SGD with four schedulers: (i) constant batch size and decaying learning rate scheduler, (ii) increasing batch size and decaying learning rate scheduler, (iii) increasing batch size and increasing learning rate scheduler, and (iv) increasing batch size and warm-up decaying learning rate scheduler. We show that mini-batch SGD using scheduler (i) does not always minimize the expectation of the full gradient norm of the empirical loss, whereas it does using any of schedulers (ii), (iii), and (iv). Furthermore, schedulers (iii) and (iv) accelerate mini-batch SGD. The paper also provides numerical results of supporting analyses showing that using scheduler (iii) or (iv) minimizes the full gradient norm of the empirical loss faster than using scheduler (i) or (ii).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08770v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hikaru Umeda, Hideaki Iiduka</dc:creator>
    </item>
  </channel>
</rss>
