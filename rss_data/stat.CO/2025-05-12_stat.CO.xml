<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 May 2025 02:49:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Tutorial on Bayesian Functional Regression Using Stan</title>
      <link>https://arxiv.org/abs/2505.05633</link>
      <description>arXiv:2505.05633v1 Announce Type: cross 
Abstract: This manuscript provides step-by-step instructions for implementing Bayesian functional regression models using Stan. Extensive simulations indicate that the inferential performance of the methods is comparable to that of state-of-the-art frequentist approaches. However, Bayesian approaches allow for more flexible modeling and provide an alternative when frequentist methods are not available or may require additional development. Methods and software are illustrated using the accelerometry data from the National Health and Nutrition Examination Survey (NHANES).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05633v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziren Jiang, Ciprian Crainiceanu, Erjia Cui</dc:creator>
    </item>
    <item>
      <title>GEORCE: A Fast New Control Algorithm for Computing Geodesics</title>
      <link>https://arxiv.org/abs/2505.05961</link>
      <description>arXiv:2505.05961v1 Announce Type: cross 
Abstract: Computing geodesics for Riemannian manifolds is a difficult task that often relies on numerical approximations. However, these approximations tend to be either numerically unstable, have slow convergence, or scale poorly with manifold dimension and number of grid points. We introduce a new algorithm called GEORCE that computes geodesics via a transformation into a discrete control problem. We show that GEORCE has global convergence and quadratic local convergence. In addition, we show that it extends to Finsler manifolds. For both Finslerian and Riemannian manifolds, we thoroughly benchmark GEORCE against several alternative optimization algorithms and show empirically that it has a much faster and more accurate performance for a variety of manifolds, including key manifolds from information theory and manifolds that are learned using generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05961v1</guid>
      <category>math.DG</category>
      <category>stat.CO</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frederik M\"obius Rygaard, S{\o}ren Hauberg</dc:creator>
    </item>
    <item>
      <title>Pigeonhole Stochastic Gradient Langevin Dynamics for Large Crossed Mixed Effects Models</title>
      <link>https://arxiv.org/abs/2212.09126</link>
      <description>arXiv:2212.09126v2 Announce Type: replace 
Abstract: Large crossed mixed effects models with imbalanced structures and missing data pose major computational challenges for standard Bayesian posterior sampling algorithms, as the computational complexity is usually superlinear in the number of observations. We propose two efficient subset-based stochastic gradient MCMC algorithms for such crossed mixed effects models, which facilitate scalable inference on both the variance components and regression coefficients. The first algorithm is developed for balanced design without missing observations, where we leverage the closed-form expression of the precision matrix for the full data matrix. The second algorithm, which we call the pigeonhole stochastic gradient Langevin dynamics (PSGLD), is developed for both balanced and unbalanced designs with potentially a large proportion of missing observations. Our PSGLD algorithm imputes the latent crossed random effects by running short Markov chains and then samples the model parameters of variance components and regression coefficients at each MCMC iteration. We provide theoretical guarantees by showing the convergence of the output distribution from the proposed algorithms to the target non-log-concave posterior distribution. A variety of numerical experiments based on both synthetic and real data demonstrate that the proposed algorithms can significantly reduce the computational cost of the standard MCMC algorithms and better balance the approximation accuracy and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.09126v2</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinyu Zhang, Cheng Li</dc:creator>
    </item>
    <item>
      <title>Sparse inverse Cholesky factorization of dense kernel matrices by greedy conditional selection</title>
      <link>https://arxiv.org/abs/2307.11648</link>
      <description>arXiv:2307.11648v2 Announce Type: replace 
Abstract: Dense kernel matrices resulting from pairwise evaluations of a kernel function arise naturally in machine learning and statistics. Previous work in constructing sparse approximate inverse Cholesky factors of such matrices by minimizing Kullback-Leibler divergence recovers the Vecchia approximation for Gaussian processes. These methods rely only on the geometry of the evaluation points to construct the sparsity pattern. In this work, we instead construct the sparsity pattern by leveraging a greedy selection algorithm that maximizes mutual information with target points, conditional on all points previously selected. For selecting $k$ points out of $N$, the naive time complexity is $\mathcal{O}(N k^4)$, but by maintaining a partial Cholesky factor we reduce this to $\mathcal{O}(N k^2)$. Furthermore, for multiple ($m$) targets we achieve a time complexity of $\mathcal{O}(N k^2 + N m^2 + m^3)$, which is maintained in the setting of aggregated Cholesky factorization where a selected point need not condition every target. We apply the selection algorithm to image classification and recovery of sparse Cholesky factors. By minimizing Kullback-Leibler divergence, we apply the algorithm to Cholesky factorization, Gaussian process regression, and preconditioning with the conjugate gradient, improving over $k$-nearest neighbors selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11648v2</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephen Huan, Joseph Guinness, Matthias Katzfuss, Houman Owhadi, Florian Sch\"afer</dc:creator>
    </item>
    <item>
      <title>Optimization perspective on raking</title>
      <link>https://arxiv.org/abs/2407.20520</link>
      <description>arXiv:2407.20520v3 Announce Type: replace-cross 
Abstract: Raking is widely used in survey inference and global health models to adjust the observations in contingency tables to given marginals, in the latter case reconciling estimates between models with different granularities. We review the convex optimization foundation of raking and focus on a dual perspective that simplifies and streamlines prior raking extensions and provides new functionality, enabling a unified approach to n-dimensional raking, raking with differential weights, ensuring bounds on estimates are respected, raking to margins either as hard constraints or as aggregate observations, handling missing data, and allowing efficient uncertainty propagation. The dual perspective also enables a uniform fast and scalable matrix-free optimization approach for all of these extensions. All of the methods are implemented in an open source Python package with an intuitive user interface, installable from PyPi (https://pypi.org/project/raking/), and we illustrate the capabilities using synthetic data and real mortality estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20520v3</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ariane Ducellier (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Alexander Hsu (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA), Parkes Kendrick (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Bill Gustafson (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Laura Dwyer-Lindgren (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Christopher Murray (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Peng Zheng (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Aleksandr Aravkin (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA)</dc:creator>
    </item>
    <item>
      <title>Geometric Ergodicity of Gibbs Algorithms for a Normal Model With a Global-Local Shrinkage Prior</title>
      <link>https://arxiv.org/abs/2503.00538</link>
      <description>arXiv:2503.00538v3 Announce Type: replace-cross 
Abstract: In this paper, we consider Gibbs samplers for a normal linear regression model with a global-local shrinkage prior. We show that they produce geometrically ergodic Markov chains under some assumptions. In the first half of the paper, we prove geometric ergodicity under the horseshoe local prior and a three-parameter beta global prior which does not have a finite $(p / 5)$-th negative moment, where $p$ is the number of regression coefficients. This is in contrast to the case of a known general result which is applicable if the global parameter has a finite approximately $(p / 2)$-th negative moment. In the second half of the paper, we consider a more general class of global-local shrinkage priors. Geometric ergodicity is proved for two-stage and three-stage Gibbs samplers based on rejection sampling without assuming the negative moment condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00538v3</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 12 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Hamura</dc:creator>
    </item>
  </channel>
</rss>
