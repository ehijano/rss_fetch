<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Reyes's I: Measuring Spatial Autocorrelation in Compositions</title>
      <link>https://arxiv.org/abs/2512.04289</link>
      <description>arXiv:2512.04289v1 Announce Type: new 
Abstract: Compositional observations arise when measurements are recorded as parts of a whole, so that only relative information is meaningful and the natural sample space is the simplex equipped with Aitchison geometry. Despite extensive development of compositional methods, a direct analogue of Moran's \(I\) for assessing spatial autocorrelation in areal compositional data has been lacking. We propose Reyes's \(I\), a Moran type statistic defined through the Aitchison inner product and norm, which is invariant to scale, to permutations of the parts, and to the choice of the \(\operatorname{ilr}\) contrast matrix. Under the randomization assumption, we derive an upper bound, the expected value, and the noncentral second moment, and we describe exact and Monte Carlo permutation procedures for inference. Through simulations covering identical, independent, and spatially correlated compositions under multiple covariance structures and neighborhood definitions, we show that Reyes's \(I\) provides stable behavior, competitive calibration, and improved efficiency relative to a naive alternative based on averaging componentwise Moran statistics. We illustrate practical utility by studying the spatial dependence of a composition measuring COVID-19 severity across Colombian departments during January 2021, documenting significant positive autocorrelation early in the month that attenuates over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04289v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lina Buitrago, Juan Sosa, Oscar Melo</dc:creator>
    </item>
    <item>
      <title>Sequential Randomization Tests Using E-values: A Betting Approach for Clinical Trials</title>
      <link>https://arxiv.org/abs/2512.04366</link>
      <description>arXiv:2512.04366v1 Announce Type: new 
Abstract: Sequential monitoring of randomized trials traditionally relies on parametric assumptions or asymptotic approximations. We present a nonparametric sequential test, the randomization e-process (e-RT), that derives validity solely from the randomization mechanism. Using a betting framework, e-RT constructs a test martingale by sequentially wagering on treatment assignments given observed outcomes. Under the null hypothesis of no treatment effect, the expected wealth cannot grow, guaranteeing anytime-valid Type I error control regardless of stopping rule. We prove validity and present simulation studies demonstrating calibration and power. The e-RT provides a conservative, assumption-free complement to model-based sequential analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04366v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando G Zampieri</dc:creator>
    </item>
    <item>
      <title>Learning Heterogeneous Ordinal Graphical Models via Bayesian Nonparametric Clustering</title>
      <link>https://arxiv.org/abs/2512.04407</link>
      <description>arXiv:2512.04407v1 Announce Type: new 
Abstract: Graphical models are powerful tools for capturing conditional dependence structures in complex systems but remain underexplored in analyzing ordinal data, especially in sports analytics. Ordinal variables, such as team rankings, player performance ratings, and survey responses, are pervasive in sports data but present unique challenges, particularly when accounting for heterogeneous subgroups, such as teams with varying styles or players with distinct roles. Existing methods, including probit graphical models, struggle with modeling heterogeneity and selecting the number of subgroups effectively. We propose a novel nonparametric Bayesian framework using the Mixture of Finite Mixtures (MFM) approach to address these challenges. Our method allows for flexible subgroup discovery and models each subgroup with a probit graphical model, simultaneously estimating the number of clusters and their configurations. We develop an efficient Gibbs sampling algorithm for inference, enabling robust estimation of cluster-specific structures and parameters. This framework is particularly suited to sports analytics, uncovering latent patterns in player performance metrics. Our work bridges critical gaps in modeling ordinal data and provides a foundation for advanced decision-making in sports performance and strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04407v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wang Wen, Ziqi Chen, Guanyu Hu</dc:creator>
    </item>
    <item>
      <title>Multi-source Learning for Target Population by High-dimensional Calibration</title>
      <link>https://arxiv.org/abs/2512.04412</link>
      <description>arXiv:2512.04412v1 Announce Type: new 
Abstract: Multi-source learning is an emerging area of research in statistics, where information from multiple datasets with heterogeneous distributions is combined to estimate the parameter of interest for a target population without observed responses. We propose a high-dimensional debiased calibration (HDC) method and a multi-source HDC (MHDC) estimator for general estimating equations. The HDC method uses a novel approach to achieve Neyman orthogonality for the target parameter via high-dimensional covariate balancing on an augmented set of covariates. It avoids the augmented inverse probability weighting formulation and leads to an easier optimization algorithm for the target parameter in estimating equations and M-estimation. The proposed MHDC estimator integrates multi-source data while supporting flexible specifications for both density ratio and outcome regression models, achieving multiple robustness against model misspecification. Its asymptotic normality is established, and a specification test is proposed to examine the transferability condition for the multi-source data. Compared to the linear combination of single-source HDC estimators, the MHDC estimator improves efficiency by jointly utilizing all data sources. Through simulation studies, we show that the MHDC estimator accommodates multiple sources and multiple working models effectively and performs better than the existing doubly robust estimators for multi-source learning. An empirical analysis of a meteorological dataset demonstrates the utility of the proposed method in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04412v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoxiang Zhan, Jae Kwang Kim, Yumou Qiu</dc:creator>
    </item>
    <item>
      <title>Bayesian Graphical High-Dimensional Time Series Models for Detecting Structural Changes</title>
      <link>https://arxiv.org/abs/2512.04444</link>
      <description>arXiv:2512.04444v1 Announce Type: new 
Abstract: We study the structural changes in multivariate time-series by estimating and comparing stationary graphs for macroeconomic time series before and after an economic crisis such as the Great Recession. Building on a latent time series framework called Orthogonally-rotated Univariate Time-series (OUT), we propose a shared-parameter framework-the spOUT autoregressive model (spOUTAR)-that jointly models two related multivariate time series and enables coherent Bayesian estimation of their corresponding stationary precision matrices. This framework provides a principled mechanism to detect and quantify which conditional relationships among the variables changed, or formed following the crisis. Specifically, we study the impact of the Great Recession (December 2007-June 2009) that substantially disrupted global and national economies, prompting long-lasting shifts in macroeconomic indicators and their interrelationships. While many studies document its economic consequences, far less is known about how the underlying conditional dependency structure among economic variables changed as economies moved from pre-crisis stability through the shock and back to normalcy. Using the proposed approach to analyze U.S. and OECD macroeconomic data, we demonstrate that spOUTAR effectively captures recession-induced changes in stationary graphical structure, offering a flexible and interpretable tool for studying structural shifts in economic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04444v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuvrarghya Ghosh, Arkaprava Roy, Anindya Roy, Subhashis Ghosal</dc:creator>
    </item>
    <item>
      <title>Tensor Neyman-Pearson Classification: Theory, Algorithms, and Error Control</title>
      <link>https://arxiv.org/abs/2512.04583</link>
      <description>arXiv:2512.04583v1 Announce Type: new 
Abstract: Biochemical discovery increasingly relies on classifying molecular structures when the consequences of different errors are highly asymmetric. In mutagenicity and carcinogenicity, misclassifying a harmful compound as benign can trigger substantial scientific, regulatory, and health risks, whereas false alarms primarily increase laboratory workload. Modern representations transform molecular graphs into persistence image tensors that preserve multiscale geometric and topological structure, yet existing tensor classifiers and deep tensor neural networks provide no finite-sample guarantees on type I error and often exhibit severe error inflation in practice.
  We develop the first Tensor Neyman-Pearson (Tensor-NP) classification framework that achieves finite-sample control of type I error while exploiting the multi-mode structure of tensor data. Under a tensor-normal mixture model, we derive the oracle NP discriminant, characterize its Tucker low-rank manifold geometry, and establish tensor-specific margin and conditional detection conditions enabling high-probability bounds on excess type II error. We further propose a Discriminant Tensor Iterative Projection estimator and a Tensor-NP Neural Classifier combining deep learning with Tensor-NP umbrella calibration, yielding the first distribution-free NP-valid methods for multiway data. Across four biochemical datasets, Tensor-NP classifiers maintain type I errors at prespecified levels while delivering competitive type II error performance, providing reliable tools for asymmetric-risk decisions with complex molecular tensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04583v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingchong Liu, Elynn Chen, Yuefeng Han, Lucy Xia</dc:creator>
    </item>
    <item>
      <title>Model-Free Assessment of Simulator Fidelity via Quantile Curves</title>
      <link>https://arxiv.org/abs/2512.05024</link>
      <description>arXiv:2512.05024v1 Announce Type: new 
Abstract: Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05024v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Garud Iyengar, Yu-Shiou Willy Lin, Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>High-Resolution Retrieval of Atmospheric Boundary Layers with Nonstationary Gaussian Processes</title>
      <link>https://arxiv.org/abs/2512.04217</link>
      <description>arXiv:2512.04217v1 Announce Type: cross 
Abstract: The atmospheric boundary layer (ABL) plays a critical role in governing turbulent exchanges of momentum, heat moisture, and trace gases between the Earth's surface and the free atmosphere, thereby influencing meteorological phenomena, air quality, and climate processes. Accurate and temporally continuous characterization of the ABL structure and height evolution is crucial for both scientific understanding and practical applications. High-resolution retrievals of the ABL height from vertical velocity measurements is challenging because it is often estimated using empirical thresholds applied to profiles of vertical velocity variance or related turbulence diagnostics at each measurement altitude, which can suffer from limited sampling and sensitivity to noise. To address these limitations, this work employs nonstationary Gaussian process (GP) modeling to more effectively capture the spatio-temporal dependence structure in the data, enabling high-quality -- and, if desired, high-resolution -- estimates of the ABL height without reliance on ad-hoc parameter tuning. By leveraging Vecchia approximations, the proposed method can be applied to large-scale datasets, and example applications using full-day vertical velocity profiles comprising approximately $5$M measurements are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04217v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haoran Xiong, Paytsar Muradyan, Christopher J. Geoga</dc:creator>
    </item>
    <item>
      <title>Detecting Perspective Shifts in Multi-agent Systems</title>
      <link>https://arxiv.org/abs/2512.05013</link>
      <description>arXiv:2512.05013v1 Announce Type: cross 
Abstract: Generative models augmented with external tools and update mechanisms (or \textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05013v1</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Bridgeford, Hayden Helm</dc:creator>
    </item>
    <item>
      <title>Multi-species count transformation models</title>
      <link>https://arxiv.org/abs/2201.13095</link>
      <description>arXiv:2201.13095v2 Announce Type: replace 
Abstract: Joint Species Distribution Models are essential for understanding how ecological drivers shape species communities. However, most existing approaches are limited by rigid parametric distributions for count data and the inability to model how interspecific interactions change in response to those drivers. We introduce multi-species count transformation models, a novel framework designed to overcome these limitations. Our approach combines flexible, distribution-free marginal species count transformation models for each species' count abundance, with a driver-dependent latent Gaussian copula modelling interspecific correlations, interpretable as Spearman's rank correlation on the scale of the counts. All model parameters are estimated efficiently via joint maximum likelihood estimation, implemented in the R package cotram.
  We apply this framework to model the joint abundance of three fish-eating bird species, using seasonality as the primary driver. Our model successfully captured the complex, species-specific seasonal abundance patterns, including periods of high zero-counts and seasonal shifts in variance. Furthermore, the model revealed strong, seasonally-varying correlations between the species. These findings are consistent with an empirical approach and similar to those from the computationally expensive parametric Bayesian Hierarchical Modelling of Species Communities (HMSC) framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.13095v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Graz, Luisa Barbanti, Roland Brandl, Torsten Hothorn</dc:creator>
    </item>
    <item>
      <title>A generalized e-value feature detection method with FDR control at multiple resolutions</title>
      <link>https://arxiv.org/abs/2409.17039</link>
      <description>arXiv:2409.17039v4 Announce Type: replace 
Abstract: Multiple resolutions arise across a range of explanatory features due to domain-specific structures, leading to the formation of feature groups. It follows that the simultaneous detection of significant features and groups aimed at a specific response with false discovery rate (FDR) control stands as a crucial issue, such as the spatial genome-wide association studies. Nevertheless, existing methods such as the multilayer knockoff filter (MKF) generally require a uniform detection approach across resolutions to achieve multilayer FDR control, which can be not powerful or even not applicable in several settings. To fix this issue effectively, this article develops a novel method of stabilized flexible e-filter procedure (SFEFP), by constructing unified generalized e-values, developing a generalized e-filter, and adopting a stabilization treatment. This method flexibly incorporates a wide variety of base detection procedures that operate effectively across different resolutions to provide stable and consistent results, while controlling the false discovery rate at multiple resolutions simultaneously. Furthermore, we investigate the statistical theories of the SFEFP, encompassing multilayer FDR control and stability guarantee. We develop several examples for SFEFP such as eDS-filter and eDS+gKF-filter. Simulation studies demonstrate that the eDS-filter effectively controls FDR at multiple resolutions while either maintaining or enhancing power compared to MKF. The superiority of the eDS-filter is also demonstrated through the analysis of HIV mutation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17039v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyao Yu, Ruixing Ming, Min Xiao, Zhanfeng Wang, Bingyi Jing</dc:creator>
    </item>
    <item>
      <title>Thompson, Ulam, or Gauss? Multi-criteria recommendations for posterior probability computation methods in Bayesian response-adaptive trials</title>
      <link>https://arxiv.org/abs/2411.19871</link>
      <description>arXiv:2411.19871v3 Announce Type: replace 
Abstract: Bayesian adaptive designs enable flexible clinical trials by adapting features based on accumulating data. Among these, Bayesian Response-Adaptive Randomization (BRAR) skews patient allocation towards more promising treatments based on interim data. Implementing BRAR requires the relatively quick evaluation of posterior probabilities. However, the limitations of existing closed-form solutions mean trials often rely on computationally intensive approximations which can impact accuracy and the scope of scenarios explored. While faster Gaussian approximations exist, their reliability is not guaranteed. Critically, the approximation method used is often poorly reported, and the literature lacks practical guidance for selecting and comparing these methods, particularly regarding the trade-offs between computational speed, inferential accuracy, and their implications for patient benefit.
  In this paper, we focus on BRAR trials with binary endpoints, developing a novel algorithm that efficiently and exactly computes these posterior probabilities, enabling a robust assessment of existing approximation methods in use. Leveraging these exact computations, we establish a comprehensive benchmark for evaluating approximation methods based on their computational speed, patient benefit, and inferential accuracy. Our comprehensive analysis, conducted through a range of simulations in the two-armed case and a re-analysis of the three-armed Established Status Epilepticus Treatment Trial, reveals that the exact calculation algorithm is often the fastest, even for up to 12 treatment arms. Furthermore, we demonstrate that commonly used approximation methods can lead to significant power loss and Type I error rate inflation. We conclude by providing practical guidance to aid practitioners in selecting the most appropriate computation method for various clinical trial settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19871v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Kaddaj, Stef Baas, Edwin Y. N. Tang, David S. Robertson, Lukas Pin, Sof\'ia S. Villar</dc:creator>
    </item>
    <item>
      <title>Lecture Notes on High Dimensional Linear Regression</title>
      <link>https://arxiv.org/abs/2412.15633</link>
      <description>arXiv:2412.15633v2 Announce Type: replace 
Abstract: These lecture notes cover advanced topics in linear regression, with an in-depth exploration of the existence, uniqueness, relations, computation, and non-asymptotic properties of the most prominent estimators in this setting. The covered estimators include least squares, ridgeless, ridge, and lasso. The content follows a proposition-proof structure, making it suitable for students seeking a formal and rigorous understanding of the statistical theory underlying machine learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15633v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alberto Quaini</dc:creator>
    </item>
    <item>
      <title>Variational inference for approximate objective priors using neural networks</title>
      <link>https://arxiv.org/abs/2502.02364</link>
      <description>arXiv:2502.02364v2 Announce Type: replace 
Abstract: In Bayesian statistics, the choice of the prior can have an important influence on the posterior and the parameter estimation, especially when few data samples are available. To limit the added subjectivity from a priori information, one can use the framework of objective priors, more particularly, we focus on reference priors in this work. However, computing such priors is a difficult task in general. Hence, we consider cases where the reference prior simplifies to the Jeffreys prior. We develop in this paper a flexible algorithm based on variational inference which computes approximations of priors from a set of parametric distributions using neural networks. We also show that our algorithm can retrieve modified Jeffreys priors when constraints are specified in the optimization problem to ensure the solution is proper. We propose a simple method to recover a relevant approximation of the parametric posterior distribution using Markov Chain Monte Carlo (MCMC) methods even if the density function of the parametric prior is not known in general. Numerical experiments on several statistical models of increasing complexity are presented. We show the usefulness of this approach by recovering the target distribution. The performance of the algorithm is evaluated on both prior and posterior distributions, jointly using variational inference and MCMC sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02364v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.57750/76fh-t442</arxiv:DOI>
      <arxiv:journal_reference>Computo, December 2025</arxiv:journal_reference>
      <dc:creator>Nils Baillie, Antoine Van Biesbroeck, Cl\'ement Gauchy</dc:creator>
    </item>
    <item>
      <title>Constrained Gaussian Random Fields with Continuous Linear Boundary Restrictions for Physics-informed Modeling of States</title>
      <link>https://arxiv.org/abs/2511.22868</link>
      <description>arXiv:2511.22868v2 Announce Type: replace 
Abstract: Boundary constraints in physical, environmental and engineering models restrict smooth states such as temperature to follow known physical laws at the edges of their spatio-temporal domain. Examples include fixed-state or fixed-derivative (insulated) boundary conditions, and constraints that relate the state and the derivatives, such as in models of heat transfer. Despite their flexibility as prior models over system states, Gaussian random fields do not in general enable exact enforcement of such constraints. This work develops a new general framework for constructing linearly boundary-constrained Gaussian random fields from unconstrained Gaussian random fields over multi-dimensional, convex domains. This new class of models provides flexible priors for modeling smooth states with known physical mechanisms acting at the domain boundaries. Simulation studies illustrate how such physics-informed probability models yield improved predictive performance and more realistic uncertainty quantification in applications including probabilistic numerics, data-driven discovery of dynamical systems, and boundary-constrained state estimation, as compared to unconstrained alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22868v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Ma, Oksana A. Chkrebtii, Stephen R. Niezgoda</dc:creator>
    </item>
    <item>
      <title>Implicit score-driven filters for time-varying parameter models</title>
      <link>https://arxiv.org/abs/2512.02744</link>
      <description>arXiv:2512.02744v2 Announce Type: replace 
Abstract: We propose an observation-driven modeling framework that permits time variation in the model parameters using an implicit score-driven (ISD) update. The ISD update maximizes the logarithmic observation density with respect to the parameter vector, while penalizing the weighted L2 norm relative to a one-step-ahead predicted parameter. This yields an implicit stochastic-gradient update. We show that the popular class of explicit score-driven (ESD) models arises if the observation log density is linearly approximated around the prediction. By preserving the full density, the ISD update globalizes favorable local properties of the ESD update. Namely, for log-concave observation densities, whether correctly specified or not, the ISD filter is stable for all learning rates, while its updates are contractive in mean squared error toward the (pseudo-)true parameter at every time step. We demonstrate the usefulness of ISD filters in simulations and empirical illustrations in finance and macroeconomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02744v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rutger-Jan Lange, Bram van Os, Dick van Dijk</dc:creator>
    </item>
    <item>
      <title>Parsimonious Factor Models for Asymmetric Dependence in Multivariate Extremes</title>
      <link>https://arxiv.org/abs/2512.03543</link>
      <description>arXiv:2512.03543v2 Announce Type: replace 
Abstract: Modelling multivariate extreme events is essential when extrapolating beyond the range of observed data. Parametric models that are suitable for real-world extremes must be flexible -- particularly in their ability to capture asymmetric dependence structures -- while also remaining parsimonious for interpretability and computationally scalable in high dimensions. Although many models have been proposed, it is rare for any single construction to satisfy all of these requirements. For instance, the popular H\"usler-Reiss model is limited to symmetric dependence structures. In this manuscript, we introduce a class of additive factor models and derive their extreme-value limits. This leads to a broad and tractable family of models characterised by a manageable number of parameters. These models naturally accommodate asymmetric tail dependence and allow for non-stationary behaviour. We present the limiting models from both the componentwise-maxima and Peaks-over-Thresholds perspectives, via the multivariate extreme value and multivariate generalized Pareto distributions, respectively. Simulation studies illustrate identifiability properties based on existing inference methodologies. Finally, applications to summer temperature maxima in Melbourne, Australia, and to weekly negative returns from four major UK banks demonstrate improved fit compared with the H\"usler-Reiss model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03543v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pavel Krupskii, Boris B\'eranger</dc:creator>
    </item>
    <item>
      <title>SUP: An Inferable Private Multiple Testing Framework with Super Uniformity</title>
      <link>https://arxiv.org/abs/2512.03859</link>
      <description>arXiv:2512.03859v2 Announce Type: replace 
Abstract: Multiple testing is widely applied across scientific fields, particularly in genomic and health data analysis, where protecting sensitive personal information is imperative. However, developing private multiple testing algorithms for super uniform $p$-values remains an open question, as privacy mechanisms introduce intricate dependence among the peeled $p$-values and disrupt their super uniformity, complicating post-selection inference. To address this, we introduce a general Super Uniform Private (SUP) multiple testing framework with three key components. First, we develop a novel \( p \)-value transformation that is compatible with diverse privacy regimes while retaining the super uniformity. Next, a reversed peeling algorithm is designed to reduce privacy budgets while facilitating inference. Then, we provide diverse rejection thresholds that are privacy-parameter-free and tailored for different Type-I errors, including the family-wise error rate (FWER) and the false discovery rate (FDR). Building upon these, we advance adaptive techniques to determine the peeling number and boost thresholds. Theoretically, we propose a technique overcoming the post-selection obstacle to Type-I error control, quantify the privacy-induced power loss of SUP relative to its non-private counterpart, and demonstrate that SUP surpasses existing private methods in terms of power. The results of extensive simulations and a real data application validate our theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03859v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kehan Wang, Wenxuan Song, Wangli Xu, Linglong Kong</dc:creator>
    </item>
  </channel>
</rss>
