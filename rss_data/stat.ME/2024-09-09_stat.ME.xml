<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Sep 2024 02:49:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exploratory Visual Analysis for Increasing Data Readiness in Artificial Intelligence Projects</title>
      <link>https://arxiv.org/abs/2409.03805</link>
      <description>arXiv:2409.03805v1 Announce Type: new 
Abstract: We present experiences and lessons learned from increasing data readiness of heterogeneous data for artificial intelligence projects using visual analysis methods. Increasing the data readiness level involves understanding both the data as well as the context in which it is used, which are challenges well suitable to visual analysis. For this purpose, we contribute a mapping between data readiness aspects and visual analysis techniques suitable for different data types. We use the defined mapping to increase data readiness levels in use cases involving time-varying data, including numerical, categorical, and text. In addition to the mapping, we extend the data readiness concept to better take aspects of the task and solution into account and explicitly address distribution shifts during data collection time. We report on our experiences in using the presented visual analysis techniques to aid future artificial intelligence projects in raising the data readiness level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03805v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mattias Tiger, Daniel Jakobsson, Anders Ynnerman, Fredrik Heintz, Daniel J\"onsson</dc:creator>
    </item>
    <item>
      <title>Average Causal Effect Estimation in DAGs with Hidden Variables: Extensions of Back-Door and Front-Door Criteria</title>
      <link>https://arxiv.org/abs/2409.03962</link>
      <description>arXiv:2409.03962v1 Announce Type: new 
Abstract: The identification theory for causal effects in directed acyclic graphs (DAGs) with hidden variables is well-developed, but methods for estimating and inferring functionals beyond the g-formula remain limited. Previous studies have proposed semiparametric estimators for identifiable functionals in a broad class of DAGs with hidden variables. While demonstrating double robustness in some models, existing estimators face challenges, particularly with density estimation and numerical integration for continuous variables, and their estimates may fall outside the parameter space of the target estimand. Their asymptotic properties are also underexplored, especially when using flexible statistical and machine learning models for nuisance estimation. This study addresses these challenges by introducing novel one-step corrected plug-in and targeted minimum loss-based estimators of causal effects for a class of DAGs that extend classical back-door and front-door criteria (known as the treatment primal fixability criterion in prior literature). These estimators leverage machine learning to minimize modeling assumptions while ensuring key statistical properties such as asymptotic linearity, double robustness, efficiency, and staying within the bounds of the target parameter space. We establish conditions for nuisance functional estimates in terms of L2(P)-norms to achieve root-n consistent causal effect estimates. To facilitate practical application, we have developed the flexCausal package in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03962v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Guo, Razieh Nabi</dc:creator>
    </item>
    <item>
      <title>Fitting the Discrete Swept Skeletal Representation to Slabular Objects</title>
      <link>https://arxiv.org/abs/2409.04079</link>
      <description>arXiv:2409.04079v1 Announce Type: new 
Abstract: Statistical shape analysis of slabular objects like groups of hippocampi is highly useful for medical researchers as it can be useful for diagnoses and understanding diseases. This work proposes a novel object representation based on locally parameterized discrete swept skeletal structures. Further, model fitting and analysis of such representations are discussed. The model fitting procedure is based on boundary division and surface flattening. The quality of the model fitting is evaluated based on the symmetry and tidiness of the skeletal structure as well as the volume of the implied boundary. The power of the method is demonstrated by visual inspection and statistical analysis of a synthetic and an actual data set in comparison with an available skeletal representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04079v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Taheri, Stephen M. Pizer, J\"orn Schulz</dc:creator>
    </item>
    <item>
      <title>Incorporating external data for analyzing randomized clinical trials: A transfer learning approach</title>
      <link>https://arxiv.org/abs/2409.04126</link>
      <description>arXiv:2409.04126v1 Announce Type: new 
Abstract: Randomized clinical trials are the gold standard for analyzing treatment effects, but high costs and ethical concerns can limit recruitment, potentially leading to invalid inferences. Incorporating external trial data with similar characteristics into the analysis using transfer learning appears promising for addressing these issues. In this paper, we present a formal framework for applying transfer learning to the analysis of clinical trials, considering three key perspectives: transfer algorithm, theoretical foundation, and inference method. For the algorithm, we adopt a parameter-based transfer learning approach to enhance the lasso-adjusted stratum-specific estimator developed for estimating treatment effects. A key component in constructing the transfer learning estimator is deriving the regression coefficient estimates within each stratum, accounting for the bias between source and target data. To provide a theoretical foundation, we derive the $l_1$ convergence rate for the estimated regression coefficients and establish the asymptotic normality of the transfer learning estimator. Our results show that when external trial data resembles current trial data, the sample size requirements can be reduced compared to using only the current trial data. Finally, we propose a consistent nonparametric variance estimator to facilitate inference. Numerical studies demonstrate the effectiveness and robustness of our proposed estimator across various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04126v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yujia Gu, Hanzhong Liu, Wei Ma</dc:creator>
    </item>
    <item>
      <title>Modelling multivariate spatio-temporal data with identifiable variational autoencoders</title>
      <link>https://arxiv.org/abs/2409.04162</link>
      <description>arXiv:2409.04162v1 Announce Type: new 
Abstract: Modelling multivariate spatio-temporal data with complex dependency structures is a challenging task but can be simplified by assuming that the original variables are generated from independent latent components. If these components are found, they can be modelled univariately. Blind source separation aims to recover the latent components by estimating the unmixing transformation based on the observed data only. The current methods for spatio-temporal blind source separation are restricted to linear unmixing, and nonlinear variants have not been implemented. In this paper, we extend identifiable variational autoencoder to the nonlinear nonstationary spatio-temporal blind source separation setting and demonstrate its performance using comprehensive simulation studies. Additionally, we introduce two alternative methods for the latent dimension estimation, which is a crucial task in order to obtain the correct latent representation. Finally, we illustrate the proposed methods using a meteorological application, where we estimate the latent dimension and the latent components, interpret the components, and show how nonstationarity can be accounted and prediction accuracy can be improved by using the proposed nonlinear blind source separation method as a preprocessing method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04162v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mika Sipil\"a, Claudia Cappello, Sandra De Iaco, Klaus Nordhausen, Sara Taskinen</dc:creator>
    </item>
    <item>
      <title>The $\infty$-S test via regression quantile affine LASSO</title>
      <link>https://arxiv.org/abs/2409.04256</link>
      <description>arXiv:2409.04256v1 Announce Type: new 
Abstract: The nonparametric sign test dates back to the early 18th century with a data analysis by John Arbuthnot. It is an alternative to Gosset's more recent $t$-test for consistent differences between two sets of observations. Fisher's $F$-test is a generalization of the $t$-test to linear regression and linear null hypotheses. Only the sign test is robust to non-Gaussianity. Gutenbrunner et al. [1993] derived a version of the sign test for linear null hypotheses in the spirit of the F-test, which requires the difficult estimation of the sparsity function. We propose instead a new sign test called $\infty$-S test via the convex analysis of a point estimator that thresholds the estimate towards the null hypothesis of the test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04256v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sylvain Sardy, Xiaoyu Ma, Hugo Gaible</dc:creator>
    </item>
    <item>
      <title>Robust Elicitable Functionals</title>
      <link>https://arxiv.org/abs/2409.04412</link>
      <description>arXiv:2409.04412v1 Announce Type: new 
Abstract: Elicitable functionals and (strict) consistent scoring functions are of interest due to their utility of determining (uniquely) optimal forecasts, and thus the ability to effectively backtest predictions. However, in practice, assuming that a distribution is correctly specified is too strong a belief to reliably hold. To remediate this, we incorporate a notion of statistical robustness into the framework of elicitable functionals, meaning that our robust functional accounts for "small" misspecifications of a baseline distribution. Specifically, we propose a robustified version of elicitable functionals by using the Kullback-Leibler divergence to quantify potential misspecifications from a baseline distribution. We show that the robust elicitable functionals admit unique solutions lying at the boundary of the uncertainty region. Since every elicitable functional possesses infinitely many scoring functions, we propose the class of b-homogeneous strictly consistent scoring functions, for which the robust functionals maintain desirable statistical properties. We show the applicability of the REF in two examples: in the reinsurance setting and in robust regression problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04412v1</guid>
      <category>stat.ME</category>
      <category>q-fin.MF</category>
      <category>q-fin.RM</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kathleen E. Miao, Silvana M. Pesenti</dc:creator>
    </item>
    <item>
      <title>A tutorial on panel data analysis using partially observed Markov processes via the R package panelPomp</title>
      <link>https://arxiv.org/abs/2409.03876</link>
      <description>arXiv:2409.03876v1 Announce Type: cross 
Abstract: The R package panelPomp supports analysis of panel data via a general class of partially observed Markov process models (PanelPOMP). This package tutorial describes how the mathematical concept of a PanelPOMP is represented in the software and demonstrates typical use-cases of panelPomp. Monte Carlo methods used for POMP models require adaptation for PanelPOMP models due to the higher dimensionality of panel data. The package takes advantage of recent advances for PanelPOMP, including an iterated filtering algorithm, Monte Carlo adjusted profile methodology and block optimization methodology to assist with the large parameter spaces that can arise with panel models. In addition, tools for manipulation of models and data are provided that take advantage of the panel structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03876v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carles Breto, Jesse Wheeler, Aaron A. King, Edward L. Ionides</dc:creator>
    </item>
    <item>
      <title>Extreme Quantile Treatment Effects under Endogeneity: Evaluating Policy Effects for the Most Vulnerable Individuals</title>
      <link>https://arxiv.org/abs/2409.03979</link>
      <description>arXiv:2409.03979v1 Announce Type: cross 
Abstract: We introduce a novel method for estimating and conducting inference about extreme quantile treatment effects (QTEs) in the presence of endogeneity. Our approach is applicable to a broad range of empirical research designs, including instrumental variables design and regression discontinuity design, among others. By leveraging regular variation and subsampling, the method ensures robust performance even in extreme tails, where data may be sparse or entirely absent. Simulation studies confirm the theoretical robustness of our approach. Applying our method to assess the impact of job training provided by the Job Training Partnership Act (JTPA), we find significantly negative QTEs for the lowest quantiles (i.e., the most disadvantaged individuals), contrasting with previous literature that emphasizes positive QTEs for intermediate quantiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03979v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yuya Sasaki, Yulong Wang</dc:creator>
    </item>
    <item>
      <title>Over-parameterized regression methods and their application to semi-supervised learning</title>
      <link>https://arxiv.org/abs/2409.04001</link>
      <description>arXiv:2409.04001v1 Announce Type: cross 
Abstract: The minimum norm least squares is an estimation strategy under an over-parameterized case and, in machine learning, is known as a helpful tool for understanding a nature of deep learning. In this paper, to apply it in a context of non-parametric regression problems, we established several methods which are based on thresholding of SVD (singular value decomposition) components, wihch are referred to as SVD regression methods. We considered several methods that are singular value based thresholding, hard-thresholding with cross validation, universal thresholding and bridge thresholding. Information on output samples is not utilized in the first method while it is utilized in the other methods. We then applied them to semi-supervised learning, in which unlabeled input samples are incorporated into kernel functions in a regressor. The experimental results for real data showed that, depending on the datasets, the SVD regression methods is superior to a naive ridge regression method. Unfortunately, there were no clear advantage of the methods utilizing information on output samples. Furthermore, for depending on datasets, incorporation of unlabeled input samples into kernels is found to have certain advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04001v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katsuyuki Hagiwara</dc:creator>
    </item>
    <item>
      <title>Leveraging Machine Learning for Official Statistics: A Statistical Manifesto</title>
      <link>https://arxiv.org/abs/2409.04365</link>
      <description>arXiv:2409.04365v1 Announce Type: cross 
Abstract: It is important for official statistics production to apply ML with statistical rigor, as it presents both opportunities and challenges. Although machine learning has enjoyed rapid technological advances in recent years, its application does not possess the methodological robustness necessary to produce high quality statistical results. In order to account for all sources of error in machine learning models, the Total Machine Learning Error (TMLE) is presented as a framework analogous to the Total Survey Error Model used in survey methodology. As a means of ensuring that ML models are both internally valid as well as externally valid, the TMLE model addresses issues such as representativeness and measurement errors. There are several case studies presented, illustrating the importance of applying more rigor to the application of machine learning in official statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04365v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Puts, David Salgado, Piet Daas</dc:creator>
    </item>
    <item>
      <title>Local times of self-intersection and sample path properties of Volterra Gaussian processes</title>
      <link>https://arxiv.org/abs/2409.04377</link>
      <description>arXiv:2409.04377v1 Announce Type: cross 
Abstract: We study a Volterra Gaussian process of the form $X(t)=\int^t_0K(t,s)d{W(s)},$ where $W$ is a Wiener process and $K$ is a continuous kernel. In dimension one, we prove a law of the iterated logarithm, discuss the existence of local times and verify a continuous dependence between the local time and the kernel that generates the process. Furthermore, we prove the existence of the Rosen renormalized self-intersection local times for a planar Gaussian Volterra process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04377v1</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olga Izyumtseva, Wasiur R. KhudaBukhsh</dc:creator>
    </item>
    <item>
      <title>A confounding bridge approach for double negative control inference on causal effects</title>
      <link>https://arxiv.org/abs/1808.04945</link>
      <description>arXiv:1808.04945v4 Announce Type: replace 
Abstract: Unmeasured confounding is a key challenge for causal inference. In this paper, we establish a framework for unmeasured confounding adjustment with negative control variables. A negative control outcome is associated with the confounder but not causally affected by the exposure in view, and a negative control exposure is correlated with the primary exposure or the confounder but does not causally affect the outcome of interest. We introduce an outcome confounding bridge function that depicts the relationship between the confounding effects on the primary outcome and the negative control outcome, and we incorporate a negative control exposure to identify the bridge function and the average causal effect. We also consider the extension to the positive control setting by allowing for nonzero causal effect of the primary exposure on the control outcome. We illustrate our approach with simulations and apply it to a study about the short-term effect of air pollution on mortality. Although a standard analysis shows a significant acute effect of PM2.5 on mortality, our analysis indicates that this effect may be confounded, and after double negative control adjustment, the effect is attenuated toward zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:1808.04945v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wang Miao, Xu Shi, Yilin Li, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Estimation of Over-parameterized Models from an Auto-Modeling Perspective</title>
      <link>https://arxiv.org/abs/2206.01824</link>
      <description>arXiv:2206.01824v4 Announce Type: replace 
Abstract: From a model-building perspective, we propose a paradigm shift for fitting over-parameterized models. Philosophically, the mindset is to fit models to future observations rather than to the observed sample. Technically, given an imputation method to generate future observations, we fit over-parameterized models to these future observations by optimizing an approximation of the desired expected loss function based on its sample counterpart and an adaptive $\textit{duality function}$. The required imputation method is also developed using the same estimation technique with an adaptive $m$-out-of-$n$ bootstrap approach. We illustrate its applications with the many-normal-means problem, $n &lt; p$ linear regression, and neural network-based image classification of MNIST digits. The numerical results demonstrate its superior performance across these diverse applications. While primarily expository, the paper conducts an in-depth investigation into the theoretical aspects of the topic. It concludes with remarks on some open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01824v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jiang, Chuanhai Liu</dc:creator>
    </item>
    <item>
      <title>Jeffreys-prior penalty for high-dimensional logistic regression: A conjecture about aggregate bias</title>
      <link>https://arxiv.org/abs/2311.11290</link>
      <description>arXiv:2311.11290v2 Announce Type: replace 
Abstract: Firth (1993, Biometrika) shows that the maximum Jeffreys' prior penalized likelihood estimator in logistic regression has asymptotic bias decreasing with the square of the number of observations when the number of parameters is fixed, which is an order faster than the typical rate from maximum likelihood. The widespread use of that estimator in applied work is supported by the results in Kosmidis and Firth (2021, Biometrika), who show that it takes finite values, even in cases where the maximum likelihood estimate does not exist. Kosmidis and Firth (2021, Biometrika) also provide empirical evidence that the estimator has good bias properties in high-dimensional settings where the number of parameters grows asymptotically linearly but slower than the number of observations. We design and carry out a large-scale computer experiment covering a wide range of such high-dimensional settings and produce strong empirical evidence for a simple rescaling of the maximum Jeffreys' prior penalized likelihood estimator that delivers high accuracy in signal recovery, in terms of aggregate bias, in the presence of an intercept parameter. The rescaled estimator is effective even in cases where estimates from maximum likelihood and other recently proposed corrective methods based on approximate message passing do not exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11290v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Kosmidis, Patrick Zietkiewicz</dc:creator>
    </item>
    <item>
      <title>LASPATED: a Library for the Analysis of SPAtio-TEmporal Discrete data</title>
      <link>https://arxiv.org/abs/2401.04156</link>
      <description>arXiv:2401.04156v3 Announce Type: replace 
Abstract: We describe methods, tools, and a software library called LASPATED, available on GitHub (at https://github.com/vguigues/) to fit models using spatio-temporal data and space-time discretization. A video tutorial for this library is available on YouTube. We consider two types of methods to estimate a non-homogeneous Poisson process in space and time. The methods approximate the arrival intensity function of the Poisson process by discretizing space and time, and estimating arrival intensity as a function of subregion and time interval. With such methods, it is typical that the dimension of the estimator is large relative to the amount of data, and therefore the performance of the estimator can be improved by using additional data. The first method uses additional data to add a regularization term to the likelihood function for calibrating the intensity of the Poisson process. The second method uses additional data to estimate arrival intensity as a function of covariates. We describe a Python package to perform various types of space and time discretization. We also describe two packages for the calibration of the models, one in Matlab and one in C++. We demonstrate the advantages of our methods compared to basic maximum likelihood estimation with simulated and real data. The experiments with real data calibrate models of the arrival process of emergencies to be handled by the Rio de Janeiro emergency medical service.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04156v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Guigues, Anton Kleywegt, Giovanni Amorim, Andr\'e Mazal Krauss, Victor Hugo Nascimento</dc:creator>
    </item>
    <item>
      <title>Adaptive Bayesian Structure Learning of DAGs With Non-conjugate Prior</title>
      <link>https://arxiv.org/abs/2403.17489</link>
      <description>arXiv:2403.17489v2 Announce Type: replace 
Abstract: Directed Acyclic Graphs (DAGs) are solid structures used to describe and infer the dependencies among variables in multivariate scenarios. Having a thorough comprehension of the accurate DAG-generating model is crucial for causal discovery and estimation. Our work suggests utilizing a non-conjugate prior for Gaussian DAG structure learning to enhance the posterior probability. We employ the idea of using the Bessel function to address the computational burden, providing faster MCMC computation compared to the use of conjugate priors. In addition, our proposal exhibits a greater rate of adaptation when compared to the conjugate prior, specifically for the inclusion of nodes in the DAG-generating model. Simulation studies demonstrate the superior accuracy of DAG learning, and we obtain the same maximum a posteriori and median probability model estimate for the AML data, using the non-conjugate prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17489v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. Nazari, M. Arashi, A. Sadeghkhani</dc:creator>
    </item>
    <item>
      <title>Multiply-Robust Causal Change Attribution</title>
      <link>https://arxiv.org/abs/2404.08839</link>
      <description>arXiv:2404.08839v4 Announce Type: replace 
Abstract: Comparing two samples of data, we observe a change in the distribution of an outcome variable. In the presence of multiple explanatory variables, how much of the change can be explained by each possible cause? We develop a new estimation strategy that, given a causal model, combines regression and re-weighting methods to quantify the contribution of each causal mechanism. Our proposed methodology is multiply robust, meaning that it still recovers the target parameter under partial misspecification. We prove that our estimator is consistent and asymptotically normal. Moreover, it can be incorporated into existing frameworks for causal attribution, such as Shapley values, which will inherit the consistency and large-sample distribution properties. Our method demonstrates excellent performance in Monte Carlo simulations, and we show its usefulness in an empirical application. Our method is implemented as part of the Python library DoWhy (arXiv:2011.04216, arXiv:2206.06821).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08839v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 41st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024</arxiv:journal_reference>
      <dc:creator>Victor Quintas-Martinez, Mohammad Taha Bahadori, Eduardo Santiago, Jeff Mu, Dominik Janzing, David Heckerman</dc:creator>
    </item>
    <item>
      <title>Weighted Regression with Sybil Networks</title>
      <link>https://arxiv.org/abs/2408.17426</link>
      <description>arXiv:2408.17426v3 Announce Type: replace 
Abstract: In many online domains, Sybil networks -- or cases where a single user assumes multiple identities -- is a pervasive feature. This complicates experiments, as off-the-shelf regression estimators at least assume known network topologies (if not fully independent observations) when Sybil network topologies in practice are often unknown. The literature has exclusively focused on techniques to detect Sybil networks, leading many experimenters to subsequently exclude suspected networks entirely before estimating treatment effects. I present a more efficient solution in the presence of these suspected Sybil networks: a weighted regression framework that applies weights based on the probabilities that sets of observations are controlled by single actors. I show in the paper that the MSE-minimizing solution is to set the weight matrix equal to the inverse of the expected network topology. I demonstrate the methodology on simulated data, and then I apply the technique to a competition with suspected Sybil networks run on the Sui blockchain and show reductions in the standard error of the estimate by 6 - 24%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17426v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nihar Shah</dc:creator>
    </item>
    <item>
      <title>Structural adaptation via directional regularity: rate accelerated estimation in multivariate functional data</title>
      <link>https://arxiv.org/abs/2409.00817</link>
      <description>arXiv:2409.00817v2 Announce Type: replace 
Abstract: We introduce directional regularity, a new definition of anisotropy for multivariate functional data. Instead of taking the conventional view which determines anisotropy as a notion of smoothness along a dimension, directional regularity additionally views anisotropy through the lens of directions. We show that faster rates of convergence can be obtained through a change-of-basis by adapting to the directional regularity of a multivariate process. An algorithm for the estimation and identification of the change-of-basis matrix is constructed, made possible due to the unique replication structure of functional data. Non-asymptotic bounds are provided for our algorithm, supplemented by numerical evidence from an extensive simulation study. We discuss two possible applications of the directional regularity approach, and advocate its consideration as a standard pre-processing step in multivariate functional data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00817v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Omar Kassi, Sunny G. W. Wang</dc:creator>
    </item>
    <item>
      <title>The Stochastic Proximal Distance Algorithm</title>
      <link>https://arxiv.org/abs/2210.12277</link>
      <description>arXiv:2210.12277v4 Announce Type: replace-cross 
Abstract: Stochastic versions of proximal methods have gained much attention in statistics and machine learning. These algorithms tend to admit simple, scalable forms, and enjoy numerical stability via implicit updates. In this work, we propose and analyze a stochastic version of the recently proposed proximal distance algorithm, a class of iterative optimization methods that recover a desired constrained estimation problem as a penalty parameter $\rho \rightarrow \infty$. By uncovering connections to related stochastic proximal methods and interpreting the penalty parameter as the learning rate, we justify heuristics used in practical manifestations of the proximal distance method, establishing their convergence guarantees for the first time. Moreover, we extend recent theoretical devices to establish finite error bounds and a complete characterization of convergence rates regimes. We validate our analysis via a thorough empirical study, also showing that unsurprisingly, the proposed method outpaces batch versions on popular learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.12277v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyu Jiang, Jason Xu</dc:creator>
    </item>
    <item>
      <title>A Hypergraph-Based Machine Learning Ensemble Network Intrusion Detection System</title>
      <link>https://arxiv.org/abs/2211.03933</link>
      <description>arXiv:2211.03933v3 Announce Type: replace-cross 
Abstract: Network intrusion detection systems (NIDS) to detect malicious attacks continue to meet challenges. NIDS are often developed offline while they face auto-generated port scan infiltration attempts, resulting in a significant time lag from adversarial adaption to NIDS response. To address these challenges, we use hypergraphs focused on internet protocol addresses and destination ports to capture evolving patterns of port scan attacks. The derived set of hypergraph-based metrics are then used to train an ensemble machine learning (ML) based NIDS that allows for real-time adaption in monitoring and detecting port scanning activities, other types of attacks, and adversarial intrusions at high accuracy, precision and recall performances. This ML adapting NIDS was developed through the combination of (1) intrusion examples, (2) NIDS update rules, (3) attack threshold choices to trigger NIDS retraining requests, and (4) a production environment with no prior knowledge of the nature of network traffic. 40 scenarios were auto-generated to evaluate the ML ensemble NIDS comprising three tree-based models. The resulting ML Ensemble NIDS was extended and evaluated with the CIC-IDS2017 dataset. Results show that under the model settings of an Update-ALL-NIDS rule (specifically retrain and update all the three models upon the same NIDS retraining request) the proposed ML ensemble NIDS evolved intelligently and produced the best results with nearly 100% detection performance throughout the simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.03933v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TSMC.2024.3446635</arxiv:DOI>
      <dc:creator>Zong-Zhi Lin, Thomas D. Pike, Mark M. Bailey, Nathaniel D. Bastian</dc:creator>
    </item>
    <item>
      <title>Debiasing Piecewise Deterministic Markov Process samplers using couplings</title>
      <link>https://arxiv.org/abs/2306.15422</link>
      <description>arXiv:2306.15422v2 Announce Type: replace-cross 
Abstract: Monte Carlo methods -- such as Markov chain Monte Carlo (MCMC) and piecewise deterministic Markov process (PDMP) samplers -- provide asymptotically exact estimators of expectations under a target distribution. There is growing interest in alternatives to this asymptotic regime, in particular in constructing estimators that are exact in the limit of an infinite amount of computing processors, rather than in the limit of an infinite number of Markov iterations. In particular, Jacob et al. (2020) introduced coupled MCMC estimators to remove the non-asymptotic bias, resulting in MCMC estimators that can be embarrassingly parallelised. In this work, we extend the estimators of Jacob et al. (2020) to the continuous-time context and derive couplings for the bouncy, the boomerang and the coordinate samplers. Some preliminary empirical results are included that demonstrate the reasonable scaling of our method with the dimension of the target.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15422v2</guid>
      <category>stat.CO</category>
      <category>cs.DC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos, Matthew Sutton, Nicolas Chopin</dc:creator>
    </item>
    <item>
      <title>Unbiased Kinetic Langevin Monte Carlo with Inexact Gradients</title>
      <link>https://arxiv.org/abs/2311.05025</link>
      <description>arXiv:2311.05025v3 Announce Type: replace-cross 
Abstract: We present an unbiased method for Bayesian posterior means based on kinetic Langevin dynamics that combines advanced splitting methods with enhanced gradient approximations. Our approach avoids Metropolis correction by coupling Markov chains at different discretization levels in a multilevel Monte Carlo approach. Theoretical analysis demonstrates that our proposed estimator is unbiased, attains finite variance, and satisfies a central limit theorem. It can achieve accuracy $\epsilon&gt;0$ for estimating expectations of Lipschitz functions in $d$ dimensions with $\mathcal{O}(d^{1/4}\epsilon^{-2})$ expected gradient evaluations, without assuming warm start. We exhibit similar bounds using both approximate and stochastic gradients, and our method's computational cost is shown to scale independently of the size of the dataset. The proposed method is tested using a multinomial regression problem on the MNIST dataset and a Poisson regression model for soccer scores. Experiments indicate that the number of gradient evaluations per effective sample is independent of dimension, even when using inexact gradients. For product distributions, we give dimension-independent variance bounds. Our results demonstrate that in large-scale applications, the unbiased algorithm we present can be 2-3 orders of magnitude more efficient than the ``gold-standard" randomized Hamiltonian Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05025v3</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil K. Chada, Benedict Leimkuhler, Daniel Paulin, Peter A. Whalley</dc:creator>
    </item>
    <item>
      <title>Clustering Consistency of General Nonparametric Classification Methods in Cognitive Diagnosis</title>
      <link>https://arxiv.org/abs/2312.11437</link>
      <description>arXiv:2312.11437v2 Announce Type: replace-cross 
Abstract: Cognitive diagnosis models have been popularly used in fields such as education, psychology, and social sciences. While parametric likelihood estimation is a prevailing method for fitting cognitive diagnosis models, nonparametric methodologies are attracting increasing attention due to their ease of implementation and robustness, particularly when sample sizes are relatively small. However, existing clustering consistency results of the nonparametric estimation methods often rely on certain restrictive conditions, which may not be easily satisfied in practice. In this article, the clustering consistency of the general nonparametric classification method is reestablished under weaker and more practical conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11437v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyu Cui, Yanlong Liu, Gongjun Xu</dc:creator>
    </item>
    <item>
      <title>Deep Limit Model-free Prediction in Regression</title>
      <link>https://arxiv.org/abs/2408.09532</link>
      <description>arXiv:2408.09532v2 Announce Type: replace-cross 
Abstract: In this paper, we provide a novel Model-free approach based on Deep Neural Network (DNN) to accomplish point prediction and prediction interval under a general regression setting. Usually, people rely on parametric or non-parametric models to bridge dependent and independent variables (Y and X). However, this classical method relies heavily on the correct model specification. Even for the non-parametric approach, some additive form is often assumed. A newly proposed Model-free prediction principle sheds light on a prediction procedure without any model assumption. Previous work regarding this principle has shown better performance than other standard alternatives. Recently, DNN, one of the machine learning methods, has received increasing attention due to its great performance in practice. Guided by the Model-free prediction idea, we attempt to apply a fully connected forward DNN to map X and some appropriate reference random variable Z to Y. The targeted DNN is trained by minimizing a specially designed loss function so that the randomness of Y conditional on X is outsourced to Z through the trained DNN. Our method is more stable and accurate compared to other DNN-based counterparts, especially for optimal point predictions. With a specific prediction procedure, our prediction interval can capture the estimation variability so that it can render a better coverage rate for finite sample cases. The superior performance of our method is verified by simulation and empirical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09532v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kejin Wu, Dimitris N. Politis</dc:creator>
    </item>
    <item>
      <title>Noise-free comparison of stochastic agent-based simulations using common random numbers</title>
      <link>https://arxiv.org/abs/2409.02086</link>
      <description>arXiv:2409.02086v2 Announce Type: replace-cross 
Abstract: Random numbers are at the heart of every agent-based model (ABM) of health and disease. By representing each individual in a synthetic population, agent-based models enable detailed analysis of intervention impact and parameter sensitivity. Yet agent-based modeling has a fundamental signal-to-noise problem, in which small changes between simulations cannot be reliably differentiated from stochastic noise resulting from misaligned random number realizations. We introduce a novel methodology that eliminates noise due to misaligned random numbers, a first for agent-based modeling. Our approach enables meaningful individual-level analysis between ABM scenarios because all differences are driven by mechanistic effects rather than random number noise. We demonstrate the benefits of our approach on three disparate examples. Results consistently show reductions in the number of simulations required to achieve a given standard error with levels exceeding 10-fold for some applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02086v2</guid>
      <category>q-bio.QM</category>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel J. Klein, Romesh G. Abeysuriya, Robyn M. Stuart, Cliff C. Kerr</dc:creator>
    </item>
  </channel>
</rss>
