<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 Jan 2026 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mapping Connectomic Structure to Function(s) in Cerebellar-like Networks using Kernel Regression</title>
      <link>https://arxiv.org/abs/2601.09320</link>
      <description>arXiv:2601.09320v1 Announce Type: new 
Abstract: Cerebellar-like networks, in which input activity patterns are separated by projection to a much higher-dimensional space before classification, are a recurring neurobiological motif, present in the cerebellum, dentate gyrus, insect olfactory system, and electrosensory system of the electric fish. Their relatively well-understood design presents a promising test-case for probing principles of biological learning. The circuits' expansive projections have long been modelled as random, enabling effective general purpose pattern separation. However, electron-microscopy studies have discovered interesting hints of structure in both the fly mushroom body and mouse cerebellum. Recent numerical work suggested that this non-random connectivity enables the circuit to prioritise learning of some, presumably natural, tasks over others. Here, rather than numerical results, we present a robust mathematical link between the observed connectivity patterns and the cerebellar circuit's learning ability. In particular, we extend a simplified kernel regression model of the system and use recent machine learning theory results to relate connectivity to learning. We find that the reported structure in the projection weights shapes the network's inductive bias in intuitive ways: functions are easier to learn if they depend on inputs that are oversampled, or on collections of neurons that tend to connect to the same hidden layer neurons. Our approach is analytically tractable and pleasingly simple, and we hope it continues to serve as a model for understanding the functional implications of other processing motifs in cerebellar-like networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09320v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Dorrell, Peter E. Latham</dc:creator>
    </item>
    <item>
      <title>Evolutionary tuning of TAM receptor-ligand interfaces highlights electrostatic features associated with regenerative phagocytic signaling</title>
      <link>https://arxiv.org/abs/2601.08855</link>
      <description>arXiv:2601.08855v1 Announce Type: cross 
Abstract: Efficient resolution of neuroinflammation and debris clearance is a key determinant of successful central nervous system regeneration. Regenerative vertebrates such as Danio rerio often exhibit faster immune resolution and debris clearance than mammals, yet the molecular determinants underlying these differences remain incompletely understood. TAM receptor tyrosine kinases (Tyro3, Axl, and Mertk) and their ligands Gas6 and Protein S are central regulators of phagocytosis and immune resolution in the nervous system, but whether intrinsic structural properties of these receptor-ligand complexes contribute to regenerative efficiency has not been systematically explored.
  Here, we present a comparative in silico analysis of TAM receptors and ligands from zebrafish, human, and mouse, integrating sequence evolution, high-confidence structural modeling, interface characterization, and electrostatic analysis. Despite substantial sequence divergence, ligand-binding domains display strong structural conservation, supporting a conserved global mode of TAM-ligand engagement. At the interface level, zebrafish complexes show enhanced electrostatic contributions and increased salt-bridge density, particularly in the Tyro3-Protein S interaction. Residue-level electrostatic analysis reveals clustered interface hotspots that are spatially conserved across species despite evolutionary rewiring of individual contacts.
  Together, these results suggest that TAM receptor-ligand interfaces are evolutionarily tuned through subtle electrostatic and geometric optimization rather than large-scale structural changes, providing a conserved yet adaptable framework for species-specific modulation of phagocytic signaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08855v1</guid>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enso O. Torres Alegre</dc:creator>
    </item>
    <item>
      <title>A Hypothesis-First Framework for Mechanistic Modeling in Neuroimaging</title>
      <link>https://arxiv.org/abs/2509.16070</link>
      <description>arXiv:2509.16070v2 Announce Type: replace 
Abstract: Turning rich neuroimaging data into mechanistic insight remains challenging. Statistical models capture associations but remain largely agnostic to underlying mechanisms. Biophysical models embody candidate mechanisms but remain difficult to deploy without specialized expertise. Here, we present a hypothesis-first framework recasting model specifications as testable mechanistic hypotheses and streamlines the procedure for rejecting inappropriate hypotheses before moving to typical analyses. The key innovation is an expectation of model behavior under feature generalization constraints: we compute the model's expected $Y$ output across the parameter space based on the likelihood for a broader/distinct feature $Z$. Mirror statistical models are derived from these expected outputs and compared to the empirical ones with standard statistics. In synthetic experiments, our framework rejected mis-specified hypotheses and penalized unnecessary degrees of freedom while retaining valid hypotheses. These results demonstrate a practical hypothesis-driven approach for using mechanistic models in neuroimaging without requiring advanced training, complementing traditional analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16070v2</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominic Boutet, Sylvain Baillet</dc:creator>
    </item>
    <item>
      <title>Direction and speed selectivity properties for spatio-temporal receptive fields according to the generalized Gaussian derivative model for visual receptive fields</title>
      <link>https://arxiv.org/abs/2511.08101</link>
      <description>arXiv:2511.08101v2 Announce Type: replace 
Abstract: This paper gives an in-depth theoretical analysis of the direction and speed selectivity properties of idealized models of the spatio-temporal receptive fields of simple cells and complex cells, based on the generalized Gaussian derivative model for visual receptive fields. According to this theory, the receptive fields are modelled as velocity-adapted affine Gaussian derivatives for different image velocities and different degrees of elongation. By probing such idealized receptive field models of visual neurons to moving sine waves with different angular frequencies and image velocities, we characterize the computational models to a structurally similar probing method as is used for characterizing the direction and speed selective properties of biological neurons.
  By comparison to results of neurophysiological measurements of direction and speed selectivity for biological neurons in the primary visual cortex, we find that our theoretical results are consistent with (i) velocity-tuned visual neurons that are sensitive to particular motion directions and speeds, and (ii) different visual neurons having broader vs. sharper direction and speed selective properties. Our theoretical results in combination with results from neurophysiological characterizations of motion-sensitive visual neurons are also consistent with a previously formulated hypothesis that the simple cells in the primary visual cortex ought to be covariant under local Galilean transformations, so as to enable processing of visual stimuli with different motion directions and speeds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08101v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tony Lindeberg</dc:creator>
    </item>
    <item>
      <title>Training Large Neural Networks With Low-Dimensional Error Feedback</title>
      <link>https://arxiv.org/abs/2502.20580</link>
      <description>arXiv:2502.20580v4 Announce Type: replace-cross 
Abstract: Training deep neural networks typically relies on backpropagating high dimensional error signals a computationally intensive process with little evidence supporting its implementation in the brain. However, since most tasks involve low-dimensional outputs, we propose that low-dimensional error signals may suffice for effective learning. To test this hypothesis, we introduce a novel local learning rule based on Feedback Alignment that leverages indirect, low-dimensional error feedback to train large networks. Our method decouples the backward pass from the forward pass, enabling precise control over error signal dimensionality while maintaining high-dimensional representations. We begin with a detailed theoretical derivation for linear networks, which forms the foundation of our learning framework, and extend our approach to nonlinear, convolutional, and transformer architectures. Remarkably, we demonstrate that even minimal error dimensionality on the order of the task dimensionality can achieve performance matching that of traditional backpropagation. Furthermore, our rule enables efficient training of convolutional networks, which have previously been resistant to Feedback Alignment methods, with minimal error. This breakthrough not only paves the way toward more biologically accurate models of learning but also challenges the conventional reliance on high-dimensional gradient signals in neural network training. Our findings suggest that low-dimensional error signals can be as effective as high-dimensional ones, prompting a reevaluation of gradient-based learning in high-dimensional systems. Ultimately, our work offers a fresh perspective on neural network optimization and contributes to understanding learning mechanisms in both artificial and biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20580v4</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maher Hanut, Jonathan Kadmon</dc:creator>
    </item>
  </channel>
</rss>
