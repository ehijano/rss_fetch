<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Untangling Sample and Population Level Estimands in Bayesian Causal Inference</title>
      <link>https://arxiv.org/abs/2508.15016</link>
      <description>arXiv:2508.15016v1 Announce Type: new 
Abstract: Bayesian inference for causal estimands has been growing in popularity, however many misconceptions and implementation errors arise from conflating sample and population-level estimands. We have anecdotally witnessed these at conference talks, in the course of peer review service, and even in published and arXiv-ed papers. Our goal here is to elucidate the crucial differences between sample and population-level quantities when it comes to identification, modeling, Bayesian computation, and interpretation. For example, common sample-level estimands require cross-world assumptions for identification, whereas common population-level estimands do not. Similarly, the former requires explicit imputation of counterfactuals from their joint posterior, whereas the latter typically only requires a posterior distribution over parameters. We start by defining some examples of both types of estimands, then discuss the full joint posterior over all unknowns (both missing counterfactuals and population distribution parameters). We continue to outline how inference for different estimands are derived from different marginals of this joint posterior. Because the differences are conceptually subtle but can be practically substantial, we provide an illustration of using synthetic data in Stan. We also provide a detailed appendix with derivations and computational tips along with a discussion of common implementation errors. The overarching message here is to always engage in first-principles thinking about which marginal of the joint posterior is of interest in a particular application, then follow the strict logic of Bayes' theorem and probability to avoid common implementation errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15016v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arman Oganisian</dc:creator>
    </item>
    <item>
      <title>Generalized random forest for extreme quantile regression</title>
      <link>https://arxiv.org/abs/2508.15095</link>
      <description>arXiv:2508.15095v1 Announce Type: new 
Abstract: Quantile regression is a statistical method which, unlike classical regression, aims to predict the conditional quantiles. Classical quantile regression methods face difficulties, particularly when the quantile under consideration is extreme, due to the limited number of data available in the tail of the distribution, or when the quantile function is complex. We propose an extreme quantile regression method based on extreme value theory and statistical learning to overcome these difficulties. Following the Block Maxima approach of extreme value theory, we approximate the conditional distribution of block maxima by the generalized extreme value distribution, with covariate-dependent parameters. These parameters are estimated using a method based on generalized random forests. Applications on simulated data show that our proposed method effectively addresses the mentioned quantile regression issues and highlights its performance compared to other quantile regression approaches based on statistical learning methods. We apply our methodology to daily meteorological data from the Fort Collins station in Colorado (USA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15095v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/03610918.2025.2543854</arxiv:DOI>
      <arxiv:journal_reference>Communications in Statistics - Simulation and Computation, 1-24 (2025)</arxiv:journal_reference>
      <dc:creator>Lucien M. Vidagbandji, Alexandre Berred, Cyrille Bertelle, Laurent Amanton</dc:creator>
    </item>
    <item>
      <title>Simulating from marginal structural models for hazards, cause-specific hazards and subdistribution hazards using general copulas</title>
      <link>https://arxiv.org/abs/2508.15145</link>
      <description>arXiv:2508.15145v1 Announce Type: new 
Abstract: Seaman and Keogh (Biometrical Journal 2024) proposed a method for simulating data compatible with a marginal structural model (MSM) for the hazard of a survival time outcome. In this short report, I propose two extensions of this method. First, Seaman and Keogh favoured the use of a Gaussian copula, because this enables the function of the confounder history through which the hazard of failure depends on confounders to be interpreted as a risk score. Here, I describe how this interpretation can be preserved even when a non-Gaussian copula is used. Second, I extend Seaman and Keogh's method to allow simulation of data compatible with a MSM for a cause-specific or subdistribution hazard of failure in the presence of a competing event.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15145v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaun R Seaman</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Inference with General Missingness Patterns and Machine Learning Imputation</title>
      <link>https://arxiv.org/abs/2508.15162</link>
      <description>arXiv:2508.15162v1 Announce Type: new 
Abstract: Pre-trained machine learning (ML) predictions have been increasingly used to complement incomplete data to enable downstream scientific inquiries, but their naive integration risks biased inferences. Recently, multiple methods have been developed to provide valid inference with ML imputations regardless of prediction quality and to enhance efficiency relative to complete-case analyses. However, existing approaches are often limited to missing outcomes under a missing-completely-at-random (MCAR) assumption, failing to handle general missingness patterns under the more realistic missing-at-random (MAR) assumption. This paper develops a novel method which delivers valid statistical inference framework for general Z-estimation problems using ML imputations under the MAR assumption and for general missingness patterns. The core technical idea is to stratify observations by distinct missingness patterns and construct an estimator by appropriately weighting and aggregating pattern-specific information through a masking-and-imputation procedure on the complete cases. We provide theoretical guarantees of asymptotic normality of the proposed estimator and efficiency dominance over weighted complete-case analyses. Practically, the method affords simple implementations by leveraging existing weighted complete-case analysis software. Extensive simulations are carried out to validate theoretical results. The paper concludes with a brief discussion on practical implications, limitations, and potential future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15162v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingran Chen, Tyler McCormick, Bhramar Mukherjee, Zhenke Wu</dc:creator>
    </item>
    <item>
      <title>An adaptive procedure for detecting replicated signals with $k$-family-wise error rate control</title>
      <link>https://arxiv.org/abs/2508.15363</link>
      <description>arXiv:2508.15363v1 Announce Type: new 
Abstract: Partial conjunction (PC) hypothesis testing is widely used to assess the replicability of scientific findings across multiple comparable studies. In high-throughput meta-analyses, testing a large number of PC hypotheses with k-family-wise error rate (k-FWER) control often suffers from low statistical power due to the multiplicity burden. The state-of-the-art AdaFilter-Bon procedure by Wang et al. (2022, Ann. Stat., 50(4), 1890-1909) alleviates this problem by filtering out hypotheses unlikely to be false before applying a rejection rule. However, a side effect of filtering is that it renders the rejection rule more stringent than necessary, leading to conservative k-FWER control. In this paper, we mitigate this conservativeness - and thereby improve the power of AdaFilter-Bon - by incorporating a post-filter null proportion estimate into the procedure. The resulting method, AdaFilter-AdaBon, has proven asymptotic k-FWER control under weak dependence and demonstrates empirical finite-sample control with higher power than the original AdaFilter-Bon in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15363v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ninh Tran</dc:creator>
    </item>
    <item>
      <title>Swap Regression Methodology for Predicting Relationship with Historical Bivariate Data</title>
      <link>https://arxiv.org/abs/2508.15479</link>
      <description>arXiv:2508.15479v1 Announce Type: new 
Abstract: This study revisits regression for samples with alternating predictors (SWAP) proposed in Chow et al.[2015] with the purpose of finding the best fit model when the role of the response and the explanatory variables was established. In the current work, we explore the directional relationship between the two variables at a given point of time, by a novel approach which draws direct inspiration from the concept of SWAP regression. Our method, based on the Gaussian Mixture Model (GMM) and the beta distribution, while estimating the probability of a latent variable, predicts the suitable model, i.e., earmarks if a variable can take the role of an explanatory or response, at any point of time. To make this switch-over role between variables, a valid consideration, we have established the existence of a bi-directional (Granger) causality between the two variables. A detailed real data analysis of the methodology is carried out using the historical quarterly data on probably the two most intertwined macroeconomic indicators explaining the health of an economy, viz., the Gross Domestic Product (GDP) and Public Debt, thereby making the application, in real data, more challenging. In particular, we use data of the US economy during the sample period 1966-2023.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15479v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Viral Chitlangia, Mosuk Chow, Sharmishtha Mitra</dc:creator>
    </item>
    <item>
      <title>High-dimensional Asymptotics of Generalization Performance in Continual Ridge Regression</title>
      <link>https://arxiv.org/abs/2508.15494</link>
      <description>arXiv:2508.15494v1 Announce Type: new 
Abstract: Continual learning is motivated by the need to adapt to real-world dynamics in tasks and data distribution while mitigating catastrophic forgetting. Despite significant advances in continual learning techniques, the theoretical understanding of their generalization performance lags behind. This paper examines the theoretical properties of continual ridge regression in high-dimensional linear models, where the dimension is proportional to the sample size in each task. Using random matrix theory, we derive exact expressions of the asymptotic prediction risk, thereby enabling the characterization of three evaluation metrics of generalization performance in continual learning: average risk, backward transfer, and forward transfer. Furthermore, we present the theoretical risk curves to illustrate the trends in these evaluation metrics throughout the continual learning process. Our analysis reveals several intriguing phenomena in the risk curves, demonstrating how model specifications influence the generalization performance. Simulation studies are conducted to validate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15494v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihan Zhao, Wenqing Su, Ying Yang</dc:creator>
    </item>
    <item>
      <title>Subgroup comparisons within and across studies in meta-analysis</title>
      <link>https://arxiv.org/abs/2508.15531</link>
      <description>arXiv:2508.15531v1 Announce Type: new 
Abstract: Subgroup-specific meta-analysis synthesizes treatment effects for patient subgroups across randomized trials. Methods include joint or separate modeling of subgroup effects and treatment-by-subgroup interactions, but inconsistencies arise when subgroup prevalence differs between studies (e.g., proportion of non-smokers). A key distinction is between study-generated evidence within trials and synthesis-generated evidence obtained by contrasting results across trials. This distinction matters for identifying which subgroups benefit or are harmed most. Failing to separate these evidence types can bias estimates and obscure true subgroup-specific effects, leading to misleading conclusions about relative efficacy. Standard approaches often suffer from such inconsistencies, motivating alternatives. We investigate standard and novel estimators of subgroup and interaction effects in random-effects meta-analysis and study their properties. We show that using the same weights across different analyses (SWADA) resolves inconsistencies from unbalanced subgroup distributions and improves subgroup and interaction estimates. Analytical and simulation studies demonstrate that SWADA reduces bias and improves coverage, especially under pronounced imbalance. To illustrate, we revisit recent meta-analyses of randomized trials of COVID-19 therapies. Beyond COVID-19, the findings outline a general strategy for correcting compositional bias in evidence synthesis, with implications for decision-making and statistical modeling. We recommend the Interaction RE-weights SWADA as a practical default when aggregation bias is plausible: it ensures collapsibility, maintains nominal coverage with modest width penalty, and yields BLUE properties for the interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15531v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renato Panaro, Christian R\"over, Tim Friede</dc:creator>
    </item>
    <item>
      <title>On Prior Distributions for Orthogonal Function Sequences</title>
      <link>https://arxiv.org/abs/2508.15552</link>
      <description>arXiv:2508.15552v1 Announce Type: new 
Abstract: We propose a novel class of prior distributions for sequences of orthogonal functions, which are frequently required in various statistical models such as functional principal component analysis (FPCA). Our approach constructs priors sequentially by imposing adaptive orthogonality constraints through a hierarchical formulation of conditionally normal distributions. The orthogonality is controlled via hyperparameters, allowing for flexible trade-offs between exactness and smoothness, which can be learned from the observed data. We illustrate the properties of the proposed prior and show that it leads to nearly orthogonal posterior estimates. The proposed prior is employed in Bayesian FPCA, providing more interpretable principal functions and efficient low-rank representations. Through simulation studies and analysis of human mobility data in Tokyo, we demonstrate the superior performance of our approach in inducing orthogonality and improving functional component estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15552v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shonosuke Sugasawa, Daichi Mochihashi</dc:creator>
    </item>
    <item>
      <title>Clustering-based aggregate value regression</title>
      <link>https://arxiv.org/abs/2508.15567</link>
      <description>arXiv:2508.15567v1 Announce Type: new 
Abstract: In various practical situations, forecasting of aggregate values rather than individual ones is often our main focus. For instance, electricity companies are interested in forecasting the total electricity demand in a specific region to ensure reliable grid operation and resource allocation. However, to our knowledge, statistical learning specifically for forecasting aggregate values has not yet been well-established. In particular, the relationship between forecast error and the number of clusters has not been well studied, as clustering is usually treated as unsupervised learning. This study introduces a novel forecasting method specifically focused on the aggregate values in the linear regression model. We call it the Aggregate Value Regression (AVR), and it is constructed by combining all regression models into a single model. With the AVR, we must estimate a huge number of parameters when the number of regression models to be combined is large, resulting in overparameterization. To address the overparameterization issue, we introduce a hierarchical clustering technique, referred to as AVR-C (C stands for clustering). In this approach, several clusters of regression models are constructed, and the AVR is performed within each cluster. The AVR-C introduces a novel bias-variance trade-off theory under the assumption of a misspecified model. In this framework, the number of clusters characterizes model complexity. Monte Carlo simulation is conducted to investigate the behavior of training and test errors of our proposed clustering technique. The bias-variance trade-off theory is also demonstrated through the analysis of electricity demand forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15567v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kei Hirose, Hidetoshi Matsui, Hiroki Masuda</dc:creator>
    </item>
    <item>
      <title>Sequential Confirmatory Factor Analysis: A Novel Approach to Latent Variable Measurement</title>
      <link>https://arxiv.org/abs/2508.15611</link>
      <description>arXiv:2508.15611v1 Announce Type: new 
Abstract: Factor score estimation in small sample sizes often encounters parameter bias and convergence failures when constructing hierarchical national/sub-national indices. This paper proposes a novel method for hierarchical factor analysis called "sequential Confirmatory Factor Analysis". Instead of estimating multiple levels of factors at the same time, this approach calculates factor scores sequentially from the lowest to highest levels. This sequential estimation keeps the original sample size in each step and also removes cross-level covariance estimation. Using a series of Monte Carlo simulations, we isolate the difference between sequential Confirmatory Factor Analysis and traditional Confirmatory Factor Analysis by comparing their resulting factor scores to the true latent variables under varying conditions. We also estimate the WJP Rule of Law Index using traditional Confirmatory Factor Analysis, Bayesian Confirmatory Factor Analysis, and sequential Confirmatory Factor Analysis to test performance. Our findings demonstrate that sequential Confirmatory Factor Analysis significantly outperforms the traditional model for indices with simple/moderate complexity. Traditional Confirmatory Factor Analysis performs better where the data are skewed. Where the hierarchical model becomes complex, the two methods perform similarly. Finally, sequential Confirmatory Factor Analysis can provide valid estimates where traditional or Bayesian Confirmatory Factor Analysis fail to converge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15611v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary Esses Johnson</dc:creator>
    </item>
    <item>
      <title>Conditional cross-fitting for unbiased machine-learning-assisted covariate adjustment in randomized experiments</title>
      <link>https://arxiv.org/abs/2508.15664</link>
      <description>arXiv:2508.15664v1 Announce Type: new 
Abstract: Randomized experiments are the gold standard for estimating the average treatment effect (ATE). While covariate adjustment can reduce the asymptotic variances of the unbiased Horvitz-Thompson estimators for the ATE, it suffers from finite-sample biases due to data reuse in both prediction and estimation. Traditional sample-splitting and cross-fitting methods can address the problem of data reuse and obtain unbiased estimators. However, they require that the data are independently and identically distributed, which is usually violated under the design-based inference framework for randomized experiments. To address this challenge, we propose a novel conditional cross-fitting method, under the design-based inference framework, where potential outcomes and covariates are fixed and the randomization is the sole source of randomness. We propose sample-splitting algorithms for various randomized experiments, including Bernoulli randomized experiments, completely randomized experiments, and stratified randomized experiments. Based on the proposed algorithms, we construct unbiased covariate-adjusted ATE estimators and propose valid inference procedures. Our methods can accommodate flexible machine-learning-assisted covariate adjustments and allow for model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15664v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xin Lu, Lei Shi, Hanzhong Liu, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Fast approximate Bayesian inference of HIV indicators using PCA adaptive Gauss-Hermite quadrature</title>
      <link>https://arxiv.org/abs/2508.15665</link>
      <description>arXiv:2508.15665v1 Announce Type: new 
Abstract: Naomi is a spatial evidence synthesis model used to produce district-level HIV epidemic indicators in sub-Saharan Africa. Multiple outcomes of policy interest, including HIV prevalence, HIV incidence, and antiretroviral therapy treatment coverage are jointly modelled using both household survey data and routinely reported health system data. The model is provided as a tool for countries to input their data to and generate estimates with during a yearly process supported by UNAIDS. Previously, inference has been conducted using empirical Bayes and a Gaussian approximation, implemented via the TMB R package. We propose a new inference method based on an extension of adaptive Gauss-Hermite quadrature to deal with more than 20 hyperparameters. Using data from Malawi, our method improves the accuracy of inferences for model parameters, while being substantially faster to run than Hamiltonian Monte Carlo with the No-U-Turn sampler. Our implementation leverages the existing TMB C++ template for the model's log-posterior, and is compatible with any model with such a template.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15665v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Howes, Alex Stringer, Seth R. Flaxman, Jeffrey W. Imai-Eaton</dc:creator>
    </item>
    <item>
      <title>Large-dimensional Factor Analysis with Weighted PCA</title>
      <link>https://arxiv.org/abs/2508.15675</link>
      <description>arXiv:2508.15675v1 Announce Type: new 
Abstract: Principal component analysis (PCA) is arguably the most widely used approach for large-dimensional factor analysis. While it is effective when the factors are sufficiently strong, it can be inconsistent when the factors are weak and/or the noise has complex dependence structure. We argue that the inconsistency often stems from bias and introduce a general approach to restore consistency. Specifically, we propose a general weighting scheme for PCA and show that with a suitable choice of weighting matrices, it is possible to deduce consistent and asymptotic normal estimators under much weaker conditions than the usual PCA. While the optimal weight matrix may require knowledge about the factors and covariance of the idiosyncratic noise that are not known a priori, we develop an agnostic approach to adaptively choose from a large class of weighting matrices that can be viewed as PCA for weighted linear combinations of auto-covariances among the observations. Theoretical and numerical results demonstrate the merits of our methodology over the usual PCA and other recently developed techniques for large-dimensional approximate factor models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15675v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongyuan Lyu, Ming Yuan</dc:creator>
    </item>
    <item>
      <title>Effect Identification and Unit Categorization in the Multi-Score Regression Discontinuity Design with Application to LED Manufacturing</title>
      <link>https://arxiv.org/abs/2508.15692</link>
      <description>arXiv:2508.15692v1 Announce Type: new 
Abstract: The RDD (regression discontinuity design) is a widely used framework for identification and estimation of causal effects at a cutoff of a single running variable. Practical settings, in particular those encountered in production systems, often involve decision-making defined by multiple thresholds and criteria. Common MRD (multi-score RDD) approaches transform these to a one-dimensional design, to employ identification and estimation results. However, this practice can introduce non-compliant behavior. We develop theoretical tools to identify and reduce some of this "fuzziness" when estimating the cutoff-effect on compliers of sub-rules. We provide a sound definition and categorization of unit behavior types for multi-dimensional cutoff-rules, extending existing categorizations. We identify conditions for the existence and identification of the cutoff-effect on complier in multiple dimensions, and specify when identification remains stable after excluding nevertaker and alwaystaker. Further, we investigate how decomposing cutoff-rules into simpler parts alters the unit behavior. This allows identification and removal of non-compliant units potentially improving estimates. We validate our framework on simulated and real-world data from opto-electronic semiconductor manufacturing. Our empirical results demonstrate the usability for refining production policies. Particularly we show that our approach decreases the estimation variance, highlighting the practical value of the MRD framework in manufacturing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15692v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Alexander Schwarz, Oliver Schacht, Sven Klaassen, Johannes Oberpriller, Martin Spindler</dc:creator>
    </item>
    <item>
      <title>A U-Statistic-based random forest approach for genetic interaction study</title>
      <link>https://arxiv.org/abs/2508.14924</link>
      <description>arXiv:2508.14924v1 Announce Type: cross 
Abstract: Variations in complex traits are influenced by multiple genetic variants, environmental risk factors, and their interactions. Though substantial progress has been made in identifying single genetic variants associated with complex traits, detecting the gene-gene and gene-environment interactions remains a great challenge. When a large number of genetic variants and environmental risk factors are involved, searching for interactions is limited to pair-wise interactions due to the exponentially increased feature space and computational intensity. Alternatively, recursive partitioning approaches, such as random forests, have gained popularity in high-dimensional genetic association studies. In this article, we propose a U-Statistic-based random forest approach, referred to as Forest U-Test, for genetic association studies with quantitative traits. Through simulation studies, we showed that the Forest U-Test outperformed existing methods. The proposed method was also applied to study Cannabis Dependence CD, using three independent datasets from the Study of Addiction: Genetics and Environment. A significant joint association was detected with an empirical p-value less than 0.001. The finding was also replicated in two independent datasets with p-values of 5.93e-19 and 4.70e-17, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14924v1</guid>
      <category>q-bio.GN</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2741/e576</arxiv:DOI>
      <dc:creator>Ming Li, Ruo-Sin Peng, Changshuai Wei, Qing Lu</dc:creator>
    </item>
    <item>
      <title>Sampling by averaging: A multiscale approach to score estimation</title>
      <link>https://arxiv.org/abs/2508.15069</link>
      <description>arXiv:2508.15069v1 Announce Type: cross 
Abstract: We introduce a novel framework for efficient sampling from complex, unnormalised target distributions by exploiting multiscale dynamics. Traditional score-based sampling methods either rely on learned approximations of the score function or involve computationally expensive nested Markov chain Monte Carlo (MCMC) loops. In contrast, the proposed approach leverages stochastic averaging within a slow-fast system of stochastic differential equations (SDEs) to estimate intermediate scores along a diffusion path without training or inner-loop MCMC. Two algorithms are developed under this framework: MultALMC, which uses multiscale annealed Langevin dynamics, and MultCDiff, based on multiscale controlled diffusions for the reverse-time Ornstein-Uhlenbeck process. Both overdamped and underdamped variants are considered, with theoretical guarantees of convergence to the desired diffusion path. The framework is extended to handle heavy-tailed target distributions using Student's t-based noise models and tailored fast-process dynamics. Empirical results across synthetic and real-world benchmarks, including multimodal and high-dimensional distributions, demonstrate that the proposed methods are competitive with existing samplers in terms of accuracy and efficiency, without the need for learned models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15069v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Paula Cordero-Encinar, Andrew B. Duncan, Sebastian Reich, O. Deniz Akyildiz</dc:creator>
    </item>
    <item>
      <title>K-Means Panel Data Clustering in the Presence of Small Groups</title>
      <link>https://arxiv.org/abs/2508.15408</link>
      <description>arXiv:2508.15408v1 Announce Type: cross 
Abstract: We consider panel data models with group structure. We study the asymptotic behavior of least-squares estimators and information criterion for the number of groups, allowing for the presence of small groups that have an asymptotically negligible relative size. Our contributions are threefold. First, we derive sufficient conditions under which the least-squares estimators are consistent and asymptotically normal. One of the conditions implies that a longer sample period is required as there are smaller groups. Second, we show that information criteria for the number of groups proposed in earlier works can be inconsistent or perform poorly in the presence of small groups. Third, we propose modified information criteria (MIC) designed to perform well in the presence of small groups. A Monte Carlo simulation confirms their good performance in finite samples. An empirical application illustrates that K-means clustering paired with the proposed MIC allows one to discover small groups without producing too many groups. This enables characterizing small groups and differentiating them from the other large groups in a parsimonious group structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15408v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikihito Nishi</dc:creator>
    </item>
    <item>
      <title>Multiply Robust Conformal Risk Control with Coarsened Data</title>
      <link>https://arxiv.org/abs/2508.15489</link>
      <description>arXiv:2508.15489v1 Announce Type: cross 
Abstract: Conformal Prediction (CP) has recently received a tremendous amount of interest, leading to a wide range of new theoretical and methodological results for predictive inference with formal theoretical guarantees. However, the vast majority of CP methods assume that all units in the training data have fully observed data on both the outcome and covariates of primary interest, an assumption that rarely holds in practice. In reality, training data are often missing the outcome, a subset of covariates, or both on some units. In addition, time-to-event outcomes in the training set may be censored due to dropout or administrative end-of-follow-up. Accurately accounting for such coarsened data in the training sample while fulfilling the primary objective of well-calibrated conformal predictive inference, requires robustness and efficiency considerations. In this paper, we consider the general problem of obtaining distribution-free valid prediction regions for an outcome given coarsened training data. Leveraging modern semiparametric theory, we achieve our goal by deriving the efficient influence function of the quantile of the outcome we aim to predict, under a given semiparametric model for the coarsened data, carefully combined with a novel conformal risk control procedure. Our principled use of semiparametric theory has the key advantage of facilitating flexible machine learning methods such as random forests to learn the underlying nuisance functions of the semiparametric model. A straightforward application of the proposed general framework produces prediction intervals with stronger coverage properties under covariate shift, as well as the construction of multiply robust prediction sets in monotone missingness scenarios. We further illustrate the performance of our methods through various simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15489v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manit Paul, Arun Kumar Kuchibhotla, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Flexible yet Sparse Bayesian Survival Models with Time-Varying Coefficients and Unobserved Heterogeneity</title>
      <link>https://arxiv.org/abs/2206.11320</link>
      <description>arXiv:2206.11320v3 Announce Type: replace 
Abstract: Survival analysis is an important area of medical research, yet existing models often struggle to balance simplicity with flexibility. Simple models require minimal adjustments but come with strong assumptions, while more flexible models require significant input and tuning from researchers. We present a survival model using a Bayesian hierarchical shrinkage method that automatically determines whether each covariate should be treated as static, time-varying, or excluded altogether. This approach strikes a balance between simplicity and flexibility, minimizes the need for tuning, and naturally quantifies uncertainty. The method is supported by an efficient Markov chain Monte Carlo sampler, implemented in the R package shrinkDSM. Comprehensive simulation studies and an application to a clinical dataset involving patients with adenocarcinoma of the gastroesophageal junction showcase the advantages of our approach compared to existing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.11320v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Knaus, Daniel Winkler, Sebastian F. Schoppmann, Gerd Jomrich</dc:creator>
    </item>
    <item>
      <title>A General Framework for Multiple Testing via E-value Aggregation and Data-Dependent Weighting</title>
      <link>https://arxiv.org/abs/2312.02905</link>
      <description>arXiv:2312.02905v2 Announce Type: replace 
Abstract: Motivated by recent findings in Li and Zhang (2025), which established an equivalence between certain p-value-based multiple testing procedures and the e-Benjamini-Hochberg procedure (Wang and Ramdas, 2022), we introduce a general framework for constructing novel multiple testing methods through the aggregation and combination of e-values. Specifically, we propose methodologies for three distinct scenarios: (i) assembly of e-values obtained from different subsets of data, simultaneously controlling group-wise and overall false discovery rates; (ii) aggregation of e-values derived from different procedures or the same procedure employing different test statistics; and (iii) adaptive multiple testing methods that incorporate external structural information to enhance statistical power. A notable feature of our approach is the use of data-dependent weighting of e-values, significantly improving the efficiency of the resulting e-Benjamini-Hochberg procedures. The construction of these weights is non-trivial and inspired by leave-one-out analysis, a widely utilized technique for proving false discovery rate control in p-value-based methodologies. We theoretically establish that the proposed e-Benjamini-Hochberg procedures, when equipped with data-dependent weights, guarantee finite-sample false discovery rate control across all three considered applications. Additionally, numerical studies illustrate the efficacy and advantages of the proposed methods within each application scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02905v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanxun Li, Xianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Penalized GEE for Complex Carry-Over in Repeated-Measures Crossover Designs</title>
      <link>https://arxiv.org/abs/2402.16362</link>
      <description>arXiv:2402.16362v3 Announce Type: replace 
Abstract: It has been argued for many years that models used to analyze data from crossover designs are not appropriate when simple carryover effects are assumed. Furthermore, a statistical model that could estimate complex carry-over effects in crossover designs had never been found. However, in this paper, the estimability conditions of the complex carryover effects and a theoretical result that supports them are found. In addition, a simulation example is developed in a non-linear dose-response test for a typical AB/BA crossover design with repeated measures. This simulation shows that a semiparametric model can detect complex carryover effects and that this estimation improves the precision of the estimators of the treatment effect. It is concluded that when there are at least five replicates in each observation period per individual, semiparametric statistical models provide a good estimator of the treatment effect and reduce bias with respect to models that assume the absence of carryover effects or simplex carryover effects. Furthermore, an application of the methodology is shown and the wealth of analysis gained by estimating complex carryover effects is evident.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16362v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>N. A. Cruz, K. Mylona, O. O. Melo</dc:creator>
    </item>
    <item>
      <title>Copas-Heckman-type sensitivity analysis for publication bias in rare-event meta-analysis under generalized linear mixed models</title>
      <link>https://arxiv.org/abs/2405.03603</link>
      <description>arXiv:2405.03603v2 Announce Type: replace 
Abstract: In systematic reviews and meta-analyses, publication bias (PB) is one of the serious concerns and mainly induced by selective publication of academic literatures. Although many methods have been proposed to deal with PB, almost all the methods are based on the normal-normal (NN) random-effects model assuming that data are normally distributed in both the within-study and the between-study levels. For rare-event meta-analysis where data contain rare occurrences of events, the standard NN random-effects model may perform poorly. Instead, some generalized linear mixed models (GLMMs) which employ the exact distribution for the number of events in within-study level provide alternatives and have been widely used in practice. However, limited methods can be applied to deal with PB in the GLMMs. To address this limitation, we propose a framework of sensitivity analysis for evaluating the impact of PB in various GLMMs. The proposed framework is developed based on the famous Copas-Heckman-type sensitivity analysis methods and can be easily implemented with the standard software with small computational cost. In this paper, we conduct simulation studies to assess the performance of proposed methods in adjusting PB and compare the results with related existing methods. Several real-world examples are also analyzed to show the broad applicability of our proposal in evaluating the potential impact of PB in meta-analysis of odds ratios and proportions with rare-event outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03603v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Zhou, Taojun Hu, Yuji Sakamoto, Ao Huang, Xiao-Hua Zhou, Satoshi Hattori</dc:creator>
    </item>
    <item>
      <title>Missing data imputation using a truncated Gaussian infinite factor model with application to metabolomics data</title>
      <link>https://arxiv.org/abs/2410.10633</link>
      <description>arXiv:2410.10633v2 Announce Type: replace 
Abstract: Metabolomics is the study of small molecules in biological samples. Metabolomics data are typically high-dimensional and contain highly correlated variables and frequent missing values. Both missing at random (MAR) data, due to acquisition or processing errors, and missing not at random (MNAR) data, caused by values falling below detection thresholds, are common. Thus, imputation is a critical component of downstream analysis. Existing imputation methods generally assume one type of data missingness mechanism, or impute values outside the data's physical constraints. A novel truncated Gaussian infinite factor analysis (TGIFA) model is proposed to perform statistically principled and physically realistic imputation in metabolomics data. By incorporating truncated Gaussian assumptions, TGIFA respects the data's physical constraints, while leveraging an infinite latent factor framework to capture high-dimensional dependencies without pre-specifying the number of latent factors. Our Bayesian inference approach enables uncertainty quantification in both the values of the imputed data, and the missing data mechanism. A computationally efficient exchange algorithm enables scalable posterior inference via Markov Chain Monte Carlo. We validate TGIFA through a comprehensive simulation study and demonstrate its utility in a motivating urinary metabolomics dataset, where it yields useful imputations, with associated uncertainty quantification. Open-source R code, available at https://github.com/kfinucane/TGIFA, accompanies TGIFA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10633v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kate Finucane, Lorraine Brennan, Roberta De Vito, Massimiliano Russo, Isobel Claire Gormley</dc:creator>
    </item>
    <item>
      <title>Central limit theorems for interacting innovation processes, related statistical tools and general results</title>
      <link>https://arxiv.org/abs/2501.09648</link>
      <description>arXiv:2501.09648v3 Announce Type: replace 
Abstract: We study a networked system of innovation processes, where each process is modeled as an urn with infinitely many colors-a classical framework for capturing the emergence of novelties. Extending this paradigm, we analyze a model of interacting urns, where the probability of generating or reusing elements in one process is influenced by the histories of others. This interaction is governed by two matrices that control innovation triggering and reinforcement dynamics across the system. The core contribution of this work is a detailed analysis of the second-order asymptotic behavior of the model. Building on these theoretical results, we develop statistical tools to infer the structure and strength of inter-process influence. The methodology is framed in a general setting, making it broadly applicable. We validate our approach with applications to two real-world datasets from Reddit discussions and Gutenberg text corpora.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09648v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giacomo Aletti, Irene Crimaldi, Andrea Ghiglietti</dc:creator>
    </item>
    <item>
      <title>Testing Prioritized Composite Endpoint with Multiple Follow-up Time Examinations</title>
      <link>https://arxiv.org/abs/2502.20180</link>
      <description>arXiv:2502.20180v2 Announce Type: replace 
Abstract: Composite endpoints are widely used in cardiovascular clinical trials. In recent years, hierarchical composite endpoints-particularly the win ratio approach and its predecessor, the Finkelstein-Schoenfeld (FS) test, also known as the unmatched win ratio test-have gained popularity. These methods involve comparing individuals across multiple endpoints, ranked by priority, with mortality typically assigned the highest priority in many applications. However, these methods have not accounted for varying treatment effects, known as non-constant hazards over time in the context of survival analysis. To address this limitation, we propose an adaptation of the FS test that incorporates progressive follow-up time, which we will refer to as ProFS. This proposed test can jointly evaluate treatment effects at various follow-up time points by incorporating the maximum of several FS test statistics calculated at those specific times. Moreover, ProFS also supports clinical trials with group sequential monitoring strategies, providing flexibility in trial design. As demonstrated through extensive simulations, ProFS offers increased statistical power in scenarios where the treatment effect is mainly in the short term or when the second (non-fatal) layer might be concealed by a lack of effect or weak effect on the top (fatal) layer. We also apply ProFS to the SPRINT clinical trial, illustrating how our proposed method improves the performance of FS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20180v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhan Mou, Haitao Pan, Yu Jiang, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>The role of post intercurrent event data in the estimation of hypothetical estimands in clinical trials</title>
      <link>https://arxiv.org/abs/2504.00929</link>
      <description>arXiv:2504.00929v2 Announce Type: replace 
Abstract: Estimation of hypothetical estimands in clinical trials typically does not make use of data that may be collected after the intercurrent event (ICE). Some recent papers have shown that such data can be used for estimation of hypothetical estimands, and that statistical efficiency and power can be increased compared to using estimators that only use data before the ICE. In this paper we critically examine the efficiency and bias of estimators that do and do not exploit data collected after ICEs, in a simplified setting. We find that efficiency can only be improved by assuming certain covariate effects are common between patients who do and do not experience ICEs, and that even when such an assumption holds, gains in efficiency will typically be modest. We moreover argue that the assumptions needed to gain efficiency by using post-ICE outcomes will often not hold, such that estimators using post-ICE data may lead to biased estimates and invalid inferences. As such, we recommend that in general estimation of hypothetical estimands should be based on estimators that do not make use of post-ICE data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00929v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan W. Bartlett, Rhian M. Daniel</dc:creator>
    </item>
    <item>
      <title>ROSE: Randomized Optimal Selection Design for Dose Optimization</title>
      <link>https://arxiv.org/abs/2505.03898</link>
      <description>arXiv:2505.03898v4 Announce Type: replace 
Abstract: The U.S. Food and Drug Administration (FDA) launched Project Optimus to shift the objective of dose selection from the maximum tolerated dose to the optimal biological dose (OBD), optimizing the benefit-risk tradeoff. One approach recommended by the FDA's guidance is to conduct randomized trials comparing multiple doses. In this paper, using the selection design framework (Simon et al., 1985), we propose a randomized optimal selection (ROSE) design, which minimizes sample size while ensuring the probability of correct selection of the OBD at prespecified accuracy levels. The ROSE design is simple to implement, involving a straightforward comparison of the difference in response rates between two dose arms against a predetermined decision boundary. We further consider a two-stage ROSE design that allows for early selection of the OBD at the interim when there is sufficient evidence, further reducing the sample size. Simulation studies demonstrate that the ROSE design exhibits desirable operating characteristics in correctly identifying the OBD. A sample size of 15 to 40 patients per dosage arm typically results in a percentage of correct selection of the optimal dose ranging from 60% to 70%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03898v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuqi Wang, Ying Yuan, Suyu Liu</dc:creator>
    </item>
    <item>
      <title>Statistical analysis of multivariate planar curves and applications to X-ray classification</title>
      <link>https://arxiv.org/abs/2508.11780</link>
      <description>arXiv:2508.11780v2 Announce Type: replace 
Abstract: Recent developments in computer vision have enabled the availability of segmented images across various domains, such as medicine, where segmented radiography images play an important role in diagnosis-making. As prediction problems are common in medical image analysis, this work explores the use of segmented images (through the associated contours they highlight) as predictors in a supervised classification context. Consequently, we develop a new approach for image analysis that takes into account the shape of objects within images. For this aim, we introduce a new formalism that extends the study of single random planar curves to the joint analysis of multiple planar curves-referred to here as multivariate planar curves. In this framework, we propose a solution to the alignment issue in statistical shape analysis. The obtained multivariate shape variables are then used in functional classification methods through tangent projections. Detection of cardiomegaly in segmented X-rays and numerical experiments on synthetic data demonstrate the appeal and robustness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11780v2</guid>
      <category>stat.ME</category>
      <category>cs.CV</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Issam-Ali Moindji\'e, Marie-H\'el\`ene Descary, C\'edric Beaulac</dc:creator>
    </item>
    <item>
      <title>Potato Potahto in the FAO-GAEZ Productivity Measures? Nonclassical Measurement Error with Multiple Proxies</title>
      <link>https://arxiv.org/abs/2502.12141</link>
      <description>arXiv:2502.12141v4 Announce Type: replace-cross 
Abstract: The FAO-GAEZ productivity data are widely used in Economics. However, the empirical literature rarely discusses measurement error. We use two proxies to derive novel analytical bounds around the effect of agricultural productivity in a setting with nonclassical measurement error. These bounds rely on assumptions that are weaker than the ones imposed in empirical studies and exhaust the information contained in the first two moments of the data. We reevaluate three influential studies, documenting that measurement error matters and that the impact of agricultural productivity may be smaller than previously reported. Our methodology has broad applications in empirical research involving mismeasured variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12141v4</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rafael Araujo, Vitor Possebom</dc:creator>
    </item>
    <item>
      <title>Implementing Errors on Errors: Bayesian vs Frequentist</title>
      <link>https://arxiv.org/abs/2505.06521</link>
      <description>arXiv:2505.06521v2 Announce Type: replace-cross 
Abstract: When combining apparently inconsistent experimental results, one often implements errors on errors. The Particle Data Group's phenomenological prescription offers a practical solution but lacks a firm theoretical foundation. To address this, D'Agostini and Cowan have proposed Bayesian and frequentist approaches, respectively, both introducing gamma-distributed auxiliary variables to model uncertainty in quoted errors. In this Letter, we show that these two formulations admit a parameter-by-parameter correspondence, and are structurally equivalent. This identification clarifies how Bayesian prior choices can be interpreted in terms of frequentist sampling assumptions, providing a unified probabilistic framework for modeling uncertainty in quoted variances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06521v2</guid>
      <category>hep-ph</category>
      <category>astro-ph.IM</category>
      <category>hep-ex</category>
      <category>stat.ME</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satoshi Mishima, Kin-ya Oda</dc:creator>
    </item>
    <item>
      <title>Bayes Error Rate Estimation in Difficult Situations</title>
      <link>https://arxiv.org/abs/2506.03159</link>
      <description>arXiv:2506.03159v2 Announce Type: replace-cross 
Abstract: The Bayes Error Rate (BER) is the fundamental limit on the achievable generalizable classification accuracy of any machine learning model due to inherent uncertainty within the data. BER estimators offer insight into the difficulty of any classification problem and set expectations for optimal classification performance. In order to be useful, the estimators must also be accurate with a limited number of samples on multivariate problems with unknown class distributions. To determine which estimators meet the minimum requirements for "usefulness", an in-depth examination of their accuracy is conducted using Monte Carlo simulations with synthetic data in order to obtain their confidence bounds for binary classification. To examine the usability of the estimators for real-world applications, new non-linear multi-modal test scenarios are introduced. In each scenario, 2500 Monte Carlo simulations per scenario are run over a wide range of BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized Henze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques, results show that kNN is overwhelmingly the more accurate non-parametric estimator. In order to reach the target of an under 5% range for the 95% confidence bounds, the minimum number of required samples per class is 1000. As more features are added, more samples are needed, so that 2500 samples per class are required at only 4 features. Other estimators do become more accurate than kNN as more features are added, but continuously fail to meet the target range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03159v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lesley Wheat, Martin v. Mohrenschildt, Saeid Habibi</dc:creator>
    </item>
  </channel>
</rss>
