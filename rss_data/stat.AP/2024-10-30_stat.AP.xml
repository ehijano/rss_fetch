<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Oct 2024 02:06:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>VISTA-SSM: Varying and Irregular Sampling Time-series Analysis via State Space Models</title>
      <link>https://arxiv.org/abs/2410.21527</link>
      <description>arXiv:2410.21527v1 Announce Type: new 
Abstract: We introduce VISTA, a clustering approach for multivariate and irregularly sampled time series based on a parametric state space mixture model. VISTA is specifically designed for the unsupervised identification of groups in datasets originating from healthcare and psychology where such sampling issues are commonplace. Our approach adapts linear Gaussian state space models (LGSSMs) to provide a flexible parametric framework for fitting a wide range of time series dynamics. The clustering approach itself is based on the assumption that the population can be represented as a mixture of a given number of LGSSMs. VISTA's model formulation allows for an explicit derivation of the log-likelihood function, from which we develop an expectation-maximization scheme for fitting model parameters to the observed data samples. Our algorithmic implementation is designed to handle populations of multivariate time series that can exhibit large changes in sampling rate as well as irregular sampling. We evaluate the versatility and accuracy of our approach on simulated and real-world datasets, including demographic trends, wearable sensor data, epidemiological time series, and ecological momentary assessments. Our results indicate that VISTA outperforms most comparable standard times series clustering methods. We provide an open-source implementation of VISTA in Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21527v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Brindle, Thomas Derrick Hull, Matteo Malgaroli, Nicolas Charon</dc:creator>
    </item>
    <item>
      <title>Communicating Risk with Possibility, Not Probability</title>
      <link>https://arxiv.org/abs/2410.21664</link>
      <description>arXiv:2410.21664v1 Announce Type: new 
Abstract: Communicating forecast uncertainty effectively is a persistent challenge in predictive endeavours such as weather forecasting. This paper explores the application of possibility theory as a complementary approach to traditional probability in risk communication. Unlike probability, possibility theory allows for the representation of uncertain events as ranges of potential, distinguished by degrees of plausibility possibility and certainty necessity. Using a simplified fuzzy-logic inference system, we generate possibility forecasts of ozone-concentration forecasts from meteorological inputs. Observations are pre-processed as degrees of an adjective, e.g., 1 m/s wind speed may belong to the adjective "calm" at degree 0.8 (perhaps represented with an adverb as "substantially"). Aggregation of all rule activations yields a possibility distribution over the output range. As possibility is an upper bound of probability, possibilities provides most value for risk-averse stakeholders sensitive to the event of interest, especially near the event's predictability horizon when there is most uncertainty to remove. By setting the forecast challenge as the possibility of an event, we effectively extend the predictability horizon: trading some specificity for detection of fainter signals of the event's future occurrence. Possibility theory appreciates uncertainty's dual nature: inherent randomness (aleatoric) and knowledge deficiency (epistemic). While the unfamiliar nature of the theory requires more accessible language before operational use and public communication, possibility offers substantial practical benefits for assessment of uncertainty at the edge of predictability limits for vulnerable users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21664v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John R. Lawson</dc:creator>
    </item>
    <item>
      <title>Reconstructing East Asian Temperatures from 1368 to 1911 Using Historical Documents, Climate Models, and Data Assimilation</title>
      <link>https://arxiv.org/abs/2410.21790</link>
      <description>arXiv:2410.21790v1 Announce Type: new 
Abstract: We present a novel approach for reconstructing annual temperatures in East Asia from 1368 to 1911, leveraging the Reconstructed East Asian Climate Historical Encoded Series (REACHES). The lack of instrumental data during this period poses significant challenges to understanding past climate conditions. REACHES digitizes historical documents from the Ming and Qing dynasties of China, converting qualitative descriptions into a four-level ordinal temperature scale. However, these index-based data are biased toward abnormal or extreme weather phenomena, leading to data gaps that likely correspond to normal conditions. To address this bias and reconstruct historical temperatures at any point within East Asia, including locations without direct historical data, we employ a three-tiered statistical framework. First, we perform kriging to interpolate temperature data across East Asia, adopting a zero-mean assumption to handle missing information. Next, we utilize the Last Millennium Ensemble (LME) reanalysis data and apply quantile mapping to calibrate the kriged REACHES data to Celsius temperature scales. Finally, we introduce a novel Bayesian data assimilation method that integrates the kriged Celsius data with LME simulations to enhance reconstruction accuracy. We model the LME data at each geographic location using a flexible nonstationary autoregressive time series model and employ regularized maximum likelihood estimation with a fused lasso penalty. The resulting dynamic distribution serves as a prior, which is refined via Kalman filtering by incorporating the kriged Celsius REACHES data to yield posterior temperature estimates. This comprehensive integration of historical documentation, contemporary climate models, and advanced statistical methods improves the accuracy of historical temperature reconstructions and provides a crucial resource for future environmental and climate studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21790v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Sun, Kuan-hui Elaine Lin, Wan-Ling Tseng, Pao K. Wang, Hsin-Cheng Huang</dc:creator>
    </item>
    <item>
      <title>Expected Improvement applied to an industrialcontext -- Prediction of new geometries increasing theefficiency of fans</title>
      <link>https://arxiv.org/abs/2410.21830</link>
      <description>arXiv:2410.21830v1 Announce Type: new 
Abstract: In automotive industry, client needs evolve quickly in a competitiveness context, particularly, regarding the fan involved in the engine cooling module. This study has been done in cooperation with the automotive supplier Valeo. Here, we propose to use the Kriging interpolation and the Expected Improvement algorithm to provide new fan designs with high performances in terms of eciency. As far as we know, such a use of Kriging and Expected Improvement methodologies are innovative and provide really promising results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21830v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal de la Societe Fran{\c c}aise de Statistique, 2021, 162 (1), pp 22-45</arxiv:journal_reference>
      <dc:creator>Agn\`es Lagnoux (IMT), T. M. Ngoc Nguyen, Bruno Demory, Manuel Henner</dc:creator>
    </item>
    <item>
      <title>Pedestrian crash causation analysis near bus stops: Insights from random parameters NB-Lindley models</title>
      <link>https://arxiv.org/abs/2410.22253</link>
      <description>arXiv:2410.22253v1 Announce Type: new 
Abstract: Pedestrian safety near bus stops is a growing concern due to rising injuries and fatalities, underscoring the need to mitigate crash risks and encourage active travel through public transportation in urban corridors. However, existing research has often overlooked the effects of exposure characteristics and bus stop design elements on pedestrian-vehicle crashes, limiting the development of data-driven interventions. This study aims to fill that gap by quantifying the relationship between various features (e.g., bus stop design, traffic, and passenger activity, roadway environment, etc.) and vehicle-pedestrian crashes in Fort Worth, Texas, using the Random Parameters Negative Binomial-Lindley (RPNB-L) model to address the unobserved heterogeneity. By accounting for site-specific variability, the RPNB-L model offers a more nuanced understanding of the factors influencing crash frequency. The analysis covers 596 bus stop sites, integrating crash data from 2018 to 2022 with roadway network and stop design characteristics. Results show that the RPNB-L model outperforms traditional Negative Binomial (NB) and fixed-coefficient NB-L models in capturing variability across sites. Significant predictors of higher pedestrian KABCO crash frequencies include average annual daily traffic (AADT), bus passenger boarding rates, stops located near-side of intersections, mixed-use areas, and the absence of medians, sidewalks, or crosswalks. Additional factors, such as poor lighting, high school numbers, and lower speed limits (e.g., 35 mph), also increase crash frequency. The study applies the Potential for Safety Improvement (PSI) metric to identify high-risk sites and prioritize key corridors for intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22253v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Anis, Srinivas R. Geedipally, Dominique Lord</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Models for Multiple Raters: a General Statistical Framework</title>
      <link>https://arxiv.org/abs/2410.21498</link>
      <description>arXiv:2410.21498v1 Announce Type: cross 
Abstract: Rating procedure is crucial in many applied fields (e.g., educational, clinical, emergency). It implies that a rater (e.g., teacher, doctor) rates a subject (e.g., student, doctor) on a rating scale. Given raters variability, several statistical methods have been proposed for assessing and improving the quality of ratings. The analysis and the estimate of the Intraclass Correlation Coefficient (ICC) are major concerns in such cases. As evidenced by the literature, ICC might differ across different subgroups of raters and might be affected by contextual factors and subject heterogeneity. Model estimation in the presence of heterogeneity has been one of the recent challenges in this research line. Consequently, several methods have been proposed to address this issue under a parametric multilevel modelling framework, in which strong distributional assumptions are made. We propose a more flexible model under the Bayesian nonparametric (BNP) framework, in which most of those assumptions are relaxed. By eliciting hierarchical discrete nonparametric priors, the model accommodates clusters among raters and subjects, naturally accounts for heterogeneity, and improves estimate accuracy. We propose a general BNP heteroscedastic framework to analyze rating data and possible latent differences among subjects and raters. The estimated densities are used to make inferences about the rating process and the quality of the ratings. By exploiting a stick-breaking representation of the Dirichlet Process a general class of ICC indices might be derived for these models. Theoretical results about the ICC are provided together with computational strategies. Simulations and a real-world application are presented and possible future directions are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21498v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe Mignemi, Ioanna Manolopoulou</dc:creator>
    </item>
    <item>
      <title>Enhancing parameter estimation in finite mixture of generalized normal distributions</title>
      <link>https://arxiv.org/abs/2410.21559</link>
      <description>arXiv:2410.21559v1 Announce Type: cross 
Abstract: Mixtures of generalized normal distributions (MGND) have gained popularity for modelling datasets with complex statistical behaviours. However, the estimation of the shape parameter within the maximum likelihood framework is quite complex, presenting the risk of numerical and degeneracy issues. This study introduced an expectation conditional maximization algorithm that includes an adaptive step size function within Newton-Raphson updates of the shape parameter and a modified criterion for stopping the EM iterations. Through extensive simulations, the effectiveness of the proposed algorithm in overcoming the limitations of existing approaches, especially in scenarios with high shape parameter values, high parameters overalp and low sample sizes, is shown. A detailed comparative analysis with a mixture of normals and Student-t distributions revealed that the MGND model exhibited superior goodness-of-fit performance when used to fit the density of the returns of 50 stocks belonging to the Euro Stoxx index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21559v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierdomenico Duttilo, Stefano Antonio Gattone</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach to Harnessing the Power of LLMs in Authorship Attribution</title>
      <link>https://arxiv.org/abs/2410.21716</link>
      <description>arXiv:2410.21716v1 Announce Type: cross 
Abstract: Authorship attribution aims to identify the origin or author of a document. Traditional approaches have heavily relied on manual features and fail to capture long-range correlations, limiting their effectiveness. Recent advancements leverage text embeddings from pre-trained language models, which require significant fine-tuning on labeled data, posing challenges in data dependency and limited interpretability. Large Language Models (LLMs), with their deep reasoning capabilities and ability to maintain long-range textual associations, offer a promising alternative. This study explores the potential of pre-trained LLMs in one-shot authorship attribution, specifically utilizing Bayesian approaches and probability outputs of LLMs. Our methodology calculates the probability that a text entails previous writings of an author, reflecting a more nuanced understanding of authorship. By utilizing only pre-trained models such as Llama-3-70B, our results on the IMDb and blog datasets show an impressive 85\% accuracy in one-shot authorship classification across ten authors. Our findings set new baselines for one-shot authorship analysis using LLMs and expand the application scope of these models in forensic linguistics. This work also includes extensive ablation studies to validate our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21716v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhengmian Hu, Tong Zheng, Heng Huang</dc:creator>
    </item>
    <item>
      <title>PK-YOLO: Pretrained Knowledge Guided YOLO for Brain Tumor Detection in Multiplanar MRI Slices</title>
      <link>https://arxiv.org/abs/2410.21822</link>
      <description>arXiv:2410.21822v1 Announce Type: cross 
Abstract: Brain tumor detection in multiplane Magnetic Resonance Imaging (MRI) slices is a challenging task due to the various appearances and relationships in the structure of the multiplane images. In this paper, we propose a new You Only Look Once (YOLO)-based detection model that incorporates Pretrained Knowledge (PK), called PK-YOLO, to improve the performance for brain tumor detection in multiplane MRI slices. To our best knowledge, PK-YOLO is the first pretrained knowledge guided YOLO-based object detector. The main components of the new method are a pretrained pure lightweight convolutional neural network-based backbone via sparse masked modeling, a YOLO architecture with the pretrained backbone, and a regression loss function for improving small object detection. The pretrained backbone allows for feature transferability of object queries on individual plane MRI slices into the model encoders, and the learned domain knowledge base can improve in-domain detection. The improved loss function can further boost detection performance on small-size brain tumors in multiplanar two-dimensional MRI slices. Experimental results show that the proposed PK-YOLO achieves competitive performance on the multiplanar MRI brain tumor detection datasets compared to state-of-the-art YOLO-like and DETR-like object detectors. The code is available at https://github.com/mkang315/PK-YOLO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21822v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Kang, Fung Fung Ting, Rapha\"el C. -W. Phan, Chee-Ming Ting</dc:creator>
    </item>
    <item>
      <title>Hypothesis tests and model parameter estimation on data sets with missing correlation information</title>
      <link>https://arxiv.org/abs/2410.22333</link>
      <description>arXiv:2410.22333v1 Announce Type: cross 
Abstract: Ideally, all analyses of normally distributed data should include the full covariance information between all data points. In practice, the full covariance matrix between all data points is not always available. Either because a result was published without a covariance matrix, or because one tries to combine multiple results from separate publications. For simple hypothesis tests, it is possible to define robust test statistics that will behave conservatively in the presence on unknown correlations. For model parameter fits, one can inflate the variance by factor to ensure that things remain conservative at least up to a chosen confidence level. This paper describes a class of robust test statistics for simply hypothesis tests, as well as an algorithm to determine the necessary inflation factor model parameter fits. It then presents some example applications of the methods to real neutrino interaction data and model comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22333v1</guid>
      <category>stat.ME</category>
      <category>hep-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Koch</dc:creator>
    </item>
    <item>
      <title>Spatial Risk Patterns-ANOVA: Multivariate Analysis Of Suicide-Related Emergency Calls</title>
      <link>https://arxiv.org/abs/2410.21227</link>
      <description>arXiv:2410.21227v2 Announce Type: replace 
Abstract: Multivariate spatial disease mapping has become a pivotal part of everyday practice in social epidemiology. Despite the existence of several specifications for the relation between different outcomes, there is still a need for a new strategy that focuses on comparing the spatial risk patterns of different subgroups of the population. This paper introduces a new approach for detecting differences in spatial risk patterns between different populations at risk, using suicide-related emergency calls to study suicide risks in the Valencian Community (Spain).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21227v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>P. Escobar-Hern\'andez, A. L\'opez-Qu\'ilez, F. Palm\'i-Perales, M. Marco</dc:creator>
    </item>
    <item>
      <title>On the potential benefits of entropic regularization for smoothing Wasserstein estimators</title>
      <link>https://arxiv.org/abs/2210.06934</link>
      <description>arXiv:2210.06934v3 Announce Type: replace-cross 
Abstract: This paper is focused on the study of entropic regularization in optimal transport as a smoothing method for Wasserstein estimators, through the prism of the classical tradeoff between approximation and estimation errors in statistics. Wasserstein estimators are defined as solutions of variational problems whose objective function involves the use of an optimal transport cost between probability measures. Such estimators can be regularized by replacing the optimal transport cost by its regularized version using an entropy penalty on the transport plan. The use of such a regularization has a potentially significant smoothing effect on the resulting estimators. In this work, we investigate its potential benefits on the approximation and estimation properties of regularized Wasserstein estimators. Our main contribution is to discuss how entropic regularization may reach, at a lower computational cost, statistical performances that are comparable to those of un-regularized Wasserstein estimators in statistical learning problems involving distributional data analysis. To this end, we present new theoretical results on the convergence of regularized Wasserstein estimators. We also study their numerical performances using simulated and real data in the supervised learning problem of proportions estimation in mixture models using optimal transport.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.06934v3</guid>
      <category>stat.ML</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J\'er\'emie Bigot, Paul Freulon, Boris P. Hejblum, Arthur Leclaire</dc:creator>
    </item>
    <item>
      <title>Automated threshold selection and associated inference uncertainty for univariate extremes</title>
      <link>https://arxiv.org/abs/2310.17999</link>
      <description>arXiv:2310.17999v5 Announce Type: replace-cross 
Abstract: Threshold selection is a fundamental problem in any threshold-based extreme value analysis. While models are asymptotically motivated, selecting an appropriate threshold for finite samples is difficult and highly subjective through standard methods. Inference for high quantiles can also be highly sensitive to the choice of threshold. Too low a threshold choice leads to bias in the fit of the extreme value model, while too high a choice leads to unnecessary additional uncertainty in the estimation of model parameters. We develop a novel methodology for automated threshold selection that directly tackles this bias-variance trade-off. We also develop a method to account for the uncertainty in the threshold estimation and propagate this uncertainty through to high quantile inference. Through a simulation study, we demonstrate the effectiveness of our method for threshold selection and subsequent extreme quantile estimation, relative to the leading existing methods, and show how the method's effectiveness is not sensitive to the tuning parameters. We apply our method to the well-known, troublesome example of the River Nidd dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17999v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Conor Murphy, Jonathan A. Tawn, Zak Varty</dc:creator>
    </item>
    <item>
      <title>Distribution of lowest eigenvalue in $k$-body bosonic random matrix ensembles</title>
      <link>https://arxiv.org/abs/2405.00190</link>
      <description>arXiv:2405.00190v3 Announce Type: replace-cross 
Abstract: We numerically study the distribution of the lowest eigenvalue of finite many-boson systems with $k$-body interactions modeled by Bosonic Embedded Gaussian Orthogonal [BEGOE($k$)] and Unitary [BEGUE($k$)] random matrix Ensembles. Following the recently published result that the $q$-normal describes the smooth form of the eigenvalue density of the $k$-body embedded ensembles, the first four moments of the distribution of lowest eigenvalues have been analyzed as a function of the $q$ parameter, with $q \sim 1$ for $k = 1$ and $q = 0$ for $k = m$; $m$ being the number of bosons. Analytics are difficult as we are dealing with highly correlated variables, however we provide ansatzs for centroids and variances of these distributions. These match very well with the numerical results obtained. Our results show the distribution exhibits a smooth transition from Gaussian like for $q$ close to 1 to a modified Gumbel like for intermediate values of $q$ to the well-known Tracy-Widom distribution for $q=0$. It should be emphasized that this is a new result which numerically demonstrates that the distribution of the lowest eigenvalue of finite many-boson systems with $k$-body interactions exhibits a smooth transition from Gaussian like (for $q$ close to 1) to a modified Gumbel like (for intermediate values of $q$) to the well-known Tracy-Widom distribution (for $q=0$). In addition, we have also studied the distribution of normalized spacing between the lowest and next lowest eigenvalues and it is seen that this distribution exhibits a transition from Wigner's surmise (for $k=1$) to Poisson (for intermediate $k$ values with $k \le m/2$) to Wigner's surmise (starting from $k = m/2$ to $k = m$) with decreasing $q$ value. Thus, the spacings at the spectrum edge behave differently from the spacings inside the spectrum bulk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00190v3</guid>
      <category>quant-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>N. D. Chavda, Priyanka Rao, V. K. B. Kota, Manan Vyas</dc:creator>
    </item>
    <item>
      <title>Best Linear Unbiased Estimate from Privatized Histograms</title>
      <link>https://arxiv.org/abs/2409.04387</link>
      <description>arXiv:2409.04387v3 Announce Type: replace-cross 
Abstract: In differential privacy (DP) mechanisms, it can be beneficial to release "redundant" outputs, in the sense that a quantity can be estimated by combining different combinations of privatized values. Indeed, this structure is present in the DP 2020 Decennial Census products published by the U.S. Census Bureau. With this structure, the DP output can be improved by enforcing self-consistency (i.e., estimators obtained by combining different values result in the same estimate) and we show that the minimum variance processing is a linear projection. However, standard projection algorithms are too computationally expensive in terms of both memory and execution time for applications such as the Decennial Census. We propose the Scalable Efficient Algorithm for Best Linear Unbiased Estimate (SEA BLUE), based on a two step process of aggregation and differencing that 1) enforces self-consistency through a linear and unbiased procedure, 2) is computationally and memory efficient, 3) achieves the minimum variance solution under certain structural assumptions, and 4) is empirically shown to be robust to violations of these structural assumptions. We propose three methods of calculating confidence intervals from our estimates, under various assumptions. We apply SEA BLUE to two 2010 Census demonstration products, illustrating its scalability and validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04387v3</guid>
      <category>stat.CO</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Adam Edwards, Paul Bartholomew, Andrew Sillers</dc:creator>
    </item>
    <item>
      <title>Establishing Nationwide Power System Vulnerability Index across US Counties Using Interpretable Machine Learning</title>
      <link>https://arxiv.org/abs/2410.19754</link>
      <description>arXiv:2410.19754v2 Announce Type: replace-cross 
Abstract: Power outages have become increasingly frequent, intense, and prolonged in the US due to climate change, aging electrical grids, and rising energy demand. However, largely due to the absence of granular spatiotemporal outage data, we lack data-driven evidence and analytics-based metrics to quantify power system vulnerability. This limitation has hindered the ability to effectively evaluate and address vulnerability to power outages in US communities. Here, we collected ~179 million power outage records at 15-minute intervals across 3022 US contiguous counties (96.15% of the area) from 2014 to 2023. We developed a power system vulnerability assessment framework based on three dimensions (intensity, frequency, and duration) and applied interpretable machine learning models (XGBoost and SHAP) to compute Power System Vulnerability Index (PSVI) at the county level. Our analysis reveals a consistent increase in power system vulnerability over the past decade. We identified 318 counties across 45 states as hotspots for high power system vulnerability, particularly in the West Coast (California and Washington), the East Coast (Florida and the Northeast area), the Great Lakes megalopolis (Chicago-Detroit metropolitan areas), and the Gulf of Mexico (Texas). Heterogeneity analysis indicates that urban counties, counties with interconnected grids, and states with high solar generation exhibit significantly higher vulnerability. Our results highlight the significance of the proposed PSVI for evaluating the vulnerability of communities to power outages. The findings underscore the widespread and pervasive impact of power outages across the country and offer crucial insights to support infrastructure operators, policymakers, and emergency managers in formulating policies and programs aimed at enhancing the resilience of the US power infrastructure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19754v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junwei Ma, Bo Li, Olufemi A. Omitaomu, Ali Mostafavi</dc:creator>
    </item>
    <item>
      <title>Valid Bootstraps for Networks with Applications to Network Visualisation</title>
      <link>https://arxiv.org/abs/2410.20895</link>
      <description>arXiv:2410.20895v2 Announce Type: replace-cross 
Abstract: Quantifying uncertainty in networks is an important step in modelling relationships and interactions between entities. We consider the challenge of bootstrapping an inhomogeneous random graph when only a single observation of the network is made and the underlying data generating function is unknown. We utilise an exchangeable network test that can empirically validate bootstrap samples generated by any method, by testing if the observed and bootstrapped networks are statistically distinguishable. We find that existing methods fail this test. To address this, we propose a principled, novel, distribution-free network bootstrap using k-nearest neighbour smoothing, that can regularly pass this exchangeable network test in both synthetic and real-data scenarios. We demonstrate the utility of this work in combination with the popular data visualisation method t-SNE, where uncertainty estimates from bootstrapping are used to explain whether visible structures represent real statistically sound structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20895v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emerald Dilworth, Ed Davis, Daniel J. Lawson</dc:creator>
    </item>
  </channel>
</rss>
