<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jun 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Examining the effects of music on cognitive skills of children in early childhood with the Pythagorean fuzzy set approach</title>
      <link>https://arxiv.org/abs/2506.12016</link>
      <description>arXiv:2506.12016v1 Announce Type: new 
Abstract: There are many genetic and environmental factors that affect cognitive development. Music education can also be considered as one of the environmental factors. Some researchers emphasize that Music is an action that requires meta-cognitive functions such as mathematics and chess and supports spatial intelligence. The effect of Music on cognitive development in early childhood was examined using the Pythagorean Fuzzy Sets(PFS) method defined by Yager. This study created PFS based on experts' opinions, and an algorithm was given according to PFS. The algorithm's results supported the experts' data on the development of spatial-temporal skills in music education given in early childhood. The algorithm's ranking was done using the Expectation Score Function. The rankings obtained from the algorithm overlap with the experts' rankings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12016v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Murat Kirisci, Nihat Topac, Musa Bardak</dc:creator>
    </item>
    <item>
      <title>Towards Unified Neural Decoding with Brain Functional Network Modeling</title>
      <link>https://arxiv.org/abs/2506.12055</link>
      <description>arXiv:2506.12055v1 Announce Type: new 
Abstract: Recent achievements in implantable brain-computer interfaces (iBCIs) have demonstrated the potential to decode cognitive and motor behaviors with intracranial brain recordings; however, individual physiological and electrode implantation heterogeneities have constrained current approaches to neural decoding within single individuals, rendering interindividual neural decoding elusive. Here, we present Multi-individual Brain Region-Aggregated Network (MIBRAIN), a neural decoding framework that constructs a whole functional brain network model by integrating intracranial neurophysiological recordings across multiple individuals. MIBRAIN leverages self-supervised learning to derive generalized neural prototypes and supports group-level analysis of brain-region interactions and inter-subject neural synchrony. To validate our framework, we recorded stereoelectroencephalography (sEEG) signals from a cohort of individuals performing Mandarin syllable articulation. Both real-time online and offline decoding experiments demonstrated significant improvements in both audible and silent articulation decoding, enhanced decoding accuracy with increased multi-subject data integration, and effective generalization to unseen subjects. Furthermore, neural predictions for regions without direct electrode coverage were validated against authentic neural data. Overall, this framework paves the way for robust neural decoding across individuals and offers insights for practical clinical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12055v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Di Wu, Linghao Bu, Yifei Jia, Lu Cao, Siyuan Li, Siyu Chen, Yueqian Zhou, Sheng Fan, Wenjie Ren, Dengchang Wu, Kang Wang, Yue Zhang, Yuehui Ma, Jie Yang, Mohamad Sawan</dc:creator>
    </item>
    <item>
      <title>Wanting to Be Understood Explains the Meta-Problem of Consciousness</title>
      <link>https://arxiv.org/abs/2506.12086</link>
      <description>arXiv:2506.12086v1 Announce Type: new 
Abstract: Because we are highly motivated to be understood, we created public external representations -- mime, language, art -- to externalise our inner states. We argue that such external representations are a pre-condition for access consciousness, the global availability of information for reasoning. Yet the bandwidth of access consciousness is tiny compared with the richness of `raw experience', so no external representation can reproduce that richness in full. Ordinarily an explanation of experience need only let an audience `grasp' the relevant pattern, not relive the phenomenon. But our drive to be understood, and our low level sensorimotor capacities for `grasping' so rich, that the demand for an explanation of the feel of experience cannot be ``satisfactory''. That inflated epistemic demand (the preeminence of our expectation that we could be perfectly understood by another or ourselves) rather than an irreducible metaphysical gulf -- keeps the hard problem of consciousness alive. But on the plus side, it seems we will simply never give up creating new ways to communicate and think about our experiences. In this view, to be consciously aware is to strive to have one's agency understood by oneself and others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12086v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chrisantha Fernando, Dylan Banarse, Simon Osindero</dc:creator>
    </item>
    <item>
      <title>Scale-Invariance Drives Convergence in AI and Brain Representations</title>
      <link>https://arxiv.org/abs/2506.12117</link>
      <description>arXiv:2506.12117v1 Announce Type: new 
Abstract: Despite variations in architecture and pretraining strategies, recent studies indicate that large-scale AI models often converge toward similar internal representations that also align with neural activity. We propose that scale-invariance, a fundamental structural principle in natural systems, is a key driver of this convergence. In this work, we propose a multi-scale analytical framework to quantify two core aspects of scale-invariance in AI representations: dimensional stability and structural similarity across scales. We further investigate whether these properties can predict alignment performance with functional Magnetic Resonance Imaging (fMRI) responses in the visual cortex. Our analysis reveals that embeddings with more consistent dimension and higher structural similarity across scales align better with fMRI data. Furthermore, we find that the manifold structure of fMRI data is more concentrated, with most features dissipating at smaller scales. Embeddings with similar scale patterns align more closely with fMRI data. We also show that larger pretraining datasets and the inclusion of language modalities enhance the scale-invariance properties of embeddings, further improving neural alignment. Our findings indicate that scale-invariance is a fundamental structural principle that bridges artificial and biological representations, providing a new framework for evaluating the structural quality of human-like AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12117v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junjie Yu, Wenxiao Ma, Jianyu Zhang, Haotian Deng, Zihan Deng, Yi Guo, Quanying Liu</dc:creator>
    </item>
    <item>
      <title>Characterizing Neural Manifolds' Properties and Curvatures using Normalizing Flows</title>
      <link>https://arxiv.org/abs/2506.12187</link>
      <description>arXiv:2506.12187v1 Announce Type: new 
Abstract: Neuronal activity is found to lie on low-dimensional manifolds embedded within the high-dimensional neuron space. Variants of principal component analysis are frequently employed to assess these manifolds. These methods are, however, limited by assuming a Gaussian data distribution and a flat manifold. In this study, we introduce a method designed to satisfy three core objectives: (1) extract coordinated activity across neurons, described either statistically as correlations or geometrically as manifolds; (2) identify a small number of latent variables capturing these structures; and (3) offer an analytical and interpretable framework characterizing statistical properties by a characteristic function and describing manifold geometry through a collection of charts.
  To this end, we employ Normalizing Flows (NFs), which learn an underlying probability distribution of data by an invertible mapping between data and latent space. Their simplicity and ability to compute exact likelihoods distinguish them from other generative networks. We adjust the NF's training objective to distinguish between relevant (in manifold) and noise dimensions (out of manifold). Additionally, we find that different behavioral states align with the components of the latent Gaussian mixture model, enabling their treatment as distinct curved manifolds. Subsequently, we approximate the network for each mixture component with a quadratic mapping, allowing us to characterize both neural manifold curvature and non-Gaussian correlations among recording channels.
  Applying the method to recordings in macaque visual cortex, we demonstrate that state-dependent manifolds are curved and exhibit complex statistical dependencies. Our approach thus enables an expressive description of neural population activity, uncovering non-linear interactions among groups of neurons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12187v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Bouss, Sandra Nester, Kirsten Fischer, Claudia Merger, Alexandre Ren\'e, Moritz Helias</dc:creator>
    </item>
    <item>
      <title>Mapping Neural Theories of Consciousness onto the Common Model of Cognition</title>
      <link>https://arxiv.org/abs/2506.12224</link>
      <description>arXiv:2506.12224v1 Announce Type: new 
Abstract: A beginning is made at mapping four neural theories of consciousness onto the Common Model of Cognition. This highlights how the four jointly depend on recurrent local modules plus a cognitive cycle operating on a global working memory with complex states, and reveals how an existing integrative view of consciousness from a neural perspective aligns with the Com-mon Model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12224v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul S. Rosenbloom, John E. Laird, Christian Lebiere, Andrea Stocco</dc:creator>
    </item>
    <item>
      <title>Metric Framework of Coherent Activity Patterns Identification in Spiking Neuronal Networks</title>
      <link>https://arxiv.org/abs/2506.12291</link>
      <description>arXiv:2506.12291v1 Announce Type: new 
Abstract: Partial synchronization plays a crucial role in the functioning of neuronal networks: selective, coordinated activation of neurons enables information processing that flexibly adapts to a changing computational context. Since the structure of coherent activity patterns reflects the network's current state, developing automated tools to identify them is a key challenge in neurodynamics. Existing methods for analyzing neuronal dynamics tend to focus on global characteristics of the network, such as its aggregated synchrony level. While this approach can distinguish between the network's main dynamical states, it cannot reveal the localization or properties of distinct coherent patterns.
  In this work, we propose a new perspective on neural dynamics analysis that enables the study of network coherence at the single-neuron scale. We interpret the network as a metric space of neurons and represent its instantaneous state as an activity function on that space. We identify specific coherent activity clusters as regions where the activity function exhibits spatial continuity. Each cluster's activity is further characterized using the analytical properties of the activity function within that region. This approach yields a concise yet detailed algorithmic profile of the network's activity patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12291v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniil Radushev, Olesia Dogonasheva, Boris Gutkin, Denis Zakharov</dc:creator>
    </item>
    <item>
      <title>Amplitude equations for stored spatially heterogeneous states</title>
      <link>https://arxiv.org/abs/2506.13576</link>
      <description>arXiv:2506.13576v1 Announce Type: new 
Abstract: Coupled evolution equations for the amplitudes of heterogeneous states (memories) stored in the connectivity of distributed systems with distance-constrained non-local interactions are derived. The resulting system of coupled amplitude equations describes the spatio-temporal dynamics of the pattern amplitudes. It is shown that these types of distributed systems perform pattern completion and selection, and that the distance restricted connections afford spatio-temporal memory pattern dynamics taking the form of patterning fronts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13576v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>nlin.PS</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akke Mats Houben</dc:creator>
    </item>
    <item>
      <title>Effective Stimulus Propagation in Neural Circuits: Driver Node Selection</title>
      <link>https://arxiv.org/abs/2506.13615</link>
      <description>arXiv:2506.13615v1 Announce Type: new 
Abstract: Precise control of signal propagation in modular neural networks represents a fundamental challenge in computational neuroscience. We establish a framework for identifying optimal control nodes that maximize stimulus transmission between weakly coupled neural populations. Using spiking stochastic block model networks, we systematically compare driver node selection strategies - including random sampling and topology-based centrality measures (degree, betweenness, closeness, eigenvector, harmonic, and percolation centrality) - to determine minimal control inputs for achieving inter-population synchronization.
  Targeted stimulation of just 10-20% of the most central neurons in the source population significantly enhances spiking propagation fidelity compared to random selection. This approach yields a 2.7-fold increase in signal transfer efficiency at critical inter-module connection densities p_inter = 0.04-0.07. These findings establish a theoretical foundation for precision neuromodulation in biological neural systems and neurotechnology applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13615v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bulat Batuev, Arsenii Onuchin, Sergey Sukhov</dc:creator>
    </item>
    <item>
      <title>Hopfield Networks as Models of Emergent Function in Biology</title>
      <link>https://arxiv.org/abs/2506.13076</link>
      <description>arXiv:2506.13076v1 Announce Type: cross 
Abstract: Hopfield models, originally developed to study memory retrieval in neural networks, have become versatile tools for modeling diverse biological systems in which function emerges from collective dynamics. In this review, we provide a pedagogical introduction to both classical and modern Hopfield networks from a biophysical perspective. After presenting the underlying mathematics, we build physical intuition through three complementary interpretations of Hopfield dynamics: as noise discrimination, as a geometric construction defining a natural coordinate system in pattern space, and as gradient-like descent on an energy landscape. We then survey recent applications of Hopfield networks a variety of biological setting including cellular differentiation and epigenetic memory, molecular self-assembly, and spatial neural representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13076v1</guid>
      <category>physics.bio-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.soft</category>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Yampolskaya, Pankaj Mehta</dc:creator>
    </item>
    <item>
      <title>Stimulus Motion Perception Studies Imply Specific Neural Computations in Human Visual Stabilization</title>
      <link>https://arxiv.org/abs/2506.13506</link>
      <description>arXiv:2506.13506v1 Announce Type: cross 
Abstract: Even during fixation the human eye is constantly in low amplitude motion, jittering over small angles in random directions at up to 100Hz. This motion results in all features of the image on the retina constantly traversing a number of cones, yet objects which are stable in the world are perceived to be stable, and any object which is moving in the world is perceived to be moving. A series of experiments carried out over a dozen years revealed the psychophysics of visual stabilization to be more nuanced than might be assumed, say, from the mechanics of stabilization of camera images, or what might be assumed to be the simplest solution from an evolutionary perspective. The psychophysics revealed by the experiments strongly implies a specific set of operations on retinal signals resulting in the observed stabilization behavior. The presentation is in two levels. First is a functional description of the action of the mechanism that is very likely responsible for the experimentally observed behavior. Second is a more speculative proposal of circuit-level neural elements that might implement the functional behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13506v1</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David W Arathorn, Josephine C. D'Angelo, Austin Roorda</dc:creator>
    </item>
    <item>
      <title>Contrastive Self-Supervised Learning As Neural Manifold Packing</title>
      <link>https://arxiv.org/abs/2506.13717</link>
      <description>arXiv:2506.13717v1 Announce Type: cross 
Abstract: Contrastive self-supervised learning based on point-wise comparisons has been widely studied for vision tasks. In the visual cortex of the brain, neuronal responses to distinct stimulus classes are organized into geometric structures known as neural manifolds. Accurate classification of stimuli can be achieved by effectively separating these manifolds, akin to solving a packing problem. We introduce Contrastive Learning As Manifold Packing (CLAMP), a self-supervised framework that recasts representation learning as a manifold packing problem. CLAMP introduces a loss function inspired by the potential energy of short-range repulsive particle systems, such as those encountered in the physics of simple liquids and jammed packings. In this framework, each class consists of sub-manifolds embedding multiple augmented views of a single image. The sizes and positions of the sub-manifolds are dynamically optimized by following the gradient of a packing loss. This approach yields interpretable dynamics in the embedding space that parallel jamming physics, and introduces geometrically meaningful hyperparameters within the loss function. Under the standard linear evaluation protocol, which freezes the backbone and trains only a linear classifier, CLAMP achieves competitive performance with state-of-the-art self-supervised models. Furthermore, our analysis reveals that neural manifolds corresponding to different categories emerge naturally and are effectively separated in the learned representation space, highlighting the potential of CLAMP to bridge insights from physics, neural science, and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13717v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanming Zhang, David J. Heeger, Stefano Martiniani</dc:creator>
    </item>
    <item>
      <title>Hodge-Decomposition of Brain Networks</title>
      <link>https://arxiv.org/abs/2211.10542</link>
      <description>arXiv:2211.10542v3 Announce Type: replace 
Abstract: We propose to analyze dynamically changing brain networks by decomposing them into three orthogonal components through the Hodge decomposition. We propose to quantify the magnitude and relative strength of each component. We performed extensive simulation studies with known ground truth. The Hodge decomposition is then applied to the dynamically changing human brain networks obtained from a resting-state functional magnetic resonance imaging study. Our results indicate that the components of the Hodge decomposition contain biologically interpretable topological features that provide statistically significant findings not easily captured by traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.10542v3</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>D. Vijay Anand, Anass B El-Yaagoubi, Hernando Ombao, Moo K. Chung</dc:creator>
    </item>
    <item>
      <title>On the Minimal Theory of Consciousness Implicit in Active Inference</title>
      <link>https://arxiv.org/abs/2410.06633</link>
      <description>arXiv:2410.06633v2 Announce Type: replace 
Abstract: The multifaceted nature of subjective experience poses a challenge to the study of consciousness. Traditional neuroscientific approaches often concentrate on isolated facets, such as perceptual awareness or the global state of consciousness and construct a theory around the relevant empirical paradigms and findings. Theories of consciousness are, therefore, often difficult to compare; indeed, there might be little overlap in the phenomena such theories aim to explain. Here, we take a different approach: starting with active inference, a first principles framework for modelling behaviour as (approximate) Bayesian inference, and building up to a minimal theory of consciousness, which emerges from the shared features of computational models derived under active inference. We review a body of work applying active inference models to the study of consciousness and argue that there is implicit in all these models a small set of theoretical commitments that point to a minimal (and testable) theory of consciousness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06633v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher J. Whyte, Andrew W. Corcoran, Jonathan Robinson, Ryan Smith, Rosalyn J. Moran, Thomas Parr, Karl J. Friston, Anil K. Seth, Jakob Hohwy</dc:creator>
    </item>
    <item>
      <title>Noninvasive precision modulation of high-level neural population activity via natural vision perturbations</title>
      <link>https://arxiv.org/abs/2506.05633</link>
      <description>arXiv:2506.05633v3 Announce Type: replace 
Abstract: Precise control of neural activity -- modulating target neurons deep in the brain while leaving nearby neurons unaffected -- is an outstanding challenge in neuroscience, generally approached using invasive techniques. This study investigates the possibility of precisely and noninvasively modulating neural activity in the high-level primate ventral visual stream via perturbations on one's natural visual feed. When tested on macaque inferior temporal (IT) neural populations, we found quantitative agreement between the model-predicted and biologically realized effect: strong modulation concentrated on targeted neural sites. We extended this to demonstrate accurate injection of experimenter-chosen neural population patterns via subtle perturbations applied on the background of typical natural visual feeds. These results highlight that current machine-executable models of the ventral stream can now design noninvasive, visually-delivered, possibly imperceptible neural interventions at the resolution of individual neurons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05633v3</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Gaziv, Sarah Goulding, Ani Ayvazian-Hancock, Yoon Bai, James J. DiCarlo</dc:creator>
    </item>
    <item>
      <title>Performance Modeling for Correlation-based Neural Decoding of Auditory Attention to Speech</title>
      <link>https://arxiv.org/abs/2503.09349</link>
      <description>arXiv:2503.09349v2 Announce Type: replace-cross 
Abstract: Correlation-based auditory attention decoding (AAD) algorithms exploit neural tracking mechanisms to determine listener attention among competing speech sources via, e.g., electroencephalography signals. The correlation coefficients between the decoded neural responses and encoded speech stimuli of the different speakers then serve as AAD decision variables. A critical trade-off exists between the temporal resolution (the decision window length used to compute these correlations) and the AAD accuracy. This trade-off is typically characterized by evaluating AAD accuracy across multiple window lengths, leading to the performance curve. We propose a novel method to model this trade-off curve using labeled correlations from only a single decision window length. Our approach models the (un)attended correlations with a normal distribution after applying the Fisher transformation, enabling accurate AAD accuracy prediction across different window lengths. We validate the method on two distinct AAD implementations: a linear decoder and the non-linear VLAAI deep neural network, evaluated on separate datasets. Results show consistently low modeling errors of approximately 2 percent points, with 94% of true accuracies falling within estimated 95%-confidence intervals. The proposed method enables efficient performance curve modeling without extensive multi-window length evaluation, facilitating practical applications in, e.g., performance tracking in neuro-steered hearing devices to continuously adapt the system parameters over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09349v2</guid>
      <category>eess.SP</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Geirnaert, Jonas Vanthornhout, Tom Francart, Alexander Bertrand</dc:creator>
    </item>
    <item>
      <title>Latent Structured Hopfield Network for Semantic Association and Retrieval</title>
      <link>https://arxiv.org/abs/2506.01303</link>
      <description>arXiv:2506.01303v2 Announce Type: replace-cross 
Abstract: Episodic memory enables humans to recall past experiences by associating semantic elements such as objects, locations, and time into coherent event representations. While large pretrained models have shown remarkable progress in modeling semantic memory, the mechanisms for forming associative structures that support episodic memory remain underexplored. Inspired by hippocampal CA3 dynamics and its role in associative memory, we propose the Latent Structured Hopfield Network (LSHN), a biologically inspired framework that integrates continuous Hopfield attractor dynamics into an autoencoder architecture. LSHN mimics the cortical-hippocampal pathway: a semantic encoder extracts compact latent representations, a latent Hopfield network performs associative refinement through attractor convergence, and a decoder reconstructs perceptual input. Unlike traditional Hopfield networks, our model is trained end-to-end with gradient descent, achieving scalable and robust memory retrieval. Experiments on MNIST, CIFAR-10, and a simulated episodic memory task demonstrate superior performance in recalling corrupted inputs under occlusion and noise, outperforming existing associative memory models. Our work provides a computational perspective on how semantic elements can be dynamically bound into episodic memory traces through biologically grounded attractor mechanisms. Code: https://github.com/fudan-birlab/LSHN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01303v2</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chong Li, Xiangyang Xue, Jianfeng Feng, Taiping Zeng</dc:creator>
    </item>
    <item>
      <title>Transient dynamics of associative memory models</title>
      <link>https://arxiv.org/abs/2506.05303</link>
      <description>arXiv:2506.05303v2 Announce Type: replace-cross 
Abstract: Associative memory models such as the Hopfield network and its dense generalizations with higher-order interactions exhibit a "blackout catastrophe"--a discontinuous transition where stable memory states abruptly vanish when the number of stored patterns exceeds a critical capacity. This transition is often interpreted as rendering networks unusable beyond capacity limits. We argue that this interpretation is largely an artifact of the equilibrium perspective. We derive dynamical mean-field equations using a bipartite cavity approach for graded-activity dense associative memory models, with the Hopfield model as a special case, and solve them using a numerical scheme. We show that patterns can be transiently retrieved with high accuracy above capacity despite the absence of stable attractors. This occurs because slow regions persist in the above-capacity energy landscape near stored patterns as lingering traces of the stable basins that existed below capacity. The same transient-retrieval effect occurs in below-capacity networks initialized outside basins of attraction. "Transient-recovery curves" provide a concise visual summary of these effects, revealing graceful, non-catastrophic changes in retrieval behavior above capacity and allowing us to compare the behavior across interaction orders. This dynamical perspective reveals rich energy landscape structure obscured by equilibrium analysis and suggests biological neural circuits may exploit transient dynamics for memory retrieval. Furthermore, our approach suggests ways of understanding computational properties of neural circuits without reference to fixed points, advances the technical repertoire of numerical mean-field solution methods for recurrent neural networks, and yields new theoretical results on generalizations of the Hopfield model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05303v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David G. Clark</dc:creator>
    </item>
  </channel>
</rss>
