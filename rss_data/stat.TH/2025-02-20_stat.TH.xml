<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Feb 2025 02:45:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Precise quantile function estimation from the characteristic function</title>
      <link>https://arxiv.org/abs/2502.13537</link>
      <description>arXiv:2502.13537v1 Announce Type: new 
Abstract: We provide theoretical error bounds for the accurate numerical computation of the quantile function given the characteristic function of a continuous random variable. We show theoretically and empirically that the numerical error of the quantile function is typically several orders of magnitude larger than the numerical error of the cumulative distribution function for probabilities close to zero or one. We introduce the COS method for computing the quantile function. This method converges exponentially when the density is smooth and has semi-heavy tails and all parameters necessary to tune the COS method are given explicitly. Finally, we numerically test our theoretical results on the normal-inverse Gaussian and the tempered stable distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13537v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gero Junike</dc:creator>
    </item>
    <item>
      <title>On noncentral Wishart mixtures of noncentral Wisharts and their use for testing random effects in factorial design models</title>
      <link>https://arxiv.org/abs/2502.13711</link>
      <description>arXiv:2502.13711v1 Announce Type: new 
Abstract: It is shown that a noncentral Wishart mixture of noncentral Wishart distributions with the same degrees of freedom yields a noncentral Wishart distribution, thereby extending the main result of Jones and Marchand [Stat 10 (2021), Paper No. e398, 7 pp.] from the chi-square to the Wishart setting. To illustrate its use, this fact is then employed to derive the finite-sample distribution of test statistics for random effects in a two-factor factorial design model with $d$-dimensional normal data, thereby broadening the findings of Bilodeau [ArXiv (2022), 6 pp.], who treated the case $d = 1$. The same approach makes it possible to test random effects in more general factorial design models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13711v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Genest, Anne MacKay, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Arbitrage-free catastrophe reinsurance valuation for compound dynamic contagion claims</title>
      <link>https://arxiv.org/abs/2502.13325</link>
      <description>arXiv:2502.13325v1 Announce Type: cross 
Abstract: In this paper, we consider catastrophe stop-loss reinsurance valuation for a reinsurance company with dynamic contagion claims. To deal with conventional and emerging catastrophic events, we propose the use of a compound dynamic contagion process for the catastrophic component of the liability. Under the premise that there is an absence of arbitrage opportunity in the market, we obtain arbitrage-free premiums for these contacts. To this end, the Esscher transform is adopted to specify an equivalent martingale probability measure. We show that reinsurers have various ways of levying the security loading on the net premiums to quantify the catastrophic liability in light of the growing challenges posed by emerging risks arising from climate change, cyberattacks, and pandemics. We numerically compare arbitrage-free catastrophe stop-loss reinsurance premiums via the Monte Carlo simulation method. Sensitivity analyzes are performed by changing the Esscher parameters and the retention level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13325v1</guid>
      <category>q-fin.RM</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiwook Jang, Patrick J. Laub, Tak Kuen Siu, Hongbiao Zhao</dc:creator>
    </item>
    <item>
      <title>Flow-based generative models as iterative algorithms in probability space</title>
      <link>https://arxiv.org/abs/2502.13394</link>
      <description>arXiv:2502.13394v1 Announce Type: cross 
Abstract: Generative AI (GenAI) has revolutionized data-driven modeling by enabling the synthesis of high-dimensional data across various applications, including image generation, language modeling, biomedical signal processing, and anomaly detection. Flow-based generative models provide a powerful framework for capturing complex probability distributions, offering exact likelihood estimation, efficient sampling, and deterministic transformations between distributions. These models leverage invertible mappings governed by Ordinary Differential Equations (ODEs), enabling precise density estimation and likelihood evaluation. This tutorial presents an intuitive mathematical framework for flow-based generative models, formulating them as neural network-based representations of continuous probability densities. We explore key theoretical principles, including the Wasserstein metric, gradient flows, and density evolution governed by ODEs, to establish convergence guarantees and bridge empirical advancements with theoretical insights. By providing a rigorous yet accessible treatment, we aim to equip researchers and practitioners with the necessary tools to effectively apply flow-based generative models in signal processing and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13394v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yao Xie, Xiuyuan Cheng</dc:creator>
    </item>
    <item>
      <title>Kernel Mean Embedding Topology: Weak and Strong Forms for Stochastic Kernels and Implications for Model Learning</title>
      <link>https://arxiv.org/abs/2502.13486</link>
      <description>arXiv:2502.13486v1 Announce Type: cross 
Abstract: We introduce a novel topology, called Kernel Mean Embedding Topology, for stochastic kernels, in a weak and strong form. This topology, defined on the spaces of Bochner integrable functions from a signal space to a space of probability measures endowed with a Hilbert space structure, allows for a versatile formulation. This construction allows one to obtain both a strong and weak formulation. (i) For its weak formulation, we highlight the utility on relaxed policy spaces, and investigate connections with the Young narrow topology and Borkar (or $w^*$)-topology, and establish equivalence properties. We report that, while both the $w^*$-topology and kernel mean embedding topology are relatively compact, they are not closed. Conversely, while the Young narrow topology is closed, it lacks relative compactness. (ii) We show that the strong form provides an appropriate formulation for placing topologies on spaces of models characterized by stochastic kernels with explicit robustness and learning theoretic implications on optimal stochastic control under discounted or average cost criteria. (iii) We show that this topology possesses several properties making it ideal to study optimality, approximations, robustness and continuity properties. In particular, the kernel mean embedding topology has a Hilbert space structure, which is particularly useful for approximating stochastic kernels through simulation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13486v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naci Saldi, Serdar Yuksel</dc:creator>
    </item>
    <item>
      <title>An Efficient Permutation-Based Kernel Two-Sample Test</title>
      <link>https://arxiv.org/abs/2502.13570</link>
      <description>arXiv:2502.13570v1 Announce Type: cross 
Abstract: Two-sample hypothesis testing-determining whether two sets of data are drawn from the same distribution-is a fundamental problem in statistics and machine learning with broad scientific applications. In the context of nonparametric testing, maximum mean discrepancy (MMD) has gained popularity as a test statistic due to its flexibility and strong theoretical foundations. However, its use in large-scale scenarios is plagued by high computational costs. In this work, we use a Nystr\"om approximation of the MMD to design a computationally efficient and practical testing algorithm while preserving statistical guarantees. Our main result is a finite-sample bound on the power of the proposed test for distributions that are sufficiently separated with respect to the MMD. The derived separation rate matches the known minimax optimal rate in this setting. We support our findings with a series of numerical experiments, emphasizing realistic scientific data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13570v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Chatalic, Marco Letizia, Nicolas Schreuder, and Lorenzo Rosasco</dc:creator>
    </item>
    <item>
      <title>Evaluability of paired comparison data in stochastic paired comparison models: Necessary and sufficient condition</title>
      <link>https://arxiv.org/abs/2502.13617</link>
      <description>arXiv:2502.13617v1 Announce Type: cross 
Abstract: In this paper, paired comparison models with stochastic background are investigated. We focus on the models that allow three options for choice. We estimate all parameters, the strength of the objects and the boundaries of equal decision, by maximum likelihood method. The existence and uniqueness of the estimator are key issues of the evaluation. Although a necessary and sufficient condition for the general case of three options has not been known until now, there are some different sufficient conditions that are formulated in the literature. In this paper, we provide a necessary and sufficient condition for the existence of a maximum and the uniqueness of the argument that maximizes the value, i.e. for the evaluability of the data in models of these types. By computer simulation, we present the efficiency of the condition, comparing it to the previously known sufficient conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13617v1</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>L\'aszl\'o Gyarmati, Csaba Mih\'alyk\'o, Eva Orb\'an-Mih\'alyk\'o, Andr\'as Mih\'alyk\'o</dc:creator>
    </item>
    <item>
      <title>Generalization error bound for denoising score matching under relaxed manifold assumption</title>
      <link>https://arxiv.org/abs/2502.13662</link>
      <description>arXiv:2502.13662v1 Announce Type: cross 
Abstract: We examine theoretical properties of the denoising score matching estimate. We model the density of observations with a nonparametric Gaussian mixture. We significantly relax the standard manifold assumption allowing the samples step away from the manifold. At the same time, we are still able to leverage a nice distribution structure. We derive non-asymptotic bounds on the approximation and generalization errors of the denoising score matching estimate. The rates of convergence are determined by the intrinsic dimension. Furthermore, our bounds remain valid even if we allow the ambient dimension grow polynomially with the sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13662v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantin Yakovlev, Nikita Puchkin</dc:creator>
    </item>
    <item>
      <title>Tight Generalization Bounds for Large-Margin Halfspaces</title>
      <link>https://arxiv.org/abs/2502.13692</link>
      <description>arXiv:2502.13692v1 Announce Type: cross 
Abstract: We prove the first generalization bound for large-margin halfspaces that is asymptotically tight in the tradeoff between the margin, the fraction of training points with the given margin, the failure probability and the number of training points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13692v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kasper Green Larsen, Natascha Schalburg</dc:creator>
    </item>
    <item>
      <title>Optimal Overlap Detection of Shotgun Reads</title>
      <link>https://arxiv.org/abs/2502.13813</link>
      <description>arXiv:2502.13813v1 Announce Type: cross 
Abstract: We consider the problem of detecting the overlap between a pair of short fragments sampled in random locations from an exponentially longer sequence, via their possibly noisy reads. We consider a noiseless setting, in which the reads are noiseless, and the sequence is only assumed to be stationary and ergodic. Under mild conditions on the mixing property of the process generating the sequence, we characterize exactly the asymptotic error probability of the optimal Bayesian detector. Similarly, we consider a noisy setting, in which the reads are noisy versions of the sampled fragments obtained via a memoryless channel. We further assume that the sequence is stationary and memoryless, and similarly characterize exactly the asymptotic error probability of the optimal Bayesian detector for this case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13813v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nir Luria, Nir Weinberger</dc:creator>
    </item>
    <item>
      <title>Continuous logistic Gaussian random measure fields for spatial distributional modelling</title>
      <link>https://arxiv.org/abs/2110.02876</link>
      <description>arXiv:2110.02876v3 Announce Type: replace 
Abstract: We study Spatial Logistic Gaussian Process (SLGP) models for non-parametric estimation of probability density fields using scattered samples of heterogeneous sizes. SLGPs are examined from the perspective of random measures and their densities, investigating the relationships between SLGPs and underlying processes. Our inquiries are motivated by SLGP's abilities in delivering probabilistic predictions of conditional distributions at candidate points, allowing conditional simulations of probability densities, and jointly predicting multiple functionals of target distributions. We demonstrate that SLGP models exhibit joint Gaussianity of their log-increments, enabling us to establish theoretical results regarding spatial regularity. Additionally, we extend the notion of mean-square continuity to random measure fields and establish sufficient conditions on covariance kernels underlying SLGPs to ensure these models enjoy such regularity properties. Finally, we propose an implementation using Random Fourier Features and showcase its applicability on synthetic examples and on temperature distributions at meteorological stations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.02876v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ath\'ena\"is Gautier, David Ginsbourger</dc:creator>
    </item>
    <item>
      <title>Negative Moment Bounds for Sample Autocovariance Matrices of Stationary Processes Driven by Conditional Heteroscedastic Errors and Their Applications</title>
      <link>https://arxiv.org/abs/2301.07476</link>
      <description>arXiv:2301.07476v4 Announce Type: replace 
Abstract: We establish a negative moment bound for the sample autocovariance matrix of a stationary process driven by conditional heteroscedastic errors. This moment bound enables us to asymptotically express the mean squared prediction error (MSPE) of the least squares predictor as the sum of three terms related to model complexity, model misspecification, and conditional heteroscedasticity. A direct application of this expression is the development of a model selection criterion that can asymptotically identify the best (in the sense of MSPE) subset AR model in the presence of misspecification and conditional heteroscedasticity. Finally, numerical simulations are conducted to confirm our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07476v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hsueh-Han Huang, Shu-Hui Yu, Ching-Kang Ing</dc:creator>
    </item>
    <item>
      <title>Estimation and inference for Deep Neuronal Networks</title>
      <link>https://arxiv.org/abs/2305.08193</link>
      <description>arXiv:2305.08193v3 Announce Type: replace 
Abstract: Nonlinear regression problem is one of the most popular and important statistical tasks. The first methods like least squares estimation go back to Gauss and Legendre. Recent models and developments in statistics and machine learning like Deep Neuronal Networks (DNN) or nonlinear PDE stimulate new research in this direction which has to address the important issues and challenges of modern statistical inference such as huge complexity and parameter dimension of the model, limited sample size, lack of convexity and identifiability, among many others. Classical results of nonparametric statistics in terms of rate of convergence do not really address the mentioned issues. This paper offers a general approach to studying a nonlinear regression problem based on the notion of effective dimension. First, a special case of models with stochastically linear structure (SLS) is studied. The results provide finite sample expansions for the loss of the penalized maximum likelihood estimation (MLE). The leading term of such expansions as well as the corresponding remainder are given via the effective dimension and the effective sample size. The obtained expansions can be used to obtain sharp risk bounds and for statistical inference. Despite generality, all the presented bounds are nearly sharp and the classical asymptotic results can be obtained as simple corollaries. Although the basic SLS assumptions are not fulfilled for nonlinear smooth regression, we explain how the stochastic linearity can be achieved by extending the parameter space. The obtained general results are specified to nonlinear smooth regression and to a DNN with one hidden layer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.08193v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Spokoiny</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Fisher Market Equilibrium and First-Price Pacing Equilibrium</title>
      <link>https://arxiv.org/abs/2402.02303</link>
      <description>arXiv:2402.02303v4 Announce Type: replace 
Abstract: The linear Fisher market (LFM) is a basic equilibrium model from economics, which also has applications in fair and efficient resource allocation. First-price pacing equilibrium (FPPE) is a model capturing budget-management mechanisms in first-price auctions. In certain practical settings such as advertising auctions, there is an interest in performing statistical inference over these models. A popular methodology for general statistical inference is the bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for the valid application of bootstrap procedures. In this paper, we introduce and devise several statistically valid bootstrap inference procedures for LFM and FPPE. The most challenging part is to bootstrap general FPPE, which reduces to bootstrapping constrained M-estimators, a largely unexplored problem. We devise a bootstrap procedure for FPPE under mild degeneracy conditions by using the powerful tool of epi-convergence theory. Experiments with synthetic and semi-real data verify our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02303v4</guid>
      <category>math.ST</category>
      <category>cs.GT</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luofeng Liao, Christian Kroer</dc:creator>
    </item>
    <item>
      <title>Cointegration with Occasionally Binding Constraints</title>
      <link>https://arxiv.org/abs/2211.09604</link>
      <description>arXiv:2211.09604v3 Announce Type: replace-cross 
Abstract: In the literature on nonlinear cointegration, a long-standing open problem relates to how a (nonlinear) vector autoregression, which provides a unified description of the short- and long-run dynamics of a vector of time series, can generate 'nonlinear cointegration' in the profound sense of those series sharing common nonlinear stochastic trends. We consider this problem in the setting of the censored and kinked structural VAR (CKSVAR), which provides a flexible yet tractable framework within which to model time series that are subject to threshold-type nonlinearities, such as those arising due to occasionally binding constraints, of which the zero lower bound (ZLB) on short-term nominal interest rates provides a leading example. We provide a complete characterisation of how common linear and nonlinear stochastic trends may be generated in this model, via unit roots and appropriate generalisations of the usual rank conditions, providing the first extension to date of the Granger-Johansen representation theorem to a nonlinearly cointegrated setting, and thereby giving the first successful treatment of the open problem. The limiting common trend processes include regulated, censored and kinked Brownian motions, none of which have previously appeared in the literature on cointegrated VARs. Our results and running examples illustrate that the CKSVAR is capable of supporting a far richer variety of long-run behaviour than is a linear VAR, in ways that may be particularly useful for the identification of structural parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.09604v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James A. Duffy, Sophocles Mavroeidis, Sam Wycherley</dc:creator>
    </item>
    <item>
      <title>Stationarity with Occasionally Binding Constraints</title>
      <link>https://arxiv.org/abs/2307.06190</link>
      <description>arXiv:2307.06190v2 Announce Type: replace-cross 
Abstract: This paper studies a class of multivariate threshold autoregressive models, known as censored and kinked structural vector autoregressions (CKSVAR), which are notably able to accommodate series that are subject to occasionally binding constraints. We develop a set of sufficient conditions for the processes generated by a CKSVAR to be stationary, ergodic, and weakly dependent. Our conditions relate directly to the stability of the deterministic part of the model, and are therefore less conservative than those typically available for general vector threshold autoregressive (VTAR) models. Though our criteria refer to quantities, such as refinements of the joint spectral radius, that cannot feasibly be computed exactly, they can be approximated numerically to a high degree of precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.06190v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James A. Duffy, Sophocles Mavroeidis, Sam Wycherley</dc:creator>
    </item>
    <item>
      <title>High-dimensional manifold of solutions in neural networks: insights from statistical physics</title>
      <link>https://arxiv.org/abs/2309.09240</link>
      <description>arXiv:2309.09240v2 Announce Type: replace-cross 
Abstract: In these pedagogic notes I review the statistical mechanics approach to neural networks, focusing on the paradigmatic example of the perceptron architecture with binary an continuous weights, in the classification setting. I will review the Gardner's approach based on replica method and the derivation of the SAT/UNSAT transition in the storage setting. Then, I discuss some recent works that unveiled how the zero training error configurations are geometrically arranged, and how this arrangement changes as the size of the training set increases. I also illustrate how different regions of solution space can be explored analytically and how the landscape in the vicinity of a solution can be characterized. I give evidence how, in binary weight models, algorithmic hardness is a consequence of the disappearance of a clustered region of solutions that extends to very large distances. Finally, I demonstrate how the study of linear mode connectivity between solutions can give insights into the average shape of the solution manifold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.09240v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Enrico M. Malatesta</dc:creator>
    </item>
    <item>
      <title>On estimation and order selection for multivariate extremes via clustering</title>
      <link>https://arxiv.org/abs/2406.14535</link>
      <description>arXiv:2406.14535v4 Announce Type: replace-cross 
Abstract: We investigate the estimation of multivariate extreme models with a discrete spectral measure using spherical clustering techniques. The primary contribution involves devising a method for selecting the order, that is, the number of clusters. The method consistently identifies the true order, i.e., the number of spectral atoms, and enjoys intuitive implementation in practice. Specifically, we introduce an extra penalty term to the well-known simplified average silhouette width, which penalizes small cluster sizes and small dissimilarities between cluster centers. Consequently, we provide a consistent method for determining the order of a max-linear factor model, where a typical information-based approach is not viable. Our second contribution is a large-deviation-type analysis for estimating the discrete spectral measure through clustering methods, which serves as an assessment of the convergence quality of clustering-based estimation for multivariate extremes. Additionally, as a third contribution, we discuss how estimating the discrete measure can lead to parameter estimations of heavy-tailed factor models. We also present simulations and real-data studies that demonstrate order selection and factor model estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14535v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiyuan Deng, He Tang, Shuyang Bai</dc:creator>
    </item>
  </channel>
</rss>
