<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Apr 2025 04:01:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Dynamic Discrete-Continuous Choice Models: Identification and Conditional Choice Probability Estimation</title>
      <link>https://arxiv.org/abs/2504.16630</link>
      <description>arXiv:2504.16630v1 Announce Type: new 
Abstract: This paper develops a general framework for dynamic models in which individuals simultaneously make both discrete and continuous choices. The framework incorporates a wide range of unobserved heterogeneity. I show that such models are nonparametrically identified. Based on constructive identification arguments, I build a novel two-step estimation method in the lineage of Hotz and Miller (1993) and Arcidiacono and Miller (2011) but extended to simultaneous discrete-continuous choice. In the first step, I recover the (type-dependent) optimal choices with an expectation-maximization algorithm and instrumental variable quantile regression. In the second step, I estimate the primitives of the model taking the estimated optimal choices as given. The method is especially attractive for complex dynamic models because it significantly reduces the computational burden associated with their estimation compared to alternative full solution methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16630v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Bruneel-Zupanc</dc:creator>
    </item>
    <item>
      <title>Evaluating Meta-Regression Techniques: A Simulation Study on Heterogeneity in Location and Time</title>
      <link>https://arxiv.org/abs/2504.16696</link>
      <description>arXiv:2504.16696v1 Announce Type: new 
Abstract: In this paper, we conduct a simulation study to evaluate conventional meta-regression approaches (study-level random, fixed, and mixed effects) against seven methodology specifications new to meta-regressions that control joint heterogeneity in location and time (including a new one that we introduce). We systematically vary heterogeneity levels to assess statistical power, estimator bias and model robustness for each methodology specification. This assessment focuses on three aspects: performance under joint heterogeneity in location and time, the effectiveness of our proposed settings incorporating location fixed effects and study-level fixed effects with a time trend, as well as guidelines for model selection. The results show that jointly modeling heterogeneity when heterogeneity is in both dimensions improves performance compared to modeling only one type of heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16696v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Habibnia, Jonathan Gendron</dc:creator>
    </item>
    <item>
      <title>MLOps Monitoring at Scale for Digital Platforms</title>
      <link>https://arxiv.org/abs/2504.16789</link>
      <description>arXiv:2504.16789v1 Announce Type: new 
Abstract: Machine learning models are widely recognized for their strong performance in forecasting. To keep that performance in streaming data settings, they have to be monitored and frequently re-trained. This can be done with machine learning operations (MLOps) techniques under supervision of an MLOps engineer. However, in digital platform settings where the number of data streams is typically large and unstable, standard monitoring becomes either suboptimal or too labor intensive for the MLOps engineer. As a consequence, companies often fall back on very simple worse performing ML models without monitoring. We solve this problem by adopting a design science approach and introducing a new monitoring framework, the Machine Learning Monitoring Agent (MLMA), that is designed to work at scale for any ML model with reasonable labor cost. A key feature of our framework concerns test-based automated re-training based on a data-adaptive reference loss batch. The MLOps engineer is kept in the loop via key metrics and also acts, pro-actively or retrospectively, to maintain performance of the ML model in the production stage. We conduct a large-scale test at a last-mile delivery platform to empirically validate our monitoring framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16789v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu Jeffrey Hu, Jeroen Rombouts, Ines Wilms</dc:creator>
    </item>
    <item>
      <title>Common Functional Decompositions Can Mis-attribute Differences in Outcomes Between Populations</title>
      <link>https://arxiv.org/abs/2504.16864</link>
      <description>arXiv:2504.16864v1 Announce Type: cross 
Abstract: In science and social science, we often wish to explain why an outcome is different in two populations. For instance, if a jobs program benefits members of one city more than another, is that due to differences in program participants (particular covariates) or the local labor markets (outcomes given covariates)? The Kitagawa-Oaxaca-Blinder (KOB) decomposition is a standard tool in econometrics that explains the difference in the mean outcome across two populations. However, the KOB decomposition assumes a linear relationship between covariates and outcomes, while the true relationship may be meaningfully nonlinear. Modern machine learning boasts a variety of nonlinear functional decompositions for the relationship between outcomes and covariates in one population. It seems natural to extend the KOB decomposition using these functional decompositions. We observe that a successful extension should not attribute the differences to covariates -- or, respectively, to outcomes given covariates -- if those are the same in the two populations. Unfortunately, we demonstrate that, even in simple examples, two common decompositions -- functional ANOVA and Accumulated Local Effects -- can attribute differences to outcomes given covariates, even when they are identical in two populations. We provide a characterization of when functional ANOVA misattributes, as well as a general property that any discrete decomposition must satisfy to avoid misattribution. We show that if the decomposition is independent of its input distribution, it does not misattribute. We further conjecture that misattribution arises in any reasonable additive decomposition that depends on the distribution of the covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16864v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Quintero, William T. Stephenson, Advik Shreekumar, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Shrinkage Methods for Treatment Choice</title>
      <link>https://arxiv.org/abs/2210.17063</link>
      <description>arXiv:2210.17063v3 Announce Type: replace 
Abstract: This study examines the problem of determining whether to treat individuals based on observed covariates. The most common decision rule is the conditional empirical success (CES) rule proposed by Manski (2004), which assigns individuals to treatments that yield the best experimental outcomes conditional on the observed covariates. Conversely, using shrinkage estimators, which shrink unbiased but noisy preliminary estimates toward the average of these estimates, is a common approach in statistical estimation problems because it is well-known that shrinkage estimators may have smaller mean squared errors than unshrunk estimators. Inspired by this idea, we propose a computationally tractable shrinkage rule that selects the shrinkage factor by minimizing an upper bound of the maximum regret. Then, we compare the maximum regret of the proposed shrinkage rule with those of the CES and pooling rules when the space of conditional average treatment effects (CATEs) is correctly specified or misspecified. Our theoretical results demonstrate that the shrinkage rule performs well in many cases and these findings are further supported by numerical experiments. Specifically, we show that the maximum regret of the shrinkage rule can be strictly smaller than those of the CES and pooling rules in certain cases when the space of CATEs is correctly specified. In addition, we find that the shrinkage rule is robust against misspecification of the space of CATEs. Finally, we apply our method to experimental data from the National Job Training Partnership Act Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.17063v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takuya Ishihara, Daisuke Kurisu</dc:creator>
    </item>
    <item>
      <title>Inference with Many Weak Instruments and Heterogeneity</title>
      <link>https://arxiv.org/abs/2408.11193</link>
      <description>arXiv:2408.11193v3 Announce Type: replace 
Abstract: This paper considers inference in a linear instrumental variable regression model with many potentially weak instruments, in the presence of heterogeneous treatment effects. I first show that existing test procedures, including those that are robust to either weak instruments or heterogeneous treatment effects, can be arbitrarily oversized. I propose a novel and valid test based on a score statistic and a ``leave-three-out" variance estimator. In the presence of heterogeneity and within the class of tests that are functions of the leave-one-out analog of a maximal invariant, this test is asymptotically the uniformly most powerful unbiased test. In two applications to judge and quarter-of-birth instruments, the proposed inference procedure also yields a bounded confidence set while some existing methods yield unbounded or empty confidence sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11193v3</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luther Yap</dc:creator>
    </item>
    <item>
      <title>Program Evaluation with Remotely Sensed Outcomes</title>
      <link>https://arxiv.org/abs/2411.10959</link>
      <description>arXiv:2411.10959v2 Announce Type: replace 
Abstract: Economists often estimate treatment effects in experiments using remotely sensed variables (RSVs), e.g. satellite images or mobile phone activity, in place of directly measured economic outcomes. A common practice is to use an observational sample to train a predictor of the economic outcome from the RSV, and then to use its predictions as the outcomes in the experiment. We show that this method is biased whenever the RSV is post-outcome, i.e. if variation in the economic outcome causes variation in the RSV. In program evaluation, changes in poverty or environmental quality cause changes in satellite images, but not vice versa. As our main result, we nonparametrically identify the treatment effect by formalizing the intuition that underlies common practice: the conditional distribution of the RSV given the outcome and treatment is stable across the samples.Based on our identifying formula, we find that the efficient representation of RSVs for causal inference requires three predictions rather than one. Valid inference does not require any rate conditions on RSV predictions, justifying the use of complex deep learning algorithms with unknown statistical properties. We re-analyze the effect of an anti-poverty program in India using satellite images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10959v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ashesh Rambachan, Rahul Singh, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Synthetic Controls for Experimental Design</title>
      <link>https://arxiv.org/abs/2108.02196</link>
      <description>arXiv:2108.02196v5 Announce Type: replace-cross 
Abstract: This article studies experimental design in settings where the experimental units are large aggregate entities (e.g., markets), and only one or a small number of units can be exposed to the treatment. In such settings, randomization of the treatment may result in treated and control groups with very different characteristics at baseline, inducing biases. We propose a variety of experimental non-randomized synthetic control designs (Abadie, Diamond and Hainmueller, 2010, Abadie and Gardeazabal, 2003) that select the units to be treated, as well as the untreated units to be used as a control group. Average potential outcomes are estimated as weighted averages of the outcomes of treated units for potential outcomes with treatment, and weighted averages the outcomes of control units for potential outcomes without treatment. We analyze the properties of estimators based on synthetic control designs and propose new inferential techniques. We show that in experimental settings with aggregate units, synthetic control designs can substantially reduce estimation biases in comparison to randomization of the treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.02196v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 24 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Abadie, Jinglong Zhao</dc:creator>
    </item>
  </channel>
</rss>
