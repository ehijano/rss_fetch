<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Feb 2026 02:45:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Neural Fields as World Models</title>
      <link>https://arxiv.org/abs/2602.18690</link>
      <description>arXiv:2602.18690v1 Announce Type: new 
Abstract: How does the brain predict physical outcomes while acting in the world? Machine learning world models compress visual input into latent spaces, discarding the spatial structure that characterizes sensory cortex. We propose isomorphic world models: architectures preserving sensory topology so that physics prediction becomes geometric propagation rather than abstract state transition. We implement this using neural fields with motor-gated channels, where activity evolves through local lateral connectivity and motor commands multiplicatively modulate specific populations. Three experiments support this approach: (1) local connectivity is sufficient to learn ballistic physics, with predictions traversing intermediate locations rather than "teleporting"; (2) policies trained entirely in imagination transfer to real physics at nearly twice the rate of latent-space alternatives; and (3) motor-gated channels spontaneously develop body-selective encoding through visuomotor prediction alone. These findings suggest intuitive physics and body schema may share a common origin in spatially structured neural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18690v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Nunley</dc:creator>
    </item>
    <item>
      <title>A Data-Driven Method to Map the Functional Organisation of Human Brain White Matter</title>
      <link>https://arxiv.org/abs/2602.18715</link>
      <description>arXiv:2602.18715v1 Announce Type: new 
Abstract: The white matter of the brain is organised into axonal bundles that support long-range neural communication. Although diffusion MRI (dMRI) enables detailed mapping of these pathways through tractography, how white matter pathways directly facilitate large-scale neural synchronisation remains poorly understood. We developed a data-driven framework that integrates dMRI and functional MRI (fMRI) to model the dynamic coupling supported by white matter tracks. Specifically, we employed track dynamic functional connectivity (Track-DFC) to characterise functional coupling of remote grey matter connected by individual white matter tracks. Using independent component analysis followed by k-medoids clustering, we derived functionally-coherent clusters of white matter tracks from the Human Connectome Project young adult cohort. When applied to the HCP ageing cohort, these clusters exhibited widespread age-related declines in both functional coupling strength and temporal variability. Importantly, specific clusters encompassing pathways linking control, default mode, attention, and visual systems significantly mediated the relationship between age and cognitive performance. Together, these findings depict the functional organisation of white matter tracks and provide a powerful tool to study brain ageing and cognitive decline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18715v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Sun, James M. Shine, Robert D. Sanders, Robin F. H. Cash, Sharon L. Naismith, Fernando Calamante, Jinglei Lv</dc:creator>
    </item>
    <item>
      <title>From Modules to Movement: Deconstructing the Modular Architecture of the Motor System</title>
      <link>https://arxiv.org/abs/2602.18787</link>
      <description>arXiv:2602.18787v1 Announce Type: new 
Abstract: Coordinating multi-articulated bodies to generate purposeful movement is a formidable computational challenge. Yet the human motor system performs this task robustly in dynamic, uncertain environments, despite noisy and delayed feedback, slow actuators, and strict energetic constraints. A central question is what organizational principles underlie this efficiency. One widely recognized principle of neural organization is modularity, which enables complex problems to be decomposed into simpler subproblems that specialized modules are optimized to solve. In this review, we argue that modularity is a fundamental organizing principle of the motor system. We first summarize evidence for brain modularity, ranging from classical lesion studies to contemporary graph-theoretical analyses. We next discuss the main factors underlying the emergence and evolutionary selection of modular architectures, highlighting the computational advantages they provide. We then review the major neuroanatomical modules that structure current descriptions of the motor system and compare three prominent computational frameworks of motor control$-$optimal feedback control theory, muscle synergy theory, and dynamical systems approaches$-$showing that all implicitly or explicitly rely on specialized computational modules. We conclude by contrasting the key strengths and limitations of existing frameworks and by proposing promising directions toward more comprehensive theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18787v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alessandro Salatiello</dc:creator>
    </item>
    <item>
      <title>CRCC: Contrast-Based Robust Cross-Subject and Cross-Site Representation Learning for EEG</title>
      <link>https://arxiv.org/abs/2602.19138</link>
      <description>arXiv:2602.19138v1 Announce Type: new 
Abstract: EEG-based neural decoding models often fail to generalize across acquisition sites due to structured, site-dependent biases implicitly exploited during training. We reformulate cross-site clinical EEG learning as a bias-factorized generalization problem, in which domain shifts arise from multiple interacting sources. We identify three fundamental bias factors and propose a general training framework that mitigates their influence through data standardization and representation-level constraints. We construct a standardized multi-site EEG benchmark for Major Depressive Disorder and introduce CRCC, a two-stage training paradigm combining encoder-decoder pretraining with joint fine-tuning via cross-subject/site contrastive learning and site-adversarial optimization. CRCC consistently outperforms state-of-the-art baselines and achieves a 10.7 percentage-point improvement in balanced accuracy under strict zero-shot site transfer, demonstrating robust generalization to unseen environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19138v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaobin Wong, Zhonghua Zhao, Haoran Guo, Zhengyi Liu, Yu Wu, Feng Yan, Zhiren Wang, Sen Song</dc:creator>
    </item>
    <item>
      <title>Fine-Pruning: A Biologically Inspired Algorithm for Personalization of Machine Learning Models</title>
      <link>https://arxiv.org/abs/2602.18507</link>
      <description>arXiv:2602.18507v1 Announce Type: cross 
Abstract: Neural networks have long strived to emulate the learning capabilities of the human brain. While deep neural networks (DNNs) draw inspiration from the brain in neuron design, their training methods diverge from biological foundations. Backpropagation, the primary training method for DNNs, requires substantial computational resources and fully labeled datasets, presenting major bottlenecks in development and application. This work demonstrates that by returning to biomimicry, specifically mimicking how the brain learns through pruning, we can solve various classical machine learning problems while utilizing orders of magnitude fewer computational resources and no labels. Our experiments successfully personalized multiple speech recognition and image classification models, including ResNet50 on ImageNet, resulting in increased sparsity of approximately 70\% while simultaneously improving model accuracy to around 90\%, all without the limitations of backpropagation. This biologically inspired approach offers a promising avenue for efficient, personalized machine learning models in resource-constrained environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18507v1</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.patter.2025.101242</arxiv:DOI>
      <arxiv:journal_reference>Patterns (New York, N.Y.), vol. 6, no. 5, Elsevier BV, May 2025, p. 101242</arxiv:journal_reference>
      <dc:creator>Joseph Bingham, Saman Zonouz, Dvir Aran</dc:creator>
    </item>
    <item>
      <title>Online decoding of rat self-paced locomotion speed from EEG using recurrent neural networks</title>
      <link>https://arxiv.org/abs/2602.18637</link>
      <description>arXiv:2602.18637v1 Announce Type: cross 
Abstract: $\textit{Objective.}$ Accurate neural decoding of locomotion holds promise for advancing rehabilitation, prosthetic control, and understanding neural correlates of action. Recent studies have demonstrated decoding of locomotion kinematics across species on motorized treadmills. However, efforts to decode locomotion speed in more natural contexts$-$where pace is self-selected rather than externally imposed$-$are scarce, generally achieve only modest accuracy, and require intracranial implants. Here, we aim to decode self-paced locomotion speed non-invasively and continuously using cortex-wide EEG recordings from rats. $\textit{Approach.}$ We introduce an asynchronous brain$-$computer interface (BCI) that processes a stream of 32-electrode skull-surface EEG (0.01$-$45 Hz) to decode instantaneous speed from a non-motorized treadmill during self-paced locomotion in head-fixed rats. Using recurrent neural networks and a dataset of over 133 h of recordings, we trained decoders to map ongoing EEG activity to treadmill speed. $\textit{Main results.}$ Our decoding achieves a correlation of 0.88 ($R^2$ = 0.78) for speed, primarily driven by visual cortex electrodes and low-frequency ($&lt; 8$ Hz) oscillations. Moreover, pre-training on a single session permitted decoding on other sessions from the same rat, suggesting uniform neural signatures that generalize across sessions but fail to transfer across animals. Finally, we found that cortical states not only carry information about current speed, but also about future and past dynamics, extending up to 1000 ms. $\textit{Significance.}$ These findings demonstrate that self-paced locomotion speed can be decoded accurately and continuously from non-invasive, cortex-wide EEG. Our approach provides a framework for developing high-performing, non-invasive BCI systems and contributes to understanding distributed neural representations of action dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18637v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro de Miguel, Nelson Totah, Uri Maoz</dc:creator>
    </item>
    <item>
      <title>Modularity is the Bedrock of Natural and Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2602.18960</link>
      <description>arXiv:2602.18960v1 Announce Type: cross 
Abstract: The remarkable performance of modern AI systems has been driven by unprecedented scales of data, computation, and energy -- far exceeding the resources required by human intelligence. This disparity highlights the need for new guiding principles and motivates drawing inspiration from the fundamental organizational principles of brain computation. Among these principles, modularity has been shown to be critical for supporting the efficient learning and strong generalization abilities consistently exhibited by humans. Furthermore, modularity aligns well with the No Free Lunch Theorem, which highlights the need for problem-specific inductive biases and motivates architectures composed of specialized components that solve subproblems. However, despite its fundamental role in natural intelligence and its demonstrated benefits across a range of seemingly disparate AI subfields, modularity remains relatively underappreciated in mainstream AI research. In this work, we review several research threads in artificial intelligence and neuroscience through a conceptual framework that highlights the central role of modularity in supporting both artificial and natural intelligence. In particular, we examine what computational advantages modularity provides, how it has emerged as a solution across several AI research areas, which modularity principles the brain exploits, and how modularity can help bridge the gap between natural and artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18960v1</guid>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>ICLR 2025 - Second Workshop on Representational Alignment (Re-Align) https://iclr.cc/virtual/2025/36838</arxiv:journal_reference>
      <dc:creator>Alessandro Salatiello</dc:creator>
    </item>
    <item>
      <title>Critical Scaling and Metabolic Regulation in a Ginzburg--Landau Theory of Cognitive Dynamics</title>
      <link>https://arxiv.org/abs/2602.19023</link>
      <description>arXiv:2602.19023v1 Announce Type: cross 
Abstract: We formulate a phenomenological effective field theory in which biological intelligence emerges as a macroscopic order parameter sustained by continuous metabolic flux. By modeling cognition as a coarse-grained neural activity field governed by a variational free energy, we derive closed-form expressions for information capacity and structural susceptibility using a Gaussian maximum entropy approximation. The theory predicts a universal algebraic divergence of the susceptibility, $\chi \sim K^{-3/2}$, as the structural stiffness $K$ approaches the instability threshold. The exponent $\gamma = 3/2$ is consistent with the mean-field branching process universality class, thereby providing a theoretical rationale for the observed avalanche size exponent $\tau \approx 3/2$ in cortical dynamics without invoking microscopic equivalence. We identify adult cognition as a metabolically pinned non-equilibrium steady state maintained near the critical regime $\Gamma \equiv K/\alpha \approx 1$ by continuous metabolic regulation, while pathological decline corresponds to a delocalization transition triggered by the violation of structural stability conditions. The framework generates concrete, falsifiable predictions for attention scaling, altered states of consciousness, and transcranial magnetic stimulation responses, each of which can be tested against existing neuroimaging and electrophysiological datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19023v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gunn Kim</dc:creator>
    </item>
    <item>
      <title>Functional Emotion Modeling in Biomimetic Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2507.11027</link>
      <description>arXiv:2507.11027v2 Announce Type: replace 
Abstract: We explore a functionalist approach to emotion by employing an ansatz -- an initial set of assumptions -- that a hypothetical concept generation model incorporates unproven but biologically plausible traits. From these traits, we mathematically construct a theoretical reinforcement learning framework grounded in functionalist principles and examine how the resulting utility function aligns with emotional valence in biological systems. Our focus is on structuring the functionalist perspective through a conceptual network, particularly emphasizing the construction of the utility function, not to provide an exhaustive explanation of emotions. The primary emphasis is not of planning or action execution, but such factors are addressed when pertinent. Finally, we apply the framework to psychological phenomena such as humor, psychopathy, and advertising, demonstrating its breadth of explanatory power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11027v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Wang</dc:creator>
    </item>
    <item>
      <title>Setting up for failure: automatic discovery of the neural mechanisms of cognitive errors</title>
      <link>https://arxiv.org/abs/2512.04808</link>
      <description>arXiv:2512.04808v2 Announce Type: replace 
Abstract: Discovering the neural mechanisms underpinning cognition is one of the grand challenges of neuroscience. However, previous approaches for building models of RNN dynamics that explain behaviour required iterative refinement of architectures and/or optimisation objectives, resulting in a piecemeal, and mostly heuristic, human-in-the-loop process. Here, we offer an alternative approach that automates the discovery of viable RNN mechanisms by explicitly training RNNs to reproduce behaviour, including the same characteristic errors and suboptimalities, that humans and animals produce in a cognitive task. Achieving this required two main innovations. First, as the amount of behavioural data that can be collected in experiments is often too limited to train RNNs, we use a non-parametric generative model of behavioural responses to produce surrogate data for training RNNs. Second, to capture all relevant statistical aspects of the data, we developed a novel diffusion model-based approach for training RNNs. To showcase the potential of our approach, we chose a visual working memory task as our test-bed, as behaviour in this task is well known to produce response distributions that are patently multimodal (due to swap errors). The resulting network dynamics correctly qualitative features of macaque neural data. Importantly, these results were not possible to obtain with more traditional approaches, i.e., when only a limited set of behavioural signatures (rather than the full richness of behavioural response distributions) were fitted, or when RNNs were trained for task optimality (instead of reproducing behaviour). Our approach also yields novel predictions about the mechanism of swap errors, which can be readily tested in experiments. These results suggest that fitting RNNs to rich patterns of behaviour provides a powerful way to automatically discover mechanisms of important cognitive functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04808v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Puria Radmard, Paul M. Bays, M\'at\'e Lengyel</dc:creator>
    </item>
    <item>
      <title>KINESIS: Motion Imitation for Human Musculoskeletal Locomotion</title>
      <link>https://arxiv.org/abs/2503.14637</link>
      <description>arXiv:2503.14637v2 Announce Type: replace-cross 
Abstract: How do humans move? Advances in reinforcement learning (RL) have produced impressive results in capturing human motion using physics-based humanoid control. However, torque-controlled humanoids fail to model key aspects of human motor control such as biomechanical joint constraints \&amp; non-linear and overactuated musculotendon control. We present KINESIS, a model-free motion imitation framework that tackles these challenges. KINESIS is trained on 1.8 hours of locomotion data and achieves strong motion imitation performance on unseen trajectories. Through a negative mining approach, KINESIS learns robust locomotion priors that we leverage to deploy the policy on several downstream tasks such as text-to-control, target point reaching, and football penalty kicks. Importantly, KINESIS learns to generate muscle activity patterns that correlate well with human EMG activity. We show that these results scale seamlessly across biomechanical model complexity, demonstrating control of up to 290 muscles. Overall, the physiological plausibility makes KINESIS a promising model for tackling challenging problems in human motor control. Code, videos and benchmarks are available at https://github.com/amathislab/Kinesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14637v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Merkourios Simos, Alberto Silvio Chiappa, Alexander Mathis</dc:creator>
    </item>
  </channel>
</rss>
