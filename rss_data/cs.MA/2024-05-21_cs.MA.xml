<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MA</link>
    <description>cs.MA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 May 2024 04:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions</title>
      <link>https://arxiv.org/abs/2405.11106</link>
      <description>arXiv:2405.11106v1 Announce Type: new 
Abstract: In recent years, Large Language Models (LLMs) have shown great abilities in various tasks, including question answering, arithmetic problem solving, and poem writing, among others. Although research on LLM-as-an-agent has shown that LLM can be applied to Reinforcement Learning (RL) and achieve decent results, the extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the RL frameworks of a single agent. To inspire more research on LLM-based MARL, in this letter, we survey the existing LLM-based single-agent and multi-agent RL frameworks and provide potential research directions for future research. In particular, we focus on the cooperative tasks of multiple agents with a common goal and communication among them. We also consider human-in/on-the-loop scenarios enabled by the language component in the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11106v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuanneng Sun, Songjun Huang, Dario Pompili</dc:creator>
    </item>
    <item>
      <title>Safe by Design Autonomous Driving Systems</title>
      <link>https://arxiv.org/abs/2405.11995</link>
      <description>arXiv:2405.11995v1 Announce Type: new 
Abstract: Developing safe autonomous driving systems is a major scientific and technical challenge. Existing AI-based end-to-end solutions do not offer the necessary safety guarantees, while traditional systems engineering approaches are defeated by the complexity of the problem. Currently, there is an increasing interest in hybrid design solutions, integrating machine learning components, when necessary, while using model-based components for goal management and planning.
  We study a method for building safe by design autonomous driving systems, based on the assumption that the capability to drive boils down to the coordinated execution of a given set of driving operations. The assumption is substantiated by a compositionality result considering that autopilots are dynamic systems receiving a small number of types of vistas as input, each vista defining a free space in its neighborhood. It is shown that safe driving for each type of vista in the corresponding free space, implies safe driving for any possible scenario under some easy-to-check conditions concerning the transition between vistas. The designed autopilot comprises distinct control policies one per type of vista, articulated in two consecutive phases. The first phase consists of carefully managing a potentially risky situation by virtually reducing speed, while the second phase consists of exiting the situation by accelerating.
  The autopilots designed use for their predictions simple functions characterizing the acceleration and deceleration capabilities of the vehicles. They cover the main driving operations, including entering a main road, overtaking, crossing intersections protected by traffic lights or signals, and driving on freeways. The results presented reinforce the case for hybrid solutions that incorporate mathematically elegant and robust decision methods that are safe by design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11995v1</guid>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marius Bozga, Joseph Sifakis</dc:creator>
    </item>
    <item>
      <title>Learning to connect in action: Measuring and understanding the emergence of boundary spanners in volatile times</title>
      <link>https://arxiv.org/abs/2405.11998</link>
      <description>arXiv:2405.11998v1 Announce Type: new 
Abstract: Collective intelligence of diverse groups is key for tackling many of today's grand challenges such as fostering resilience and climate adaptation. Information exchange across such diverse groups is crucial for collective intelligence, especially in volatile environments. To facilitate inter-group information exchange, Informational Boundary Spanners (IBSs) as pivotal information exchange 'hubs' are promising. However, the mechanisms that drive the emergence of IBSs remain poorly understood. To address this gap there is first a need for a method to identify and measure the emergence of IBSs. Second, an Agent-Based Modelling (ABM) framework is not available to systematically study mechanisms for the emergence of IBSs in volatile environments. Third, even though the ability to learn who provides high-quality information is thought to be essential to explain the emergence of IBSs, a rigorous test of this mechanism is missing. The learning mechanism is formalized using an ABM framework, with the model's outputs analyzed using the proposed IBS emergence measurement method. To illustrate both the method and the learning mechanism, we present a case study focused on information sharing in the volatile environment of a disaster. The study shows that learning constitutes a mechanism for the emergence of effective IBSs in (a) low-volatility environments characterised by low uncertainty and (b) in high-volatility environments characterised by rapid change if the number of inter-group connections is sufficient. With the method and model, this paper aims to lay the foundations for exploring mechanisms for the emergence of IBSs that facilitate inter-group information exchange. This article advances collective intelligence by providing the essential elements for measuring and understanding the emergence of IBSs and exploring the effect of learning on their emergence in volatile environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11998v1</guid>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vittorio Nespeca (Delft University of Technology), Tina Comes (Delft University of Technology), Frances Brazier (Delft University of Technology)</dc:creator>
    </item>
    <item>
      <title>High-Resolution Agent-Based Modeling of Campus Population Behaviors for Pandemic Response Planning</title>
      <link>https://arxiv.org/abs/2405.11414</link>
      <description>arXiv:2405.11414v1 Announce Type: cross 
Abstract: This paper reports a case study of an application of high-resolution agent-based modeling and simulation to pandemic response planning on a university campus. In the summer of 2020, we were tasked with a COVID-19 pandemic response project to create a detailed behavioral simulation model of the entire campus population at Binghamton University. We conceptualized this problem as an agent migration process on a multilayer transportation network, in which each layer represented a different transportation mode. As no direct data were available about people's behaviors on campus, we collected as much indirect information as possible to inform the agents' behavioral rules. Each agent was assumed to move along the shortest path between two locations within each transportation layer and switch layers at a parking lot or a bus stop, along with several other behavioral assumptions. Using this model, we conducted simulations of the whole campus population behaviors on a typical weekday, involving more than 25,000 agents. We measured the frequency of close social contacts at each spatial location and identified several busy locations and corridors on campus that needed substantial behavioral intervention. Moreover, systematic simulations with varying population density revealed that the effect of population density reduction was nonlinear, and that reducing the population density to 40-45% would be optimal and sufficient to suppress disease spreading on campus. These results were reported to the university administration and utilized in the pandemic response planning, which led to successful outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11414v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.MA</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroki Sayama, Shun Cao</dc:creator>
    </item>
    <item>
      <title>Configurable Mirror Descent: Towards a Unification of Decision Making</title>
      <link>https://arxiv.org/abs/2405.11746</link>
      <description>arXiv:2405.11746v1 Announce Type: cross 
Abstract: Decision-making problems, categorized as single-agent, e.g., Atari, cooperative multi-agent, e.g., Hanabi, competitive multi-agent, e.g., Hold'em poker, and mixed cooperative and competitive, e.g., football, are ubiquitous in the real world. Various methods are proposed to address the specific decision-making problems. Despite the successes in specific categories, these methods typically evolve independently and cannot generalize to other categories. Therefore, a fundamental question for decision-making is: \emph{Can we develop \textbf{a single algorithm} to tackle \textbf{ALL} categories of decision-making problems?} There are several main challenges to address this question: i) different decision-making categories involve different numbers of agents and different relationships between agents, ii) different categories have different solution concepts and evaluation measures, and iii) there lacks a comprehensive benchmark covering all the categories. This work presents a preliminary attempt to address the question with three main contributions. i) We propose the generalized mirror descent (GMD), a generalization of MD variants, which considers multiple historical policies and works with a broader class of Bregman divergences. ii) We propose the configurable mirror descent (CMD) where a meta-controller is introduced to dynamically adjust the hyper-parameters in GMD conditional on the evaluation measures. iii) We construct the \textsc{GameBench} with 15 academic-friendly games across different decision-making categories. Extensive experiments demonstrate that CMD achieves empirically competitive or better outcomes compared to baselines while providing the capability of exploring diverse dimensions of decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11746v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengdeng Li, Shuxin Li, Chang Yang, Xinrun Wang, Shuyue Hu, Xiao Huang, Hau Chan, Bo An</dc:creator>
    </item>
    <item>
      <title>Efficient Multi-agent Reinforcement Learning by Planning</title>
      <link>https://arxiv.org/abs/2405.11778</link>
      <description>arXiv:2405.11778v1 Announce Type: cross 
Abstract: Multi-agent reinforcement learning (MARL) algorithms have accomplished remarkable breakthroughs in solving large-scale decision-making tasks. Nonetheless, most existing MARL algorithms are model-free, limiting sample efficiency and hindering their applicability in more challenging scenarios. In contrast, model-based reinforcement learning (MBRL), particularly algorithms integrating planning, such as MuZero, has demonstrated superhuman performance with limited data in many tasks. Hence, we aim to boost the sample efficiency of MARL by adopting model-based approaches. However, incorporating planning and search methods into multi-agent systems poses significant challenges. The expansive action space of multi-agent systems often necessitates leveraging the nearly-independent property of agents to accelerate learning. To tackle this issue, we propose the MAZero algorithm, which combines a centralized model with Monte Carlo Tree Search (MCTS) for policy search. We design a novel network structure to facilitate distributed execution and parameter sharing. To enhance search efficiency in deterministic environments with sizable action spaces, we introduce two novel techniques: Optimistic Search Lambda (OS($\lambda$)) and Advantage-Weighted Policy Optimization (AWPO). Extensive experiments on the SMAC benchmark demonstrate that MAZero outperforms model-free approaches in terms of sample efficiency and provides comparable or better performance than existing model-based methods in terms of both sample and computational efficiency. Our code is available at https://github.com/liuqh16/MAZero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11778v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qihan Liu, Jianing Ye, Xiaoteng Ma, Jun Yang, Bin Liang, Chongjie Zhang</dc:creator>
    </item>
    <item>
      <title>Equilibria in multiagent online problems with predictions</title>
      <link>https://arxiv.org/abs/2405.11873</link>
      <description>arXiv:2405.11873v1 Announce Type: cross 
Abstract: We study the power of (competitive) algorithms with predictions in a multiagent setting. For this we introduce a multiagent version of the ski-rental problem. In this problem agents can collaborate by pooling resources to get a group license for some asset. If the license price is not met agents have to rent the asset individually for the day at a unit price. Otherwise the license becomes available forever to everyone at no extra cost. Our main contribution is a best-response analysis of a single-agent competitive algorithm that assumes perfect knowledge of other agents' actions (but no knowledge of its own renting time). We then analyze the setting when agents have a predictor for their own active time, yielding a tradeoff between robustness and consistency. We investigate the effect of using such a predictor in an equilibrium, as well as the new equilibria formed in this way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11873v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>cs.MA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Istrate, Cosmin Bonchi\c{s}, Victor Bogdan</dc:creator>
    </item>
    <item>
      <title>AgentScope: A Flexible yet Robust Multi-Agent Platform</title>
      <link>https://arxiv.org/abs/2402.14034</link>
      <description>arXiv:2402.14034v2 Announce Type: replace 
Abstract: With the rapid advancement of Large Language Models (LLMs), significant progress has been made in multi-agent applications. However, the complexities in coordinating agents' cooperation and LLMs' erratic performance pose notable challenges in developing robust and efficient multi-agent applications. To tackle these challenges, we propose AgentScope, a developer-centric multi-agent platform with message exchange as its core communication mechanism. The abundant syntactic tools, built-in agents and service functions, user-friendly interfaces for application demonstration and utility monitor, zero-code programming workstation, and automatic prompt tuning mechanism significantly lower the barriers to both development and deployment. Towards robust and flexible multi-agent application, AgentScope provides both built-in and customizable fault tolerance mechanisms. At the same time, it is also armed with system-level support for managing and utilizing multi-modal data, tools, and external knowledge. Additionally, we design an actor-based distribution framework, enabling easy conversion between local and distributed deployments and automatic parallel optimization without extra effort. With these features, AgentScope empowers developers to build applications that fully realize the potential of intelligent agents. We have released AgentScope at https://github.com/modelscope/agentscope, and hope AgentScope invites wider participation and innovation in this fast-moving field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14034v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhijian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang, Yuexiang Xie, Daoyuan Chen, Liuyi Yao, Hongyi Peng, Zeyu Zhang, Lin Zhu, Chen Cheng, Hongzhu Shi, Yaliang Li, Bolin Ding, Jingren Zhou</dc:creator>
    </item>
    <item>
      <title>Cooperative Task Execution in Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2403.04370</link>
      <description>arXiv:2403.04370v2 Announce Type: replace 
Abstract: We propose a multi-agent system that enables groups of agents to collaborate and work autonomously to execute tasks. Groups can work in a decentralized manner and can adapt to dynamic changes in the environment. Groups of agents solve assigned tasks by exploring the solution space cooperatively based on the highest reward first. The tasks have a dependency structure associated with them. We rigorously evaluated the performance of the system and the individual group performance using centralized and decentralized control approaches for task distribution. Based on the results, the centralized approach is more efficient for systems with a less-dependent system $G_{18}$ (a well-known program graph that contains $18$ nodes with few links), while the decentralized approach performs better for systems with a highly-dependent system $G_{40}$ (a program graph that contains $40$ highly interlinked nodes). We also evaluated task allocation to groups that do not have interdependence. Our findings reveal that there was significantly less difference in the number of tasks allocated to each group in a less-dependent system than in a highly-dependent one. The experimental results showed that a large number of small-size cooperative groups of agents unequivocally improved the system's performance compared to a small number of large-size cooperative groups of agents. Therefore, it is essential to identify the optimal group size for a system to enhance its performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04370v2</guid>
      <category>cs.MA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator> Karishma, Shrisha Rao</dc:creator>
    </item>
    <item>
      <title>Emergence of Social Norms in Generative Agent Societies: Principles and Architecture</title>
      <link>https://arxiv.org/abs/2403.08251</link>
      <description>arXiv:2403.08251v2 Announce Type: replace 
Abstract: Social norms play a crucial role in guiding agents towards understanding and adhering to standards of behavior, thus reducing social conflicts within multi-agent systems (MASs). However, current LLM-based (or generative) MASs lack the capability to be normative. In this paper, we propose a novel architecture, named CRSEC, to empower the emergence of social norms within generative MASs. Our architecture consists of four modules: Creation &amp; Representation, Spreading, Evaluation, and Compliance. This addresses several important aspects of the emergent processes all in one: (i) where social norms come from, (ii) how they are formally represented, (iii) how they spread through agents' communications and observations, (iv) how they are examined with a sanity check and synthesized in the long term, and (v) how they are incorporated into agents' planning and actions. Our experiments deployed in the Smallville sandbox game environment demonstrate the capability of our architecture to establish social norms and reduce social conflicts within generative MASs. The positive outcomes of our human evaluation, conducted with 30 evaluators, further affirm the effectiveness of our approach. Our project can be accessed via the following link: https://github.com/sxswz213/CRSEC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08251v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyue Ren, Zhiyao Cui, Ruiqi Song, Zhen Wang, Shuyue Hu</dc:creator>
    </item>
    <item>
      <title>Partial gathering of mobile agents in dynamic rings</title>
      <link>https://arxiv.org/abs/2212.03457</link>
      <description>arXiv:2212.03457v2 Announce Type: replace-cross 
Abstract: In this paper, we consider the partial gathering problem of mobile agents in synchronous dynamic bidirectional ring networks. When k agents are distributed in the network, the partial gathering problem requires, for a given positive integer g (&lt; k), that agents terminate in a configuration such that either at least g agents or no agent exists at each node. So far, the partial gathering problem has been considered in static graphs. In this paper, we start considering partial gathering in dynamic graphs. As a first step, we consider this problem in 1-interval connected rings, that is, one of the links in a ring may be missing at each time step. In such networks, focusing on the relationship between the values of k and g, we fully characterize the solvability of the partial gathering problem and analyze the move complexity of the proposed algorithms when the problem can be solved. First, we show that the g-partial gathering problem is unsolvable when k &lt;= 2g. Second, we show that the problem can be solved with O(n log g) time and the total number of O(gn log g) moves when 2g + 1 &lt;= k &lt;= 3g - 2. Third, we show that the problem can be solved with O(n) time and the total number of O(kn) moves when 3g - 1 &lt;= k &lt;= 8g - 4. Notice that since k = O(g) holds when 3g - 1 &lt;= k &lt;= 8g - 4, the move complexity O(kn) in this case can be represented also as O(gn). Finally, we show that the problem can be solved with O(n) time and the total number of O(gn) moves when k &gt;= 8g - 3. These results mean that the partial gathering problem can be solved also in dynamic rings when k &gt;= 2g + 1. In addition, agents require a total number of \Omega(gn) moves to solve the partial (resp., total) gathering problem. Thus, when k &gt;= 3g - 1, agents can solve the partial gathering problem with the asymptotically optimal total number of O(gn) moves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.03457v2</guid>
      <category>cs.CC</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Shibata, Yuichi Sudo, Junya Nakamura, Yonghwan Kim</dc:creator>
    </item>
    <item>
      <title>Global Rewards in Multi-Agent Deep Reinforcement Learning for Autonomous Mobility on Demand Systems</title>
      <link>https://arxiv.org/abs/2312.08884</link>
      <description>arXiv:2312.08884v2 Announce Type: replace-cross 
Abstract: We study vehicle dispatching in autonomous mobility on demand (AMoD) systems, where a central operator assigns vehicles to customer requests or rejects these with the aim of maximizing its total profit. Recent approaches use multi-agent deep reinforcement learning (MADRL) to realize scalable yet performant algorithms, but train agents based on local rewards, which distorts the reward signal with respect to the system-wide profit, leading to lower performance. We therefore propose a novel global-rewards-based MADRL algorithm for vehicle dispatching in AMoD systems, which resolves so far existing goal conflicts between the trained agents and the operator by assigning rewards to agents leveraging a counterfactual baseline. Our algorithm shows statistically significant improvements across various settings on real-world data compared to state-of-the-art MADRL algorithms with local rewards. We further provide a structural analysis which shows that the utilization of global rewards can improve implicit vehicle balancing and demand forecasting abilities. Our code is available at https://github.com/tumBAIS/GR-MADRL-AMoD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08884v2</guid>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heiko Hoppe, Tobias Enders, Quentin Cappart, Maximilian Schiffer</dc:creator>
    </item>
    <item>
      <title>No-Regret Learning for Stackelberg Equilibrium Computation in Newsvendor Pricing Games</title>
      <link>https://arxiv.org/abs/2404.00203</link>
      <description>arXiv:2404.00203v2 Announce Type: replace-cross 
Abstract: We introduce the application of online learning in a Stackelberg game pertaining to a system with two learning agents in a dyadic exchange network, consisting of a supplier and retailer, specifically where the parameters of the demand function are unknown. In this game, the supplier is the first-moving leader, and must determine the optimal wholesale price of the product. Subsequently, the retailer who is the follower, must determine both the optimal procurement amount and selling price of the product. In the perfect information setting, this is known as the classical price-setting Newsvendor problem, and we prove the existence of a unique Stackelberg equilibrium when extending this to a two-player pricing game. In the framework of online learning, the parameters of the reward function for both the follower and leader must be learned, under the assumption that the follower will best respond with optimism under uncertainty. A novel algorithm based on contextual linear bandits with a measurable uncertainty set is used to provide a confidence bound on the parameters of the stochastic demand. Consequently, optimal finite time regret bounds on the Stackelberg regret, along with convergence guarantees to an approximate Stackelberg equilibrium, are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00203v2</guid>
      <category>cs.CE</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Larkin Liu, Yuming Rong</dc:creator>
    </item>
  </channel>
</rss>
