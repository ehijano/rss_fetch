<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Aug 2025 04:01:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>riemtan, riemstats: R packages for Riemannian geometry techniques in the analysis of multiple samples of connectomes</title>
      <link>https://arxiv.org/abs/2508.01342</link>
      <description>arXiv:2508.01342v1 Announce Type: new 
Abstract: Symmetric positive definite (SPD) matrices arising from functional connectivity analysis of neuroimaging data can be endowed with a Riemannian geometric structure that standard methods fail to respect. While existing R packages provide some tools for SPD matrix analysis, they suffer from limitations in scalability, numerical stability, and metric flexibility that hinder their application to modern large-scale connectomics studies. We present riemtan, a comprehensive R package that addresses these challenges through a unified, high-level interface supporting multiple Riemannian metrics, efficient parallel computation, and seamless conversion between manifold, tangent, and vectorized representations. Building on riemtan's foundation, we also introduce riemstats, which implements advanced statistical methods including Fr\'echet ANOVA, Riemannian ANOVA with classic test statistics, and harmonization techniques for multi-site studies. The modular design facilitates integration with existing R workflows and provides an extensible framework for future methodological developments in manifold-valued data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01342v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Escobar-Velasquez</dc:creator>
    </item>
    <item>
      <title>A Dynamic, Context-Aware Framework for Risky Driving Prediction Using Naturalistic Data</title>
      <link>https://arxiv.org/abs/2508.00888</link>
      <description>arXiv:2508.00888v1 Announce Type: cross 
Abstract: Naturalistic driving studies offer a powerful means for observing and quantifying real-world driving behaviour. One of their prominent applications in traffic safety is the continuous monitoring and classification of risky driving behaviour. However, many existing frameworks rely on fixed time windows and static thresholds for distinguishing between safe and risky behaviour - limiting their ability to respond to the stochastic nature of real-world driving. This study proposes a dynamic and individualised framework for identifying risky driving behaviour using Belgian naturalistic driving data. The approach leverages a rolling time window and bi-level optimisation to dynamically calibrate both risk thresholds and model hyperparameters, capturing subtle behavioural shifts. Two safety indicators, speed-weighted headway and harsh driving events, were evaluated using three data-driven models: Random Forest, XGBoost, and Deep Neural Network (DNN). The DNN demonstrated strong capability in capturing subtle changes in driving behaviour, particularly excelling in high-recall tasks, making it promising for early-stage risk detection. XGBoost provided the most balanced and stable performance across different thresholds and evaluation metrics. While random forest showed more variability, it responded sensitively to dynamic threshold adjustments, which may be advantageous during model adaptation or tuning. Speed-weighted headway emerged as a more stable and context-sensitive risk indicator than harsh driving events, likely due to its robustness to label sparsity and contextual variation. Overall, the findings support the value of adaptive, personalised risk detection approaches for enhancing real-time safety feedback and tailoring driver support in intelligent transport systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00888v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Hossein Kalantari, Eleonora Papadimitriou, Amir Pooyan Afghari</dc:creator>
    </item>
    <item>
      <title>A strategy to avoid particle depletion in recursive Bayesian inference</title>
      <link>https://arxiv.org/abs/2508.01572</link>
      <description>arXiv:2508.01572v1 Announce Type: cross 
Abstract: Recursive Bayesian inference, in which posterior beliefs are updated in light of accumulating data, is a tool for implementing Bayesian models in applications with streaming and/or very large data sets. As the posterior of one iteration becomes the prior for the next, beliefs are updated sequentially instead of all-at-once. Thus, recursive inference is relevant for both streaming data and settings where data too numerous to be analyzed together can be partitioned into manageable pieces. In practice, posteriors are characterized by samples obtained using, e.g., acceptance/rejection sampling in which draws from the posterior of one iteration are used as proposals for the next. While simple to implement, such filtering approaches suffer from particle depletion, degrading each sample's ability to represent its target posterior. As a remedy, we investigate generating proposals from a smoothed version of the preceding sample's empirical distribution. The method retains computationally valuable properties of similar methods, but without particle depletion, and we demonstrate its accuracy in simulation. We apply the method to data simulated from both a simple, logistic regression model as well as a hierarchical model originally developed for classifying forest vegetation in New Mexico using satellite imagery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01572v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Henry R. Scharf</dc:creator>
    </item>
    <item>
      <title>Polymorphic Combinatorial Frameworks (PCF): Guiding the Design of Mathematically-Grounded, Adaptive AI Agents</title>
      <link>https://arxiv.org/abs/2508.01581</link>
      <description>arXiv:2508.01581v1 Announce Type: cross 
Abstract: The Polymorphic Combinatorial Framework (PCF) leverages Large Language Models (LLMs) and mathematical frameworks to guide the meta-prompt enabled design of solution spaces and adaptive AI agents for complex, dynamic environments. Unlike static agent architectures, PCF enables real-time parameter reconfiguration through mathematically-grounded combinatorial spaces, allowing agents to adapt their core behavioral traits dynamically. Grounded in combinatorial logic, topos theory, and rough fuzzy set theory, PCF defines a multidimensional SPARK parameter space (Skills, Personalities, Approaches, Resources, Knowledge) to capture agent behaviors. This paper demonstrates how LLMs can parameterize complex spaces and estimate likely parameter values/variabilities. Using PCF, we parameterized mock caf\'e domains (five levels of complexity), estimated variables/variabilities, and conducted over 1.25 million Monte Carlo simulations. The results revealed trends in agent adaptability and performance across the five complexity tiers, with diminishing returns at higher complexity levels highlighting thresholds for scalable designs. PCF enables the generation of optimized agent configurations for specific scenarios while maintaining logical consistency. This framework supports scalable, dynamic, explainable, and ethical AI applications in domains like customer service, healthcare, robotics, and collaborative systems, paving the way for adaptable and cooperative next-generation polymorphic agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01581v1</guid>
      <category>cs.AI</category>
      <category>math.CO</category>
      <category>stat.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Pearl, Matthew Murphy, James Intriligator</dc:creator>
    </item>
    <item>
      <title>Efficient optimization of expensive black-box simulators via marginal means, with application to neutrino detector design</title>
      <link>https://arxiv.org/abs/2508.01834</link>
      <description>arXiv:2508.01834v1 Announce Type: cross 
Abstract: With advances in scientific computing, computer experiments are increasingly used for optimizing complex systems. However, for modern applications, e.g., the optimization of nuclear physics detectors, each experiment run can require hundreds of CPU hours, making the optimization of its black-box simulator over a high-dimensional space a challenging task. Given limited runs at inputs $\mathbf{x}_1, \cdots, \mathbf{x}_n$, the best solution from these evaluated inputs can be far from optimal, particularly as dimensionality increases. Existing black-box methods, however, largely employ this ''pick-the-winner'' (PW) solution, which leads to mediocre optimization performance. To address this, we propose a new Black-box Optimization via Marginal Means (BOMM) approach. The key idea is a new estimator of a global optimizer $\mathbf{x}^*$ that leverages the so-called marginal mean functions, which can be efficiently inferred with limited runs in high dimensions. Unlike PW, this estimator can select solutions beyond evaluated inputs for improved optimization performance. Assuming the objective function follows a generalized additive model with unknown link function and under mild conditions, we prove that the BOMM estimator not only is consistent for optimization, but also has an optimization rate that tempers the ''curse-of-dimensionality'' faced by existing methods, thus enabling better performance as dimensionality increases. We present a practical framework for implementing BOMM using the transformed additive Gaussian process surrogate model. Finally, we demonstrate the effectiveness of BOMM in numerical experiments and an application on neutrino detector optimization in nuclear physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01834v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hwanwoo Kim, Simon Mak, Ann-Kathrin Schuetz, Alan Poon</dc:creator>
    </item>
    <item>
      <title>Fast Gaussian process inference by exact Mat\'ern kernel decomposition</title>
      <link>https://arxiv.org/abs/2508.01864</link>
      <description>arXiv:2508.01864v1 Announce Type: cross 
Abstract: To speed up Gaussian process inference, a number of fast kernel matrix-vector multiplication (MVM) approximation algorithms have been proposed over the years. In this paper, we establish an exact fast kernel MVM algorithm based on exact kernel decomposition into weighted empirical cumulative distribution functions, compatible with a class of kernels which includes multivariate Mat\'ern kernels with half-integer smoothness parameter. This algorithm uses a divide-and-conquer approach, during which sorting outputs are stored in a data structure. We also propose a new algorithm to take into account some linear fixed effects predictor function. Our numerical experiments confirm that our algorithm is very effective for low-dimensional Gaussian process inference problems with hundreds of thousands of data points. An implementation of our algorithm is available at https://gitlab.com/warin/fastgaussiankernelregression.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01864v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Langren\'e, Xavier Warin, Pierre Gruet</dc:creator>
    </item>
    <item>
      <title>Decision Theory For Large Scale Outlier Detection Using Aleatoric Uncertainty: With a Note on Bayesian FDR</title>
      <link>https://arxiv.org/abs/2508.01988</link>
      <description>arXiv:2508.01988v1 Announce Type: cross 
Abstract: Aleatoric and Epistemic uncertainty have achieved recent attention in the literature as different sources from which uncertainty can emerge in stochastic modeling. Epistemic being intrinsic or model based notions of uncertainty, and aleatoric being the uncertainty inherent in the data. We propose a novel decision theoretic framework for outlier detection in the context of aleatoric uncertainty; in the context of Bayesian modeling. The model incorporates bayesian false discovery rate control for multiplicty adjustment, and a new generalization of Bayesian FDR is introduced. The model is applied to simulations based on temporally fluctuating outlier detection where fixing thresholds often results in poor performance due to nonstationarity, and a case study is outlined on on a novel cybersecurity detection. Cyberthreat signals are highly nonstationary; giving a credible stress test of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01988v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ryan Warnick</dc:creator>
    </item>
    <item>
      <title>The ECME Algorithm Using Factor Analysis for DOA Estimation in Nonuniform Noise</title>
      <link>https://arxiv.org/abs/2508.02223</link>
      <description>arXiv:2508.02223v1 Announce Type: cross 
Abstract: Maximum likelihood factor analysis has been used for direction of arrival estimation in unknown nonuniform noise and some iterative approaches have been developed. In particular, the Factor Analysis for Anisotropic Noise (FAAN) method proposed by Stoica and Babu has excellent convergence properties. In this letter, the Expectation/Conditional Maximization Either (ECME) algorithm, an extension of the expectation-maximization algorithm, is designed, which has almost the same computational complexity at each iteration as the FAAN method. However, numerical results show that the ECME algorithm yields faster stable convergence and is computationally more efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02223v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyan Gong</dc:creator>
    </item>
    <item>
      <title>Posterior Sampling of Probabilistic Word Embeddings</title>
      <link>https://arxiv.org/abs/2508.02337</link>
      <description>arXiv:2508.02337v1 Announce Type: cross 
Abstract: Quantifying uncertainty in word embeddings is crucial for reliable inference from textual data. However, existing Bayesian methods such as Hamiltonian Monte Carlo (HMC) and mean-field variational inference (MFVI) are either computationally infeasible for large data or rely on restrictive assumptions.
  We propose a scalable Gibbs sampler using Polya-Gamma augmentation as well as Laplace approximation and compare them with MFVI and HMC for word embeddings. In addition, we address non-identifiability in word embeddings. Our Gibbs sampler and HMC correctly estimate uncertainties, while MFVI does not, and Laplace approximation only does so on large sample sizes, as expected. Applying the Gibbs sampler to the US Congress and the Movielens datasets, we demonstrate the feasibility on larger real data. Finally, as a result of having draws from the full posterior, we show that the posterior mean of word embeddings improves over maximum a posteriori (MAP) estimates in terms of hold-out likelihood, especially for smaller sampling sizes, further strengthening the need for posterior sampling of word embeddings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02337v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V\"ain\"o Yrj\"an\"ainen, Isac Bostr\"om, M{\aa}ns Magnusson, Johan Jonasson</dc:creator>
    </item>
    <item>
      <title>Exploring the generalizability of the optimal 0.234 acceptance rate in random-walk Metropolis and parallel tempering algorithms</title>
      <link>https://arxiv.org/abs/2408.06894</link>
      <description>arXiv:2408.06894v4 Announce Type: replace 
Abstract: For random-walk Metropolis (RWM) and parallel tempering (PT) algorithms, an asymptotic acceptance rate of around 0.234 is known to be optimal in certain high-dimensional limits. However, its practical relevance is uncertain due to restrictive derivation conditions. We synthesise previous theoretical advances in extending the 0.234 acceptance rate to more general settings, and demonstrate its applicability with a comprehensive empirical simulation study on examples examining how acceptance rates affect Expected Squared Jumping Distance (ESJD). Our experiments show the optimality of the 0.234 acceptance rate for RWM is surprisingly robust even in lower dimensions across various non-spherically symmetric proposal distributions, multimodal target distributions that may not have an i.i.d. product density, and curved Rosenbrock target distributions with nonlinear correlation structure. Parallel tempering experiments also show that the idealized 0.234 spacing of inverse temperatures may be approximately optimal for low dimensions and non i.i.d. product target densities, and that constructing an inverse temperature ladder with spacings given by a swap acceptance of 0.234 is a viable strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06894v4</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aidan Li, Liyan Wang, Tianye Dou, Jeffrey S. Rosenthal</dc:creator>
    </item>
    <item>
      <title>Collision-based Dynamics for Multi-Marginal Optimal Transport</title>
      <link>https://arxiv.org/abs/2412.16385</link>
      <description>arXiv:2412.16385v2 Announce Type: replace-cross 
Abstract: Inspired by the Boltzmann kinetics, we propose a collision-based dynamics with a Monte Carlo solution algorithm that approximates the solution of the multi-marginal optimal transport problem via randomized pairwise swapping of sample indices. The computational complexity and memory usage of the proposed method scale linearly with the number of samples, making it highly attractive for high-dimensional settings. In several examples, we demonstrate the efficiency of the proposed method compared to the state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16385v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Sadr, Hossein Gorji</dc:creator>
    </item>
    <item>
      <title>Scalable Subset Selection in Linear Mixed Models</title>
      <link>https://arxiv.org/abs/2506.20425</link>
      <description>arXiv:2506.20425v2 Announce Type: replace-cross 
Abstract: Linear mixed models (LMMs), which incorporate fixed and random effects, are key tools for analyzing heterogeneous data, such as in personalized medicine. Nowadays, this type of data is increasingly wide, sometimes containing thousands of candidate predictors, necessitating sparsity for prediction and interpretation. However, existing sparse learning methods for LMMs do not scale well beyond tens or hundreds of predictors, leaving a large gap compared with sparse methods for linear models, which ignore random effects. This paper closes the gap with a new $\ell_0$ regularized method for LMM subset selection that can run on datasets containing thousands of predictors in seconds to minutes. On the computational front, we develop a coordinate descent algorithm as our main workhorse and provide a guarantee of its convergence. We also develop a local search algorithm to help traverse the nonconvex optimization surface. Both algorithms readily extend to subset selection in generalized LMMs via a penalized quasi-likelihood approximation. On the statistical front, we provide a finite-sample bound on the Kullback-Leibler divergence of the new method. We then demonstrate its excellent performance in experiments involving synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20425v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Thompson, Matt P. Wand, Joanna J. J. Wang</dc:creator>
    </item>
  </channel>
</rss>
