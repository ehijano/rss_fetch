<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Nov 2025 05:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>PDA-LSTM: Knowledge-driven page data arrangement based on LSTM for LCM supression in QLC 3D NAND flash memories</title>
      <link>https://arxiv.org/abs/2511.00075</link>
      <description>arXiv:2511.00075v1 Announce Type: new 
Abstract: Quarter level cell (QLC) 3D NAND flash memory is emerging as the predominant storage solution in the era of artificial intelligence. QLC 3D NAND flash stores 4 bit per cell to expand the storage density, resulting in narrower read margins. Constrained to read margins, QLC always suffers from lateral charge migration (LCM), which caused by non-uniform charge density across adjacent memory cells. To suppress charge density gap between cells, there are some algorithm in form of intra-page data mapping such as WBVM, DVDS. However, we observe inter-page data arrangements also approach the suppression. Thus, we proposed an intelligent model PDA-LSTM to arrange intra-page data for LCM suppression, which is a physics-knowledge-driven neural network model. PDA-LSTM applies a long-short term memory (LSTM) neural network to compute a data arrangement probability matrix from input page data pattern. The arrangement is to minimize the global impacts derived from the LCM among wordlines. Since each page data can be arranged only once, we design a transformation from output matrix of LSTM network to non-repetitive sequence generation probability matrix to assist training process. The arranged data pattern can decrease the bit error rate (BER) during data retention. In addition, PDA-LSTM do not need extra flag bits to record data transport of 3D NAND flash compared with WBVM, DVDS. The experiment results show that the PDA-LSTM reduces the average BER by 80.4% compared with strategy without data arrangement, and by 18.4%, 15.2% compared respectively with WBVM and DVDS with code-length 64.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00075v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianhui Li, Weiya Wang, Qianqi Zhao, Tong Qu, Jing He, Xuhong Qiang, Jingwen Hou, Ke Chen, Bao Zhang, Qi Wang</dc:creator>
    </item>
    <item>
      <title>H-FA: A Hybrid Floating-Point and Logarithmic Approach to Hardware Accelerated FlashAttention</title>
      <link>https://arxiv.org/abs/2511.00295</link>
      <description>arXiv:2511.00295v1 Announce Type: new 
Abstract: Transformers have significantly advanced AI and machine learning through their powerful attention mechanism. However, computing attention on long sequences can become a computational bottleneck. FlashAttention mitigates this by fusing the softmax and matrix operations into a tiled computation pattern that decouples performance from sequence length. Though designed for GPUs, its simplicity also makes it well suited for direct hardware acceleration. To improve hardware implementation, we compute FlashAttention using a mixture of floating-point and fixed-point logarithm domain representations. Floating-point is used to compute attention scores from query and key matrices, while logarithmic computation simplifies the fused computation of softmax normalization and the multiplication with the value matrix. This transformation, called H-FA, replaces vector-wide floating-point multiplication and division operations by additions and subtractions implemented efficiently with fixed-point arithmetic in the logarithm domain. Exponential function evaluations are effectively omitted and fused with the rest operations, and the final result is directly returned to floating-point arithmetic without any additional hardware overhead. Hardware implementation results at 28nm demonstrate that H-FA achieves a 26.5% reduction in area and a 23.4% reduction in power, on average, compared to FlashAttention parallel hardware architectures built solely with floating-point datapaths, without hindering performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00295v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosmas Alexandridis, Giorgos Dimitrakopoulos</dc:creator>
    </item>
    <item>
      <title>Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits</title>
      <link>https://arxiv.org/abs/2511.00321</link>
      <description>arXiv:2511.00321v1 Announce Type: new 
Abstract: The expansion of context windows in large language models (LLMs) to multi-million tokens introduces severe memory and compute bottlenecks, particularly in managing the growing Key-Value (KV) cache. While Compute Express Link (CXL) enables non-eviction frameworks that offload the full KV-cache to scalable external memory, these frameworks still suffer from costly data transfers when recalling non-resident KV tokens to limited GPU memory as context lengths increase. This work proposes scalable Processing-Near-Memory (PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that coordinates memory and computation beyond GPU limits. Our design offloads token page selection to a PNM accelerator within CXL memory, eliminating costly recalls and enabling larger GPU batch sizes. We further introduce a hybrid parallelization strategy and a steady-token selection mechanism to enhance compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM system, our solution delivers consistent performance gains for LLMs with up to 405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV) and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x throughput improvement, up to 60x lower energy per token, and up to 7.3x better total cost efficiency than the baseline, demonstrating that CXL-enabled multi-PNM architectures can serve as a scalable backbone for future long-context LLM inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00321v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dowon Kim, MinJae Lee, Janghyeon Kim, HyuckSung Kwon, Hyeonggyu Jeong, Sang-Soo Park, Minyong Yoon, Si-Dong Roh, Yongsuk Kwon, Jinin So, Jungwook Choi</dc:creator>
    </item>
    <item>
      <title>Simulation-Driven Evaluation of Chiplet-Based Architectures Using VisualSim</title>
      <link>https://arxiv.org/abs/2511.01244</link>
      <description>arXiv:2511.01244v1 Announce Type: new 
Abstract: This paper focuses on the simulation of multi-die System-on-Chip (SoC) architectures using VisualSim, emphasiz- ing chiplet-based system modeling and performance analysis. Chiplet technology presents a promising alternative to traditional monolithic chips, which face increasing challenges in manufactur- ing costs, power efficiency, and performance scaling. By integrat- ing multiple small modular silicon units into a single package, chiplet-based architectures offer greater flexibility and scalability at a lower overall cost. In this study, we developed a detailed sim- ulation model of a chiplet-based system, incorporating multicore ARM processor clusters interconnected through a ARM CMN600 network-on-chip (NoC) for efficient communication [4], [7]. The simulation framework in VisualSim enables the evaluation of critical system metrics, including inter-chiplet communication latency, memory access efficiency, workload distribution, and the power-performance tradeoff under various workloads. Through simulation-driven insights, this research highlights key factors influencing chiplet system performance and provides a foundation for optimizing future chiplet-based semiconductor designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01244v1</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wajid Ali, Ayaz Akram, Deepak Shankar</dc:creator>
    </item>
    <item>
      <title>PEARL: Power- and Energy-Aware Multicore Intermittent Computing</title>
      <link>https://arxiv.org/abs/2511.00316</link>
      <description>arXiv:2511.00316v1 Announce Type: cross 
Abstract: Low-power multicore platforms are suitable for running data-intensive tasks in parallel, but they are highly inefficient for computing on intermittent power. In this work, we present PEARL (PowEr And eneRgy-aware MuLticore Intermittent Computing), a novel systems support that can make existing multicore microcontroller (MCU) platforms suitable for efficient intermittent computing. PEARL achieves this by leveraging only a three-threshold voltage tracking circuit and an external fast non-volatile memory, which multicore MCUs can smoothly interface. PEARL software runtime manages these components and performs energy- and power-aware adaptation of the multicore configuration to introduce minimal backup overheads and boost performance. Our evaluation shows that PEARL outperforms the state-of-the-art solutions by up to 30x and consumes up to 32x less energy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00316v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Khakim Akhunov, Eren Yildiz, Kasim Sinan Yildirim</dc:creator>
    </item>
    <item>
      <title>FeNN-DMA: A RISC-V SoC for SNN acceleration</title>
      <link>https://arxiv.org/abs/2511.00732</link>
      <description>arXiv:2511.00732v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) are a promising, energy-efficient alternative to standard Artificial Neural Networks (ANNs) and are particularly well-suited to spatio-temporal tasks such as keyword spotting and video classification. However, SNNs have a much lower arithmetic intensity than ANNs and are therefore not well-matched to standard accelerators like GPUs and TPUs. Field Programmable Gate Arrays(FPGAs) are designed for such memory-bound workloads and here we develop a novel, fully-programmable RISC-V-based system-on-chip (FeNN-DMA), tailored to simulating SNNs on modern UltraScale+ FPGAs. We show that FeNN-DMA has comparable resource usage and energy requirements to state-of-the-art fixed-function SNN accelerators, yet it is capable of simulating much larger and more complex models. Using this functionality, we demonstrate state-of-the-art classification accuracy on the Spiking Heidelberg Digits and Neuromorphic MNIST tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00732v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zainab Aizaz, James C. Knight, Thomas Nowotny</dc:creator>
    </item>
    <item>
      <title>Per-Row Activation Counting on Real Hardware: Demystifying Performance Overheads</title>
      <link>https://arxiv.org/abs/2507.05556</link>
      <description>arXiv:2507.05556v2 Announce Type: replace 
Abstract: Per-Row Activation Counting (PRAC), a DRAM read disturbance mitigation method, modifies key DRAM timing parameters, reportedly causing significant performance overheads in simulator-based studies. However, given known discrepancies between simulators and real hardware, real-machine experiments are vital for accurate PRAC performance estimation. We present the first real-machine performance analysis of PRAC. After verifying timing modifications on the latest CPUs using microbenchmarks, our analysis shows that PRAC's average and maximum overheads are just 1.06% and 3.28% for the SPEC CPU2017 workloads -- up to 9.15x lower than simulator-based reports. Further, we show that the close page policy minimizes this overhead by effectively hiding the elongated DRAM row precharge operations due to PRAC from the critical path.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05556v2</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LCA.2025.3587293</arxiv:DOI>
      <dc:creator>Jumin Kim, Seungmin Baek, Minbok Wi, Hwayong Nam, Michael Jaemin Kim, Sukhan Lee, Kyomin Sohn, Jung Ho Ahn</dc:creator>
    </item>
    <item>
      <title>Autocomp: A Powerful and Portable Code Optimizer for Tensor Accelerators</title>
      <link>https://arxiv.org/abs/2505.18574</link>
      <description>arXiv:2505.18574v4 Announce Type: replace-cross 
Abstract: Hardware accelerators, especially those designed for tensor processing, have become ubiquitous in today's computing landscape. However, even with significant efforts in building compilers, programming these tensor accelerators remains challenging, leaving much of their potential underutilized. Recently, large language models (LLMs), trained on large amounts of code, have shown significant promise in code generation and optimization tasks, but generating low-resource languages, such as specialized tensor accelerator code still poses a significant challenge. We tackle this challenge with Autocomp, an approach that empowers accelerator programmers to leverage domain knowledge and hardware feedback to optimize code via an automated LLM-driven search. We accomplish this by: 1) formulating each optimization pass as a structured two-phase prompt, divided into planning and code generation phases, 2) inserting domain knowledge during planning via a concise and adaptable optimization menu, and 3) integrating correctness and performance metrics from hardware as feedback at each search iteration. Across three distinct hardware platforms, we demonstrate that Autocomp-optimized code runs 5.6x faster than the vendor-provided library (Gemmini), outperforms expert-level hand-tuned code by 1.9x (AWS Trainium), and achieves 3.8x higher performance than a machine learning-based cost model for GPUs (NVIDIA L40S). Additionally, we demonstrate that optimization schedules generated from Autocomp can be reused across similar tensor operations, improving speedups by up to 24% under a fixed sample budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18574v4</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Hong, Sahil Bhatia, Alvin Cheung, Yakun Sophia Shao</dc:creator>
    </item>
    <item>
      <title>HEEPidermis: a versatile SoC for BioZ recording</title>
      <link>https://arxiv.org/abs/2509.04528</link>
      <description>arXiv:2509.04528v2 Announce Type: replace-cross 
Abstract: Biological impedance (BioZ) is an information-packed modality that allows for non-invasive monitoring of health and emotional state. Currently, most research involving tissue impedance is based on bulky or fixed-purpose hardware, which limits the scope of research and the possibilities of experiments. In this work, we present HEEPidermis: a System-on-Chip (SoC) which integrates all the blocks needed for tissue impedance measurement, including two 8-bit, arbitrary-signal current DACs, two VCO-based ADCs, and a RISC-V CPU to enable on-chip feature extraction for closed-loop operation. An event-based sub-sampler improves storage and energy efficiency for long-term recording. In addition to the versatile SoC, the digital back-end and behavioral models of the analog front-end are open-source, allowing fast system-level simulations or repurposing. The SoC was taped out on TSMC 65 nm LP process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04528v2</guid>
      <category>physics.ins-det</category>
      <category>cs.AR</category>
      <category>physics.med-ph</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sapriza, Beatrice Grassano, Alessio Naclerio, Filippo Quadri, Tommaso Terzano, David Mallas\'en, Davide Schiavone, Robin Leplae, J\'er\'emie Moullet, Alexandre Levisse, Christoph M\"uller, Mariagrazia Graziano, Mat\'ias Miguez, David Atienza</dc:creator>
    </item>
  </channel>
</rss>
