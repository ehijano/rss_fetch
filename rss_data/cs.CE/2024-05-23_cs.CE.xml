<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2024 04:01:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Kinematics of Abdominal Aortic Aneurysms</title>
      <link>https://arxiv.org/abs/2405.13377</link>
      <description>arXiv:2405.13377v1 Announce Type: new 
Abstract: A search in Scopus within "Article title, Abstract, Keywords" unveils 2,444 documents focused on the biomechanics of Abdominal Aortic Aneurysm (AAA), mostly on AAA wall stress. Only 24 documents investigated AAA kinematics, an important topic that could potentially offer insights into the biomechanics of AAA. In this paper, we present an image-based approach for patient-specific, in vivo, and non-invasive AAA kinematic analysis using patient's time-resolved 3D computed tomography angiography (4D CTA) images. Our approach relies on regularized deformable image registration for estimating wall displacement, estimation of the local wall strain as the ratio of its normal displacement to its local radius of curvature, and local surface fitting with non-deterministic outlier detection for estimating the wall radius of curvature. We verified our approach against synthetic ground truth image data created by warping a 3D CTA image of AAA using a realistic displacement field obtained from a finite element biomechanical model. We applied our approach to assess AAA wall displacements and strains in ten patients. Our kinematic analysis results indicated that the 99th percentile of circumferential wall strain, among all patients, ranged from 3.16% to 7.31%, with an average of 5.36% and a standard deviation of 1.28%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13377v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mostafa Jamshidian, Adam Wittek, Saeideh Sekhavat, Karol Miller</dc:creator>
    </item>
    <item>
      <title>Elastic-gap free strain gradient crystal plasticity model that effectively account for plastic slip gradient and grain boundary dissipation</title>
      <link>https://arxiv.org/abs/2405.13384</link>
      <description>arXiv:2405.13384v1 Announce Type: new 
Abstract: This paper proposes an elastic-gap free strain gradient crystal plasticity model that addresses dissipation caused by plastic slip gradient and grain boundary (GB) Burger tensor. The model involves splitting plastic slip gradient and GB Burger tensor into energetic dissipative quantities. Unlike conventional models, the bulk and GB defect energy are considered to be a quadratic functional of the energetic portion of slip gradient and GB Burgers tensor. The higher-order stresses for each individual slip systems and GB stresses are derived from the defect energy, following a similar evolution as the Armstrong-Frederick type backstress model in classical plasticity. The evolution equations consist of a hardening and a relaxation term. The relaxation term brings the nonlinearity in hardening and causes an additional dissipation. The applicability of the proposed model is numerically established with the help of two-dimensional finite element implementation. Specifically, the bulk and GB relaxation coefficients are critically evaluated based on various circumstances, considering single crystal infinite shear layer, periodic bicrystal shearing, and bicrystal tension problem. In contrast to the Gurtin-type model, the proposed model smoothly captures the apparent strengthening at saturation without causing any abrupt stress jump under non-proportional loading conditions. Moreover, when subjected to cyclic loading, the stress-strain curve maintains its curvature during reverse loading. The numerical simulation reveals that the movement of geometrically necessary dislocation (GND) towards the GB is influenced by the bulk recovery coefficient, while the dissipation and amount of accumulation of GND near the GB are controlled by the GB recovery coefficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13384v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anjan Mukherjee, Biswanath Banerjee</dc:creator>
    </item>
    <item>
      <title>Identification of microstructure from macroscopic measurement using inverse multiscale analysis</title>
      <link>https://arxiv.org/abs/2405.13559</link>
      <description>arXiv:2405.13559v1 Announce Type: new 
Abstract: Most of the tailored materials are heterogeneous at the ingredient level. Analysis of those heterogeneous structures requires the knowledge of microstructure. With the knowledge of microstructure, multiscale analysis is carried out with homogenization at the micro level. Second-order homogenization is carried out whenever the ingredient size is comparable to the structure size. Therefore, knowledge of microstructure and its size is indispensable to analyzing those heterogeneous structures. Again, any structural response contains all the information of microstructure, like microstructure distribution, volume fraction, size of ingredients, etc. Here, inverse analysis is carried out to identify a heterogeneous microstructure from macroscopic measurement. Two-step inverse analysis is carried out in the identification process; in the first step, the macrostructures length scale and effective properties are identified from the macroscopic measurement using gradient-based optimization. In the second step, those effective properties and length scales are used to determine the microstructure in inverse second-order homogenization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13559v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anjan Mukherjee, Biswanth Banerjee</dc:creator>
    </item>
    <item>
      <title>Fully automated construction of three-dimensional finite element simulations from Optical Coherence Tomography</title>
      <link>https://arxiv.org/abs/2405.13643</link>
      <description>arXiv:2405.13643v1 Announce Type: new 
Abstract: Despite recent advances in diagnosis and treatment, atherosclerotic coronary artery diseases remain a leading cause of death worldwide. Various imaging modalities and metrics can detect lesions and predict patients at risk; however, identifying unstable lesions is still difficult. Current techniques cannot fully capture the complex morphology-modulated mechanical responses that affect plaque stability, leading to catastrophic failure and mute the benefit of device and drug interventions. Finite Element (FE) simulations utilizing intravascular imaging OCT (Optical Coherence Tomography) are effective in defining physiological stress distributions. However, creating 3D FE simulations of coronary arteries from OCT images is challenging to fully automate given OCT frame sparsity, limited material contrast, and restricted penetration depth. To address such limitations, we developed an algorithmic approach to automatically produce 3D FE-ready digital twins from labeled OCT images. The 3D models are anatomically faithful and recapitulate mechanically relevant tissue lesion components, automatically producing morphologies structurally similar to manually constructed models whilst including more minute details. A mesh convergence study highlighted the ability to reach stress and strain convergence with average errors of just 5.9% and 1.6% respectively in comparison to FE models with approximately twice the number of elements in areas of refinement. Such an automated procedure will enable analysis of large clinical cohorts at a previously unattainable scale and opens the possibility for in-silico methods for patient specific diagnoses and treatment planning for coronary artery disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13643v1</guid>
      <category>cs.CE</category>
      <category>q-bio.TO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.compbiomed.2023.107341</arxiv:DOI>
      <arxiv:journal_reference>Comp. Bio. Med. Volume 165, October 2023, 107341</arxiv:journal_reference>
      <dc:creator>Ross Straughan, Karim Kadry, Sahil A. Parikh, Elazer R. Edelman, Farhad R. Nezami</dc:creator>
    </item>
    <item>
      <title>Enhancing Bayesian model updating in structural health monitoring via learnable mappings</title>
      <link>https://arxiv.org/abs/2405.13648</link>
      <description>arXiv:2405.13648v1 Announce Type: new 
Abstract: In the context of structural health monitoring (SHM), the selection and extraction of damage-sensitive features from raw sensor recordings represent a critical step towards solving the inverse problem underlying the structural health identification. This work introduces a new way to enhance stochastic approaches to SHM through the use of deep neural networks. A learnable feature extractor and a feature-oriented surrogate model are synergistically exploited to evaluate a likelihood function within a Markov chain Monte Carlo sampling algorithm. The feature extractor undergoes a supervised pairwise training to map sensor recordings onto a low-dimensional metric space, which encapsulates the sensitivity to structural health parameters. The surrogate model maps the structural health parameters onto their feature description. The procedure enables the updating of beliefs about structural health parameters, effectively replacing the need for a computationally expensive numerical (finite element) model. A preliminary offline phase involves the generation of a labeled dataset to train both the feature extractor and the surrogate model. Within a simulation-based SHM framework, training vibration responses are cost-effectively generated by means of a multi-fidelity surrogate modeling strategy to approximate sensor recordings under varying damage and operational conditions. The multi-fidelity surrogate exploits model order reduction and artificial neural networks to speed up the data generation phase while ensuring the damage-sensitivity of the approximated signals. The proposed strategy is assessed through three synthetic case studies, demonstrating remarkable results in terms of accuracy of the estimated quantities and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13648v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Torzoni, Andrea Manzoni, Stefano Mariani</dc:creator>
    </item>
    <item>
      <title>Application of Internet of Energy in Smart Grids Using Deep Reinforcement Learning and Convolutional Neural Network</title>
      <link>https://arxiv.org/abs/2405.13831</link>
      <description>arXiv:2405.13831v1 Announce Type: new 
Abstract: The increasing demand for electricity, coupled with the rise in greenhouse gas emissions, necessitates the integration of Renewable Energy Sources (RESs) into power grids. However, the fluctuating nature of RESs introduces new challenges in energy management. The Internet of Energy (IoE) framework provides a solution by enabling real-time monitoring, dynamic scheduling, and enhanced energy routing. This paper proposes a comprehensive approach to optimizing energy management in smart grids using Deep Reinforcement Learning (DRL) and Convolutional Neural Networks (CNN). The research focuses on three main objectives: optimizing operation scheduling, improving energy routing, and enhancing cyber-physical security. A DRL-based scheduling algorithm is developed to manage energy components effectively, while an optimized energy routing algorithm ensures efficient electricity flow. Additionally, a security framework utilizing Long Short-Term Memory (LSTM) and CNN is proposed to detect False Data Injection (FDI) attacks and electricity theft. The proposed methods aim to improve energy efficiency, reduce costs, and ensure the security of IoE-enabled power systems. This research bridges existing gaps by addressing the dynamic and complex nature of modern energy networks. The integration of these advanced technologies promises significant advancements in the reliability and efficiency of smart grids. Ultimately, this work contributes to the development of a sustainable and secure energy future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13831v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Mohammadi Ruzbahani</dc:creator>
    </item>
    <item>
      <title>Calibration of stochastic, agent-based neuron growth models with Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2405.13905</link>
      <description>arXiv:2405.13905v1 Announce Type: new 
Abstract: Understanding how genetically encoded rules drive and guide complex neuronal growth processes is essential to comprehending the brain's architecture, and agent-based models (ABMs) offer a powerful simulation approach to further develop this understanding. However, accurately calibrating these models remains a challenge. Here, we present a novel application of Approximate Bayesian Computation (ABC) to address this issue. ABMs are based on parametrized stochastic rules that describe the time evolution of small components -- the so-called agents -- discretizing the system, leading to stochastic simulations that require appropriate treatment. Mathematically, the calibration defines a stochastic inverse problem. We propose to address it in a Bayesian setting using ABC. We facilitate the repeated comparison between data and simulations by quantifying the morphological information of single neurons with so-called morphometrics and resort to statistical distances to measure discrepancies between populations thereof. We conduct experiments on synthetic as well as experimental data. We find that ABC utilizing Sequential Monte Carlo sampling and the Wasserstein distance finds accurate posterior parameter distributions for representative ABMs. We further demonstrate that these ABMs capture specific features of pyramidal cells of the hippocampus (CA1). Overall, this work establishes a robust framework for calibrating agent-based neuronal growth models and opens the door for future investigations using Bayesian techniques for model building, verification, and adequacy assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13905v1</guid>
      <category>cs.CE</category>
      <category>physics.bio-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Duswald, Lukas Breitwieser, Thomas Thorne, Barbara Wohlmuth, Roman Bauer</dc:creator>
    </item>
    <item>
      <title>A Methodology to Identify Physical or Computational Experiment Conditions for Uncertainty Mitigation</title>
      <link>https://arxiv.org/abs/2405.13931</link>
      <description>arXiv:2405.13931v1 Announce Type: new 
Abstract: Complex engineering systems require integration of simulation of sub-systems and calculation of metrics to drive design decisions. This paper introduces a methodology for designing computational or physical experiments for system-level uncertainty mitigation purposes. The methodology follows a previously determined problem ontology, where physical, functional and modeling architectures are decided upon. By carrying out sensitivity analysis techniques utilizing system-level tools, critical epistemic uncertainties can be identified. Afterwards, a framework is introduced to design specific computational and physical experimentation for generating new knowledge about parameters, and for uncertainty mitigation. The methodology is demonstrated through a case study on an early-stage design Blended-Wing-Body (BWB) aircraft concept, showcasing how aerostructures analyses can be leveraged for mitigating system-level uncertainty, by computer experiments or guiding physical experimentation. The proposed methodology is versatile enough to tackle uncertainty management across various design challenges, highlighting the potential for more risk-informed design processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13931v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Efe Y. Yarbasi, Dimitri N. Mavris</dc:creator>
    </item>
    <item>
      <title>Transient Nonlinear Electrothermal Adjoint Sensitivity Analysis for HVDC Cable Joints</title>
      <link>https://arxiv.org/abs/2405.14284</link>
      <description>arXiv:2405.14284v1 Announce Type: new 
Abstract: Efficient computation of sensitivities is a promising approach for efficiently of designing and optimizing high voltage direct current cable joints. This paper presents the adjoint variable method for coupled nonlinear transient electrothermal problems as an efficient approach to compute sensitivities with respect to a large number of design parameters. The method is used to compute material sensitivities of a 320kV high voltage direct current cable joint specimen. The results are validated against sensitivities obtained via the direct sensitivity method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14284v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Greta Ruppert, Yvonne Sp\"ack-Leigsnering, Herbert De Gersem</dc:creator>
    </item>
    <item>
      <title>Rapid modelling of reactive transport in porous media using machine learning: limitations and solutions</title>
      <link>https://arxiv.org/abs/2405.14548</link>
      <description>arXiv:2405.14548v1 Announce Type: new 
Abstract: Reactive transport in porous media plays a pivotal role in subsurface reservoir processes, influencing fluid properties and geochemical characteristics. However, coupling fluid flow and transport with geochemical reactions is computationally intensive, requiring geochemical calculations at each grid cell and each time step within a discretized simulation domain. Although recent advancements have integrated machine learning techniques as surrogates for geochemical simulations, ensuring computational efficiency and accuracy remains a challenge. This chapter investigates machine learning models as replacements for a geochemical module in a reactive transport in porous media simulation. We test this approach on a well-documented cation exchange problem. While the surrogate models excel in isolated predictions, they fall short in rollout predictions over successive time steps. By introducing modifications, including physics-based constraints and tailored dataset generation strategies, we show that machine learning surrogates can achieve accurate rollout predictions. Our findings emphasize that, when judiciously designed, machine learning surrogates can substantially expedite the cation exchange problem without compromising accuracy, offering significant potential for a range of reactive transport applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14548v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vinicius L S Silva, Geraldine Regnier, Pablo Salinas, Claire E Heaney, Matthew D Jackson, Christopher C Pain</dc:creator>
    </item>
    <item>
      <title>Applied Machine Learning to Anomaly Detection in Enterprise Purchase Processes</title>
      <link>https://arxiv.org/abs/2405.14754</link>
      <description>arXiv:2405.14754v1 Announce Type: new 
Abstract: In a context of a continuous digitalisation of processes, organisations must deal with the challenge of detecting anomalies that can reveal suspicious activities upon an increasing volume of data. To pursue this goal, audit engagements are carried out regularly, and internal auditors and purchase specialists are constantly looking for new methods to automate these processes. This work proposes a methodology to prioritise the investigation of the cases detected in two large purchase datasets from real data. The goal is to contribute to the effectiveness of the companies' control efforts and to increase the performance of carrying out such tasks. A comprehensive Exploratory Data Analysis is carried out before using unsupervised Machine Learning techniques addressed to detect anomalies. A univariate approach has been applied through the z-Score index and the DBSCAN algorithm, while a multivariate analysis is implemented with the k-Means and Isolation Forest algorithms, and the Silhouette index, resulting in each method having a transaction candidates' proposal to be reviewed. An ensemble prioritisation of the candidates is provided jointly with a proposal of explicability methods (LIME, Shapley, SHAP) to help the company specialists in their understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14754v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>A. Herreros-Mart\'inez, R. Magdalena-Benedicto, J. Vila-Franc\'es, A. J. Serrano-L\'opez, S. P\'erez-D\'iaz</dc:creator>
    </item>
    <item>
      <title>High-fidelity level-set modeling of diffusive solid-state phase transformations for polycrystalline materials</title>
      <link>https://arxiv.org/abs/2405.12995</link>
      <description>arXiv:2405.12995v1 Announce Type: cross 
Abstract: The formation of microstructures in metallic alloys during hot metal forming involves simultaneous metallurgical complex phenomena. Traditional high-fidelity numerical frameworks used on the polycrystalline scale tend to focus on single-phase microstructures or isolate phase transformations from grain boundary migration mechanisms. The level-set method is highlighted as effective in proposing a global framework for modeling multiphase polycrystalline materials and diffusive solid-state phase transformations. This framework includes novel techniques for efficient large-scale microstructural representation, strong coupling with ThermoCalc software for real-time thermodynamic data, application for ternary alloys and beyond by taking solute drag aspects, and the use of advanced nucleation models. Numerous applications are then illustrated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12995v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nitish Chandrappa, Marc Bernacki</dc:creator>
    </item>
    <item>
      <title>Enhancing Multiscale Simulations with Constitutive Relations-Aware Deep Operator Networks</title>
      <link>https://arxiv.org/abs/2405.13759</link>
      <description>arXiv:2405.13759v1 Announce Type: cross 
Abstract: Multiscale problems are widely observed across diverse domains in physics and engineering. Translating these problems into numerical simulations and solving them using numerical schemes, e.g. the finite element method, is costly due to the demand of solving initial boundary-value problems at multiple scales. On the other hand, multiscale finite element computations are commended for their ability to integrate micro-structural properties into macroscopic computational analyses using homogenization techniques. Recently, neural operator-based surrogate models have shown trustworthy performance for solving a wide range of partial differential equations. In this work, we propose a hybrid method in which we utilize deep operator networks for surrogate modeling of the microscale physics. This allows us to embed the constitutive relations of the microscale into the model architecture and to predict microscale strains and stresses based on the prescribed macroscale strain inputs. Furthermore, numerical homogenization is carried out to obtain the macroscale quantities of interest. We apply the proposed approach to quasi-static problems of solid mechanics. The results demonstrate that our constitutive relations-aware DeepONet can yield accurate solutions even when being confronted with a restricted dataset during model development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13759v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hamidreza Eivazi, Mahyar Alikhani, Jendrik-Alexander Tr\"oger, Stefan Wittek, Stefan Hartmann, Andreas Rausch</dc:creator>
    </item>
    <item>
      <title>Design Editing for Offline Model-based Optimization</title>
      <link>https://arxiv.org/abs/2405.13964</link>
      <description>arXiv:2405.13964v1 Announce Type: cross 
Abstract: Offline model-based optimization (MBO) aims to maximize a black-box objective function using only an offline dataset of designs and scores. A prevalent approach involves training a conditional generative model on existing designs and their associated scores, followed by the generation of new designs conditioned on higher target scores. However, these newly generated designs often underperform due to the lack of high-scoring training data. To address this challenge, we introduce a novel method, Design Editing for Offline Model-based Optimization (DEMO), which consists of two phases. In the first phase, termed pseudo-target distribution generation, we apply gradient ascent on the offline dataset using a trained surrogate model, producing a synthetic dataset where the predicted scores serve as new labels. A conditional diffusion model is subsequently trained on this synthetic dataset to capture a pseudo-target distribution, which enhances the accuracy of the conditional diffusion model in generating higher-scoring designs. Nevertheless, the pseudo-target distribution is susceptible to noise stemming from inaccuracies in the surrogate model, consequently predisposing the conditional diffusion model to generate suboptimal designs. We hence propose the second phase, existing design editing, to directly incorporate the high-scoring features from the offline dataset into design generation. In this phase, top designs from the offline dataset are edited by introducing noise, which are subsequently refined using the conditional diffusion model to produce high-scoring designs. Overall, high-scoring designs begin with inheriting high-scoring features from the second phase and are further refined with a more accurate conditional diffusion model in the first phase. Empirical evaluations on 7 offline MBO tasks show that DEMO outperforms various baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13964v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ye Yuan, Youyuan Zhang, Can Chen, Haolun Wu, Zixuan Li, Jianmo Li, James J. Clark, Xue Liu</dc:creator>
    </item>
    <item>
      <title>AutoLCZ: Towards Automatized Local Climate Zone Mapping from Rule-Based Remote Sensing</title>
      <link>https://arxiv.org/abs/2405.13993</link>
      <description>arXiv:2405.13993v1 Announce Type: cross 
Abstract: Local climate zones (LCZs) established a standard classification system to categorize the landscape universe for improved urban climate studies. Existing LCZ mapping is guided by human interaction with geographic information systems (GIS) or modelled from remote sensing (RS) data. GIS-based methods do not scale to large areas. However, RS-based methods leverage machine learning techniques to automatize LCZ classification from RS. Yet, RS-based methods require huge amounts of manual labels for training.
  We propose a novel LCZ mapping framework, termed AutoLCZ, to extract the LCZ classification features from high-resolution RS modalities. We study the definition of numerical rules designed to mimic the LCZ definitions. Those rules model geometric and surface cover properties from LiDAR data. Correspondingly, we enable LCZ classification from RS data in a GIS-based scheme. The proposed AutoLCZ method has potential to reduce the human labor to acquire accurate metadata. At the same time, AutoLCZ sheds light on the physical interpretability of RS-based methods. In a proof-of-concept for New York City (NYC) we leverage airborne LiDAR surveys to model 4 LCZ features to distinguish 10 LCZ types. The results indicate the potential of AutoLCZ as promising avenue for large-scale LCZ mapping from RS data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13993v1</guid>
      <category>cs.CV</category>
      <category>cs.CE</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenying Liu, Hunsoo Song, Anamika Shreevastava, Conrad M Albrecht</dc:creator>
    </item>
    <item>
      <title>Representative electricity price profiles for European day-ahead and intraday spot markets</title>
      <link>https://arxiv.org/abs/2405.14403</link>
      <description>arXiv:2405.14403v1 Announce Type: cross 
Abstract: We propose a method to construct representative price profiles of the day-ahead (DA) and the intraday (ID) electricity spot markets and use this method to provide examples of ready-to-use price data sets. In contrast to common scenario generation approaches, the method is deterministic and relies on a small number of degrees of freedom, with the aim to be well defined and easy to use. We thereby target an enhanced comparability of future research studies on demand-side management and energy cost optimization. We construct the price profiles based on historical time series from the spot markets of interest, e.g., European Power Exchange (EPEX) spot. To this end, we extract key price components from the data while also accounting for known dominant mechanisms in the price variation. Further, the method is able to preserve key statistical features of the historical data (e.g., mean and standard deviation) when constructing the benchmark profile. Finally, our approach ensures comparability of ID and DA price profiles by design, as their cumulative (integral) price can be made identical if needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14403v1</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chrysanthi Papadimitriou, Jan C. Schulze, Alexander Mitsos</dc:creator>
    </item>
    <item>
      <title>A Unification Between Deep-Learning Vision, Compartmental Dynamical Thermodynamics, and Robotic Manipulation for a Circular Economy</title>
      <link>https://arxiv.org/abs/2405.14406</link>
      <description>arXiv:2405.14406v1 Announce Type: cross 
Abstract: The shift from a linear to a circular economy has the potential to simultaneously reduce uncertainties of material supplies and waste generation. To date, the development of robotic and, more generally, autonomous systems have been rarely integrated into circular economy implementation strategies. In this review, we merge deep-learning vision, compartmental dynamical thermodynamics, and robotic manipulation into a theoretically-coherent physics-based research framework to lay the foundations of circular flow designs of materials, and hence, to speed-up the transition from linearity to circularity. Then, we discuss opportunities for robotics in circular economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14406v1</guid>
      <category>cs.RO</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Zocco, Wassim M. Haddad, Andrea Corti, Monica Malvezzi</dc:creator>
    </item>
    <item>
      <title>Explainable automatic industrial carbon footprint estimation from bank transaction classification using natural language processing</title>
      <link>https://arxiv.org/abs/2405.14505</link>
      <description>arXiv:2405.14505v1 Announce Type: cross 
Abstract: Concerns about the effect of greenhouse gases have motivated the development of certification protocols to quantify the industrial carbon footprint (CF). These protocols are manual, work-intensive, and expensive. All of the above have led to a shift towards automatic data-driven approaches to estimate the CF, including Machine Learning (ML) solutions. Unfortunately, the decision-making processes involved in these solutions lack transparency from the end user's point of view, who must blindly trust their outcomes compared to intelligible traditional manual approaches. In this research, manual and automatic methodologies for CF estimation were reviewed, taking into account their transparency limitations. This analysis led to the proposal of a new explainable ML solution for automatic CF calculations through bank transaction classification. Consideration should be given to the fact that no previous research has considered the explainability of bank transaction classification for this purpose. For classification, different ML models have been employed based on their promising performance in the literature, such as Support Vector Machine, Random Forest, and Recursive Neural Networks. The results obtained were in the 90 % range for accuracy, precision, and recall evaluation metrics. From their decision paths, the proposed solution estimates the CO2 emissions associated with bank transactions. The explainability methodology is based on an agnostic evaluation of the influence of the input terms extracted from the descriptions of transactions using locally interpretable models. The explainability terms were automatically validated using a similarity metric over the descriptions of the target categories. Conclusively, the explanation performance is satisfactory in terms of the proximity of the explanations to the associated activity sector descriptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14505v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2022.3226324</arxiv:DOI>
      <dc:creator>Jaime Gonz\'alez-Gonz\'alez, Silvia Garc\'ia-M\'endez, Francisco de Arriba-P\'erez, Francisco J. Gonz\'alez-Casta\~no, \'Oscar Barba-Seara</dc:creator>
    </item>
    <item>
      <title>Space-time unfitted finite elements on moving explicit geometry representations</title>
      <link>https://arxiv.org/abs/2401.12649</link>
      <description>arXiv:2401.12649v2 Announce Type: replace 
Abstract: This work proposes a novel variational approximation of partial differential equations on moving geometries determined by explicit boundary representations. The benefits of the proposed formulation are the ability to handle large displacements of explicitly represented domain boundaries without generating body-fitted meshes and remeshing techniques. For the space discretization, we use a background mesh and an unfitted method that relies on integration on cut cells only. We perform this intersection by using clipping algorithms. To deal with the mesh movement, we pullback the equations to a reference configuration (the spatial mesh at the initial time slab times the time interval) that is constant in time. This way, the geometrical intersection algorithm is only required in 3D, another key property of the proposed scheme. At the end of the time slab, we compute the deformed mesh, intersect the deformed boundary with the background mesh, and consider an exact transfer operator between meshes to compute jump terms in the time discontinuous Galerkin integration. The transfer is also computed using geometrical intersection algorithms. We demonstrate the applicability of the method to fluid problems around rotating (2D and 3D) geometries described by oriented boundary meshes. We also provide a set of numerical experiments that show the optimal convergence of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12649v2</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santiago Badia, Pere A. Martorell, Francesc Verdugo</dc:creator>
    </item>
    <item>
      <title>Pre-Training on Large-Scale Generated Docking Conformations with HelixDock to Unlock the Potential of Protein-ligand Structure Prediction Models</title>
      <link>https://arxiv.org/abs/2310.13913</link>
      <description>arXiv:2310.13913v4 Announce Type: replace-cross 
Abstract: Protein-ligand structure prediction is an essential task in drug discovery, predicting the binding interactions between small molecules (ligands) and target proteins (receptors). Recent advances have incorporated deep learning techniques to improve the accuracy of protein-ligand structure prediction. Nevertheless, the experimental validation of docking conformations remains costly, it raises concerns regarding the generalizability of these deep learning-based methods due to the limited training data. In this work, we show that by pre-training on a large-scale docking conformation generated by traditional physics-based docking tools and then fine-tuning with a limited set of experimentally validated receptor-ligand complexes, we can obtain a protein-ligand structure prediction model with outstanding performance. Specifically, this process involved the generation of 100 million docking conformations for protein-ligand pairings, an endeavor consuming roughly 1 million CPU core days. The proposed model, HelixDock, aims to acquire the physical knowledge encapsulated by the physics-based docking tools during the pre-training phase. HelixDock has been rigorously benchmarked against both physics-based and deep learning-based baselines, demonstrating its exceptional precision and robust transferability in predicting binding confirmation. In addition, our investigation reveals the scaling laws governing pre-trained protein-ligand structure prediction models, indicating a consistent enhancement in performance with increases in model parameters and the volume of pre-training data. Moreover, we applied HelixDock to several drug discovery-related tasks to validate its practical utility. HelixDock demonstrates outstanding capabilities on both cross-docking and structure-based virtual screening benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.13913v4</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>q-bio.BM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lihang Liu, Shanzhuo Zhang, Donglong He, Xianbin Ye, Jingbo Zhou, Xiaonan Zhang, Yaoyao Jiang, Weiming Diao, Hang Yin, Hua Chai, Fan Wang, Jingzhou He, Liang Zheng, Yonghui Li, Xiaomin Fang</dc:creator>
    </item>
    <item>
      <title>Conditional Generative Representation for Black-Box Optimization with Implicit Constraints</title>
      <link>https://arxiv.org/abs/2310.18449</link>
      <description>arXiv:2310.18449v3 Announce Type: replace-cross 
Abstract: Black-box optimization (BBO) has become increasingly relevant for tackling complex decision-making problems, especially in public policy domains such as police districting. However, its broader application in public policymaking is hindered by the complexity of defining feasible regions and the high-dimensionality of decisions. This paper introduces a novel BBO framework, termed as the Conditional And Generative Black-box Optimization (CageBO). This approach leverages a conditional variational autoencoder to learn the distribution of feasible decisions, enabling a two-way mapping between the original decision space and a simplified, constraint-free latent space. The CageBO efficiently handles the implicit constraints often found in public policy applications, allowing for optimization in the latent space while evaluating objectives in the original space. We validate our method through a case study on large-scale police districting problems in Atlanta, Georgia. Our results reveal that our CageBO offers notable improvements in performance and efficiency compared to the baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18449v3</guid>
      <category>stat.ML</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenqian Xing, Jungho Lee, Chong Liu, Shixiang Zhu</dc:creator>
    </item>
    <item>
      <title>Feature Mapping in Physics-Informed Neural Networks (PINNs)</title>
      <link>https://arxiv.org/abs/2402.06955</link>
      <description>arXiv:2402.06955v2 Announce Type: replace-cross 
Abstract: In this paper, the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel is investigated, shedding light on the convergence of PINNs; Although the commonly used Fourier-based feature mapping has achieved great success, we show its inadequacy in some physics scenarios. Via these two scopes, we propose conditionally positive definite Radial Basis Function as a better alternative. Lastly, we explore the feature mapping numerically in a wide neural networks. Our empirical results reveal the efficacy of our method in diverse forward and inverse problem sets. Composing feature functions is found to be a practical way to address the expressivity and generalisability trade-off, viz., tuning the bandwidth of the kernels and the surjectivity of the feature mapping function. This simple technique can be implemented for coordinate inputs and benefits the broader PINNs research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06955v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chengxi Zeng, Tilo Burghardt, Alberto M Gambaruto</dc:creator>
    </item>
    <item>
      <title>Physics-Informed Diffusion Models</title>
      <link>https://arxiv.org/abs/2403.14404</link>
      <description>arXiv:2403.14404v2 Announce Type: replace-cross 
Abstract: Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework to inform denoising diffusion models of underlying constraints on such generated samples during model training. Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed. Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting. Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14404v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Hendrik Bastek, WaiChing Sun, Dennis M. Kochmann</dc:creator>
    </item>
    <item>
      <title>The Hyperdrive Protocol: An Automated Market Maker for Fixed and Variable Rates</title>
      <link>https://arxiv.org/abs/2404.05036</link>
      <description>arXiv:2404.05036v2 Announce Type: replace-cross 
Abstract: Hyperdrive is a protocol designed to facilitate the trading of fixed and variable rate assets. The protocol's unique pricing model consolidates liquidity into a single pool which addresses the challenges of fragmented liquidity across terms, eliminates the need for rollovers, and allows terms to be issued on demand. Its design meaningfully improves trading efficiency, liquidity provisioning, and user experience over existing fixed and variable rate protocol models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05036v2</guid>
      <category>cs.GT</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jonny Rhea, Alex Towle, Mihai Cosma</dc:creator>
    </item>
    <item>
      <title>FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model</title>
      <link>https://arxiv.org/abs/2404.14688</link>
      <description>arXiv:2404.14688v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a pre-trained foundation model \textbf{FMint} (\textbf{F}oundation \textbf{M}odel based on \textbf{In}i\textbf{t}ialization), designed to speed up large-scale simulations of various differential equations with high accuracy via error correction. Human-designed simulation algorithms excel at capturing the fundamental physics of engineering problems, but often need to balance the trade-off between accuracy and efficiency. While deep learning methods offer innovative solutions across numerous scientific fields, they frequently fall short in domain-specific knowledge. FMint bridges these gaps through conditioning on the initial coarse solutions obtained from conventional human-designed algorithms, and trained to obtain refined solutions for various differential equations. Based on the backbone of large language models, we adapt the in-context learning scheme to learn a universal error correction method for dynamical systems from given prompted sequences of coarse solutions. The model is pre-trained on a corpus of 600K ordinary differential equations (ODEs), and we conduct extensive experiments on both in-distribution and out-of-distribution tasks. FMint outperforms various baselines on large-scale simulation, and demonstrates its capability in generalization to unseen ODEs. Our approach achieves an accuracy improvement of 1 to 2 orders of magnitude over state-of-the-art dynamical system simulators, and delivers a 5X speedup compared to traditional numerical algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14688v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zezheng Song, Jiaxin Yuan, Haizhao Yang</dc:creator>
    </item>
  </channel>
</rss>
