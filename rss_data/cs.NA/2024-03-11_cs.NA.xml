<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Mar 2024 04:01:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Analysis of Hybrid MC/Deterministic Methods for Transport Problems Based on Low-Order Equations Discretized by Finite Volume Schemes</title>
      <link>https://arxiv.org/abs/2403.05673</link>
      <description>arXiv:2403.05673v1 Announce Type: new 
Abstract: This paper presents hybrid numerical techniques for solving the Boltzmann transport equation formulated by means of low-order equations for angular moments of the angular flux. The moment equations are derived by the projection operator approach. The projected equations are closed exactly using a high-order transport solution. The low-order equations of the hybrid methods are approximated with a finite volume scheme of the second-order accuracy. Functionals defining the closures in the discretized low-order equations are calculated by Monte Carlo techniques. In this study, we analyze effects of statistical noise and discretization error on the accuracy of the hybrid transport solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05673v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent N. Novellion, Dmitriy Y. Anistratov</dc:creator>
    </item>
    <item>
      <title>Enhancing non-intrusive Reduced Order Models with space-dependent aggregation methods</title>
      <link>https://arxiv.org/abs/2403.05710</link>
      <description>arXiv:2403.05710v1 Announce Type: new 
Abstract: In this manuscript, we combine non-intrusive reduced order models (ROMs) with space-dependent aggregation techniques to build a mixed-ROM. The prediction of the mixed formulation is given by a convex linear combination of the predictions of some previously-trained ROMs, where we assign to each model a space-dependent weight. The ROMs taken into account to build the mixed model exploit different reduction techniques, such as Proper Orthogonal Decomposition (POD) and AutoEncoders (AE), and/or different approximation techniques, namely a Radial Basis Function Interpolation (RBF), a Gaussian Process Regression (GPR) or a feed-forward Artificial Neural Network (ANN). The contribution of each model is retained with higher weights in the regions where the model performs best, and, vice versa, with smaller weights where the model has a lower accuracy with respect to the other models. Finally, a regression technique, namely a Random Forest, is exploited to evaluate the weights for unseen conditions. The performance of the aggregated model is evaluated on two different test cases: the 2D flow past a NACA 4412 airfoil, with an angle of attack of 5 degrees, having as parameter the Reynolds number varying between 1e5 and 1e6 and a transonic flow over a NACA 0012 airfoil, considering as parameter the angle of attack. In both cases, the mixed-ROM has provided improved accuracy with respect to each individual ROM technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05710v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Ivagnes, Niccol\`o Tonicello, Paola Cinnella, Gianluigi Rozza</dc:creator>
    </item>
    <item>
      <title>Numerical cubature and hyperinterpolation over Spherical Polygons</title>
      <link>https://arxiv.org/abs/2403.05733</link>
      <description>arXiv:2403.05733v1 Announce Type: new 
Abstract: The purpose of this work is to introduce a strategy for determining the nodes and weights of a low-cardinality positive cubature formula nearly exact for polynomials of a given degree over spherical polygons. In the numerical section we report the results about numerical cubature over a spherical polygon $\cal P$ approximating Australia and reconstruction of functions over such $\cal P$, also affected by perturbations, via hyperinterpolation and some of its variants. The open-source Matlab software used in the numerical tests is available at the author's homepage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05733v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alvise Sommariva</dc:creator>
    </item>
    <item>
      <title>Shallow ReLU neural networks and finite elements</title>
      <link>https://arxiv.org/abs/2403.05809</link>
      <description>arXiv:2403.05809v1 Announce Type: new 
Abstract: We point out that (continuous or discontinuous) piecewise linear functions on a convex polytope mesh can be represented by two-hidden-layer ReLU neural networks in a weak sense. In addition, the numbers of neurons of the two hidden layers required to weakly represent are accurately given based on the numbers of polytopes and hyperplanes involved in this mesh. The results naturally hold for constant and linear finite element functions. Such weak representation establishes a bridge between shallow ReLU neural networks and finite element functions, and leads to a perspective for analyzing approximation capability of ReLU neural networks in $L^p$ norm via finite element functions. Moreover, we discuss the strict representation for tensor finite element functions via the recent tensor neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05809v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengzhan Jin</dc:creator>
    </item>
    <item>
      <title>Multilevel Monte Carlo methods for positivity-preserving approximations of the Heston 3/2-model</title>
      <link>https://arxiv.org/abs/2403.05837</link>
      <description>arXiv:2403.05837v1 Announce Type: new 
Abstract: This article is concerned with the multilevel Monte Carlo (MLMC) methods for approximating expectations of some functions of the solution to the Heston 3/2-model from mathematical finance, which takes values in $(0, \infty)$ and possesses superlinearly growing drift and diffusion coefficients. To discretize the SDE model, a new Milstein-type scheme is proposed to produce independent sample paths. The proposed scheme can be explicitly solved and is positivity-preserving unconditionally, i.e., for any time step-size $h&gt;0$. This positivity-preserving property for large discretization time steps is particularly desirable in the MLMC setting. Furthermore, a mean-square convergence rate of order one is proved in the non-globally Lipschitz regime, which is not trivial, as the diffusion coefficient grows super-linearly. The obtained order-one convergence in turn promises the desired relevant variance of the multilevel estimator and justifies the optimal complexity $\mathcal{O}(\epsilon^{-2})$ for the MLMC approach, where $\epsilon &gt; 0$ is the required target accuracy. Numerical experiments are finally reported to confirm the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05837v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiaojuan Wu, Siqing Gan</dc:creator>
    </item>
    <item>
      <title>A new quadratic and cubic polynomial enrichment of the Crouzeix-Raviart finite element</title>
      <link>https://arxiv.org/abs/2403.05844</link>
      <description>arXiv:2403.05844v1 Announce Type: new 
Abstract: In this paper, we introduce quadratic and cubic polynomial enrichments of the classical Crouzeix--Raviart finite element, with the aim of constructing accurate approximations in such enriched elements. To achieve this goal, we respectively add three and seven weighted line integrals as enriched degrees of freedom. For each case, we present a necessary and sufficient condition under which these augmented elements are well-defined. For illustration purposes, we then use a general approach to define two-parameter families of admissible degrees of freedom. Additionally, we provide explicit expressions for the associated basis functions and subsequently introduce new quadratic and cubic approximation operators based on the proposed admissible elements. The efficiency of the enriched methods is compared to the triangular Crouzeix--Raviart element. As expected, the numerical results exhibit a significant improvement, confirming the effectiveness of the developed enrichment strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05844v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Dell'Accio, Allal Guessab, Federico Nudo</dc:creator>
    </item>
    <item>
      <title>A Second-Order Nonlocal Approximation to Manifold Poisson Models with Neumann Boundary</title>
      <link>https://arxiv.org/abs/2403.05888</link>
      <description>arXiv:2403.05888v1 Announce Type: new 
Abstract: In this paper, we propose a class of nonlocal models to approximate the Poisson model on manifolds with homogeneous Neumann boundary condition, where the manifolds are assumed to be embedded in high dimensional Euclid spaces. In comparison to the existing nonlocal approximation of Poisson models with Neumann boundary, we optimize the truncation error of model by adding an augmented term along the $2\delta$ layer of boundary, with $2\delta$ be the nonlocal interaction horizon. Such term is formulated by the integration of the second order normal derivative of solution through the boundary, while the second order normal derivative is expressed as the difference between the interior Laplacian and the boundary Laplacian. The concentration of our paper is on the construction of nonlocal model, the well-posedness of model, and its second-order convergence rate to its local counterpart. The localization rate of our nonlocal model is currently optimal among all related works even for the case of high dimensional Euclid spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05888v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yajie Zhang, Zuoqiang Shi</dc:creator>
    </item>
    <item>
      <title>The exponential trapezoidal method for semilinear integro-differential equations</title>
      <link>https://arxiv.org/abs/2403.05900</link>
      <description>arXiv:2403.05900v1 Announce Type: new 
Abstract: The exponential trapezoidal rule is proposed and analyzed for the numerical integration of semilinear integro-differential equations. Although the method is implicit, the numerical solution is easily obtained by standard fixed-point iteration, making its implementation straightforward. Second-order convergence in time is shown in an abstract Hilbert space framework under reasonable assumptions on the problem. Numerical experiments illustrate the proven order of convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05900v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexander Ostermann, Nasrin Vaisi</dc:creator>
    </item>
    <item>
      <title>Unique reconstruction for discretized inverse problems: a random sketching approach</title>
      <link>https://arxiv.org/abs/2403.05935</link>
      <description>arXiv:2403.05935v1 Announce Type: new 
Abstract: Inverse problem theory is often studied in the ideal infinite-dimensional setting. Through the lens of the PDE-constrained optimization, the well-posedness PDE theory suggests unique reconstruction of the parameter function that attain the zero-loss property of the mismatch function, when infinite amount of data is provided. Unfortunately, this is not the case in practice, when we are limited to finite amount of measurements due to experimental or economical reasons. Consequently, one must compromise the inference goal to a discrete approximation of the unknown smooth function.
  What is the reconstruction power of a fixed number of data observations? How many parameters can one reconstruct? Here we describe a probabilistic approach, and spell out the interplay of the observation size $(r)$ and the number of parameters to be uniquely identified $(m)$. The technical pillar is the random sketching strategy, in which the matrix concentration inequality and sampling theory are largely employed. By analyzing randomly sub-sampled Hessian matrix, we attain well-conditioned reconstruction problem with high probability. Our main theory is finally validated in numerical experiments. We set tests on both synthetic and the data from an elliptic inverse problem. The empirical performance shows that given suitable sampling quality, the well-conditioning of the sketched Hessian is certified with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05935v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruhui Jin, Qin Li, Anjali Nair, Samuel Stechmann</dc:creator>
    </item>
    <item>
      <title>Fully discretized Sobolev gradient flow for the Gross-Pitaevskii eigenvalue problem</title>
      <link>https://arxiv.org/abs/2403.06028</link>
      <description>arXiv:2403.06028v1 Announce Type: new 
Abstract: For the ground state of the Gross-Pitaevskii (GP) eigenvalue problem, we consider a fully discretized Sobolev gradient flow, which can be regarded as the Riemannian gradient descent on the sphere under a metric induced by a modified $H^1$-norm. We prove its global convergence to a critical point of the discrete GP energy and its local exponential convergence to the ground state of the discrete GP energy. The local exponential convergence rate depends on the eigengap of the discrete GP energy. When the discretization is the classical second-order finite difference in two dimensions, such an eigengap can be further proven to be mesh independent, i.e., it has a uniform positive lower bound, thus the local exponential convergence rate is mesh independent. Numerical experiments with discretization by high order $Q^k$ spectral element methods in two and three dimensions are provided to validate the efficiency of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06028v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Chen, Jianfeng Lu, Yulong Lu, Xiangxiong Zhang</dc:creator>
    </item>
    <item>
      <title>pETNNs: Partial Evolutionary Tensor Neural Networks for Solving Time-dependent Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2403.06084</link>
      <description>arXiv:2403.06084v1 Announce Type: new 
Abstract: We present partial evolutionary tensor neural networks (pETNNs), a novel framework for solving time-dependent partial differential equations with both of high accuracy and remarkable extrapolation. Our proposed architecture leverages the inherent accuracy of tensor neural networks, while incorporating evolutionary parameters that enable remarkable extrapolation capabilities. By adopting innovative parameter update strategies, the pETNNs achieve a significant reduction in computational cost while maintaining precision and robustness. Notably, the pETNNs enhance the accuracy of conventional evolutional deep neural networks and empowers computational abilities to address high-dimensional problems. Numerical experiments demonstrate the superior performance of the pETNNs in solving time-dependent complex equations, including the Navier-Stokes equations, high-dimensional heat equation, high-dimensional transport equation and Korteweg-de Vries type equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06084v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tunan Kao, Jin Zhao, Lei Zhang</dc:creator>
    </item>
    <item>
      <title>An approach using the null space to implement Dirichlet and constraint boundary conditions into FEM</title>
      <link>https://arxiv.org/abs/2403.06160</link>
      <description>arXiv:2403.06160v1 Announce Type: new 
Abstract: A handy technique for the Finite Element Method (FEM) is presented that uses the null space for the implementation of Dirichlet and constraint boundary conditions. The focus of this method is to present an illustrative approach to modeling boundary constraints within FEM simulations for teaching. It presents a consistent way of including the boundary terms in the forcing and constructing the field solution after solving the algebraic system of equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06160v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Schoder</dc:creator>
    </item>
    <item>
      <title>An adaptive mesh refinement strategy to ensure quasi-optimality of the conforming finite element method for the Helmholtz equation via T-coercivity</title>
      <link>https://arxiv.org/abs/2403.06266</link>
      <description>arXiv:2403.06266v1 Announce Type: new 
Abstract: It is well known that the quasi-optimality of the Galerkin finite element method for the Helmholtz equation is dependent on the mesh size and the wave-number. In literature, different criteria have been proposed to ensure quasi-optimality. Often these criteria are difficult to obtain and depend on wave-number explicit regularity estimates. In the present work, we focus on criteria based on T-coercivity and weak T-coercivity, which highlight mesh size dependence on the gap between the square of the wavenumber and Laplace eigenvalues. We also propose an adaptive scheme, coupled with a residual-based indicator, for optimal mesh generation with minimal degrees of freedom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06266v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim van Beeck, Umberto Zerbinati</dc:creator>
    </item>
    <item>
      <title>The Numerical Solution in the Sense of Prager&amp;Synge</title>
      <link>https://arxiv.org/abs/2403.06273</link>
      <description>arXiv:2403.06273v1 Announce Type: new 
Abstract: The solution in sense of Prager&amp;Synge is the alternative to the commonly used notion of the numerical solution, which is considered as a limit of grid functions at mesh refinement. Prager&amp;Synge solution is defined as a hypersphere containing the projection of the true solution of the system of partial differentiation equations (PDE) onto the computational grid and does not use any asymptotics. In the original variant it is determined using orthogonal properties specific for certain equations. In the proposed variant, the center and radius of the hypersphere is estimated using the ensemble of numerical solutions obtained by independent algorithms. This approach may be easily expanded for solutions of an arbitrary system of partial differentiation equations that significantly expands the domain of its applicability. Several options for the computation of the Prager&amp;Synge solution are considered and compared herein. The first one is based on the search for the orthogonal truncation errors and their transformation. The second is based on the orthogonalization of approximation errors obtained using the defect correction method and applies a superposition of numerical solutions. These options are intrusive. In third option (nonintrusive) the information regarding orthogonality of errors, which is crucial for the Prager&amp;Synge approach method, is replaced by information that stems from the properties of the ensemble of numerical solutions, obtained by independent numerical algorithms. The values of the angle between the truncation errors on such ensemble or the distances between elements of the ensemble may be used to replace the orthogonality. The variant based on the width of the ensemble of independent numerical solutions does not require any additional a priori information and is the approximate nonintrusive version of the method based on the orthogonalization of approximation errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06273v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. K. Alekseev, A. E. Bondarev</dc:creator>
    </item>
    <item>
      <title>Higher-order spring-coupled multilevel Monte Carlo method for invariant measures</title>
      <link>https://arxiv.org/abs/2403.06310</link>
      <description>arXiv:2403.06310v1 Announce Type: new 
Abstract: A higher-order change-of-measure multilevel Monte Carlo (MLMC) method is developed for computing weak approximations of the invariant measures of SDE with drift coefficients that do not satisfy the contractivity condition. This is achieved by introducing a spring term in the pairwise coupling of the MLMC trajectories employing the order 1.5 strong It\^o--Taylor method. Through this, we can recover the contractivity property of the drift coefficient while still retaining the telescoping sum property needed for implementing the MLMC method.
  We show that the variance of the change-of-measure MLMC method grows linearly in time $T$ for all $T &gt; 0$, and for all sufficiently small timestep size $h &gt; 0$. For a given error tolerance $\epsilon &gt; 0$, we prove that the method achieves a mean-square-error accuracy of $O(\epsilon^2)$ with a computational cost of $O(\epsilon^{-2} \big\vert \log \epsilon \big\vert^{3/2} (\log \big\vert \log \epsilon \big\vert)^{1/2})$ for uniformly Lipschitz continuous payoff functions and $O \big( \epsilon^{-2} \big\vert \log \epsilon \big\vert^{5/3 + \xi} \big)$ for discontinuous payoffs, respectively, where $\xi &gt; 0$. We also observe an improvement in the constant associated with the computational cost of the higher-order change-of-measure MLMC method, marking an improvement over the Milstein change-of-measure method in the aforementioned seminal work by M. Giles and W. Fang. Several numerical tests were performed to verify the theoretical results and assess the robustness of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06310v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sankarasubramanian Ragunathan, H{\aa}kon Andreas Hoel</dc:creator>
    </item>
    <item>
      <title>Separable Physics-informed Neural Networks for Solving the BGK Model of the Boltzmann Equation</title>
      <link>https://arxiv.org/abs/2403.06342</link>
      <description>arXiv:2403.06342v1 Announce Type: new 
Abstract: In this study, we introduce a method based on Separable Physics-Informed Neural Networks (SPINNs) for effectively solving the BGK model of the Boltzmann equation. While the mesh-free nature of PINNs offers significant advantages in handling high-dimensional partial differential equations (PDEs), challenges arise when applying quadrature rules for accurate integral evaluation in the BGK operator, which can compromise the mesh-free benefit and increase computational costs. To address this, we leverage the canonical polyadic decomposition structure of SPINNs and the linear nature of moment calculation, achieving a substantial reduction in computational expense for quadrature rule application. The multi-scale nature of the particle density function poses difficulties in precisely approximating macroscopic moments using neural networks. To improve SPINN training, we introduce the integration of Gaussian functions into SPINNs, coupled with a relative loss approach. This modification enables SPINNs to decay as rapidly as Maxwellian distributions, thereby enhancing the accuracy of macroscopic moment approximations. The relative loss design further ensures that both large and small-scale features are effectively captured by the SPINNs. The efficacy of our approach is demonstrated through a series of five numerical experiments, including the solution to a challenging 3D Riemann problem. These results highlight the potential of our novel method in efficiently and accurately addressing complex challenges in computational physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06342v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaemin Oh, Seung Yeon Cho, Seok-Bae Yun, Eunbyung Park, Youngjoon Hong</dc:creator>
    </item>
    <item>
      <title>A Functionally Connected Element Method for Solving Boundary Value Problems</title>
      <link>https://arxiv.org/abs/2403.06393</link>
      <description>arXiv:2403.06393v1 Announce Type: new 
Abstract: We present the general forms of piece-wise functions on partitioned domains satisfying an intrinsic $C^0$ or $C^1$ continuity across the sub-domain boundaries. These general forms are constructed based on a strategy stemming from the theory of functional connections, and we refer to partitioned domains endowed with these general forms as functionally connected elements (FCE). We further present a method, incorporating functionally connected elements and a least squares collocation approach, for solving boundary and initial value problems. This method exhibits a spectral-like accuracy, with the free functions involved in the FCE form represented by polynomial bases or by non-polynomial bases of quasi-random sinusoidal functions. The FCE method offers a unique advantage over traditional element-based methods for boundary value problems involving relative boundary conditions. A number of linear and nonlinear numerical examples in one and two dimensions are presented to demonstrate the performance of the FCE method developed herein.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06393v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jielin Yang, Suchuan Dong</dc:creator>
    </item>
    <item>
      <title>Unisolvence of random Kansa collocation by Thin-Plate Splines for the Poisson equation</title>
      <link>https://arxiv.org/abs/2403.06646</link>
      <description>arXiv:2403.06646v1 Announce Type: new 
Abstract: Existence of sufficient conditions for unisolvence of Kansa unsymmetric collocation for PDEs is still an open problem. In this paper we make a first step in this direction, proving that unsymmetric collocation matrices with Thin-Plate Splines for the 2D Poisson equation are almost surely nonsingular, when the discretization points are chosen randomly on domains with analytic boundary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06646v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Dell'Accio, Alvise Sommariva, Marco Vianello</dc:creator>
    </item>
    <item>
      <title>Greedy construction of quadratic manifolds for nonlinear dimensionality reduction and nonlinear model reduction</title>
      <link>https://arxiv.org/abs/2403.06732</link>
      <description>arXiv:2403.06732v1 Announce Type: new 
Abstract: Dimensionality reduction on quadratic manifolds augments linear approximations with quadratic correction terms. Previous works rely on linear approximations given by projections onto the first few leading principal components of the training data; however, linear approximations in subspaces spanned by the leading principal components alone can miss information that are necessary for the quadratic correction terms to be efficient. In this work, we propose a greedy method that constructs subspaces from leading as well as later principal components so that the corresponding linear approximations can be corrected most efficiently with quadratic terms.
  Properties of the greedily constructed manifolds allow applying linear algebra reformulations so that the greedy method scales to data points with millions of dimensions. Numerical experiments demonstrate that an orders of magnitude higher accuracy is achieved with the greedily constructed quadratic manifolds compared to manifolds that are based on the leading principal components alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06732v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Schwerdtner, Benjamin Peherstorfer</dc:creator>
    </item>
    <item>
      <title>Weak form Shallow Ice Approximation models with an improved time step restriction</title>
      <link>https://arxiv.org/abs/2403.06811</link>
      <description>arXiv:2403.06811v1 Announce Type: new 
Abstract: The Shallow Ice Approximation (SIA) model written on strong form is commonly used for inferring the dynamics of ice sheets and glaciers. The model describes non-Newtonian, viscous, and gravity driven flow of ice in grounded ice sheets. The solution to the SIA model is a closed-form expression for the velocity field. A disadvantage is that when using the SIA velocities to advance the ice surface in time, the time step restriction has a quadratic scaling in terms of the horizontal mesh size. In this paper we write the SIA model on weak form, and add in the Free Surface Stabilization Algorithm (FSSA) terms. We find numerically that the time step restriction scaling is improved from quadratic to linear, but only for large horizontal mesh sizes. We then extend the weak formulation by adding in the normal stress terms which are originally neglected. This allows for a linear time step restriction across the whole range of the horizontal mesh sizes and as such leads to a computationally more efficient SIA model. To support the numerical results we theoretically show that the addition of the FSSA stabilization terms switches the explicit time stepping treatment of the second derivative surface terms to an implicit time stepping treatment. In addition we perform a computational cost analysis, which, when combined with the numerical results on stability properties and accuracy, speaks for favouring SIA models on weak form over the standard SIA model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06811v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Igor Tominec, Josefin Ahlkrona</dc:creator>
    </item>
    <item>
      <title>A method for accelerating low precision operations by sparse matrix multiplication</title>
      <link>https://arxiv.org/abs/2403.06924</link>
      <description>arXiv:2403.06924v1 Announce Type: new 
Abstract: In recent years, the fervent demand for computational power across various domains has prompted hardware manufacturers to introduce specialized computing hardware aimed at enhancing computational capabilities. Particularly, the utilization of tensor hardware supporting low precision has gained increasing prominence in scientific research. However, the use of low-precision tensor hardware for computational acceleration often introduces errors, posing a fundamental challenge of simultaneously achieving effective acceleration while maintaining computational accuracy.
  This paper proposes improvements in the methodology by incorporating low-precision quantization and employing a residual matrix for error correction and combines vector-wise quantization method.. The key innovation lies in the use of sparse matrices instead of dense matrices when compensating for errors with a residual matrix. By focusing solely on values that may significantly impact relative errors under a specified threshold, this approach aims to control quantization errors while reducing computational complexity. Experimental results demonstrate that this method can effectively control the quantization error while maintaining high acceleration effect.The improved algorithm on the CPU can achieve up to 15\% accuracy improvement while 1.46 times speed improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06924v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyaoxing Gu</dc:creator>
    </item>
    <item>
      <title>POD-ROM methods: from a finite set of snapshots to continuous-in-time approximations</title>
      <link>https://arxiv.org/abs/2403.06967</link>
      <description>arXiv:2403.06967v1 Announce Type: new 
Abstract: This paper studies discretization of time-dependent partial differential equations (PDEs) by proper orthogonal decomposition reduced order models (POD-ROMs). Most of the analysis in the literature has been performed on fully-discrete methods using first order methods in time, typically the implicit Euler time integrator. Our aim is to show which kind of error bounds can be obtained using any time integrator, both in the full order model (FOM), applied to compute the snapshots, and in the POD-ROM method. To this end, we analyze in this paper the continuous-in-time case for both the FOM and POD-ROM methods, although the POD basis is obtained from snapshots taken at a discrete (i.e., not continuous) set times. Two cases for the set of snapshots are considered: The case in which the snapshots are based on first order divided differences in time and the case in which they are based on temporal derivatives. Optimal pointwise-in-time error bounds {between the FOM and the POD-ROM solutions} are proved for the $L^2(\Omega)$ norm of the error for a semilinear reaction-diffusion model problem. The dependency of the errors on the distance in time between two consecutive snapshots and on the tail of the POD eigenvalues is tracked. Our detailed analysis allows to show that, in some situations, a small number of snapshots in a given time interval might be sufficient to accurately approximate the solution in the full interval. Numerical studies support the error analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06967v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bosco Garcia-Archilla, Volker John, Julia Novo</dc:creator>
    </item>
    <item>
      <title>Another look at Residual Dynamic Mode Decomposition in the regime of fewer Snapshots than Dictionary Size</title>
      <link>https://arxiv.org/abs/2403.05891</link>
      <description>arXiv:2403.05891v1 Announce Type: cross 
Abstract: Residual Dynamic Mode Decomposition (ResDMD) offers a method for accurately computing the spectral properties of Koopman operators. It achieves this by calculating an infinite-dimensional residual from snapshot data, thus overcoming issues associated with finite truncations of Koopman operators, such as spurious eigenvalues. These spectral properties include spectra and pseudospectra, spectral measures, Koopman mode decompositions, and dictionary verification. In scenarios where there are fewer snapshots than dictionary size, particularly for exact DMD and kernelized EDMD, ResDMD has traditionally been applied by dividing snapshot data into a training set and a quadrature set. Through a novel computational approach of solving a dual least-squares problem, we demonstrate how to eliminate the need for two datasets. We provide an analysis of these new residuals for exact DMD and kernelized EDMD, demonstrating ResDMD's versatility and broad applicability across various dynamical systems, including those modeled by high-dimensional and nonlinear observables. The utility of these new residuals is showcased through three diverse examples: the analysis of cylinder wake, the study of aerofoil cascades, and the compression of transient shockwave experimental data. This approach not only simplifies the application of ResDMD but also extends its potential for deeper insights into the dynamics of complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05891v1</guid>
      <category>math.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.SP</category>
      <category>nlin.CD</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew J. Colbrook</dc:creator>
    </item>
    <item>
      <title>An in-silico approach to meniscus tissue regeneration: Modeling, numerical simulation, and experimental analysis</title>
      <link>https://arxiv.org/abs/2403.05909</link>
      <description>arXiv:2403.05909v1 Announce Type: cross 
Abstract: We develop a model the dynamics of human mesenchymal stem cells (hMSCs) and chondrocytes evolving in a nonwoven polyethylene terephtalate (PET) scaffold impregnated with hyaluron and supplied with a differentiation medium. The scaffold and the cells are assumed to be contained in a bioreactor with fluid perfusion. The differentiation of hMSCs into chondrocytes favors the production of extracellular matrix (ECM) and is influenced by fluid stress. The model takes deformations of ECM and PET scaffold into account. The scaffold structure is explicitly included by statistical assessment of the fibre distribution from CT images. The effective macroscopic equations are obtained by appropriate upscaling from dynamics on lower (microscopic and mesoscopic) scales and feature in the motility terms an explicit cell diffusion tensor encoding the assessed anisotropic scaffold structure. Numerical simulations show its influence on the overall cell and tissue dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05909v1</guid>
      <category>q-bio.TO</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>q-bio.CB</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elise Grosjean, Alex Keilmann, Henry J\"ager, Shimi Mohanan, Claudia Redenbach, Bernd Simeon, Christina Surulescu, Luisa de Roy, Andreas Seitz, Graciosa Teixeira, Martin Dauner, Carsten Linti, G\"unter Schmidt</dc:creator>
    </item>
    <item>
      <title>Asymptotic behavior of unstable perturbations of the Fubini-Study metric in Ricci flow</title>
      <link>https://arxiv.org/abs/2403.06427</link>
      <description>arXiv:2403.06427v1 Announce Type: cross 
Abstract: Kr\"oncke has shown that the Fubini-Study metric is an unstable generalized stationary solution of Ricci flow [Kr\"o20]. In this paper, we carry out numerical simulations which indicate that Ricci flow solutions originating at unstable perturbations of the Fubini-Study metric develop local singularities modeled by the blowdown soliton discovered in [FIK03].</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06427v1</guid>
      <category>math.DG</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Garfinkle, James Isenberg, Dan Knopf, Haotian Wu</dc:creator>
    </item>
    <item>
      <title>On infinite matrices</title>
      <link>https://arxiv.org/abs/2403.06445</link>
      <description>arXiv:2403.06445v1 Announce Type: cross 
Abstract: We consider linear bounded operators acting in Banach spaces with a basis, such operators can be represented by an infinite matrix. We prove that for an invertible operator there exists a sequence of invertible finite-dimensional operators so that the family of norms of their inverses is uniformly bounded. It leads to the fact that solutions of finite-dimensional equations converge to the solution of initial operator equation with infinite-dimensional matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06445v1</guid>
      <category>math.FA</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Vasilyev, Vladimir Vasilyev, Abu Bakarr Kamanda Bongay</dc:creator>
    </item>
    <item>
      <title>A preconditioning for the spectral solution of incompressible variable-density flows</title>
      <link>https://arxiv.org/abs/2403.06654</link>
      <description>arXiv:2403.06654v1 Announce Type: cross 
Abstract: In the present study, the efficiency of preconditioners for solving linear systems associated with the discretized variable-density incompressible Navier-Stokes equations with semiimplicit second-order accuracy in time and spectral accuracy in space is investigated. The method, in which the inverse operator for the constant-density flow system acts as preconditioner, is implemented for three iterative solvers: the General Minimal Residual, the Conjugate Gradient and the Richardson Minimal Residual. We discuss the method, first, in the context of the one-dimensional flow case where a top-hat like profile for the density is used. Numerical evidence shows that the convergence is significantly improved due to the notable decrease in the condition number of the operators. Most importantly, we then validate the robustness and convergence properties of the method on two more realistic problems: the two-dimensional Rayleigh-Taylor instability problem and the three-dimensional variable-density swirling jet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06654v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.compfluid.2023.106024</arxiv:DOI>
      <arxiv:journal_reference>Computers and Fluids, 266, pp.106024</arxiv:journal_reference>
      <dc:creator>L. ReynierLMFA, Bastien Di PierroLMFA, Fr\'ed\'eric AlizardLMFA, Anne CadiouLMFA, Lionel Le PenvenLMFA, Marc BuffatLMFA</dc:creator>
    </item>
    <item>
      <title>Efficient first-order algorithms for large-scale, non-smooth maximum entropy models with application to wildfire science</title>
      <link>https://arxiv.org/abs/2403.06816</link>
      <description>arXiv:2403.06816v1 Announce Type: cross 
Abstract: Maximum entropy (Maxent) models are a class of statistical models that use the maximum entropy principle to estimate probability distributions from data. Due to the size of modern data sets, Maxent models need efficient optimization algorithms to scale well for big data applications. State-of-the-art algorithms for Maxent models, however, were not originally designed to handle big data sets; these algorithms either rely on technical devices that may yield unreliable numerical results, scale poorly, or require smoothness assumptions that many practical Maxent models lack. In this paper, we present novel optimization algorithms that overcome the shortcomings of state-of-the-art algorithms for training large-scale, non-smooth Maxent models. Our proposed first-order algorithms leverage the Kullback-Leibler divergence to train large-scale and non-smooth Maxent models efficiently. For Maxent models with discrete probability distribution of $n$ elements built from samples, each containing $m$ features, the stepsize parameters estimation and iterations in our algorithms scale on the order of $O(mn)$ operations and can be trivially parallelized. Moreover, the strong $\ell_{1}$ convexity of the Kullback--Leibler divergence allows for larger stepsize parameters, thereby speeding up the convergence rate of our algorithms. To illustrate the efficiency of our novel algorithms, we consider the problem of estimating probabilities of fire occurrences as a function of ecological features in the Western US MTBS-Interagency wildfire data set. Our numerical results show that our algorithms outperform the state of the arts by one order of magnitude and yield results that agree with physical models of wildfire occurrence and previous statistical analyses of wildfire drivers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06816v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel P. Langlois, Jatan Buch, J\'er\^ome Darbon</dc:creator>
    </item>
    <item>
      <title>Error analysis for parabolic optimal control problems with measure data in a nonconvex polygonal domain</title>
      <link>https://arxiv.org/abs/2112.06432</link>
      <description>arXiv:2112.06432v2 Announce Type: replace 
Abstract: This paper considers the finite element approximation to parabolic optimal control problems with measure data in a nonconvex polygonal domain. Such problems usually possess low regularity in the state variable due to the presence of measure data and the nonconvex nature of the domain. The low regularity of the solution allows the finite element approximations to converge at lower orders. We prove the existence, uniqueness and regularity results for the solution to the control problem satisfying the first order optimality condition. For our error analysis we have used piecewise linear elements for the approximation of the state and co-state variables, whereas piecewise constant functions are employed to approximate the control variable. The temporal discretization is based on the implicit Euler scheme. We derive both a priori and a posteriori error bounds for the state, control and co-state variables. Numerical experiments are performed to validate the theoretical rates of convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.06432v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratibha Shakya</dc:creator>
    </item>
    <item>
      <title>A systematic approach to Lyapunov analyses of continuous-time models in convex optimization</title>
      <link>https://arxiv.org/abs/2205.12772</link>
      <description>arXiv:2205.12772v2 Announce Type: replace 
Abstract: First-order methods are often analyzed via their continuous-time models, where their worst-case convergence properties are usually approached via Lyapunov functions. In this work, we provide a systematic and principled approach to find and verify Lyapunov functions for classes of ordinary and stochastic differential equations. More precisely, we extend the performance estimation framework, originally proposed by Drori and Teboulle [10], to continuous-time models. We retrieve convergence results comparable to those of discrete methods using fewer assumptions and convexity inequalities, and provide new results for stochastic accelerated gradient flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.12772v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>SIAM Journal on Optimization, 2023, 33 (3), pp.1558-1586. \&amp;\#x27E8;10.1137/22M1498486\&amp;\#x27E9</arxiv:journal_reference>
      <dc:creator>C\'eline MoucerSIERRA, ENPC, Adrien TaylorSIERRA, Francis BachSIERRA</dc:creator>
    </item>
    <item>
      <title>An unconditionally stable space-time isogeometric method for the acoustic wave equation</title>
      <link>https://arxiv.org/abs/2303.07268</link>
      <description>arXiv:2303.07268v4 Announce Type: replace 
Abstract: We study space--time isogeometric discretizations of the linear acoustic wave equation that use splines of arbitrary degree p, both in space and time. We propose a space--time variational formulation that is obtained by adding a non-consistent penalty term of order 2p+2 to the bilinear form coming from integration by parts. This formulation, when discretized with tensor-product spline spaces with maximal regularity in time, is unconditionally stable: the mesh size in time is not constrained by the mesh size in space. We give extensive numerical evidence for the good stability, approximation, dissipation and dispersion properties of the stabilized isogeometric formulation, comparing against stabilized finite element schemes, for a range of wave propagation problems with constant and variable wave speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07268v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Fraschini, Gabriele Loli, Andrea Moiola, Giancarlo Sangalli</dc:creator>
    </item>
    <item>
      <title>Sparse Cholesky Factorization for Solving Nonlinear PDEs via Gaussian Processes</title>
      <link>https://arxiv.org/abs/2304.01294</link>
      <description>arXiv:2304.01294v3 Announce Type: replace 
Abstract: In recent years, there has been widespread adoption of machine learning-based approaches to automate the solving of partial differential equations (PDEs). Among these approaches, Gaussian processes (GPs) and kernel methods have garnered considerable interest due to their flexibility, robust theoretical guarantees, and close ties to traditional methods. They can transform the solving of general nonlinear PDEs into solving quadratic optimization problems with nonlinear, PDE-induced constraints. However, the complexity bottleneck lies in computing with dense kernel matrices obtained from pointwise evaluations of the covariance kernel, and its \textit{partial derivatives}, a result of the PDE constraint and for which fast algorithms are scarce.
  The primary goal of this paper is to provide a near-linear complexity algorithm for working with such kernel matrices. We present a sparse Cholesky factorization algorithm for these matrices based on the near-sparsity of the Cholesky factor under a novel ordering of pointwise and derivative measurements. The near-sparsity is rigorously justified by directly connecting the factor to GP regression and exponential decay of basis functions in numerical homogenization. We then employ the Vecchia approximation of GPs, which is optimal in the Kullback-Leibler divergence, to compute the approximate factor. This enables us to compute $\epsilon$-approximate inverse Cholesky factors of the kernel matrices with complexity $O(N\log^d(N/\epsilon))$ in space and $O(N\log^{2d}(N/\epsilon))$ in time. We integrate sparse Cholesky factorizations into optimization algorithms to obtain fast solvers of the nonlinear PDE. We numerically illustrate our algorithm's near-linear space/time complexity for a broad class of nonlinear PDEs such as the nonlinear elliptic, Burgers, and Monge-Amp\`ere equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01294v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Chen, Houman Owhadi, Florian Sch\"afer</dc:creator>
    </item>
    <item>
      <title>Numerical approximation of the solution of Koiter's model for an elliptic membrane shell subjected to an obstacle via the penalty method</title>
      <link>https://arxiv.org/abs/2304.07671</link>
      <description>arXiv:2304.07671v3 Announce Type: replace 
Abstract: This paper is devoted to the analysis of a numerical scheme based on the Finite Element Method for approximating the solution of Koiter's model for a linearly elastic elliptic membrane shell subjected to remaining confined in a prescribed half-space. First, we show that the solution of the obstacle problem under consideration is uniquely determined and satisfies a set of variational inequalities which are governed by a fourth order elliptic operator, and which are posed over a non-empty, closed, and convex subset of a suitable space. Second, we show that the solution of the obstacle problem under consideration can be approximated by means of the penalty method. Third, we show that the solution of the corresponding penalised problem is more regular up to the boundary. Fourth, we write down the mixed variational formulation corresponding to the penalised problem under consideration, and we show that the solution of the mixed variational formulation is more regular up to the boundary as well. In view of this result concerning the augmentation of the regularity of the solution of the mixed penalised problem, we are able to approximate the solution of the one such problem by means of a Finite Element scheme. Finally, we present numerical experiments corroborating the validity of the mathematical results we obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.07671v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Peng, Paolo Piersanti, Xiaoqin Shen</dc:creator>
    </item>
    <item>
      <title>Automated Importance Sampling via Optimal Control for Stochastic Reaction Networks: A Markovian Projection-based Approach</title>
      <link>https://arxiv.org/abs/2306.02660</link>
      <description>arXiv:2306.02660v2 Announce Type: replace 
Abstract: We propose a novel alternative approach to our previous work (Ben Hammouda et al., 2023) to improve the efficiency of Monte Carlo (MC) estimators for rare event probabilities for stochastic reaction networks (SRNs). In the same spirit of (Ben Hammouda et al., 2023), an efficient path-dependent measure change is derived based on a connection between determining optimal importance sampling (IS) parameters within a class of probability measures and a stochastic optimal control formulation, corresponding to solving a variance minimization problem. In this work, we propose a novel approach to address the encountered curse of dimensionality by mapping the problem to a significantly lower-dimensional space via a Markovian projection (MP) idea. The output of this model reduction technique is a low-dimensional SRN (potentially even one dimensional) that preserves the marginal distribution of the original high-dimensional SRN system. The dynamics of the projected process are obtained by solving a related optimization problem via a discrete $L^2$ regression. By solving the resulting projected Hamilton-Jacobi-Bellman (HJB) equations for the reduced-dimensional SRN, we obtain projected IS parameters, which are then mapped back to the original full-dimensional SRN system, resulting in an efficient IS-MC estimator for rare events probabilities of the full-dimensional SRN. Our analysis and numerical experiments reveal that the proposed MP-HJB-IS approach substantially reduces the MC estimator variance, resulting in a lower computational complexity in the rare event regime than standard MC estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.02660v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>q-bio.MN</category>
      <category>q-bio.QM</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cam.2024.115853</arxiv:DOI>
      <dc:creator>Chiheb Ben Hammouda, Nadhir Ben Rached, Ra\'ul Tempone, Sophia Wiechert</dc:creator>
    </item>
    <item>
      <title>Generalized Pseudospectral Shattering and Inverse-Free Matrix Pencil Diagonalization</title>
      <link>https://arxiv.org/abs/2306.03700</link>
      <description>arXiv:2306.03700v4 Announce Type: replace 
Abstract: We present a randomized, inverse-free algorithm for producing an approximate diagonalization of any $n \times n$ matrix pencil $(A,B)$. The bulk of the algorithm rests on a randomized divide-and-conquer eigensolver for the generalized eigenvalue problem originally proposed by Ballard, Demmel, and Dumitriu [Technical Report 2010]. We demonstrate that this divide-and-conquer approach can be formulated to succeed with high probability provided the input pencil is sufficiently well-behaved, which is accomplished by generalizing the recent pseudospectral shattering work of Banks, Garza-Vargas, Kulkarni, and Srivastava [Foundations of Computational Mathematics 2022]. In particular, we show that perturbing and scaling $(A,B)$ regularizes its pseudospectra, allowing divide-and-conquer to run over a simple random grid and in turn producing an accurate diagonalization of $(A,B)$ in the backward error sense. The main result of the paper states the existence of a randomized algorithm that with high probability (and in exact arithmetic) produces invertible $S,T$ and diagonal $D$ such that $||A - SDT^{-1}||_2 \leq \varepsilon$ and $||B - ST^{-1}||_2 \leq \varepsilon$ in at most $O \left(\log^2 \left( \frac{n}{\varepsilon} \right) T_{\text{MM}}(n) \right)$ operations, where $T_{\text{MM}}(n)$ is the asymptotic complexity of matrix multiplication. This not only provides a new set of guarantees for highly parallel generalized eigenvalue solvers but also establishes nearly matrix multiplication time as an upper bound on the complexity of inverse-free, exact arithmetic matrix pencil diagonalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.03700v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>James Demmel, Ioana Dumitriu, Ryan Schneider</dc:creator>
    </item>
    <item>
      <title>Minimal Convex Environmental Contours</title>
      <link>https://arxiv.org/abs/2308.01753</link>
      <description>arXiv:2308.01753v2 Announce Type: replace 
Abstract: We develop a numerical method for the computation of a minimal convex and compact set, $\mathcal{B}\subset\mathbb{R}^N$, in the sense of mean width. This minimisation is constrained by the requirement that $\max_{b\in\mathcal{B}}\langle b , u\rangle\geq C(u)$ for all unit vectors $u\in S^{N-1}$ given some Lipschitz function $C$.
  This problem arises in the construction of environmental contours under the assumption of convex failure sets. Environmental contours offer descriptions of extreme environmental conditions commonly applied for reliability analysis in the early design phase of marine structures. Usually, they are applied in order to reduce the number of computationally expensive response analyses needed for reliability estimation.
  We solve this problem by reformulating it as a linear programming problem. Rigorous convergence analysis is performed, both in terms of convergence of mean widths and in the sense of the Hausdorff metric. Additionally, numerical examples are provided to illustrate the presented methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01753v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\AA}smund Hausken Sande, Johan S. Wind</dc:creator>
    </item>
    <item>
      <title>Sparse-grid sampling recovery and numerical integration of functions having mixed smoothness</title>
      <link>https://arxiv.org/abs/2309.04994</link>
      <description>arXiv:2309.04994v3 Announce Type: replace 
Abstract: We give a short survey of recent results on sparse-grid linear algorithms of approximate recovery and integration of functions possessing a unweighted or weighted Sobolev mixed smoothness based on their sampled values at a certain finite set. Some of them are extended to more general cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04994v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dinh D\~ung</dc:creator>
    </item>
    <item>
      <title>DynAMO: Multi-agent reinforcement learning for dynamic anticipatory mesh optimization with applications to hyperbolic conservation laws</title>
      <link>https://arxiv.org/abs/2310.01695</link>
      <description>arXiv:2310.01695v2 Announce Type: replace 
Abstract: We introduce DynAMO, a reinforcement learning paradigm for Dynamic Anticipatory Mesh Optimization. Adaptive mesh refinement is an effective tool for optimizing computational cost and solution accuracy in numerical methods for partial differential equations. However, traditional adaptive mesh refinement approaches for time-dependent problems typically rely only on instantaneous error indicators to guide adaptivity. As a result, standard strategies often require frequent remeshing to maintain accuracy. In the DynAMO approach, multi-agent reinforcement learning is used to discover new local refinement policies that can anticipate and respond to future solution states by producing meshes that deliver more accurate solutions for longer time intervals. By applying DynAMO to discontinuous Galerkin methods for the linear advection and compressible Euler equations in two dimensions, we demonstrate that this new mesh refinement paradigm can outperform conventional threshold-based strategies while also generalizing to different mesh sizes, remeshing and simulation times, and initial conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01695v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tarik Dzanic, Ketan Mittal, Dohyun Kim, Jiachen Yang, Socratis Petrides, Brendan Keith, Robert Anderson</dc:creator>
    </item>
    <item>
      <title>Quantitative passive imaging by iterative holography: The example of helioseismic holography</title>
      <link>https://arxiv.org/abs/2310.03837</link>
      <description>arXiv:2310.03837v2 Announce Type: replace 
Abstract: In passive imaging, one attempts to reconstruct some coefficients in a wave equation from correlations of observed randomly excited solutions to this wave equation. Many methods proposed for this class of inverse problem so far are only qualitative, e.g., trying to identify the support of a perturbation. Major challenges are the increase in dimensionality when computing correlations from primary data in a preprocessing step, and often very poor pointwise signal-to-noise ratios. In this paper, we propose an approach that addresses both of these challenges: It works only on the primary data while implicitly using the full information contained in the correlation data, and it provides quantitative estimates and convergence by iteration.
  Our work is motivated by helioseismic holography, a well-established imaging method to map heterogenities and flows in the solar interior. We show that the back-propagation used in classical helioseismic holography can be interpreted as the adjoint of the Fr\'echet derivative of the operator which maps the properties of the solar interior to the correlation data on the solar surface. The theoretical and numerical framework for passive imaging problems developed in this paper extends helioseismic holography to nonlinear problems and allows for quantitative reconstructions. We present a proof of concept in uniform media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03837v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1361-6420/ad2b9a</arxiv:DOI>
      <arxiv:journal_reference>Inverse Problems, 2024, 40, 045016</arxiv:journal_reference>
      <dc:creator>Bj\"orn M\"uller, Thorsten Hohage, Damien Fournier, Laurent Gizon</dc:creator>
    </item>
    <item>
      <title>On the Optimality of CVOD-based Column Selection</title>
      <link>https://arxiv.org/abs/2403.00121</link>
      <description>arXiv:2403.00121v2 Announce Type: replace 
Abstract: While there exists a rich array of matrix column subset selection problem (CSSP) algorithms for use with interpolative and CUR-type decompositions, their use can often become prohibitive as the size of the input matrix increases. In an effort to address these issues, the authors in \cite{emelianenko2024adaptive} developed a general framework that pairs a column-partitioning routine with a column-selection algorithm. Two of the four algorithms presented in that work paired the Centroidal Voronoi Orthogonal Decomposition (\textsf{CVOD}) and an adaptive variant (\textsf{adaptCVOD}) with the Discrete Empirical Interpolation Method (\textsf{DEIM}) \cite{sorensen2016deim}. In this work, we extend this framework and pair the \textsf{CVOD}-type algorithms with any CSSP algorithm that returns linearly independent columns. Our results include detailed error bounds for the solutions provided by these paired algorithms, as well as expressions that explicitly characterize how the quality of the selected column partition affects the resulting CSSP solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00121v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Emelianenko, Guy B. Oldaker IV</dc:creator>
    </item>
    <item>
      <title>Sample Efficient Learning of Factored Embeddings of Tensor Fields</title>
      <link>https://arxiv.org/abs/2209.00372</link>
      <description>arXiv:2209.00372v2 Announce Type: replace-cross 
Abstract: Data tensors of orders 2 and greater are now routinely being generated. These data collections are increasingly huge and growing. Many scientific and medical data tensors are tensor fields (e.g., images, videos, geographic data) in which the spatial neighborhood contains important information. Directly accessing such large data tensor collections for information has become increasingly prohibitive. We learn approximate full-rank and compact tensor sketches with decompositive representations providing compact space, time and spectral embeddings of tensor fields. All information querying and post-processing on the original tensor field can now be achieved more efficiently and with customizable accuracy as they are performed on these compact factored sketches in latent generative space. We produce optimal rank-r sketchy Tucker decomposition of arbitrary order data tensors by building compact factor matrices from a sample-efficient sub-sampling of tensor slices. Our sample efficient policy is learned via an adaptable stochastic Thompson sampling using Dirichlet distributions with conjugate priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00372v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taemin Heo, Chandrajit Bajaj</dc:creator>
    </item>
    <item>
      <title>The evolution of turbulence theories and the need for continuous wavelets</title>
      <link>https://arxiv.org/abs/2209.01808</link>
      <description>arXiv:2209.01808v2 Announce Type: replace-cross 
Abstract: In the first part of this article, I summarise two centuries of research on turbulence. I also critically discuss some of the interpretations that are still in use, as turbulence remains an inherently non-linear problem that is still unsolved to this day. In the second part, I tell the story of how Alex Grossmann introduced me to the continuous wavelet representation in 1983, and how he instantly convinced me that this is the tool I was looking for to study turbulence. In the third part, I present a selection of results I obtained in collaboration with several students and colleagues to represent, analyse and filter different turbulent flows using the continuous wavelet transform. I have chosen to present both these theories and results without the use of equations, in the hope that the reading of this article will be more enjoyable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.01808v2</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marie Farge</dc:creator>
    </item>
    <item>
      <title>Differential-Equation Constrained Optimization With Stochasticity</title>
      <link>https://arxiv.org/abs/2305.04024</link>
      <description>arXiv:2305.04024v2 Announce Type: replace-cross 
Abstract: Most inverse problems from physical sciences are formulated as PDE-constrained optimization problems. This involves identifying unknown parameters in equations by optimizing the model to generate PDE solutions that closely match measured data. The formulation is powerful and widely used in many sciences and engineering fields. However, one crucial assumption is that the unknown parameter must be deterministic. In reality, however, many problems are stochastic in nature, and the unknown parameter is random. The challenge then becomes recovering the full distribution of this unknown random parameter. It is a much more complex task. In this paper, we examine this problem in a general setting. In particular, we conceptualize the PDE solver as a push-forward map that pushes the parameter distribution to the generated data distribution. This way, the SDE-constrained optimization translates to minimizing the distance between the generated distribution and the measurement distribution. We then formulate a gradient-flow equation to seek the ground-truth parameter probability distribution. This opens up a new paradigm for extending many techniques in PDE-constrained optimization to that for systems with stochasticity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04024v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qin Li, Li Wang, Yunan Yang</dc:creator>
    </item>
    <item>
      <title>Score Operator Newton transport</title>
      <link>https://arxiv.org/abs/2305.09792</link>
      <description>arXiv:2305.09792v3 Announce Type: replace-cross 
Abstract: We propose a new approach for sampling and Bayesian computation that uses the score of the target distribution to construct a transport from a given reference distribution to the target. Our approach is an infinite-dimensional Newton method, involving a linear PDE, for finding a zero of a ``score-residual'' operator. We prove sufficient conditions for convergence to a valid transport map. Our Newton iterates can be computed by exploiting fast solvers for elliptic PDEs, resulting in new algorithms for Bayesian inference and other sampling tasks. We identify elementary settings where score-operator Newton transport achieves fast convergence while avoiding mode collapse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.09792v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nisha Chandramoorthy, Florian Schaefer, Youssef Marzouk</dc:creator>
    </item>
    <item>
      <title>Error Feedback Can Accurately Compress Preconditioners</title>
      <link>https://arxiv.org/abs/2306.06098</link>
      <description>arXiv:2306.06098v4 Announce Type: replace-cross 
Abstract: Leveraging second-order information about the loss at the scale of deep networks is one of the main lines of approach for improving the performance of current optimizers for deep learning. Yet, existing approaches for accurate full-matrix preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate Curvature (M-FAC) suffer from massive storage costs when applied even to small-scale models, as they must store a sliding window of gradients, whose memory requirements are multiplicative in the model dimension. In this paper, we address this issue via a novel and efficient error-feedback technique that can be applied to compress preconditioners by up to two orders of magnitude in practice, without loss of convergence. Specifically, our approach compresses the gradient information via sparsification or low-rank compression \emph{before} it is fed into the preconditioner, feeding the compression error back into future iterations. Experiments on deep neural networks show that this approach can compress full-matrix preconditioners to up to 99\% sparsity without accuracy loss, effectively removing the memory overhead of full-matrix preconditioners such as GGT and M-FAC. Our code is available at \url{https://github.com/IST-DASLab/EFCP}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06098v4</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ionut-Vlad Modoranu, Aleksei Kalinov, Eldar Kurtic, Elias Frantar, Dan Alistarh</dc:creator>
    </item>
    <item>
      <title>Sampling via Gradient Flows in the Space of Probability Measures</title>
      <link>https://arxiv.org/abs/2310.03597</link>
      <description>arXiv:2310.03597v3 Announce Type: replace-cross 
Abstract: Sampling a target probability distribution with an unknown normalization constant is a fundamental challenge in computational science and engineering. Recent work shows that algorithms derived by considering gradient flows in the space of probability measures open up new avenues for algorithm development. This paper makes three contributions to this sampling approach by scrutinizing the design components of such gradient flows. Any instantiation of a gradient flow for sampling needs an energy functional and a metric to determine the flow, as well as numerical approximations of the flow to derive algorithms. Our first contribution is to show that the Kullback-Leibler divergence, as an energy functional, has the unique property (among all f-divergences) that gradient flows resulting from it do not depend on the normalization constant of the target distribution. Our second contribution is to study the choice of metric from the perspective of invariance. The Fisher-Rao metric is known as the unique choice (up to scaling) that is diffeomorphism invariant. As a computationally tractable alternative, we introduce a relaxed, affine invariance property for the metrics and gradient flows. In particular, we construct various affine invariant Wasserstein and Stein gradient flows. Affine invariant gradient flows are shown to behave more favorably than their non-affine-invariant counterparts when sampling highly anisotropic distributions, in theory and by using particle methods. Our third contribution is to study, and develop efficient algorithms based on Gaussian approximations of the gradient flows; this leads to an alternative to particle methods. We establish connections between various Gaussian approximate gradient flows, discuss their relation to gradient methods arising from parametric variational inference, and study their convergence properties both theoretically and numerically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03597v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Chen, Daniel Zhengyu Huang, Jiaoyang Huang, Sebastian Reich, Andrew M Stuart</dc:creator>
    </item>
  </channel>
</rss>
