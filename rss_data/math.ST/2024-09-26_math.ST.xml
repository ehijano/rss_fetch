<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Sep 2024 04:01:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Is speckle noise more challenging to mitigate than additive noise?</title>
      <link>https://arxiv.org/abs/2409.16585</link>
      <description>arXiv:2409.16585v1 Announce Type: new 
Abstract: We study the problem of estimating a function in the presence of both speckle and additive noises. Although additive noise has been thoroughly explored in nonparametric estimation, speckle noise, prevalent in applications such as synthetic aperture radar, ultrasound imaging, and digital holography, has not received as much attention. Consequently, there is a lack of theoretical investigations into the fundamental limits of mitigating the speckle noise.
  This paper is the first step in filling this gap. Our focus is on investigating the minimax estimation error for estimating a $\beta$-H\"older continuous function and determining the rate of the minimax risk. Specifically, if $n$ represents the number of data points, $f$ denotes the underlying function to be estimated, and $\hat{\nu}_n$ is an estimate of $f$, then $\inf_{\hat{\nu}_n} \sup_f \mathbb{E}_f\| \hat{\nu}_n - f \|^2_2$ decays at the rate $n^{-\frac{2\beta}{2\beta+1}}$. Interestingly, this rate is identical to the one achieved for mitigating additive noise when the noise's variance is $\Theta(1)$.
  To validate the accuracy of our minimax upper bounds, we implement the minimax optimal algorithms on simulated data and employ Monte Carlo simulations to characterize their exact risk. Our simulations closely mirror the expected behaviors in decay rate as per our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16585v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reihaneh Malekian, Arian Maleki</dc:creator>
    </item>
    <item>
      <title>Limiting Spectral Distribution of a Random Commutator Matrix</title>
      <link>https://arxiv.org/abs/2409.16780</link>
      <description>arXiv:2409.16780v1 Announce Type: new 
Abstract: We study the spectral properties of a class of random matrices of the form $S_n^{-} = n^{-1}(X_1 X_2^* - X_2 X_1^*)$ where $X_k = \Sigma^{1/2}Z_k$, for $k=1,2$, $Z_k$'s are independent $p\times n$ complex-valued random matrices, and $\Sigma$ is a $p\times p$ positive semi-definite matrix, independent of the $Z_k$'s. We assume that $Z_k$'s have independent entries with zero mean and unit variance. The skew-symmetric/skew-Hermitian matrix $S_n^{-}$ will be referred to as a random commutator matrix associated with the samples $X_1$ and $X_2$. We show that, when the dimension $p$ and sample size $n$ increase simultaneously, so that $p/n \to c \in (0,\infty)$, there exists a limiting spectral distribution (LSD) for $S_n^{-}$, supported on the imaginary axis, under the assumptions that the spectral distribution of $\Sigma$ converges weakly and the entries of $Z_k$'s have moments of sufficiently high order. This nonrandom LSD can be described through its Stieltjes transform, which satisfies a coupled Mar\v{c}enko-Pastur-type functional equations. In the special case when $\Sigma = I_p$, we show that the LSD of $S_n^{-}$ is a mixture of a degenerate distribution at zero (with positive mass if $c &gt; 2$), and a continuous distribution with a symmetric density function supported on a compact interval on the imaginary axis. Moreover, we show that the companion matrix $S_n^{+} = \Sigma_n^\frac{1}{2}(Z_1Z_2^* + Z_2Z_1^*)\Sigma_n^\frac{1}{2}$, under identical assumptions, has an LSD supported on the real line, which can be similarly characterized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16780v1</guid>
      <category>math.ST</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javed Hazarika, Debashis Paul</dc:creator>
    </item>
    <item>
      <title>Asymptotically efficient estimators for tail probabilities of extremals of $\beta$-Jacobi ensembles</title>
      <link>https://arxiv.org/abs/2409.16868</link>
      <description>arXiv:2409.16868v1 Announce Type: new 
Abstract: In this paper, we consider the tail probabilities of extremals of $\beta$-Jacobi ensemble which plays an important role in multivariate analysis. The key steps in constructing estimators rely on the rate functions of large deviations. Therefore, under specific conditions, we consider stretching and shifting transformations applied to the $\beta$-Jacobi ensemble to ensure that its extremals satisfy the large deviations. The estimator we construct characterize the large deviation behavior and moderate deviation behavior of extremals under different assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16868v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutao Ma, Siyu Wang</dc:creator>
    </item>
    <item>
      <title>Gaussian Processes for Observational Dose-Response Inference</title>
      <link>https://arxiv.org/abs/2409.17043</link>
      <description>arXiv:2409.17043v1 Announce Type: new 
Abstract: We adapt Gaussian processes for estimating the average dose-response function in observational settings, introducing a powerful complement to treatment effect estimation for understanding heterogeneous effects. We incorporate samples from a Gaussian process posterior for the propensity score into a Gaussian process response model using Girard's approach to integrating over uncertainty in training data. We show Girard's method admits a positive-definite kernel, and provide theoretical justification by identifying it with an inner product of kernel mean embeddings. We demonstrate double robustness of our approach under a misspecified response function or propensity score. We characterize and mitigate regularization-induced confounding in Gaussian process response models. We show improvement over other methods for average dose-response function estimation in terms of coverage of the dose-response function and estimation bias, with less sensitivity to misspecification across experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17043v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jake R. Dailey</dc:creator>
    </item>
    <item>
      <title>Double-Estimation-Friendly Inference for High Dimensional Misspecified Measurement Error Models</title>
      <link>https://arxiv.org/abs/2409.16463</link>
      <description>arXiv:2409.16463v1 Announce Type: cross 
Abstract: In this paper, we introduce an innovative testing procedure for assessing individual hypotheses in high-dimensional linear regression models with measurement errors. This method remains robust even when either the X-model or Y-model is misspecified. We develop a double robust score function that maintains a zero expectation if one of the models is incorrect, and we construct a corresponding score test. We first show the asymptotic normality of our approach in a low-dimensional setting, and then extend it to the high-dimensional models. Our analysis of high-dimensional settings explores scenarios both with and without the sparsity condition, establishing asymptotic normality and non-trivial power performance under local alternatives. Simulation studies and real data analysis demonstrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16463v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Cui, Xu Guo, Runze Li, Songshan Yang, Zhe Zhang</dc:creator>
    </item>
    <item>
      <title>BSDEs driven by G-Brownian motion with time-varying uniformly continuous generators</title>
      <link>https://arxiv.org/abs/2409.16574</link>
      <description>arXiv:2409.16574v1 Announce Type: cross 
Abstract: In this paper, we study the backward stochastic differential equations driven by G-Brownian motion under the condition that the generator is time-varying Lipschitz continuous with respect to y and time-varying uniformly continuous with respect to z. With the help of linearization method and the G-stochastic analysis techniques, we construct the approximating sequences of G-BSDE and obtain some precise a priori estimates. By combining this with the approximation method, we prove the existence and uniqueness of the solution under the time-varying conditions, as well as the comparison theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16574v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingru Zhao</dc:creator>
    </item>
    <item>
      <title>Robust Max Statistics for High-Dimensional Inference</title>
      <link>https://arxiv.org/abs/2409.16683</link>
      <description>arXiv:2409.16683v1 Announce Type: cross 
Abstract: Although much progress has been made in the theory and application of bootstrap approximations for max statistics in high dimensions, the literature has largely been restricted to cases involving light-tailed data. To address this issue, we propose an approach to inference based on robust max statistics, and we show that their distributions can be accurately approximated via bootstrapping when the data are both high-dimensional and heavy-tailed. In particular, the data are assumed to satisfy an extended version of the well-established $L^{4}$-$L^2$ moment equivalence condition, as well as a weak variance decay condition. In this setting, we show that near-parametric rates of bootstrap approximation can be achieved in the Kolmogorov metric, independently of the data dimension. Moreover, this theoretical result is complemented by favorable empirical results involving both synthetic data and an application to financial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16683v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingshuo Liu, Miles E. Lopes</dc:creator>
    </item>
    <item>
      <title>Quasi Maximum Likelihood Estimation and Inference of Large Approximate Dynamic Factor Models via the EM algorithm</title>
      <link>https://arxiv.org/abs/1910.03821</link>
      <description>arXiv:1910.03821v5 Announce Type: replace 
Abstract: We study estimation of large Dynamic Factor models implemented through the Expectation Maximization (EM) algorithm, jointly with the Kalman smoother. We prove that as both the cross-sectional dimension, $n$, and the sample size, $T$, diverge to infinity: (i) the estimated loadings are $\sqrt T$-consistent, asymptotically normal and equivalent to their Quasi Maximum Likelihood estimates; (ii) the estimated factors are $\sqrt n$-consistent, asymptotically normal and equivalent to their Weighted Least Squares estimates. Moreover, the estimated loadings are asymptotically as efficient as those obtained by Principal Components analysis, while the estimated factors are more efficient if the idiosyncratic covariance is sparse enough.</description>
      <guid isPermaLink="false">oai:arXiv.org:1910.03821v5</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Matteo Luciani</dc:creator>
    </item>
    <item>
      <title>The loss landscape of deep linear neural networks: a second-order analysis</title>
      <link>https://arxiv.org/abs/2107.13289</link>
      <description>arXiv:2107.13289v3 Announce Type: replace 
Abstract: We study the optimization landscape of deep linear neural networks with the square loss. It is known that, under weak assumptions, there are no spurious local minima and no local maxima. However, the existence and diversity of non-strict saddle points, which can play a role in first-order algorithms' dynamics, have only been lightly studied. We go a step further with a full analysis of the optimization landscape at order 2. We characterize, among all critical points, which are global minimizers, strict saddle points, and non-strict saddle points. We enumerate all the associated critical values. The characterization is simple, involves conditions on the ranks of partial matrix products, and sheds some light on global convergence or implicit regularization that have been proved or observed when optimizing linear neural networks. In passing, we provide an explicit parameterization of the set of all global minimizers and exhibit large sets of strict and non-strict saddle points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.13289v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research, 2024, 25 (242), pp.1-76</arxiv:journal_reference>
      <dc:creator>El Mehdi Achour (IMT), Fran\c{c}ois Malgouyres (IMT), S\'ebastien Gerchinovitz (IMT)</dc:creator>
    </item>
    <item>
      <title>Metric Entropy-Free Sample Complexity Bounds for Sample Average Approximation in Convex Stochastic Programming</title>
      <link>https://arxiv.org/abs/2401.00664</link>
      <description>arXiv:2401.00664v4 Announce Type: replace-cross 
Abstract: This paper studies sample average approximation (SAA) in solving convex or strongly convex stochastic programming (SP) problems. Under some common regularity conditions, we show -- perhaps for the first time -- that SAA's sample complexity can be completely free from any quantification of metric entropy (such as the logarithm of the covering number), leading to a significantly more efficient rate with dimensionality $d$ than most existing results. From the newly established complexity bounds, an important revelation is that SAA and the canonical stochastic mirror descent (SMD) method, two mainstream solution approaches to SP, entail almost identical rates of sample efficiency, rectifying a persistent theoretical discrepancy of SAA from SMD by the order of $O(d)$. Furthermore, this paper explores non-Lipschitzian scenarios where SAA maintains provable efficacy but the corresponding results for SMD remain mostly unexplored, indicating the potential of SAA's better applicability in some irregular settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00664v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongcheng Liu, Jindong Tong</dc:creator>
    </item>
    <item>
      <title>Detection and inference of changes in high-dimensional linear regression with non-sparse structures</title>
      <link>https://arxiv.org/abs/2402.06915</link>
      <description>arXiv:2402.06915v3 Announce Type: replace-cross 
Abstract: For data segmentation in high-dimensional linear regression settings, the regression parameters are often assumed to be sparse segment-wise, which enables many existing methods to estimate the parameters locally via $\ell_1$-regularised maximum likelihood-type estimation and then contrast them for change point detection. Contrary to this common practice, we show that the sparsity of neither regression parameters nor their differences, a.k.a. differential parameters, is necessary for consistency in multiple change point detection. In fact, both statistically and computationally, better efficiency is attained by a simple strategy that scans for large discrepancies in local covariance between the regressors and the response. We go a step further and propose a suite of tools for directly inferring about the differential parameters post-segmentation, which are applicable even when the regression parameters themselves are non-sparse. Theoretical investigations are conducted under general conditions permitting non-Gaussianity, temporal dependence and ultra-high dimensionality. Numerical results from simulated and macroeconomic datasets demonstrate the competitiveness and efficacy of the proposed methods. Implementation of all methods is provided in the R package inferchange on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06915v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haeran Cho, Tobias Kley, Housen Li</dc:creator>
    </item>
  </channel>
</rss>
