<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Jun 2024 04:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Ethics of AI in Education</title>
      <link>https://arxiv.org/abs/2406.11842</link>
      <description>arXiv:2406.11842v1 Announce Type: new 
Abstract: The transition of Artificial Intelligence (AI) from a lab-based science to live human contexts brings into sharp focus many historic, socio-cultural biases, inequalities, and moral dilemmas. Many questions that have been raised regarding the broader ethics of AI are also relevant for AI in Education (AIED). AIED raises further specific challenges related to the impact of its technologies on users, how such technologies might be used to reinforce or alter the way that we learn and teach, and what we, as a society and individuals, value as outcomes of education. This chapter discusses key ethical dimensions of AI and contextualises them within AIED design and engineering practices to draw connections between the AIED systems we build, the questions about human learning and development we ask, the ethics of the pedagogies we use, and the considerations of values that we promote in and through AIED within a wider socio-technical system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11842v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.4324/9780429329067</arxiv:DOI>
      <dc:creator>Kaska Porayska-Pomsta, Wayne Holmes, Selena Nemorin</dc:creator>
    </item>
    <item>
      <title>Explanation Hacking: The perils of algorithmic recourse</title>
      <link>https://arxiv.org/abs/2406.11843</link>
      <description>arXiv:2406.11843v1 Announce Type: new 
Abstract: We argue that the trend toward providing users with feasible and actionable explanations of AI decisions, known as recourse explanations, comes with ethical downsides. Specifically, we argue that recourse explanations face several conceptual pitfalls and can lead to problematic explanation hacking, which undermines their ethical status. As an alternative, we advocate that explanations of AI decisions should aim at understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11843v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Emily Sullivan, Atoosa Kasirzadeh</dc:creator>
    </item>
    <item>
      <title>Prompting the E-Brushes: Users as Authors in Generative AI</title>
      <link>https://arxiv.org/abs/2406.11844</link>
      <description>arXiv:2406.11844v1 Announce Type: new 
Abstract: Since its introduction in 2022, Generative AI has significantly impacted the art world, from winning state art fairs to creating complex videos from simple prompts. Amid this renaissance, a pivotal issue emerges: should users of Generative AI be recognized as authors eligible for copyright protection? The Copyright Office, in its March 2023 Guidance, argues against this notion. By comparing the prompts to clients' instructions for commissioned art, the Office denies users authorship due to their limited role in the creative process. This Article challenges this viewpoint and advocates for the recognition of Generative AI users who incorporate these tools into their creative endeavors. It argues that the current policy fails to consider the intricate and dynamic interaction between Generative AI users and the models, where users actively influence the output through a process of adjustment, refinement, selection, and arrangement. Rather than dismissing the contributions generated by AI, this Article suggests a simplified and streamlined registration process that acknowledges the role of AI in creation. This approach not only aligns with the constitutional goal of promoting the progress of science and useful arts but also encourages public engagement in the creative process, which contributes to the pool of training data for AI. Moreover, it advocates for a flexible framework that evolves alongside technological advancements while ensuring safety and public interest. In conclusion, by examining text-to-image generators and addressing misconceptions about Generative AI and user interaction, this Article calls for a regulatory framework that adapts to technological developments and safeguards public interests</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11844v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>International Journal of Law, Ethics, and Technology 2024</arxiv:journal_reference>
      <dc:creator>Yiyang Mei</dc:creator>
    </item>
    <item>
      <title>Decoding the Digital Fine Print: Navigating the potholes in Terms of service/ use of GenAI tools against the emerging need for Transparent and Trustworthy Tech Futures</title>
      <link>https://arxiv.org/abs/2406.11845</link>
      <description>arXiv:2406.11845v1 Announce Type: new 
Abstract: The research investigates the crucial role of clear and intelligible terms of service in cultivating user trust and facilitating informed decision-making in the context of AI, in specific GenAI. It highlights the obstacles presented by complex legal terminology and detailed fine print, which impede genuine user consent and recourse, particularly during instances of algorithmic malfunctions, hazards, damages, or inequities, while stressing the necessity of employing machine-readable terms for effective service licensing. The increasing reliance on General Artificial Intelligence (GenAI) tools necessitates transparent, comprehensible, and standardized terms of use, which facilitate informed decision-making while fostering trust among stakeholders. Despite recent efforts promoting transparency via system and model cards, existing documentation frequently falls short of providing adequate disclosures, leaving users ill-equipped to evaluate potential risks and harms. To address this gap, this research examines key considerations necessary in terms of use or terms of service for Generative AI tools, drawing insights from multiple studies. Subsequently, this research evaluates whether the terms of use or terms of service of prominent Generative AI tools against the identified considerations. Findings indicate inconsistencies and variability in document quality, signaling a pressing demand for uniformity in disclosure practices. Consequently, this study advocates for robust, enforceable standards ensuring complete and intelligible disclosures prior to the release of GenAI tools, thereby empowering end-users to make well-informed choices and enhancing overall accountability in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11845v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sundaraparipurnan Narayanan</dc:creator>
    </item>
    <item>
      <title>Lifecycle of a sub-metered tertiary multi-use (GreEn-ER) building's open energy data: from resource mobilisation to data re-usability</title>
      <link>https://arxiv.org/abs/2406.11846</link>
      <description>arXiv:2406.11846v1 Announce Type: new 
Abstract: The proliferation of sensors in buildings has given us access to more data than before. To shepherd this rise in data, many open data lifecycles have been proposed over the past decade. However, many of the proposed lifecycles do not reflect the necessary complexity in the built environment. In this paper, we present a new open data lifecycle model: Open Energy Data Lifecycle (OPENDAL). OPENDAL builds on the key themes in more popular lifecycles and looks to extend them by better accounting for the information flows between cycles and the interactions between stakeholders around the data. These elements are included in the lifecycle in a bid to increase the reuse of published datasets. In addition, we apply the lifecycle model to the datasets from the GreEn-ER building, a mixed-use education building in France. Different use cases of these datasets are highlighted and discussed as a way to incentivise the use of data by other individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11846v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seun Osonuga, Vincent Imard, Benoit Delinchant, Frederic Wurtz</dc:creator>
    </item>
    <item>
      <title>Integrating behavior analysis with machine learning to predict online learning performance: A scientometric review and empirical study</title>
      <link>https://arxiv.org/abs/2406.11847</link>
      <description>arXiv:2406.11847v1 Announce Type: new 
Abstract: The interest in predicting online learning performance using ML algorithms has been steadily increasing. We first conducted a scientometric analysis to provide a systematic review of research in this area. The findings show that most existing studies apply the ML methods without considering learning behavior patterns, which may compromise the prediction accuracy and precision of the ML methods. This study proposes an integration framework that blends learning behavior analysis with ML algorithms to enhance the prediction accuracy of students' online learning performance. Specifically, the framework identifies distinct learning patterns among students by employing clustering analysis and implements various ML algorithms to predict performance within each pattern. For demonstration, the integration framework is applied to a real dataset from edX and distinguishes two learning patterns, as in, low autonomy students and motivated students. The results show that the framework yields nearly perfect prediction performance for autonomous students and satisfactory performance for motivated students. Additionally, this study compares the prediction performance of the integration framework to that of directly applying ML methods without learning behavior analysis using comprehensive evaluation metrics. The results consistently demonstrate the superiority of the integration framework over the direct approach, particularly when integrated with the best-performing XGBoosting method. Moreover, the framework significantly improves prediction accuracy for the motivated students and for the worst-performing random forest method. This study also evaluates the importance of various learning behaviors within each pattern using LightGBM with SHAP values. The implications of the integration framework and the results for online education practice and future research are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11847v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jin Yuan, Xuelan Qiu, Jinran Wu, Jiesi Guo, Weide Li, You-Gan Wang</dc:creator>
    </item>
    <item>
      <title>Model Of Information System Towards Harmonized Industry And Computer Science</title>
      <link>https://arxiv.org/abs/2406.11848</link>
      <description>arXiv:2406.11848v1 Announce Type: new 
Abstract: The aim of attending an educational institution is learning, which in turn is sought after for the reason of independence of thoughts, ideologies as well as physical and material independence. This physical and material independence is gotten from working in the industry, that is, being a part of the independent working population of the country. There needs to be a way by which students upon graduation can easily adapt to the real world with necessary skills and knowledge required. This problem has been a challenge in some computer science departments, which after effects known after the student begins to work in an industry. The objectives of this project include: Designing a web based chat application for the industry and computer science department, Develop a web based chat application for the industry and computer science and Evaluate the web based chat application for the industry and computer science department. Waterfall system development lifecycle is used in establishing a system project plan, because it gives an overall list of processes and sub-processes required in developing a system. The descriptive research method applied in this project is documentary analysis of previous articles. The result of the project is the design, software a web-based chat application that aids communication between the industry and the computer science department and the evaluation of the system. The application is able to store this information which can be decided to be used later. Awareness of the software to companies and universities, implementation of the suggestions made by the industry in the computer science curriculum, use of this software in universities across Nigeria and use of this not just in the computer science field but in other field of study</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11848v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edafetanure-Ibeh Faith, Evah Patrick Tamarauefiye, Mark Uwuoruya Uyi</dc:creator>
    </item>
    <item>
      <title>What do we know about Computing Education in Africa? A Systematic Review of Computing Education Research Literature</title>
      <link>https://arxiv.org/abs/2406.11849</link>
      <description>arXiv:2406.11849v1 Announce Type: new 
Abstract: Noticeably, Africa is underrepresented in the computing education research (CER) community. However, there has been some effort from the researchers in the region to contribute to the growing need for computing for all. To understand the body of works that emerged from the global south region and their area of focus in computing education, we conducted a systematic review of the literature. This research investigates the prominent CER journals and conferences to discern the kind of research that has been published and how much contribution they have made to the growing field. Of the 68 selected studies, 45 papers were from South Africa. The prominent aspect of computing in the literature is programming, which accounts for 43%. We identified open areas for research in the context and discussed the implication of our findings for the development of CER in Africa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11849v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ismaila Temitayo Sanusi, Fitsum Gizachew Deriba</dc:creator>
    </item>
    <item>
      <title>Closed-loop Teaching via Demonstrations to Improve Policy Transparency</title>
      <link>https://arxiv.org/abs/2406.11850</link>
      <description>arXiv:2406.11850v1 Announce Type: new 
Abstract: Demonstrations are a powerful way of increasing the transparency of AI policies. Though informative demonstrations may be selected a priori through the machine teaching paradigm, student learning may deviate from the preselected curriculum in situ. This paper thus explores augmenting a curriculum with a closed-loop teaching framework inspired by principles from the education literature, such as the zone of proximal development and the testing effect. We utilize tests accordingly to close to the loop and maintain a novel particle filter model of human beliefs throughout the learning process, allowing us to provide demonstrations that are targeted to the human's current understanding in real time. A user study finds that our proposed closed-loop teaching framework reduces the regret in human test responses by 43% over a baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11850v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael S. Lee, Reid Simmons, Henny Admoni</dc:creator>
    </item>
    <item>
      <title>GUARD-D-LLM: An LLM-Based Risk Assessment Engine for the Downstream uses of LLMs</title>
      <link>https://arxiv.org/abs/2406.11851</link>
      <description>arXiv:2406.11851v1 Announce Type: new 
Abstract: Amidst escalating concerns about the detriments inflicted by AI systems, risk management assumes paramount importance, notably for high-risk applications as demanded by the European Union AI Act. Guidelines provided by ISO and NIST aim to govern AI risk management; however, practical implementations remain scarce in scholarly works. Addressing this void, our research explores risks emanating from downstream uses of large language models (LLMs), synthesizing a taxonomy grounded in earlier research. Building upon this foundation, we introduce a novel LLM-based risk assessment engine (GUARD-D-LLM: Guided Understanding and Assessment for Risk Detection for Downstream use of LLMs) designed to pinpoint and rank threats relevant to specific use cases derived from text-based user inputs. Integrating thirty intelligent agents, this innovative approach identifies bespoke risks, gauges their severity, offers targeted suggestions for mitigation, and facilitates risk-aware development. The paper also documents the limitations of such an approach along with way forward suggestions to augment experts in such risk assessment thereby leveraging GUARD-D-LLM in identifying risks early on and enabling early mitigations. This paper and its associated code serve as a valuable resource for developers seeking to mitigate risks associated with LLM-based applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11851v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>sundaraparipurnan Narayanan, Sandeep Vishwakarma</dc:creator>
    </item>
    <item>
      <title>Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation</title>
      <link>https://arxiv.org/abs/2406.11852</link>
      <description>arXiv:2406.11852v1 Announce Type: new 
Abstract: Amidst the growing interest in developing task-autonomous AI for automated mental health care, this paper addresses the ethical and practical challenges associated with the issue and proposes a structured framework that delineates levels of autonomy, outlines ethical requirements, and defines beneficial default behaviors for AI agents in the context of mental health support. We also evaluate ten state-of-the-art language models using 16 mental health-related questions designed to reflect various mental health conditions, such as psychosis, mania, depression, suicidal thoughts, and homicidal tendencies. The question design and response evaluations were conducted by mental health clinicians (M.D.s). We find that existing language models are insufficient to match the standard provided by human professionals who can navigate nuances and appreciate context. This is due to a range of issues, including overly cautious or sycophantic responses and the absence of necessary safeguards. Alarmingly, we find that most of the tested models could cause harm if accessed in mental health emergencies, failing to protect users and potentially exacerbating existing symptoms. We explore solutions to enhance the safety of current models. Before the release of increasingly task-autonomous AI systems in mental health, it is crucial to ensure that these models can reliably detect and manage symptoms of common psychiatric disorders to prevent harm to users. This involves aligning with the ethical framework and default behaviors outlined in our study. We contend that model developers are responsible for refining their systems per these guidelines to safeguard against the risks posed by current AI technologies to user mental health and safety.
  Trigger warning: Contains and discusses examples of sensitive mental health topics, including suicide and self-harm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11852v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Declan Grabb, Max Lamparth, Nina Vasan</dc:creator>
    </item>
    <item>
      <title>Decoding the Sociotechnical Dimensions of Digital Misinformation: A Comprehensive Literature Review</title>
      <link>https://arxiv.org/abs/2406.11853</link>
      <description>arXiv:2406.11853v1 Announce Type: new 
Abstract: This paper presents a systematic literature review in Computer Science that provide an overview of the initiatives related to digital misinformation. This is an exploratory study that covers research from 1993 to 2020, focusing on the investigation of the phenomenon of misinformation. The review consists of 788 studies from SCOPUS, IEEE, and ACM digital libraries, synthesizing the primary research directions and sociotechnical challenges. These challenges are classified into Physical, Empirical, Syntactic, Semantic, Pragmatic, and Social dimensions, drawing from Organizational Semiotics. The mapping identifies issues related to the concept of misinformation, highlights deficiencies in mitigation strategies, discusses challenges in approaching stakeholders, and unveils various sociotechnical aspects relevant to understanding and mitigating the harmful effects of digital misinformation. As contributions, this study present a novel categorization of mitigation strategies, a sociotechnical taxonomy for classifying types of false information and elaborate on the inter-relation of sociotechnical aspects and their impacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11853v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alisson Andrey Puska, Luiz Adolpho Baroni, Roberto Pereira</dc:creator>
    </item>
    <item>
      <title>Attributions toward Artificial Agents in a modified Moral Turing Test</title>
      <link>https://arxiv.org/abs/2406.11854</link>
      <description>arXiv:2406.11854v1 Announce Type: new 
Abstract: Advances in artificial intelligence (AI) raise important questions about whether people view moral evaluations by AI systems similarly to human-generated moral evaluations. We conducted a modified Moral Turing Test (m-MTT), inspired by Allen and colleagues' (2000) proposal, by asking people to distinguish real human moral evaluations from those made by a popular advanced AI language model: GPT-4. A representative sample of 299 U.S. adults first rated the quality of moral evaluations when blinded to their source. Remarkably, they rated the AI's moral reasoning as superior in quality to humans' along almost all dimensions, including virtuousness, intelligence, and trustworthiness, consistent with passing what Allen and colleagues call the comparative MTT. Next, when tasked with identifying the source of each evaluation (human or computer), people performed significantly above chance levels. Although the AI did not pass this test, this was not because of its inferior moral reasoning but, potentially, its perceived superiority, among other possible explanations. The emergence of language models capable of producing moral responses perceived as superior in quality to humans' raises concerns that people may uncritically accept potentially harmful moral guidance from AI. This possibility highlights the need for safeguards around generative language models in matters of morality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11854v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Scientific Reports (2024)</arxiv:journal_reference>
      <dc:creator>Eyal Aharoni, Sharlene Fernandes, Daniel J. Brady, Caelan Alexander, Michael Criner, Kara Queen, Javier Rando, Eddy Nahmias, Victor Crespo</dc:creator>
    </item>
    <item>
      <title>Law and the Emerging Political Economy of Algorithmic Audits</title>
      <link>https://arxiv.org/abs/2406.11855</link>
      <description>arXiv:2406.11855v1 Announce Type: new 
Abstract: For almost a decade now, scholarship in and beyond the ACM FAccT community has been focusing on novel and innovative ways and methodologies to audit the functioning of algorithmic systems. Over the years, this research idea and technical project has matured enough to become a regulatory mandate. Today, the Digital Services Act (DSA) and the Online Safety Act (OSA) have established the framework within which technology corporations and (traditional) auditors will develop the `practice' of algorithmic auditing thereby presaging how this `ecosystem' will develop. In this paper, we systematically review the auditing provisions in the DSA and the OSA in light of observations from the emerging industry of algorithmic auditing. Who is likely to occupy this space? What are some political and ethical tensions that are likely to arise? How are the mandates of `independent auditing' or `the evaluation of the societal context of an algorithmic function' likely to play out in practice? By shaping the picture of the emerging political economy of algorithmic auditing, we draw attention to strategies and cultures of traditional auditors that risk eroding important regulatory pillars of the DSA and the OSA. Importantly, we warn that ambitious research ideas and technical projects of/for algorithmic auditing may end up crashed by the standardising grip of traditional auditors and/or diluted within a complex web of (sub-)contractual arrangements, diverse portfolios, and tight timelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11855v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 2024 ACM Conference on Fairness, Accountability and Transparency (FAccT '24) (ACM 2024)</arxiv:journal_reference>
      <dc:creator>Petros Terzis, Michael Veale, No\"elle Gaumann</dc:creator>
    </item>
    <item>
      <title>Google's Chrome Antitrust Paradox</title>
      <link>https://arxiv.org/abs/2406.11856</link>
      <description>arXiv:2406.11856v1 Announce Type: new 
Abstract: This article delves into Google's dominance of the browser market, highlighting how Google's Chrome browser is playing a critical role in asserting Google's dominance in other markets. While Google perpetuates the perception that Google Chrome is a neutral platform built on open-source technologies, we argue that Chrome is instrumental in Google's strategy to reinforce its dominance in online advertising, publishing, and the browser market itself. Our examination of Google's strategic acquisitions, anti-competitive practices, and the implementation of so-called "privacy controls," shows that Chrome is far from a neutral gateway to the web. Rather, it serves as a key tool for Google to maintain and extend its market power, often to the detriment of competition and innovation.
  We examine how Chrome not only bolsters Google's position in advertising and publishing through practices such as coercion, and self-preferencing, it also helps leverage its advertising clout to engage in a "pay-to-play" paradigm, which serves as a cornerstone in Google's larger strategy of market control. We also discuss potential regulatory interventions and remedies, drawing on historical antitrust precedents. We propose a triad of solutions motivated from our analysis of Google's abuse of Chrome: behavioral remedies targeting specific anti-competitive practices, structural remedies involving an internal separation of Google's divisions, and divestment of Chrome from Google.
  Despite Chrome's dominance and its critical role in Google's ecosystem, it has escaped antitrust scrutiny - a gap our article aims to bridge. Addressing this gap is instrumental to solve current market imbalances and future challenges brought on by increasingly hegemonizing technology firms, ensuring a competitive digital environment that nurtures innovation and safeguards consumer interests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11856v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaoor Munir, Konrad Kollnig, Anastasia Shuba, Zubair Shafiq</dc:creator>
    </item>
    <item>
      <title>AI Royalties -- an IP Framework to Compensate Artists &amp; IP Holders for AI-Generated Content</title>
      <link>https://arxiv.org/abs/2406.11857</link>
      <description>arXiv:2406.11857v1 Announce Type: new 
Abstract: This article investigates how AI-generated content can disrupt central revenue streams of the creative industries, in particular the collection of dividends from intellectual property (IP) rights. It reviews the IP and copyright questions related to the input and output of generative AI systems. A systematic method is proposed to assess whether AI-generated outputs, especially images, infringe previous copyrights, using a similarity metric (CLIP) between images against historical copyright rulings. An examination (economic and technical feasibility) of previously proposed compensation frameworks reveals their financial implications for creatives and IP holders. Lastly, we propose a novel IP framework for compensation of artists and IP holders based on their published "licensed AIs" as a new medium and asset from which to collect AI royalties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11857v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pablo Ducru, Jonathan Raiman, Ronaldo Lemos, Clay Garner, George He, Hanna Balcha, Gabriel Souto, Sergio Branco, Celina Bottino</dc:creator>
    </item>
    <item>
      <title>Student Perspectives on Using a Large Language Model (LLM) for an Assignment on Professional Ethics</title>
      <link>https://arxiv.org/abs/2406.11858</link>
      <description>arXiv:2406.11858v1 Announce Type: new 
Abstract: The advent of Large Language Models (LLMs) started a serious discussion among educators on how LLMs would affect, e.g., curricula, assessments, and students' competencies. Generative AI and LLMs also raised ethical questions and concerns for computing educators and professionals. This experience report presents an assignment within a course on professional competencies, including some related to ethics, that computing master's students need in their careers. For the assignment, student groups discussed the ethical process by Lennerfors et al. by analyzing a case: a fictional researcher considers whether to attend the real CHI 2024 conference in Hawaii. The tasks were (1) to participate in in-class discussions on the case, (2) to use an LLM of their choice as a discussion partner for said case, and (3) to document both discussions, reflecting on their use of the LLM. Students reported positive experiences with the LLM as a way to increase their knowledge and understanding, although some identified limitations. The LLM provided a wider set of options for action in the studied case, including unfeasible ones. The LLM would not select a course of action, so students had to choose themselves, which they saw as coherent. From the educators' perspective, there is a need for more instruction for students using LLMs: some students did not perceive the tools as such but rather as an authoritative knowledge base. Therefore, this work has implications for educators considering the use of LLMs as discussion partners or tools to practice critical thinking, especially in computing ethics education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11858v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Virginia Grande, Natalie Kiesler, Maria Andreina Francisco R</dc:creator>
    </item>
    <item>
      <title>"Sora is Incredible and Scary": Emerging Governance Challenges of Text-to-Video Generative AI Models</title>
      <link>https://arxiv.org/abs/2406.11859</link>
      <description>arXiv:2406.11859v1 Announce Type: new 
Abstract: Text-to-video generative AI models such as Sora OpenAI have the potential to disrupt multiple industries. In this paper, we report a qualitative social media analysis aiming to uncover people's perceived impact of and concerns about Sora's integration. We collected and analyzed comments (N=292) under popular posts about Sora-generated videos, comparison between Sora videos and Midjourney images, and artists' complaints about copyright infringement by Generative AI. We found that people were most concerned about Sora's impact on content creation-related industries. Emerging governance challenges included the for-profit nature of OpenAI, the blurred boundaries between real and fake content, human autonomy, data privacy, copyright issues, and environmental impact. Potential regulatory solutions proposed by people included law-enforced labeling of AI content and AI literacy education for the public. Based on the findings, we discuss the importance of gauging people's tech perceptions early and propose policy recommendations to regulate Sora before its public release.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11859v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyrie Zhixuan Zhou, Abhinav Choudhry, Ece Gumusel, Madelyn Rose Sanfilippo</dc:creator>
    </item>
    <item>
      <title>An ethical study of generative AI from the Actor-Network Theory perspective</title>
      <link>https://arxiv.org/abs/2406.11860</link>
      <description>arXiv:2406.11860v1 Announce Type: new 
Abstract: The widespread use of Generative Artificial Intelligence in the innovation and generation of communication content is mainly due to its exceptional creative ability, operational efficiency, and compatibility with diverse industries. Nevertheless, this has also sparked ethical problems, such as unauthorized access to data, biased decision-making by algorithms, and criminal use of generated content. In order to tackle the security vulnerabilities linked to Generative Artificial Intelligence, we analyze ChatGPT as a case study within the framework of Actor-Network Theory. We have discovered a total of nine actors, including both human and non-human creatures. We examine the actors and processes of translation involved in the ethical issues related to ChatGPT and analyze the key players involved in the emergence of moral issues. The objective is to explore the origins of the ethical issues that arise with Generative Artificial Intelligence and provide a particular perspective on the governance of Generative Artificial Intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11860v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.5121/ijci.2024.130106</arxiv:DOI>
      <dc:creator>Yuying Li, Jinchi Zhu</dc:creator>
    </item>
    <item>
      <title>Digital Transformation of Education, Systems Approach and Applied Research</title>
      <link>https://arxiv.org/abs/2406.11861</link>
      <description>arXiv:2406.11861v1 Announce Type: new 
Abstract: This article proposes the construction of a systemic model of digital education as part of research applied to public policy (French Ministry of Education). Considering the digital domain in its pervasiveness, it highlights the importance of a complex approach to understanding the transformation of practices. As an applied research modality, we present digital theme groups (GTnum). The methodological approach combines a reflexive posture informed by research contributions, conceptual choices centered on digital humanities and the systems approach, participatory research and open science via the Hypotheses ''Education, digital and research'' notebook. As a result, our modeling is centered on a ''digital environment'' and six units of action put to the test via the GTnum themes. We interpret these results through a comparison with other systemic frameworks, an application to the axes of digital transformation in academies, a prospective reflection with the development of generative AI and perspectives for participatory research. Finally, the article discusses the limits and contributions of this approach: variability in the understanding of the issues at stake and in the integration of research contributions, as well as avenues for anticipating a new digital configuration with the place of AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11861v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elie Allouche</dc:creator>
    </item>
    <item>
      <title>The Transformation Risk-Benefit Model of Artificial Intelligence: Balancing Risks and Benefits Through Practical Solutions and Use Cases</title>
      <link>https://arxiv.org/abs/2406.11863</link>
      <description>arXiv:2406.11863v1 Announce Type: new 
Abstract: This paper summarizes the most cogent advantages and risks associated with Artificial Intelligence from an in-depth review of the literature. Then the authors synthesize the salient risk-related models currently being used in AI, technology and business-related scenarios. Next, in view of an updated context of AI along with theories and models reviewed and expanded constructs, the writers propose a new framework called "The Transformation Risk-Benefit Model of Artificial Intelligence" to address the increasing fears and levels of AI risk. Using the model characteristics, the article emphasizes practical and innovative solutions where benefits outweigh risks and three use cases in healthcare, climate change/environment and cyber security to illustrate unique interplay of principles, dimensions and processes of this powerful AI transformational model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11863v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.5121/ijaia.15201</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Artificial Intelligence and Applications (IJAIA), Vol 15, No. 2, March, 2024 pp1-22 (2024)</arxiv:journal_reference>
      <dc:creator>Richard Fulton (Department of Computer Science, Troy University, Troy, Alabama, USA), Diane Fulton (Department of Management, Clayton State University, Morrow, Georgia, USA), Nate Hayes (Modal Technologies, Minneapolis, Minnesota), Susan Kaplan (Modal Technologies, Minneapolis, Minnesota)</dc:creator>
    </item>
    <item>
      <title>Gender Differences in Class Participation in Online versus In-Person Core CS Courses</title>
      <link>https://arxiv.org/abs/2406.11864</link>
      <description>arXiv:2406.11864v1 Announce Type: new 
Abstract: The COVID-19 pandemic significantly altered how post-secondary students receive their education. Namely, the transition from an in-person to an online class format changed how students interact with their instructors and their classmates. In this paper, we use student participation scores from two core computer science classes across ten in-person and three online quarters at a public research university to analyze whether the shift to primarily asynchronous online learning has impacted the gender gap in student participation scores and students' attitudes towards themselves and their peers. We observe a shift on the online class forum: in in-person classes, males score higher on average and dominate the top scores while in online classes, male and female students participate at approximately the same rate classwide. To understand what might be driving changes in participation behavior, we analyze survey responses from over a quarter of the students enrolled in the online classes. While we find that students of both genders tend to compare themselves to their peers less when classes are online, we also find that this trend is much more accentuated for females than males. This data suggests that observed female participation habits in typical in-person classes are not inherent gender differences, but rather, a product of the environment. Therefore, it is critical the community investigates the root causes of these behavioral differences, and experiments with ways to mitigate them, before we soon return to an in-person format.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11864v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Madison Brigham, Jo\"el Porquet-Lupine</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence in Everyday Life 2.0: Educating University Students from Different Majors</title>
      <link>https://arxiv.org/abs/2406.11865</link>
      <description>arXiv:2406.11865v1 Announce Type: new 
Abstract: With the surge in data-centric AI and its increasing capabilities, AI applications have become a part of our everyday lives. However, misunderstandings regarding their capabilities, limitations, and associated advantages and disadvantages are widespread. Consequently, in the university setting, there is a crucial need to educate not only computer science majors but also students from various disciplines about AI. In this experience report, we present an overview of an introductory course that we offered to students coming from different majors. Moreover, we discuss the assignments and quizzes of the course, which provided students with a firsthand experience of AI processes and insights into their learning patterns. Additionally, we provide a summary of the course evaluation, as well as students' performance. Finally, we present insights gained from teaching this course and elaborate on our future plans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11865v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3649217.3653542</arxiv:DOI>
      <dc:creator>Maria Kasinidou (Open University of Cyprus), Styliani Kleanthous (CYENS Centre of Excellence, Open University of Cyprus), Matteo Busso (University of Trento), Marcelo Rodas (University of Trento, Fondazione Bruno Kessler), Jahna Otterbacher (Open University of Cyprus, CYENS Centre of Excellence), Fausto Giunchiglia (University of Trento)</dc:creator>
    </item>
    <item>
      <title>Toward an Artist-Centred AI</title>
      <link>https://arxiv.org/abs/2406.11866</link>
      <description>arXiv:2406.11866v1 Announce Type: new 
Abstract: Awareness about the immense impact that artificial intelligence (AI) might have or already has made on the social, economic, political, and cultural realities of our world has become part of the mainstream public discourse. Attributes such as ethical, responsible, or explainable emerge as associative and descriptive nominal references in guidelines that influence perspectives on AI application and development. This paper contextualizes the notions of suitability and desirability of principles, practices, and tools related to the use of AI in the arts. The result is a framework drafted as a set of atomic attributes that summarize the values of AI deemed important for artistic creativity. It was composed by examining the challenges that AI poses to art production, distribution, consumption, and monetization. Considering the differentiating potentials of AI and taking a perspective aside from the purely technical ontology, we argue that artistically pertinent AI should be unexpected, diversified, affordant, and evolvable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11866v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.34632/jsta.2023.15724</arxiv:DOI>
      <arxiv:journal_reference>Journal of Science and Technology of the Arts, 15(2), 101-120</arxiv:journal_reference>
      <dc:creator>Gordan Krekovic, Antonio Poscic, Dejan Grba</dc:creator>
    </item>
    <item>
      <title>Not as Simple as It Looked: Are We Concluding for Biased Arrest Practices?</title>
      <link>https://arxiv.org/abs/2406.11867</link>
      <description>arXiv:2406.11867v1 Announce Type: new 
Abstract: This study examines racial disparities in violent arrest outcomes, challenging conventional methods through a nuanced analysis of Cincinnati Police Department data. Acknowledging the intricate nature of racial disparity, the study categorizes explanations into types of place, types of person, and a combination of both, emphasizing the impact of neighborhood characteristics on crime distribution and police deployment. By introducing alternative scenarios, such as spuriousness, directed policing, and the geo-concentration of racial groups, the study underscores the complexity of racial disparity calculations. Employing a case study approach, the analysis of violent arrest outcomes reveals approximately 40 percent of the observed variation attributed to neighborhood-level characteristics, with concentrated disadvantage neutralizing the influence of race on arrest rates. Contrary to expectations, the study challenges the notion of unintentional racism, suggesting that neighborhood factors play a more significant role than the racial composition in explaining arrests. Policymakers are urged to focus on comprehensive community development initiatives addressing socioeconomic inequalities and support the development of robust racial disparity indices. The study calls for nuanced explorations of unintentional racism and future research addressing potential limitations, aiming to enhance understanding of the complexities surrounding racial disparities in arrests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11867v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Murat Ozer, Halil Akbas, Ismail Onat, Mehmet Bastug, Arif Akgul, Nelly ElSayed, Zag ElSayed, Multu Koseli, Niyazi Ekici</dc:creator>
    </item>
    <item>
      <title>Ethical Framework for Responsible Foundational Models in Medical Imaging</title>
      <link>https://arxiv.org/abs/2406.11868</link>
      <description>arXiv:2406.11868v1 Announce Type: new 
Abstract: Foundational models (FMs) have tremendous potential to revolutionize medical imaging. However, their deployment in real-world clinical settings demands extensive ethical considerations. This paper aims to highlight the ethical concerns related to FMs and propose a framework to guide their responsible development and implementation within medicine. We meticulously examine ethical issues such as privacy of patient data, bias mitigation, algorithmic transparency, explainability and accountability. The proposed framework is designed to prioritize patient welfare, mitigate potential risks, and foster trust in AI-assisted healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11868v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhijit Das, Debesh Jha, Jasmer Sanjotra, Onkar Susladkar, Suramyaa Sarkar, Ashish Rauniyar, Nikhil Tomar, Vanshali Sharma, Ulas Bagci</dc:creator>
    </item>
    <item>
      <title>Understanding Help-Seeking and Help-Giving on Social Media for Image-Based Sexual Abuse</title>
      <link>https://arxiv.org/abs/2406.12161</link>
      <description>arXiv:2406.12161v1 Announce Type: new 
Abstract: Image-based sexual abuse (IBSA), like other forms of technology-facilitated abuse, is a growing threat to people's digital safety. Attacks include unwanted solicitations for sexually explicit images, extorting people under threat of leaking their images, or purposefully leaking images to enact revenge or exert control. In this paper, we explore how people seek and receive help for IBSA on social media. Specifically, we identify over 100,000 Reddit posts that engage relationship and advice communities for help related to IBSA. We draw on a stratified sample of 261 posts to qualitatively examine how various types of IBSA unfold, including the mapping of gender, relationship dynamics, and technology involvement to different types of IBSA. We also explore the support needs of victim-survivors experiencing IBSA and how communities help victim-survivors navigate their abuse through technical, emotional, and relationship advice. Finally, we highlight sociotechnical gaps in connecting victim-survivors with important care, regardless of whom they turn to for help.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12161v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 33rd USENIX Security Symposium (USENIX Security 2024)</arxiv:journal_reference>
      <dc:creator>Miranda Wei, Sunny Consolvo, Patrick Gage Kelley, Tadayoshi Kohno, Tara Matthews, Sarah Meiklejohn, Franziska Roesner, Renee Shelby, Kurt Thomas, Rebecca Umbach</dc:creator>
    </item>
    <item>
      <title>Bounds and Bugs: The Limits of Symmetry Metrics to Detect Partisan Gerrymandering</title>
      <link>https://arxiv.org/abs/2406.12167</link>
      <description>arXiv:2406.12167v1 Announce Type: new 
Abstract: We provide both a theoretical and empirical analysis of the Mean-Median Difference (MM) and Partisan Bias (PB), which are both symmetry metrics intended to detect gerrymandering. We consider vote-share, seat-share pairs $(V, S)$ for which one can construct election data having vote share $V$ and seat share $S$, and turnout is equal in each district. We calculate the range of values that MM and PB can achieve on that constructed election data. In the process, we find the range of vote-share, seat share pairs $(V, S)$ for which there is constructed election data with vote share $V$, seat share $S$, and $MM=0$, and see that the corresponding range for PB is the same set of $(V,S)$ pairs. We show how the set of such $(V,S)$ pairs allowing for $MM=0$ (and $PB=0$) changes when turnout in each district is allowed to be different.
  Although the set of $(V,S)$ pairs for which there is election data with $MM=0$ is the same as the set of $(V,S)$ pairs for which there is election data with $PB=0$, the range of possible values for MM and PB on a fixed $(V, S)$ is different. Additionally, for a fixed constructed election outcome, the values of the Mean-Median Difference and Partisan Bias can theoretically be as large as 0.5. We show empirically that these two metric values can differ by as much as 0.33 in US congressional map data. We use both neutral ensemble analysis and the short-burst method to show that neither the Mean-Median Difference nor the Partisan Bias can reliably detect when a districting map has an extreme number of districts won by a particular party. Finally, we give additional empirical and logical arguments in an attempt to explain why other metrics are better at detecting when a districting map has an extreme number of districts won by a particular party.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12167v1</guid>
      <category>cs.CY</category>
      <category>math.CO</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ellen Veomett</dc:creator>
    </item>
    <item>
      <title>Navigating Knowledge Management Implementation Success in Government Organizations: A type-2 fuzzy approach</title>
      <link>https://arxiv.org/abs/2406.12345</link>
      <description>arXiv:2406.12345v1 Announce Type: new 
Abstract: Optimal information and knowledge management is crucial for organizations to achieve their objectives efficiently. As a rare and valuable resource, effective knowledge management provides a strategic advantage and has become a key determinant of organizational success. The study aims to identify critical success and failure factors for implementing knowledge management systems in government organizations. This research employs a descriptive survey methodology, collecting data through random interviews and questionnaires. The study highlights the critical success factors for knowledge management systems in government organizations, including cooperation, an open atmosphere, staff training, creativity and innovation, removal of organizational constraints, reward policies, role modeling, and focus. Conversely, failure to consider formality, staff participation, collaboration technologies, network and hardware infrastructure, complexity, IT staff, and trust can pose significant obstacles to successful implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12345v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saman Foroutani, Nasim Fahimian, Reyhaneh Jalalinejad, Morteza Hezarkhani, Samaneh Mahmoudi, Behrooz Gharleghi</dc:creator>
    </item>
    <item>
      <title>Who Checks the Checkers? Exploring Source Credibility in Twitter's Community Notes</title>
      <link>https://arxiv.org/abs/2406.12444</link>
      <description>arXiv:2406.12444v1 Announce Type: new 
Abstract: In recent years, the proliferation of misinformation on social media platforms has become a significant concern. Initially designed for sharing information and fostering social connections, platforms like Twitter (now rebranded as X) have also unfortunately become conduits for spreading misinformation. To mitigate this, these platforms have implemented various mechanisms, including the recent suggestion to use crowd-sourced non-expert fact-checkers to enhance the scalability and efficiency of content vetting. An example of this is the introduction of Community Notes on Twitter.
  While previous research has extensively explored various aspects of Twitter tweets, such as information diffusion, sentiment analytics and opinion summarization, there has been a limited focus on the specific feature of Twitter Community Notes, despite its potential role in crowd-sourced fact-checking. Prior research on Twitter Community Notes has involved empirical analysis of the feature's dataset and comparative studies that also include other methods like expert fact-checking. Distinguishing itself from prior works, our study covers a multi-faceted analysis of sources and audience perception within Community Notes. We find that the majority of cited sources are news outlets that are left-leaning and are of high factuality, pointing to a potential bias in the platform's community fact-checking. Left biased and low factuality sources validate tweets more, while Center sources are used more often to refute tweet content. Additionally, source factuality significantly influences public agreement and helpfulness of the notes, highlighting the effectiveness of the Community Notes Ranking algorithm. These findings showcase the impact and biases inherent in community-based fact-checking initiatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12444v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uku Kangur, Roshni Chakraborty, Rajesh Sharma</dc:creator>
    </item>
    <item>
      <title>DAOs' Business Value from an Open Systems Perspective: A Best-Fit Framework Synthesis</title>
      <link>https://arxiv.org/abs/2406.12445</link>
      <description>arXiv:2406.12445v1 Announce Type: new 
Abstract: Decentralized autonomous organizations (DAOs) are emerging innovative organizational structures, enabling collective coordination, and reshaping digital collaboration. Despite the promising and transformative characteristics of DAOs, the potential technological advancements and the understanding of the business value that organizations derive from implementing DAO characteristics are limited. This research applies a systematic review of DAOs' business applicability from an open systems perspective following a best-fit framework methodology. Within our approach, combining both framework and thematic analysis, we discuss how the open business principles apply to DAOs and present a new DAO business framework comprising of four core business elements: i) token, ii) transactions, iii) value system and iv) strategy with their corresponding sub-characteristics. This paper offers a preliminary DAO business framework that enhances the understanding of DAOs' transformative potential and guides organizations in innovating more inclusive business models (BMs), while also providing a theoretical foundation for researchers to build upon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12445v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas K\"ung, George M. Giaglis</dc:creator>
    </item>
    <item>
      <title>RIGL: A Unified Reciprocal Approach for Tracing the Independent and Group Learning Processes</title>
      <link>https://arxiv.org/abs/2406.12465</link>
      <description>arXiv:2406.12465v1 Announce Type: new 
Abstract: In the realm of education, both independent learning and group learning are esteemed as the most classic paradigms. The former allows learners to self-direct their studies, while the latter is typically characterized by teacher-directed scenarios. Recent studies in the field of intelligent education have leveraged deep temporal models to trace the learning process, capturing the dynamics of students' knowledge states, and have achieved remarkable performance. However, existing approaches have primarily focused on modeling the independent learning process, with the group learning paradigm receiving less attention. Moreover, the reciprocal effect between the two learning processes, especially their combined potential to foster holistic student development, remains inadequately explored. To this end, in this paper, we propose RIGL, a unified Reciprocal model to trace knowledge states at both the individual and group levels, drawing from the Independent and Group Learning processes. Specifically, we first introduce a time frame-aware reciprocal embedding module to concurrently model both student and group response interactions across various time frames. Subsequently, we employ reciprocal enhanced learning modeling to fully exploit the comprehensive and complementary information between the two behaviors. Furthermore, we design a relation-guided temporal attentive network, comprised of dynamic graph modeling coupled with a temporal self-attention mechanism. It is used to delve into the dynamic influence of individual and group interactions throughout the learning processes. Conclusively, we introduce a bias-aware contrastive learning module to bolster the stability of the model's training. Extensive experiments on four real-world educational datasets clearly demonstrate the effectiveness of the proposed RIGL model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12465v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoshan Yu, Chuan Qin, Dazhong Shen, Shangshang Yang, Haiping Ma, Hengshu Zhu, Xingyi Zhang</dc:creator>
    </item>
    <item>
      <title>Prompt Design Matters for Computational Social Science Tasks but in Unpredictable Ways</title>
      <link>https://arxiv.org/abs/2406.11980</link>
      <description>arXiv:2406.11980v1 Announce Type: cross 
Abstract: Manually annotating data for computational social science tasks can be costly, time-consuming, and emotionally draining. While recent work suggests that LLMs can perform such annotation tasks in zero-shot settings, little is known about how prompt design impacts LLMs' compliance and accuracy. We conduct a large-scale multi-prompt experiment to test how model selection (ChatGPT, PaLM2, and Falcon7b) and prompt design features (definition inclusion, output type, explanation, and prompt length) impact the compliance and accuracy of LLM-generated annotations on four CSS tasks (toxicity, sentiment, rumor stance, and news frames). Our results show that LLM compliance and accuracy are highly prompt-dependent. For instance, prompting for numerical scores instead of labels reduces all LLMs' compliance and accuracy. The overall best prompting setup is task-dependent, and minor prompt changes can cause large changes in the distribution of generated labels. By showing that prompt design significantly impacts the quality and distribution of LLM-generated annotations, this work serves as both a warning and practical guide for researchers and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11980v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shubham Atreja, Joshua Ashkinaze, Lingyao Li, Julia Mendelsohn, Libby Hemphill</dc:creator>
    </item>
    <item>
      <title>Decomposed evaluations of geographic disparities in text-to-image models</title>
      <link>https://arxiv.org/abs/2406.11988</link>
      <description>arXiv:2406.11988v1 Announce Type: cross 
Abstract: Recent work has identified substantial disparities in generated images of different geographic regions, including stereotypical depictions of everyday objects like houses and cars. However, existing measures for these disparities have been limited to either human evaluations, which are time-consuming and costly, or automatic metrics evaluating full images, which are unable to attribute these disparities to specific parts of the generated images. In this work, we introduce a new set of metrics, Decomposed Indicators of Disparities in Image Generation (Decomposed-DIG), that allows us to separately measure geographic disparities in the depiction of objects and backgrounds in generated images. Using Decomposed-DIG, we audit a widely used latent diffusion model and find that generated images depict objects with better realism than backgrounds and that backgrounds in generated images tend to contain larger regional disparities than objects. We use Decomposed-DIG to pinpoint specific examples of disparities, such as stereotypical background generation in Africa, struggling to generate modern vehicles in Africa, and unrealistically placing some objects in outdoor settings. Informed by our metric, we use a new prompting structure that enables a 52% worst-region improvement and a 20% average improvement in generated background diversity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11988v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Sureddy, Dishant Padalia, Nandhinee Periyakaruppa, Oindrila Saha, Adina Williams, Adriana Romero-Soriano, Megan Richards, Polina Kirichenko, Melissa Hall</dc:creator>
    </item>
    <item>
      <title>The Benefits and Risks of Transductive Approaches for AI Fairness</title>
      <link>https://arxiv.org/abs/2406.12011</link>
      <description>arXiv:2406.12011v1 Announce Type: cross 
Abstract: Recently, transductive learning methods, which leverage holdout sets during training, have gained popularity for their potential to improve speed, accuracy, and fairness in machine learning models. Despite this, the composition of the holdout set itself, particularly the balance of sensitive sub-groups, has been largely overlooked. Our experiments on CIFAR and CelebA datasets show that compositional changes in the holdout set can substantially influence fairness metrics. Imbalanced holdout sets exacerbate existing disparities, while balanced holdouts can mitigate issues introduced by imbalanced training data. These findings underline the necessity of constructing holdout sets that are both diverse and representative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12011v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammed Razzak, Andreas Kirsch, Yarin Gal</dc:creator>
    </item>
    <item>
      <title>Centering Policy and Practice: Research Gaps around Usable Differential Privacy</title>
      <link>https://arxiv.org/abs/2406.12103</link>
      <description>arXiv:2406.12103v1 Announce Type: cross 
Abstract: As a mathematically rigorous framework that has amassed a rich theoretical literature, differential privacy is considered by many experts to be the gold standard for privacy-preserving data analysis. Others argue that while differential privacy is a clean formulation in theory, it poses significant challenges in practice. Both perspectives are, in our view, valid and important. To bridge the gaps between differential privacy's promises and its real-world usability, researchers and practitioners must work together to advance policy and practice of this technology. In this paper, we outline pressing open questions towards building usable differential privacy and offer recommendations for the field, such as developing risk frameworks to align with user needs, tailoring communications for different stakeholders, modeling the impact of privacy-loss parameters, investing in effective user interfaces, and facilitating algorithmic and procedural audits of differential privacy systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12103v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>2023 5th IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA). IEEE Computer Society, 2023</arxiv:journal_reference>
      <dc:creator>Rachel Cummings, Jayshree Sarathy</dc:creator>
    </item>
    <item>
      <title>Slicing Through Bias: Explaining Performance Gaps in Medical Image Analysis using Slice Discovery Methods</title>
      <link>https://arxiv.org/abs/2406.12142</link>
      <description>arXiv:2406.12142v1 Announce Type: cross 
Abstract: Machine learning models have achieved high overall accuracy in medical image analysis. However, performance disparities on specific patient groups pose challenges to their clinical utility, safety, and fairness. This can affect known patient groups - such as those based on sex, age, or disease subtype - as well as previously unknown and unlabeled groups. Furthermore, the root cause of such observed performance disparities is often challenging to uncover, hindering mitigation efforts. In this paper, to address these issues, we leverage Slice Discovery Methods (SDMs) to identify interpretable underperforming subsets of data and formulate hypotheses regarding the cause of observed performance disparities. We introduce a novel SDM and apply it in a case study on the classification of pneumothorax and atelectasis from chest x-rays. Our study demonstrates the effectiveness of SDMs in hypothesis formulation and yields an explanation of previously observed but unexplained performance disparities between male and female patients in widely used chest X-ray datasets and models. Our findings indicate shortcut learning in both classification tasks, through the presence of chest drains and ECG wires, respectively. Sex-based differences in the prevalence of these shortcut features appear to cause the observed classification performance gap, representing a previously underappreciated interaction between shortcut learning and model fairness analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12142v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Vincent Olesen, Nina Weng, Aasa Feragen, Eike Petersen</dc:creator>
    </item>
    <item>
      <title>ToxiCloakCN: Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations</title>
      <link>https://arxiv.org/abs/2406.12223</link>
      <description>arXiv:2406.12223v1 Announce Type: cross 
Abstract: Detecting hate speech and offensive language is essential for maintaining a safe and respectful digital environment. This study examines the limitations of state-of-the-art large language models (LLMs) in identifying offensive content within systematically perturbed data, with a focus on Chinese, a language particularly susceptible to such perturbations. We introduce \textsf{ToxiCloakCN}, an enhanced dataset derived from ToxiCN, augmented with homophonic substitutions and emoji transformations, to test the robustness of LLMs against these cloaking perturbations. Our findings reveal that existing models significantly underperform in detecting offensive content when these perturbations are applied. We provide an in-depth analysis of how different types of offensive content are affected by these perturbations and explore the alignment between human and model explanations of offensiveness. Our work highlights the urgent need for more advanced techniques in offensive language detection to combat the evolving tactics used to evade detection mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12223v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunze Xiao, Yujia Hu, Kenny Tsu Wei Choo, Roy Ka-wei Lee</dc:creator>
    </item>
    <item>
      <title>QueerBench: Quantifying Discrimination in Language Models Toward Queer Identities</title>
      <link>https://arxiv.org/abs/2406.12399</link>
      <description>arXiv:2406.12399v1 Announce Type: cross 
Abstract: With the increasing role of Natural Language Processing (NLP) in various applications, challenges concerning bias and stereotype perpetuation are accentuated, which often leads to hate speech and harm. Despite existing studies on sexism and misogyny, issues like homophobia and transphobia remain underexplored and often adopt binary perspectives, putting the safety of LGBTQIA+ individuals at high risk in online spaces. In this paper, we assess the potential harm caused by sentence completions generated by English large language models (LLMs) concerning LGBTQIA+ individuals. This is achieved using QueerBench, our new assessment framework, which employs a template-based approach and a Masked Language Modeling (MLM) task. The analysis indicates that large language models tend to exhibit discriminatory behaviour more frequently towards individuals within the LGBTQIA+ community, reaching a difference gap of 7.2% in the QueerBench score of harmfulness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12399v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mae Sosto, Alberto Barr\'on-Cede\~no</dc:creator>
    </item>
    <item>
      <title>Tracing the Unseen: Uncovering Human Trafficking Patterns in Job Listings</title>
      <link>https://arxiv.org/abs/2406.12469</link>
      <description>arXiv:2406.12469v1 Announce Type: cross 
Abstract: In the shadow of the digital revolution, the insidious issue of human trafficking has found new breeding grounds within the realms of social media and online job boards. Previous research efforts have predominantly centered on identifying victims via the analysis of escort advertisements. However, our work shifts the focus towards enabling a proactive approach: pinpointing potential traffickers before they lure their preys through false job opportunities. In this study, we collect and analyze a vast dataset comprising over a quarter million job postings collected from eight relevant regions across the United States, spanning nearly two decades (2006-2024). The job boards we considered are specifically catered towards Chinese-speaking immigrants in the US. We classify the job posts into distinct groups based on the self-reported information of the posting user. Our investigation into the types of advertised opportunities, the modes of preferred contact, and the frequency of postings uncovers the patterns characterizing suspicious ads. Additionally, we highlight how external events such as health emergencies and conflicts appear to strongly correlate with increased volume of suspicious job posts: traffickers are more likely to prey upon vulnerable populations in times of crises. This research underscores the imperative for a deeper dive into how online job boards and communication platforms could be unwitting facilitators of human trafficking. More importantly, it calls for the urgent formulation of targeted strategies to dismantle these digital conduits of exploitation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12469v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.36190/2024.27</arxiv:DOI>
      <dc:creator>Siyi Zhou, Jiankun Peng, Emilio Ferrara</dc:creator>
    </item>
    <item>
      <title>Whose Emotions and Moral Sentiments Do Language Models Reflect?</title>
      <link>https://arxiv.org/abs/2402.11114</link>
      <description>arXiv:2402.11114v2 Announce Type: replace-cross 
Abstract: Language models (LMs) are known to represent the perspectives of some social groups better than others, which may impact their performance, especially on subjective tasks such as content moderation and hate speech detection. To explore how LMs represent different perspectives, existing research focused on positional alignment, i.e., how closely the models mimic the opinions and stances of different groups, e.g., liberals or conservatives. However, human communication also encompasses emotional and moral dimensions. We define the problem of affective alignment, which measures how LMs' emotional and moral tone represents those of different groups. By comparing the affect of responses generated by 36 LMs to the affect of Twitter messages, we observe significant misalignment of LMs with both ideological groups. This misalignment is larger than the partisan divide in the U.S. Even after steering the LMs towards specific ideological perspectives, the misalignment and liberal tendencies of the model persist, suggesting a systemic bias within LMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11114v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao He, Siyi Guo, Ashwin Rao, Kristina Lerman</dc:creator>
    </item>
    <item>
      <title>How Susceptible are Large Language Models to Ideological Manipulation?</title>
      <link>https://arxiv.org/abs/2402.11725</link>
      <description>arXiv:2402.11725v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the influence of ideological manipulations on LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11725v3</guid>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Chen, Zihao He, Jun Yan, Taiwei Shi, Kristina Lerman</dc:creator>
    </item>
    <item>
      <title>Investigating Human Values in Online Communities</title>
      <link>https://arxiv.org/abs/2402.14177</link>
      <description>arXiv:2402.14177v2 Announce Type: replace-cross 
Abstract: Human values play a vital role as an analytical tool in social sciences, enabling the study of diverse dimensions within society as a whole and among individual communities. This paper addresses the limitations of traditional survey-based studies of human values by proposing a computational application of Schwartz's values framework to Reddit, a platform organized into distinct online communities. After ensuring the reliability of automated value extraction tools for Reddit content, we automatically annotate six million posts across 10,000 subreddits with Schwartz values. Our analysis unveils both previously recorded and novel insights into the values prevalent within various online communities. For instance, when examining subreddits with differing opinions on controversial topics, we discover higher universalism values in the Vegan subreddit compared to Carnivores. Additionally, our study of geographically specific subreddits highlights the correlation between traditional values and conservative U.S. states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14177v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nadav Borenstein, Arnav Arora, Lucie-Aim\'ee Kaffee, Isabelle Augenstein</dc:creator>
    </item>
    <item>
      <title>Addressing Shortcomings in Fair Graph Learning Datasets: Towards a New Benchmark</title>
      <link>https://arxiv.org/abs/2403.06017</link>
      <description>arXiv:2403.06017v2 Announce Type: replace-cross 
Abstract: Fair graph learning plays a pivotal role in numerous practical applications. Recently, many fair graph learning methods have been proposed; however, their evaluation often relies on poorly constructed semi-synthetic datasets or substandard real-world datasets. In such cases, even a basic Multilayer Perceptron (MLP) can outperform Graph Neural Networks (GNNs) in both utility and fairness. In this work, we illustrate that many datasets fail to provide meaningful information in the edges, which may challenge the necessity of using graph structures in these problems. To address these issues, we develop and introduce a collection of synthetic, semi-synthetic, and real-world datasets that fulfill a broad spectrum of requirements. These datasets are thoughtfully designed to include relevant graph structures and bias information crucial for the fair evaluation of models. The proposed synthetic and semi-synthetic datasets offer the flexibility to create data with controllable bias parameters, thereby enabling the generation of desired datasets with user-defined bias values with ease. Moreover, we conduct systematic evaluations of these proposed datasets and establish a unified evaluation approach for fair graph learning models. Our extensive experimental results with fair graph learning methods across our datasets demonstrate their effectiveness in benchmarking the performance of these methods. Our datasets and the code for reproducing our experiments are available at https://github.com/XweiQ/Benchmark-GraphFairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06017v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaowei Qian, Zhimeng Guo, Jialiang Li, Haitao Mao, Bingheng Li, Suhang Wang, Yao Ma</dc:creator>
    </item>
    <item>
      <title>Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds</title>
      <link>https://arxiv.org/abs/2404.02866</link>
      <description>arXiv:2404.02866v3 Announce Type: replace-cross 
Abstract: Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as "features" (or, less commonly, as "embeddings" or "feature embeddings"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, "MNIST" and "CIFAR-10," which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guarantee confidentiality of the inputs to inference with standard deep neural nets, "ResNet-18" and "Swin-T," pre-trained on the data set, "ImageNet-1000," which contains 1000 classes. Supplementing the addition of noise to features with other methods for providing confidentiality may be warranted in the case of ImageNet. In all cases, the results reported here limit consideration to amounts of added noise that incur little degradation in the accuracy of classification from the noisy features. Thus, the added noise enhances confidentiality without much reduction in the accuracy on the task of image classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02866v3</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kamalika Chaudhuri, Chuan Guo, Laurens van der Maaten, Saeed Mahloujifar, Mark Tygert</dc:creator>
    </item>
    <item>
      <title>White Men Lead, Black Women Help? Benchmarking Language Agency Social Biases in LLMs</title>
      <link>https://arxiv.org/abs/2404.10508</link>
      <description>arXiv:2404.10508v2 Announce Type: replace-cross 
Abstract: Language agency is an important aspect of evaluating social biases in texts. While several studies approached agency-related bias in human-written language, very limited research has investigated such biases in Large Language Model (LLM)-generated content. In addition, previous research often relies on string-matching techniques to identify agentic and communal words within texts, which fall short of accurately classifying language agency. We introduce the novel Language Agency Bias Evaluation (LABE) benchmark, which comprehensively evaluates biases in LLMs by analyzing agency levels attributed to different demographic groups in model generations. LABE leverages 5,400 template-based prompts, an accurate agency classifier, and corresponding bias metrics to test for gender, racial, and intersectional language agency biases in LLMs on 3 text generation tasks: biographies, professor reviews, and reference letters. To build better and more accurate automated agency classifiers, we also contribute and release the Language Agency Classification (LAC) dataset, consisting of 3,724 agentic and communal sentences. Using LABE, we unveil previously under-explored language agency social biases in 3 recent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) For the same text category, LLM generations demonstrate higher levels of gender bias than human-written texts; (2) On most generation tasks, models show remarkably higher levels of intersectional bias than the other bias aspects. Those who are at the intersection of gender and racial minority groups -- such as Black females -- are consistently described by texts with lower levels of agency; (3) Among the 3 LLMs investigated, Llama3 demonstrates greatest overall bias in language agency; (4) Not only does prompt-based mitigation fail to resolve language agency bias in LLMs, but it frequently leads to the exacerbation of biases in generated texts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10508v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yixin Wan, Kai-Wei Chang</dc:creator>
    </item>
    <item>
      <title>CLST: Cold-Start Mitigation in Knowledge Tracing by Aligning a Generative Language Model as a Students' Knowledge Tracer</title>
      <link>https://arxiv.org/abs/2406.10296</link>
      <description>arXiv:2406.10296v2 Announce Type: replace-cross 
Abstract: Knowledge tracing (KT), wherein students' problem-solving histories are used to estimate their current levels of knowledge, has attracted significant interest from researchers. However, most existing KT models were developed with an ID-based paradigm, which exhibits limitations in cold-start performance. These limitations can be mitigated by leveraging the vast quantities of external knowledge possessed by generative large language models (LLMs). In this study, we propose cold-start mitigation in knowledge tracing by aligning a generative language model as a students' knowledge tracer (CLST) as a framework that utilizes a generative LLM as a knowledge tracer. Upon collecting data from math, social studies, and science subjects, we framed the KT task as a natural language processing task, wherein problem-solving data are expressed in natural language, and fine-tuned the generative LLM using the formatted KT dataset. Subsequently, we evaluated the performance of the CLST in situations of data scarcity using various baseline models for comparison. The results indicate that the CLST significantly enhanced performance with a dataset of fewer than 100 students in terms of prediction, reliability, and cross-domain generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10296v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heeseok Jung, Jaesang Yoo, Yohaan Yoon, Yeonju Jang</dc:creator>
    </item>
    <item>
      <title>Dynamic Normativity: Necessary and Sufficient Conditions for Value Alignment</title>
      <link>https://arxiv.org/abs/2406.11039</link>
      <description>arXiv:2406.11039v2 Announce Type: replace-cross 
Abstract: The critical inquiry pervading the realm of Philosophy, and perhaps extending its influence across all Humanities disciplines, revolves around the intricacies of morality and normativity. Surprisingly, in recent years, this thematic thread has woven its way into an unexpected domain, one not conventionally associated with pondering "what ought to be": the field of artificial intelligence (AI) research. Central to morality and AI, we find "alignment", a problem related to the challenges of expressing human goals and values in a manner that artificial systems can follow without leading to unwanted adversarial effects. More explicitly and with our current paradigm of AI development in mind, we can think of alignment as teaching human values to non-anthropomorphic entities trained through opaque, gradient-based learning techniques. This work addresses alignment as a technical-philosophical problem that requires solid philosophical foundations and practical implementations that bring normative theory to AI system development. To accomplish this, we propose two sets of necessary and sufficient conditions that, we argue, should be considered in any alignment process. While necessary conditions serve as metaphysical and metaethical roots that pertain to the permissibility of alignment, sufficient conditions establish a blueprint for aligning AI systems under a learning-based paradigm. After laying such foundations, we present implementations of this approach by using state-of-the-art techniques and methods for aligning general-purpose language systems. We call this framework Dynamic Normativity. Its central thesis is that any alignment process under a learning paradigm that cannot fulfill its necessary and sufficient conditions will fail in producing aligned systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11039v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nicholas Kluge Corr\^ea</dc:creator>
    </item>
    <item>
      <title>The Evolution of Language in Social Media Comments</title>
      <link>https://arxiv.org/abs/2406.11450</link>
      <description>arXiv:2406.11450v2 Announce Type: replace-cross 
Abstract: Understanding the impact of digital platforms on user behavior presents foundational challenges, including issues related to polarization, misinformation dynamics, and variation in news consumption. Comparative analyses across platforms and over different years can provide critical insights into these phenomena. This study investigates the linguistic characteristics of user comments over 34 years, focusing on their complexity and temporal shifts. Utilizing a dataset of approximately 300 million English comments from eight diverse platforms and topics, we examine the vocabulary size and linguistic richness of user communications and their evolution over time. Our findings reveal consistent patterns of complexity across social media platforms and topics, characterized by a nearly universal reduction in text length, diminished lexical richness, but decreased repetitiveness. Despite these trends, users consistently introduce new words into their comments at a nearly constant rate. This analysis underscores that platforms only partially influence the complexity of user comments. Instead, it reflects a broader, universal pattern of human behaviour, suggesting intrinsic linguistic tendencies of users when interacting online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11450v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niccol\`o Di Marco, Edoardo Loru, Anita Bonetti, Alessandra Olga Grazia Serra, Matteo Cinelli, Walter Quattrociocchi</dc:creator>
    </item>
  </channel>
</rss>
