<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MM</link>
    <description>cs.MM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Nov 2025 05:01:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Real-Time Interactive Hybrid Ocean: Spectrum-Consistent Wave Particle-FFT Coupling</title>
      <link>https://arxiv.org/abs/2511.02852</link>
      <description>arXiv:2511.02852v1 Announce Type: cross 
Abstract: Fast Fourier Transform-based (FFT) spectral oceans are widely adopted for their efficiency and large-scale realism, but they assume global stationarity and spatial homogeneity, making it difficult to represent non-uniform seas and near-field interactions (e.g., ships and floaters). In contrast, wave particles capture local wakes and ripples, yet are costly to maintain at scale and hard to match global spectral statistics.We present a real-time interactive hybrid ocean: a global FFT background coupled with local wave-particle (WP) patch regions around interactive objects, jointly driven under a unified set of spectral parameters and dispersion. At patch boundaries, particles are injected according to the same directional spectrum as the FFT, aligning the local frequency-direction distribution with the background and matching energy density, without disturbing the far field.Our approach introduces two main innovations: (1) Hybrid ocean representation. We couple a global FFT background with local WP patches under a unified spectrum, achieving large-scale spectral consistency while supporting localized wakes and ripples.(2) Frequency-bucketed implementation. We design a particle sampling and GPU-parallel synthesis scheme based on frequency buckets, which preserves spectral energy consistency and sustains real-time interactive performance.Together, these innovations enable a unified framework that delivers both large-scale spectral realism and fine-grained interactivity in real time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02852v1</guid>
      <category>eess.SP</category>
      <category>cs.GR</category>
      <category>cs.MM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengze Xue, Yu Ren, Jiacheng Hong, Run Ni, Shuangjiu Xiao, Deli Dong</dc:creator>
    </item>
    <item>
      <title>Node-Based Editing for Multimodal Generation of Text, Audio, Image, and Vide</title>
      <link>https://arxiv.org/abs/2511.03227</link>
      <description>arXiv:2511.03227v1 Announce Type: cross 
Abstract: We present a node-based storytelling system for multimodal content generation. The system represents stories as graphs of nodes that can be expanded, edited, and iteratively refined through direct user edits and natural-language prompts. Each node can integrate text, images, audio, and video, allowing creators to compose multimodal narratives. A task selection agent routes between specialized generative tasks that handle story generation, node structure reasoning, node diagram formatting, and context generation. The interface supports targeted editing of individual nodes, automatic branching for parallel storylines, and node-based iterative refinement. Our results demonstrate that node-based editing supports control over narrative structure and iterative generation of text, images, audio, and video. We report quantitative outcomes on automatic story outline generation and qualitative observations of editing workflows. Finally, we discuss current limitations such as scalability to longer narratives and consistency across multiple nodes, and outline future work toward human-in-the-loop and user-centered creative AI tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03227v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.MM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Htet Kyaw, Lenin Ravindranath Sivalingam</dc:creator>
    </item>
    <item>
      <title>Seeing What You Say: Expressive Image Generation from Speech</title>
      <link>https://arxiv.org/abs/2511.03423</link>
      <description>arXiv:2511.03423v1 Announce Type: cross 
Abstract: This paper proposes VoxStudio, the first unified and end-to-end speech-to-image model that generates expressive images directly from spoken descriptions by jointly aligning linguistic and paralinguistic information. At its core is a speech information bottleneck (SIB) module, which compresses raw speech into compact semantic tokens, preserving prosody and emotional nuance. By operating directly on these tokens, VoxStudio eliminates the need for an additional speech-to-text system, which often ignores the hidden details beyond text, e.g., tone or emotion. We also release VoxEmoset, a large-scale paired emotional speech-image dataset built via an advanced TTS engine to affordably generate richly expressive utterances. Comprehensive experiments on the SpokenCOCO, Flickr8kAudio, and VoxEmoset benchmarks demonstrate the feasibility of our method and highlight key challenges, including emotional consistency and linguistic ambiguity, paving the way for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03423v1</guid>
      <category>eess.AS</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiyoung Lee, Song Park, Sanghyuk Chun, Soo-Whan Chung</dc:creator>
    </item>
    <item>
      <title>SyMuPe: Affective and Controllable Symbolic Music Performance</title>
      <link>https://arxiv.org/abs/2511.03425</link>
      <description>arXiv:2511.03425v1 Announce Type: cross 
Abstract: Emotions are fundamental to the creation and perception of music performances. However, achieving human-like expression and emotion through machine learning models for performance rendering remains a challenging task. In this work, we present SyMuPe, a novel framework for developing and training affective and controllable symbolic piano performance models. Our flagship model, PianoFlow, uses conditional flow matching trained to solve diverse multi-mask performance inpainting tasks. By design, it supports both unconditional generation and infilling of music performance features. For training, we use a curated, cleaned dataset of 2,968 hours of aligned musical scores and expressive MIDI performances. For text and emotion control, we integrate a piano performance emotion classifier and tune PianoFlow with the emotion-weighted Flan-T5 text embeddings provided as conditional inputs. Objective and subjective evaluations against transformer-based baselines and existing models show that PianoFlow not only outperforms other approaches, but also achieves performance quality comparable to that of human-recorded and transcribed MIDI samples. For emotion control, we present and analyze samples generated under different text conditioning scenarios. The developed model can be integrated into interactive applications, contributing to the creation of more accessible and engaging music performance systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03425v1</guid>
      <category>cs.SD</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3746027.3755871</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 33rd ACM International Conference on Multimedia (MM '25), October 27-31, 2025, Dublin, Ireland, pp. 10699-10708</arxiv:journal_reference>
      <dc:creator>Ilya Borovik, Dmitrii Gavrilev, Vladimir Viro</dc:creator>
    </item>
    <item>
      <title>A Versatile Depth Video Encoding Scheme Based on Low-rank Tensor Modeling for Free Viewpoint Video</title>
      <link>https://arxiv.org/abs/2104.04678</link>
      <description>arXiv:2104.04678v2 Announce Type: replace 
Abstract: The compression quality losses of depth sequences determine quality of view synthesis in free-viewpoint video. The depth map intra prediction in 3D extensions of the HEVC applies intra modes with auxiliary depth modeling modes (DMMs) to better preserve depth edges and handle motion discontinuities. Although such modes enable high efficiency compression, but at the cost of very high encoding complexity. Skipping conventional intra coding modes and DMMs in depth coding limits practical applicability of the HEVC for 3D display applications. In this paper, we introduce a novel low-complexity scheme for depth video compression based on low-rank tensor decomposition and HEVC intra coding. The proposed scheme leverages spatial and temporal redundancy by compactly representing the depth sequence as a high-order tensor. Tensor factorization into a set of factor matrices following CANDECOMP PARAFAC (CP) decomposition via alternating least squares give a low-rank approximation of the scene geometry. Further, compression of factor matrices with HEVC intra prediction support arbitrary target accuracy by flexible adjustment of bitrate, varying tensor decomposition ranks and quantization parameters. The results demonstrate proposed approach achieves significant rate gains by efficiently compressing depth planes in low-rank approximated representation. The proposed algorithm is applied to encode depth maps of benchmark Ballet and Breakdancing sequences. The decoded depth sequences are used for view synthesis in a multi-view video system, maintaining appropriate rendering quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.04678v2</guid>
      <category>cs.MM</category>
      <category>eess.IV</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mansi Sharma, Jyotsana Grover</dc:creator>
    </item>
    <item>
      <title>Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing</title>
      <link>https://arxiv.org/abs/2511.00801</link>
      <description>arXiv:2511.00801v2 Announce Type: replace-cross 
Abstract: Recent advances in multimodal large language models have enabled remarkable medical image editing capabilities. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built specifically for medical image editing with strict anatomical and clinical constraints. We introduce Med-Banana-50K, a comprehensive 50K-image dataset for instruction-based medical image editing spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23 disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image to generate bidirectional edits (lesion addition and removal) from real medical images. What distinguishes Med-Banana-50K from general-domain editing datasets is our systematic approach to medical quality control: we employ LLM-as-Judge with a medically grounded rubric (instruction compliance, structural plausibility, realism, and fidelity preservation) and history-aware iterative refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K includes 37K failed attempts with full conversation logs for preference learning and alignment research. By providing this large-scale, medically validated, and fully documented resource, Med-Banana-50K establishes a foundation for training and evaluating the next generation of medical image editing models.Our dataset and code are publicly available at [https://github.com/richardChenzhihui/med-banana-50k].</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00801v2</guid>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhihui Chen, Mengling Feng</dc:creator>
    </item>
  </channel>
</rss>
