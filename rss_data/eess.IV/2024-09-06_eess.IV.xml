<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Sep 2024 04:00:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Enhancing Alzheimer's Disease Prediction: A Novel Approach to Leveraging GAN-Augmented Data for Improved CNN Model Accuracy</title>
      <link>https://arxiv.org/abs/2409.02961</link>
      <description>arXiv:2409.02961v1 Announce Type: new 
Abstract: Alzheimer's Disease (AD) is a neurodegenerative disease affecting millions of individuals across the globe. As the prevalence of this disease continues to rise, early diagnosis is crucial to improve clinical outcomes. Neural networks, specifically Convolutional Neural Networks (CNNs), are promising tools for diagnosing individuals with Alzheimer's. However, neural networks such as ANNs and CNNs typically yield lower validation accuracies when fed lower quantities of data. Hence, Generative Adversarial Networks (GANs) can be utilized to synthesize data to augment these existing MRI datasets, potentially yielding higher validation accuracies. In this study, we use this principle while examining a novel application of the SSMI metric in selecting high-quality synthetic data generated by our GAN to compare its accuracies with shuffled data generated by our GAN. We observed that incorporating GANs with an SSMI metric returned the highest accuracies when compared to a traditional dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02961v1</guid>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshay Sunkara, Rajiv Morthala, Anav Jain, Srinjoy Ghose, Santosh Morthala</dc:creator>
    </item>
    <item>
      <title>Coupling AI and Citizen Science in Creation of Enhanced Training Dataset for Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2409.03087</link>
      <description>arXiv:2409.03087v1 Announce Type: new 
Abstract: Recent advancements in medical imaging and artificial intelligence (AI) have greatly enhanced diagnostic capabilities, but the development of effective deep learning (DL) models is still constrained by the lack of high-quality annotated datasets. The traditional manual annotation process by medical experts is time- and resource-intensive, limiting the scalability of these datasets. In this work, we introduce a robust and versatile framework that combines AI and crowdsourcing to improve both the quality and quantity of medical image datasets across different modalities. Our approach utilises a user-friendly online platform that enables a diverse group of crowd annotators to label medical images efficiently. By integrating the MedSAM segmentation AI with this platform, we accelerate the annotation process while maintaining expert-level quality through an algorithm that merges crowd-labelled images. Additionally, we employ pix2pixGAN, a generative AI model, to expand the training dataset with synthetic images that capture realistic morphological features. These methods are combined into a cohesive framework designed to produce an enhanced dataset, which can serve as a universal pre-processing pipeline to boost the training of any medical deep learning segmentation model. Our results demonstrate that this framework significantly improves model performance, especially when training data is limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03087v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amir Syahmi, Xiangrong Lu, Yinxuan Li, Haoxuan Yao, Hanjun Jiang, Ishita Acharya, Shiyi Wang, Yang Nan, Xiaodan Xing, Guang Yang</dc:creator>
    </item>
    <item>
      <title>MSTT-199: MRI Dataset for Musculoskeletal Soft Tissue Tumor Segmentation</title>
      <link>https://arxiv.org/abs/2409.03110</link>
      <description>arXiv:2409.03110v1 Announce Type: new 
Abstract: Accurate musculoskeletal soft tissue tumor segmentation is vital for assessing tumor size, location, diagnosis, and response to treatment, thereby influencing patient outcomes. However, segmentation of these tumors requires clinical expertise, and an automated segmentation model would save valuable time for both clinician and patient. Training an automatic model requires a large dataset of annotated images. In this work, we describe the collection of an MR imaging dataset of 199 musculoskeletal soft tissue tumors from 199 patients. We trained segmentation models on this dataset and then benchmarked them on a publicly available dataset. Our model achieved the state-of-the-art dice score of 0.79 out of the box without any fine tuning, which shows the diversity and utility of our curated dataset. We analyzed the model predictions and found that its performance suffered on fibrous and vascular tumors due to their diverse anatomical location, size, and intensity heterogeneity. The code and models are available in the following github repository, https://github.com/Reasat/mstt</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03110v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tahsin Reasat, Stephen Chenard, Akhil Rekulapelli, Nicholas Chadwick, Joanna Shechtel, Katherine van Schaik, David S. Smith, Joshua Lawrenz</dc:creator>
    </item>
    <item>
      <title>Perceptual-Distortion Balanced Image Super-Resolution is a Multi-Objective Optimization Problem</title>
      <link>https://arxiv.org/abs/2409.03179</link>
      <description>arXiv:2409.03179v1 Announce Type: new 
Abstract: Training Single-Image Super-Resolution (SISR) models using pixel-based regression losses can achieve high distortion metrics scores (e.g., PSNR and SSIM), but often results in blurry images due to insufficient recovery of high-frequency details. Conversely, using GAN or perceptual losses can produce sharp images with high perceptual metric scores (e.g., LPIPS), but may introduce artifacts and incorrect textures. Balancing these two types of losses can help achieve a trade-off between distortion and perception, but the challenge lies in tuning the loss function weights. To address this issue, we propose a novel method that incorporates Multi-Objective Optimization (MOO) into the training process of SISR models to balance perceptual quality and distortion. We conceptualize the relationship between loss weights and image quality assessment (IQA) metrics as black-box objective functions to be optimized within our Multi-Objective Bayesian Optimization Super-Resolution (MOBOSR) framework. This approach automates the hyperparameter tuning process, reduces overall computational cost, and enables the use of numerous loss functions simultaneously. Extensive experiments demonstrate that MOBOSR outperforms state-of-the-art methods in terms of both perceptual quality and distortion, significantly advancing the perception-distortion Pareto frontier. Our work points towards a new direction for future research on balancing perceptual quality and fidelity in nearly all image restoration tasks. The source code and pretrained models are available at: https://github.com/ZhuKeven/MOBOSR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03179v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiwen Zhu, Yanjie Wang, Shilv Cai, Liqun Chen, Jiahuan Zhou, Luxin Yan, Sheng Zhong, Xu Zou</dc:creator>
    </item>
    <item>
      <title>Enhancing digital core image resolution using optimal upscaling algorithm: with application to paired SEM images</title>
      <link>https://arxiv.org/abs/2409.03265</link>
      <description>arXiv:2409.03265v1 Announce Type: new 
Abstract: The porous media community extensively utilizes digital rock images for core analysis. High-resolution digital rock images that possess sufficient quality are essential but often challenging to acquire. Super-resolution (SR) approaches enhance the resolution of digital rock images and provide improved visualization of fine features and structures, aiding in the analysis and interpretation of rock properties, such as pore connectivity and mineral distribution. However, there is a current shortage of real paired microscopic images for super-resolution training. In this study, we used two types of Scanning Electron Microscopes (SEM) to obtain the images of shale samples in five regions, with 1X, 2X, 4X, 8X and 16X magnifications. We used these real scanned paired images as a reference to select the optimal method of image generation and validated it using Enhanced Deep Super Resolution (EDSR) and Very Deep Super Resolution (VDSR) methods. Our experiments show that the bilinear algorithm is more suitable than the commonly used bicubic method, for establishing low-resolution datasets in the SR approaches, which is partially attributed to the mechanism of Scanning Electron Microscopes (SEM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03265v1</guid>
      <category>eess.IV</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaohua You, Shuqi Sun, Zhengting Yan, Qinzhuo Liao, Huiying Tang, Lianhe Sun, Gensheng Li</dc:creator>
    </item>
    <item>
      <title>TBConvL-Net: A Hybrid Deep Learning Architecture for Robust Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2409.03367</link>
      <description>arXiv:2409.03367v1 Announce Type: new 
Abstract: Deep learning has shown great potential for automated medical image segmentation to improve the precision and speed of disease diagnostics. However, the task presents significant difficulties due to variations in the scale, shape, texture, and contrast of the pathologies. Traditional convolutional neural network (CNN) models have certain limitations when it comes to effectively modelling multiscale context information and facilitating information interaction between skip connections across levels. To overcome these limitations, a novel deep learning architecture is introduced for medical image segmentation, taking advantage of CNNs and vision transformers. Our proposed model, named TBConvL-Net, involves a hybrid network that combines the local features of a CNN encoder-decoder architecture with long-range and temporal dependencies using biconvolutional long-short-term memory (LSTM) networks and vision transformers (ViT). This enables the model to capture contextual channel relationships in the data and account for the uncertainty of segmentation over time. Additionally, we introduce a novel composite loss function that considers both the segmentation robustness and the boundary agreement of the predicted output with the gold standard. Our proposed model shows consistent improvement over the state of the art on ten publicly available datasets of seven different medical imaging modalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03367v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahzaib Iqbal, Tariq M. Khan, Syed S. Naqvi, Asim Naveed, Erik Meijering</dc:creator>
    </item>
    <item>
      <title>Tissue Concepts: supervised foundation models in computational pathology</title>
      <link>https://arxiv.org/abs/2409.03519</link>
      <description>arXiv:2409.03519v1 Announce Type: new 
Abstract: Due to the increasing workload of pathologists, the need for automation to support diagnostic tasks and quantitative biomarker evaluation is becoming more and more apparent. Foundation models have the potential to improve generalizability within and across centers and serve as starting points for data efficient development of specialized yet robust AI models. However, the training foundation models themselves is usually very expensive in terms of data, computation, and time. This paper proposes a supervised training method that drastically reduces these expenses. The proposed method is based on multi-task learning to train a joint encoder, by combining 16 different classification, segmentation, and detection tasks on a total of 912,000 patches. Since the encoder is capable of capturing the properties of the samples, we term it the Tissue Concepts encoder. To evaluate the performance and generalizability of the Tissue Concepts encoder across centers, classification of whole slide images from four of the most prevalent solid cancers - breast, colon, lung, and prostate - was used. The experiments show that the Tissue Concepts model achieve comparable performance to models trained with self-supervision, while requiring only 6% of the amount of training patches. Furthermore, the Tissue Concepts encoder outperforms an ImageNet pre-trained encoder on both in-domain and out-of-domain data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03519v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Till Nicke, Jan Raphael Schaefer, Henning Hoefener, Friedrich Feuerhake, Dorit Merhof, Fabian Kiessling, Johannes Lotz</dc:creator>
    </item>
    <item>
      <title>Geometry of the cumulant series in neuroimaging</title>
      <link>https://arxiv.org/abs/2409.03010</link>
      <description>arXiv:2409.03010v1 Announce Type: cross 
Abstract: Water diffusion gives rise to micrometer-scale sensitivity of diffusion MRI (dMRI) to cellular-level tissue structure. The advent of precision medicine and quantitative imaging hinges on revealing the information content of dMRI, and providing its parsimonious basis- and hardware-independent "fingerprint". Here we reveal the geometry of a multi-dimensional dMRI signal, classify all 21 invariants of diffusion and covariance tensors in terms of irreducible representations of the group of rotations, and relate them to tissue properties. Previously studied dMRI contrasts are expressed via 7 invariants, while the remaining 14 provide novel complementary information. We design acquisitions based on icosahedral vertices guaranteeing minimal number of measurements to determine 3-4 most used invariants in only 1-2 minutes for the whole brain. Representing dMRI signals via scalar invariant maps with definite symmetries will underpin machine learning classifiers of brain pathology, development, and aging, while fast protocols will enable translation of advanced dMRI into clinical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03010v1</guid>
      <category>physics.med-ph</category>
      <category>eess.IV</category>
      <category>physics.bio-ph</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santiago Coelho, Filip Szczepankiewicz, Els Fieremans, Dmitry S. Novikov</dc:creator>
    </item>
    <item>
      <title>Large \'Etendue 3D Holographic Display with Content-adpative Dynamic Fourier Modulation</title>
      <link>https://arxiv.org/abs/2409.03143</link>
      <description>arXiv:2409.03143v1 Announce Type: cross 
Abstract: Emerging holographic display technology offers unique capabilities for next-generation virtual reality systems. Current holographic near-eye displays, however, only support a small \'etendue, which results in a direct tradeoff between achievable field of view and eyebox size. \'Etendue expansion has recently been explored, but existing approaches are either fundamentally limited in the image quality that can be achieved or they require extremely high-speed spatial light modulators.
  We describe a new \'etendue expansion approach that combines multiple coherent sources with content-adaptive amplitude modulation of the hologram spectrum in the Fourier plane. To generate time-multiplexed phase and amplitude patterns for our spatial light modulators, we devise a pupil-aware gradient-descent-based computer-generated holography algorithm that is supervised by a large-baseline target light field. Compared with relevant baseline approaches, our method demonstrates significant improvements in image quality and \'etendue in simulation and with an experimental holographic display prototype.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03143v1</guid>
      <category>cs.GR</category>
      <category>eess.IV</category>
      <category>physics.optics</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3680528.3687600</arxiv:DOI>
      <dc:creator>Brian Chao, Manu Gopakumar, Suyeon Choi, Jonghyun Kim, Liang Shi, Gordon Wetzstein</dc:creator>
    </item>
    <item>
      <title>Pixel super-resolved lensless on-chip sensor with scattering multiplexing</title>
      <link>https://arxiv.org/abs/2105.14746</link>
      <description>arXiv:2105.14746v3 Announce Type: replace 
Abstract: Lensless on-chip microscopy has shown great potential for biomedical imaging due to its large-area and high-throughput imaging capabilities. By combining the pixel super-resolution (PSR) technique, it can improve the resolution beyond the limit of the imaging detector. However, existing PSR techniques are restricted to the feature size and crosstalk of modulation components (such as spatial light modulator), which cannot efficiently encode target information. Besides, the reconstruction algorithms suffer from the trade-off between image quality, reconstruction resolution and computational efficiency. In this work, we constructed a novel integrated lensless on-chip sensor via scattering multiplexing, and reported a robust PSR algorithm for sample reconstruction. The sensor employed a scattering layer as a modulator, which was permanently integrated with the detector. Benefiting from the high-degree-of-freedom reconstruction of the scattering layer, we realized fine wavefront modulation with a small feature size. The integration engineering avoided repetitious calibration and reduce the measurement complexity. The reported PSR algorithm combines both model-driven and data-driven strategies to efficiently exploit the high-frequency information from the fine modulation. A series of experiments validated that the reported sensor provides a low-cost solution for large-scale microscopic imaging, with significant advantages in resolution, image contrast and noise robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.14746v3</guid>
      <category>eess.IV</category>
      <category>physics.optics</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuyang Chang, Shaowei Jiang, Yongcun Hu, Liheng Bian</dc:creator>
    </item>
    <item>
      <title>Prediction of soil fertility parameters using USB-microscope imagery and portable X-ray fluorescence spectrometry</title>
      <link>https://arxiv.org/abs/2404.12415</link>
      <description>arXiv:2404.12415v2 Announce Type: replace 
Abstract: This study investigated the use of portable X-ray fluorescence (PXRF) spectrometry and soil image analysis for rapid soil fertility assessment, with a focus on key indicators such as available boron (B), organic carbon (OC), available manganese (Mn), available sulfur (S), and the sulfur availability index (SAI). A total of 1,133 soil samples from diverse agro-climatic zones in Eastern India were analyzed. The research integrated color and texture features from microscopic soil images, PXRF data, and auxiliary soil variables (AVs) using a Random Forest model. Results showed that combining image features (IFs) with AVs significantly improved prediction accuracy for available B (R2 = 0.80) and OC (R2 = 0.88). A data fusion approach, incorporating IFs, AVs, and PXRF data, further enhanced predictions for available Mn and SAI, with R2 values of 0.72 and 0.70, respectively. The study highlights the potential of integrating these technologies to offer rapid, cost-effective soil testing methods, paving the way for more advanced predictive models and a deeper understanding of soil fertility. Future work should explore the application of deep learning models on a larger dataset, incorporating soils from a wider range of agro-climatic zones under field conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12415v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.soilad.2024.100016</arxiv:DOI>
      <arxiv:journal_reference>Soil Advances, Volume 2, 2024, 100016</arxiv:journal_reference>
      <dc:creator>Shubhadip Dasgupta, Satwik Pate, Divya Rathore, L. G. Divyanth, Ayan Das, Anshuman Nayak, Subhadip Dey, Asim Biswas, David C. Weindorf, Bin Li, Sergio Henrique Godinho Silva, Bruno Teixeira Ribeiro, Sanjay Srivastava, Somsubhra Chakraborty</dc:creator>
    </item>
    <item>
      <title>Efficient Image Compression Using Advanced State Space Models</title>
      <link>https://arxiv.org/abs/2409.02743</link>
      <description>arXiv:2409.02743v2 Announce Type: replace 
Abstract: Transformers have led to learning-based image compression methods that outperform traditional approaches. However, these methods often suffer from high complexity, limiting their practical application. To address this, various strategies such as knowledge distillation and lightweight architectures have been explored, aiming to enhance efficiency without significantly sacrificing performance. This paper proposes a State Space Model-based Image Compression (SSMIC) architecture. This novel architecture balances performance and computational efficiency, making it suitable for real-world applications. Experimental evaluations confirm the effectiveness of our model in achieving a superior BD-rate while significantly reducing computational complexity and latency compared to competitive learning-based image compression methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02743v2</guid>
      <category>eess.IV</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bouzid Arezki, Anissa Mokraoui, Fangchen Feng</dc:creator>
    </item>
    <item>
      <title>TransKD: Transformer Knowledge Distillation for Efficient Semantic Segmentation</title>
      <link>https://arxiv.org/abs/2202.13393</link>
      <description>arXiv:2202.13393v4 Announce Type: replace-cross 
Abstract: Semantic segmentation benchmarks in the realm of autonomous driving are dominated by large pre-trained transformers, yet their widespread adoption is impeded by substantial computational costs and prolonged training durations. To lift this constraint, we look at efficient semantic segmentation from a perspective of comprehensive knowledge distillation and aim to bridge the gap between multi-source knowledge extractions and transformer-specific patch embeddings. We put forward the Transformer-based Knowledge Distillation (TransKD) framework which learns compact student transformers by distilling both feature maps and patch embeddings of large teacher transformers, bypassing the long pre-training process and reducing the FLOPs by &gt;85.0%. Specifically, we propose two fundamental modules to realize feature map distillation and patch embedding distillation, respectively: (1) Cross Selective Fusion (CSF) enables knowledge transfer between cross-stage features via channel attention and feature map distillation within hierarchical transformers; (2) Patch Embedding Alignment (PEA) performs dimensional transformation within the patchifying process to facilitate the patch embedding distillation. Furthermore, we introduce two optimization modules to enhance the patch embedding distillation from different perspectives: (1) Global-Local Context Mixer (GL-Mixer) extracts both global and local information of a representative embedding; (2) Embedding Assistant (EA) acts as an embedding method to seamlessly bridge teacher and student models with the teacher's number of channels. Experiments on Cityscapes, ACDC, NYUv2, and Pascal VOC2012 datasets show that TransKD outperforms state-of-the-art distillation frameworks and rivals the time-consuming pre-training method. The source code is publicly available at https://github.com/RuipingL/TransKD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.13393v4</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiping Liu, Kailun Yang, Alina Roitberg, Jiaming Zhang, Kunyu Peng, Huayao Liu, Yaonan Wang, Rainer Stiefelhagen</dc:creator>
    </item>
  </channel>
</rss>
