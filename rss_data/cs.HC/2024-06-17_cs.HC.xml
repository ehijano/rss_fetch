<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Jun 2024 02:48:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sentient House: Designing for Discourse</title>
      <link>https://arxiv.org/abs/2406.09419</link>
      <description>arXiv:2406.09419v1 Announce Type: new 
Abstract: The Sentient House project is an investigation into approaches that the artistdesigner can take to better involve the public in developing a critical perspective on pervasive technology in the home and the surrounding environment. Using Interaction Design approaches including workshops, surveys, rapidprototyping and critical thinking, this thesis suggests a framework for developing a more participatory atmosphere for Critical Design. As the world becomes more connected, and smarter, citizens concerns are being sidelined in favour of rapid progress and solutionism. Many of these initiatives are backed by government and commercial concerns who may not have the publics best interest at heart. The designs and approaches generated from this public participation seek to provide an outlet for a more agonistic debate and to develop tools and approaches to engage the public in questioning and addressing how technology affects them in the future. The outcomes of this research suggest that the public is receptive to a more active involvement in designing their digital future, and that the designer can be a critical component in revealing hidden consequences and alternative pathways for a more transparent and desirable future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09419v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Robert Collins</dc:creator>
    </item>
    <item>
      <title>"I see it as a wellspring for my positive and upward journey in life.": Understanding Current Practices of Assistive Technology's Customized Modification in China</title>
      <link>https://arxiv.org/abs/2406.09467</link>
      <description>arXiv:2406.09467v1 Announce Type: new 
Abstract: Due to the significant differences in physical conditions and living environments of people with disabilities, standardized assistive technologies (ATs) often fail to meet their needs. Modified AT, especially DIY (Do It Yourself) ATs, are a popular solution in many high-income countries, but there is a lack of documentation for low- and middle-income areas, especially in China, where the culture of philanthropy is undeveloped. To understand the current situation in this paper, we conducted semi-structured interviews with 10 individuals with disabilities using modified ATs and 10 individuals involved in providing these, including family members, standard assistive device manufacturers, and individuals employed for their modification skills, etc. Based on the results of the thematic analysis, we have summarized the general process of modified ATs for people with disabilities in China and the benefits these devices bring. We found that modified ATs not only make the lives of people with disabilities more comfortable and convenient but also bring them confidence, reduce social pressure, and even help them achieve self-realization. Additionally, we summarized the challenges they encountered before, during, and after the modification, including awareness gaps, family resistance, a lack of a business model, and so on. Specifically, we conducted a special case study about the typical business models and challenges currently faced by AT modification organizations in China. Our research provides important design foundations and research insights for the future of universal and personalized production of AT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09467v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>CSCW2024</arxiv:journal_reference>
      <dc:creator>Kexin Yang, Junyi Wu, Haokun Xin, Jiangtao Gong</dc:creator>
    </item>
    <item>
      <title>Hovering Over the Key to Text Input in XR</title>
      <link>https://arxiv.org/abs/2406.09579</link>
      <description>arXiv:2406.09579v1 Announce Type: new 
Abstract: Virtual, Mixed, and Augmented Reality (XR) technologies hold immense potential for transforming productivity beyond PC. Therefore there is a critical need for improved text input solutions for XR. However, achieving efficient text input in these environments remains a significant challenge. This paper examines the current landscape of XR text input techniques, focusing on the importance of keyboards (both physical and virtual) as essential tools. We discuss the unique challenges and opportunities presented by XR, synthesizing key trends from existing solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09579v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mar Gonzalez-Franco, Diar Abdlkarim, Arpit Bhatia, Stuart Macgregor, Jason Alexander Fotso-Puepi, Eric J Gonzalez, Hasti Seifi, Massimiliano Di Luca, Karan Ahuja</dc:creator>
    </item>
    <item>
      <title>Workload Assessment of Human-Machine Interface: A Simulator Study with Psychophysiological Measures</title>
      <link>https://arxiv.org/abs/2406.09603</link>
      <description>arXiv:2406.09603v1 Announce Type: new 
Abstract: Human-machine Interface (HMI) is critical for safety during automated driving, as it serves as the only media between the automated system and human users. To enable a transparent HMI, we first need to know how to evaluate it. However, most of the assessment methods used for HMI designs are subjective and thus not efficient. To bridge the gap, an objective and standardized HMI assessment method is needed, and the first step is to find an objective method for workload measurement for this context. In this study, two psychophysiological measures, electrocardiography (ECG) and electrodermal activity (EDA), were evaluated for their effectiveness in finding differences in mental workload among different HMI designs in a simulator study. Three HMI designs were developed and used. Results showed that both workload measures were able to identify significant differences in objective mental workload when interacting with in-vehicle HMIs. As a first step toward a standardized assessment method, the results could be used as a firm ground for future studies. Marie Sk{\l}odowska-Curie Actions; Innovative Training Network (ITN); SHAPE-IT; Grant number 860410; Publication date: [29 Sep 2023]; DOI: [10.54941/ahfe1004172]</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09603v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>AHFE (2023) International Conference. AHFE Open Access, vol 112. AHFE International, USA</arxiv:journal_reference>
      <dc:creator>Yuan-Cheng Liu, Nikol Figalova, Juergen Pichen, Philipp Hock, Martin Baumann, Klaus Bengler</dc:creator>
    </item>
    <item>
      <title>Human-Machine Interface Evaluation Using EEG in Driving Simulator</title>
      <link>https://arxiv.org/abs/2406.09608</link>
      <description>arXiv:2406.09608v1 Announce Type: new 
Abstract: Automated vehicles are pictured as the future of transportation, and facilitating safer driving is only one of the many benefits. However, due to the constantly changing role of the human driver, users are easily confused and have little knowledge about their responsibilities. Being the bridge between automation and human, the human-machine interface (HMI) is of great importance to driving safety. This study was conducted in a static driving simulator. Three HMI designs were developed, among which significant differences in mental workload using NASA-TLX and the subjective transparency test were found. An electroencephalogram was applied throughout the study to determine if differences in the mental workload could also be found using EEG's spectral power analysis. Results suggested that more studies are required to determine the effectiveness of the spectral power of EEG on mental workload, but the three interface designs developed in this study could serve as a solid basis for future research to evaluate the effectiveness of psychophysiological measures. Marie Sklodowska-Curie Actions; Innovative Training Network (ITN); SHAPE-IT; Grant number 860410; Publication date: [27 July 2023]; DOI: [10.1109/IV55152.2023.10186567]</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09608v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>2023 IEEE Intelligent Vehicles Symposium (IV), Anchorage, AK, USA, 2023, pp. 1-6</arxiv:journal_reference>
      <dc:creator>Y. C. Liu, N. Figalova, M. Baumann, K Bengler</dc:creator>
    </item>
    <item>
      <title>Recy-ctronics: Designing Fully Recyclable Electronics With Varied Form Factors</title>
      <link>https://arxiv.org/abs/2406.09611</link>
      <description>arXiv:2406.09611v1 Announce Type: new 
Abstract: For today's electronics manufacturing process, the emphasis on stable functionality, durability, and fixed physical forms is designed to ensure long-term usability. However, this focus on robustness and permanence complicates the disassembly and recycling processes, leading to significant environmental repercussions. In this paper, we present three approaches that leverage easily recyclable materials-specifically, polyvinyl alcohol (PVA) and liquid metal (LM)-alongside accessible manufacturing techniques to produce electronic components and systems with versatile form factors. Our work centers on the development of recyclable electronics through three methods: 1) creating sheet electronics by screen printing LM traces on PVA substrates; 2) developing foam-based electronics by immersing mechanically stirred PVA foam into an LM solution; and 3) fabricating recyclable electronic tubes by injecting LM into mold cast PVA tubes, which can then be woven into various structures. To further assess the sustainability of our proposed methods, we conducted a life cycle assessment (LCA) to evaluate the environmental impact of our recyclable electronics in comparison to their conventional counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09611v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tingyu Cheng, Zhihan Zhang, Han Huang, Yingting Gao, Wei Sun, Gregory D. Abowd, HyunJoo Oh, Josiah Hester</dc:creator>
    </item>
    <item>
      <title>Enhancing Text Corpus Exploration with Post Hoc Explanations and Comparative Design</title>
      <link>https://arxiv.org/abs/2406.09686</link>
      <description>arXiv:2406.09686v1 Announce Type: new 
Abstract: Text corpus exploration (TCE) spans the range of exploratory search tasks: it goes beyond simple retrieval to include item discovery and learning about the corpus and topic. Systems support TCE with tools such as similarity-based recommendations and embedding-based spatial maps. However, these tools address specific tasks; current systems lack the flexibility to support the range of tasks encountered in practice and the iterative, multiscale, workflows users employ. In this paper, we provide methods that enhance TCE tools with post hoc explanations and multiscale, comparative designs to provide flexible support for user needs. We introduce salience functions as a mechanism to provide post hoc explanations of similarity, recommendations, and spatial placement. This post hoc strategy allows our approach to complement a variety of underlying algorithms; the salience functions provide both exemplar- and feature-based explanations at scales ranging from individual documents through to the entire corpus. These explanations are incorporated into a set of views that operate at multiple scales. The views use design elements that explicitly support comparison to enable flexible integration. Together, these form an approach that provides a flexible toolset that can address a range of tasks. We demonstrate our approach in a prototype system that enables the exploration of corpora of paper abstracts and newspaper archives. Examples illustrate how our approach enables the system to flexibly support a wide range of tasks and workflows that emerge in user scenarios. A user study confirms that researchers are able to use our system to achieve a variety of tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09686v1</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Gleicher, Keaton Leppenan, Yunyu Bai</dc:creator>
    </item>
    <item>
      <title>Road to Serenity: Individual Variations in the Efficacy of Unobtrusive Respiratory Guidance for Driving Stress Regulation</title>
      <link>https://arxiv.org/abs/2406.09777</link>
      <description>arXiv:2406.09777v1 Announce Type: new 
Abstract: Stress impacts driving-related cognitive functions like attention and decision-making, and may arise in automated vehicles due to non-driving tasks. Unobtrusive relaxation techniques are needed to regulate stress without distracting from driving. Tactile wearables have shown efficacy in stress regulation through respiratory guidance, but individual variations may affect their efficacy. This study assessed slow-breathing tactile guidance under different stress levels on 85 participants. Physiological, behavioral and subjective data were collected. The influence of individual variations (e.g., driving habits and behavior, personality) using logistic regression analysis was explored. Participants could follow the guidance and adjust breathing while driving, but subjective efficacy depended on individual variations linked to different efficiency in using the technique, in relation with its attentional cost. An influence of factors linked to the evaluation of context criticality was also found. The results suggest that considering individual and contextual variations is crucial in designing and using such techniques in demanding driving contexts. In this line some design recommendations and insights for further studies are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09777v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.apergo.2024.104334</arxiv:DOI>
      <arxiv:journal_reference>A.J. Bequet, C. Jallais, J. Quick, D. Ndiaye, A.R. Hidalgo-Munoz, Road to serenity: Individual variations in the efficacy of unobtrusive respiratory guidance for driving stress regulation, Applied Ergonomics, Volume 120, 2024, 104334</arxiv:journal_reference>
      <dc:creator>A. J. Bequet, C. Jallais, J. Quick, D. Ndiaye, A. R. Hidalgo-Munoz</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Personalized Voice Activity Detection Systems: Assessing Real-World Effectiveness</title>
      <link>https://arxiv.org/abs/2406.09443</link>
      <description>arXiv:2406.09443v1 Announce Type: cross 
Abstract: Voice activity detection (VAD) is a critical component in various applications such as speech recognition, speech enhancement, and hands-free communication systems. With the increasing demand for personalized and context-aware technologies, the need for effective personalized VAD systems has become paramount. In this paper, we present a comparative analysis of Personalized Voice Activity Detection (PVAD) systems to assess their real-world effectiveness. We introduce a comprehensive approach to assess PVAD systems, incorporating various performance metrics such as frame-level and utterance-level error rates, detection latency and accuracy, alongside user-level analysis. Through extensive experimentation and evaluation, we provide a thorough understanding of the strengths and limitations of various PVAD variants. This paper advances the understanding of PVAD technology by offering insights into its efficacy and viability in practical applications using a comprehensive set of metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09443v1</guid>
      <category>eess.AS</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Satyam Kumar (Oggi), Sai Srujana Buddi (Oggi), Utkarsh Oggy Sarawgi (Oggi), Vineet Garg (Oggi), Shivesh Ranjan (Oggi),  Ognjen (Oggi),  Rudovic, Ahmed Hussen Abdelaziz, Saurabh Adya</dc:creator>
    </item>
    <item>
      <title>Multimodal Large Language Models with Fusion Low Rank Adaptation for Device Directed Speech Detection</title>
      <link>https://arxiv.org/abs/2406.09617</link>
      <description>arXiv:2406.09617v1 Announce Type: cross 
Abstract: Although Large Language Models (LLMs) have shown promise for human-like conversations, they are primarily pre-trained on text data. Incorporating audio or video improves performance, but collecting large-scale multimodal data and pre-training multimodal LLMs is challenging. To this end, we propose a Fusion Low Rank Adaptation (FLoRA) technique that efficiently adapts a pre-trained unimodal LLM to consume new, previously unseen modalities via low rank adaptation. For device-directed speech detection, using FLoRA, the multimodal LLM achieves 22% relative reduction in equal error rate (EER) over the text-only approach and attains performance parity with its full fine-tuning (FFT) counterpart while needing to tune only a fraction of its parameters. Furthermore, with the newly introduced adapter dropout, FLoRA is robust to missing data, improving over FFT by 20% lower EER and 56% lower false accept rate. The proposed approach scales well for model sizes from 16M to 3B parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09617v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shruti Palaskar, Oggi Rudovic, Sameer Dharur, Florian Pesce, Gautam Krishna, Aswin Sivaraman, Jack Berkowitz, Ahmed Hussen Abdelaziz, Saurabh Adya, Ahmed Tewfik</dc:creator>
    </item>
    <item>
      <title>Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for Improving User Experience at First Meeting</title>
      <link>https://arxiv.org/abs/2406.09839</link>
      <description>arXiv:2406.09839v1 Announce Type: cross 
Abstract: Rapport is known as a conversational aspect focusing on relationship building, which influences outcomes in collaborative tasks. This study aims to establish human-agent rapport through small talk by using a rapport-building strategy. We implemented this strategy for the virtual agents based on dialogue strategies by prompting a large language model (LLM). In particular, we utilized two dialogue strategies-predefined sequence and free-form-to guide the dialogue generation framework. We conducted analyses based on human evaluations, examining correlations between total turn, utterance characters, rapport score, and user experience variables: naturalness, satisfaction, interest, engagement, and usability. We investigated correlations between rapport score and naturalness, satisfaction, engagement, and conversation flow. Our experimental results also indicated that using free-form to prompt the rapport-building strategy performed the best in subjective scores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09839v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Yeza Baihaqi, Angel Garc\'ia Contreras, Seiya Kawano, Koichiro Yoshino</dc:creator>
    </item>
    <item>
      <title>What Does it Take to Generalize SER Model Across Datasets? A Comprehensive Benchmark</title>
      <link>https://arxiv.org/abs/2406.09933</link>
      <description>arXiv:2406.09933v1 Announce Type: cross 
Abstract: Speech emotion recognition (SER) is essential for enhancing human-computer interaction in speech-based applications. Despite improvements in specific emotional datasets, there is still a research gap in SER's capability to generalize across real-world situations. In this paper, we investigate approaches to generalize the SER system across different emotion datasets. In particular, we incorporate 11 emotional speech datasets and illustrate a comprehensive benchmark on the SER task. We also address the challenge of imbalanced data distribution using over-sampling methods when combining SER datasets for training. Furthermore, we explore various evaluation protocols for adeptness in the generalization of SER. Building on this, we explore the potential of Whisper for SER, emphasizing the importance of thorough evaluation. Our approach is designed to advance SER technology by integrating speaker-independent methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09933v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adham Ibrahim, Shady Shehata, Ajinkya Kulkarni, Mukhtar Mohamed, Muhammad Abdul-Mageed</dc:creator>
    </item>
    <item>
      <title>Bridging the Communication Gap: Artificial Agents Learning Sign Language through Imitation</title>
      <link>https://arxiv.org/abs/2406.10043</link>
      <description>arXiv:2406.10043v1 Announce Type: cross 
Abstract: Artificial agents, particularly humanoid robots, interact with their environment, objects, and people using cameras, actuators, and physical presence. Their communication methods are often pre-programmed, limiting their actions and interactions. Our research explores acquiring non-verbal communication skills through learning from demonstrations, with potential applications in sign language comprehension and expression. In particular, we focus on imitation learning for artificial agents, exemplified by teaching a simulated humanoid American Sign Language. We use computer vision and deep learning to extract information from videos, and reinforcement learning to enable the agent to replicate observed actions. Compared to other methods, our approach eliminates the need for additional hardware to acquire information. We demonstrate how the combination of these different techniques offers a viable way to learn sign language. Our methodology successfully teaches 5 different signs involving the upper body (i.e., arms and hands). This research paves the way for advanced communication skills in artificial agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10043v1</guid>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federico Tavella, Aphrodite Galata, Angelo Cangelosi</dc:creator>
    </item>
    <item>
      <title>Detecting the terminality of speech-turn boundary for spoken interactions in French TV and Radio content</title>
      <link>https://arxiv.org/abs/2406.10073</link>
      <description>arXiv:2406.10073v1 Announce Type: cross 
Abstract: Transition Relevance Places are defined as the end of an utterance where the interlocutor may take the floor without interrupting the current speaker --i.e., a place where the turn is terminal. Analyzing turn terminality is useful to study the dynamic of turn-taking in spontaneous conversations. This paper presents an automatic classification of spoken utterances as Terminal or Non-Terminal in multi-speaker settings. We compared audio, text, and fusions of both approaches on a French corpus of TV and Radio extracts annotated with turn-terminality information at each speaker change. Our models are based on pre-trained self-supervised representations. We report results for different fusion strategies and varying context sizes. This study also questions the problem of performance variability by analyzing the differences in results for multiple training runs with random initialization. The measured accuracy would allow the use of these models for large-scale analysis of turn-taking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10073v1</guid>
      <category>eess.AS</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>R\'emi Uro, Marie Tahon, David Doukhan, Antoine Laurent, Albert Rilliard</dc:creator>
    </item>
    <item>
      <title>Mysterious and Manipulative Black Boxes: A Qualitative Analysis of Perceptions on Recommender Systems</title>
      <link>https://arxiv.org/abs/2302.09933</link>
      <description>arXiv:2302.09933v5 Announce Type: replace 
Abstract: Recommender systems are used to provide relevant suggestions on various matters. Although these systems are a classical research topic, knowledge is still limited regarding the public opinion about these systems. Public opinion is also important because the systems are known to cause various problems. To this end, this paper presents a qualitative analysis of the perceptions of ordinary citizens, civil society groups, businesses, and others on recommender systems in Europe. The dataset examined is based on the answers submitted to a consultation about the Digital Services Act (DSA) recently enacted in the European Union (EU). Therefore, not only does the paper contribute to the pressing question about regulating new technologies and online platforms, but it also reveals insights about the policy-making of the DSA. According to the qualitative results, Europeans have generally negative opinions about recommender systems and the quality of their recommendations. The systems are widely seen to violate privacy and other fundamental rights. According to many Europeans, these also cause various societal problems, including even threats to democracy. Furthermore, existing regulations in the EU are commonly seen to have failed due to a lack of proper enforcement. Numerous suggestions were made by the respondents to the consultation for improving the situation, but only a few of these ended up to the DSA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09933v5</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5210/fm.v29i6.13357</arxiv:DOI>
      <dc:creator>Jukka Ruohonen</dc:creator>
    </item>
    <item>
      <title>How to Teach Programming in the AI Era? Using LLMs as a Teachable Agent for Debugging</title>
      <link>https://arxiv.org/abs/2310.05292</link>
      <description>arXiv:2310.05292v4 Announce Type: replace 
Abstract: Large Language Models (LLMs) now excel at generative skills and can create content at impeccable speeds. However, they are imperfect and still make various mistakes. In a Computer Science education context, as these models are widely recognized as "AI pair programmers," it becomes increasingly important to train students on evaluating and debugging the LLM-generated code. In this work, we introduce HypoCompass, a novel system to facilitate deliberate practice on debugging, where human novices play the role of Teaching Assistants and help LLM-powered teachable agents debug code. We enable effective task delegation between students and LLMs in this learning-by-teaching environment: students focus on hypothesizing the cause of code errors, while adjacent skills like code completion are offloaded to LLM-agents. Our evaluations demonstrate that HypoCompass generates high-quality training materials (e.g., bugs and fixes), outperforming human counterparts fourfold in efficiency, and significantly improves student performance on debugging by 12% in the pre-to-post test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05292v4</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianou Ma, Hua Shen, Kenneth Koedinger, Tongshuang Wu</dc:creator>
    </item>
    <item>
      <title>Anticipating User Needs: Insights from Design Fiction on Conversational Agents for Computational Thinking</title>
      <link>https://arxiv.org/abs/2311.06887</link>
      <description>arXiv:2311.06887v2 Announce Type: replace 
Abstract: Computational thinking, and by extension, computer programming, is notoriously challenging to learn. Conversational agents and generative artificial intelligence (genAI) have the potential to facilitate this learning process by offering personalized guidance, interactive learning experiences, and code generation. However, current genAI-based chatbots focus on professional developers and may not adequately consider educational needs. Involving educators in conceiving educational tools is critical for ensuring usefulness and usability. We enlisted nine instructors to engage in design fiction sessions in which we elicited abilities such a conversational agent supported by genAI should display. Participants envisioned a conversational agent that guides students stepwise through exercises, tuning its method of guidance with an awareness of the educational background, skills and deficits, and learning preferences. The insights obtained in this paper can guide future implementations of tutoring conversational agents oriented toward teaching computational thinking and computer programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06887v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-54975-5_12</arxiv:DOI>
      <dc:creator>Jacob Penney, Jo\~ao Felipe Pimentel, Igor Steinmacher, Marco A. Gerosa</dc:creator>
    </item>
    <item>
      <title>Sunnie: An Anthropomorphic LLM-Based Conversational Agent for Mental Well-Being Activity Recommendation</title>
      <link>https://arxiv.org/abs/2405.13803</link>
      <description>arXiv:2405.13803v2 Announce Type: replace 
Abstract: A longstanding challenge in mental well-being support is the reluctance of people to adopt psychologically beneficial activities, often due to lack of motivation, low perceived trustworthiness, and limited personalization of recommendations. Chatbots have shown promise in promoting positive mental health practices, yet their rigid interaction flows and less human-like conversational experiences present significant limitations. In this work, we explore whether the anthropomorphic design (both LLM's persona design and conversational experience design) can enhance users' perception of the system and their willingness to adopt mental well-being activity recommendations. To this end, we introduce Sunnie, an anthropomorphic LLM-based conversational agent designed to offer personalized well-being support through multi-turn conversation and recommend practical actions grounded in positive psychology and social psychology. An empirical user study comparing the user experience with Sunnie and with a traditional survey-based activity recommendation system suggests that the anthropomorphic characteristics of Sunnie significantly enhance users' perception of the system and the overall usability; nevertheless, users' willingness to adopt activity recommendations did not change significantly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13803v2</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Siyi Wu, Feixue Han, Bingsheng Yao, Tianyi Xie, Xuan Zhao, Dakuo Wang</dc:creator>
    </item>
    <item>
      <title>POEM: Interactive Prompt Optimization for Enhancing Multimodal Reasoning of Large Language Models</title>
      <link>https://arxiv.org/abs/2406.03843</link>
      <description>arXiv:2406.03843v2 Announce Type: replace 
Abstract: Large language models (LLMs) have exhibited impressive abilities for multimodal content comprehension and reasoning with proper prompting in zero- or few-shot settings. Despite the proliferation of interactive systems developed to support prompt engineering for LLMs across various tasks, most have primarily focused on textual or visual inputs, thus neglecting the complex interplay between modalities within multimodal inputs. This oversight hinders the development of effective prompts that guide model multimodal reasoning processes by fully exploiting the rich context provided by multiple modalities. In this paper, we present POEM, a visual analytics system to facilitate efficient prompt engineering for enhancing the multimodal reasoning performance of LLMs. The system enables users to explore the interaction patterns across modalities at varying levels of detail for a comprehensive understanding of the multimodal knowledge elicited by various prompts. Through diverse recommendations of demonstration examples and instructional principles, POEM supports users in iteratively crafting and refining prompts to better align and enhance model knowledge with human insights. The effectiveness and efficiency of our system are validated through two case studies and interviews with experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03843v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianben He, Xingbo Wang, Shiyi Liu, Guande Wu, Claudio Silva, Huamin Qu</dc:creator>
    </item>
    <item>
      <title>A computational medical XR discipline</title>
      <link>https://arxiv.org/abs/2108.04136</link>
      <description>arXiv:2108.04136v5 Announce Type: replace-cross 
Abstract: Computational Medical Extended Reality (CMXR), brings together life sciences and neuroscience with mathematics, engineering and computer science. It unifies computational science (scientific computing) with intelligent extended reality and spatial computing for the medical field. It significantly differs from previous "Clinical XR" or "Medical XR" terms, as it is focusing on how to integrate computational methods from neural simulation to computational geometry, computational vision and computer graphics with deep learning models to solve specific hard problems in medicine and neuroscience: from low/no-code/genAI authoring platforms to deep learning XR systems for training, planning, operative navigation, therapy and rehabilitation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.04136v5</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Papagiannakis, Walter Greenleaf, Michael Cole, Mark Zhang, Rabi Datta, Mathias Delahaye, Eleni Grigoriou, Manos Kamarianakis, Antonis Protopsaltis, Philippe Bijlenga, Nadia Magnenat-Thalmann, Eleftherios Tsiridis, Eustathios Kenanidis, Kyriakos Vamvakidis, Ioannis Koutelidakis, Oliver A Kannape</dc:creator>
    </item>
    <item>
      <title>Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding</title>
      <link>https://arxiv.org/abs/2301.11564</link>
      <description>arXiv:2301.11564v2 Announce Type: replace-cross 
Abstract: Robotic grasping is a fundamental ability for a robot to interact with the environment. Current methods focus on how to obtain a stable and reliable grasping pose in object level, while little work has been studied on part (shape)-wise grasping which is related to fine-grained grasping and robotic affordance. Parts can be seen as atomic elements to compose an object, which contains rich semantic knowledge and a strong correlation with affordance. However, lacking a large part-wise 3D robotic dataset limits the development of part representation learning and downstream applications. In this paper, we propose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to promote 3D part-level affordance and grasping ability learning. From the perspective of robotic cognition, we design a two-stage fine-grained robotic grasping framework (named LangPartGPD), including a novel 3D part language grounding model and a part-aware grasp pose detection model, in which explicit language input from human or large language models (LLMs) could guide a robot to generate part-level 6-DoF grasping pose with textual explanation. Our method combines the advantages of human-robot collaboration and LLMs' planning ability using explicit language as a symbolic intermediate. To evaluate the effectiveness of our proposed method, we perform 3D part grounding and fine-grained grasp detection experiments on both simulation and physical robot settings, following language instructions across different degrees of textual complexity. Results show our method achieves competitive performance in 3D geometry fine-grained grounding, object affordance inference, and 3D part-aware grasping tasks. Our dataset and code are available on our project website https://sites.google.com/view/lang-shape</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.11564v2</guid>
      <category>cs.RO</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yaoxian Song, Penglei Sun, Piaopiao Jin, Yi Ren, Yu Zheng, Zhixu Li, Xiaowen Chu, Yue Zhang, Tiefeng Li, Jason Gu</dc:creator>
    </item>
    <item>
      <title>Context-Aware Prediction of User Engagement on Online Social Platforms</title>
      <link>https://arxiv.org/abs/2310.14533</link>
      <description>arXiv:2310.14533v2 Announce Type: replace-cross 
Abstract: The success of online social platforms hinges on their ability to predict and understand user behavior at scale. Here, we present data suggesting that context-aware modeling approaches may offer a holistic yet lightweight and potentially privacy-preserving representation of user engagement on online social platforms. Leveraging deep LSTM neural networks to analyze more than 100 million Snapchat sessions from almost 80.000 users, we demonstrate that patterns of active and passive use are predictable from past behavior (R2=0.345) and that the integration of context features substantially improves predictive performance compared to the behavioral baseline model (R2=0.522). Features related to smartphone connectivity status, location, temporal context, and weather were found to capture non-redundant variance in user engagement relative to features derived from histories of in-app behaviors. Further, we show that a large proportion of variance can be accounted for with minimal behavioral histories if momentary context is considered (R2=0.442). These results indicate the potential of context-aware approaches for making models more efficient and privacy-preserving by reducing the need for long data histories. Finally, we employ model explainability techniques to glean preliminary insights into the underlying behavioral mechanisms. Our findings are consistent with the notion of context-contingent, habit-driven patterns of active and passive use, underscoring the value of contextualized representations of user behavior for predicting user engagement on social platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14533v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heinrich Peters, Yozen Liu, Francesco Barbieri, Raiyan Abdul Baten, Sandra C. Matz, Maarten W. Bos</dc:creator>
    </item>
    <item>
      <title>Multisensory extended reality applications offer benefits for volumetric biomedical image analysis in research and medicine</title>
      <link>https://arxiv.org/abs/2311.03986</link>
      <description>arXiv:2311.03986v2 Announce Type: replace-cross 
Abstract: 3D data from high-resolution volumetric imaging is a central resource for diagnosis and treatment in modern medicine. While the fast development of AI enhances imaging and analysis, commonly used visualization methods lag far behind. Recent research used extended reality (XR) for perceiving 3D images with visual depth perception and touch but used restrictive haptic devices. While unrestricted touch benefits volumetric data examination, implementing natural haptic interaction with XR is challenging. The research question is whether a multisensory XR application with intuitive haptic interaction adds value and should be pursued. In a study, 24 experts for biomedical images in research and medicine explored 3D medical shapes with 3 applications: a multisensory virtual reality (VR) prototype using haptic gloves, a simple VR prototype using controllers, and a standard PC application. Results of standardized questionnaires showed no significant differences between all application types regarding usability and no significant difference between both VR applications regarding presence. Participants agreed to statements that VR visualizations provide better depth information, using the hands instead of controllers simplifies data exploration, the multisensory VR prototype allows intuitive data exploration, and it is beneficial over traditional data examination methods. While most participants mentioned manual interaction as best aspect, they also found it the most improvable. We conclude that a multisensory XR application with improved manual interaction adds value for volumetric biomedical data examination. We will proceed with our open-source research project ISH3DE (Intuitive Stereoptic Haptic 3D Data Exploration) to serve medical education, therapeutic decisions, surgery preparations, or research data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03986v2</guid>
      <category>cs.SE</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <category>eess.IV</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10278-024-01094-x</arxiv:DOI>
      <arxiv:journal_reference>Journal of Imaging Informatics in Medicine, 1-10 (2024)</arxiv:journal_reference>
      <dc:creator>Kathrin Krieger, Jan Egger, Jens Kleesiek, Matthias Gunzer, Jianxu Chen</dc:creator>
    </item>
    <item>
      <title>Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling</title>
      <link>https://arxiv.org/abs/2402.17019</link>
      <description>arXiv:2402.17019v3 Announce Type: replace-cross 
Abstract: Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 294 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop approach to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through randomized controlled trials (RCTs) with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17019v3</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hang Jiang, Xiajie Zhang, Robert Mahari, Daniel Kessler, Eric Ma, Tal August, Irene Li, Alex 'Sandy' Pentland, Yoon Kim, Jad Kabbara, Deb Roy</dc:creator>
    </item>
    <item>
      <title>"Did They F***ing Consent to That?": Safer Digital Intimacy via Proactive Protection Against Image-Based Sexual Abuse</title>
      <link>https://arxiv.org/abs/2403.04659</link>
      <description>arXiv:2403.04659v2 Announce Type: replace-cross 
Abstract: As many as 8 in 10 adults share intimate content such as nude or lewd images. Sharing such content has significant benefits for relationship intimacy and body image, and can offer employment. However, stigmatizing attitudes and a lack of technological mitigations put those sharing such content at risk of sexual violence. An estimated 1 in 3 people have been subjected to image-based sexual abuse (IBSA), a spectrum of violence that includes the nonconsensual distribution or threat of distribution of consensually-created intimate content (also called NDII). In this work, we conducted a rigorous empirical interview study of 52 European creators of intimate content to examine the threats they face and how they defend against them, situated in the context of their different use cases for intimate content sharing and their choice of technologies for storing and sharing such content. Synthesizing our results with the limited body of prior work on technological prevention of NDII, we offer concrete next steps for both platforms and security &amp; privacy researchers to work toward safer intimate content sharing through proactive protection.
  Content Warning: This work discusses sexual violence, specifically, the harms of image-based sexual abuse (particularly in Sections 2 and 6).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04659v2</guid>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lucy Qin, Vaughn Hamilton, Sharon Wang, Yigit Aydinalp, Marin Scarlett, Elissa M. Redmiles</dc:creator>
    </item>
  </channel>
</rss>
