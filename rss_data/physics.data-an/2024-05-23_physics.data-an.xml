<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics</title>
      <link>https://arxiv.org/abs/2405.14806</link>
      <description>arXiv:2405.14806v1 Announce Type: new 
Abstract: Extracting scientific understanding from particle-physics experiments requires solving diverse learning problems with high precision and good data efficiency. We propose the Lorentz Geometric Algebra Transformer (L-GATr), a new multi-purpose architecture for high-energy physics. L-GATr represents high-energy data in a geometric algebra over four-dimensional space-time and is equivariant under Lorentz transformations, the symmetry group of relativistic kinematics. At the same time, the architecture is a Transformer, which makes it versatile and scalable to large systems. L-GATr is first demonstrated on regression and classification tasks from particle physics. We then construct the first Lorentz-equivariant generative model: a continuous normalizing flow based on an L-GATr network, trained with Riemannian flow matching. Across our experiments, L-GATr is on par with or outperforms strong domain-specific baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14806v1</guid>
      <category>physics.data-an</category>
      <category>cs.LG</category>
      <category>hep-ph</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Spinner, Victor Bres\'o, Pim de Haan, Tilman Plehn, Jesse Thaler, Johann Brehmer</dc:creator>
    </item>
    <item>
      <title>Machine learning for exoplanet detection in high-contrast spectroscopy Combining cross correlation maps and deep learning on medium-resolution integral-field spectra</title>
      <link>https://arxiv.org/abs/2405.13468</link>
      <description>arXiv:2405.13468v1 Announce Type: cross 
Abstract: The advent of high-contrast imaging instruments combined with medium-resolution spectrographs allows spectral and temporal dimensions to be combined with spatial dimensions to detect and potentially characterize exoplanets with higher sensitivity. We develop a new method to effectively leverage the spectral and spatial dimensions in integral-field spectroscopy (IFS) datasets using a supervised deep-learning algorithm to improve the detection sensitivity to high-contrast exoplanets. We begin by applying a data transform whereby the IFS datasets are replaced by cross-correlation coefficient tensors obtained by cross-correlating our data with young gas giant spectral template spectra. This transformed data is then used to train machine learning (ML) algorithms. We train a 2D CNN and 3D LSTM with our data. We compare the ML models with a non-ML algorithm, based on the STIM map of arXiv:1810.06895. We test our algorithms on simulated young gas giants in a dataset that contains no known exoplanet, and explore the sensitivity of algorithms to detect these exoplanets at contrasts ranging from 1e-3 to 1e-4 at different radial separations. We quantify the sensitivity using modified receiver operating characteristic curves (mROC). We discover that the ML algorithms produce fewer false positives and have a higher true positive rate than the STIM-based algorithm, and the true positive rate of ML algorithms is less impacted by changing radial separation. We discover that the velocity dimension is an important differentiating factor. Through this paper, we demonstrate that ML techniques have the potential to improve the detection limits and reduce false positives for directly imaged planets in IFS datasets, after transforming the spectral dimension into a radial velocity dimension through a cross-correlation operation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13468v1</guid>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>physics.app-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rakesh Nath-Ranga, Olivier Absil, Valentin Christiaens, Emily O. Garvin</dc:creator>
    </item>
    <item>
      <title>Window and inpainting: dealing with data gaps for TianQin</title>
      <link>https://arxiv.org/abs/2405.14274</link>
      <description>arXiv:2405.14274v1 Announce Type: cross 
Abstract: Space-borne gravitational wave detectors like TianQin might encounter data gaps due to factors like micro-meteoroid collisions or hardware failures. Such glitches will cause discontinuity in the data and have been observed in the LISA Pathfinder. The existence of such data gaps presents challenges to the data analysis for TianQin, especially for massive black hole binary mergers, since its signal-to-noise ratio (SNR) accumulates in a non-linear way, a gap near the merger could lead to significant loss of SNR. It could introduce bias in the estimate of noise properties, and furthermore the results of the parameter estimation. In this work, using simulated TianQin data with injected a massive black hole binary merger, we study the window function method, and for the first time, the inpainting method to cope with the data gap, and an iterative estimate scheme is designed to properly estimate the noise spectrum. We find that both methods can properly estimate noise and signal parameters. The easy-to-implement window function method can already perform well, except that it will sacrifice some SNR due to the adoption of the window. The inpainting method is slower, but it can minimize the impact of the data gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14274v1</guid>
      <category>gr-qc</category>
      <category>astro-ph.GA</category>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Wang, Hong-Yu Chen, Xiangyu Lyu, En-Kun Li, Yi-Ming Hu</dc:creator>
    </item>
    <item>
      <title>Leveraging Machine Learning for Advanced Nanoscale X-ray Analysis: Unmixing Multicomponent Signals and Enhancing Chemical Quantification</title>
      <link>https://arxiv.org/abs/2405.14649</link>
      <description>arXiv:2405.14649v1 Announce Type: cross 
Abstract: Energy dispersive X-ray (EDX) spectroscopy in the transmission electron microscope is a key tool for nanomaterials analysis, providing a direct link between spatial and chemical information. However, using it for precisely determining chemical compositions presents challenges of noisy data from low X-ray yields and mixed signals from phases that overlap along the electron beam trajectory. Here, we introduce a novel method, non-negative matrix factorisation based pan-sharpening (PSNMF), to address these limitations. Leveraging the Poisson nature of EDX spectral noise and binning operations, PSNMF retrieves high quality phase spectral and spatial signatures via consecutive factorisations. After validating PSNMF with synthetic datasets of different noise levels, we illustrate its effectiveness on two distinct experimental cases: a nano-mineralogical lamella, and supported catalytic nanoparticles. Not only does PSNMF obtain accurate phase signatures, datasets reconstructed from the outputs have demonstrably lower noise and better fidelity than from the benchmark denoising method of principle component analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14649v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.mes-hall</category>
      <category>physics.data-an</category>
      <category>physics.ins-det</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hui Chen, Duncan T. L. Alexander, C\'ecile H\'ebert</dc:creator>
    </item>
    <item>
      <title>Decomposing causality into its synergistic, unique, and redundant components</title>
      <link>https://arxiv.org/abs/2405.12411</link>
      <description>arXiv:2405.12411v2 Announce Type: replace 
Abstract: Causality lies at the heart of scientific inquiry, serving as the fundamental basis for understanding interactions among variables in physical systems. Despite its central role, current methods for causal inference face significant challenges due to nonlinear dependencies, stochastic interactions, self-causation, collider effects, and influences from exogenous factors, among others. While existing methods can effectively address some of these challenges, no single approach has successfully integrated all these aspects. Here, we address these challenges with SURD: Synergistic-Unique-Redundant Decomposition of causality. SURD quantifies causality as the increments of redundant, unique, and synergistic information gained about future events from past observations. The formulation is non-intrusive and applicable to both computational and experimental investigations, even when samples are scarce. We benchmark SURD in scenarios that pose significant challenges for causal inference and demonstrate that it offers a more reliable quantification of causality compared to previous methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12411v2</guid>
      <category>physics.data-an</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>\'Alvaro Mart\'inez-S\'anchez, Gonzalo Arranz, Adri\'an Lozano-Dur\'an</dc:creator>
    </item>
    <item>
      <title>Data-driven analysis of annual rain distributions</title>
      <link>https://arxiv.org/abs/2312.03861</link>
      <description>arXiv:2312.03861v2 Announce Type: replace-cross 
Abstract: Rainfall is an important component of the climate system and its statistical properties are vital for prediction purposes. In this study, we have developed a statistical method for constructing the distribution of annual precipitation. The method is based on the convolution of the measured monthly rainfall distributions and does not depend on any presumed annual rainfall distribution. Using a simple statistical model, we demonstrate that our approach allows for a better prediction of extremely dry or wet years with a recurrence time several times longer than the original time series. The method that has been proposed can be utilized for other climate variables as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03861v2</guid>
      <category>physics.ao-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevResearch.6.023187</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. Research 6, 023187 (2024)</arxiv:journal_reference>
      <dc:creator>Yosef Ashkenazy, Naftali R. Smith</dc:creator>
    </item>
    <item>
      <title>Importance of hyper-parameter optimization during training of physics-informed deep learning networks</title>
      <link>https://arxiv.org/abs/2405.08580</link>
      <description>arXiv:2405.08580v2 Announce Type: replace-cross 
Abstract: Incorporating scientific knowledge into deep learning (DL) models for materials-based simulations can constrain the network's predictions to be within the boundaries of the material system. Altering loss functions or adding physics-based regularization (PBR) terms to reflect material properties informs a network about the physical constraints the simulation should obey. The training and tuning process of a DL network greatly affects the quality of the model, but how this process differs when using physics-based loss functions or regularization terms is not commonly discussed. In this manuscript, several PBR methods are implemented to enforce stress equilibrium on a network predicting the stress fields of a high elastic contrast composite. Models with PBR enforced the equilibrium constraint more accurately than a model without PBR, and the stress equilibrium converged more quickly. More importantly, it was observed that independently fine-tuning each implementation resulted in more accurate models. More specifically, each loss formulation and dataset required different learning rates and loss weights for the best performance. This result has important implications on assessing the relative effectiveness of different DL models and highlights important considerations when making a comparison between DL methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08580v2</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ashley Lenau, Dennis M. Dimiduk, Stephen R. Niezgoda</dc:creator>
    </item>
  </channel>
</rss>
