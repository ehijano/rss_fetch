<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Jul 2024 04:00:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Whose Knowledge is Valued?: Epistemic Injustice in CSCW Applications</title>
      <link>https://arxiv.org/abs/2407.03477</link>
      <description>arXiv:2407.03477v1 Announce Type: new 
Abstract: Social computing scholars have long known that people do not interact with knowledge in straightforward ways, especially in digital environments. While policies around knowledge are essential for targeting misinformation, they are value-laden; in choosing how to present information, we undermine non-traditional -- often non-Western -- ways of knowing. Epistemic injustice is the systemic exclusion of certain people and methods from the knowledge canon. Epistemic injustice chips away at one's testimony and vocabulary until they are stripped of their due right to know and understand. In this paper, we articulate how epistemic injustice in sociotechnical applications leads to material harm. Inspired by a hybrid collaborative autoethnography of 14 CSCW practitioners, we present three cases of epistemic injustice in sociotechnical applications: online transgender healthcare, identity sensemaking on r/bisexual, and Indigenous ways of knowing on r/AskHistorians. We further explore signature tensions across our autoethnographic materials and relate them to previous CSCW research areas and personal non-technological experiences. We argue that epistemic injustice can serve as a unifying and intersectional lens for CSCW research by surfacing dimensions of epistemic community and power. Finally, we present a call to action of three changes the CSCW community should make to move toward its own goals of research justice. We call for CSCW researchers to center individual experiences, bolster communities, and remediate issues of epistemic power as a means towards epistemic justice. In sum, we recount, synthesize, and propose solutions for the various forms of epistemic injustice that CSCW sites of study -- including CSCW itself -- propagate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03477v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Leah Hope Ajmani, Jasmine C Foriest, Jordan Taylor, Kyle Pittman, Sarah Gilbert, Michael Ann Devito</dc:creator>
    </item>
    <item>
      <title>Human-Centered Design Recommendations for LLM-as-a-Judge</title>
      <link>https://arxiv.org/abs/2407.03479</link>
      <description>arXiv:2407.03479v1 Announce Type: new 
Abstract: Traditional reference-based metrics, such as BLEU and ROUGE, are less effective for assessing outputs from Large Language Models (LLMs) that produce highly creative or superior-quality text, or in situations where reference outputs are unavailable. While human evaluation remains an option, it is costly and difficult to scale. Recent work using LLMs as evaluators (LLM-as-a-judge) is promising, but trust and reliability remain a significant concern. Integrating human input is crucial to ensure criteria used to evaluate are aligned with the human's intent, and evaluations are robust and consistent. This paper presents a user study of a design exploration called EvaluLLM, that enables users to leverage LLMs as customizable judges, promoting human involvement to balance trust and cost-saving potential with caution. Through interviews with eight domain experts, we identified the need for assistance in developing effective evaluation criteria aligning the LLM-as-a-judge with practitioners' preferences and expectations. We offer findings and design recommendations to optimize human-assisted LLM-as-judge systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03479v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Pan, Zahra Ashktorab, Michael Desmond, Martin Santillan Cooper, James Johnson, Rahul Nair, Elizabeth Daly, Werner Geyer</dc:creator>
    </item>
    <item>
      <title>On-Device Training Empowered Transfer Learning For Human Activity Recognition</title>
      <link>https://arxiv.org/abs/2407.03644</link>
      <description>arXiv:2407.03644v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) is an attractive topic to perceive human behavior and supplying assistive services. Besides the classical inertial unit and vision-based HAR methods, new sensing technologies, such as ultrasound and body-area electric fields, have emerged in HAR to enhance user experience and accommodate new application scenarios. As those sensors are often paired with AI for HAR, they frequently encounter challenges due to limited training data compared to the more widely IMU or vision-based HAR solutions. Additionally, user-induced concept drift (UICD) is common in such HAR scenarios. UICD is characterized by deviations in the sample distribution of new users from that of the training participants, leading to deteriorated recognition performance. This paper proposes an on-device transfer learning (ODTL) scheme tailored for energy- and resource-constrained IoT edge devices. Optimized on-device training engines are developed for two representative MCU-level edge computing platforms: STM32F756ZG and GAP9. Based on this, we evaluated the ODTL benefits in three HAR scenarios: body capacitance-based gym activity recognition, QVAR- and ultrasonic-based hand gesture recognition. We demonstrated an improvement of 3.73%, 17.38%, and 3.70% in the activity recognition accuracy, respectively. Besides this, we observed that the RISC-V-based GAP9 achieves 20x and 280x less latency and power consumption than STM32F7 MCU during the ODTL deployment, demonstrating the advantages of employing the latest low-power parallel computing devices for edge tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03644v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pixi Kang, Julian Moosmann, Sizhen Bian, Michele Magno</dc:creator>
    </item>
    <item>
      <title>Navigating Ethics and Power Dynamics through Participant-Designer Journey Mapping</title>
      <link>https://arxiv.org/abs/2407.03735</link>
      <description>arXiv:2407.03735v1 Announce Type: new 
Abstract: As Digital Transformation and innovation driven by Information and Communication Technology (ICT) continue to mark the evolution of society, ethics emerges as a central concern not only in terms of the outcomes and implications of technological systems but also throughout the activities carried out through the development of those systems. Power dynamics have been identified as a recurrent ethical challenge in the design and development process. As designers, participants, and project stakeholders engage in the process, potential conflicts, power imbalances, and ethical challenges emerge. This requires that awareness is raised on these imbalances and that teams proactively act on them. To address this issue, we propose the Participant-Designer Journey Map (PDJM), a tool to assist designers in conducting an ethical design process, aware of power imbalances. The proposal for the PDJM was evaluated, based on a set of criteria derived from the literature, by three design professionals, against nine other alternative tools. The PDJM was identified as the tool with the most potential to facilitate a structured approach to navigating ethical dilemmas, particularly those related to power dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03735v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonor Tejo, Paula Alexandra Silva</dc:creator>
    </item>
    <item>
      <title>Designing Value-Centered Consent Interfaces: A Mixed-Methods Approach to Support Patient Values in Data-Sharing Decisions</title>
      <link>https://arxiv.org/abs/2407.03808</link>
      <description>arXiv:2407.03808v1 Announce Type: new 
Abstract: In the digital health domain, ethical data collection practices are crucial for ensuring the availability of quality datasets that drive medical advancement. Data donation, allowing patients to share their clinical data for secondary research use, presents a promising resource for such datasets. Yet, current consent interfaces mediating data-sharing decisions are found to favor data-collectors' values by leveraging cognitive biases in data-subjects towards higher data-sharing rates. This raises ethical concerns about the use of data collected, as well as concerning the quality of the resulting datasets. Seeking to establish patient-centered data collection practices in digital health, we investigate the design of consent interfaces that support end-users in making value-congruent health data-sharing decisions. Focusing our research efforts on the situated context of health data donation at the psychosomatic unit of a German university hospital, we demonstrate how a human-centered design can ground technology within the perspective of a vulnerable group. We employed an exploratory sequential mixed-method approach consisting of five phases: Participatory workshops explore patient values, informing the design of a proposed Value-Centered Consent Interface. An online experiment demonstrates our interface element's effect, increasing value-congruence in data-sharing decisions. Our proposed consent interface design is then adapted to the research context through a co-creation workshop with subject experts and a user evaluation with patients. Our work contributes to recent discourse in CSCW concerning ethical implications of new data practices within their socio-technological context by exploring patient values on medical data sharing, introducing a novel consent interface to support value-congruent decision-making, and providing a situated evaluation of the proposed interface with patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03808v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Leimst\"adtner, Peter S\"orries, Claudia-M\"uller Birn</dc:creator>
    </item>
    <item>
      <title>Reliability Criteria for News Websites</title>
      <link>https://arxiv.org/abs/2407.03865</link>
      <description>arXiv:2407.03865v1 Announce Type: new 
Abstract: Misinformation poses a threat to democracy and to people's health. Reliability criteria for news websites can help people identify misinformation. But despite their importance, there has been no empirically substantiated list of criteria for distinguishing reliable from unreliable news websites. We identify reliability criteria, describe how they are applied in practice, and compare them to prior work. Based on our analysis, we distinguish between manipulable and less manipulable criteria and compare politically diverse laypeople as end users and journalists as expert users. We discuss 11 widely recognized criteria, including the following 6 criteria that are difficult to manipulate: content, political alignment, authors, professional standards, what sources are used, and a website's reputation. Finally, we describe how technology may be able to support people in applying these criteria in practice to assess the reliability of websites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03865v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3635147</arxiv:DOI>
      <arxiv:journal_reference>ACM Trans. Comput.-Hum. Interact. 31, 2, Article 21 (April 2024), 33 pages</arxiv:journal_reference>
      <dc:creator>Hendrik Heuer, Elena Leah Glassman</dc:creator>
    </item>
    <item>
      <title>Behavioural gap assessment of human-vehicle interaction in real and virtual reality-based scenarios in autonomous driving</title>
      <link>https://arxiv.org/abs/2407.04070</link>
      <description>arXiv:2407.04070v1 Announce Type: new 
Abstract: In the field of autonomous driving research, the use of immersive virtual reality (VR) techniques is widespread to enable a variety of studies under safe and controlled conditions. However, this methodology is only valid and consistent if the conduct of participants in the simulated setting mirrors their actions in an actual environment. In this paper, we present a first and innovative approach to evaluating what we term the behavioural gap, a concept that captures the disparity in a participant's conduct when engaging in a VR experiment compared to an equivalent real-world situation. To this end, we developed a digital twin of a pre-existed crosswalk and carried out a field experiment (N=18) to investigate pedestrian-autonomous vehicle interaction in both real and simulated driving conditions. In the experiment, the pedestrian attempts to cross the road in the presence of different driving styles and an external Human-Machine Interface (eHMI). By combining survey-based and behavioural analysis methodologies, we develop a quantitative approach to empirically assess the behavioural gap, as a mechanism to validate data obtained from real subjects interacting in a simulated VR-based environment. Results show that participants are more cautious and curious in VR, affecting their speed and decisions, and that VR interfaces significantly influence their actions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04070v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergio. Mart\'in Serrano, Rub\'en Izquierdo, Iv\'an Garc\'ia Daza, Miguel \'Angel Sotelo, D. Fern\'andez Llorca</dc:creator>
    </item>
    <item>
      <title>UpStory: the Uppsala Storytelling dataset</title>
      <link>https://arxiv.org/abs/2407.04352</link>
      <description>arXiv:2407.04352v1 Announce Type: new 
Abstract: Friendship and rapport play an important role in the formation of constructive social interactions, and have been widely studied in educational settings due to their impact on student outcomes. Given the growing interest in automating the analysis of such phenomena through Machine Learning (ML), access to annotated interaction datasets is highly valuable. However, no dataset on dyadic child-child interactions explicitly capturing rapport currently exists. Moreover, despite advances in the automatic analysis of human behaviour, no previous work has addressed the prediction of rapport in child-child dyadic interactions in educational settings. We present UpStory -- the Uppsala Storytelling dataset: a novel dataset of naturalistic dyadic interactions between primary school aged children, with an experimental manipulation of rapport. Pairs of children aged 8-10 participate in a task-oriented activity: designing a story together, while being allowed free movement within the play area. We promote balanced collection of different levels of rapport by using a within-subjects design: self-reported friendships are used to pair each child twice, either minimizing or maximizing pair separation in the friendship network. The dataset contains data for 35 pairs, totalling 3h 40m of audio and video recordings. It includes two video sources covering the play area, as well as separate voice recordings for each child. An anonymized version of the dataset is made publicly available, containing per-frame head pose, body pose, and face features; as well as per-pair information, including the level of rapport. Finally, we provide ML baselines for the prediction of rapport.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04352v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Fraile, Natalia Calvo-Barajas, Anastasia Sophia Apeiron, Giovanna Varni, Joakim Lindblad, Nata\v{s}a Sladoje, Ginevra Castellano</dc:creator>
    </item>
    <item>
      <title>Enabling On-Device LLMs Personalization with Smartphone Sensing</title>
      <link>https://arxiv.org/abs/2407.04418</link>
      <description>arXiv:2407.04418v1 Announce Type: new 
Abstract: This demo presents a novel end-to-end framework that combines on-device large language models (LLMs) with smartphone sensing technologies to achieve context-aware and personalized services. The framework addresses critical limitations of current personalization solutions via cloud-based LLMs, such as privacy concerns, latency and cost, and limited personal sensor data. To achieve this, we innovatively proposed deploying LLMs on smartphones with multimodal sensor data and customized prompt engineering, ensuring privacy and enhancing personalization performance through context-aware sensing. A case study involving a university student demonstrated the proposed framework's capability to provide tailored recommendations. In addition, we show that the proposed framework achieves the best trade-off in privacy, performance, latency, cost, battery and energy consumption between on-device and cloud LLMs. Future work aims to integrate more diverse sensor data and conduct large-scale user studies to further refine the personalization. We envision the proposed framework could significantly improve user experiences in various domains such as healthcare, productivity, and entertainment by providing secure, context-aware, and efficient interactions directly on users' devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04418v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiquan Zhang, Ying Ma, Le Fang, Hong Jia, Simon D'Alfonso, Vassilis Kostakos</dc:creator>
    </item>
    <item>
      <title>Gamification of Motor Imagery Brain-Computer Interface Training Protocols: a systematic review</title>
      <link>https://arxiv.org/abs/2407.04610</link>
      <description>arXiv:2407.04610v1 Announce Type: new 
Abstract: Current Motor Imagery Brain-Computer Interfaces (MI-BCI) require a lengthy and monotonous training procedure to train both the system and the user. Considering many users struggle with effective control of MI-BCI systems, a more user-centered approach to training might help motivate users and facilitate learning, alleviating inefficiency of the BCI system. With the increase of BCI-controlled games, researchers have suggested using game principles for BCI training, as games are naturally centered on the player. This review identifies and evaluates the application of game design elements to MI-BCI training, a process known as gamification. Through a systematic literature search, we examined how MI-BCI training protocols have been gamified and how specific game elements impacted the training outcomes. We identified 86 studies that employed gamified MI-BCI protocols in the past decade. The prevalence and reported effects of individual game elements on user experience and performance were extracted and synthesized. Results reveal that MI-BCI training protocols are most often gamified by having users move an avatar in a virtual environment that provides visual feedback. Furthermore, in these virtual environments, users were provided with goals that guided their actions. Using gamification, the reviewed protocols allowed users to reach effective MI-BCI control, with studies reporting positive effects of four individual elements on user performance and experience, namely: feedback, avatars, assistance, and social interaction. Based on these elements, this review makes current and future recommendations for effective gamification, such as the use of virtual reality and adaptation of game difficulty to user skill level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04610v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fred Atilla, Marie Postma, Maryam Alimardani</dc:creator>
    </item>
    <item>
      <title>A Multi-Modal Explainability Approach for Human-Aware Robots in Multi-Party Conversation</title>
      <link>https://arxiv.org/abs/2407.03340</link>
      <description>arXiv:2407.03340v1 Announce Type: cross 
Abstract: The addressee estimation (understanding to whom somebody is talking) is a fundamental task for human activity recognition in multi-party conversation scenarios. Specifically, in the field of human-robot interaction, it becomes even more crucial to enable social robots to participate in such interactive contexts. However, it is usually implemented as a binary classification task, restricting the robot's capability to estimate whether it was addressed and limiting its interactive skills. For a social robot to gain the trust of humans, it is also important to manifest a certain level of transparency and explainability. Explainable artificial intelligence thus plays a significant role in the current machine learning applications and models, to provide explanations for their decisions besides excellent performance. In our work, we a) present an addressee estimation model with improved performance in comparison with the previous SOTA; b) further modify this model to include inherently explainable attention-based segments; c) implement the explainable addressee estimation as part of a modular cognitive architecture for multi-party conversation in an iCub robot; d) propose several ways to incorporate explainability and transparency in the aforementioned architecture; and e) perform a pilot user study to analyze the effect of various explanations on how human participants perceive the robot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03340v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iveta Be\v{c}kov\'a, \v{S}tefan P\'oco\v{s}, Giulia Belgiovine, Marco Matarese, Alessandra Sciutti, Carlo Mazzola</dc:creator>
    </item>
    <item>
      <title>The Role of Privacy Guarantees in Voluntary Donation of Private Data for Altruistic Goals</title>
      <link>https://arxiv.org/abs/2407.03451</link>
      <description>arXiv:2407.03451v1 Announce Type: cross 
Abstract: Voluntary donation of private information for altruistic purposes, such as advancing research, is common. However, concerns about data misuse and leakage may deter individuals from donating their information. While prior research has indicated that Privacy Enhancement Technologies (PETs) can alleviate these concerns, the extent to which these techniques influence willingness to donate data remains unclear.
  This study conducts a vignette survey (N=485) to examine people's willingness to donate medical data for developing new treatments under four privacy guarantees: data expiration, anonymization, use restriction, and access control. The study explores two mechanisms for verifying these guarantees: self-auditing and expert auditing, and evaluates the impact on two types of data recipient entities: for-profit and non-profit institutions.
  Our findings reveal that the type of entity collecting data strongly influences respondents' privacy expectations, which in part influence their willingness to donate data. Respondents have such high expectations of the privacy provided by non-profit entities that explicitly stating the privacy protections provided makes little adjustment to those expectations. In contrast, statements about privacy bring respondents' expectations of the privacy provided by for-profit entities nearly in-line with non-profit expectations. We highlight the risks of these respective results as well as the need for future research to better align technical community and end-user perceptions about the effectiveness of auditing PETs and to effectively set expectations about the efficacy of PETs in the face of end-user concerns about data breaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03451v1</guid>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ruizhe Wang, Roberta De Viti, Aarushi Dubey, Elissa M. Redmiles</dc:creator>
    </item>
    <item>
      <title>On Evaluating Explanation Utility for Human-AI Decision Making in NLP</title>
      <link>https://arxiv.org/abs/2407.03545</link>
      <description>arXiv:2407.03545v1 Announce Type: cross 
Abstract: Is explainability a false promise? This debate has emerged from the insufficient evidence that explanations aid people in situations they are introduced for. More human-centered, application-grounded evaluations of explanations are needed to settle this. Yet, with no established guidelines for such studies in NLP, researchers accustomed to standardized proxy evaluations must discover appropriate measurements, tasks, datasets, and sensible models for human-AI teams in their studies.
  To help with this, we first review fitting existing metrics. We then establish requirements for datasets to be suitable for application-grounded evaluations. Among over 50 datasets available for explainability research in NLP, we find that 4 meet our criteria. By finetuning Flan-T5-3B, we demonstrate the importance of reassessing the state of the art to form and study human-AI teams. Finally, we present the exemplar studies of human-AI decision-making for one of the identified suitable tasks -- verifying the correctness of a legal claim given a contract.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03545v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fateme Hashemi Chaleshtori, Atreya Ghosal, Alexander Gill, Purbid Bambroo, Ana Marasovi\'c</dc:creator>
    </item>
    <item>
      <title>Assessing Consensus of Developers' Views on Code Readability</title>
      <link>https://arxiv.org/abs/2407.03790</link>
      <description>arXiv:2407.03790v1 Announce Type: cross 
Abstract: The rapid rise of Large Language Models (LLMs) has changed software development, with tools like Copilot, JetBrains AI Assistant, and others boosting developers' productivity. However, developers now spend more time reviewing code than writing it, highlighting the importance of Code Readability for code comprehension. Our previous research found that existing Code Readability models were inaccurate in representing developers' notions and revealed a low consensus among developers, highlighting a need for further investigations in this field.
  Building on this, we surveyed 10 Java developers with similar coding experience to evaluate their consensus on Code Readability assessments and related aspects. We found significant agreement among developers on Code Readability evaluations and identified specific code aspects strongly correlated with Code Readability. Overall, our study sheds light on Code Readability within LLM contexts, offering insights into how these models can align with developers' perceptions of Code Readability, enhancing software development in the AI era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03790v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Agnia Sergeyuk, Olga Lvova, Sergey Titov, Anastasiia Serova, Farid Bagirov, Timofey Bryksin</dc:creator>
    </item>
    <item>
      <title>Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality Norms</title>
      <link>https://arxiv.org/abs/2407.04183</link>
      <description>arXiv:2407.04183v1 Announce Type: cross 
Abstract: Large language models (LLMs) are trained on broad corpora and then used in communities with specialized norms. Is providing LLMs with community rules enough for models to follow these norms? We evaluate LLMs' capacity to detect (Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's Neutral Point of View (NPOV) policy. LLMs struggled with bias detection, achieving only 64% accuracy on a balanced dataset. Models exhibited contrasting biases (some under- and others over-predicted bias), suggesting distinct priors about neutrality. LLMs performed better at generation, removing 79% of words removed by Wikipedia editors. However, LLMs made additional changes beyond Wikipedia editors' simpler neutralizations, resulting in high-recall but low-precision editing. Interestingly, crowdworkers rated AI rewrites as more neutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative analysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia editors but often made extraneous non-NPOV-related changes (such as grammar). LLMs may apply rules in ways that resonate with the public but diverge from community experts. While potentially effective for generation, LLMs may reduce editor agency and increase moderation workload (e.g., verifying additions). Even when rules are easy to articulate, having LLMs apply them like community members may still be difficult.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04183v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Ashkinaze, Ruijia Guan, Laura Kurek, Eytan Adar, Ceren Budak, Eric Gilbert</dc:creator>
    </item>
    <item>
      <title>Understanding the Landscape of Leveraging IoT for Sustainable Growth in Saudi Arabia</title>
      <link>https://arxiv.org/abs/2407.04273</link>
      <description>arXiv:2407.04273v1 Announce Type: cross 
Abstract: The integration of Internet of Things (IoT) technologies in agriculture holds promise for transforming farming practices, particularly in the Kingdom of Saudi Arabia (KSA). This study explores the adoption of smart farming practices among KSA farmers. Due to the geographical location and nature of KSA, it faces significant challenges in agriculture. The objective of this research is to discuss how IoT will enhance agriculture in KSA and identify its current usage by conducting a study on Saudi farmers with varying ages, regions, and years of experience. The results indicate that 90% of the farmers encounter challenges in farming, and all of them express interest in adopting smart farming to address these issues. While 60% of farmers are currently utilizing IoT technologies, they encounter challenges in implementing smart farming practices. Thus, smart farming presents solutions to prevalent challenges including adverse weather, water scarcity, and labor shortages, though barriers include cost and educational challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04273v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Manal Alshehri, Ohoud Alharbi</dc:creator>
    </item>
    <item>
      <title>Towards Context-aware Support for Color Vision Deficiency: An Approach Integrating LLM and AR</title>
      <link>https://arxiv.org/abs/2407.04362</link>
      <description>arXiv:2407.04362v1 Announce Type: cross 
Abstract: People with color vision deficiency often face challenges in distinguishing colors such as red and green, which can complicate daily tasks and require the use of assistive tools or environmental adjustments. Current support tools mainly focus on presentation-based aids, like the color vision modes found in iPhone accessibility settings. However, offering context-aware support, like indicating the doneness of meat, remains a challenge since task-specific solutions are not cost-effective for all possible scenarios. To address this, our paper proposes an application that provides contextual and autonomous assistance. This application is mainly composed of: (i) an augmented reality interface that efficiently captures context; and (ii) a multi-modal large language model-based reasoner that serves to cognitize the context and then reason about the appropriate support contents. Preliminary user experiments with two color vision deficient users across five different scenarios have demonstrated the effectiveness and universality of our application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04362v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shogo Morita, Yan Zhang, Takuto Yamauchi, Sinan Chen, Jialong Li, Kenji Tei</dc:creator>
    </item>
    <item>
      <title>A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic Materials</title>
      <link>https://arxiv.org/abs/2407.04379</link>
      <description>arXiv:2407.04379v1 Announce Type: cross 
Abstract: This paper presents a mapping strategy for interacting with the latent spaces of generative AI models. Our approach involves using unsupervised feature learning to encode a human control space and mapping it to an audio synthesis model's latent space. To demonstrate how this mapping strategy can turn high-dimensional sensor data into control mechanisms of a deep generative model, we present a proof-of-concept system that uses visual sketches to control an audio synthesis model. We draw on emerging discourses in XAIxArts to discuss how this approach can contribute to XAI in artistic and creative contexts, we also discuss its current limitations and propose future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04379v1</guid>
      <category>cs.SD</category>
      <category>cs.HC</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuoyang Zheng, Anna Xamb\'o Sed\'o, Nick Bryan-Kinns</dc:creator>
    </item>
    <item>
      <title>Prompting AI Art: An Investigation into the Creative Skill of Prompt Engineering</title>
      <link>https://arxiv.org/abs/2303.13534</link>
      <description>arXiv:2303.13534v3 Announce Type: replace 
Abstract: We are witnessing a novel era of creativity where anyone can create digital content via prompt-based learning (known as prompt engineering). This paper investigates prompt engineering as a novel creative skill for creating AI art with text-to-image generation. In three consecutive studies, we explore whether crowdsourced participants can 1) discern prompt quality, 2) write prompts, and 3) refine prompts. We find that participants could evaluate prompt quality and crafted descriptive prompts, but they lacked style-specific vocabulary necessary for effective prompting. This is in line with our hypothesis that prompt engineering is a new type of skill that is non-intuitive and must first be acquired (e.g., through means of practice and learning) before it can be used. Our studies deepen our understanding of prompt engineering and chart future research directions. We conclude by envisioning four potential futures for prompt engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.13534v3</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Oppenlaender, Rhema Linder, Johanna Silvennoinen</dc:creator>
    </item>
    <item>
      <title>Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Mining Insights at Scale</title>
      <link>https://arxiv.org/abs/2306.05036</link>
      <description>arXiv:2306.05036v4 Announce Type: replace 
Abstract: Large language models (LLMs), such as ChatGPT and GPT-4, are gaining wide-spread real world use. Yet, these LLMs are closed source, and little is known about their performance in real-world use cases. In this paper, we apply and evaluate the combination of ChatGPT and GPT-4 for the real-world task of mining insights from a text corpus in order to identify research challenges in the field of HCI. We extract 4,392 research challenges in over 100 topics from the 2023~CHI conference proceedings and visualize the research challenges for interactive exploration. We critically evaluate the LLMs on this practical task and conclude that the combination of ChatGPT and GPT-4 makes an excellent cost-efficient means for analyzing a text corpus at scale. Cost-efficiency is key for flexibly prototyping research ideas and analyzing text corpora from different perspectives, with implications for applying LLMs for mining insights in academia and practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.05036v4</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Oppenlaender, Joonas H\"am\"al\"ainen</dc:creator>
    </item>
    <item>
      <title>Using Large Language Models to Assist Video Content Analysis: An Exploratory Study of Short Videos on Depression</title>
      <link>https://arxiv.org/abs/2406.19528</link>
      <description>arXiv:2406.19528v2 Announce Type: replace 
Abstract: Despite the growing interest in leveraging Large Language Models (LLMs) for content analysis, current studies have primarily focused on text-based content. In the present work, we explored the potential of LLMs in assisting video content analysis by conducting a case study that followed a new workflow of LLM-assisted multimodal content analysis. The workflow encompasses codebook design, prompt engineering, LLM processing, and human evaluation. We strategically crafted annotation prompts to get LLM Annotations in structured form and explanation prompts to generate LLM Explanations for a better understanding of LLM reasoning and transparency. To test LLM's video annotation capabilities, we analyzed 203 keyframes extracted from 25 YouTube short videos about depression. We compared the LLM Annotations with those of two human coders and found that LLM has higher accuracy in object and activity Annotations than emotion and genre Annotations. Moreover, we identified the potential and limitations of LLM's capabilities in annotating videos. Based on the findings, we explore opportunities and challenges for future research and improvements to the workflow. We also discuss ethical concerns surrounding future studies based on LLM-assisted video analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19528v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaying Liu, Yunlong Wang, Yao Lyu, Yiheng Su, Shuo Niu, Xuhai Orson Xu, Yan Zhang</dc:creator>
    </item>
    <item>
      <title>Understanding Modality Preferences in Search Clarification</title>
      <link>https://arxiv.org/abs/2406.19546</link>
      <description>arXiv:2406.19546v2 Announce Type: replace 
Abstract: This study is the first attempt to explore the impact of clarification question modality on user preference in search engines. We introduce the multi-modal search clarification dataset, MIMICS-MM, containing clarification questions with associated expert-collected and model-generated images. We analyse user preferences over different clarification modes of text, image, and combination of both through crowdsourcing by taking into account image and text quality, clarity, and relevance. Our findings demonstrate that users generally prefer multi-modal clarification over uni-modal approaches. We explore the use of automated image generation techniques and compare the quality, relevance, and user preference of model-generated images with human-collected ones. The study reveals that text-to-image generation models, such as Stable Diffusion, can effectively generate multi-modal clarification questions. By investigating multi-modal clarification, this research establishes a foundation for future advancements in search systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19546v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Leila Tavakoli, Giovanni Castiglia, Federica Calo, Yashar Deldjoo, Hamed Zamani, Johanne R. Trippas</dc:creator>
    </item>
    <item>
      <title>How AI Ideas Affect the Creativity, Diversity, and Evolution of Human Ideas: Evidence From a Large, Dynamic Experiment</title>
      <link>https://arxiv.org/abs/2401.13481</link>
      <description>arXiv:2401.13481v2 Announce Type: replace-cross 
Abstract: Exposure to large language model output is rapidly increasing. How will seeing AI-generated ideas affect human ideas? We conducted an experiment (800+ participants, 40+ countries) where participants viewed creative ideas that were from ChatGPT or prior experimental participants and then brainstormed their own idea. We varied the number of AI-generated examples (none, low, or high exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic experiment design -- ideas from prior participants in an experimental condition are used as stimuli for future participants in the same experimental condition -- speaks to the interdependent process of cultural creation: creative ideas are built upon prior ideas. Hence, we capture the compounding effects of having LLMs 'in the culture loop'. We find that high AI exposure (but not low AI exposure) did not affect the creativity of individual ideas but did increase the average amount and rate of change of collective idea diversity. AI made ideas different, not better. There were no main effects of disclosure. We also found that self-reported creative people were less influenced by knowing an idea was from AI and that participants may knowingly adopt AI ideas when the task is difficult. Our findings suggest that introducing AI ideas may increase collective diversity but not individual creativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13481v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Ashkinaze, Julia Mendelsohn, Li Qiwei, Ceren Budak, Eric Gilbert</dc:creator>
    </item>
    <item>
      <title>ESI-GAL: EEG Source Imaging-based Kinematics Parameter Estimation for Grasp and Lift Task</title>
      <link>https://arxiv.org/abs/2406.11500</link>
      <description>arXiv:2406.11500v3 Announce Type: replace-cross 
Abstract: Electroencephalogram (EEG) signals-based motor kinematics prediction (MKP) has been an active area of research to develop brain-computer interface (BCI) systems such as exosuits, prostheses, and rehabilitation devices. However, EEG source imaging (ESI) based kinematics prediction is sparsely explored in the literature. In this study, pre-movement EEG features are utilized to predict three-dimensional (3D) hand kinematics for the grasp-and-lift motor task. A public dataset, WAY-EEG-GAL, is utilized for MKP analysis. In particular, sensor-domain (EEG data) and source-domain (ESI data) based features from the frontoparietal region are explored for MKP. Deep learning-based models are explored to achieve efficient kinematics decoding. Various time-lagged and window sizes are analyzed for hand kinematics prediction. Subsequently, intra-subject and inter-subject MKP analysis is performed to investigate the subject-specific and subject-independent motor-learning capabilities of the neural decoders. The Pearson correlation coefficient (PCC) is used as the performance metric for kinematics trajectory decoding. The rEEGNet neural decoder achieved the best performance with sensor-domain and source-domain features with a time lag and window size of 100 ms and 450 ms, respectively. The highest mean PCC values of 0.790, 0.795, and 0.637 are achieved using sensor-domain features, while 0.769, 0.777, and 0.647 are achieved using source-domain features in x, y, and z-directions, respectively. This study explores the feasibility of trajectory prediction using EEG sensor-domain and source-domain EEG features for the grasp-and-lift task. Furthermore, inter-subject trajectory estimation is performed using the proposed deep learning decoder with EEG source domain features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11500v3</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anant Jain, Lalan Kumar</dc:creator>
    </item>
    <item>
      <title>MetaDesigner: Advancing Artistic Typography through AI-Driven, User-Centric, and Multilingual WordArt Synthesis</title>
      <link>https://arxiv.org/abs/2406.19859</link>
      <description>arXiv:2406.19859v2 Announce Type: replace-cross 
Abstract: MetaDesigner revolutionizes artistic typography synthesis by leveraging the strengths of Large Language Models (LLMs) to drive a design paradigm centered around user engagement. At the core of this framework lies a multi-agent system comprising the Pipeline, Glyph, and Texture agents, which collectively enable the creation of customized WordArt, ranging from semantic enhancements to the imposition of complex textures. MetaDesigner incorporates a comprehensive feedback mechanism that harnesses insights from multimodal models and user evaluations to refine and enhance the design process iteratively. Through this feedback loop, the system adeptly tunes hyperparameters to align with user-defined stylistic and thematic preferences, generating WordArt that not only meets but exceeds user expectations of visual appeal and contextual relevance. Empirical validations highlight MetaDesigner's capability to effectively serve diverse WordArt applications, consistently producing aesthetically appealing and context-sensitive results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19859v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Qi He, Wangmeng Xiang, Hanyuan Chen, Jin-Peng Lan, Xianhui Lin, Kang Zhu, Bin Luo, Yifeng Geng, Xuansong Xie, Alexander G. Hauptmann</dc:creator>
    </item>
  </channel>
</rss>
