<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Aug 2025 04:00:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Efficient Computation of Dominant Eigenvalues Using Adaptive Block Lanczos with Chebyshev Filtering</title>
      <link>https://arxiv.org/abs/2508.08495</link>
      <description>arXiv:2508.08495v1 Announce Type: new 
Abstract: We present an efficient method for computing dominant eigenvalues of large, nonsymmetric, diagonalizable matrices based on an adaptive block Lanczos algorithm combined with Chebyshev polynomial filtering. The proposed approach improves numerical stability through two key components: (i) the Adaptive Block Lanczos (ABLE) method, which maintains biorthogonality using SVD based stabilization, and (ii) Chebyshev filtering, which enhances spectral separation via iterative polynomial filtering. Numerical experiments on dense and sparse test problems confirm the effectiveness of the ABLE Chebyshev algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08495v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. El Guide, K. Jbilou, K. Lachhab</dc:creator>
    </item>
    <item>
      <title>Fast adaptive tubal rank-revealing algorithm for t-product based tensor approximation</title>
      <link>https://arxiv.org/abs/2508.08557</link>
      <description>arXiv:2508.08557v1 Announce Type: new 
Abstract: Color images and video sequences can be modeled as three-way tensors, which admit low tubal-rank approximations via convex surrogate minimization. This optimization problem is efficiently addressed by tensor singular value thresholding (t-SVT). To mitigate the computational burden of tensor singular value decomposition (t-SVD) in each iteration, this paper introduces an adaptive randomized algorithm for tubal rank revelation in data tensors \(\mathcal{A}\). Our method selectively captures the principal information from frontal slices in the Fourier domain using a predefined threshold, obviating the need for priori tubal-rank and Fourier-domain singular values estimations while providing an explicit tensor approximation. Leveraging optimality results from matrix randomized SVD, we establish theoretical guarantees demonstrating that the proposed algorithm computes low tubal-rank approximations within constants dependent on data dimensions and the Fourier-domain singular value gap. Empirical evaluations validate its efficacy in image processing and background modeling tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08557v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiaohua Liu, Jiehui Gu</dc:creator>
    </item>
    <item>
      <title>Efficient Function Approximation Under Heteroskedastic Noise</title>
      <link>https://arxiv.org/abs/2508.08683</link>
      <description>arXiv:2508.08683v1 Announce Type: new 
Abstract: Approximating a function $f(x)$ on $[-1,1]$ based on $N+1$ samples is a classical problem in numerical analysis. If the samples come with heteroskedastic noise depending on $x$ of variance $\sigma(x)^2$, an $O(N\log N)$ algorithm for this problem has not yet been found in the current literature. In this paper, we propose a method called HeteroChebtrunc, adapted from an algorithm named NoisyChebtrunc. Using techniques in high-dimensional probability, we show that with high probability, HeteroChebtrunc achieves a tighter infinity-norm error bound than NoisyChebtrunc under heteroskedastic noise. This algorithm runs in $O(N+\hat{N}\log \hat{N})$ operations, where $\hat{N}\ll N$ is a chosen parameter. While investigating the properties of HeteroChebtrunc, we also derive a high-probability non-asymptotic relative error bound on the sample variance estimator for subgaussian variables, which is potentially another result of broader interest. We provide numerical experiments to demonstrate the improved uniform error of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08683v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuji Nakatsukasa, Yifu Zhang</dc:creator>
    </item>
    <item>
      <title>Solving Approximation Tasks with Greedy Deep Kernel Methods</title>
      <link>https://arxiv.org/abs/2508.08759</link>
      <description>arXiv:2508.08759v1 Announce Type: new 
Abstract: Kernel methods are versatile tools for function approximation and surrogate modeling. In particular, greedy techniques offer computational efficiency and reliability through inherent sparsity and provable convergence. Inspired by the success of deep neural networks and structured deep kernel networks, we consider deep, multilayer kernels for greedy approximation. This multilayer structure, consisting of linear kernel layers and optimizable kernel activation function layers in an alternating fashion, increases the expressiveness of the kernels and thus of the resulting approximants. Compared to standard kernels, deep kernels are able to adapt kernel intrinsic shape parameters automatically, incorporate transformations of the input space and induce a data-dependent reproducing kernel Hilbert space. For this, deep kernels need to be pretrained using a specifically tailored optimization objective. In this work, we not only introduce deep kernel greedy models, but also present numerical investigations and comparisons with neural networks, which clearly show the advantages in terms of approximation accuracies. As applications we consider the approximation of model problems, the prediction of breakthrough curves for reactive flow through porous media and the approximation of solutions for parametrized ordinary differential equation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08759v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marian Klink, Tobias Ehring, Robin Herkert, Robin Lautenschlager, Dominik G\"oddeke, Bernard Haasdonk</dc:creator>
    </item>
    <item>
      <title>A Parareal Algorithm with Spectral Coarse Solver</title>
      <link>https://arxiv.org/abs/2508.08873</link>
      <description>arXiv:2508.08873v1 Announce Type: new 
Abstract: We consider a new class of Parareal algorithms, which use ideas from localized reduced basis methods to construct the coarse solver from spectral approximations of the transfer operators mapping initial values for a given time interval to the solution at the end of the interval. By leveraging randomized singular value decompositions, these spectral approximations are obtained embarrassingly parallel by computing local fine solutions for random initial values. We show a priori and a posteriori error bounds in terms of the computed singular values of the transfer operators. Our numerical experiments demonstrate that our approach can significantly outperform Parareal with single-step coarse solvers. At the same time, it permits to further increase parallelism in Parareal by trading global iterations for a larger number of independent local solves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08873v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin J. Gander, Mario Ohlberger, Stephan Rave</dc:creator>
    </item>
    <item>
      <title>Provably positivity-preserving, globally divergence-free central DG methods for ideal MHD system</title>
      <link>https://arxiv.org/abs/2508.08913</link>
      <description>arXiv:2508.08913v1 Announce Type: new 
Abstract: This paper proposes a numerical method, termed PosDiv-CDG, that provably preserves both positivity and the globally divergence-free (DF) condition at arbitrarily high order in multiple dimensions. It resolves the fundamental structural incompatibility between standard positivity-preserving limiters and global DF enforcement in the central discontinuous Galerkin (CDG) framework. The method integrates a novel positivity-limiting strategy, a modified dissipation mechanism guided by convex decomposition, and an auxiliary evolution equation for the magnetic field, which are designed based on rigorous theoretical analysis. Notably, we provide a rigorous proof of positivity preservation for the updated cell averages under an explicit CFL-type condition. The proof leverages the geometric quasi-linearization (GQL) technique, which reformulates the nonlinear positivity constraint into an equivalent linear form. This enables the derivation of flux-based inequalities and technical estimates under the global DF constraint. To suppress nonphysical oscillations near shocks, we develop a compact, non-intrusive convex-oscillation-suppressing (COS) procedure based on the entropy function. The COS process acts only on non-magnetic variables, avoids costly characteristic decomposition, and maintains both the globally DF property and high-order accuracy. Several challenging experiments -- including low plasma-beta MHD jets with Mach numbers up to 1,000,000 -- demonstrate the proposed method robustness, high-order accuracy, non-oscillatory behavior, and its ability to preserve both positivity and globally DF structures under extreme conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08913v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruifang Yan, Huihui Cao, Kailiang Wu</dc:creator>
    </item>
    <item>
      <title>An effective implementation of high-order compact gas-kinetic scheme on structured meshes for compressible flows</title>
      <link>https://arxiv.org/abs/2508.08965</link>
      <description>arXiv:2508.08965v1 Announce Type: new 
Abstract: A novel fifth-order compact gas-kinetic scheme is developed for high-resolution simulation of compressible flows on structured meshes. Its accuracy relies on a new multidimensional fifth-order compact reconstruction that uses line-averaged derivatives to introduce additional degrees of freedom, enabling a compact stencil with superior resolution. For non-orthogonal meshes, reconstruction is performed on a standard reference cell in a transformed computational space. This approach provides a unified polynomial form, significantly reducing memory usage and computational cost while simplifying implementation compared to direct multi-dimensional or dimension-by-dimension methods. A nonlinear adaptive method ensures high accuracy and robustness by smoothly transitioning from the high-order linear scheme in smooth regions to a second-order scheme at discontinuities. The method is implemented with multi-GPU parallelization using CUDA and MPI for large-scale applications. Comprehensive numerical tests, from subsonic to supersonic turbulence, validate the scheme's high accuracy, resolution and excellent robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08965v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaqing Yang, Fengxiang Zhao, Kun Xu</dc:creator>
    </item>
    <item>
      <title>Optimality of adaptive $H(\operatorname{div}\operatorname{div})$ mixed finite element methods for the Kirchhoff-Love plate bending problem</title>
      <link>https://arxiv.org/abs/2508.09008</link>
      <description>arXiv:2508.09008v1 Announce Type: new 
Abstract: This paper presents a reliable and efficient residual-based a posteriori error analysis for the symmetric $H(\operatorname{div}\operatorname{div})$ mixed finite element method for the Kirchhoff-Love plate bending problem with mixed boundary conditions. The key ingredient lies in the construction of boundary-condition-preserving complexes at both continuous and discrete levels. Additionally, the discrete symmetric $H(\operatorname{div}\operatorname{div})$ space is extended to ensure nestedness, which leads to optimality for the adaptive algorithm. Numerical examples confirm the effectiveness of the a posteriori error estimator and demonstrate the optimal convergence rate under adaptive refinements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09008v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun Hu, Rui Ma, Min Zhang</dc:creator>
    </item>
    <item>
      <title>Weighted Proper Orthogonal Decomposition for High-Dimensional Optimization</title>
      <link>https://arxiv.org/abs/2508.09084</link>
      <description>arXiv:2508.09084v1 Announce Type: new 
Abstract: While proper orthogonal decomposition (POD) is widely used for model reduction, its standard form does not take into account any parametric model structure. Extensions to POD have been proposed to address this, but these either require large amounts of solution data, lack online adaptivity, or have limited approximation accuracy. We circumvent these limitations by instead assigning weights to the snapshot matrix columns, and updating these whenever the model is evaluated at a new point in the parameter space. We derive an a posteriori error bound that depends on these snapshot weights, show how these weights can be chosen to tighten the error bound, and present an algorithm to compute the corresponding reduced basis efficiently. We show how this weighted POD approach can be used to naturally generalize the calculation of reduced basis derivatives to situations with multidimensional parameter spaces and snapshots at multiple locations in the parameter space. Lastly, we cover how these approaches can be implemented within an optimization algorithm, without the need for an offline training phase. The proposed weighted POD methods with and without reduced basis derivatives are applied to a gradient-based shell thickness optimization problem with 105 design parameters and a time-dependent partial differential equation. The numerical solutions obtained for this problem attain errors that are several orders of magnitude smaller when using weighted POD than those computed with regular POD and Grassmann manifold interpolation, while having comparable wall times per query and requiring fewer high-dimensional model snapshots to reach an optimal solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09084v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastiaan P. C. van Schie, Boris Kramer, John T. Hwang</dc:creator>
    </item>
    <item>
      <title>UGM2N: An Unsupervised and Generalizable Mesh Movement Network via M-Uniform Loss</title>
      <link>https://arxiv.org/abs/2508.08615</link>
      <description>arXiv:2508.08615v1 Announce Type: cross 
Abstract: Partial differential equations (PDEs) form the mathematical foundation for modeling physical systems in science and engineering, where numerical solutions demand rigorous accuracy-efficiency tradeoffs. Mesh movement techniques address this challenge by dynamically relocating mesh nodes to rapidly-varying regions, enhancing both simulation accuracy and computational efficiency. However, traditional approaches suffer from high computational complexity and geometric inflexibility, limiting their applicability, and existing supervised learning-based approaches face challenges in zero-shot generalization across diverse PDEs and mesh topologies.In this paper, we present an Unsupervised and Generalizable Mesh Movement Network (UGM2N). We first introduce unsupervised mesh adaptation through localized geometric feature learning, eliminating the dependency on pre-adapted meshes. We then develop a physics-constrained loss function, M-Uniform loss, that enforces mesh equidistribution at the nodal level.Experimental results demonstrate that the proposed network exhibits equation-agnostic generalization and geometric independence in efficient mesh adaptation. It demonstrates consistent superiority over existing methods, including robust performance across diverse PDEs and mesh geometries, scalability to multi-scale resolutions and guaranteed error reduction without mesh tangling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08615v1</guid>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhichao Wang, Xinhai Chen, Qinglin Wang, Xiang Gao, Qingyang Zhang, Menghan Jia, Xiang Zhang, Jie Liu</dc:creator>
    </item>
    <item>
      <title>SonicRadiation: A Hybrid Numerical Solution for Sound Radiation without Ghost Cells</title>
      <link>https://arxiv.org/abs/2508.08775</link>
      <description>arXiv:2508.08775v1 Announce Type: cross 
Abstract: Interactive synthesis of physical sound effects is crucial in digital media production. Sound radiation simulation, a key component of physically based sound synthesis, has posed challenges in the context of complex object boundaries. Previous methods, such as ghost cell-based finite-difference time-domain (FDTD) wave solver, have struggled to address these challenges, leading to large errors and failures in complex boundaries because of the limitation of ghost cells. We present SonicRadiation, a hybrid numerical solution capable of handling complex and dynamic object boundaries in sound radiation simulation without relying on ghost cells. We derive a consistent formulation to connect the physical quantities on grid cells in FDTD with the boundary elements in the time-domain boundary element method (TDBEM). Hereby, we propose a boundary grid synchronization strategy to seamlessly integrate TDBEM with FDTD while maintaining high numerical accuracy. Our method holds both advantages from the accuracy of TDBEM for the near-field and the efficiency of FDTD for the far-field. Experimental results demonstrate the superiority of our method in sound radiation simulation over previous approaches in terms of accuracy and efficiency, particularly in complex scenes, further validating its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08775v1</guid>
      <category>cs.SD</category>
      <category>cs.GR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xutong Jin, Guoping Wang, Sheng Li</dc:creator>
    </item>
    <item>
      <title>Puzzle Game: Prediction and Classification of Wordle Solution Words</title>
      <link>https://arxiv.org/abs/2403.19433</link>
      <description>arXiv:2403.19433v4 Announce Type: replace 
Abstract: We study the prediction and classification of Wordle solution words. After cleaning the public results log, we fit an ARIMA model to forecast the daily volume of reported outcomes through March 1, 2023. For each solution word, we compute three interpretable attributes: usage frequency (FREQ), word information entropy (WIE), and the number of repeated letters (NRE), and analyze their correlations with the empirical attempt distribution (1-6 attempts plus failure, coded as 7). We then train an XGBoost regressor to predict the full 1-7 outcome distribution for unseen words; a case study of "EERIE" illustrates the model's behavior. To categorize difficulty, we cluster words into three tiers (simple, moderate, difficult) via K-means and train a decision-tree classifier that maps FREQ, WIE, and NRE to these tiers, yielding interpretable rules. For each word, we also report the share of players requiring three or more attempts. Sensitivity analyses and full modeling details are provided in the appendix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19433v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haidong Xin, Fang Wu, Zhitong Zhou</dc:creator>
    </item>
    <item>
      <title>Robust optimal design of large-scale Bayesian nonlinear inverse problems</title>
      <link>https://arxiv.org/abs/2409.09137</link>
      <description>arXiv:2409.09137v2 Announce Type: replace 
Abstract: We consider robust optimal experimental design (ROED) for nonlinear Bayesian inverse problems governed by partial differential equations (PDEs). An optimal design is one that maximizes some utility quantifying the quality of the solution of an inverse problem. However, the optimal design is dependent on elements of the inverse problem such as the simulation model, the prior, or the measurement error model. ROED aims to produce an optimal design that is aware of the additional uncertainties encoded in the inverse problem and remains optimal even after variations in them. We follow a worst-case scenario approach to develop a new framework for robust optimal design of nonlinear Bayesian inverse problems. The proposed framework a) is scalable and designed for infinite-dimensional Bayesian nonlinear inverse problems constrained by PDEs; b) develops efficient approximations of the utility, namely, the expected information gain; c) employs eigenvalue sensitivity techniques to develop analytical forms and efficient evaluation methods of the gradient of the utility with respect to the uncertainties we wish to be robust against; and d) employs a probabilistic optimization paradigm that properly defines and efficiently solves the resulting combinatorial max-min optimization problem. The effectiveness of the proposed approach is illustrated for optimal sensor placement problem in an inverse problem governed by an elliptic PDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09137v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhijit Chowdhary, Ahmed Attia, Alen Alexanderian</dc:creator>
    </item>
    <item>
      <title>Improving the robustness of neural ODEs with minimal weight perturbation</title>
      <link>https://arxiv.org/abs/2501.10740</link>
      <description>arXiv:2501.10740v2 Announce Type: replace 
Abstract: We propose a method to enhance the stability of a neural ordinary differential equation (neural ODE) by reducing the maximum error growth subsequent to a perturbation of the initial value. Since the stability depends on the logarithmic norm of the Jacobian matrix associated with the neural ODE, we control the logarithmic norm by perturbing the weight matrices of the neural ODE by a smallest possible perturbation (in Frobenius norm). We do so by engaging an eigenvalue optimisation problem, for which we propose a nested two-level algorithm. For a given perturbation size of the weight matrix, the inner level computes optimal perturbations of that size, while - at the outer level - we tune the perturbation amplitude until we reach the desired uniform stability bound. We embed the proposed algorithm in the training of the neural ODE to improve its robustness to perturbations of the initial value, as adversarial attacks. Numerical experiments on classical image datasets show that an image classifier including a neural ODE in its architecture trained according to our strategy is more stable than the same classifier trained in the classical way, and therefore, it is more robust and less vulnerable to adversarial attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10740v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arturo De Marinis, Nicola Guglielmi, Stefano Sicilia, Francesco Tudisco</dc:creator>
    </item>
    <item>
      <title>Numerical evaluation of the Kirchhoff-Helmholtz integral outside a sphere</title>
      <link>https://arxiv.org/abs/2506.04809</link>
      <description>arXiv:2506.04809v2 Announce Type: replace 
Abstract: A method is presented for the fast evaluation of the transient acoustic field generated outside a spherical surface using surface data on the sphere. The method employs Lebedev quadratures, which are optimal integration on the sphere, and Lagrange interpolation and differentiation in an advanced time algorithm for the evaluation of the transient field. Numerical testing demonstrates that the approach gives near machine-precision accuracy and a speed-up in evaluation time which depends on the order of quadrature rule employed but breaks even with direct evaluation at a number of field points about 1.15 times the number of surface quadrature nodes, making the method an efficient means of evaluating the field generated by a large number of sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04809v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>physics.class-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael J. Carley</dc:creator>
    </item>
    <item>
      <title>Numerical Analysis of the Causal Action Principle in Low Dimensions</title>
      <link>https://arxiv.org/abs/2201.06382</link>
      <description>arXiv:2201.06382v2 Announce Type: replace-cross 
Abstract: The numerical analysis of causal fermion systems is advanced by employing differentiable programming methods. The causal action principle for weighted counting measures is introduced for general values of the integer parameters $f$ (the particle number), $n$ (the spin dimension) and $m$ (the number of spacetime points). In the case $n=1$, the causal relations are clarified geometrically in terms of causal cones. Discrete Dirac spheres are introduced as candidates for minimizers for large $m$ in the cases $n=1, f=2$ and $n=2, f=4$. We provide a thorough numerical analysis of the causal action principle for weighted counting measures for large $m$ in the cases $n=1,2$ and $f=2,3,4$. Our numerical findings corroborate that all minimizers for large $m$ are good approximations of the discrete Dirac spheres. In the example $n=1, f=3$ it is explained how numerical minimizers can be visualized by projected spacetime plots. Methods and prospects are discussed to numerically investigate settings in which hitherto no analytic candidates for minimizers are known.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.06382v2</guid>
      <category>math-ph</category>
      <category>cs.NA</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Finster, Robert H. Jonsson, Niki Kilbertus</dc:creator>
    </item>
    <item>
      <title>Regularity of vector fields with piecewise regular curl and divergence</title>
      <link>https://arxiv.org/abs/2408.16556</link>
      <description>arXiv:2408.16556v3 Announce Type: replace-cross 
Abstract: We consider a bounded Lipschitz domain $\Omega\subseteq\mathbb{R}^3$ with sufficiently smooth boundary and prove piecewise Sobolev regularity of vector fields that have piecewise regular curl and divergence, but may be discontinuous across mutually disjoint and sufficiently smooth surfaces inside of $\Omega$. The main idea behind our approach is to employ recently developed parametrices for the curl-operator and the regularity theory of Poisson transmission problems. We conclude our work by applying our findings to the heterogeneous time-harmonic Maxwell equations with either a) impedance, b) natural or c) essential boundary conditions and providing wavenumber-explicit piecewise regularity estimates for these equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16556v3</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jens Markus Melenk, David W\"org\"otter</dc:creator>
    </item>
    <item>
      <title>Fast Mixed-Precision Real Evaluation</title>
      <link>https://arxiv.org/abs/2410.07468</link>
      <description>arXiv:2410.07468v3 Announce Type: replace-cross 
Abstract: Evaluating real-valued expressions to high precision is a key building block in computational mathematics, physics, and numerics. A typical implementation evaluates the whole expression in a uniform precision, doubling that precision until a sufficiently-accurate result is achieved. This is wasteful: usually only a few operations really need to be performed at high precision, and the bulk of the expression could be computed much faster. However, such non-uniform precision assignments have, to date, been impractical to compute. We propose a fast new algorithm for deriving such precision assignments. The algorithm leverages results computed at lower precisions to analytically determine a mixed-precision assignment that will result in a sufficiently-accurate result. Our implementation, Reval, achieves an average speed-up of 1.72x compared to the state-of-the-art Sollya tool, with the speed-up increasing to 5.21x on the most difficult input points. An examination of the precisions used with and without precision tuning shows that the speed-up results from assigning lower precisions for the majority of operations, though additional optimizations enabled by the non-uniform precision assignments also play a role.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07468v3</guid>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Artem Yadrov, Pavel Panchekha</dc:creator>
    </item>
    <item>
      <title>Randomised Postiterations for Calibrated BayesCG</title>
      <link>https://arxiv.org/abs/2504.04247</link>
      <description>arXiv:2504.04247v2 Announce Type: replace-cross 
Abstract: The Bayesian conjugate gradient method offers probabilistic solutions to linear systems but suffers from poor calibration, limiting its utility in uncertainty quantification tasks. Recent approaches leveraging postiterations to construct priors have improved computational properties but failed to correct calibration issues. In this work, we propose a novel randomised postiteration strategy that enhances the calibration of the BayesCG posterior while preserving its favourable convergence characteristics. We present theoretical guarantees for the improved calibration, supported by results on the distribution of posterior errors. Numerical experiments demonstrate the efficacy of the method in both synthetic and inverse problem settings, showing enhanced uncertainty quantification and better propagation of uncertainties through computational pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04247v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niall Vyas, Disha Hegde, Jon Cockayne</dc:creator>
    </item>
    <item>
      <title>Structural Optimal Jacobian Accumulation and Minimum Edge Count are NP-Complete Under Vertex Elimination</title>
      <link>https://arxiv.org/abs/2506.17521</link>
      <description>arXiv:2506.17521v2 Announce Type: replace-cross 
Abstract: We study graph-theoretic formulations of two fundamental problems in algorithmic differentiation. The first (Structural Optimal Jacobian Accumulation) is that of computing a Jacobian while minimizing multiplications. The second (Minimum Edge Count) is to find a minimum-size computational graph. For both problems, we consider the vertex elimination operation. Our main contribution is to show that both problems are NP-complete, thus resolving longstanding open questions. In contrast to prior work, our reduction for Structural Optimal Jacobian Accumulation does not rely on any assumptions about the algebraic relationships between local partial derivatives; we allow these values to be mutually independent. We also provide $O^*(2^n)$-time exact algorithms for both problems, and show that under the exponential time hypothesis these running times are essentially tight. Finally, we provide a data reduction rule for Structural Optimal Jacobian Accumulation by showing that false twins may always be eliminated consecutively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17521v2</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, Alex Crane, P{\aa}l Gr{\o}n{\aa}s Drange, Yosuke Mizutani, Blair D. Sullivan</dc:creator>
    </item>
  </channel>
</rss>
