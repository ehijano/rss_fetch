<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Dec 2025 05:01:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Adaptive Kalman Filter for Systems with Unknown Initial Values</title>
      <link>https://arxiv.org/abs/2512.15810</link>
      <description>arXiv:2512.15810v1 Announce Type: new 
Abstract: The models of partially observed linear stochastic differential equations with unknown initial values of the non-observed component are considered in two situations. In the first problem, the initial value is deterministic, and in the second problem, it is assumed to be a Gaussian random variable. The main problem is the computation of adaptive Kalman filters and the discussion of their asymptotic optimality. The realization of this program for both models is done in several steps. First, a preliminary estimator of the unknown parameter is constructed by observations on some learning interval. Then, this estimator is used for the calculation of recurrent one-step MLE estimators, which are subsequently substituted in the equations of Kalman filtration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15810v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yury A Kutoyants (UM)</dc:creator>
    </item>
    <item>
      <title>Rao-Blackwellized e-variables</title>
      <link>https://arxiv.org/abs/2512.16759</link>
      <description>arXiv:2512.16759v1 Announce Type: new 
Abstract: We show that for any concave utility, the expected utility of an e-variable can only increase after conditioning on a sufficient statistic. The simplest form of the result has an extremely straightforward proof, which follows from a single application of Jensen's inequality. Similar statements hold for compound e-variables, asymptotic e-variables, and e-processes. These results echo the Rao-Blackwell theorem, which states that the expected squared error of an estimator can only decrease after conditioning on a sufficient statistic. We provide several applications of this insight, including a simplified derivation of the log-optimal e-variable for linear regression with known variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16759v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dante de Roos, Ben Chugg, Peter Gr\"unwald, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Empirical Likelihood Meets Prediction-Powered Inference</title>
      <link>https://arxiv.org/abs/2512.16363</link>
      <description>arXiv:2512.16363v1 Announce Type: cross 
Abstract: We study inference with a small labeled sample, a large unlabeled sample, and high-quality predictions from an external model. We link prediction-powered inference with empirical likelihood by stacking supervised estimating equations based on labeled outcomes with auxiliary moment conditions built from predictions, and then optimizing empirical likelihood under these joint constraints. The resulting empirical likelihood-based prediction-powered inference (EPI) estimator is asymptotically normal, has asymptotic variance no larger than the fully supervised estimator, and attains the semiparametric efficiency bound when the auxiliary functions span the predictable component of the supervised score. For hypothesis testing and confidence sets, empirical likelihood ratio statistics admit chi-squared-type limiting distributions. As a by-product, the empirical likelihood weights induce a calibrated empirical distribution that integrates supervised and prediction-based information, enabling estimation and uncertainty quantification for general functionals beyond parameters defined by estimating equations. We present two practical implementations: one based on basis expansions in the predictions and covariates, and one that learns an approximately optimal auxiliary function by cross-fitting. In simulations and applications, EPI reduces mean squared error and shortens confidence intervals while maintaining nominal coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16363v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanghui Wang, Mengtao Wen, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection</title>
      <link>https://arxiv.org/abs/2512.16411</link>
      <description>arXiv:2512.16411v1 Announce Type: cross 
Abstract: Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16411v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>q-fin.TR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthieu Garcin, Louis Perot</dc:creator>
    </item>
    <item>
      <title>Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery</title>
      <link>https://arxiv.org/abs/2512.16875</link>
      <description>arXiv:2512.16875v1 Announce Type: cross 
Abstract: We study the problem of finding confidence ellipsoids for an arbitrary distribution in high dimensions. Given samples from a distribution $D$ and a confidence parameter $\alpha$, the goal is to find the smallest volume ellipsoid $E$ which has probability mass $\Pr_{D}[E] \ge 1-\alpha$. Ellipsoids are a highly expressive class of confidence sets as they can capture correlations in the distribution, and can approximate any convex set. This problem has been studied in many different communities. In statistics, this is the classic minimum volume estimator introduced by Rousseeuw as a robust non-parametric estimator of location and scatter. However in high dimensions, it becomes NP-hard to obtain any non-trivial approximation factor in volume when the condition number $\beta$ of the ellipsoid (ratio of the largest to the smallest axis length) goes to $\infty$. This motivates the focus of our paper: can we efficiently find confidence ellipsoids with volume approximation guarantees when compared to ellipsoids of bounded condition number $\beta$?
  Our main result is a polynomial time algorithm that finds an ellipsoid $E$ whose volume is within a $O(\beta^{\gamma d})$ multiplicative factor of the volume of best $\beta$-conditioned ellipsoid while covering at least $1-O(\alpha/\gamma)$ probability mass for any $\gamma &lt; \alpha$. We complement this with a computational hardness result that shows that such a dependence seems necessary up to constants in the exponent. The algorithm and analysis uses the rich primal-dual structure of the minimum volume enclosing ellipsoid and the geometric Brascamp-Lieb inequality. As a consequence, we obtain the first polynomial time algorithm with approximation guarantees on worst-case instances of the robust subspace recovery problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16875v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Gao, Liren Shan, Vaidehi Srinivas, Aravindan Vijayaraghavan</dc:creator>
    </item>
    <item>
      <title>Theoretical Foundations of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2411.11824</link>
      <description>arXiv:2411.11824v4 Announce Type: replace 
Abstract: This book is about conformal prediction and related inferential techniques that build on permutation tests and exchangeability. These techniques are useful in a diverse array of tasks, including hypothesis testing and providing uncertainty quantification guarantees for machine learning systems. Much of the current interest in conformal prediction is due to its ability to integrate into complex machine learning workflows, solving the problem of forming prediction sets without any assumptions on the form of the data generating distribution. Since contemporary machine learning algorithms have generally proven difficult to analyze directly, conformal prediction's main appeal is its ability to provide formal, finite-sample guarantees when paired with such methods.
  The goal of this book is to teach the reader about the fundamental technical arguments that arise when researching conformal prediction and related questions in distribution-free inference. Many of these proof strategies, especially the more recent ones, are scattered among research papers, making it difficult for researchers to understand where to look, which results are important, and how exactly the proofs work. We hope to bridge this gap by curating what we believe to be some of the most important results in the literature and presenting their proofs in a unified language, with illustrations, and with an eye towards pedagogy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11824v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos, Rina Foygel Barber, Stephen Bates</dc:creator>
    </item>
    <item>
      <title>Cartan meets Cram\'er-Rao</title>
      <link>https://arxiv.org/abs/2511.15612</link>
      <description>arXiv:2511.15612v2 Announce Type: replace 
Abstract: A Cartan-geometric, jet bundle formulation of curvature-aware variance bounds in parametric statistical estimation is developed. Building on our earlier extrinsic Hilbert space approach to the Cram\'er-Rao and Bhattacharyya-type inequalities, we show that the curvature corrections induced by the square root embedding of a statistical model admit a canonical intrinsic interpretation via jet geometry and Cartan's prolongation theory.
  For a scalar-parameter family with square root map $s_\theta=\sqrt{f(\cdot;\theta)}\in L^2(\mu)$, we regard $s_\theta$ as a section of the statistical bundle $E=\Theta\times L^2(\mu)$ and study its finite-order prolongations. We point out that the classical algebraic efficiency condition--that the estimator residual $(T-\theta)s_\theta$ lies in the span of derivatives of $s_\theta$ up to order $m$--is equivalent to the existence of a linear ordinary differential equation (ODE) of order $m$ satisfied by the square root map. Geometrically, this means the prolonged section lies in an ODE-defined submanifold of the jet bundle and is an integral curve of the restricted Cartan vector field.
  The obstruction to such finite-order integrability is identified with the vertical component of the canonical Ehresmann connection on the jet tower, which coincides with the curvature correction term in variance bounds. This establishes a direct correspondence between algebraic projection conditions in $L^2(\mu)$ and intrinsic holonomy properties of statistical sections, yielding a unified geometric interpretation of higher-order information inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15612v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.DG</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunder Ram Krishnan</dc:creator>
    </item>
    <item>
      <title>Understanding Overparametrization in Survival Models through Interpolation</title>
      <link>https://arxiv.org/abs/2512.12463</link>
      <description>arXiv:2512.12463v2 Announce Type: replace-cross 
Abstract: Classical statistical learning theory predicts a U-shaped relationship between test loss and model capacity, driven by the bias-variance trade-off. Recent advances in modern machine learning have revealed a more complex pattern, \textit{double-descent}, in which test loss, after peaking near the interpolation threshold, decreases again as model capacity continues to grow. While this behavior has been extensively analyzed in regression and classification, its manifestation in survival analysis remains unexplored. This study investigates overparametrization in four representative survival models: DeepSurv, PC-Hazard, Nnet-Survival, and N-MTLR. We rigorously define \textit{interpolation} and \textit{finite-norm interpolation}, two key characteristics of loss-based models to understand \textit{double-descent}. We then show the existence (or absence) of \textit{(finite-norm) interpolation} of all four models. Our findings clarify how likelihood-based losses and model implementation jointly determine the feasibility of \textit{interpolation} and show that overparametrization should not be regarded as benign for survival models. All theoretical results are supported by numerical experiments that highlight the distinct generalization behaviors of survival models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12463v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Liu, Jianwen Cai, Didong Li</dc:creator>
    </item>
  </channel>
</rss>
