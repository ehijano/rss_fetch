<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jun 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Efficient Hardware Implementation of Elliptic Curve Point Multiplication over $GF(2^m)$ on FPGA</title>
      <link>https://arxiv.org/abs/2506.12359</link>
      <description>arXiv:2506.12359v1 Announce Type: new 
Abstract: Elliptic Curve Cryptography (ECC) is widely accepted for ensuring secure data exchange between resource-limited IoT devices. The National Institute of Standards and Technology (NIST) recommended implementation, such as B-163, is particularly well-suited for Internet of Things (IoT) applications. Here, Elliptic Curve Point Multiplication (ECPM) is the most time-critical and resource-intensive operation due to the finite field multiplier. This paper proposes a new implementation method of finite field multiplication using a hybrid Karatsuba multiplier, which achieves a significant improvement in computation time while maintaining a reasonable area footprint. The proposed multiplier, along with a finite field adder, squarer, and extended Euclidean inversion circuit, is used to implement an architecture for ECPM using the Montgomery algorithm. The architecture is evaluated for $GF(2^{163})$ on the Xilinx Virtex-7 FPGA platform, achieving a maximum frequency of 213~MHz and occupying 14,195 Lookup Tables (LUTs). The results demonstrate a significant speedup in computation time and overall performance compared to other reported designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12359v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-981-97-3756-7_20</arxiv:DOI>
      <arxiv:journal_reference>Lecture Notes in Electrical Engineering 1210 (2025) 223-236</arxiv:journal_reference>
      <dc:creator>Ruby Kumari, Tapas Rout, Babul Saini, Jai Gopal Pandey, Abhijit Karmakar</dc:creator>
    </item>
    <item>
      <title>PuDHammer: Experimental Analysis of Read Disturbance Effects of Processing-using-DRAM in Real DRAM Chips</title>
      <link>https://arxiv.org/abs/2506.12947</link>
      <description>arXiv:2506.12947v1 Announce Type: new 
Abstract: Processing-using-DRAM (PuD) is a promising paradigm for alleviating the data movement bottleneck using DRAM's massive internal parallelism and bandwidth to execute very wide operations. Performing a PuD operation involves activating multiple DRAM rows in quick succession or simultaneously, i.e., multiple-row activation. Multiple-row activation is fundamentally different from conventional memory access patterns that activate one DRAM row at a time. However, repeatedly activating even one DRAM row (e.g., RowHammer) can induce bitflips in unaccessed DRAM rows because modern DRAM is subject to read disturbance. Unfortunately, no prior work investigates the effects of multiple-row activation on DRAM read disturbance.
  In this paper, we present the first characterization study of read disturbance effects of multiple-row activation-based PuD (which we call PuDHammer) using 316 real DDR4 DRAM chips from four major DRAM manufacturers. Our detailed characterization show that 1) PuDHammer significantly exacerbates the read disturbance vulnerability, causing up to 158.58x reduction in the minimum hammer count required to induce the first bitflip ($HC_{first}$), compared to RowHammer, 2) PuDHammer is affected by various operational conditions and parameters, 3) combining RowHammer with PuDHammer is more effective than using RowHammer alone to induce read disturbance error, e.g., doing so reduces $HC_{first}$ by 1.66x on average, and 4) PuDHammer bypasses an in-DRAM RowHammer mitigation mechanism (Target Row Refresh) and induces more bitflips than RowHammer.
  To develop future robust PuD-enabled systems in the presence of PuDHammer, we 1) develop three countermeasures and 2) adapt and evaluate the state-of-the-art RowHammer mitigation standardized by industry, called Per Row Activation Counting (PRAC). We show that the adapted PRAC incurs large performance overheads (48.26%, on average).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12947v1</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ismail Emir Yuksel, Akash Sood, Ataberk Olgun, O\u{g}uzhan Canpolat, Haocong Luo, F. Nisa Bostanc{\i}, Mohammad Sadrosadati, A. Giray Ya\u{g}l{\i}k\c{c}{\i}, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>FPGA &amp; VPU Co-Processing in Space Applications: Development and Testing with DSP/AI Benchmarks</title>
      <link>https://arxiv.org/abs/2506.12968</link>
      <description>arXiv:2506.12968v1 Announce Type: new 
Abstract: The advent of computationally demanding algorithms and high data rate instruments in new space applications pushes the space industry to explore disruptive solutions for on-board data processing. We examine heterogeneous computing architectures involving high-performance and low-power commercial SoCs. The current paper implements an FPGA with VPU co-processing architecture utilizing the CIF &amp; LCD interfaces for I/O data transfers. A Kintex FPGA serves as our framing processor and heritage accelerator, while we offload novel DSP/AI functions to a Myriad2 VPU. We prototype our architecture in the lab to evaluate the interfaces, the FPGA resource utilization, the VPU computational throughput, as well as the entire data handling system's performance, via custom benchmarking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12968v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICECS53924.2021.9665462</arxiv:DOI>
      <arxiv:journal_reference>28th IEEE International Conference on Electronics, Circuits and Systems (ICECS), 2021</arxiv:journal_reference>
      <dc:creator>Vasileios Leon, Charalampos Bezaitis, George Lentaris, Dimitrios Soudris, Dionysios Reisis, Elissaios-Alexios Papatheofanous, Angelos Kyriakos, Aubrey Dunne, Arne Samuelsson, David Steenari</dc:creator>
    </item>
    <item>
      <title>Towards Employing FPGA and ASIP Acceleration to Enable Onboard AI/ML in Space Applications</title>
      <link>https://arxiv.org/abs/2506.12970</link>
      <description>arXiv:2506.12970v1 Announce Type: new 
Abstract: The success of AI/ML in terrestrial applications and the commercialization of space are now paving the way for the advent of AI/ML in satellites. However, the limited processing power of classical onboard processors drives the community towards extending the use of FPGAs in space with both rad-hard and Commercial-Off-The-Shelf devices. The increased performance of FPGAs can be complemented with VPU or TPU ASIP co-processors to further facilitate high-level AI development and in-flight reconfiguration. Thus, selecting the most suitable devices and designing the most efficient avionics architecture becomes crucial for the success of novel space missions. The current work presents industrial trends, comparative studies with in-house benchmarking, as well as architectural designs utilizing FPGAs and AI accelerators towards enabling AI/ML in future space missions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12970v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/VLSI-SoC54400.2022.9939566</arxiv:DOI>
      <arxiv:journal_reference>30th IFIP/IEEE International Conference on Very Large Scale Integration (VLSI-SoC), 2022</arxiv:journal_reference>
      <dc:creator>Vasileios Leon, George Lentaris, Dimitrios Soudris, Simon Vellas, Mathieu Bernou</dc:creator>
    </item>
    <item>
      <title>Combining Fault Tolerance Techniques and COTS SoC Accelerators for Payload Processing in Space</title>
      <link>https://arxiv.org/abs/2506.12971</link>
      <description>arXiv:2506.12971v1 Announce Type: new 
Abstract: The ever-increasing demand for computational power and I/O throughput in space applications is transforming the landscape of on-board computing. A variety of Commercial-Off-The-Shelf (COTS) accelerators emerges as an attractive solution for payload processing to outperform the traditional radiation-hardened devices. Towards increasing the reliability of such COTS accelerators, the current paper explores and evaluates fault-tolerance techniques for the Zynq FPGA and the Myriad VPU, which are two device families being integrated in industrial space avionics architectures/boards, such as Ubotica's CogniSat, Xiphos' Q7S, and Cobham Gaisler's GR-VPX-XCKU060. On the FPGA side, we combine techniques such as memory scrubbing, partial reconfiguration, triple modular redundancy, and watchdogs. On the VPU side, we detect and correct errors in the instruction and data memories, as well as we apply redundancy at processor level (SHAVE cores). When considering FPGA with VPU co-processing, we also develop a fault-tolerant interface between the two devices based on the CIF/LCD protocols and our custom CRC error-detecting code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12971v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/VLSI-SoC54400.2022.9939621</arxiv:DOI>
      <arxiv:journal_reference>30th IFIP/IEEE International Conference on Very Large Scale Integration (VLSI-SoC), 2022</arxiv:journal_reference>
      <dc:creator>Vasileios Leon, Elissaios Alexios Papatheofanous, George Lentaris, Charalampos Bezaitis, Nikolaos Mastorakis, Georgios Bampilis, Dionysios Reisis, Dimitrios Soudris</dc:creator>
    </item>
    <item>
      <title>Reconfigurable Digital RRAM Logic Enables In-Situ Pruning and Learning for Edge AI</title>
      <link>https://arxiv.org/abs/2506.13151</link>
      <description>arXiv:2506.13151v1 Announce Type: new 
Abstract: The human brain simultaneously optimizes synaptic weights and topology by growing, pruning, and strengthening synapses while performing all computation entirely in memory. In contrast, modern artificial-intelligence systems separate weight optimization from topology optimization and depend on energy-intensive von Neumann architectures. Here, we present a software-hardware co-design that bridges this gap. On the algorithmic side, we introduce a real-time dynamic weight-pruning strategy that monitors weight similarity during training and removes redundancies on the fly, reducing operations by 26.80% on MNIST and 59.94% on ModelNet10 without sacrificing accuracy (91.44% and 77.75%, respectively). On the hardware side, we fabricate a reconfigurable, fully digital compute-in-memory (CIM) chip based on 180 nm one-transistor-one-resistor (1T1R) RRAM arrays. Each array embeds flexible Boolean logic (NAND, AND, XOR, OR), enabling both convolution and similarity evaluation inside memory and eliminating all ADC/DAC overhead. The digital design achieves zero bit-error, reduces silicon area by 72.30% and overall energy by 57.26% compared to analogue RRAM CIM, and lowers energy by 75.61% and 86.53% on MNIST and ModelNet10, respectively, relative to an NVIDIA RTX 4090. Together, our co-design establishes a scalable brain-inspired paradigm for adaptive, energy-efficient edge intelligence in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13151v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Songqi Wang, Yue Zhang, Jia Chen, Xinyuan Zhang, Yi Li, Ning Lin, Yangu He, Jichang Yang, Yingjie Yu, Yi Li, Zhongrui Wang, Xiaojuan Qi, Han Wang</dc:creator>
    </item>
    <item>
      <title>PRO-V: An Efficient Program Generation Multi-Agent System for Automatic RTL Verification</title>
      <link>https://arxiv.org/abs/2506.12200</link>
      <description>arXiv:2506.12200v1 Announce Type: cross 
Abstract: LLM-assisted hardware verification is gaining substantial attention due to its potential to significantly reduce the cost and effort of crafting effective testbenches. It also serves as a critical enabler for LLM-aided end-to-end hardware language design. However, existing current LLMs often struggle with Register Transfer Level (RTL) code generation, resulting in testbenches that exhibit functional errors in Hardware Description Languages (HDL) logic. Motivated by the strong performance of LLMs in Python code generation under inference-time sampling strategies, and their promising capabilities as judge agents, we propose PRO-V a fully program generation multi-agent system for robust RTL verification. Pro-V incorporates an efficient best-of-n iterative sampling strategy to enhance the correctness of generated testbenches. Moreover, it introduces an LLM-as-a-judge aid validation framework featuring an automated prompt generation pipeline. By converting rule-based static analysis from the compiler into natural language through in-context learning, this pipeline enables LLMs to assist the compiler in determining whether verification failures stem from errors in the RTL design or the testbench. PRO-V attains a verification accuracy of 87.17% on golden RTL implementations and 76.28% on RTL mutants. Our code is open-sourced at https://github.com/stable-lab/Pro-V.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12200v1</guid>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujie Zhao, Zhijing Wu, Hejia Zhang, Zhongming Yu, Wentao Ni, Chia-Tung Ho, Haoxing Ren, Jishen Zhao</dc:creator>
    </item>
    <item>
      <title>A Novel Thermal Network Model and Electro-Thermal Coupling Study for NSFETs and CFETs Considering Thermal Crosstalk</title>
      <link>https://arxiv.org/abs/2506.12264</link>
      <description>arXiv:2506.12264v1 Announce Type: cross 
Abstract: As the technology node continues to shrink, nanosheet field effect transistors (NSFETs) and complementary FETs (CFETs) become valid candidates for the 3nm and sub-nanometre nodes. However, due to the shrinking device size, self-heating and inter-device thermal crosstalk of NSFETs and CFETs become more severe. It is important to accurately calculate the self-heating and thermal crosstalk of devices and to study the electrical and thermal characteristics of logic gates, etc. In this work, a thermal network model considering the thermal crosstalk of neighboring devices is proposed, which can accurately calculate the self-heating and thermal crosstalk. The electrical and thermal characteristics of NSFETs and CFETs are compared, and it is found that CFETs have more severe self-heating and thermal crosstalk. The electro-thermal characteristics of inverters, logic gates and ring oscillators composed of NSFETs and CFETs are further investigated. Compared with NSFETs, logic gates and ring oscillators composed of CFETs are more seriously affected by self-heating and should be given extra attention. The thermal network model proposed in this paper can be further used to study the thermal optimization strategy of devices and circuits to enhance the electrical performance, achieving the design technology co-optimizations (DTCO).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12264v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianci Miao, Qihang Zheng, Yangyang Hu, Xiaoyu Cheng, Jie Liang, Liang Chen, Aiying Guo, Jingjing Liu, Kailin Ren, Jianhua Zhang</dc:creator>
    </item>
    <item>
      <title>Serving Large Language Models on Huawei CloudMatrix384</title>
      <link>https://arxiv.org/abs/2506.12708</link>
      <description>arXiv:2506.12708v1 Announce Type: cross 
Abstract: The rapid evolution of large language models (LLMs), driven by growing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure. Traditional AI clusters face limitations in compute intensity, memory bandwidth, inter-chip communication, and latency, compounded by variable workloads and strict service-level objectives. Addressing these issues requires fundamentally redesigned hardware-software integration. This paper introduces Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified Bus (UB) network, enabling direct all-to-all communication and dynamic pooling of resources. These features optimize performance for communication-intensive operations, such as large-scale MoE expert parallelism and distributed key-value cache access. To fully leverage CloudMatrix384, we propose CloudMatrix-Infer, an advanced LLM serving solution incorporating three core innovations: a peer-to-peer serving architecture that independently scales prefill, decode, and caching; a large-scale expert parallelism strategy supporting EP320 via efficient UB-based token dispatch; and hardware-aware optimizations including specialized operators, microbatch-based pipelining, and INT8 quantization. Evaluation with the DeepSeek-R1 model shows CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of 6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (&lt;50 ms TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s even under stringent 15 ms latency constraints, while INT8 quantization maintains model accuracy across benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12708v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pengfei Zuo, Huimin Lin, Junbo Deng, Nan Zou, Xingkun Yang, Yingyu Diao, Weifeng Gao, Ke Xu, Zhangyu Chen, Shirui Lu, Zhao Qiu, Peiyang Li, Xianyu Chang, Zhengzhong Yu, Fangzheng Miao, Jia Zheng, Ying Li, Yuan Feng, Bei Wang, Zaijian Zong, Mosong Zhou, Wenli Zhou, Houjiang Chen, Xingyu Liao, Yipeng Li, Wenxiao Zhang, Ping Zhu, Yinggang Wang, Chuanjie Xiao, Depeng Liang, Dong Cao, Juncheng Liu, Yongqiang Yang, Xiaolong Bai, Yi Li, Huaguo Xie, Huatao Wu, Zhibin Yu, Lv Chen, Hu Liu, Yujun Ding, Haipei Zhu, Jing Xia, Yi Xiong, Zhou Yu, Heng Liao</dc:creator>
    </item>
    <item>
      <title>Reconfigurable Stream Network Architecture</title>
      <link>https://arxiv.org/abs/2411.17966</link>
      <description>arXiv:2411.17966v3 Announce Type: replace 
Abstract: As AI systems grow increasingly specialized and complex, managing hardware heterogeneity becomes a pressing challenge. How can we efficiently coordinate and synchronize heterogeneous hardware resources to achieve high utilization? How can we minimize the friction of transitioning between diverse computation phases, reducing costly stalls from initialization, pipeline setup, or drain? Our insight is that a network abstraction at the ISA level naturally unifies heterogeneous resource orchestration and phase transitions.
  This paper presents a Reconfigurable Stream Network Architecture (RSN), a novel ISA abstraction designed for the DNN domain. RSN models the datapath as a circuit-switched network with stateful functional units as nodes and data streaming on the edges. Programming a computation corresponds to triggering a path. Software is explicitly exposed to the compute and communication latency of each functional unit, enabling precise control over data movement for optimizations such as compute-communication overlap and layer fusion. As nodes in a network naturally differ, the RSN abstraction can efficiently virtualize heterogeneous hardware resources by separating control from the data plane, enabling low instruction-level intervention.
  We build a proof-of-concept design RSN-XNN on VCK190, a heterogeneous platform with FPGA fabric and AI engines. Compared to the SOTA solution on this platform, it reduces latency by 6.1x and improves throughput by 2.4x-3.2x. Compared to the T4 GPU with the same FP32 performance, it matches latency with only 18% of the memory bandwidth. Compared to the A100 GPU at the same 7nm process node, it achieves 2.1x higher energy efficiency in FP32.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17966v3</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3695053.3731088</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 52nd Annual International Symposium on Computer Architecture (ISCA), Tokyo, Japan, June 2025, ACM, pp. 1-19</arxiv:journal_reference>
      <dc:creator>Chengyue Wang, Xiaofan Zhang, Jason Cong, James C. Hoe</dc:creator>
    </item>
    <item>
      <title>AnalogXpert: Automating Analog Topology Synthesis by Incorporating Circuit Design Expertise into Large Language Models</title>
      <link>https://arxiv.org/abs/2412.19824</link>
      <description>arXiv:2412.19824v2 Announce Type: replace 
Abstract: Analog circuits are crucial in modern electronic systems, and automating their design has attracted significant research interest. One of major challenges is topology synthesis, which determines circuit components and their connections. Recent studies explore large language models (LLM) for topology synthesis. However, the scenarios addressed by these studies do not align well with practical applications. Specifically, existing work uses vague design requirements as input and outputs an ideal model, but detailed structural requirements and device-level models are more practical. Moreover, current approaches either formulate topology synthesis as graph generation or Python code generation, whereas practical topology design is a complex process that demands extensive design knowledge. In this work, we propose AnalogXpert, a LLM-based agent aiming at solving practical topology synthesis problem by incorporating circuit design expertise into LLMs. First, we represent analog topology as SPICE code and introduce a subcircuit library to reduce the design space, in the same manner as experienced designers. Second, we decompose the problem into two sub-task (i.e., block selection and block connection) through the use of CoT and incontext learning techniques, to mimic the practical design process. Third, we introduce a proofreading strategy that allows LLMs to incrementally correct the errors in the initial design, akin to human designers who iteratively check and adjust the initial topology design to ensure accuracy. Finally, we construct a high-quality benchmark containing both real data (30) and synthetic data (2k). AnalogXpert achieves 40% and 23% success rates on the synthetic dataset and real dataset respectively, which is markedly better than those of GPT-4o (3% on both the synthetic dataset and the real dataset).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19824v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyi Zhang, Shizhao Sun, Yibo Lin, Runsheng Wang, Jiang Bian</dc:creator>
    </item>
    <item>
      <title>Refining Datapath for Microscaling ViTs</title>
      <link>https://arxiv.org/abs/2505.22194</link>
      <description>arXiv:2505.22194v2 Announce Type: replace 
Abstract: Vision Transformers (ViTs) leverage the transformer architecture to effectively capture global context, demonstrating strong performance in computer vision tasks. A major challenge in ViT hardware acceleration is that the model family contains complex arithmetic operations that are sensitive to model accuracy, such as the Softmax and LayerNorm operations, which cannot be mapped onto efficient hardware with low precision. Existing methods only exploit parallelism in the matrix multiplication operations of the model on hardware and keep these complex operations on the CPU. This results in suboptimal performance due to the communication overhead between the CPU and accelerator. Can new data formats solve this problem?
  In this work, we present the first ViT accelerator that maps all operations of the ViT models onto FPGAs. We exploit a new arithmetic format named Microscaling Integer (MXInt) for datapath designs and evaluate how different design choices can be made to trade off accuracy, hardware performance, and hardware utilization. Our contributions are twofold. First, we quantize ViTs using the MXInt format, achieving both high area efficiency and accuracy. Second, we propose MXInt-specific hardware optimization that map these complex arithmetic operations into custom hardware. Within 1\% accuracy loss, our method achieves at least 93$\times$ speedup compared to Float16 and at least 1.9$\times$ speedup compared to related work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22194v2</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Can Xiao, Jianyi Cheng, Aaron Zhao</dc:creator>
    </item>
    <item>
      <title>DSAC: Low-Cost RowHammer Mitigation Using In-DRAM Stochastic and Approximate Counting Algorithm</title>
      <link>https://arxiv.org/abs/2302.03591</link>
      <description>arXiv:2302.03591v3 Announce Type: replace-cross 
Abstract: This paper provides the fundamental mechanisms of two types of row activation-induced bit flips and proposes in-DRAM protection techniques. RowBleed occurs when a victim row experiences charge leakage due to transistor's threshold voltage lowering induced by long activation of a neighboring aggressor row. Therefore, this paper proposes Time-Weighted Counting for RowBleed mitigation, which assigns greater counter weights to rows that are activated for longer durations.
  On the other hand, RowHammer occurs when a victim row experiences electron injection due to frequent activation of a neighboring aggressor row. Similarly, Extended RowHammer, the phenomenon where victim rows are two rows beyond aggressor rows, is also caused by electron injection due to frequent activation of a neighboring aggressor row. Consequently, accurate detection of aggressor rows is crucial. Therefore, this paper proposes RowHammer mitigation algorithm named DSAC (in-DRAM Stochastic and Approximate Counting algorithm), which utilizes a replacement probability that adjusts based on the count of the old row.
  This paper introduces a RowHammer protection index called Maximum Disturbance, which measures the maximum accumulated number of row activations within an observation period. The experimental results demonstrate that DSAC can achieve 133x lower Maximum Disturbance than the state-of-the-art counter-based algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.03591v3</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seungki Hong, Dongha Kim, Jaehyung Lee, Reum Oh, Changsik Yoo, Sangjoon Hwang, Jooyoung Lee</dc:creator>
    </item>
    <item>
      <title>Quantum Register Machine: Efficient Implementation of Quantum Recursive Programs</title>
      <link>https://arxiv.org/abs/2408.10054</link>
      <description>arXiv:2408.10054v3 Announce Type: replace-cross 
Abstract: Quantum recursive programming has been recently introduced for describing sophisticated and complicated quantum algorithms in a compact and elegant way. However, implementation of quantum recursion involves intricate interplay between quantum control flow and recursive procedure calls. In this paper, we aim at resolving this fundamental challenge and develop a series of techniques to efficiently implement quantum recursive programs. Our main contributions include:
  1. We propose a notion of quantum register machine, the first quantum architecture (including an instruction set) that provides instruction-level support for quantum control flow and recursive procedure calls at the same time.
  2. Based on quantum register machine, we describe the first comprehensive implementation process of quantum recursive programs, including the compilation, the partial evaluation of quantum control flow, and the execution on the quantum register machine.
  3. As a bonus, our efficient implementation of quantum recursive programs also offers automatic parallelisation of quantum algorithms. For implementing certain quantum algorithmic subroutine, like the widely used quantum multiplexor, we can even obtain exponential parallel speed-up (over the straightforward implementation) from this automatic parallelisation. This demonstrates that quantum recursive programming can be win-win for both modularity of programs and efficiency of their implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10054v3</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>cs.PL</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3729283</arxiv:DOI>
      <dc:creator>Zhicheng Zhang, Mingsheng Ying</dc:creator>
    </item>
    <item>
      <title>PHOENIX: Pauli-Based High-Level Optimization Engine for Instruction Execution on NISQ Devices</title>
      <link>https://arxiv.org/abs/2504.03529</link>
      <description>arXiv:2504.03529v4 Announce Type: replace-cross 
Abstract: Variational quantum algorithms (VQA) based on Hamiltonian simulation represent a specialized class of quantum programs well-suited for near-term quantum computing applications due to its modest resource requirements in terms of qubits and circuit depth. Unlike the conventional single-qubit (1Q) and two-qubit (2Q) gate sequence representation, Hamiltonian simulation programs are essentially composed of disciplined subroutines known as Pauli exponentiations (Pauli strings with coefficients) that are variably arranged. To capitalize on these distinct program features, this study introduces PHOENIX, a highly effective compilation framework that primarily operates at the high-level Pauli-based intermediate representation (IR) for generic Hamiltonian simulation programs. PHOENIX exploits global program optimization opportunities to the greatest extent, compared to existing SOTA methods despite some of them also utilizing similar IRs. Experimental results demonstrate that PHOENIX outperforms SOTA VQA compilers across diverse program categories, backend ISAs, and hardware topologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03529v4</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>cs.PL</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaohui Yang, Dawei Ding, Chenghong Zhu, Jianxin Chen, Yuan Xie</dc:creator>
    </item>
    <item>
      <title>COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning</title>
      <link>https://arxiv.org/abs/2506.00424</link>
      <description>arXiv:2506.00424v2 Announce Type: replace-cross 
Abstract: Sparse tensor programs are essential in deep learning and graph analytics, driving the need for optimized processing. To meet this demand, specialized hardware accelerators are being developed. Optimizing these programs for accelerators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffective for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general-purpose hardware (e.g., CPUs) to train cost models, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5% of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE outperforms existing techniques, achieving average speedups of 1.47x (up to 5.46x) for SpMM and 1.39x (up to 4.22x) for SDDMM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00424v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chamika Sudusinghe, Gerasimos Gerogiannis, Damitha Lenadora, Charles Block, Josep Torrellas, Charith Mendis</dc:creator>
    </item>
    <item>
      <title>Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation</title>
      <link>https://arxiv.org/abs/2506.11105</link>
      <description>arXiv:2506.11105v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have significant impact on the healthcare scenarios but remain prohibitively large for deployment in real-time, resource-constrained environments such as edge devices. In this work, we introduce a novel medical assistant system, optimized through our general-purpose compression framework, which tailors Large Language Models (LLMs) for deployment in specialized domains. By measuring neuron saliency on domain-specific data, our method can aggressively prune irrelevant neurons, reducing model size while preserving performance. Following pruning, we apply post-training quantization to further reduce the memory footprint, and evaluate the compressed model across medical benchmarks including MedMCQA, MedQA, and PubMedQA. We also deploy the 50\% compressed Gemma and the 67\% compressed LLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak), achieving real-time, energy-efficient inference under hardware constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11105v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Uttej Kallakurik, Edward Humes, Rithvik Jonna, Xiaomin Lin, Tinoosh Mohsenin</dc:creator>
    </item>
  </channel>
</rss>
