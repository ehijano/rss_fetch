<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 04:01:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Debiased Kernel Estimation of Spot Volatility in the Presence of Infinite Variation Jumps</title>
      <link>https://arxiv.org/abs/2510.14285</link>
      <description>arXiv:2510.14285v1 Announce Type: new 
Abstract: Volatility estimation is a central problem in financial econometrics, but becomes particularly challenging when jump activity is high, a phenomenon observed empirically in highly traded financial securities. In this paper, we revisit the problem of spot volatility estimation for an It\^o semimartingale with jumps of unbounded variation. We construct truncated kernel-based estimators and debiased variants that extend the efficiency frontier for spot volatility estimation in terms of the jump activity index $Y$, raising the previous bound $Y&lt;4/3$ to $Y&lt;20/11$, thereby covering nearly the entire admissible range $Y&lt;2$. Compared with earlier work, our approach attains smaller asymptotic variances through the use of unbounded kernels, is simpler to implement, and has broader applicability under more flexible model assumptions. A comprehensive simulation study confirms that our procedures substantially outperform competing methods in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14285v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>B. Cooper Boniece, Jos\'e E. Figueroa-L\'opez, Tianwei Zhou</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effect Boundaries: A Continuous Functional Framework from Navier-Stokes Equations</title>
      <link>https://arxiv.org/abs/2510.14409</link>
      <description>arXiv:2510.14409v1 Announce Type: new 
Abstract: I develop a comprehensive theoretical framework for dynamic spatial treatment effect boundaries using continuous functional definitions grounded in Navier-Stokes partial differential equations. Rather than discrete treatment effect estimators, the framework characterizes treatment intensity as a continuous function $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of propagation dynamics, boundary evolution, and cumulative exposure patterns. Building on exact self-similar solutions expressible through Kummer confluent hypergeometric and modified Bessel functions, I establish that treatment effects follow scaling laws $\tau(d, t) = t^{-\alpha} f(d/t^\beta)$ where exponents characterize diffusion mechanisms. Empirical validation using 42 million TROPOMI satellite observations of NO$_2$ pollution from U.S. coal-fired power plants demonstrates strong exponential spatial decay ($\kappa_s = 0.004$ per km, $R^2 = 0.35$) with detectable boundaries at 572 km. Monte Carlo simulations confirm superior performance over discrete parametric methods in boundary detection and false positive avoidance (94\% vs 27\% correct rejection). Regional heterogeneity analysis validates diagnostic capability: positive decay parameters within 100 km confirm coal plant dominance; negative parameters beyond 100 km correctly signal when urban sources dominate. The continuous functional perspective unifies spatial econometrics with mathematical physics, providing theoretically grounded methods for boundary detection, exposure quantification, and policy evaluation across environmental economics, banking, and healthcare applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14409v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Evaluating Policy Effects under Network Interference without Network Information: A Transfer Learning Approach</title>
      <link>https://arxiv.org/abs/2510.14415</link>
      <description>arXiv:2510.14415v1 Announce Type: cross 
Abstract: This paper develops a sensitivity analysis framework that transfers the average total treatment effect (ATTE) from source data with a fully observed network to target data whose network is completely unknown. The ATTE represents the average social impact of a policy that assigns the treatment to every individual in the dataset. We postulate a covariate-shift type assumption that both source and target datasets share the same conditional mean outcome. However, because the target network is unobserved, this assumption alone is not sufficient to pin down the ATTE for the target data. To address this issue, we consider a sensitivity analysis based on the uncertainty of the target network's degree distribution, where the extent of uncertainty is measured by the Wasserstein distance from a given reference degree distribution. We then construct bounds on the target ATTE using a linear programming-based estimator. The limiting distribution of the bound estimator is derived via the functional delta method, and we develop a wild bootstrap approach to approximate the distribution. As an empirical illustration, we revisit the social network experiment on farmers' weather insurance adoption in China by Cai et al. (2015).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14415v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tadao Hoshino</dc:creator>
    </item>
    <item>
      <title>Regression Model Selection Under General Conditions</title>
      <link>https://arxiv.org/abs/2510.14822</link>
      <description>arXiv:2510.14822v1 Announce Type: cross 
Abstract: Model selection criteria are one of the most important tools in statistics. Proofs showing a model selection criterion is asymptotically optimal are tailored to the type of model (linear regression, quantile regression, penalized regression, etc.), the estimation method (linear smoothers, maximum likelihood, generalized method of moments, etc.), the type of data (i.i.d., dependent, high dimensional, etc.), and the type of model selection criterion. Moreover, assumptions are often restrictive and unrealistic making it a slow and winding process for researchers to determine if a model selection criterion is selecting an optimal model. This paper provides general proofs showing asymptotic optimality for a wide range of model selection criteria under general conditions. This paper not only asymptotically justifies model selection criteria for most situations, but it also unifies and extends a range of previously disparate results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14822v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amaze Lusompa</dc:creator>
    </item>
    <item>
      <title>Factor-augmented sparse MIDAS regressions with an application to nowcasting</title>
      <link>https://arxiv.org/abs/2306.13362</link>
      <description>arXiv:2306.13362v4 Announce Type: replace 
Abstract: This article investigates factor-augmented sparse MIDAS (Mixed Data Sampling) regressions for high-dimensional time series data, which may be observed at different frequencies. Our novel approach integrates sparse and dense dimensionality reduction techniques. We derive the convergence rate of our estimator under misspecification due to the MIDAS approximation error, $\tau$-mixing dependence, and polynomial tails. Our method's finite sample performance is assessed via Monte Carlo simulations. We apply the methodology to nowcasting U.S. GDP growth and demonstrate that it outperforms both sparse regression and standard factor-augmented regression during the COVID-19 pandemic. These findings indicate that the growth through this period was influenced by both idiosyncratic (sparse) and common (dense) shocks. The approach is implemented in the midasml R package, available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13362v4</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jad Beyhum, Jonas Striaukas</dc:creator>
    </item>
    <item>
      <title>Treatment Effects Inference with High-Dimensional Instruments and Control Variables</title>
      <link>https://arxiv.org/abs/2503.20149</link>
      <description>arXiv:2503.20149v2 Announce Type: replace 
Abstract: Obtaining valid treatment effect inference remains a challenging problem when dealing with numerous instruments and non-sparse control variables. In this paper, we propose a novel ridge regularization-based instrumental variables method for estimation and inference in the presence of both high-dimensional instrumental variables and high-dimensional control variables. These methods are applicable both with and without sparsity assumptions. To remove the estimation bias, we introduce a two-step procedure employing a ridge regression coupled with data-splitting in the first step, and a ridge style projection matrix with a simple least squares regression in the second. We establish statistical properties of the estimator, including consistency and asymptotic normality. Furthermore, we develop practical statistical inference procedures by providing a consistent estimator for the asymptotic variance of the estimator. The finite sample performance of the proposed methods is evaluated through numerical simulations. Results indicate that the new estimator consistently outperforms existing sparsity-based approaches across various settings, offering valuable insights for complex scenarios. Finally, we provide an empirical application estimating the causal effect of schooling on earnings addressing potential endogeneity through the use of high-dimensional instrumental variables and high-dimensional covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20149v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiduo Chen, Xingdong Feng, Antonio F. Galvao, Yeheng Ge</dc:creator>
    </item>
    <item>
      <title>Causal Inference under Interference through Designed Markets</title>
      <link>https://arxiv.org/abs/2504.07217</link>
      <description>arXiv:2504.07217v2 Announce Type: replace 
Abstract: In auction and matching markets, estimating the welfare effects of demand-side treatments is challenging because of spillovers through the mechanism. We develop a quasi-experimental approach that avoids parametric assumptions typically imposed by structural methods. For a class of strategy-proof "cutoff" mechanisms, we propose an estimator that runs a weighted and perturbed version of the mechanism on data from a single market. The estimator is semi-parametrically efficient, asymptotically normal, and robust to a wide class of demand-side specifications. We propose spillover-aware targeting rules with vanishing asymptotic regret. Empirically, spillovers diminish the effect of information on inequality in Chilean schools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07217v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Munro</dc:creator>
    </item>
    <item>
      <title>When is p-hacking detectable?</title>
      <link>https://arxiv.org/abs/2506.20035</link>
      <description>arXiv:2506.20035v2 Announce Type: replace 
Abstract: Some forms of p-hacking cannot be detected by examining the t-curve (or p-curve). Standard tests may also fail to find even detectable forms of selective reporting. We propose a novel test that is consistent against every detectable form of p-hacking and remains interpretable even when the t-scores are not exactly normal. The test statistic is the distance between the smoothed empirical t-curve and the set of all distributions that would be possible in the absence of any selective reporting. This novel projection test can only be evaded in large meta-samples by selective reporting that also evades all other valid tests of restrictions on the t-curve. A second benefit of the projection test is that under the null hypothesis of no p-hacking we can check whether the projection residual could have been produced by other distortions not related to selective reporting, e.g. rounding and de-rounding. Applying the test to the Brodeur et al. (2020) meta-data, we find that the t-curves for RCTs, IVs, and DIDs are more distorted than could arise by chance. We confirm that these distortions cannot be explained by (de)rounding of t-scores or by the limited degrees of freedom of the underlying studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20035v2</guid>
      <category>econ.EM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Faridani</dc:creator>
    </item>
    <item>
      <title>Regularizing Extrapolation in Causal Inference</title>
      <link>https://arxiv.org/abs/2509.17180</link>
      <description>arXiv:2509.17180v2 Announce Type: replace-cross 
Abstract: Many common estimators in machine learning and causal inference are linear smoothers, where the prediction is a weighted average of the training outcomes. Some estimators, such as ordinary least squares and kernel ridge regression, allow for arbitrarily negative weights, which improve feature imbalance but often at the cost of increased dependence on parametric modeling assumptions and higher variance. By contrast, estimators like importance weighting and random forests (sometimes implicitly) restrict weights to be non-negative, reducing dependence on parametric modeling and variance at the cost of worse imbalance. In this paper, we propose a unified framework that directly penalizes the level of extrapolation, replacing the current practice of a hard non-negativity constraint with a soft constraint and corresponding hyperparameter. We derive a worst-case extrapolation error bound and introduce a novel "bias-bias-variance" tradeoff, encompassing biases due to feature imbalance, model misspecification, and estimator variance; this tradeoff is especially pronounced in high dimensions, particularly when positivity is poor. We then develop an optimization procedure that regularizes this bound while minimizing imbalance and outline how to use this approach as a sensitivity analysis for dependence on parametric modeling assumptions. We demonstrate the effectiveness of our approach through synthetic experiments and a real-world application, involving the generalization of randomized controlled trial estimates to a target population of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17180v2</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Arbour, Harsh Parikh, Bijan Niknam, Elizabeth Stuart, Kara Rudolph, Avi Feller</dc:creator>
    </item>
    <item>
      <title>Ranking Policies Under Loss Aversion and Inequality Aversion</title>
      <link>https://arxiv.org/abs/2510.09590</link>
      <description>arXiv:2510.09590v2 Announce Type: replace-cross 
Abstract: Strong empirical evidence from laboratory experiments, and more recently from population surveys, shows that individuals, when evaluating their situations, pay attention to whether they experience gains or losses, with losses weighing more heavily than gains. The electorate's loss aversion, in turn, influences politicians' choices. We propose a new framework for welfare analysis of policy outcomes that, in addition to the traditional focus on post-policy incomes, also accounts for individuals' gains and losses resulting from policies. We develop several bivariate stochastic dominance criteria for ranking policy outcomes that are sensitive to features of the joint distribution of individuals' income changes and absolute incomes. The main social objective assumes that individuals are loss averse with respect to income gains and losses, inequality averse with respect to absolute incomes, and hold varying preferences regarding the association between incomes and income changes. We translate these and other preferences into functional inequalities that can be tested using sample data. The concepts and methods are illustrated using data from an income support experiment conducted in Connecticut.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09590v2</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Martyna Kobus, Rados{\l}aw Kurek, Thomas Parker</dc:creator>
    </item>
  </channel>
</rss>
