<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Oct 2025 01:51:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Bayesian Characterization of Ensemble Kalman Updates</title>
      <link>https://arxiv.org/abs/2510.00158</link>
      <description>arXiv:2510.00158v1 Announce Type: new 
Abstract: The update in the Ensemble Kalman Filter (EnKF), called the Ensemble Kalman Update (EnKU), is widely used for Bayesian inference in inverse problems and data assimilation. At each filtering step, it approximates the solution to a likelihood-free Bayesian inversion from an ensemble of particles $(X_i,Y_i)\sim\pi$ sampled from a joint measure $\pi$ and an observation $y_\ast\in\mathbb{R}^m$. The posterior ${\pi}_{X|Y=y_\ast}$ is approximated by transporting $(X_i,Y_i)$ through an affine map $L^{\mathrm{EnKU}}_{y_\ast}(x,y)$ determined by the Kalman gain. While the EnKU is exact for Gaussian joints $\pi$ in the mean-field limit, exactness alone does not fix the update: infinitely many affine maps $L_{y_\ast}$ push a Gaussian $\pi$ to $\pi_{X|Y=y_\ast}$. This raises a question: which affine map should estimate the posterior? We provide a characterization of the EnKU among all such maps. First, we describe the set $\mathrm{E}^{\mathrm{EnKU}}$ of laws where the EnKU yields exact conditioning, showing it is larger than the Gaussian family. Next, we prove that, except for a small class of highly symmetric distributions in $\mathrm{E}^{\mathrm{EnKU}}$ (including Gaussians), the EnKU is the unique exact affine conditioning map. Finally, we ask for the largest possible set $\mathrm{F}$ where any measure-dependent affine transport could be exact; after characterizing $\mathrm{F}$, we show the EnKU's exactness set is almost maximal: $\mathrm{F}=\mathrm{E}^{\mathrm{EnKU}}\cup\mathrm{S}_{\mathrm{nl-dec}}$, where $\mathrm{S}_{\mathrm{nl-dec}}$ is a small symmetry class. Thus, among affine transports, the EnKU is near-optimal for exact conditioning beyond Gaussians and is the unique affine update achieving exactness for any measure in $\mathrm{F}$ except a subclass of strongly symmetric laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00158v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederic J. N. Jorgensen, Youssef M. Marzouk</dc:creator>
    </item>
    <item>
      <title>Zero variance self-normalized importance sampling via estimating equations</title>
      <link>https://arxiv.org/abs/2510.00389</link>
      <description>arXiv:2510.00389v1 Announce Type: new 
Abstract: In ordinary importance sampling with a nonnegative integrand there exists an importance sampling strategy with zero variance. Practical sampling strategies are often based on approximating that optimal solution, potentially approaching zero variance. There is a positivisation extension of that method to handle integrands that take both positive and negative values. Self-normalized importance sampling uses a ratio estimate, for which the optimal sampler does not have zero variance and so zero variance cannot even be approached in practice. Strategies that separately estimate the numerator and denominator of that ratio can approach zero variance. This paper develops another zero variance solution for self-normalized importance sampling. The first step is to write the desired expectation as the zero of an estimating equation using Fieller's technique. Then we apply the positivisation strategy to the estimating equation. This paper give conditions for existence and uniqueness of the sample solution to the estimating equation. Then it give conditions for consistency and asymptotic normality and an expression for the asymptotic variance. The sample size multiplied by the variance of the asymptotic formula becomes arbitrarily close to zero for certain sampling strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00389v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Art B. Owen</dc:creator>
    </item>
    <item>
      <title>Stable Phase Retrieval: Optimal Rates in Poisson and Heavy-tailed Models</title>
      <link>https://arxiv.org/abs/2510.00551</link>
      <description>arXiv:2510.00551v1 Announce Type: new 
Abstract: We investigate stable recovery guarantees for phase retrieval under two realistic and challenging noise models: the Poisson model and the heavy-tailed model. Our analysis covers both nonconvex least squares (NCVX-LS) and convex least squares (CVX-LS) estimators. For the Poisson model, we demonstrate that in the high-energy regime where the true signal $pmb{x}$ exceeds a certain energy threshold, both estimators achieve a signal-independent, minimax optimal error rate $\mathcal{O}(\sqrt{\frac{n}{m}})$, with $n$ denoting the signal dimension and $m$ the number of sampling vectors. In contrast, in the low-energy regime, the NCVX-LS estimator attains an error rate of $\mathcal{O}(\|\pmb{x}\|^{1/4}_2\cdot(\frac{n}{m})^{1/4})$, which decreases as the energy of signal $\pmb{x}$ diminishes and remains nearly optimal with respect to the oversampling ratio. This demonstrates a signal-energy-adaptive behavior in the Poisson setting. For the heavy-tailed model with noise having a finite $q$-th moment ($q&gt;2$), both estimators attain the minimax optimal error rate $\mathcal{O}( \frac{\| \xi \|_{L_q}}{\| \pmb{x} \|_2} \cdot \sqrt{\frac{n}{m}} )$ in the high-energy regime, while the NCVX-LS estimator further achieves the minimax optimal rate $\mathcal{O}( \sqrt{\|\xi \|_{L_q}}\cdot (\frac{n}{m})^{1/4} )$ in the low-energy regime. Our analysis builds on two key ideas: the use of multiplier inequalities to handle noise that may exhibit dependence on the sampling vectors, and a novel interpretation of Poisson noise as sub-exponential in the high-energy regime yet heavy-tailed in the low-energy regime. These insights form the foundation of a unified analytical framework, which we further apply to a range of related problems, including sparse phase retrieval, low-rank PSD matrix recovery, and random blind deconvolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00551v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Gao Huang, Song Li, Deanna Needell</dc:creator>
    </item>
    <item>
      <title>Mathematical Theory of Collinearity Effects on Machine Learning Variable Importance Measures</title>
      <link>https://arxiv.org/abs/2510.00557</link>
      <description>arXiv:2510.00557v1 Announce Type: new 
Abstract: In many machine learning problems, understanding variable importance is a central concern. Two common approaches are Permute-and-Predict (PaP), which randomly permutes a feature in a validation set, and Leave-One-Covariate-Out (LOCO), which retrains models after permuting a training feature. Both methods deem a variable important if predictions with the original data substantially outperform those with permutations. In linear regression, empirical studies have linked PaP to regression coefficients and LOCO to $t$-statistics, but a formal theory has been lacking. We derive closed-form expressions for both measures, expressed using square-root transformations. PaP is shown to be proportional to the coefficient and predictor variability: $\text{PaP}_i = \beta_i \sqrt{2\operatorname{Var}(\mathbf{x}^v_i)}$, while LOCO is proportional to the coefficient but dampened by collinearity (captured by $\Delta$): $\text{LOCO}_i = \beta_i (1 -\Delta)\sqrt{1 + c}$. These derivations explain why PaP is largely unaffected by multicollinearity, whereas LOCO is highly sensitive to it. Monte Carlo simulations confirm these findings across varying levels of collinearity. Although derived for linear regression, we also show that these results provide reasonable approximations for models like Random Forests. Overall, this work establishes a theoretical basis for two widely used importance measures, helping analysts understand how they are affected by the true coefficients, dimension, and covariance structure. This work bridges empirical evidence and theory, enhancing the interpretability and application of variable importance measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00557v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelvyn K. Bladen, D. Richard Cutler, Alan Wisler</dc:creator>
    </item>
    <item>
      <title>Quantifying the noise sensitivity of the Wasserstein metric for images</title>
      <link>https://arxiv.org/abs/2510.01015</link>
      <description>arXiv:2510.01015v1 Announce Type: new 
Abstract: Wasserstein metrics are increasingly being used as similarity scores for images treated as discrete measures on a grid, yet their behavior under noise remains poorly understood. In this work, we consider the sensitivity of the signed Wasserstein distance with respect to pixel-wise additive noise and derive non-asymptotic upper bounds. Among other results, we prove that the error in the signed 2-Wasserstein distance scales with the square root of the noise standard deviation, whereas the Euclidean norm scales linearly. We present experiments that support our theoretical findings and point to a peculiar phenomenon where increasing the level of noise can decrease the Wasserstein distance. A case study on cryo-electron microscopy images demonstrates that the Wasserstein metric can preserve the geometric structure even when the Euclidean metric fails to do so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01015v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik Lager, Gilles Mordant, Amit Moscovich</dc:creator>
    </item>
    <item>
      <title>Identifying All {\epsilon}-Best Arms in (Misspecified) Linear Bandits</title>
      <link>https://arxiv.org/abs/2510.00073</link>
      <description>arXiv:2510.00073v1 Announce Type: cross 
Abstract: Motivated by the need to efficiently identify multiple candidates in high trial-and-error cost tasks such as drug discovery, we propose a near-optimal algorithm to identify all {\epsilon}-best arms (i.e., those at most {\epsilon} worse than the optimum). Specifically, we introduce LinFACT, an algorithm designed to optimize the identification of all {\epsilon}-best arms in linear bandits. We establish a novel information-theoretic lower bound on the sample complexity of this problem and demonstrate that LinFACT achieves instance optimality by matching this lower bound up to a logarithmic factor. A key ingredient of our proof is to integrate the lower bound directly into the scaling process for upper bound derivation, determining the termination round and thus the sample complexity. We also extend our analysis to settings with model misspecification and generalized linear models. Numerical experiments, including synthetic and real drug discovery data, demonstrate that LinFACT identifies more promising candidates with reduced sample complexity, offering significant computational efficiency and accelerating early-stage exploratory experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00073v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhekai Li, Tianyi Ma, Cheng Hua, Ruihao Zhu</dc:creator>
    </item>
    <item>
      <title>Assumption-lean Inference for Network-linked Data</title>
      <link>https://arxiv.org/abs/2510.00287</link>
      <description>arXiv:2510.00287v1 Announce Type: cross 
Abstract: We consider statistical inference for network-linked regression problems, where covariates may include network summary statistics computed for each node. In settings involving network data, it is often natural to posit that latent variables govern connection probabilities in the graph. Since the presence of these latent features makes classical regression assumptions even less tenable, we propose an assumption-lean framework for linear regression with jointly exchangeable regression arrays. We establish an analog of the Aldous-Hoover representation for such arrays, which may be of independent interest. Moreover, we consider two different projection parameters as potential targets and establish conditions under which asymptotic normality and bootstrap consistency hold when commonly used network statistics, including local subgraph frequencies and spectral embeddings, are used as covariates. In the case of linear regression with local count statistics, we show that a bias-corrected estimator allows one to target a more natural inferential target under weaker sparsity conditions compared to the OLS estimator. Our inferential tools are illustrated using both simulated data and real data related to the academic climate of elementary schools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00287v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Nilanjan Chakraborty, Robert Lunde</dc:creator>
    </item>
    <item>
      <title>CINDES: Classification induced neural density estimator and simulator</title>
      <link>https://arxiv.org/abs/2510.00367</link>
      <description>arXiv:2510.00367v1 Announce Type: cross 
Abstract: Neural network-based methods for (un)conditional density estimation have recently gained substantial attention, as various neural density estimators have outperformed classical approaches in real-data experiments. Despite these empirical successes, implementation can be challenging due to the need to ensure non-negativity and unit-mass constraints, and theoretical understanding remains limited. In particular, it is unclear whether such estimators can adaptively achieve faster convergence rates when the underlying density exhibits a low-dimensional structure. This paper addresses these gaps by proposing a structure-agnostic neural density estimator that is (i) straightforward to implement and (ii) provably adaptive, attaining faster rates when the true density admits a low-dimensional composition structure. Another key contribution of our work is to show that the proposed estimator integrates naturally into generative sampling pipelines, most notably score-based diffusion models, where it achieves provably faster convergence when the underlying density is structured. We validate its performance through extensive simulations and a real-data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00367v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dehao Dai, Jianqing Fan, Yihong Gu, Debarghya Mukherjee</dc:creator>
    </item>
    <item>
      <title>Guaranteed Noisy CP Tensor Recovery via Riemannian Optimization on the Segre Manifold</title>
      <link>https://arxiv.org/abs/2510.00569</link>
      <description>arXiv:2510.00569v1 Announce Type: cross 
Abstract: Recovering a low-CP-rank tensor from noisy linear measurements is a central challenge in high-dimensional data analysis, with applications spanning tensor PCA, tensor regression, and beyond. We exploit the intrinsic geometry of rank-one tensors by casting the recovery task as an optimization problem over the Segre manifold, the smooth Riemannian manifold of rank-one tensors. This geometric viewpoint yields two powerful algorithms: Riemannian Gradient Descent (RGD) and Riemannian Gauss-Newton (RGN), each of which preserves feasibility at every iteration. Under mild noise assumptions, we prove that RGD converges at a local linear rate, while RGN exhibits an initial local quadratic convergence phase that transitions to a linear rate as the iterates approach the statistical noise floor. Extensive synthetic experiments validate these convergence guarantees and demonstrate the practical effectiveness of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00569v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Xu, Yuefeng Han</dc:creator>
    </item>
    <item>
      <title>A Weighted Regression Approach to Break-Point Detection in Panel Data</title>
      <link>https://arxiv.org/abs/2510.00598</link>
      <description>arXiv:2510.00598v1 Announce Type: cross 
Abstract: New procedures for detecting a change in the cross-sectional mean of panel data are proposed. The procedures rely on estimating nuisance parameters using certain cross-sectional means across panels using a weighted least squares regression. In the case of weak cross-sectional dependence between panels, we show how test statistics can be constructed to have a limit null distribution not depending on any choice of bandwidths typically needed to estimate the long-run variances of the panel errors. The theoretical assertions are derived for general choices of the regression weights, and it is shown that consistent test procedures can be obtained from the proposed process. The theoretical results are extended to the case where strong cross-sectional dependence exist between panels. The paper concludes with a numerical study illustrating the behavior of several special cases of the test procedure in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00598v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Charl Pretorius, Heinrich Roodt</dc:creator>
    </item>
    <item>
      <title>The Good, the Bad, and the Sampled: a No-Regret Approach to Safe Online Classification</title>
      <link>https://arxiv.org/abs/2510.01020</link>
      <description>arXiv:2510.01020v1 Announce Type: cross 
Abstract: We study the problem of sequentially testing individuals for a binary disease outcome whose true risk is governed by an unknown logistic model. At each round, a patient arrives with feature vector $x_t$, and the decision maker may either pay to administer a (noiseless) diagnostic test--revealing the true label--or skip testing and predict the patient's disease status based on their feature vector and prior history. Our goal is to minimize the total number of costly tests required while guaranteeing that the fraction of misclassifications does not exceed a prespecified error tolerance $\alpha$, with probability at least $1-\delta$. To address this, we develop a novel algorithm that interleaves label-collection and distribution estimation to estimate both $\theta^{*}$ and the context distribution $P$, and computes a conservative, data-driven threshold $\tau_t$ on the logistic score $|x_t^\top\theta|$ to decide when testing is necessary. We prove that, with probability at least $1-\delta$, our procedure does not exceed the target misclassification rate, and requires only $O(\sqrt{T})$ excess tests compared to the oracle baseline that knows both $\theta^{*}$ and the patient feature distribution $P$. This establishes the first no-regret guarantees for error-constrained logistic testing, with direct applications to cost-sensitive medical screening. Simulations corroborate our theoretical guarantees, showing that in practice our procedure efficiently estimates $\theta^{*}$ while retaining safety guarantees, and does not require too many excess tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01020v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tavor Z. Baharav, Spyros Dragazis, Aldo Pacchiano</dc:creator>
    </item>
    <item>
      <title>Random Matrices, Intrinsic Freeness, and Sharp Non-Asymptotic Inequalities</title>
      <link>https://arxiv.org/abs/2510.01021</link>
      <description>arXiv:2510.01021v1 Announce Type: cross 
Abstract: Random matrix theory has played a major role in several areas of pure and applied mathematics, as well as statistics, physics, and computer science. This lecture aims to describe the intrinsic freeness phenomenon and how it provides new easy-to-use sharp non-asymptotic bounds on the spectrum of general random matrices. We will also present a couple of illustrative applications in high dimensional statistical inference. This article accompanies a lecture that will be given by the author at the International Congress of Mathematicians in Philadelphia in the Summer of 2026.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01021v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Afonso S. Bandeira</dc:creator>
    </item>
    <item>
      <title>Generalized Bayes in Conditional Moment Restriction Models</title>
      <link>https://arxiv.org/abs/2510.01036</link>
      <description>arXiv:2510.01036v1 Announce Type: cross 
Abstract: This paper develops a generalized (quasi-) Bayes framework for conditional moment restriction models, where the parameter of interest is a nonparametric structural function of endogenous variables. We establish contraction rates for a class of Gaussian process priors and provide conditions under which a Bernstein-von Mises theorem holds for the quasi-Bayes posterior. Consequently, we show that optimally weighted quasi-Bayes credible sets achieve exact asymptotic frequentist coverage, extending classical results for parametric GMM models. As an application, we estimate firm-level production functions using Chilean plant-level data. Simulations illustrate the favorable performance of generalized Bayes estimators relative to common alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01036v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sid Kankanala</dc:creator>
    </item>
    <item>
      <title>Learning linear dynamical systems under convex constraints</title>
      <link>https://arxiv.org/abs/2303.15121</link>
      <description>arXiv:2303.15121v4 Announce Type: replace 
Abstract: We consider the problem of finite-time identification of linear dynamical systems from $T$ samples of a single trajectory. Recent results have predominantly focused on the setup where either no structural assumption is made on the system matrix $A^* \in \mathbb{R}^{n \times n}$, or specific structural assumptions (e.g. sparsity) are made on $A^*$. We assume prior structural information on $A^*$ is available, which can be captured in the form of a convex set $\mathcal{K}$ containing $A^*$. For the solution of the ensuing constrained least squares estimator, we derive non-asymptotic error bounds in the Frobenius norm that depend on the local size of $\mathcal{K}$ at $A^*$. To illustrate the usefulness of these results, we instantiate them for four examples, namely when (i) $A^*$ is sparse and $\mathcal{K}$ is a suitably scaled $\ell_1$ ball; (ii) $\mathcal{K}$ is a subspace; (iii) $\mathcal{K}$ consists of matrices each of which is formed by sampling a bivariate convex function on a uniform $n \times n$ grid (convex regression); (iv) $\mathcal{K}$ consists of matrices each row of which is formed by uniform sampling (with step size $1/T$) of a univariate Lipschitz function. In all these situations, we show that $A^*$ can be reliably estimated for values of $T$ much smaller than what is needed for the unconstrained setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.15121v4</guid>
      <category>math.ST</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hemant Tyagi, Denis Efimov</dc:creator>
    </item>
    <item>
      <title>Extremal correlation coefficient for functional data</title>
      <link>https://arxiv.org/abs/2405.17318</link>
      <description>arXiv:2405.17318v3 Announce Type: replace 
Abstract: We propose a coefficient that measures dependence in paired samples of functions. It has properties similar to the Pearson correlation, but differs in significant ways: (i) it is designed to measure dependence between curves, (ii) it focuses only on extreme curves. The new coefficient is derived within the framework of regular variation in Banach spaces. A consistent estimator is proposed and justified by an asymptotic analysis and a simulation study. The usefulness of the new coefficient is illustrated on financial and and climate functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17318v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mihyun Kim, Piotr Kokoszka</dc:creator>
    </item>
    <item>
      <title>A Likelihood Based Approach to Distribution Regression Using Conditional Deep Generative Models</title>
      <link>https://arxiv.org/abs/2410.02025</link>
      <description>arXiv:2410.02025v2 Announce Type: replace 
Abstract: In this work, we explore the theoretical properties of conditional deep generative models under the statistical framework of distribution regression where the response variable lies in a high-dimensional ambient space but concentrates around a potentially lower-dimensional manifold. More specifically, we study the large-sample properties of a likelihood-based approach for estimating these models. Our results lead to the convergence rate of a sieve maximum likelihood estimator (MLE) for estimating the conditional distribution (and its devolved counterpart) of the response given predictors in the Hellinger (Wasserstein) metric. Our rates depend solely on the intrinsic dimension and smoothness of the true conditional distribution. These findings provide an explanation of why conditional deep generative models can circumvent the curse of dimensionality from the perspective of statistical foundations and demonstrate that they can learn a broader class of nearly singular conditional distributions. Our analysis also emphasizes the importance of introducing a small noise perturbation to the data when they are supported sufficiently close to a manifold. Finally, in our numerical studies, we demonstrate the effective implementation of the proposed approach using both synthetic and real-world datasets, which also provide complementary validation to our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02025v2</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shivam Kumar, Yun Yang, Lizhen Lin</dc:creator>
    </item>
    <item>
      <title>Performance of the empirical median for location estimation in heteroscedastic settings</title>
      <link>https://arxiv.org/abs/2501.16956</link>
      <description>arXiv:2501.16956v3 Announce Type: replace 
Abstract: We investigate the performance of the empirical median for location estimation in heteroscedastic settings. Specifically, we consider independent symmetric real-valued random variables that share a common but unknown location parameter while having different and unknown scale parameters. Estimation under heteroscedasticity arises naturally in many practical situations and has recently attracted considerable attention. In this work, we analyze the empirical median as an estimator of the common location parameter and derive matching non-asymptotic upper and lower bounds on its estimation error. These results fully characterize the behavior of the empirical median in heteroscedastic settings, clarifying both its robustness and its intrinsic limitations and offering a precise understanding of its performance in modern settings where data quality may vary across sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16956v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sirine Louati</dc:creator>
    </item>
    <item>
      <title>Dirichlet moment tensors and the correspondence between admixture and mixture of product models</title>
      <link>https://arxiv.org/abs/2509.25441</link>
      <description>arXiv:2509.25441v2 Announce Type: replace 
Abstract: Understanding posterior contraction behavior in Bayesian hierarchical models is of fundamental importance, but progress in this question is relatively sparse in comparison to the theory of density estimation. In this paper, we study two classes of hierarchical models for grouped data, where observations within groups are exchangeable. Using moment tensor decomposition of the distribution of the latent variables, we establish a precise equivalence between the class of Admixture models (such as Latent Dirichlet Allocation) and the class of Mixture of products of multinomial distributions. This correspondence enables us to leverage the result from the latter class of models, which are more well-understood, so as to arrive at the identifiability and posterior contraction rates in both classes under conditions much weaker than in existing literature. For instance, our results shed light on cases where the topics are not linearly independent or the number of topics is misspecified in the admixture setting. Finally, we analyze individual documents' latent allocation performance via the borrowing of strength properties of hierarchical Bayesian modeling. Many illustrations and simulations are provided to support the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25441v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dat Do, Sunrit Chakraborty, Jonathan Terhorst, XuanLong Nguyen</dc:creator>
    </item>
    <item>
      <title>On the Natural Gradient of the Evidence Lower Bound</title>
      <link>https://arxiv.org/abs/2307.11249</link>
      <description>arXiv:2307.11249v2 Announce Type: replace-cross 
Abstract: This article studies the Fisher-Rao gradient, also referred to as the natural gradient, of the evidence lower bound (ELBO) which plays a central role in generative machine learning. It reveals that the gap between the evidence and its lower bound, the ELBO, has essentially a vanishing natural gradient within unconstrained optimization. As a result, maximization of the ELBO is equivalent to minimization of the Kullback-Leibler divergence from a target distribution, the primary objective function of learning. Building on this insight, we derive a condition under which this equivalence persists even when optimization is constrained to a model. This condition yields a geometric characterization, which we formalize through the notion of a cylindrical model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11249v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nihat Ay, Jesse van Oostrum, Adwait Datar</dc:creator>
    </item>
    <item>
      <title>Asymptotic Expansions of Gaussian and Laguerre Ensembles at the Soft Edge II: Level Densities</title>
      <link>https://arxiv.org/abs/2503.12644</link>
      <description>arXiv:2503.12644v3 Announce Type: replace-cross 
Abstract: We continue our work [arXiv:2403.07628] on asymptotic expansions at the soft edge for the classical $n$-dimensional Gaussian and Laguerre random matrix ensembles. By revisiting the construction of the associated skew-orthogonal polynomials in terms of wave functions, we obtain concise expressions for the level densities that are well suited for proving asymptotic expansions in powers of a certain parameter $h \asymp n^{-2/3}$. In the unitary case, the expansion for the level density can be used to reconstruct the first correction term in an established asymptotic expansion of the associated generating function. In the orthogonal and symplectic cases, we can even reconstruct the conjectured first and second correction terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12644v3</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Folkmar Bornemann</dc:creator>
    </item>
    <item>
      <title>Sequential Monte Carlo with Gaussian Mixture Approximation for Infinite-Dimensional Statistical Inverse Problems</title>
      <link>https://arxiv.org/abs/2503.16028</link>
      <description>arXiv:2503.16028v3 Announce Type: replace-cross 
Abstract: By formulating the inverse problem of partial differential equations (PDEs) as a statistical inference problem, the Bayesian approach provides a general framework for quantifying uncertainties. In the inverse problem of PDEs, parameters are defined on an infinite-dimensional function space, and the PDEs induce a computationally intensive likelihood function. Additionally, sparse data tends to lead to a multi-modal posterior. These features make it difficult to apply existing sequential Monte Carlo (SMC) algorithms. To overcome these difficulties, we propose new conditions for the likelihood functions, construct a Gaussian mixture based preconditioned Crank-Nicolson transition kernel, and demonstrate the universal approximation property of the infinite-dimensional Gaussian mixture probability measure. By combining these three novel tools, we propose a new SMC algorithm, named SMC-GM. For this new algorithm, we obtain a convergence theorem that allows Gaussian priors, illustrating that the sequential particle filter actually reproduces the true posterior distribution. Furthermore, the proposed new algorithm is rigorously defined on the infinite-dimensional function space, naturally exhibiting the discretization-invariant property. Numerical experiments demonstrate that the new approach has a strong ability to probe the multi-modality of the posterior, significantly reduces the computational burden, and numerically exhibits the discretization-invariant property (important for large-scale problems).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16028v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyu Lu, Junxiong Jia, Deyu Meng</dc:creator>
    </item>
    <item>
      <title>Grassmann and Flag Varieties in Linear Algebra, Optimization, and Statistics: An Algebraic Perspective</title>
      <link>https://arxiv.org/abs/2505.15969</link>
      <description>arXiv:2505.15969v2 Announce Type: replace-cross 
Abstract: Grassmann and flag varieties lead many lives in pure and applied mathematics. Here we focus on the algebraic complexity of solving various problems in linear algebra and statistics as optimization problems over these varieties. The measure of the algebraic complexity is the amount of complex critical points of the corresponding optimization problem. After an exposition of different realizations of these manifolds as algebraic varieties we present a sample of optimization problems over them and we compute their algebraic complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15969v2</guid>
      <category>math.OC</category>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Friedman, Serkan Ho\c{s}ten</dc:creator>
    </item>
    <item>
      <title>Federated Causal Inference from Multi-Site Observational Data via Propensity Score Aggregation</title>
      <link>https://arxiv.org/abs/2505.17961</link>
      <description>arXiv:2505.17961v2 Announce Type: replace-cross 
Abstract: Causal inference typically assumes centralized access to individual-level data. Yet, in practice, data are often decentralized across multiple sites, making centralization infeasible due to privacy, logistical, or legal constraints. We address this problem by estimating the Average Treatment Effect (ATE) from decentralized observational data via a Federated Learning (FL) approach, allowing inference through the exchange of aggregate statistics rather than individual-level data. We propose a novel method to estimate propensity scores by computing a federated weighted average of local scores with Membership Weights (MW)--probabilities of site membership conditional on covariates--which can be flexibly estimated using parametric or non-parametric classification models. Unlike density ratio weights (DW) from the transportability and generalization literature, which either rely on strong modeling assumptions or cannot be implemented in FL, MW can be estimated using standard FL algorithms and are more robust, as they support flexible, non-parametric models--making them the preferred choice in multi-site settings with strict data-sharing constraints. The resulting propensity scores are used to construct Federated Inverse Propensity Weighting (Fed-IPW) and Augmented IPW (Fed-AIPW) estimators. Unlike meta-analysis methods, which fail when any site violates positivity, our approach leverages heterogeneity in treatment assignment across sites to improve overlap. We show that Fed-IPW and Fed-AIPW perform well under site-level heterogeneity in sample sizes, treatment mechanisms, and covariate distributions. Both theoretical analysis and experiments on simulated and real-world data highlight their advantages over meta-analysis and related methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17961v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Khellaf R\'emi, Bellet Aur\'elien, Josse Julie</dc:creator>
    </item>
    <item>
      <title>Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds</title>
      <link>https://arxiv.org/abs/2506.07614</link>
      <description>arXiv:2506.07614v4 Announce Type: replace-cross 
Abstract: We study the problem of sampling from strongly log-concave distributions over $\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the randomized midpoint method) for overdamped/underdamped Langevin dynamics. We prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic speedup in dependence on the target accuracy ($\epsilon$) over the Euler-Maruyama discretization, surpassing existing bounds for randomized midpoint methods. Notably, in the case of underdamped Langevin dynamics, we demonstrate the complexity of $W_2$ convergence is much smaller than the complexity lower bounds for convergence in $L^2$ strong error established in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07614v4</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rishikesh Srinivasan, Dheeraj Nagaraj</dc:creator>
    </item>
    <item>
      <title>Minimax and Bayes Optimal Best-Arm Identification</title>
      <link>https://arxiv.org/abs/2506.24007</link>
      <description>arXiv:2506.24007v3 Announce Type: replace-cross 
Abstract: This study investigates minimax and Bayes optimal strategies in fixed-budget best-arm identification. We consider an adaptive procedure consisting of a sampling phase followed by a recommendation phase, and we design an adaptive experiment within this framework to efficiently identify the best arm, defined as the one with the highest expected outcome. In our proposed strategy, the sampling phase consists of two stages. The first stage is a pilot phase, in which we allocate each arm uniformly in equal proportions to eliminate clearly suboptimal arms and estimate outcome variances. In the second stage, arms are allocated in proportion to the variances estimated during the first stage. After the sampling phase, the procedure enters the recommendation phase, where we select the arm with the highest sample mean as our estimate of the best arm. We prove that this single strategy is simultaneously asymptotically minimax and Bayes optimal for the simple regret, with upper bounds that coincide exactly with our lower bounds, including the constant terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24007v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Robust Estimation Under Heterogeneous Corruption Rates</title>
      <link>https://arxiv.org/abs/2508.15051</link>
      <description>arXiv:2508.15051v2 Announce Type: replace-cross 
Abstract: We study the problem of robust estimation under heterogeneous corruption rates, where each sample may be independently corrupted with a known but non-identical probability. This setting arises naturally in distributed and federated learning, crowdsourcing, and sensor networks, yet existing robust estimators typically assume uniform or worst-case corruption, ignoring structural heterogeneity. For mean estimation for multivariate bounded distributions and univariate gaussian distributions, we give tight minimax rates for all heterogeneous corruption patterns. For multivariate gaussian mean estimation and linear regression, we establish the minimax rate for squared error up to a factor of $\sqrt{d}$, where $d$ is the dimension. Roughly, our findings suggest that samples beyond a certain corruption threshold may be discarded by the optimal estimators -- this threshold is determined by the empirical distribution of the corruption rates given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15051v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Syomantak Chaudhuri, Jerry Li, Thomas A. Courtade</dc:creator>
    </item>
  </channel>
</rss>
