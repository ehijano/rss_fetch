<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Nov 2025 05:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multilevel non-linear interrupted time series analysis</title>
      <link>https://arxiv.org/abs/2511.05725</link>
      <description>arXiv:2511.05725v1 Announce Type: new 
Abstract: Recent advances in interrupted time series analysis permit characterization of a typical non-linear interruption effect through use of generalized additive models. Concurrently, advances in latent time series modeling allow efficient Bayesian multilevel time series models. We propose to combine these concepts with a hierarchical model selection prior to characterize interruption effects with a multilevel structure, encouraging parsimony and partial pooling while incorporating meaningful variability in causal effects across subpopulations of interest, while allowing poststratification. These models are demonstrated with three applications: 1) the effect of the introduction of the prostate specific antigen test on prostate cancer diagnosis rates by race and age group, 2) the change in stroke or trans-ischemic attack hospitalization rates across Medicare beneficiaries by rurality in the months after the start of the COVID-19 pandemic, and 3) the effect of Medicaid expansion in Missouri on the proportion of inpatient hospitalizations discharged with Medicaid as a primary payer by key age groupings and sex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05725v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>RJ Waken, Fengxian Wang, Sarah A. Eisenstein, Tim McBride, Kim Johnson, Karen Joynt-Maddox</dc:creator>
    </item>
    <item>
      <title>On the Development of Probabilistic Projections of Country-level Progress to the UN SDG Indicator of Minimum Proficiency in Reading and Mathematics</title>
      <link>https://arxiv.org/abs/2511.06107</link>
      <description>arXiv:2511.06107v1 Announce Type: new 
Abstract: As of this writing, there are five years remaining for countries to reach their Sustainable Development Goals deadline of 2030 as agreed to by the member countries of the United Nations. Countries are, therefore, naturally interested in projections of progress toward these goals. A variety of statistical measures have been used to report on country-level progress toward the goals, but they have not utilized methodologies explicitly designed to obtain optimally predic- tive measures of rate of progress as the foundation for projecting trends. The focus of this paper is to provide Bayesian probabilistic projections of progress to SDG indicator 4.1.1, attaining minimum proficiency in reading and mathe- matics, with particular emphasis on competencies among lower secondary school children. Using data from the OECD PISA, as well as indicators drawn from the World Bank, the OECD, UNDP, and UNESCO, we employ a novel combination of Bayesian latent growth curve modeling Bayesian model averaging to obtain optimal estimates of the rate of progress in minimum proficiency percentages and then use those estimate to develop probabilistic projections into the future over- all for all countries in the analysis. Four case study countries are also presented to show how the methods can be used for individual country projections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06107v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kaplan, Nina Jude, Kjorte Harra, Jonas Stampka</dc:creator>
    </item>
    <item>
      <title>Bayesian Meta-Analysis with Application in Dental Studies</title>
      <link>https://arxiv.org/abs/2511.06200</link>
      <description>arXiv:2511.06200v1 Announce Type: new 
Abstract: Dental caries remain a persistent global health challenge, and fluoride varnish is widely used as a preventive intervention. This study synthesizes evidence from multiple clinical trials to evaluate the effectiveness of fluoride varnish in reducing Decayed-Missing-Filled (DMF) surfaces. The principal measure of efficacy is the Prevented Fraction (PF), representing the proportional reduction in caries relative to untreated controls. A comprehensive meta-analysis was conducted using fixed-effect and random-effects models, complemented by hierarchical Bayesian inference. The Bayesian framework incorporated multiple prior distributions on between-study variance, including Pareto, half-normal, uniform, beta, and scaled chi-square forms, to assess robustness under alternative heterogeneity assumptions. Across all specifications, the pooled estimate indicated an approximate 43% reduction in caries incidence, with credible intervals consistently excluding the null. Compared to classical methods, the Bayesian approach provided richer uncertainty quantification through full posterior distributions, allowed principled incorporation of prior evidence, and offered improved inference under heterogeneity and small-sample conditions. The stability of posterior estimates across diverse priors reinforces the robustness and reliability of the conclusions. Overall, findings confirm fluoride varnish as an effective and consistent preventive measure, and demonstrate the value of Bayesian hierarchical modeling as a powerful complement to traditional meta-analytic techniques in dental public health research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06200v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Antonijevic (Texas A&amp;M University), Danielle Sitalo (Texas A&amp;M University), Brani Vidakovic (Texas A&amp;M University)</dc:creator>
    </item>
    <item>
      <title>A unified approach to spatial domain detection and cell-type deconvolution in spot-based spatial transcriptomics</title>
      <link>https://arxiv.org/abs/2511.06204</link>
      <description>arXiv:2511.06204v1 Announce Type: new 
Abstract: Many popular technologies for generating spatially resolved transcriptomic (SRT) data measure gene expression at the resolution of a "spot", i.e., a small tissue region 55 microns in diameter. Each spot can contain many cells of different types. In typical analyses, researchers are interested in using these data to identify discrete spatial domains in the tissue. In this paper, we propose a new method, DUET, that simultaneously identifies discrete spatial domains and estimates each spot's cell-type proportion. This allows the identified spatial domains to be characterized in terms of the cell type proportions, which affords interpretability and biological insight. DUET utilizes a constrained version of model-based convex clustering, and as such, can accommodate Poisson, negative binomial, normal, and other types of expression data. Through simulation studies and multiple applications, we show that our method can achieve better clustering and deconvolution performance than existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06204v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyun Jung Koo, Aaron J. Molstad</dc:creator>
    </item>
    <item>
      <title>Bayesian spatio--temporal disaggregation modeling using a diffusion-SPDE approach: a case study of Aerosol Optical Depth in India</title>
      <link>https://arxiv.org/abs/2511.06276</link>
      <description>arXiv:2511.06276v1 Announce Type: new 
Abstract: Accurate estimation of Aerosol Optical Depth (AOD) is crucial for understanding climate change and its impacts on public health, as aerosols are a measure of air quality conditions. AOD is usually retrieved from satellite imagery at coarse spatial and temporal resolutions. However, producing high-resolution AOD estimates in both space and time can better support evidence-based policies and interventions. We propose a spatio-temporal disaggregation model that assumes a latent spatio--temporal continuous Gaussian process observed through aggregated measurements. The model links discrete observations to the continuous domain and accommodates covariates to improve explanatory power and interpretability. The approach employs Gaussian processes with separable or non-separable covariance structures derived from a diffusion-based spatio-temporal stochastic partial differential equation (SPDE). Bayesian inference is conducted using the INLA-SPDE framework for computational efficiency. Simulation studies and an application to nowcasting AOD at 550 nm in India demonstrate the model's effectiveness, improving spatial resolution from 0.75{\deg} to 0.25{\deg} and temporal resolution from 3 hours to 1 hour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06276v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Rodriguez Avellaneda, Paula Moraga</dc:creator>
    </item>
    <item>
      <title>Bayesian Predictive Probabilities for Online Experimentation</title>
      <link>https://arxiv.org/abs/2511.06320</link>
      <description>arXiv:2511.06320v1 Announce Type: new 
Abstract: The widespread adoption of online randomized controlled experiments (A/B Tests) for decision-making has created ongoing capacity constraints which necessitate interim analyses. As a consequence, platform users are increasingly motivated to use ad-hoc means of optimizing limited resources via peeking. Such processes, however, are error prone and often misaligned with end-of-experiment outcomes (e.g., inflated type-I error). We introduce a system based on Bayesian Predictive Probabilities that enable us to perform interim analyses without compromising fidelity of the experiment; This idea has been widely utilized in applications outside of the technology domain to more efficiently make decisions in experiments. Motivated by at-scale deployment within an experimentation platform, we demonstrate how predictive probabilities can be estimated without numerical integration techniques and recommend systems to study its properties at scale as an ongoing health check, along with system design recommendations - all on experiment data from Instagram - to demonstrate practical benefits that it enables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06320v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abbas Zaidi, Rina Friedberg, Samir Khan, Yao-Yang Leow, Maulik Soneji, Houssam Nassif, Richard Mudd</dc:creator>
    </item>
    <item>
      <title>An Algebraic Approach to Evolutionary Accumulation Models</title>
      <link>https://arxiv.org/abs/2511.06999</link>
      <description>arXiv:2511.06999v1 Announce Type: new 
Abstract: We present an algebraic approach to evolutionary accumulation modelling (EvAM). EvAM is concerned with learning and predicting the order in which evolutionary features accumulate over time. Our approach is complementary to the more common optimisation-based inference methods used in this field. Namely, we first use the natural underlying polynomial structure of the evolutionary process to define a semi-algebraic set of candidate parameters consistent with a given data set before maximising the likelihood function. We consider explicit examples and show that this approach is compatible with the solutions given by various statistical evolutionary accumulation models. Furthermore, we discuss the additional information of our algebraic model relative to these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06999v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jessica Renz, Frederik Witt, Iain G. Johnston</dc:creator>
    </item>
    <item>
      <title>Conservative Software Reliability Assessments Using Collections of Bayesian Inference Problems</title>
      <link>https://arxiv.org/abs/2511.07038</link>
      <description>arXiv:2511.07038v1 Announce Type: new 
Abstract: When using Bayesian inference to support conservative software reliability assessments, it is useful to consider a collection of Bayesian inference problems, with the aim of determining the worst-case value (from this collection) for a posterior predictive probability that characterizes how reliable the software is. Using a Bernoulli process to model the occurrence of software failures, we explicitly determine (from collections of Bayesian inference problems) worst-case posterior predictive probabilities of the software operating without failure in the future. We deduce asymptotic properties of these conservative posterior probabilities and their priors, and illustrate how to use these results in assessments of safety-critical software. This work extends robust Bayesian inference results and so-called conservative Bayesian inference methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07038v1</guid>
      <category>stat.AP</category>
      <category>cs.SE</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kizito Salako, Rabiu Tsoho Muhammad</dc:creator>
    </item>
    <item>
      <title>Bayesian compartmental modelling of MRSA transmission within hospitals in Edmonton, Canada</title>
      <link>https://arxiv.org/abs/2511.07353</link>
      <description>arXiv:2511.07353v1 Announce Type: new 
Abstract: Methicillin-resistant Staphylococcus aureus (MRSA) is a bacterium that leads to severe infections in hospitalized patients. Previous epidemiological research has focused on MRSA transmission, but few studies have examined the influence of both hospital-acquired MRSA (HA-MRSA) and community-acquired MRSA (CA-MRSA) on MRSA spread in hospitals. In this study, we present a unique compartmental model for studying MRSA transmission patterns in hospitals in Edmonton, Alberta. The model consists of susceptible individuals, patients who have been colonized or infected with HA-MRSA or CA-MRSA, and isolated patients. We first use Bayesian inference with Markov chain Monte Carlo (MCMC) algorithms to estimate the posterior mean of parameters in the full model using data from hospitals in Edmonton. Then we develop multiple sub-models with varying assumptions about the origin of new MRSA colonization. We also estimate transmission rates in hospitals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07353v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyu Li, Rob Deardon, Na Li, John Conly, Jenine Leal</dc:creator>
    </item>
    <item>
      <title>Do Test Scores Help Teachers Give Better Track Advice to Students? A Principal Stratification Analysis</title>
      <link>https://arxiv.org/abs/2511.05128</link>
      <description>arXiv:2511.05128v1 Announce Type: cross 
Abstract: We study whether access to standardized test scores improves the quality of teachers' secondary school track recommendations, using Dutch data and a metric based on Principal Stratification in a quasi-randomized setting. Allowing teachers to revise their recommendations when test results exceed expectations increases the share of students successfully placed in more demanding tracks by at least 6%, but misplaces 7% of weaker students. However, only implausibly high weights on the short-term losses of students who must change track because of misplacement would justify prohibiting test-score-based upgrades. Access to test scores also induces fairer recommendations for immigrant and low-SES students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05128v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Ichino, Fabrizia Mealli, Javier Viviens</dc:creator>
    </item>
    <item>
      <title>Estimating the Impact of the Bitcoin Halving on Its Price Using Synthetic Control</title>
      <link>https://arxiv.org/abs/2511.05512</link>
      <description>arXiv:2511.05512v1 Announce Type: cross 
Abstract: The third Bitcoin halving that took place in May 2020 cut down the mining reward from 12.5 to 6.25 BTC per block and thus slowed down the rate of issuance of new Bitcoins, making it more scarce. The fourth and most recent halving happened in April 2024, cutting the block reward further to 3.125 BTC. If the demand did not decrease simultaneously after these halvings, then the neoclassical economic theory posits that the price of Bitcoin should have increased due to the halving. But did it, in fact, increase for that reason, or is this a post hoc fallacy? This paper uses synthetic control to construct a weighted Bitcoin that is different from its counterpart in one aspect - it did not undergo halving. Comparing the price trajectory of the actual and the simulated Bitcoins, I find evidence of a positive effect of the 2024 Bitcoin halving on its price three months later. The magnitude of this effect is one fifth of the total percentage change in the price of Bitcoin during the study period - from April 2, 2023, to July 21, 2024 (17 months). The second part of the study fails to obtain a statistically significant and robust causal estimate of the effect of the 2020 Bitcoin halving on Bitcoin's price. This is the first paper analyzing the effect of halving causally, building on the existing body of correlational research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05512v1</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladislav Virtonen</dc:creator>
    </item>
    <item>
      <title>The Evolution of Probabilistic Price Forecasting Techniques: A Review of the Day-Ahead, Intra-Day, and Balancing Markets</title>
      <link>https://arxiv.org/abs/2511.05523</link>
      <description>arXiv:2511.05523v1 Announce Type: cross 
Abstract: Electricity price forecasting has become a critical tool for decision-making in energy markets, particularly as the increasing penetration of renewable energy introduces greater volatility and uncertainty. Historically, research in this field has been dominated by point forecasting methods, which provide single-value predictions but fail to quantify uncertainty. However, as power markets evolve due to renewable integration, smart grids, and regulatory changes, the need for probabilistic forecasting has become more pronounced, offering a more comprehensive approach to risk assessment and market participation. This paper presents a review of probabilistic forecasting methods, tracing their evolution from Bayesian and distribution based approaches, through quantile regression techniques, to recent developments in conformal prediction. Particular emphasis is placed on advancements in probabilistic forecasting, including validity-focused methods which address key limitations in uncertainty estimation. Additionally, this review extends beyond the Day-Ahead Market to include the Intra-Day and Balancing Markets, where forecasting challenges are intensified by higher temporal granularity and real-time operational constraints. We examine state of the art methodologies, key evaluation metrics, and ongoing challenges, such as forecast validity, model selection, and the absence of standardised benchmarks, providing researchers and practitioners with a comprehensive and timely resource for navigating the complexities of modern electricity markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05523v1</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ciaran O'Connor, Mohamed Bahloul, Steven Prestwich, Andrea Visentin</dc:creator>
    </item>
    <item>
      <title>Modeling Causal Interactions Across Brain Functional Subnetworks for Population-specific Disease Analysis</title>
      <link>https://arxiv.org/abs/2511.05548</link>
      <description>arXiv:2511.05548v1 Announce Type: cross 
Abstract: Current neuroimaging studies on neurodegenerative diseases and psychological risk factors have been developed predominantly in non Hispanic White cohorts, with other populations markedly underrepresented. In this work, we construct directed hyper connectomes among large scale functional brain systems based on causal influences between brain regions, and examine their links to Alzheimer Disease progression and worry levels across racial groups. By using Health and Aging Brain Study Health Disparities (HABS HD) dataset, our experimental results suggest that neglecting racial variation in brain network architecture may reduce predictive performance in both cognitive and affective phenotypes. Important shared and population-specific hyper-connectome patterns related to both AD progression and worry levels were identified. We further observed distinct closed loop directed circuits across groups, suggesting that different populations may rely on distinct feedback based network regulation strategies when supporting cognition or managing emotional states. Together, these results indicate a common backbone of network vulnerability with population-dependent variations in regulatory coordination, underscoring the importance of population-aware neuroimaging models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05548v1</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alissen Moreno, Yingying Zhang, Qi Huang, Fabian Vazquez, Jose A. Nunez, Erik Enriquez, Dongchul Kim, Kaixiong Zhou, Hongchang Gao, Pengfei Gu, Liang Zhan, Haoteng Tang</dc:creator>
    </item>
    <item>
      <title>Bounding interventional queries from generalized incomplete contingency tables</title>
      <link>https://arxiv.org/abs/2511.05755</link>
      <description>arXiv:2511.05755v1 Announce Type: cross 
Abstract: We introduce a method for evaluating interventional queries and Average Treatment Effects (ATEs) in the presence of generalized incomplete contingency tables (GICTs), contingency tables containing a full row of random (sampling) zeros, rendering some conditional probabilities undefined. Rather than discarding such entries or imputing missing values, we model the unknown probabilities as free parameters and derive symbolic expressions for the queries that incorporate them. By extremizing these expressions over all values consistent with basic probability constraints and the support of all variables, we obtain sharp bounds for the query of interest under weak assumptions of small missing frequencies. These bounds provide a formal quantification of the uncertainty induced by the generalized incompleteness of the contingency table and ensure that the true value of the query will always lie within them. The framework applies independently of the missingness mechanism and offers a conservative yet rigorous approach to causal inference under random data gaps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05755v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivano Lodato, Aditya V. Iyer, Isaac Z. To</dc:creator>
    </item>
    <item>
      <title>Standard and comparative e-backtests for general risk measures</title>
      <link>https://arxiv.org/abs/2511.05840</link>
      <description>arXiv:2511.05840v1 Announce Type: cross 
Abstract: Backtesting risk measures is a unique and important problem for financial regulators to evaluate risk forecasts reported by financial institutions. As a natural extension to standard (or traditional) backtests, comparative backtests are introduced to evaluate different forecasts against regulatory standard models. Based on recently developed concepts of e-values and e-processes, we focus on how standard and comparative backtests can be manipulated in financial regulation by constructing e-processes. We design a model-free (non-parametric) method for standard backtests of identifiable risk measures and comparative backtests of elicitable risk measures. Our e-backtests are applicable to a wide range of common risk measures including the mean, the variance, the Value-at-Risk, the Expected Shortfall, and the expectile. Our results are illustrated by ample simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05840v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhanyi Jiao, Qiuqi Wang, Yimiao Zhao</dc:creator>
    </item>
    <item>
      <title>Prediction-based evaluation of back-four defense with spatial control in soccer</title>
      <link>https://arxiv.org/abs/2511.06191</link>
      <description>arXiv:2511.06191v1 Announce Type: cross 
Abstract: Defensive organization is critical in soccer, particularly during negative transitions when teams are most vulnerable. The back-four defensive line plays a decisive role in preventing goal-scoring opportunities, yet its collective coordination remains difficult to quantify. This study introduces interpretable spatio-temporal indicators namely, space control, stretch index, pressure index, and defensive line height (absolute and relative) to evaluate the effectiveness of the back-four during defensive transitions. Using synchronized tracking and event data from the 2023-24 LaLiga season, 2,413 defensive sequences were analyzed following possession losses by FC Barcelona and Real Madrid CF. Two-way ANOVA revealed significant effects of team, outcome, and their interaction for key indicators, with relative line height showing the strongest association with defensive success. Predictive modeling using XGBoost achieved the highest discriminative performance (ROC AUC: 0.724 for Barcelona, 0.698 for Real Madrid), identifying space score and relative line height as dominant predictors. Comparative analysis revealed distinct team-specific defensive behaviors: Barcelona's success was characterized by higher spatial control and compact line coordination, whereas Real Madrid exhibited more adaptive but less consistent defensive structures. These findings demonstrate the tactical and predictive value of interpretable spatial indicators for quantifying collective defensive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06191v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soujanya Dash, Kenjiro Ide, Rikuhei Umemoto, Kai Amino, Keisuke Fujii</dc:creator>
    </item>
    <item>
      <title>Breaking the Winner's Curse with Bayesian Hybrid Shrinkage</title>
      <link>https://arxiv.org/abs/2511.06318</link>
      <description>arXiv:2511.06318v1 Announce Type: cross 
Abstract: A 'Winner's Curse' arises in large-scale online experimentation platforms when the same experiments are used to both select treatments and evaluate their effects. In these settings, classical difference-in-means estimators of treatment effects are upwardly biased and conventional confidence intervals are rendered invalid. The bias scales with the magnitude of sampling variability and the selection threshold, and inversely with the treatment's true effect size. We propose a new Bayesian approach that incorporates experiment-specific 'local shrinkage' factors that mitigate sensitivity to the choice of prior and improve robustness to assumption violations. We demonstrate how the associated posterior distribution can be estimated without numerical integration techniques, making it a practical choice for at-scale deployment. Through simulation, we evaluate the performance of our approach under various scenarios and find that it performs well even when assumptions about the sampling and selection processes are violated. In an empirical evaluation, our approach demonstrated superior performance over alternative methods, providing more accurate estimates with well-calibrated uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06318v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Mudd, Rina Friedberg, Ilya Gorbachev, Houssam Nassif, Abbas Zaidi</dc:creator>
    </item>
    <item>
      <title>Boundary Discontinuity Designs: Theory and Practice</title>
      <link>https://arxiv.org/abs/2511.06474</link>
      <description>arXiv:2511.06474v1 Announce Type: cross 
Abstract: We review the literature on boundary discontinuity (BD) designs, a powerful non-experimental research methodology that identifies causal effects by exploiting a thresholding treatment assignment rule based on a bivariate score and a boundary curve. This methodology generalizes standard regression discontinuity designs based on a univariate score and scalar cutoff, and has specific challenges and features related to its multi-dimensional nature. We synthesize the empirical literature by systematically reviewing over $80$ empirical papers, tracing the method's application from its formative uses to its implementation in modern research. In addition to the empirical survey, we overview the latest methodological results on identification, estimation and inference for the analysis of BD designs, and offer recommendations for practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06474v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>wdiexplorer: An R package Designed for Exploratory Analysis of World Development Indicators (WDI) Data</title>
      <link>https://arxiv.org/abs/2511.07027</link>
      <description>arXiv:2511.07027v1 Announce Type: cross 
Abstract: The World Development Indicators (WDI) database provides a wide range of global development data, maintained and published by the World Bank. Our \textit{wdiexplorer} package offers a comprehensive workflow that sources WDI data via the \textit{WDI} R package, prepares and explores country-level panel data of the WDI through computational functions to calculate diagnostic metrics and visualise the outputs. By leveraging the functionalities of \textit{wdiexplorer} package, users can efficiently explore any indicator dataset of the WDI, compute diagnostic indices, and visualise the metrics by incorporating the pre-defined grouping structures to identify patterns, outliers, and other interesting features of temporal behaviours. This paper presents the \textit{wdiexplorer} package, demonstrates its functionalities using the WDI: PM$_{2.5}$ air pollution dataset, and discusses the observed patterns and outliers across countries and within groups of country-level panel data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07027v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oluwayomi Akinfenwa, Niamh Cahill, Catherine Hurley</dc:creator>
    </item>
    <item>
      <title>Scaling-aware rating of Poisson-limited demand forecasts</title>
      <link>https://arxiv.org/abs/2211.16313</link>
      <description>arXiv:2211.16313v5 Announce Type: replace 
Abstract: Forecast quality should be assessed in the context of what is possible in theory and what is reasonable to expect in practice. Often, one can identify an approximate upper bound to a probabilistic forecast's sharpness, which sets a lower, not necessarily achievable, limit to error metrics. In retail forecasting, a simple, but often unconquerable sharpness limit is given by the Poisson distribution. When evaluating forecasts using traditional metrics such as Mean Absolute Error, it is hard to judge whether a certain achieved value reflects unavoidable Poisson noise or truly indicates an over-dispersed prediction model. Moreover, every evaluation metric suffers from precision scaling: The metric's value is mostly defined by the selling rate and by the resulting rate-dependent Poisson noise, and only secondarily by the forecast quality. Comparing two groups of forecasted products often yields "the slow movers are performing worse than the fast movers" or vice versa, which we call the na\"ive scaling trap. To distill the intrinsic quality of a forecast, we stratify predictions into buckets of approximately equal predicted values and evaluate metrics separately per bucket. By comparing the achieved value per bucket to benchmarks defined by the theoretical expectation value of the metric, we obtain an intuitive visualization of forecast quality. This representation can be summarized by a single rating that makes forecast quality comparable among different products or even industries. The thereby developed scaling-aware forecast rating is applied to forecasting models used on the M5 competition dataset as well as to real-life forecasts provided by Blue Yonder's Demand Edge for Retail solution for grocery products in Sainsbury's supermarkets in the United Kingdom. The results permit a clear interpretation and high-level understanding of model quality by non-experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16313v5</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/for.70055</arxiv:DOI>
      <dc:creator>Malte C. Tichy, Illia Babounikau, Nikolas Wolke, Stefan Ulbrich, Michael Feindt</dc:creator>
    </item>
    <item>
      <title>Variable Selection for Multi-Source Count Data with Controlled False Discovery Rate</title>
      <link>https://arxiv.org/abs/2411.18986</link>
      <description>arXiv:2411.18986v2 Announce Type: replace 
Abstract: The rapid generation of complex, highly skewed, and zero-inflated multi-source count data poses significant challenges for variable selection, particularly in biomedical domains like tumor development and metabolic dysregulation. To address this, we propose a new variable selection method, Zero-Inflated Poisson-Gamma Simultaneous Knockoff (ZIPG-SK), specifically designed for multi-source count data. Our method leverages a gaussian copula based on the Zero-Inflated Poisson-Gamma (ZIPG) distribution to construct knockoffs that properly account for the properties of count data, including high skewness and zero inflation, while effectively incorporating covariate information. This framework enables the detection of common features across multi-source datasets with guaranteed false discovery rate (FDR) control. Furthermore, we enhance the power of the method by incorporating e-value aggregation, which effectively mitigates the inherent randomness in knockoff generation. Through extensive simulations, we demonstrate that ZIPG-SK significantly outperforms existing methods, achieving superior power across various scenarios. We validate the utility of our method on real-world colorectal cancer (CRC) and type 2 diabetes (T2D) datasets, identifying key variables whose characteristics align with established findings and simultaneously provide new mechanistic insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18986v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shan Tang, Shanjun Mao, Shourong Ma, Falong Tan</dc:creator>
    </item>
    <item>
      <title>Hierarchical models for small area estimation using zero-inflated forest inventory variables: comparison and implementation</title>
      <link>https://arxiv.org/abs/2503.22103</link>
      <description>arXiv:2503.22103v3 Announce Type: replace 
Abstract: National Forest Inventory (NFI) data are typically limited to sparse networks of sample locations due to cost constraints. While design-based estimators provide reliable forest parameter estimates for large areas, there is increasing interest in model-based small area estimation (SAE) methods to improve precision for smaller spatial, temporal, or biophysical domains. SAE methods can be broadly categorized into area- and unit-level models, with unit-level models offering greater flexibility, making them the focus of this study. Ensuring valid inference requires satisfying model distributional assumptions, which is particularly challenging for NFI variables that exhibit positive support and zero-inflation, such as forest biomass, carbon, and volume. Here, we evaluate nine candidate estimators, including two-stage unit-level hierarchical Bayesian models, single-stage Bayesian models, and two-stage frequentist models, for estimating forest biomass at the county level in Nevada and Washington, United States. Estimator performance is assessed using repeated sampling from simulated populations and unit-level cross-validation with FIA data. Results show that small area estimators incorporating a two-stage approach to account for zero-inflation, county-specific random intercepts and residual variances, and spatial random effects yield the most accurate and well-calibrated county-level estimates, with spatial effects providing the greatest benefits when spatial autocorrelation is present in the underlying population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22103v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grayson W. White, Andrew O. Finley, Josh K. Yamamoto, Jennifer L. Green, Tracey S. Frescino, David. W. MacFarlane, Hans-Erik Andersen, Grant M. Domke</dc:creator>
    </item>
    <item>
      <title>Probabilistic Wind Power Modelling via Heteroscedastic Non-Stationary Gaussian Processes</title>
      <link>https://arxiv.org/abs/2505.09026</link>
      <description>arXiv:2505.09026v2 Announce Type: replace 
Abstract: Accurate probabilistic prediction of wind power is crucial for maintaining grid stability and facilitating the efficient integration of renewable energy sources. Gaussian process (GP) models offer a principled framework for quantifying uncertainty; however, conventional approaches typically rely on stationary kernels and homoscedastic noise assumptions, which are inadequate for modelling the inherently non-stationary and heteroscedastic nature of wind speed and power output. We propose a heteroscedastic non-stationary GP framework based on the generalised spectral mixture kernel, enabling the model to capture input-dependent correlations as well as input-dependent variability in wind speed-power data. We evaluate the proposed model on 10-minute supervisory control and data acquisition (SCADA) measurements and compare it against GP variants with stationary and non-stationary kernels, as well as commonly used non-GP probabilistic baselines. The results highlight the necessity of modelling both non-stationarity and heteroscedasticity in wind power prediction and demonstrate the practical value of flexible non-stationary GP models in operational SCADA settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09026v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Domniki Ladopoulou, Dat Minh Hong, Petros Dellaportas</dc:creator>
    </item>
    <item>
      <title>Beyond the Trade-off Curve: Multivariate and Advanced Risk-Utility Maps for Evaluating Anonymized and Synthetic Data</title>
      <link>https://arxiv.org/abs/2510.23500</link>
      <description>arXiv:2510.23500v2 Announce Type: replace 
Abstract: Anonymizing microdata requires balancing the reduction of disclosure risk with the preservation of data utility. Traditional evaluations often rely on single measures or two-dimensional risk-utility (R-U) maps, but real-world assessments involve multiple, often correlated, indicators of both risk and utility. Pairwise comparisons of these measures can be inefficient and incomplete. We therefore systematically compare six visualization approaches for simultaneous evaluation of multiple risk and utility measures: heatmaps, dot plots, composite scatterplots, parallel coordinate plots, radial profile charts, and PCA-based biplots. We introduce blockwise PCA for composite scatterplots and joint PCA for biplots that simultaneously reveal method performance and measure interrelationships. Through systematic identification of Pareto-optimal methods in all approaches, we demonstrate how multivariate visualization supports a more informed selection of anonymization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23500v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oscar Thees, Roman M\"uller, Matthias Templ</dc:creator>
    </item>
    <item>
      <title>Forecasting Arctic Temperatures with Temporally Dependent Data Using Quantile Gradient Boosting and Adaptive Conformal Prediction Regions</title>
      <link>https://arxiv.org/abs/2510.23976</link>
      <description>arXiv:2510.23976v2 Announce Type: replace 
Abstract: Using data from the Longyearbyen weather station, quantile gradient boosting (``small AI'') is applied to forecast daily 2023 temperatures in Svalbard, Norway. The 0.60 quantile loss weights underestimates about 1.5 times more than overestimates. Predictors include five routinely collected indicators of weather conditions, each lagged by 14~days, yielding temperature forecasts with a two-week lead time. Conformal prediction regions quantify forecasting uncertainty with provably valid coverage. Forecast accuracy is evaluated with attention to local stakeholder concerns, and implications for Arctic adaptation policy are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23976v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard Berk</dc:creator>
    </item>
    <item>
      <title>NIRVAR: Network Informed Restricted Vector Autoregression</title>
      <link>https://arxiv.org/abs/2407.13314</link>
      <description>arXiv:2407.13314v4 Announce Type: replace-cross 
Abstract: High-dimensional panels of time series often arise in finance and macroeconomics, where co-movements within groups of panel components occur. Extracting these groupings from the data provides a coarse-grained description of the complex system in question and can inform subsequent prediction tasks. We develop a novel methodology to model such a panel as a restricted vector autoregressive process, where the coefficient matrix is the weighted adjacency matrix of a stochastic block model. This network time series model, which we call the Network Informed Restricted Vector Autoregression (NIRVAR) model, yields a coefficient matrix that has a sparse block-diagonal structure. We propose an estimation procedure that embeds each panel component in a low-dimensional latent space and clusters the embedded points to recover the blocks of the coefficient matrix. Crucially, the method allows for network-based time series modelling when the underlying network is unobserved. We derive the bias, consistency and asymptotic normality of the NIRVAR estimator. Simulation studies suggest that the NIRVAR estimated embedded points are Gaussian distributed around the ground truth latent positions. On three applications to finance, macroeconomics, and transportation systems, NIRVAR outperforms competing models in terms of prediction and provides interpretable results regarding group recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13314v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Martin, Francesco Sanna Passino, Mihai Cucuringu, Alessandra Luati</dc:creator>
    </item>
    <item>
      <title>Automated Vehicles at Unsignalized Intersections: Safety and Efficiency Implications of Mixed Human and Automated Traffic</title>
      <link>https://arxiv.org/abs/2410.12538</link>
      <description>arXiv:2410.12538v3 Announce Type: replace-cross 
Abstract: The integration of automated vehicles (AVs) into transportation systems presents an unprecedented opportunity to enhance road safety and efficiency. However, understanding the interactions between AVs and human-driven vehicles (HVs) at intersections remains an open research question. This study aims to bridge this gap by examining behavioral differences and adaptations of AVs and HVs at unsignalized intersections by utilizing two large-scale AV datasets from Waymo and Lyft. By using a systematic methodology, the research identifies and analyzes merging and crossing conflicts by calculating key safety and efficiency metrics, including time to collision (TTC), post-encroachment time (PET), maximum required deceleration (MRD), time advantage (TA), and speed and acceleration profiles. Through this approach, the study assesses the safety and efficiency implications of these behavioral differences and adaptations for mixed-autonomy traffic. The findings reveal a paradox: while AVs maintain larger safety margins, their conservative behavior can lead to unexpected situations for human drivers, potentially causing unsafe conditions. From a performance point of view, human drivers tend to exhibit more consistent behavior when interacting with AVs versus other HVs, suggesting AVs may contribute to harmonizing traffic flow patterns. Moreover, notable differences were observed between Waymo and Lyft vehicles, which highlights the importance of considering manufacturer-specific AV behaviors in traffic modeling and management strategies for the safe integration of AVs. The processed dataset, as well as the developed algorithms and scripts, are openly published to foster research on AV-HV interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12538v3</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1177/03611981251370343</arxiv:DOI>
      <dc:creator>Saeed Rahmani, Zhenlin Xu, Simeon C. Calvert, Bart van Arem</dc:creator>
    </item>
    <item>
      <title>Evaluating Policy Effects through Opinion Dynamics and Network Sampling</title>
      <link>https://arxiv.org/abs/2501.08150</link>
      <description>arXiv:2501.08150v2 Announce Type: replace-cross 
Abstract: In the process of enacting or introducing a new policy, policymakers frequently consider the population's responses. These considerations are critical for effective governance. There are numerous methods to gauge the ground sentiment from a subset of the population; examples include surveys or listening to various feedback channels. Many conventional approaches implicitly assume that opinions are static; however, in reality, the population will discuss and debate these new policies among themselves, and reform new opinions in the process. In this paper, we pose the following questions: Can we quantify the effect of these social dynamics on the broader opinion towards a new policy? Given some information about the relationship network that underlies the population, how does overall opinion change post-discussion? We investigate three different settings in which the policy is revealed: respondents who do not know each other, groups of respondents who all know each other, and respondents chosen randomly. By controlling who the policy is revealed to, we control the degree of discussion among the population. We quantify how these factors affect the changes in policy beliefs via the Wasserstein distance between the empirically observed data post-discussion and its distribution pre-discussion. We also provide several numerical analyses based on generated network and real-life network datasets. Our work aims to address the challenges associated with network topology and social interactions, and provide policymakers with a quantitative lens to assess policy effectiveness in the face of resource constraints and network complexities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08150v2</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugene T. Y. Ang, Yong Sheng Soh</dc:creator>
    </item>
    <item>
      <title>Forecasting intermittent time series with Gaussian Processes and Tweedie likelihood</title>
      <link>https://arxiv.org/abs/2502.19086</link>
      <description>arXiv:2502.19086v5 Announce Type: replace-cross 
Abstract: We adopt Gaussian Processes (GPs) as latent functions for probabilistic forecasting of intermittent time series. The model is trained in a Bayesian framework that accounts for the uncertainty about the latent function. We couple the latent GP variable with two types of forecast distributions: the negative binomial (NegBinGP) and the Tweedie distribution (TweedieGP). While the negative binomial has already been used in forecasting intermittent time series, this is the first time in which a fully parameterized Tweedie density is used for intermittent time series. We properly evaluate the Tweedie density, which has both a point mass at zero and heavy tails, avoiding simplifying assumptions made in existing models. We test our models on thousands of intermittent count time series. Results show that our models provide consistently better probabilistic forecasts than the competitors. In particular, TweedieGP obtains the best estimates of the highest quantiles, thus showing that it is more flexible than NegBinGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19086v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijforecast.2025.10.001</arxiv:DOI>
      <dc:creator>Stefano Damato, Dario Azzimonti, Giorgio Corani</dc:creator>
    </item>
    <item>
      <title>Nonlinear Treatment Effects in Shift-Share Designs</title>
      <link>https://arxiv.org/abs/2507.21915</link>
      <description>arXiv:2507.21915v2 Announce Type: replace-cross 
Abstract: We analyze heterogenous, nonlinear treatment effects in shift-share designs with exogenous shares. We employ a triangular model and correct for treatment endogeneity using a control function. Our tools identify four target parameters. Two of them capture the observable heterogeneity of treatment effects, while one summarizes this heterogeneity in a single measure. The last parameter analyzes counterfactual, policy-relevant treatment assignment mechanisms. We propose flexible parametric estimators for these parameters and apply them to reevaluate the impact of Chinese imports on U.S. manufacturing employment. Our results highlight substantial treatment effect heterogeneity, which is not captured by commonly used shift-share tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21915v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luigi Garzon, Vitor Possebom</dc:creator>
    </item>
    <item>
      <title>The causal structure of galactic astrophysics</title>
      <link>https://arxiv.org/abs/2510.01112</link>
      <description>arXiv:2510.01112v2 Announce Type: replace-cross 
Abstract: Data-driven astrophysics currently relies on the detection and characterisation of correlations between objects' properties, which are then used to test physical theories that make predictions for them. This process fails to utilise information in the data that forms a crucial part of the theories' predictions, namely which variables are directly correlated (as opposed to accidentally correlated through others), the directions of these determinations, and the presence or absence of confounders that correlate variables in the dataset but are themselves absent from it. We propose to recover this information through causal discovery, a well-developed methodology for inferring the causal structure of datasets that is however almost entirely unknown to astrophysics. We develop a causal discovery algorithm suitable for large astrophysical datasets and illustrate it on $\sim$5$\times10^5$ low-redshift galaxies from the Nasa Sloan Atlas, demonstrating its ability to distinguish physical mechanisms that are degenerate on the basis of correlations alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01112v2</guid>
      <category>astro-ph.GA</category>
      <category>astro-ph.CO</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harry Desmond, Joseph Ramsey</dc:creator>
    </item>
    <item>
      <title>Sub-exponential Growth of New Words and Names Online: A Piecewise Power-Law Model</title>
      <link>https://arxiv.org/abs/2511.04106</link>
      <description>arXiv:2511.04106v2 Announce Type: replace-cross 
Abstract: The diffusion of ideas and language in society has conventionally been described by S-shaped models, such as the logistic curve. However, the role of sub-exponential growth -a slower than exponential pattern known in epidemiology- has been largely overlooked in broader social phenomena. Here, we present a piecewise power-law model to characterize complex growth curves with a few parameters. We systematically analyzed a large-scale dataset of approximately one billion Japanese blog articles linked to Wikipedia vocabulary, and observed consistent patterns in web search trend data (English, Spanish, and Japanese). Our analysis of the 2,965 selected items reveals that about 55% (1,625 items) were found to have no abrupt jumps and were well captured by one or two segments. For single-segment curves, we found that (i) the mode of the shape parameter alpha was near 0.5, indicating prevalent sub-exponential growth; (ii) the ultimate diffusion scale is primarily determined by the growth rate R, with minor contributions from alpha or the duration T; and (iii) alpha showed a tendency to vary with the nature of the topic, being smaller for niche/local topics and larger for widely shared ones. Furthermore, a micro-behavioral model distinguishing outward contact with strangers from inward interaction within their community suggests that alpha can be interpreted as an index of the preference for outward-oriented communication. These findings suggest that sub-exponential growth is a common pattern of social diffusion, and our model provides a practical framework for consistently describing, comparing, and interpreting complex and diverse growth curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04106v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hayafumi Watanabe</dc:creator>
    </item>
  </channel>
</rss>
