<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Mar 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Uniformly consistent proportion estimation for composite hypotheses via integral equations: "the case of location-shift families"</title>
      <link>https://arxiv.org/abs/2503.16590</link>
      <description>arXiv:2503.16590v1 Announce Type: new 
Abstract: We consider estimating the proportion of random variables for two types of composite null hypotheses: (i) the means or medians of the random variables belonging to a non-empty, bounded interval; (ii) the means or medians of the random variables belonging to an unbounded interval that is not the whole real line. For each type of composite null hypotheses, uniformly consistent estimators of the proportion of false null hypotheses are constructed for random variables whose distributions are members of a Type I location-shift family. Further, uniformly consistent estimators of certain functions of a bounded null on the means or medians are provided for the random variables mentioned earlier; these functions are continuous and of bounded variation. The estimators are constructed via solutions to Lebesgue-Stieltjes integral equations and harmonic analysis, do not rely on a concept of p-value, and have various applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16590v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiongzhi Chen</dc:creator>
    </item>
    <item>
      <title>Glivenko-Cantelli for $f$-divergence</title>
      <link>https://arxiv.org/abs/2503.17355</link>
      <description>arXiv:2503.17355v1 Announce Type: new 
Abstract: We extend the celebrated Glivenko-Cantelli theorem, sometimes called the fundamental theorem of statistics, from its standard setting of total variation distance to all $f$-divergences. A key obstacle in this endeavor is to define $f$-divergence on a subcollection of a $\sigma$-algebra that forms a $\pi$-system but not a $\sigma$-subalgebra. This is a side contribution of our work. We will show that this notion of $f$-divergence on the $\pi$-system of rays preserves nearly all known properties of standard $f$-divergence, yields a novel integral representation of the Kolmogorov-Smirnov distance, and has a Glivenko-Cantelli theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17355v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoming Wang, Lek-Heng Lim</dc:creator>
    </item>
    <item>
      <title>A Statistical Analysis for Per-Instance Evaluation of Stochastic Optimizers: How Many Repeats Are Enough?</title>
      <link>https://arxiv.org/abs/2503.16589</link>
      <description>arXiv:2503.16589v1 Announce Type: cross 
Abstract: A key trait of stochastic optimizers is that multiple runs of the same optimizer in attempting to solve the same problem can produce different results. As a result, their performance is evaluated over several repeats, or runs, on the problem. However, the accuracy of the estimated performance metrics depends on the number of runs and should be studied using statistical tools. We present a statistical analysis of the common metrics, and develop guidelines for experiment design to measure the optimizer's performance using these metrics to a high level of confidence and accuracy. To this end, we first discuss the confidence interval of the metrics and how they are related to the number of runs of an experiment. We then derive a lower bound on the number of repeats in order to guarantee achieving a given accuracy in the metrics. Using this bound, we propose an algorithm to adaptively adjust the number of repeats needed to ensure the accuracy of the evaluated metric. Our simulation results demonstrate the utility of our analysis and how it allows us to conduct reliable benchmarking as well as hyperparameter tuning and prevent us from drawing premature conclusions regarding the performance of stochastic optimizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16589v1</guid>
      <category>cs.LG</category>
      <category>cs.ET</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moslem Noori, Elisabetta Valiante, Thomas Van Vaerenbergh, Masoud Mohseni, Ignacio Rozada</dc:creator>
    </item>
    <item>
      <title>Optimal Nonlinear Online Learning under Sequential Price Competition via s-Concavity</title>
      <link>https://arxiv.org/abs/2503.16737</link>
      <description>arXiv:2503.16737v1 Announce Type: cross 
Abstract: We consider price competition among multiple sellers over a selling horizon of $T$ periods. In each period, sellers simultaneously offer their prices and subsequently observe their respective demand that is unobservable to competitors. The demand function for each seller depends on all sellers' prices through a private, unknown, and nonlinear relationship. To address this challenge, we propose a semi-parametric least-squares estimation of the nonlinear mean function, which does not require sellers to communicate demand information. We show that when all sellers employ our policy, their prices converge at a rate of $O(T^{-1/7})$ to the Nash equilibrium prices that sellers would reach if they were fully informed. Each seller incurs a regret of $O(T^{5/7})$ relative to a dynamic benchmark policy. A theoretical contribution of our work is proving the existence of equilibrium under shape-constrained demand functions via the concept of $s$-concavity and establishing regret bounds of our proposed policy. Technically, we also establish new concentration results for the least squares estimator under shape constraints. Our findings offer significant insights into dynamic competition-aware pricing and contribute to the broader study of non-parametric learning in strategic decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16737v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Bracale, Moulinath Banerjee, Cong Shi, Yuekai Sun</dc:creator>
    </item>
    <item>
      <title>Nonparametric Factor Analysis and Beyond</title>
      <link>https://arxiv.org/abs/2503.16865</link>
      <description>arXiv:2503.16865v1 Announce Type: cross 
Abstract: Nearly all identifiability results in unsupervised representation learning inspired by, e.g., independent component analysis, factor analysis, and causal representation learning, rely on assumptions of additive independent noise or noiseless regimes. In contrast, we study the more general case where noise can take arbitrary forms, depend on latent variables, and be non-invertibly entangled within a nonlinear function. We propose a general framework for identifying latent variables in the nonparametric noisy settings. We first show that, under suitable conditions, the generative model is identifiable up to certain submanifold indeterminacies even in the presence of non-negligible noise. Furthermore, under the structural or distributional variability conditions, we prove that latent variables of the general nonlinear models are identifiable up to trivial indeterminacies. Based on the proposed theoretical framework, we have also developed corresponding estimation methods and validated them in various synthetic and real-world settings. Interestingly, our estimate of the true GDP growth from alternative measurements suggests more insightful information on the economies than official reports. We expect our framework to provide new insight into how both researchers and practitioners deal with latent variables in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16865v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujia Zheng, Yang Liu, Jiaxiong Yao, Yingyao Hu, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>An improved nonparametric test and sample size procedures for the randomized complete block designs</title>
      <link>https://arxiv.org/abs/2503.17179</link>
      <description>arXiv:2503.17179v1 Announce Type: cross 
Abstract: The Friedman test has been extensively applied as a nonparametric alternative to the conventional F procedure for comparing treatment effects in randomized complete block designs. A chi-square distribution provides a convenient approximation to determining the critical values for the Friedman procedure in hypothesis testing. However, the chi-square approximation is generally conservative and the accuracy declines with increasing number of treatments. This paper describes an alternative transformation of the Friedman statistic along with an approximate F distribution that has the same numerator degrees of freedom as the ANOVA F test. Moreover, two approximate noncentral F distributions are presented for the proposed F-transformation under the alternative hypothesis of heterogeneous location shifts. Explicit power functions are derived when the underlying populations have the uniform, normal, Laplace, and exponential distributions. Theoretical examination and empirical assessment are presented to validate the advantages of the proposed approaches over the existing methods of the Friedman test. The developed test and power procedures are recommended due to their consistently acceptable Type I error rates and accurate power calculations for the location shift structures and population distributions considered here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17179v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Show-Li Jan, Gwowen Shieh</dc:creator>
    </item>
    <item>
      <title>On Privately Estimating a Single Parameter</title>
      <link>https://arxiv.org/abs/2503.17252</link>
      <description>arXiv:2503.17252v1 Announce Type: cross 
Abstract: We investigate differentially private estimators for individual parameters within larger parametric models. While generic private estimators exist, the estimators we provide repose on new local notions of estimand stability, and these notions allow procedures that provide private certificates of their own stability. By leveraging these private certificates, we provide computationally and statistical efficient mechanisms that release private statistics that are, at least asymptotically in the sample size, essentially unimprovable: they achieve instance optimal bounds. Additionally, we investigate the practicality of the algorithms both in simulated data and in real-world data from the American Community Survey and US Census, highlighting scenarios in which the new procedures are successful and identifying areas for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17252v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hilal Asi, John C. Duchi, Kunal Talwar</dc:creator>
    </item>
    <item>
      <title>Spectral gap bounds for reversible hybrid Gibbs chains</title>
      <link>https://arxiv.org/abs/2312.12782</link>
      <description>arXiv:2312.12782v4 Announce Type: replace 
Abstract: Hybrid Gibbs samplers represent a prominent class of approximated Gibbs algorithms that utilize Markov chains to approximate conditional distributions, with the Metropolis-within-Gibbs algorithm standing out as a well-known example. Despite their widespread use in both statistical and non-statistical applications, little is known about their convergence properties. This article introduces novel methods for establishing bounds on the convergence rates of certain reversible hybrid Gibbs samplers. In particular, we examine the convergence characteristics of hybrid random-scan Gibbs algorithms. Our analysis reveals that the absolute spectral gap of a hybrid Gibbs chain can be bounded based on the absolute spectral gap of the exact Gibbs chain and the absolute spectral gaps of the Markov chains employed for conditional distribution approximations. We also provide a convergence bound of similar flavors for hybrid data augmentation algorithms, extending existing works on the topic. The general bounds are applied to three examples: a random-scan Metropolis-within-Gibbs sampler, random-scan Gibbs samplers with block updates, and a hybrid slice sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12782v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Qin, Nianqiao Ju, Guanyang Wang</dc:creator>
    </item>
    <item>
      <title>The Hidden Toll of COVID-19 on Opioid Mortality in Georgia: A Bayesian Excess Opioid Mortality Analysis</title>
      <link>https://arxiv.org/abs/2503.07918</link>
      <description>arXiv:2503.07918v2 Announce Type: replace 
Abstract: COVID-19 has had a large scale negative impact on the health of opioid users exacerbating the health of an already vulnerable population. Critical information on the total impact of COVID-19 on opioid users is unknown due to a lack of comprehensive data on COVID-19 cases, inaccurate diagnostic coding, and lack of data coverage. To assess the impact of COVID-19 on small-area opioid mortality, we developed a Bayesian hierarchical excess opioid mortality modeling approach. We incorporate spatio-temporal autocorrelation structures to allow for sharing of information across small areas and time to reduce uncertainty in small area estimates. Excess mortality is defined as the difference between observed trends after a crisis and expected trends based on observed historical trends, which captures the total increase in observed mortality rates compared to what was expected prior to the crisis. We illustrate the application of our approach to assess excess opioid mortality risk estimates for 159 counties in GA. Using our proposed approach will help inform interventions in opioid-related public health responses, policies, and resource allocation. The application of this work also provides a general framework for improving the estimation and mapping of health indicators during crisis periods for the opioid user population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07918v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyen J. Peterkin (Department of Biostatistics and Bioinformatics, Emory Rollins School of Public Health), Lance A. Waller (Department of Biostatistics and Bioinformatics, Emory Rollins School of Public Health), Emily N. Peterson (Department of Biostatistics and Bioinformatics, Emory Rollins School of Public Health)</dc:creator>
    </item>
    <item>
      <title>Sparse PCA With Multiple Components</title>
      <link>https://arxiv.org/abs/2209.14790</link>
      <description>arXiv:2209.14790v3 Announce Type: replace-cross 
Abstract: Sparse Principal Component Analysis (sPCA) is a cardinal technique for obtaining combinations of features, or principal components (PCs), that explain the variance of high-dimensional datasets in an interpretable manner. This involves solving a sparsity and orthogonality constrained convex maximization problem, which is extremely computationally challenging. Most existing works address sparse PCA via methods-such as iteratively computing one sparse PC and deflating the covariance matrix-that do not guarantee the orthogonality, let alone the optimality, of the resulting solution when we seek multiple mutually orthogonal PCs. We challenge this status by reformulating the orthogonality conditions as rank constraints and optimizing over the sparsity and rank constraints simultaneously. We design tight semidefinite relaxations to supply high-quality upper bounds, which we strengthen via additional second-order cone inequalities when each PC's individual sparsity is specified. Further, we derive a combinatorial upper bound on the maximum amount of variance explained as a function of the support. We exploit these relaxations and bounds to propose exact methods and rounding mechanisms that, together, obtain solutions with a bound gap on the order of 0%-15% for real-world datasets with p = 100s or 1000s of features and r \in {2, 3} components. Numerically, our algorithms match (and sometimes surpass) the best performing methods in terms of fraction of variance explained and systematically return PCs that are sparse and orthogonal. In contrast, we find that existing methods like deflation return solutions that violate the orthogonality constraints, even when the data is generated according to sparse orthogonal PCs. Altogether, our approach solves sparse PCA problems with multiple components to certifiable (near) optimality in a practically tractable fashion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.14790v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Cory-Wright, Jean Pauphilet</dc:creator>
    </item>
    <item>
      <title>A multiscale cavity method for sublinear-rank symmetric matrix factorization</title>
      <link>https://arxiv.org/abs/2403.07189</link>
      <description>arXiv:2403.07189v2 Announce Type: replace-cross 
Abstract: We consider a statistical model for symmetric matrix factorization with additive Gaussian noise in the high-dimensional regime where the rank $M$ of the signal matrix to infer scales with its size $N$ as $M={\rm o}(\sqrt{\ln N})$. Allowing for an $N$-dependent rank offers new challenges and requires new methods. Working in the Bayes-optimal setting, we show that whenever the signal has i.i.d.~entries, the limiting mutual information between signal and data is given by a variational formula involving a rank-one replica symmetric potential. In other words, from the information-theoretic perspective, the case of a (slowly) growing rank is the same as when $M=1$ (namely, the standard spiked Wigner model). The proof is primarily based on a novel multiscale cavity method allowing for growing rank along with some information-theoretic identities on worst noise for the vector Gaussian channel. We believe that the cavity method developed here will play a role in the analysis of a broader class of inference and spin models where the degrees of freedom are large arrays instead of vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07189v2</guid>
      <category>cs.IT</category>
      <category>cond-mat.dis-nn</category>
      <category>math-ph</category>
      <category>math.IT</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean Barbier, Justin Ko, Anas A. Rahman</dc:creator>
    </item>
    <item>
      <title>Low-Rank Thinning</title>
      <link>https://arxiv.org/abs/2502.12063</link>
      <description>arXiv:2502.12063v2 Announce Type: replace-cross 
Abstract: The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12063v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annabelle Michael Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>The Gaussian central limit theorem for a stationary time series with infinite variance</title>
      <link>https://arxiv.org/abs/2503.15894</link>
      <description>arXiv:2503.15894v2 Announce Type: replace-cross 
Abstract: We consider a borderline case: the central limit theorem for a strictly stationary time series with infinite variance but a Gaussian limit. In the iid case a well-known sufficient condition for this central limit theorem is regular variation of the marginal distribution with tail index $\alpha=2$. In the dependent case we assume the stronger condition of sequential regular variation of the time series with tail index $\alpha=2$. We assume that a sample of size $n$ from this time series can be split into $k_n$ blocks of size $r_n\to\infty$ such that $r_n/n\to 0$ as $n\to\infty$ and that the block sums are asymptotically independent. Then we apply classical central limit theory for row-wise iid triangular arrays. The necessary and sufficient conditions for such independent block sums will be verified by using large deviation results for the time series. We derive the central limit theorem for $m$-dependent sequences, linear processes, stochastic volatility processes and solutions to affine stochastic recurrence equations whose marginal distributions have infinite variance and are regularly varying with tail index $\alpha=2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15894v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muneya Matsui, Thomas Mikosch</dc:creator>
    </item>
  </channel>
</rss>
