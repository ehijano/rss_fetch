<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Oct 2025 04:01:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Structure-preserving finite-element approximations of the magnetic Euler-Poisson equations</title>
      <link>https://arxiv.org/abs/2510.11808</link>
      <description>arXiv:2510.11808v1 Announce Type: new 
Abstract: We develop a structure-preserving numerical discretization for the electrostatic Euler-Poisson equations with a constant magnetic field. The scheme preserves positivity of the density, positivity of the internal energy and a minimum principle of the specific entropy, as well as global properties, such as total energy balance. The scheme uses an operator splitting approach composed of two subsystems: the compressible Euler equations of gas dynamics and a source system. The source system couples the electrostatic potential, momentum, and Lorentz force, thus incorporating electrostatic plasma and cyclotron motions. Because of the high-frequency phenomena it describes, the source system is discretized with an implicit time-stepping scheme. We use a PDE Schur complement approach for the numerical approximation of the solution of the source system. Therefore, it reduces to a single non-symmetric Poisson-like problem that is solved for each time step.
  Our focus with the present work is on the efficient solution of problems close to the magnetic-drift limit. Such asymptotic limit is characterized by the co-existence of slowly moving, smooth flows with very high-frequency oscillations, spanning timescales that differ by over 10 orders of magnitude, making their numerical solution quite challenging.
  We illustrate the capability of the scheme by computing a diocotron instability and present growth rates that compare favorably with existing analytical results. The model, though a simplified version of the Euler-Maxwell's system, represents a stepping stone toward electromagnetic solvers that are capable of working in the electrostatic and magnetic-drift limits, as well as the hydrodynamic regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11808v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Hoffart, Matthias Maier, John N. Shadid, Ignacio Tomas</dc:creator>
    </item>
    <item>
      <title>Recovery of Integer Images from Limited DFT Measurements with Lattice Methods</title>
      <link>https://arxiv.org/abs/2510.11949</link>
      <description>arXiv:2510.11949v1 Announce Type: new 
Abstract: Exact reconstruction of an image from measurements of its Discrete Fourier Transform (DFT) typically requires all DFT coefficients to be available. However, incorporating the prior assumption that the image contains only integer values enables unique recovery from a limited subset of DFT coefficients. This paper develops both theoretical and algorithmic foundations for this problem. We use algebraic properties of the DFT to define a reduction from two-dimensional recovery to several well-chosen one-dimensional recoveries. Our reduction framework characterizes the minimum number and location of DFT coefficients that must be sampled to guarantee unique reconstruction of an integer-valued image. Algorithmically, we develop reconstruction procedures which use dynamic programming to efficiently recover an integer signal or image from its minimal set of DFT measurements. While the new inversion algorithms still involve NP-hard subproblems, we demonstrate how the divide-and-conquer approach drastically reduces the associated search space. To solve the NP-hard subproblems, we employ a lattice-based framework which leverages the LLL approximation algorithm to make the algorithms fast and practical. We provide an analysis of the lattice method, suggesting approximate parameter choices to ensure correct inversion. Numerical results for the algorithms support the parameter analysis and demonstrate successful recovery of large integer images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11949v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Howard W Levinson, Isaac Viviano</dc:creator>
    </item>
    <item>
      <title>Construction of Basis Functions for the Geometry Conforming Immersed Finite Element Method</title>
      <link>https://arxiv.org/abs/2510.12018</link>
      <description>arXiv:2510.12018v1 Announce Type: new 
Abstract: The Frenet apparatus is a new framework for constructing high order geometry-conforming immersed finite element functions for interface problems. In this report, we present a procedure for constructing the local IFE bases in some detail as well as a new approach for constructing orthonormal bases using the singular value decomposition of the local generalized Vandermonde matrix. A sample implementation in MATLAB is provided to showcase the simplicity and extensionability of the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12018v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Slimane Adjerid, Tao Lin, Haroun Meghaichi</dc:creator>
    </item>
    <item>
      <title>Monte Carlo quasi-interpolation of spherical data</title>
      <link>https://arxiv.org/abs/2510.12027</link>
      <description>arXiv:2510.12027v1 Announce Type: new 
Abstract: We establish a deterministic and stochastic spherical quasi-interpolation framework featuring scaled zonal kernels derived from radial basis functions on the ambient Euclidean space. The method incorporates both quasi-Monte Carlo and Monte Carlo quadrature rules to construct easily computable quasi-interpolants, which provide efficient approximation to Sobolev-space functions for both clean and noisy data. To enhance the approximation power and robustness of our quasi-interpolants, we develop a multilevel method in which quasi-interpolants constructed with graded resolutions join force to reduce the error of approximation. In addition, we derive probabilistic concentration inequalities for our quasi-interpolants in pertinent stochastic settings. The construction of our quasi-interpolants does not require solving any linear system of equations. Numerical experiments show that our quasi-interpolation algorithm is more stable and robust against noise than comparable ones in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12027v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengjie Sun, Mengyuan Lv, Xingping Sun</dc:creator>
    </item>
    <item>
      <title>Stochastic Finite Volume Approximation with Clustering in the Parameter Space for Forward Uncertainty Quantification of PDEs with Random Parameters</title>
      <link>https://arxiv.org/abs/2510.12109</link>
      <description>arXiv:2510.12109v1 Announce Type: new 
Abstract: The uncertainty quantification (UQ) for partial differential equations (PDEs) with random parameters is important for science and engineering. Forward UQ quantifies the impact of random parameters on the solution or the quantity-of-interest (QoI). In the current study, we propose a new extension of the stochastic finite volume (SFV) method by clustering samples in the parameter space. Compared to classic SFV based on structured grid in the parameter space, the new scheme based on clustering extends SFV to parameter spaces of higher dimensions. This paper presents the construction of SFV schemes for typical parametric elliptic, parabolic and hyperbolic equations for Darcy flows in porous media, as well as the error analysis, demonstration and validation of the new extension using typical reservoir simulation test cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12109v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhao Zhang, Na Ou</dc:creator>
    </item>
    <item>
      <title>A priori error estimates for stable generalized finite element discretization of parabolic interface optimal control problems</title>
      <link>https://arxiv.org/abs/2510.12147</link>
      <description>arXiv:2510.12147v1 Announce Type: new 
Abstract: In this paper, we investigate optimal control problems governed by the parabolic interface equation, in which the control acts on the interface. The solution to this problem exhibits low global regularity due to the jump of the coefficient across the interface and the control acting on the interface. Consequently, the traditional finite element method fails to achieve optimal convergence rates when using a uniform mesh. To discretize the problem, we use fully discrete approximations based on the stable generalized finite element method for spatial discretization and the backward Euler scheme for temporal discretization, as well as variational discretization for the control variable. We prove a priori error estimates for the control, state, and adjoint state. Numerical examples are provided to support the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12147v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xindan Zhang, Jianping Zhao, Yanren Hou</dc:creator>
    </item>
    <item>
      <title>Optimal $L^2$ error estimation for the unfitted interface finite element method based on the non-symmetric Nitsche's methods</title>
      <link>https://arxiv.org/abs/2510.12151</link>
      <description>arXiv:2510.12151v1 Announce Type: new 
Abstract: This paper establishes optimal error estimates in the $L^2$ for the non-symmetric Nitsche method in an unfitted interface finite element setting. Extending our earlier work, we give a complete analysis for the Poisson interface model and, by formulating a tailored dual problem that restores adjoint consistency, derive the desired bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12151v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gang Chen, Chaoran Liu, Yangwen Zhang</dc:creator>
    </item>
    <item>
      <title>Spatial two-grid compact difference scheme for two-dimensional nonlinear diffusion-wave equations with variable exponent</title>
      <link>https://arxiv.org/abs/2510.12188</link>
      <description>arXiv:2510.12188v1 Announce Type: new 
Abstract: This paper presents a spatial two-grid (STG) compact difference scheme for a two-dimensional (2D) nonlinear diffusion-wave equation with variable exponent, which describes, e.g., the propagation of mechanical diffusive waves in viscoelastic media with varying material properties. Following the idea of the convolution approach, the diffusion-wave model is first transformed into an equivalent formulation. A fully discrete scheme is then developed by applying a compact difference approximation in space and combining the averaged product integration rule with linear interpolation quadrature in time. An efficient high-order two-grid algorithm is constructed by solving a small-scale nonlinear system on the coarse grid and a large-scale linearized system on the fine grid, where the bicubic spline interpolation operator is used to project coarse-grid solutions to the fine grid. Under mild assumptions on the variable exponent $\alpha(t)$, the stability and convergence of the STG compact difference scheme are rigorously established. Numerical experiments are finally presented to verify the accuracy and efficiency of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12188v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Zhang, Kexin Li, Wenlin Qiu</dc:creator>
    </item>
    <item>
      <title>Fully mixed virtual element schemes for steady-state poroelastic stress-assisted diffusion</title>
      <link>https://arxiv.org/abs/2510.12307</link>
      <description>arXiv:2510.12307v1 Announce Type: new 
Abstract: We propose a fully mixed virtual element method for the numerical approximation of the coupling between stress-altered diffusion and linear elasticity equations with strong symmetry of total poroelastic stress (using the Hellinger--Reissner principle). A novelty of this work is that we introduce a less restrictive assumption on the stress-assisted diffusion coefficient, requiring an analysis of the perturbed diffusion equation using Banach spaces. The solvability of the continuous and discrete problems is established using a suitable modification of the abstract theory for perturbed saddle-point problems in Banach spaces (which is in itself a new result of independent interest). In addition, we establish optimal a priori error estimates. The method and its analysis are robust with respect to the poromechanical parameters. We also include a number of numerical examples that illustrate the properties of the proposed scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12307v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac Bermudez, Bryan Gomez-Vargas, Andres E. Rubiano, Ricardo Ruiz-Baier</dc:creator>
    </item>
    <item>
      <title>On the maximum bound principle and energy dissipation of exponential time differencing methods for the chiral liquid crystal blue phases</title>
      <link>https://arxiv.org/abs/2510.12499</link>
      <description>arXiv:2510.12499v1 Announce Type: new 
Abstract: The blue phases are fascinating and complex states of chiral liquid crystals which can be modeled by a comprehensive framework of the Landau-de theory, satisfying energy dissipation and maximum bound principle.
  In this paper, we develop and analyze first and second order exponential time differencing numerical schemes for the gradient flow of the chiral liquid crystal blue phases, which preserve the maximum bound principle and energy dissipation unconditionally at the semi-discrete level.
  The fully discrete schemes are obtained coupled with the Fourier spectral method in space.
  And we propose a novel matrix-form Helmholtz basis transformation method to diagonalize the combined operator of the Laplacian and the curl operator, which is a key step in the implementation of the proposed schemes.
  Then by constructing auxiliary functions, we drive the $L^\infty$ boundedness of the numerical solutions and obtain the energy dissipation and the error estimates in $L^2$ and $L^\infty$ norm.
  Various numerical experiments are presented to validate the theoretical results and demonstrate the effectiveness of the proposed methods in simulating the dynamics of blue phases in chiral liquid crystals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12499v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenshuai Hu, Guanghua Ji</dc:creator>
    </item>
    <item>
      <title>Why the noise model matters: A performance gap in learned regularization</title>
      <link>https://arxiv.org/abs/2510.12521</link>
      <description>arXiv:2510.12521v1 Announce Type: new 
Abstract: This article addresses the challenge of learning effective regularizers for linear inverse problems. We analyze and compare several types of learned variational regularization against the theoretical benchmark of the optimal affine reconstruction, i.e. the best possible affine linear map for minimizing the mean squared error. It is known that this optimal reconstruction can be achieved using Tikhonov regularization, but this requires precise knowledge of the noise covariance to properly weight the data fidelity term. However, in many practical applications, noise statistics are unknown. We therefore investigate the performance of regularization methods learned without access to this noise information, focusing on Tikhonov, Lavrentiev, and quadratic regularization. Our theoretical analysis and numerical experiments demonstrate that for non-white noise, a performance gap emerges between these methods and the optimal affine reconstruction. Furthermore, we show that these different types of regularization yield distinct results, highlighting that the choice of regularizer structure is critical when the noise model is not explicitly learned. Our findings underscore the significant value of accurately modeling or co-learning noise statistics in data-driven regularization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12521v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Banert, Christoph Brauer, Dirk Lorenz, Lionel Tondji</dc:creator>
    </item>
    <item>
      <title>Easy-to-Implement One-Step Schemes for Stochastic Integration</title>
      <link>https://arxiv.org/abs/2510.12583</link>
      <description>arXiv:2510.12583v1 Announce Type: new 
Abstract: Convenient, easy to implement stochastic integration methods are developed on the basis of abstract one-step deterministic order $p$ integration techniques. The abstraction as an arbitrary one step map allows the inspection of easy to implement stochastic exponential time differencing Runge-Kutta (SETDRK), stochastic integrating factor Runge-Kutta (SIFRK) and stochastic RK (SRK) schemes. Such schemes require minimal modifications to existing deterministic schemes and converging to the Stratonovich SDE, whilst inheriting many of their desirable properties.
  These schemes capture all symmetric terms in the Stratonovich-Taylor expansion, are order $p$ in the limit of vanishing noise, can attain at least strong order $p/2$ or $p/2-1/2$ (parity dependent) for drift commutative noise, strong order $1$ for commutative noise, and strong order $1/2$ for multidimensional non-commutative noise. Numerical convergence is demonstrated using different bases of noise for 2nd, 3rd and 4th order SETDRK, SIFRK and SRK schemes for a stochastic KdV equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12583v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Woodfield, A. Lobbe</dc:creator>
    </item>
    <item>
      <title>IGA Laplace Eigenfrequencies Distributions and Estimations: Impact of Reparametrization on Eigenfrequency Behavior</title>
      <link>https://arxiv.org/abs/2510.12632</link>
      <description>arXiv:2510.12632v1 Announce Type: new 
Abstract: This work addresses the Galerkin isogeometric discretization of the one-dimensional Laplace eigenvalue problem subject to homogeneous Dirichlet boundary conditions on a bounded interval. We employ GLT theory to analyze the behavior of the eigenfrequencies when a reparametrization is applied to the computational domain. Under suitable assumptions on the reparametrization transformation, we prove that a structured pattern emerges in the distribution of eigenfrequencies when the problem is reframed through GLT-symbol analysis. Additionally, we establish results that refine and extend those of [3], including a uniform discrete Weyl's law. Furthermore, we derive several eigenfrequency estimates by establishing that the symbol exhibits asymptotically linear behavior near zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12632v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.SP</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lamsahel Noureddine, Abdeladim El Akri, Ahmed Ratnani</dc:creator>
    </item>
    <item>
      <title>nuGPR: GPU-Accelerated Gaussian Process Regression with Iterative Algorithms and Low-Rank Approximations</title>
      <link>https://arxiv.org/abs/2510.12128</link>
      <description>arXiv:2510.12128v1 Announce Type: cross 
Abstract: Gaussian Process Regression (GPR) is an important type of supervised machine learning model with inherent uncertainty measure in its predictions. We propose a new framework, nuGPR, to address the well-known challenge of high computation cost associated with GPR training. Our framework includes several ideas from numerical linear algebra to reduce the amount of computation in key steps of GPR, and we combine them to establish an end-to-end training algorithm. Specifically, we leverage the preconditioned conjugate gradient method to accelerate the convergence of the linear solves required in GPR. We exploit clustering in the input data to identify block-diagonal structure of the covariance matrix and subsequently construct low-rank approximations of the off-diagonal blocks. These enhancements significantly reduce the time and space complexity of our computations. In addition, unlike other frameworks that rely on exact differentiation, we employ numerical gradients to optimize the hyperparameters of our GPR model, further reducing the training cost by eliminating the need for backpropagation. Lastly, we leverage the CUDA Toolkit to efficiently parallelize the training procedure on NVIDIA GPUs. As a result, nuGPR reduces total training time by up to 2x and peak memory consumption by up to 12x on various synthetic and real-world datasets when compared to the best existing GPU-based GPR implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12128v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1137/24M1683615</arxiv:DOI>
      <arxiv:journal_reference>SIAM Journal on Scientific Computing, 2025, Vol. 47, No. 5, pp. B1250-B1271</arxiv:journal_reference>
      <dc:creator>Ziqi Zhao, Vivek Sarin</dc:creator>
    </item>
    <item>
      <title>Empirical bounds for commuting dilations of free unitaries and the universal commuting dilation constant</title>
      <link>https://arxiv.org/abs/2510.12540</link>
      <description>arXiv:2510.12540v1 Announce Type: cross 
Abstract: For a tuple $T$ of Hilbert space operators, the 'commuting dilation constant' is the smallest number $c$ such that the operators of $T$ are a simultaneous compression of commuting normal operators of norm at most $c$. We present numerical experiments giving a strong indication that the commuting dilation constant of a pair of independent random $N{\times}N$ unitary matrices converges to $\sqrt2$ as $N \to \infty$ almost surely. Under the assumption that this is the case, we prove that the commuting dilation constant of an arbitrary pair of contractions is strictly smaller than $2$. Our experiments are based on a simple algorithm that we introduce for the purpose of computing dilation constants between tuples of matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12540v1</guid>
      <category>math.FA</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malte Gerhold, Marcel Scherer, Orr Shalit</dc:creator>
    </item>
    <item>
      <title>Mathematical modeling of the mechanical behavior of three-layer plates with a tetrachiral honeycomb core</title>
      <link>https://arxiv.org/abs/2401.04781</link>
      <description>arXiv:2401.04781v2 Announce Type: replace 
Abstract: This work examines the mechanical behavior of three-layer plates with a tetrachiral honeycomb core and solid face layers under static bending conditions. The influence of discretization, relative density, and thickness of the honeycomb core on the stress state of the composite plates is investigated under two boundary conditions: rigid clamping and elastic rotational support. The first numerical experiment setup involves a constant thickness of each composite layer while varying the core relative density. The second experiment setup maintains a constant volume of the honeycomb core solid body, which causes its thickness to change as the relative density varies. Mathematical modeling is performed using the finite element method within the framework of linear elasticity, employing both three-dimensional modeling in Comsol Multiphysics and custom algorithms for solving a plane problem to analyze the stress state of the tetrachiral honeycomb-based multilayer plates. The technical process of manufacturing the composites is described, followed by laboratory tests under three-point bending conditions. Next, the diagrams showing the dependence of maximum stresses in the composite plate layers on the relative density and thickness of the honeycomb core are discussed in the first and second setups of the numerical experiments, respectively. The results demonstrate good agreement between the numerical data from the three-dimensional and plane finite element models. Furthermore, the laboratory data from the three-point bending tests qualitatively align with the numerical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04781v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>A. V. Mazaev</dc:creator>
    </item>
    <item>
      <title>Development of discontinuous Galerkin methods for hyperbolic systems that preserve a curl or a divergence constraint: the case of linear systems</title>
      <link>https://arxiv.org/abs/2405.04347</link>
      <description>arXiv:2405.04347v5 Announce Type: replace 
Abstract: Some hyperbolic systems are known to include implicit preservation of differential constraints: these are for example the time conservation of the curl or the divergence of a vector that appear as an implicit constraint. In this article, we show that this kind of constraint can be easily conserved at the discrete level with the classical discontinuous Galerkin method, provided the right approximation space is used for the vectorial space, and under some mild assumption on the numerical flux. For this, we recall a discrete de-Rham framework in which discontinuous approximation spaces for vectors fits. The discrete adjoint divergence and curl are proven to be exactly preserved by the discontinuous Galerkin method under a small assumption on the numerical flux. Numerical tests are performed on the wave system, the two dimensional Maxwell system and the induction equation, and confirm that the differential constraints are preserved at machine precision while keeping the high order of accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04347v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Perrier (CAGIRE, LMAP)</dc:creator>
    </item>
    <item>
      <title>Parameter optimization for restarted mixed precision iterative sparse solver</title>
      <link>https://arxiv.org/abs/2412.08059</link>
      <description>arXiv:2412.08059v5 Announce Type: replace 
Abstract: The problem of optimal precision switching for the conjugate gradient (CG) method applied to sparse linear systems is considered. A sparse matrix is defined as an $n\!\times\!n$ matrix with $m\!=\!O(n)$ nonzero entries. The algorithm first computes an approximate solution in single precision with tolerance $\varepsilon_1$, then switches to double precision to refine the solution to the required stopping tolerance $\varepsilon_2$. Based on estimates of system matrix parameters -- computed in time which does not exceed $1\%$ of the time needed to solve the system in double precision -- we determine the optimal value of $\varepsilon_1$ that minimizes total computation time. This value is obtained by classifying the matrix using the $k$-nearest neighbors method on a small precomputed sample. Classification relies on a feature vector comprising: the matrix size $n$, the number of nonzeros $m$, the pseudo-diameter of the matrix sparsity graph, and the average rate of residual norm decay during the early CG iterations in single precision. We show that, in addition to the matrix condition number, the diameter of the sparsity graph influences the growth of rounding errors during iterative computations. The proposed algorithm reduces the computational complexity of the CG -- expressed in equivalent double-precision iterations -- by more than $17\%$ on average across the considered matrix types in a sequential setting. The resulting speedup is at most $1.5\%$ worse than that achieved with the optimal (oracle) choice of $\varepsilon_1$.
  While the impact of matrix structure on Krylov subspace method convergence is well understood, the use of the sparsity graph diameter as a predictive feature for rounding error growth in mixed-precision CG appears to be novel. To the best of our knowledge, no prior work employs graph diameter to guide precision switching in iterative linear solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08059v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alexander V. Prolubnikov</dc:creator>
    </item>
    <item>
      <title>Finite Element Theory for PHIMATS</title>
      <link>https://arxiv.org/abs/2502.16283</link>
      <description>arXiv:2502.16283v3 Announce Type: replace 
Abstract: This document summarizes the main concepts of the finite element (FE) theory and constitutive relations as implemented in the open-source code phase-field multiphysics materials simulator PHIMATS https://github.com/ahcomat/PHIMATS. PHIMATS is written in C++ and uses Python for pre- and post-processing. It provides tools for discretizing the weak form of partial differential equations (PDE), interfacing with PETSc data structures (Vec/Mat) and solvers (KSP/SNES). The framework supports both single-physics and coupled multiphysics problems primarily using staggered coupling schemes. Hands-on examples can be found in the CaseStudies directory on GitHub repository. Rather than detailing the derivations of specific models, this document focuses on the key mathematical formulations and numerical strategies used within the implementation. For in-depth theoretical discussions, the reader is encouraged to consult the references. For citing this document, please use: [Abdelrahman Hussein. Finite Element Theory for PHIMATS. 2025. doi: 10.48550/ARXIV.2502.16283].</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16283v3</guid>
      <category>math.NA</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdelrahman Hussein</dc:creator>
    </item>
    <item>
      <title>Energy-conserving Kansa methods for Hamiltonian wave equations</title>
      <link>https://arxiv.org/abs/2507.04361</link>
      <description>arXiv:2507.04361v2 Announce Type: replace 
Abstract: We introduce a fast, constrained meshfree solver designed specifically to inherit energy conservation (EC) in second-order time-dependent Hamiltonian wave equations. For discretization, we adopt the Kansa method, also known as the kernel-based collocation method, combined with time-stepping. This approach ensures that the critical structural feature of energy conservation is maintained over time by embedding a quadratic constraint into the definition of the numerical solution. To address the computational challenges posed by the nonlinearity in the Hamiltonian wave equations and the EC constraint, we propose a fast iterative solver based on the Newton method with successive linearization. This novel solver significantly accelerates the computation, making the method highly effective for practical applications. Numerical comparisons with the traditional secant methods highlight the competitive performance of our scheme. These results demonstrate that our method not only conserves the energy but also offers a promising new direction for solving Hamiltonian wave equations more efficiently. While we focus on the Kansa method and corresponding convergence theories in this study, the proposed solver is based solely on linear algebra techniques and has the potential to be applied to EC constrained optimization problems arising from other PDE discretization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04361v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaobin Li, Meng Chen, Zhengjie Sun, Leevan Ling, Siqing Li</dc:creator>
    </item>
    <item>
      <title>DE-Sinc approximation for unilateral rapidly decreasing functions and its computational error bound</title>
      <link>https://arxiv.org/abs/2510.11411</link>
      <description>arXiv:2510.11411v2 Announce Type: replace 
Abstract: The Sinc approximation is known to be a highly efficient approximation formula for rapidly decreasing functions. For unilateral rapidly decreasing functions, which rapidly decrease as $x\to\infty$ but does not as $x\to-\infty$, an appropriate variable transformation makes the functions rapidly decreasing. As such a variable transformation, Stenger proposed $t = \sinh(\log(\operatorname{arsinh}(\exp x)))$, which enables the Sinc approximation to achieve root-exponential convergence. Recently, another variable transformation $t = 2\sinh(\log(\log(1+\exp x)))$ was proposed, which improved the convergence rate. Furthermore, its computational error bound was provided. However, the improvement is not significant because the convergence rate is still root-exponential order. To improve the convergence rate drastically, this study proposes a new transformation $t = 2\sinh(\log(\log(1+\exp(\pi\sinh x))))$, which is categorized as the double-exponential (DE) transformation. Furthermore, this study provides its computational error bound, which shows that the proposed approximation formula may achieve almost exponential convergence. Numerical experiments that confirm the theoretical result are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11411v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomoaki Okayama</dc:creator>
    </item>
    <item>
      <title>Logarithmic Mathematical Morphology: theory and applications</title>
      <link>https://arxiv.org/abs/2309.02007</link>
      <description>arXiv:2309.02007v2 Announce Type: replace-cross 
Abstract: In Mathematical Morphology for grey-level functions, an image is analysed by another image named the structuring function. This structuring function is translated over the image domain and summed to the image. However, in an image presenting lighting variations, the amplitude of the structuring function should vary according to the image intensity. Such a property is not verified in Mathematical Morphology for grey level functions, when the structuring function is summed to the image with the usual additive law. In order to address this issue, a new framework is defined with an additive law for which the amplitude of the structuring function varies according to the image amplitude. This additive law is chosen within the Logarithmic Image Processing framework and models the lighting variations with a physical cause such as a change of light intensity. The new framework is named Logarithmic Mathematical Morphology (LMM) and allows the definition of operators which are robust to such lighting variations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02007v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Noyel (CRESTIC)</dc:creator>
    </item>
    <item>
      <title>Exponential polynomials and identification of polygonal regions from Fourier samples</title>
      <link>https://arxiv.org/abs/2409.01432</link>
      <description>arXiv:2409.01432v2 Announce Type: replace-cross 
Abstract: Consider the set $E(D, N)$ of all bivariate exponential polynomials $$ f(\xi, \eta) = \sum_{j=1}^n p_j(\xi, \eta) e^{2\pi i (x_j\xi+y_j\eta)}, $$ where the polynomials $p_j \in \mathbb{C}[\xi, \eta]$ have degree $&lt;D$, $n\le N$ and where $x_j, y_j \in \mathbb{T} = \mathbb{R}/\mathbb{Z}$. We find a set $A \subseteq \mathbb{Z}^2$ that depends on $N$ and $D$ only and is of size $O(D^2 N \log N)$ such that the values of $f$ on $A$ determine $f$. Notice that the size of $A$ is only larger by a logarithmic quantity than the number of parameters needed to write down $f$.
  We use this in order to prove some uniqueness results about polygonal regions given a small set of samples of the Fourier Transform of their indicator function. If the number of different slopes of the edges of the polygonal region is $\le k$ then the region is determined from a predetermined set of Fourier samples that depends only on $k$ and the maximum number of vertices $N$ and is of size $O(k^2 N \log N)$. In the particular case where all edges are known to be parallel to the axes the polygonal region is determined from a set of $O(N \log N)$ Fourier samples that depends on $N$ only.
  Our methods are non-constructive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01432v2</guid>
      <category>math.CA</category>
      <category>cs.NA</category>
      <category>math.MG</category>
      <category>math.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mihail N. Kolountzakis, Emmanuil Spyridakis</dc:creator>
    </item>
    <item>
      <title>Galerkin Formulation of Path Integrals in Lattice Field Theory</title>
      <link>https://arxiv.org/abs/2411.15343</link>
      <description>arXiv:2411.15343v2 Announce Type: replace-cross 
Abstract: We present a mathematical framework for Galerkin formulations of path integrals in lattice field theory. The framework is based on using the degrees of freedom associated to a Galerkin discretization as the fundamental lattice variables. We formulate standard concepts in lattice field theory, such as the partition function and correlation functions, in terms of the degrees of freedom. For example, using continuous finite element spaces, we show that the two-point spatial correlation function can be defined between any two points on the domain (as opposed to at just lattice sites) and furthermore, that this satisfies a weak propagator (or Green's function) identity, in analogy to the continuum case. Furthermore, this framework leads naturally to higher-order formulations of lattice field theories by considering higher-order finite element spaces for the Galerkin discretization. We consider analytical and numerical examples of scalar field theory to investigate how increasing the order of piecewise polynomial finite element spaces affect the approximation of lattice observables. Finally, we sketch an outline of this Galerkin framework in the context of gauge field theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15343v2</guid>
      <category>hep-lat</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <pubDate>Wed, 15 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/1751-8121/ae1273</arxiv:DOI>
      <dc:creator>Brian K. Tran, Ben S. Southworth</dc:creator>
    </item>
  </channel>
</rss>
