<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Aug 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Field-theoretic approach to compartmental neuronal networks: impact of dendritic calcium spike-dependent bursting</title>
      <link>https://arxiv.org/abs/2508.08405</link>
      <description>arXiv:2508.08405v1 Announce Type: new 
Abstract: Neurons are spatially extended cells; different parts of a neuron have specific voltage dynamics. Important types of neurons even generate different spikes in different parts of the cell. Neurons' inputs are also often spatially compartmentalized, with different sources targeting different locations on the cell. Classic mean-field theories for neural population activity, however, rely on point-neuron models with at most one type of spike. Here, we develop a statistical field-theoretic approach to understanding collective activity in networks of compartmental neurons, including those generating multiple types of spikes. We use this to examine simple models of networks with thick-tufted layer 5 pyramidal cells, which generate calcium spikes in their apical dendrite when dendritic depolarization coincides with a back-propagating somatic spike. In the weakly-coupled regime, we uncover an exact mean-field limit for these networks that maps them to a marked point process. We use this mean-field limit to compare the impact of compartmentalized recurrent excitatory and inhibitory connectivity on the equilibrium phase diagram. This exposes regions of metastability between various activity states, including activity with silent vs active dendrites, with and without inhibitory activity, and oscillations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08405v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Audrey O'Brien Teasley, Gabriel Koch Ocker</dc:creator>
    </item>
    <item>
      <title>Multi-dimensional Neural Decoding with Orthogonal Representations for Brain-Computer Interfaces</title>
      <link>https://arxiv.org/abs/2508.08681</link>
      <description>arXiv:2508.08681v1 Announce Type: new 
Abstract: Current brain-computer interfaces primarily decode single motor variables, limiting their ability to support natural, high-bandwidth neural control that requires simultaneous extraction of multiple correlated motor dimensions. We introduce Multi-dimensional Neural Decoding (MND), a task formulation that simultaneously extracts multiple motor variables (direction, position, velocity, acceleration) from single neural population recordings. MND faces two key challenges: cross-task interference when decoding correlated motor dimensions from shared cortical representations, and generalization issues across sessions, subjects, and paradigms. To address these challenges, we propose OrthoSchema, a multi-task framework inspired by cortical orthogonal subspace organization and cognitive schema reuse. OrthoSchema enforces representation orthogonality to eliminate cross-task interference and employs selective feature reuse transfer for few-shot cross-session, subject and paradigm adaptation. Experiments on macaque motor cortex datasets demonstrate that OrthoSchema significantly improves decoding accuracy in cross-session, cross-subject and challenging cross-paradigm generalization tasks, with larger performance improvements when fine-tuning samples are limited. Ablation studies confirm the synergistic effects of all components are crucial, with OrthoSchema effectively modeling cross-task features and capturing session relationships for robust transfer. Our results provide new insights into scalable and robust neural decoding for real-world BCI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08681v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaixi Tian, Shengjia Zhao, Yuhan Zhang, Shan Yu</dc:creator>
    </item>
    <item>
      <title>Neural correlates of learned categorical perception of visual stimuli with local features</title>
      <link>https://arxiv.org/abs/2508.08922</link>
      <description>arXiv:2508.08922v1 Announce Type: new 
Abstract: Learning to categorize requires distinguishing category members from non-members by detecting the features that covary with membership. Whether this process can induce changes in perception is still a matter of debate. In prior studies, wereported Learned Categorical Perception (Learned CP) effects in the form of between-category separation and within-category compression in perceived similarity after training subjects to categorize visual stimuli with distributed, holistic features.These effects were correlated with changes in an early, perceptual component of Event Related Potentials (ERPs). Using the same methodology, in this experiment we trained 96 subjects to sort line drawings of fish with local, verbalizable features into two categories by trial and error with corrective feedback. We tested for Learned CP effects and their neural correlates by measuring subjects' pairwise dissimilarity judgments and(ERPs) before and after the training. With the same frequency of trials and feedback,about 40% of the participants succeeded in learning the categories ("learners") while the rest did not ("non-learners"). Learners showed a) significant between-category separation and within-category compression after training compared to before and b) an increase in a late parietal ERP positivity (LPC; 400-600 ms) and an early, occipital positivity (P1; 80-140 ms), correlated with categorization accuracy and degree of Learned CP. These behavioral and neural changes after learning a category, absent in Non Learners, provide further evidence that category learning can modulate perceptual processes, regardless of the nature of the visual features. We complement our experimental results with a neural net model using the same stimuli.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08922v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F. P\'erez-Gay, T. Sicotte, N. Goulet, X. Kang, S. Harnad</dc:creator>
    </item>
    <item>
      <title>Understanding Transformers through the Lens of Pavlovian Conditioning</title>
      <link>https://arxiv.org/abs/2508.08289</link>
      <description>arXiv:2508.08289v1 Announce Type: cross 
Abstract: Transformer architectures have revolutionized artificial intelligence (AI) through their attention mechanisms, yet the computational principles underlying their success remain opaque. We present a novel theoretical framework that reinterprets the core computation of attention as Pavlovian conditioning. Our model finds a direct mathematical analogue in linear attention, which simplifies the analysis of the underlying associative process. We demonstrate that attention's queries, keys, and values can be mapped to the three elements of classical conditioning: test stimuli that probe associations, conditional stimuli (CS) that serve as retrieval cues, and unconditional stimuli (US) that contain response information. Through this lens, we suggest that each attention operation constructs a transient associative memory via a Hebbian rule, where CS-US pairs form dynamic associations that test stimuli can later retrieve. Our framework yields several theoretical insights grounded in this linearized model: (1) a capacity theorem showing that attention heads can store O($\sqrt{d_k}$) associations before interference degrades retrieval; (2) an error propagation analysis revealing fundamental architectural trade-offs of balancing model depth, width, and head redundancy to maintain reliability; and (3) an understanding of how biologically plausible learning rules could enhance transformer architectures. By establishing this deep connection, we suggest that the success of modern AI may stem not from architectural novelty alone, but from implementing computational principles that biology optimized over millions of years of evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08289v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mu Qiao</dc:creator>
    </item>
    <item>
      <title>Fast weight programming and linear transformers: from machine learning to neurobiology</title>
      <link>https://arxiv.org/abs/2508.08435</link>
      <description>arXiv:2508.08435v1 Announce Type: cross 
Abstract: Recent advances in artificial neural networks for machine learning, and language modeling in particular, have established a family of recurrent neural network (RNN) architectures that, unlike conventional RNNs with vector-form hidden states, use two-dimensional (2D) matrix-form hidden states. Such 2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a neural network whose synaptic weights (called fast weights) dynamically change over time as a function of input observations, and serve as short-term memory storage; corresponding synaptic weight modifications are controlled or programmed by another network (the programmer) whose parameters are trained (e.g., by gradient descent). In this Primer, we review the technical foundations of FWPs, their computational characteristics, and their connections to transformers and state space models. We also discuss connections between FWPs and models of synaptic plasticity in the brain, suggesting a convergence of natural and artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08435v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kazuki Irie, Samuel J. Gershman</dc:creator>
    </item>
    <item>
      <title>Emergence: from physics to biology, sociology, and computer science</title>
      <link>https://arxiv.org/abs/2508.08548</link>
      <description>arXiv:2508.08548v1 Announce Type: cross 
Abstract: Many systems involve numerous interacting parts and the whole system can have properties that the individual parts do not. I take this novelty as the defining characteristic of an emergent property. Other characteristics associated with emergence discussed include universality, order, complexity, unpredictability, irreducibility, diversity, self-organisation, discontinuities, and singularities.
  Emergent phenomena are widespread across physics, biology, social sciences, and computing, and are central to major scientific and societal challenges.
  Understanding emergence involves considering the stratification of reality across different scales (energy, time, length, complexity), each with its distinct ontology and epistemology, leading to semi-autonomous scientific disciplines. A central challenge is bridging the gap between macroscopic emergent properties and microscopic component interactions. Identifying an intermediate mesoscopic scale where new, weakly interacting entities or modular structures emerge is key.
  Theoretical approaches, such as effective theories (describing phenomena at a specific scale) and toy models (simplified systems for analysis), are vital. The Ising model exemplifies how toy models can elucidate emergence characteristics. Emergence is central to condensed matter physics, chaotic systems, fluid dynamics, nuclear physics, quantum gravity, neural networks, protein folding, and social segregation.
  An emergent perspective should influence scientific strategy by shaping research questions, methodologies, priorities, and resource allocation. An elusive goal is the design and control of emergent properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08548v1</guid>
      <category>physics.hist-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.NC</category>
      <category>quant-ph</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ross H. McKenzie</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence Software Structured to Simulate Human Working Memory, Mental Imagery, and Mental Continuity</title>
      <link>https://arxiv.org/abs/2204.05138</link>
      <description>arXiv:2204.05138v2 Announce Type: replace 
Abstract: This article presents an artificial intelligence (AI) architecture intended to simulate the iterative updating of the human working memory system. It features several interconnected neural networks designed to emulate the specialized modules of the cerebral cortex. These are structured hierarchically and integrated into a global workspace. They are capable of temporarily maintaining high-level representational patterns akin to the psychological items maintained in working memory. This maintenance is made possible by persistent neural activity in the form of two modalities: sustained neural firing (resulting in a focus of attention) and synaptic potentiation (resulting in a short-term store). Representations held in persistent activity are recursively replaced resulting in incremental changes to the content of the working memory system. As this content gradually evolves, successive processing states overlap and are continuous with one another. The present article will explore how this architecture can lead to iterative shift in the distribution of coactive representations, ultimately leading to mental continuity between processing states, and thus to human-like thought and cognition. Like the human brain, this AI working memory store will be linked to multiple imagery (topographic map) generation systems corresponding to various sensory modalities. As working memory is iteratively updated, the maps created in response will construct sequences of related mental imagery. Thus, neural networks emulating the prefrontal cortex and its reciprocal interactions with early sensory and motor cortex capture the imagery guidance functions of the human brain. This sensory and motor imagery creation, coupled with an iteratively updated working memory store may provide an AI system with the cognitive assets needed to achieve synthetic consciousness or artificial sentience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.05138v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.SC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jared Edward Reser</dc:creator>
    </item>
    <item>
      <title>Effective and anatomical connectivity of the dorso-central insula during the processing of action forms</title>
      <link>https://arxiv.org/abs/2310.14213</link>
      <description>arXiv:2310.14213v3 Announce Type: replace 
Abstract: In both human and monkeys the observation and execution of actions produced the activation of a network consisting of parietal and frontal areas. Although this network is involved in the encoding of the action goal, it does not consider the affective component of the action: vitality form (VF). Several studies showed that the observation and execution of actions conveying VFs selectively activated the dorso-central insula (DCI). In the present study, we aimed to clarify, by using Dynamic Causal Modeling (DCM), the direction of the information flow across DCI, parieto-frontal areas (PMv, IPL) and posterior superior temporal sulcus (pSTS) during both observation and execution of actions conveying VFs. Results indicate that, during observation, DCI receives the visual input from pSTS, and, in turn, sends it to the fronto-parietal network. Moreover, DCI significantly modulates PMv. Conversely, during execution, the motor input starts from PMv, reaches DCI and IPL, with a significant modulation from PMv to DCI. The reciprocal exchange of information between PMv and DCI suggests that these areas work closely together in the VFs action processing. An additional tractography analysis corroborates our DCM models, showing a correspondence between functional connections and anatomical tracts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14213v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G. Di Cesare, P. Zeidman, G. Lombardi, B. A. Urgen, G. Lombardi, A. Sciutti</dc:creator>
    </item>
    <item>
      <title>Diverging network architecture of the $\textit{C. elegans}$ connectome and signaling network</title>
      <link>https://arxiv.org/abs/2412.14498</link>
      <description>arXiv:2412.14498v2 Announce Type: replace 
Abstract: The connectome describes the complete set of synaptic contacts through which neurons communicate. While the architecture of the $\textit{C. elegans}$ connectome has been extensively characterized, much less is known about the organization of causal signaling networks arising from functional interactions between neurons. Understanding how effective communication pathways relate to or diverge from the underlying structure is a central question in neuroscience. Here, we analyze the modular architecture of the $\textit{C. elegans}$ signal propagation network, measured via calcium imaging and optogenetics, and compare it to the underlying anatomical wiring measured by electron microscopy. Compared to the connectome, we find that signaling modules are not aligned with the modular boundaries of the anatomical network, highlighting an instance where function deviates from structure. However, we find that some of the most striking features of the anatomical network are preserved, as exemplified by the pharynx, which is delineated into a separate community in both anatomy and signaling. We analyze the cellular compositions of the signaling architecture and find that its modules are enriched for specific cell types and functions, suggesting that the network modules are neurobiologically relevant. Lastly, we identify a "rich club" of hub neurons in the signaling network. The membership of the signaling rich club differs from the rich club detected in the anatomical network, challenging the view that structural hubs occupy positions of influence in functional (signaling) networks. Our results provide new insight into the interplay between brain structure, in the form of a complete synaptic-level connectome, and brain function, in the form of a system-wide causal signal propagation atlas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14498v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1103/6wgv-b9m6</arxiv:DOI>
      <dc:creator>Sophie Dvali, Caio Seguin, Richard Betzel, Andrew M. Leifer</dc:creator>
    </item>
    <item>
      <title>Synaptic plasticity alters the nature of chaos transition in neural networks</title>
      <link>https://arxiv.org/abs/2412.15592</link>
      <description>arXiv:2412.15592v2 Announce Type: replace 
Abstract: In realistic neural circuits, both neurons and synapses are coupled in dynamics with separate time scales. The circuit functions are intimately related to these coupled dynamics. However, it remains challenging to understand the intrinsic properties of the coupled dynamics. Here, we develop the neuron-synapse coupled quasi-potential method to demonstrate how learning induces the qualitative change in macroscopic behaviors of recurrent neural networks. We find that under the Hebbian learning, a large Hebbian strength will alter the nature of the chaos transition, from a continuous type to a discontinuous type, where the onset of chaos requires a smaller synaptic gain compared to the non-plastic counterpart network. In addition, our theory predicts that under feedback and homeostatic learning, the location and type of chaos transition are retained, and only the chaotic fluctuation is adjusted. Our theoretical calculations are supported by numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15592v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenkang Du, Haiping Huang</dc:creator>
    </item>
  </channel>
</rss>
