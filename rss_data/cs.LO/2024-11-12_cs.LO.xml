<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Nov 2024 02:44:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Automated Strategy Invention for Confluence of Term Rewrite Systems</title>
      <link>https://arxiv.org/abs/2411.06409</link>
      <description>arXiv:2411.06409v1 Announce Type: new 
Abstract: Term rewriting plays a crucial role in software verification and compiler optimization. With dozens of highly parameterizable techniques developed to prove various system properties, automatic term rewriting tools work in an extensive parameter space. This complexity exceeds human capacity for parameter selection, motivating an investigation into automated strategy invention. In this paper, we focus on confluence, an important property of term rewrite systems, and apply machine learning to develop the first learning-guided automatic confluence prover. Moreover, we randomly generate a large dataset to analyze confluence for term rewrite systems. Our results focus on improving the state-of-the-art automatic confluence prover CSI: When equipped with our invented strategies, it surpasses its human-designed strategies both on the augmented dataset and on the original human-created benchmark dataset Cops, proving/disproving the confluence of several term rewrite systems for which no automated proofs were known before.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06409v1</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liao Zhang, Fabian Mitterwallner, Jan Jakubuv, Cezary Kaliszyk</dc:creator>
    </item>
    <item>
      <title>Finite Variable Counting Logics with Restricted Requantification</title>
      <link>https://arxiv.org/abs/2411.06944</link>
      <description>arXiv:2411.06944v1 Announce Type: new 
Abstract: Counting logics with a bounded number of variables form one of the central concepts in descriptive complexity theory. Although they restrict the number of variables that a formula can contain, the variables can be nested within scopes of quantified occurrences of themselves. In other words, the variables can be requantified. We study the fragments obtained from counting logics by restricting requantification for some but not necessarily all the variables. Similar to the logics without limitation on requantification, we develop tools to investigate the restricted variants. Specifically, we introduce a bijective pebble game in which certain pebbles can only be placed once and for all, and a corresponding two-parametric family of Weisfeiler-Leman algorithms. We show close correspondences between the three concepts. By using a suitable cops-and-robber game and adaptations of the Cai-F\"urer-Immerman construction, we completely clarify the relative expressive power of the new logics. We show that the restriction of requantification has beneficial algorithmic implications in terms of graph identification. Indeed, we argue that with regard to space complexity, non-requantifiable variables only incur an additive polynomial factor when testing for equivalence. In contrast, for all we know, requantifiable variables incur a multiplicative linear factor. Finally, we observe that graphs of bounded tree-depth and 3-connected planar graphs can be identified using no, respectively, only a very limited number of requantifiable variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06944v1</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Ra{\ss}mann, Georg Schindling, Pascal Schweitzer</dc:creator>
    </item>
    <item>
      <title>Translation of Temporal Logic for Efficient Infinite-State Reactive Synthesis (Full Version)</title>
      <link>https://arxiv.org/abs/2411.07078</link>
      <description>arXiv:2411.07078v1 Announce Type: new 
Abstract: Infinite-state reactive synthesis has attracted significant attention in recent years, which has led to the emergence of novel symbolic techniques for solving infinite-state games. Temporal logics featuring variables over infinite domains offer an expressive high-level specification language for infinite-state reactive systems. Currently, the only way to translate these temporal logics into symbolic games is by naively encoding the specification to use techniques designed for the Boolean case. An inherent limitation of this approach is that it results in games in which the semantic structure of the temporal and first-order constraints present in the formula is lost. There is a clear need for techniques that leverage this information in the translation process to speed up solving the generated games.
  In this work, we propose the first approach that addresses this gap. Our technique constructs a monitor incorporating first-order and temporal reasoning at the formula level, enriching the constructed game with semantic information that leads to more efficient solving. We demonstrate that thanks to this, our method outperforms the state-of-the-art techniques across a range of benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07078v1</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe Heim, Rayna Dimitrova</dc:creator>
    </item>
    <item>
      <title>Quantifying artificial intelligence through algebraic generalization</title>
      <link>https://arxiv.org/abs/2411.05943</link>
      <description>arXiv:2411.05943v1 Announce Type: cross 
Abstract: The rapid development of modern artificial intelligence (AI) systems has created an urgent need for their scientific quantification. While their fluency across a variety of domains is impressive, modern AI systems fall short on tests requiring symbolic processing and abstraction - a glaring limitation given the necessity for interpretable and reliable technology. Despite a surge of reasoning benchmarks emerging from the academic community, no comprehensive and theoretically-motivated framework exists to quantify reasoning (and more generally, symbolic ability) in AI systems. Here, we adopt a framework from computational complexity theory to explicitly quantify symbolic generalization: algebraic circuit complexity. Many symbolic reasoning problems can be recast as algebraic expressions. Thus, algebraic circuit complexity theory - the study of algebraic expressions as circuit models (i.e., directed acyclic graphs) - is a natural framework to study the complexity of symbolic computation. The tools of algebraic circuit complexity enable the study of generalization by defining benchmarks in terms of their complexity-theoretic properties (i.e., the difficulty of a problem). Moreover, algebraic circuits are generic mathematical objects; for a given algebraic circuit, an arbitrarily large number of samples can be generated for a specific circuit, making it an optimal testbed for the data-hungry machine learning algorithms that are used today. Here, we adopt tools from algebraic circuit complexity theory, apply it to formalize a science of symbolic generalization, and address key theoretical and empirical challenges for its successful application to AI science and its impact on the broader community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05943v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Ito, Murray Campbell, Lior Horesh, Tim Klinger, Parikshit Ram</dc:creator>
    </item>
    <item>
      <title>The subTuring degrees</title>
      <link>https://arxiv.org/abs/2411.06043</link>
      <description>arXiv:2411.06043v1 Announce Type: cross 
Abstract: In this article, we introduce a notion of reducibility for partial functions on the natural numbers, which we call subTuring reducibility. One important aspect is that the subTuring degrees correspond to the structure of the realizability subtoposes of the effective topos. We show that the subTuring degrees (that is, the realizability subtoposes of the effective topos) form a dense non-modular (thus, non-distributive) lattice. We also show that there is a nonzero join-irreducible subTuring degree (which implies that there is a realizability subtopos of the effective topos that cannot be decomposed into two smaller realizability subtoposes).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06043v1</guid>
      <category>math.LO</category>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takayuki Kihara, Keng Meng Ng</dc:creator>
    </item>
    <item>
      <title>Generically Automating Separation Logic by Functors, Homomorphisms and Modules</title>
      <link>https://arxiv.org/abs/2411.06094</link>
      <description>arXiv:2411.06094v1 Announce Type: cross 
Abstract: Foundational verification considers the functional correctness of programming languages with formalized semantics and uses proof assistants (e.g., Coq, Isabelle) to certify proofs. The need for verifying complex programs compels it to involve expressive Separation Logics (SLs) that exceed the scopes of well-studied automated proof theories, e.g., symbolic heap. Consequently, automation of SL in foundational verification relies heavily on ad-hoc heuristics that lack a systematic meta-theory and face scalability issues. To mitigate the gap, we propose a theory to specify SL predicates using abstract algebras including functors, homomorphisms, and modules over rings. Based on this theory, we develop a generic SL automation algorithm to reason about any data structures that can be characterized by these algebras. In addition, we also present algorithms for automatically instantiating the algebraic models to real data structures. The instantiation reuses the algebraic models of component structures and preserves their data abstractions. Case studies on formalized imperative semantics show our algorithm can instantiate the algebraic models automatically for a variety of complex data structures. Experimental results indicate the automatically instantiated reasoners from our generic theory show similar results to the state-of-the-art systems made of specifically crafted reasoning rules. The presented theories, proofs, and the verification framework are formalized in Isabelle/HOL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06094v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Qiyuan Xu, David Sanan, Zhe Hou, Xiaokun Luan, Conrad Watt, Yang Liu</dc:creator>
    </item>
    <item>
      <title>A Taxonomy of Hoare-Like Logics: Towards a Holistic View using Predicate Transformers and Kleene Algebras with Top and Tests</title>
      <link>https://arxiv.org/abs/2411.06416</link>
      <description>arXiv:2411.06416v1 Announce Type: cross 
Abstract: We study Hoare-like logics, including partial and total correctness Hoare logic, incorrectness logic, Lisbon logic, and many others through the lens of predicate transformers \`a la Dijkstra and through the lens of Kleene algebra with top and tests (TopKAT). Our main goal is to give an overview - a taxonomy - of how these program logics relate, in particular under different assumptions like for example program termination, determinism, and reversibility. As a byproduct, we obtain a TopKAT characterization of Lisbon logic, which - to the best of our knowledge - is a novel result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06416v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lena Verscht, Benjamin Lucien Kaminski</dc:creator>
    </item>
    <item>
      <title>The internal languages of univalent categories</title>
      <link>https://arxiv.org/abs/2411.06636</link>
      <description>arXiv:2411.06636v1 Announce Type: cross 
Abstract: Internal language theorems are fundamental in categorical logic, since they express an equivalence between syntax and semantics. One of such theorems was proven by Clairambault and Dybjer, who corrected the result originally by Seely. More specifically, they constructed a biequivalence between the bicategory of locally Cartesian closed categories and the bicategory of democratic categories with families with extensional identity types, $\sum$-types, and $\prod$-types. This theorem expresses that the internal language of locally Cartesian closed is extensional Martin-L\"of type theory with dependent sums and products. In this paper, we study the theorem by Clairambault and Dybjer for univalent categories, and we extend it to various classes of toposes, among which are $\prod$-pretoposes and elementary toposes. The results in this paper have been formalized using the proof assistant Coq and the UniMath library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06636v1</guid>
      <category>math.CT</category>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niels van der Weide</dc:creator>
    </item>
    <item>
      <title>FCA using the Concept Explorer in 2024</title>
      <link>https://arxiv.org/abs/2411.06675</link>
      <description>arXiv:2411.06675v1 Announce Type: cross 
Abstract: In this note we give a very short introduction to Formal Concept Analysis, accompanied by an example in order to build concept lattices from a context. We build the lattice using the Java-based software Concept Explorer (ConExp) in a recent version of Linux. Installing an appropriate Java version is necessary, because ConExp was developed some time ago using a Sun Java version, which is not open-source. As a result, it has been observed that ConExp will not build a lattice when started with an open-source Java version. Therefore, we also sketch the procedure we followed to install an appropriate Java version which makes ConExp work again, i.e., to "build lattices again". We also show how to start ConExp with a 32 bit Java version, which requires a few additional libraries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06675v1</guid>
      <category>math.LO</category>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edith Vargas-Garc\'Ia, Andreas Wachtel</dc:creator>
    </item>
    <item>
      <title>Pushing the Limit: Verified Performance-Optimal Causally-Consistent Database Transactions</title>
      <link>https://arxiv.org/abs/2411.07049</link>
      <description>arXiv:2411.07049v1 Announce Type: cross 
Abstract: Modern web services crucially rely on high-performance distributed databases, where concurrent transactions are isolated from each other using concurrency control protocols. Relaxed isolation levels, which permit more complex concurrent behaviors than strong levels like serializability, are used in practice for higher performance and availability.
  In this paper, we present Eiger-PORT+, a concurrency control protocol that achieves a strong form of causal consistency, called TCCv (Transactional Causal Consistency with convergence). We show that Eiger-PORT+ also provides performance-optimal read transactions in the presence of transactional writes, thus refuting an open conjecture that this is impossible for TCCv. We also deductively verify that Eiger-PORT+ satisfies this isolation level by refining an abstract model of transactions. This yields the first deductive verification of a complex concurrency control protocol. Furthermore, we conduct a performance evaluation showing Eiger-PORT+'s superior performance over the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07049v1</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shabnam Ghasemirad, Christoph Sprenger, Si Liu, Luca Multazzu, David Basin</dc:creator>
    </item>
    <item>
      <title>Stochastic Processes with Expected Stopping Time</title>
      <link>https://arxiv.org/abs/2104.07278</link>
      <description>arXiv:2104.07278v5 Announce Type: replace 
Abstract: Markov chains are the de facto finite-state model for stochastic dynamical systems, and Markov decision processes (MDPs) extend Markov chains by incorporating non-deterministic behaviors. Given an MDP and rewards on states, a classical optimization criterion is the maximal expected total reward where the MDP stops after T steps, which can be computed by a simple dynamic programming algorithm. We consider a natural generalization of the problem where the stopping times can be chosen according to a probability distribution, such that the expected stopping time is T, to optimize the expected total reward. Quite surprisingly we establish inter-reducibility of the expected stopping-time problem for Markov chains with the Positivity problem (which is related to the well-known Skolem problem), for which establishing either decidability or undecidability would be a major breakthrough. Given the hardness of the exact problem, we consider the approximate version of the problem: we show that it can be solved in exponential time for Markov chains and in exponential space for MDPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.07278v5</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krishnendu Chatterjee, Laurent Doyen</dc:creator>
    </item>
    <item>
      <title>Confluence Criteria for Logically Constrained Rewrite Systems (Full Version)</title>
      <link>https://arxiv.org/abs/2309.12112</link>
      <description>arXiv:2309.12112v2 Announce Type: replace 
Abstract: Numerous confluence criteria for plain term rewrite systems are known. For logically constrained rewrite system, an attractive extension of term rewriting in which rules are equipped with logical constraints, much less is known. In this paper we extend the strongly-closed and (almost) parallel-closed critical pair criteria of Huet and Toyama to the logically constrained setting. We discuss the challenges for automation and present crest, a new tool for logically constrained rewriting in which the confluence criteria are implemented, together with experimental data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12112v2</guid>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-38499-8_27</arxiv:DOI>
      <dc:creator>Jonas Sch\"opf, Aart Middeldorp</dc:creator>
    </item>
    <item>
      <title>Using Rely/Guarantee to Pinpoint Assumptions underlying Security Protocols</title>
      <link>https://arxiv.org/abs/2311.15189</link>
      <description>arXiv:2311.15189v3 Announce Type: replace 
Abstract: The verification of security protocols is essential, in order to ensure the absence of potential attacks. However, verification results are only valid with respect to the assumptions under which the verification was performed. These assumptions are often hidden and are difficult to identify, making it unclear whether a given protocol is safe to deploy into a particular environment. Rely/guarantee provides a mechanism for abstractly reasoning about the interference from the environment. Using this approach, the assumptions are made clear and precise. This paper investigates this approach on the Needham-Schroeder Public Key protocol, showing that the technique can effectively uncover the assumptions under which the protocol can withstand attacks from intruders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15189v3</guid>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nisansala P. Yatapanage, Cliff B. Jones</dc:creator>
    </item>
    <item>
      <title>Proof-theoretic Semantics for the Logic of Bunched Implications</title>
      <link>https://arxiv.org/abs/2311.16719</link>
      <description>arXiv:2311.16719v2 Announce Type: replace 
Abstract: The logic of bunched implications (BI) can be seen as the free combination of intuitionistic propositional logic (IPL) and intuitionistic multiplicative linear logic (IMLL). We present here a base-extension semantics (B-eS) for BI in the spirit of Sandqvist's B-eS for IPL, deferring an analysis of proof-theoretic validity, in the sense of Dummett and Prawitz, to another occasion. Essential to BI's formulation in proof-theoretic terms is the concept of a `bunch' of hypotheses that is familiar from relevance logic. Bunches amount to trees whose internal vertices are labelled with either the IMLL context-former or the IPL context-former and whose leaves are labelled with propositions or units for the context-formers. This structure presents significant technical challenges in setting up a base-extension semantics for BI. Our approach starts from the B-eS for IPL and the B-eS for IMLL and provides a systematic combination. Such a combination requires that base rules carry bunched structure, and so requires a more complex notion of derivability in a base and a correspondingly richer notion of support in a base. One reason why BI is a substructural logic of interest is that the `resource interpretation' of its semantics, given in terms of sharing and separation and which gives rise to Separation Logic in the field of program verification, is quite distinct from the `number-of-uses' reading of the propositions of linear logic as resources. This resource reading of BI provides useful intuitions in the formulation of its proof-theoretic semantics. We discuss a simple example of the use of the given B-eS in security modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16719v2</guid>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Studia Logica 2024</arxiv:journal_reference>
      <dc:creator>Tao Gu, Alexander V. Gheorghiu, David J. Pym</dc:creator>
    </item>
    <item>
      <title>Bisimulations and Logics for Higher-Dimensional Automata</title>
      <link>https://arxiv.org/abs/2402.01589</link>
      <description>arXiv:2402.01589v3 Announce Type: replace 
Abstract: Higher-dimensional automata (HDAs) are models of non-interleaving concurrency for analyzing concurrent systems. There is a rich literature that deals with bisimulations for concurrent systems, and some of them have been extended to HDAs. However, no logical characterizations of these relations are currently available for HDAs.
  In this work, we address this gap by introducing Ipomset modal logic, a Hennessy-Milner type logic over HDAs, and show that it characterizes Path-bisimulation, a variant of the standard ST-bisimulation. We also define a notion of Cell-bisimulation, using the open-maps framework of Joyal, Nielsen, and Winskel, and establish the relationship between these bisimulations (and also their "strong" variants, which take restrictions into account). In our work, we rely on the new categorical definition of HDAs as presheaves over concurrency lists and on track objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01589v3</guid>
      <category>cs.LO</category>
      <category>cs.FL</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Safa Zouari, Krzysztof Ziemia\'nski, Uli Fahrenberg</dc:creator>
    </item>
    <item>
      <title>Inferentialist Resource Semantics</title>
      <link>https://arxiv.org/abs/2402.09217</link>
      <description>arXiv:2402.09217v5 Announce Type: replace 
Abstract: In systems modelling, a 'system' typically comprises located resources relative to which processes execute. One important use of logic in informatics is in modelling such systems for the purpose of reasoning (perhaps automated) about their behaviour and properties. To this end, one requires an interpretation of logical formulae in terms of the resources and states of the system; such an interpretation is called a 'resource semantics' of the logic. This paper shows how inferentialism -- the view that meaning is given in terms of inferential behaviour -- enables a versatile and expressive framework for resource semantics. Specifically, how inferentialism seamlessly incorporates the assertion-based approach of the logic of Bunched Implications, foundational in program verification (e.g., as the basis of Separation Logic), and the renowned number-of-uses reading of Linear Logic. This integration enables reasoning about shared and separated resources in intuitive and familiar ways, as well as about the composition and interfacing of system components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09217v5</guid>
      <category>cs.LO</category>
      <category>cs.CR</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>MFPS 2024</arxiv:journal_reference>
      <dc:creator>Alexander V. Gheorghiu, Tao Gu, David J. Pym</dc:creator>
    </item>
    <item>
      <title>Parametricity via Cohesion</title>
      <link>https://arxiv.org/abs/2404.03825</link>
      <description>arXiv:2404.03825v2 Announce Type: replace 
Abstract: Parametricity is a key metatheoretic property of type systems, which implies strong uniformity &amp; modularity properties of the structure of types within systems possessing it. In recent years, various systems of dependent type theory have emerged with the aim of expressing such parametric reasoning in their internal logic, toward the end of solving various problems arising from the complexity of higher-dimensional coherence conditions in type theory. This paper presents a first step toward the unification, simplification, and extension of these various methods for internalizing parametricity. Specifically, I argue that there is an essentially modal aspect of parametricity, which is intimately connected with the category-theoretic concept of cohesion. On this basis, I describe a general categorical semantics for modal parametricity, develop a corresponding framework of axioms (with computational interpretations) in dependent type theory that can be used to internally represent and reason about such parametricity, and show this in practice by implementing these axioms in Agda and using them to verify parametricity theorems therein. I then demonstrate the utility of these axioms in managing the complexity of higher-dimensional coherence by deriving induction principles for higher inductive types, and in closing, I sketch the outlines of a more general synthetic theory of parametricity, with applications in domains ranging from homotopy type theory to the analysis of program modules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03825v2</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <category>math.CT</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C. B. Aberl\'e</dc:creator>
    </item>
    <item>
      <title>Almost-Sure Termination by Guarded Refinement</title>
      <link>https://arxiv.org/abs/2404.08494</link>
      <description>arXiv:2404.08494v2 Announce Type: replace 
Abstract: Almost-sure termination is an important correctness property for probabilistic programs, and a number of program logics have been developed for establishing it. However, these logics have mostly been developed for first-order programs written in languages with specific syntactic patterns for looping. In this paper, we consider almost-sure termination for higher-order probabilistic programs with general references. This combination of features allows for recursion and looping to be encoded through a variety of patterns. Therefore, rather than developing proof rules for reasoning about particular recursion patterns, we instead propose an approach based on proving refinement between a higher-order program and a simpler probabilistic model, in such a way that the refinement preserves termination behavior. By proving a refinement, almost-sure termination behavior of the program can then be established by analyzing the simpler model. We present this approach in the form of Caliper, a higher-order separation logic for proving termination-preserving refinements. Caliper uses probabilistic couplings to carry out relational reasoning between a program and a model. To handle the range of recursion patterns found in higher-order programs, Caliper uses guarded recursion, in particular the principle of L\"ob induction. A technical novelty is that Caliper does not require the use of transfinite step indexing or other technical restrictions found in prior work on guarded recursion for termination-preservation refinement. We demonstrate the flexibility of this approach by proving almost-sure termination of several examples, including first-order loop constructs, a random list generator, treaps, and a sampler for Galton-Watson trees that uses higher-order store. All the results have been mechanized in the Coq proof assistant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08494v2</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3674632</arxiv:DOI>
      <dc:creator>Simon Oddershede Gregersen, Alejandro Aguirre, Philipp G. Haselwarter, Joseph Tassarotti, Lars Birkedal</dc:creator>
    </item>
    <item>
      <title>A propositional cirquent calculus for computability logic</title>
      <link>https://arxiv.org/abs/2406.05879</link>
      <description>arXiv:2406.05879v2 Announce Type: replace 
Abstract: Cirquent calculus is a proof system with inherent ability to account for sharing subcomponents in logical expressions. Within its framework, this article constructs an axiomatization CL18 of the basic propositional fragment of computability logic the game-semantically conceived logic of computational resources and tasks. The nonlogical atoms of this fragment represent arbitrary so called static games, and the connectives of its logical vocabulary are negation and the parallel and choice versions of conjunction and disjunction. The main technical result of the article is a proof of the soundness and completeness of CL18 with respect to the semantics of computability logic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05879v2</guid>
      <category>cs.LO</category>
      <category>math.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgi Japaridze</dc:creator>
    </item>
    <item>
      <title>MacroSwarm: A Field-based Compositional Framework for Swarm Programming</title>
      <link>https://arxiv.org/abs/2401.10969</link>
      <description>arXiv:2401.10969v2 Announce Type: replace-cross 
Abstract: Swarm behaviour engineering is an area of research that seeks to investigate methods and techniques for coordinating computation and action within groups of simple agents to achieve complex global goals like pattern formation, collective movement, clustering, and distributed sensing. Despite recent progress in the analysis and engineering of swarms (of drones, robots, vehicles), there is still a need for general design and implementation methods and tools that can be used to define complex swarm behaviour in a principled way. To contribute to this quest, this article proposes a new field-based coordination approach, called MacroSwarm, to design and program swarm behaviour in terms of reusable and fully composable functional blocks embedding collective computation and coordination. Based on the macroprogramming paradigm of aggregate computing, MacroSwarm builds on the idea of expressing each swarm behaviour block as a pure function, mapping sensing fields into actuation goal fields, e.g., including movement vectors. In order to demonstrate the expressiveness, compositionality, and practicality of MacroSwarm as a framework for swarm programming, we perform a variety of simulations covering common patterns of flocking, pattern formation, and collective decision-making. The implications of the inherent self-stabilisation properties of field-based computations in MacroSwarm are discussed, which formally guarantee some resilience properties and guided the design of the library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10969v2</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianluca Aguzzi, Roberto Casadei, Mirko Viroli</dc:creator>
    </item>
    <item>
      <title>Learning Better Representations From Less Data For Propositional Satisfiability</title>
      <link>https://arxiv.org/abs/2402.08365</link>
      <description>arXiv:2402.08365v3 Announce Type: replace-cross 
Abstract: Training neural networks on NP-complete problems typically demands very large amounts of training data and often needs to be coupled with computationally expensive symbolic verifiers to ensure output correctness. In this paper, we present NeuRes, a neuro-symbolic approach to address both challenges for propositional satisfiability, being the quintessential NP-complete problem. By combining certificate-driven training and expert iteration, our model learns better representations than models trained for classification only, with a much higher data efficiency -- requiring orders of magnitude less training data. NeuRes employs propositional resolution as a proof system to generate proofs of unsatisfiability and to accelerate the process of finding satisfying truth assignments, exploring both possibilities in parallel. To realize this, we propose an attention-based architecture that autoregressively selects pairs of clauses from a dynamic formula embedding to derive new clauses. Furthermore, we employ expert iteration whereby model-generated proofs progressively replace longer teacher proofs as the new ground truth. This enables our model to reduce a dataset of proofs generated by an advanced solver by ~32% after training on it with no extra guidance. This shows that NeuRes is not limited by the optimality of the teacher algorithm owing to its self-improving workflow. We show that our model achieves far better performance than NeuroSAT in terms of both correctly classified and proven instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08365v3</guid>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Ghanem, Frederik Schmitt, Julian Siber, Bernd Finkbeiner</dc:creator>
    </item>
    <item>
      <title>Constructibility, computational complexity and P versus NP</title>
      <link>https://arxiv.org/abs/2406.16843</link>
      <description>arXiv:2406.16843v3 Announce Type: replace-cross 
Abstract: If an algorithm is to be counted as a practically working solution to a decision problem, then the algorithm must must verifiable in some constructed and ``trusted'' theory such as PA or ZF. In this paper, a class of decision problems related to inconsistency proofs for a general class of formal theories is used to demonstrate that under this constructibility restriction, an unformalizable yet arguably convincing argument can be given for the existence of decision problems for which there exists an explicitly constructible algorithm recognizing correct solutions in polynomial time, but for which there exists no explicitly constructible solution algorithm. While these results do not solve the P versus NP problem in the classical sense of supplying a proof one way or the other in a ``trusted'' formal theory, arguably they resolve the natural constructive version of the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16843v3</guid>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arne Hole</dc:creator>
    </item>
  </channel>
</rss>
