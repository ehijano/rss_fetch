<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Jan 2025 02:33:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Performance Evaluation of Satellite-Based Data Offloading on Starlink Constellations</title>
      <link>https://arxiv.org/abs/2501.14878</link>
      <description>arXiv:2501.14878v1 Announce Type: new 
Abstract: Vehicular Edge Computing (VEC) is a key research area in autonomous driving. As Intelligent Transportation Systems (ITSs) continue to expand, ground vehicles (GVs) face the challenge of handling huge amounts of sensor data to drive safely. Specifically, due to energy and capacity limitations, GVs will need to offload resource-hungry tasks to external (cloud) computing units for faster processing. In 6th generation (6G) wireless systems, the research community is exploring the concept of Non-Terrestrial Networks (NTNs), where satellites can serve as space edge computing nodes to aggregate, store, and process data from GVs. In this paper we propose new data offloading strategies between a cluster of GVs and satellites in the Low Earth Orbits (LEOs), to optimize the trade-off between coverage and end-to-end delay. For the accuracy of the simulations, we consider real data and orbits from the Starlink constellation, one of the most representative and popular examples of commercial satellite deployments for communication. Our results demonstrate that Starlink satellites can support real-time offloading under certain conditions that depend on the onboard computational capacity of the satellites, the frame rate of the sensors, and the number of GVs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14878v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Bonora, Alessandro Traspadini, Marco Giordani, Michele Zorzi</dc:creator>
    </item>
    <item>
      <title>Enhanced AI as a Service at the Edge via Transformer Network</title>
      <link>https://arxiv.org/abs/2501.14967</link>
      <description>arXiv:2501.14967v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has become a pivotal force in reshaping next generation mobile networks. Edge computing holds promise in enabling AI as a service (AIaaS) for prompt decision-making by offloading deep neural network (DNN) inference tasks to the edge. However, current methodologies exhibit limitations in efficiently offloading the tasks, leading to possible resource underutilization and waste of mobile devices' energy. To tackle these issues, in this paper, we study AIaaS at the edge and propose an efficient offloading mechanism for renowned DNN architectures like ResNet and VGG16. We model the inference tasks as directed acyclic graphs and formulate a problem that aims to minimize the devices' energy consumption while adhering to their latency requirements and accounting for servers' capacity. To effectively solve this problem, we utilize a transformer DNN architecture. By training on historical data, we obtain a feasible and near-optimal solution to the problem. Our findings reveal that the proposed transformer model improves energy efficiency compared to established baseline schemes. Notably, when edge computing resources are limited, our model exhibits an 18\% reduction in energy consumption and significantly decreases task failure compared to existing works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14967v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vahid Pourakbar, Hamed Shah-Mansouri</dc:creator>
    </item>
    <item>
      <title>Leveraging Traceroute Inconsistencies to Improve IP Geolocation</title>
      <link>https://arxiv.org/abs/2501.15064</link>
      <description>arXiv:2501.15064v1 Announce Type: new 
Abstract: Traceroutes and geolocation are two essential network measurement tools that aid applications such as network mapping, topology generation, censorship, and Internet path analysis. However, these tools, individually and when combined, have significant limitations that can lead to inaccurate results. Prior research addressed specific issues with traceroutes and geolocation individually, often requiring additional measurements. In this paper, we introduce GeoTrace, a lightweight tool designed to identify, classify, and resolve geolocation anomalies in traceroutes using existing data. GeoTrace leverages the abundant information in traceroutes and geolocation databases to identify anomalous IP addresses with incorrect geolocation. It systematically classifies these anomalies based on underlying causes - such as MPLS effects or interface discrepancies - and refines their geolocation estimates where possible. By correcting these inaccuracies, GeoTrace enhances the reliability of traceroute-based analyses without the need for additional probing. Our work offers a streamlined solution that enhances the accuracy of geolocation in traceroute analysis, paving the way for more reliable measurement studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15064v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alagappan Ramanathan, Sangeetha Abdu Jyothi</dc:creator>
    </item>
    <item>
      <title>Quark: Implementing Convolutional Neural Networks Entirely on Programmable Data Plane</title>
      <link>https://arxiv.org/abs/2501.15100</link>
      <description>arXiv:2501.15100v1 Announce Type: new 
Abstract: The rapid development of programmable network devices and the widespread use of machine learning (ML) in networking have facilitated efficient research into intelligent data plane (IDP). Offloading ML to programmable data plane (PDP) enables quick analysis and responses to network traffic dynamics, and efficient management of network links. However, PDP hardware pipeline has significant resource limitations. For instance, Intel Tofino ASIC has only 10Mb SRAM in each stage, and lacks support for multiplication, division and floating-point operations. These constraints significantly hinder the development of IDP. This paper presents \quark, a framework that fully offloads convolutional neural network (CNN) inference onto PDP. \quark employs model pruning to simplify the CNN model, and uses quantization to support floating-point operations. Additionally, \quark divides the CNN into smaller units to improve resource utilization on the PDP. We have implemented a testbed prototype of \quark on both P4 hardware switch (Intel Tofino ASIC) and software switch (i.e., BMv2). Extensive evaluation results demonstrate that \quark achieves 97.3\% accuracy in anomaly detection task while using only 22.7\% of the SRAM resources on the Intel Tofino ASIC switch, completing inference tasks at line rate with an average latency of 42.66$\mu s$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15100v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mai Zhang, Lin Cui, Xiaoquan Zhang, Fung Po Tso, Zhang Zhen, Yuhui Deng, Zhetao Li</dc:creator>
    </item>
    <item>
      <title>UAV-Assisted MEC Architecture for Collaborative Task Offloading in Urban IoT Environment</title>
      <link>https://arxiv.org/abs/2501.15164</link>
      <description>arXiv:2501.15164v1 Announce Type: new 
Abstract: Mobile edge computing (MEC) is a promising technology to meet the increasing demands and computing limitations of complex Internet of Things (IoT) devices. However, implementing MEC in urban environments can be challenging due to factors like high device density, complex infrastructure, and limited network coverage. Network congestion and connectivity issues can adversely affect user satisfaction. Hence, in this article, we use unmanned aerial vehicle (UAV)-assisted collaborative MEC architecture to facilitate task offloading of IoT devices in urban environments. We utilize the combined capabilities of UAVs and ground edge servers (ESs) to maximize user satisfaction and thereby also maximize the service provider's (SP) profit. We design IoT task-offloading as joint IoT-UAV-ES association and UAV-network topology optimization problem. Due to NP-hard nature, we break the problem into two subproblems: offload strategy optimization and UAV topology optimization. We develop a Three-sided Matching with Size and Cyclic preference (TMSC) based task offloading algorithm to find stable association between IoTs, UAVs, and ESs to achieve system objective. We also propose a K-means based iterative algorithm to decide the minimum number of UAVs and their positions to provide offloading services to maximum IoTs in the system. Finally, we demonstrate the efficacy of the proposed task offloading scheme over benchmark schemes through simulation-based evaluation. The proposed scheme outperforms by 19%, 12%, and 25% on average in terms of percentage of served IoTs, average user satisfaction, and SP profit, respectively, with 25% lesser UAVs, making it an effective solution to support IoT task requirements in urban environments using UAV-assisted MEC architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15164v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IEEE TNSM 2025</arxiv:journal_reference>
      <dc:creator>Subhrajit Barick, Chetna Singhal</dc:creator>
    </item>
    <item>
      <title>Prediction of Received Power in Low-Power and Lossy Networks Deployed in Rough Environments</title>
      <link>https://arxiv.org/abs/2501.15182</link>
      <description>arXiv:2501.15182v1 Announce Type: new 
Abstract: Cost-efficient and low-power sensing nodes enable to monitor various physical environments. Some of these impose extreme operating conditions, subjecting the nodes to excessive heat or rainfall or motion. Rough operating conditions affect the stability of the wireless links the nodes establish and cause a significant amount of packet loss. Adaptive transmission power control (ATPC) enables nodes to adapt to extreme conditions and maintain stable wireless links and often rely on knowledge of the received power as a closed-feedback system to adjust the power of outgoing packets. However, in the presence of a significant packet loss, this knowledge may not reflect the current state of the receiver. In this paper we propose a lightweight n-step predictor which enables transmitters to adapt transmission power in the presence of lost packets. Through extensive practical deployments and testing we demonstrate that the predictor avoids expensive computation and still achieves an average prediction accuracy exceeding 90% with a low-power radio that supports a transmission rate of 250 kbps (CC2538) and 85\% with a low-power radio that supports 50 kbps (CC1200).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15182v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Waltenegus Dargie</dc:creator>
    </item>
    <item>
      <title>Joint Cell Selection and Resource Allocation Games with Backhaul Constraints</title>
      <link>https://arxiv.org/abs/2501.15687</link>
      <description>arXiv:2501.15687v1 Announce Type: new 
Abstract: In this work we study the problem of user association and resource allocation to maximize the proportional fairness of a wireless network with limited backhaul capacity. The optimal solution of this problem requires solving a mixed integer non-linear programming problem which generally cannot be solved in real time. We propose instead to model the problem as a potential game, which decreases dramatically the computational complexity and obtains a user association and resource allocation close to the optimal solution. Additionally, the use of a game-theoretic approach allows an efficient distribution of the computational burden among the computational resources of the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15687v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.pmcj.2016.06.009</arxiv:DOI>
      <arxiv:journal_reference>Pervasive and Mobile Computing, volume 35, February 2017, Pages 125-145</arxiv:journal_reference>
      <dc:creator>Jorge Ortin, Jose Ramon Gallego, Maria Canales</dc:creator>
    </item>
    <item>
      <title>Prioritized Value-Decomposition Network for Explainable AI-Enabled Network Slicing</title>
      <link>https://arxiv.org/abs/2501.15734</link>
      <description>arXiv:2501.15734v1 Announce Type: new 
Abstract: Network slicing aims to enhance flexibility and efficiency in next-generation wireless networks by allocating the right resources to meet the diverse requirements of various applications. Managing these slices with machine learning (ML) algorithms has emerged as a promising approach however explainability has been a challenge. To this end, several Explainable Artificial Intelligence (XAI) frameworks have been proposed to address the opacity in decision-making in many ML methods. In this paper, we propose a Prioritized Value-Decomposition Network (PVDN) as an XAI-driven approach for resource allocation in a multi-agent network slicing system. The PVDN method decomposes the global value function into individual contributions and prioritizes slice outputs, providing an explanation of how resource allocation decisions impact system performance. By incorporating XAI, PVDN offers valuable insights into the decision-making process, enabling network operators to better understand, trust, and optimize slice management strategies. Through simulations, we demonstrate the effectiveness of the PVDN approach with improving the throughput by 67% and 16%, while reducing latency by 35% and 22%, compared to independent and VDN-based resource allocation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15734v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shavbo Salehi, Pedro Enrique Iturria-Rivera, Medhat Elsayed, Majid Bavand, Raimundas Gaigalas, Yigit Ozcan, Melike Erol-Kantarci</dc:creator>
    </item>
    <item>
      <title>Kairos: Energy-Efficient Radio Unit Control for O-RAN via Advanced Sleep Modes</title>
      <link>https://arxiv.org/abs/2501.15853</link>
      <description>arXiv:2501.15853v1 Announce Type: new 
Abstract: The high energy footprint of 5G base stations, particularly the radio units (RUs), poses a significant environmental and economic challenge. We introduce Kairos, a novel approach to maximize the energy-saving potential of O-RAN's Advanced Sleep Modes (ASMs). Unlike state-of-the-art solutions, which often rely on complex ASM selection algorithms unsuitable for time-constrained base stations and fail to guarantee stringent QoS demands, Kairos offers a simple yet effective joint ASM selection and radio scheduling policy capable of real-time operation. This policy is then optimized using a data-driven algorithm within an xApp, which enables several key innovations: (i) a dimensionality-invariant encoder to handle variable input sizes (e.g., time-varying network slices), (ii) distributional critics to accurately model QoS metrics and ensure constraint satisfaction, and (iii) a single-actor-multiple-critic architecture to effectively manage multiple constraints. Through experimental analysis on a commercial RU and trace-driven simulations, we demonstrate Kairos's potential to achieve energy reductions ranging between 15% and 72% while meeting QoS requirements, offering a practical solution for cost- and energy-efficient 5G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15853v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Xavier Salvat Lozano, Jose A. Ayala-Romero, Andres Garcia-Saavedra, Xavier Costa-Perez</dc:creator>
    </item>
    <item>
      <title>Game theoretic approach for end-to-end resource allocation in multihop cognitive radio networks</title>
      <link>https://arxiv.org/abs/2501.15855</link>
      <description>arXiv:2501.15855v1 Announce Type: new 
Abstract: This paper presents a game theoretic solution for end-to-end channel and power allocation in multihop cognitive radio networks analyzed under the physical interference model. The objective is to find a distributed solution that maximizes the number of flows that can be established in the network. The problem is addressed through three different games: a local flow game which uses complete information about the links of the flow, a potential flow game requiring global network knowledge and a cooperative link game based on partial information regarding the links of the flow. Results show that the proposed link game highly decreases the complexity of the channel and power allocation problem in terms of computational load, reducing the information shared between the links forming each flow with a performance similar to that of the more complex flow games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15855v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LCOMM.2012.030512.112574</arxiv:DOI>
      <arxiv:journal_reference>IEEE Communications Letters, volume: 16, issue: 5, May 2012</arxiv:journal_reference>
      <dc:creator>Maria Canales, Jorge Ortin, Jose Ramon Gallego</dc:creator>
    </item>
    <item>
      <title>Generative AI for Lyapunov Optimization Theory in UAV-based Low-Altitude Economy Networking</title>
      <link>https://arxiv.org/abs/2501.15928</link>
      <description>arXiv:2501.15928v1 Announce Type: new 
Abstract: Lyapunov optimization theory has recently emerged as a powerful mathematical framework for solving complex stochastic optimization problems by transforming long-term objectives into a sequence of real-time short-term decisions while ensuring system stability. This theory is particularly valuable in unmanned aerial vehicle (UAV)-based low-altitude economy (LAE) networking scenarios, where it could effectively address inherent challenges of dynamic network conditions, multiple optimization objectives, and stability requirements. Recently, generative artificial intelligence (GenAI) has garnered significant attention for its unprecedented capability to generate diverse digital content. Extending beyond content generation, in this paper, we propose a framework integrating generative diffusion models with reinforcement learning to address Lyapunov optimization problems in UAV-based LAE networking. We begin by introducing the fundamentals of Lyapunov optimization theory and analyzing the limitations of both conventional methods and traditional AI-enabled approaches. We then examine various GenAI models and comprehensively analyze their potential contributions to Lyapunov optimization. Subsequently, we develop a Lyapunov-guided generative diffusion model-based reinforcement learning framework and validate its effectiveness through a UAV-based LAE networking case study. Finally, we outline several directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15928v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhang Liu, Dusit Niyato, Jiacheng Wang, Geng Sun, Lianfen Huang, Zhibin Gao, Xianbin Wang</dc:creator>
    </item>
    <item>
      <title>Disruption-aware Microservice Re-orchestration for Cost-efficient Multi-cloud Deployments</title>
      <link>https://arxiv.org/abs/2501.16143</link>
      <description>arXiv:2501.16143v1 Announce Type: new 
Abstract: Multi-cloud environments enable a cost-efficient scaling of cloud-native applications across geographically distributed virtual nodes with different pricing models. In this context, the resource fragmentation caused by frequent changes in the resource demands of deployed microservices, along with the allocation or termination of new and existing microservices, increases the deployment cost. Therefore, re-orchestrating deployed microservices on a cheaper configuration of multi-cloud nodes offers a practical solution to restore the cost efficiency of deployment. However, the rescheduling procedure causes frequent service interruptions due to the continuous termination and rebooting of the containerized microservices. Moreover, it may potentially interfere with and delay other deployment operations, compromising the stability of the running applications. To address this issue, we formulate a multi-objective integer linear programming problem that computes a microservice rescheduling solution capable of providing minimum deployment cost without significantly affecting the service continuity. At the same time, the proposed formulation also preserves the quality of service (QoS) requirements, including latency, expressed through microservice colocation constraints. Additionally, we present a heuristic algorithm to approximate the optimal solution, striking a balance between cost reduction and service disruption mitigation. We integrate the proposed approach as a custom plugin of the Kubernetes scheduler. Results reveal that our approach significantly reduces multi-cloud deployment costs and service disruptions compared to the default Kubernetes scheduler implementation, while ensuring QoS requirements are consistently met.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16143v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Zambianco, Silvio Cretti, Domenico Siracusa</dc:creator>
    </item>
    <item>
      <title>Unveiling Ethereum's P2P Network: The Role of Chain and Client Diversity</title>
      <link>https://arxiv.org/abs/2501.16236</link>
      <description>arXiv:2501.16236v1 Announce Type: new 
Abstract: The Ethereum network, built on the devp2p protocol stack, was designed to function as a "world computer" by supporting decentralized applications through a shared P2P infrastructure. However, the proliferation of blockchain forks has increased network diversity, complicating node discovery and reducing efficiency. Ethereum mainnet nodes cannot easily distinguish between peers from different blockchains until after establishing an expensive TCP connection, encryption, and protocol handshake. This inefficiency is further worsened by client diversity, where differences in software implementations cause protocol incompatibilities and connection failures. This paper introduces a monitoring tool that tracks devp2p message exchanges and client statuses to analyze connection dynamics and protocol variations. Our findings highlight issues such as inefficiencies in node discovery and client incompatibility, including timeouts in Geth during the discovery process. The study emphasizes the need to consider chain and client diversity when assessing the health and performance of the post-merge Ethereum network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16236v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiahao Luo</dc:creator>
    </item>
    <item>
      <title>ABACUS: A FinOps Service for Cloud Cost Optimization</title>
      <link>https://arxiv.org/abs/2501.14753</link>
      <description>arXiv:2501.14753v1 Announce Type: cross 
Abstract: In recent years, as more enterprises have moved their infrastructure to the cloud, significant challenges have emerged in achieving holistic cloud spend visibility and cost optimization. FinOps practices provide a way for enterprises to achieve these business goals by optimizing cloud costs and bringing accountability to cloud spend. This paper presents ABACUS - Automated Budget Analysis and Cloud Usage Surveillance, a FinOps solution for optimizing cloud costs by setting budgets, enforcing those budgets through blocking new deployments, and alerting appropriate teams if spending breaches a budget threshold. ABACUS also leverages best practices like Infrastructure-as-Code to alert engineering teams of the expected cost of deployment before resources are deployed in the cloud. Finally, future research directions are proposed to advance the state of the art in this important field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14753v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Saurabh Deochake</dc:creator>
    </item>
    <item>
      <title>A VM-HDL Co-Simulation Framework for Systems with PCIe-Connected FPGAs</title>
      <link>https://arxiv.org/abs/2501.14815</link>
      <description>arXiv:2501.14815v1 Announce Type: cross 
Abstract: PCIe-connected FPGAs are gaining popularity as an accelerator technology in data centers. However, it is challenging to jointly develop and debug host software and FPGA hardware. Changes to the hardware design require a time-consuming FPGA synthesis process, and modification to the software, especially the operating system and device drivers, can frequently cause the system to hang, without providing enough information for debugging. The combination of these problems results in long debug iterations and a slow development process. To overcome these problems, we designed a VM-HDL co-simulation framework, which is capable of running the same software, operating system, and hardware designs as the target physical system, while providing full visibility and significantly shorter debug iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14815v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shenghsun Cho, Mrunal Patel, Basavaraj Kaladagi, Han Chen, Tapti Palit, Michael Ferdman, Peter Milder</dc:creator>
    </item>
    <item>
      <title>QuESat: Satellite-Assisted Quantum Internet for Global-Scale Entanglement Distribution</title>
      <link>https://arxiv.org/abs/2501.15376</link>
      <description>arXiv:2501.15376v1 Announce Type: cross 
Abstract: Entanglement distribution across remote distances is critical for many quantum applications. Currently, the de facto approach for remote entanglement distribution relies on optical fiber for on-the-ground entanglement distribution. However, the fiber-based approach is incapable of global-scale entanglement distribution due to intrinsic limitations. This paper investigates a new hybrid ground-satellite quantum network architecture (QuESat) for global-scale entanglement distribution, integrating an on-the-ground fiber network with a global-scale passive optical network built with low-Earth-orbit satellites. The satellite network provides dynamic construction of photon lightpaths based on near-vacuum beam guides constructed via adjustable arrays of lenses, forwarding photons from one ground station to another with very high efficiency over long distances compared to using fiber. To assess the feasibility and effectiveness of QuESat for global communication, we formulate lightpath provisioning and entanglement distribution problems, considering the orbital dynamics of satellites and the time-varying entanglement demands from ground users. A two-stage algorithm is developed to dynamically configure the beam guides and distribute entanglements, respectively. The algorithm combines randomized and deterministic rounding for lightpath provisioning to enable global connectivity, with optimal entanglement swapping for distributing entanglements to meet users' demands. By developing a ground-satellite quantum network simulator, QuESat achieves multi-fold improvements compared to repeater networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15376v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huayue Gu, Ruozhou Yu, Zhouyu Li, Xiaojian Wang, Guoliang Xue</dc:creator>
    </item>
    <item>
      <title>Hiding in Plain Sight: An IoT Traffic Camouflage Framework for Enhanced Privacy</title>
      <link>https://arxiv.org/abs/2501.15395</link>
      <description>arXiv:2501.15395v1 Announce Type: cross 
Abstract: The rapid growth of Internet of Things (IoT) devices has introduced significant challenges to privacy, particularly as network traffic analysis techniques evolve. While encryption protects data content, traffic attributes such as packet size and timing can reveal sensitive information about users and devices. Existing single-technique obfuscation methods, such as packet padding, often fall short in dynamic environments like smart homes due to their predictability, making them vulnerable to machine learning-based attacks. This paper introduces a multi-technique obfuscation framework designed to enhance privacy by disrupting traffic analysis. The framework leverages six techniques-Padding, Padding with XORing, Padding with Shifting, Constant Size Padding, Fragmentation, and Delay Randomization-to obscure traffic patterns effectively. Evaluations on three public datasets demonstrate significant reductions in classifier performance metrics, including accuracy, precision, recall, and F1 score. We assess the framework's robustness against adversarial tactics by retraining and fine-tuning neural network classifiers on obfuscated traffic. The results reveal a notable degradation in classifier performance, underscoring the framework's resilience against adaptive attacks. Furthermore, we evaluate communication and system performance, showing that higher obfuscation levels enhance privacy but may increase latency and communication overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15395v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Adu Worae, Spyridon Mastorakis</dc:creator>
    </item>
    <item>
      <title>PCAP-Backdoor: Backdoor Poisoning Generator for Network Traffic in CPS/IoT Environments</title>
      <link>https://arxiv.org/abs/2501.15563</link>
      <description>arXiv:2501.15563v2 Announce Type: cross 
Abstract: The rapid expansion of connected devices has made them prime targets for cyberattacks. To address these threats, deep learning-based, data-driven intrusion detection systems (IDS) have emerged as powerful tools for detecting and mitigating such attacks. These IDSs analyze network traffic to identify unusual patterns and anomalies that may indicate potential security breaches. However, prior research has shown that deep learning models are vulnerable to backdoor attacks, where attackers inject triggers into the model to manipulate its behavior and cause misclassifications of network traffic. In this paper, we explore the susceptibility of deep learning-based IDS systems to backdoor attacks in the context of network traffic analysis. We introduce \texttt{PCAP-Backdoor}, a novel technique that facilitates backdoor poisoning attacks on PCAP datasets. Our experiments on real-world Cyber-Physical Systems (CPS) and Internet of Things (IoT) network traffic datasets demonstrate that attackers can effectively backdoor a model by poisoning as little as 1\% or less of the entire training dataset. Moreover, we show that an attacker can introduce a trigger into benign traffic during model training yet cause the backdoored model to misclassify malicious traffic when the trigger is present. Finally, we highlight the difficulty of detecting this trigger-based backdoor, even when using existing backdoor defense techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15563v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ajesh Koyatan Chathoth, Stephen Lee</dc:creator>
    </item>
    <item>
      <title>Brain-Inspired Decentralized Satellite Learning in Space Computing Power Networks</title>
      <link>https://arxiv.org/abs/2501.15995</link>
      <description>arXiv:2501.15995v1 Announce Type: cross 
Abstract: Satellite networks are able to collect massive space information with advanced remote sensing technologies, which is essential for real-time applications such as natural disaster monitoring. However, traditional centralized processing by the ground server incurs a severe timeliness issue caused by the transmission bottleneck of raw data. To this end, Space Computing Power Networks (Space-CPN) emerges as a promising architecture to coordinate the computing capability of satellites and enable on board data processing. Nevertheless, due to the natural limitations of solar panels, satellite power system is difficult to meet the energy requirements for ever-increasing intelligent computation tasks of artificial neural networks. To tackle this issue, we propose to employ spiking neural networks (SNNs), which is supported by the neuromorphic computing architecture, for on-board data processing. The extreme sparsity in its computation enables a high energy efficiency. Furthermore, to achieve effective training of these on-board models, we put forward a decentralized neuromorphic learning framework, where a communication-efficient inter-plane model aggregation method is developed with the inspiration from RelaySum. We provide a theoretical analysis to characterize the convergence behavior of the proposed algorithm, which reveals a network diameter related convergence speed. We then formulate a minimum diameter spanning tree problem on the inter-plane connectivity topology and solve it to further improve the learning performance. Extensive experiments are conducted to evaluate the superiority of the proposed method over benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15995v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Yang, Ting Wang, Haibin Cai, Yuanming Shi, Chunxiao Jiang, Linling Kuang</dc:creator>
    </item>
    <item>
      <title>Entanglement-Assisted Coding for Arbitrary Linear Computations Over a Quantum MAC</title>
      <link>https://arxiv.org/abs/2501.16296</link>
      <description>arXiv:2501.16296v1 Announce Type: cross 
Abstract: We study a linear computation problem over a quantum multiple access channel (LC-QMAC), where $S$ servers share an entangled state and separately store classical data streams $W_1,\cdots, W_S$ over a finite field $\mathbb{F}_d$. A user aims to compute $K$ linear combinations of these data streams, represented as $Y = \mathbf{V}_1 W_1 + \mathbf{V}_2 W_2 + \cdots + \mathbf{V}_S W_S \in \mathbb{F}_d^{K \times 1}$. To this end, each server encodes its classical information into its local quantum subsystem and transmits it to the user, who retrieves the desired computations via quantum measurements. In this work, we propose an achievable scheme for LC-QMAC based on the stabilizer formalism and the ideas from entanglement-assisted quantum error-correcting codes (EAQECC). Specifically, given any linear computation matrix, we construct a self-orthogonal matrix that can be implemented using the stabilizer formalism. Also, we apply precoding matrices to minimize the number of auxiliary qudits required. Our scheme achieves more computations per qudit, i.e., a higher computation rate, compared to the best-known methods in the literature, and attains the capacity in certain cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16296v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>quant-ph</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Hu, Mohamed Nomeir, Alptug Aytekin, Yu Shi, Sennur Ulukus, Saikat Guha</dc:creator>
    </item>
    <item>
      <title>Graph Neural Network Based Hybrid Beamforming Design in Wideband Terahertz MIMO-OFDM Systems</title>
      <link>https://arxiv.org/abs/2501.16306</link>
      <description>arXiv:2501.16306v1 Announce Type: cross 
Abstract: 6G wireless technology is projected to adopt higher and wider frequency bands, enabled by highly directional beamforming. However, the vast bandwidths available also make the impact of beam squint in massive multiple input and multiple output (MIMO) systems non-negligible. Traditional approaches such as adding a true-time-delay line (TTD) on each antenna are costly due to the massive antenna arrays required. This paper puts forth a signal processing alternative, specifically adapted to the multicarrier structure of OFDM systems, through an innovative application of Graph Neural Networks (GNNs) to optimize hybrid beamforming. By integrating two types of graph nodes to represent the analog and the digital beamforming matrices efficiently, our approach not only reduces the computational and memory burdens but also achieves high spectral efficiency performance, approaching that of all digital beamforming. The GNN runtime and memory requirement are at a fraction of the processing time and resource consumption of traditional signal processing methods, hence enabling real-time adaptation of hybrid beamforming. Furthermore, the proposed GNN exhibits strong resiliency to beam squinting, achieving almost constant spectral efficiency even as the system bandwidth increases at higher carrier frequencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16306v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beier Li, Mai Vu</dc:creator>
    </item>
    <item>
      <title>Reasoning AI Performance Degradation in 6G Networks with Large Language Models</title>
      <link>https://arxiv.org/abs/2408.17097</link>
      <description>arXiv:2408.17097v2 Announce Type: replace 
Abstract: The integration of Artificial Intelligence (AI) within 6G networks is poised to revolutionize connectivity, reliability, and intelligent decision-making. However, the performance of AI models in these networks is crucial, as any decline can significantly impact network efficiency and the services it supports. Understanding the root causes of performance degradation is essential for maintaining optimal network functionality. In this paper, we propose a novel approach to reason about AI model performance degradation in 6G networks using the Large Language Models (LLMs) empowered Chain-of-Thought (CoT) method. Our approach employs an LLM as a ''teacher'' model through zero-shot prompting to generate teaching CoT rationales, followed by a CoT ''student'' model that is fine-tuned by the generated teaching data for learning to reason about performance declines. The efficacy of this model is evaluated in a real-world scenario involving a real-time 3D rendering task with multi-Access Technologies (mATs) including WiFi, 5G, and LiFi for data transmission. Experimental results show that our approach achieves over 97% reasoning accuracy on the built test questions, confirming the validity of our collected dataset and the effectiveness of the LLM-CoT method. Our findings highlight the potential of LLMs in enhancing the reliability and efficiency of 6G networks, representing a significant advancement in the evolution of AI-native network infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17097v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liming Huang, Yulei Wu, Dimitra Simeonidou</dc:creator>
    </item>
    <item>
      <title>Research Directions and Modeling Guidelines for Industrial Internet of Things Applications</title>
      <link>https://arxiv.org/abs/2410.02610</link>
      <description>arXiv:2410.02610v2 Announce Type: replace 
Abstract: The Industrial Internet of Things (IIoT) paradigm has emerged as a transformative force, revolutionizing industrial processes by integrating advanced wireless technologies into traditional procedures to enhance their efficiency. The importance of this paradigm shift has produced a massive, yet heterogeneous, proliferation of scientific contributions. However, these works lack a standardized and cohesive characterization of the IIoT framework coming from different entities, like the 3rd Generation Partnership Project (3GPP) or the 5G Alliance for Connected Industries and Automation (5G-ACIA), resulting in divergent perspectives and potentially hindering interoperability. To bridge this gap, this article offers a unified characterization of (i) the main IIoT application domains, (ii) their respective requirements, (iii) the principal technological gaps existing in the current literature, and, most importantly, (iv) we propose a systematic approach for assessing and addressing the identified research challenges. Therefore, this article serves as a roadmap for future research endeavors, promoting a unified vision of the IIoT paradigm and fostering collaborative efforts to advance the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02610v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giampaolo Cuozzo, Enrico Testi, Salvatore Riolo, Luciano Miuccio, Gianluca Cena, Gianni Pasolini, Luca De Nardis, Daniela Panno, Marco Chiani, Maria-Gabriella Di Benedetto, Enrico Buracchini, Roberto Verdone</dc:creator>
    </item>
    <item>
      <title>Dynamic Content Caching with Waiting Costs via Restless Multi-Armed Bandits</title>
      <link>https://arxiv.org/abs/2410.18627</link>
      <description>arXiv:2410.18627v3 Announce Type: replace 
Abstract: We consider a system with a local cache connected to a backend server and an end user population. A set of contents are stored at the the server where they continuously get updated. The local cache keeps copies, potentially stale, of a subset of the contents. The users make content requests to the local cache which either can serve the local version if available or can fetch a fresh version or can wait for additional requests before fetching and serving a fresh version. Serving a stale version of a content incurs an age-of-version(AoV) dependent ageing cost, fetching it from the server incurs a fetching cost, and making a request wait incurs a per unit time waiting cost. We focus on the optimal actions subject to the cache capacity constraint at each decision epoch, aiming at minimizing the long term average cost. We pose the problem as a Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based policy which is known to be asymptotically optimal. We explicitly characterize the Whittle indices. We numerically evaluate the proposed policy and also compare it to a greedy policy. We show that it is close to the optimal policy and substantially outperforms the exising policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18627v3</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ankita Koley, Chandramani Singh</dc:creator>
    </item>
    <item>
      <title>Joint Task Offloading and Routing in Wireless Multi-hop Networks Using Biased Backpressure Algorithm</title>
      <link>https://arxiv.org/abs/2412.15385</link>
      <description>arXiv:2412.15385v3 Announce Type: replace 
Abstract: A significant challenge for computation offloading in wireless multi-hop networks is the complex interaction among traffic flows in the presence of interference. Existing approaches often ignore these key effects and/or rely on outdated queueing and channel state information. To fill these gaps, we reformulate joint offloading and routing as a routing problem on an extended graph with physical and virtual links. We adopt the state-of-the-art shortest path-biased Backpressure routing algorithm, which allows the destination and the route of a job to be dynamically adjusted at every time step based on network-wide long-term information and real-time states of local neighborhoods. In large networks, our approach achieves smaller makespan than existing approaches, such as separated Backpressure offloading and joint offloading and routing based on linear programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15385v3</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongyuan Zhao, Jake Perazzone, Gunjan Verma, Kevin Chan, Ananthram Swami, Santiago Segarra</dc:creator>
    </item>
    <item>
      <title>A Survey and Tutorial of Redundancy Mitigation for Vehicular Cooperative Perception: Standards, Strategies and Open Issues</title>
      <link>https://arxiv.org/abs/2501.01200</link>
      <description>arXiv:2501.01200v2 Announce Type: replace 
Abstract: This paper provides an in-depth review and discussion of the state of the art in redundancy mitigation for the vehicular Collective Perception Service (CPS). We focus on the evolutionary differences between the redundancy mitigation rules proposed in 2019 in ETSI TR 103 562 versus the 2023 technical specification ETSI TS 103 324, which uses a Value of Information (VoI) based mitigation approach. We also critically analyse the academic literature that has sought to quantify the communication challenges posed by the CPS and present a unique taxonomy of the redundancy mitigation approaches proposed using three distinct classifications: object inclusion filtering, data format optimisation, and frequency management. Finally, this paper identifies open research challenges that must be adequately investigated to satisfactorily deploy CPS redundancy mitigation measures. Our critical and comprehensive evaluation serves as a point of reference for those undertaking research in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01200v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tengfei Lyu, Md Noor-A-Rahim, Dirk Pesch, Aisling O'Driscoll</dc:creator>
    </item>
    <item>
      <title>Discrete-Time Modeling and Handover Analysis of Intelligent Reflecting Surface-Assisted Networks</title>
      <link>https://arxiv.org/abs/2403.07323</link>
      <description>arXiv:2403.07323v2 Announce Type: replace-cross 
Abstract: Owning to the reflection gain and double path loss featured by intelligent reflecting surface (IRS) channels, handover (HO) locations become irregular and the signal strength fluctuates sharply with variations in IRS connections during HO, the risk of HO failures (HOFs) is exacerbated and thus HO parameters require reconfiguration. However, existing HO models only assume monotonic negative exponential path loss and cannot obtain sound HO parameters. This paper proposes a discrete-time model to explicitly track the HO process with variations in IRS connections, where IRS connections and HO process are discretized as finite states by measurement intervals, and transitions between states are modeled as stochastic processes. Specifically, to capture signal fluctuations during HO, IRS connection state-dependent distributions of the user-IRS distance are modified by the correlation between measurement intervals. In addition, states of the HO process are formed with Time-to-Trigger and HO margin whose transition probabilities are integrated concerning all IRS connection states. Trigger location distributions and probabilities of HO, HOF, and ping-pong (PP) are obtained by tracing user HO states. Results show IRSs mitigate PPs by 48% but exacerbate HOFs by 90% under regular parameters. Optimal parameters are mined ensuring probabilities of HOF and PP are both less than 0.1%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07323v2</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCOMM.2025.3529246</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Communications (2025)</arxiv:journal_reference>
      <dc:creator>Haoyan Wei, Hongtao Zhang</dc:creator>
    </item>
    <item>
      <title>Recommenadation aided Caching using Combinatorial Multi-armed Bandits</title>
      <link>https://arxiv.org/abs/2405.00080</link>
      <description>arXiv:2405.00080v4 Announce Type: replace-cross 
Abstract: We study content caching with recommendations in a wireless network where the users are connected through a base station equipped with a finite-capacity cache. We assume a fixed set of contents with unknown user preferences and content popularities. The base station can cache a subset of the contents and can also recommend subsets of the contents to different users in order to encourage them to request the recommended contents. Recommendations, depending on their acceptability, can thus be used to increase cache hits. We first assume that the users' recommendation acceptabilities are known and formulate the cache hit optimization problem as a combinatorial multi-armed bandit (CMAB). We propose a UCB-based algorithm to decide which contents to cache and recommend and provide an upper bound on the regret of this algorithm. Subsequently, we consider a more general scenario where the users' recommendation acceptabilities are also unknown and propose another UCB-based algorithm that learns these as well. We numerically demonstrate the performance of our algorithms and compare these to state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00080v4</guid>
      <category>cs.LG</category>
      <category>cs.IR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavamana K J, Chandramani Kishore Singh</dc:creator>
    </item>
    <item>
      <title>Routing in Quantum Networks with End-to-End Knowledge</title>
      <link>https://arxiv.org/abs/2407.14407</link>
      <description>arXiv:2407.14407v2 Announce Type: replace-cross 
Abstract: Given the diverse array of physical systems available for quantum computing and the absence of a well-defined quantum internet protocol stack, the design and optimisation of quantum networking protocols remain largely unexplored. To address this, we introduce an approach that facilitates the establishment of paths capable of delivering end-to-end fidelity above a specified threshold, without requiring detailed knowledge of the quantum network properties. In this study, we define algorithms that are specific instances of this approach and evaluate them in comparison to Dijkstra shortest path algorithm and a fully knowledge-aware algorithm through simulations. Our results demonstrate that one of the proposed algorithms consistently outperforms the other methods in delivering paths above the fidelity threshold, across various network topologies and the number of source-destination pairs involved, while maintaining significant levels of fairness among the users and being robust to inaccurate estimations of the expected end-to-end fidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14407v2</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vinay Kumar, Claudio Cicconetti, Marco Conti, Andrea Passarella</dc:creator>
    </item>
    <item>
      <title>PANTS: Practical Adversarial Network Traffic Samples against ML-powered Networking Classifiers</title>
      <link>https://arxiv.org/abs/2409.04691</link>
      <description>arXiv:2409.04691v2 Announce Type: replace-cross 
Abstract: Multiple network management tasks, from resource allocation to intrusion detection, rely on some form of ML-based network traffic classification (MNC). Despite their potential, MNCs are vulnerable to adversarial inputs, which can lead to outages, poor decision-making, and security violations, among other issues. The goal of this paper is to help network operators assess and enhance the robustness of their MNC against adversarial inputs. The most critical step for this is generating inputs that can fool the MNC while being realizable under various threat models. Compared to other ML models, finding adversarial inputs against MNCs is more challenging due to the existence of non-differentiable components e.g., traffic engineering and the need to constrain inputs to preserve semantics and ensure reliability. These factors prevent the direct use of well-established gradient-based methods developed in adversarial ML (AML). To address these challenges, we introduce PANTS, a practical white-box framework that uniquely integrates AML techniques with Satisfiability Modulo Theories (SMT) solvers to generate adversarial inputs for MNCs. We also embed PANTS into an iterative adversarial training process that enhances the robustness of MNCs against adversarial inputs. PANTS is 70% and 2x more likely in median to find adversarial inputs against target MNCs compared to state-of-the-art baselines, namely Amoeba and BAP. PANTS improves the robustness of the target MNCs by 52.7% (even against attackers outside of what is considered during robustification) without sacrificing their accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04691v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Minhao Jin, Maria Apostolaki</dc:creator>
    </item>
    <item>
      <title>WiFi CSI Based Temporal Activity Detection via Dual Pyramid Network</title>
      <link>https://arxiv.org/abs/2412.16233</link>
      <description>arXiv:2412.16233v2 Announce Type: replace-cross 
Abstract: We address the challenge of WiFi-based temporal activity detection and propose an efficient Dual Pyramid Network that integrates Temporal Signal Semantic Encoders and Local Sensitive Response Encoders. The Temporal Signal Semantic Encoder splits feature learning into high and low-frequency components, using a novel Signed Mask-Attention mechanism to emphasize important areas and downplay unimportant ones, with the features fused using ContraNorm. The Local Sensitive Response Encoder captures fluctuations without learning. These feature pyramids are then combined using a new cross-attention fusion mechanism. We also introduce a dataset with over 2,114 activity segments across 553 WiFi CSI samples, each lasting around 85 seconds. Extensive experiments show our method outperforms challenging baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16233v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhendong Liu, Le Zhang, Bing Li, Yingjie Zhou, Zhenghua Chen, Ce Zhu</dc:creator>
    </item>
  </channel>
</rss>
