<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Jul 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Constructing targeted minimum loss/maximum likelihood estimators: a simple illustration to build intuition</title>
      <link>https://arxiv.org/abs/2507.11680</link>
      <description>arXiv:2507.11680v1 Announce Type: new 
Abstract: Use of machine learning to estimate nuisance functions (e.g. outcomes models, propensity score models) in estimators used in causal inference is increasingly common, as it can mitigate bias due to model misspecification. However, it can be challenging to achieve valid inference (e.g., estimate valid confidence intervals). The efficient influence function (EIF) provides a recipe to go from a statistical estimand relevant to our causal question, to an estimator that can validly incorporate machine learning. Our companion paper, Renson et al. 2025 (arXiv:2502.05363), provides a thorough but approachable description of the EIF, along with a guide through the steps to go from a unique statistical estimand to development of one type of EIF-based estimator, the so-called one-step estimator. Another commonly used estimator based on the EIF is the targeted maximum likelihood/minimum loss estimator (TMLE). Construction of TMLEs is well-discussed in the statistical literature, but there remains a gap in translation to a more applied audience. In this letter, which supplements Renson et al., we provide a more accessible illustration of how to construct a TMLE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11680v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachael K. Ross, Lina M. Montoya, Dana E. Goin, Ivan Diaz, Audrey Renson</dc:creator>
    </item>
    <item>
      <title>Bayesian wavelet shrinkage for low SNR data based on the Epanechnikov kernel</title>
      <link>https://arxiv.org/abs/2507.11718</link>
      <description>arXiv:2507.11718v1 Announce Type: new 
Abstract: Consider the univariate nonparametric regression model with additive Gaussian noise and the representation of the unknown regression function in terms of a wavelet basis. We propose a shrinkage rule to estimate the wavelet coefficients obtained by mixing a point mass function at zero with the Epanechnikov distribution as a prior for the coefficients. The proposed rule proved to be suitable for application in scenarios with low signal-to-noise ratio datasets and outperformed standard and Bayesian methods in simulation studies. Statistical properties, such as squared bias and variance, are provided, and an explicit expression of the rule is obtained. An application of the rule is demonstrated using a real EEG dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11718v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fidel Aniano Causil Barrios, Alex Rodrigo dos Santos Sousa</dc:creator>
    </item>
    <item>
      <title>Model averaging in the space of probability distributions</title>
      <link>https://arxiv.org/abs/2507.11719</link>
      <description>arXiv:2507.11719v1 Announce Type: new 
Abstract: This work investigates the problem of model averaging in the context of measure-valued data. Specifically, we study aggregation schemes in the space of probability distributions metrized in terms of the Wasserstein distance. The resulting aggregate models, defined via Wasserstein barycenters, are optimally calibrated to empirical data. To enhance model performance, we employ regularization schemes motivated by the standard elastic net penalization, which is shown to consistently yield models enjoying sparsity properties. The consistency properties of the proposed averaging schemes with respect to sample size are rigorously established using the variational framework of $\Gamma$-convergence. The performance of the methods is evaluated through carefully designed synthetic experiments that assess behavior across a range of distributional characteristics and stress conditions. Finally, the proposed approach is applied to a real-world dataset of insurance losses - characterized by heavy-tailed behavior - to estimate the claim size distribution and the associated tail risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11719v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emmanouil Androulakis, Georgios I. Papayiannis, Athanasios N. Yannacopoulos</dc:creator>
    </item>
    <item>
      <title>Smooth tensor decomposition with application to ambulatory blood pressure monitoring data</title>
      <link>https://arxiv.org/abs/2507.11723</link>
      <description>arXiv:2507.11723v1 Announce Type: new 
Abstract: Ambulatory blood pressure monitoring (ABPM) enables continuous measurement of blood pressure and heart rate over 24 hours and is increasingly used in clinical studies. However, ABPM data are often reduced to summary statistics, such as means or medians, which obscure temporal features like nocturnal dipping and individual chronotypes. Functional data analysis methods better capture these temporal dynamics but typically treat each ABPM measurement separately, limiting their ability to leverage correlations among matched measurements. In this work, we observe that aligning ABPM data along measurement type, time, and patient ID lends itself to a tensor representation--a multidimensional array. Although tensor learning has shown great potential in other fields, it has not been applied to ABPM data. Existing tensor learning approaches often lack temporal smoothing constraints, assume no missing data, and can be computationally demanding. To address these limitations, we propose a novel smooth tensor decomposition method that incorporates a temporal smoothing penalty and accommodates missing data. We also develop an automatic procedure for selecting the optimal smoothing parameter and tensor ranks. Simulation studies demonstrate that our method reliably reconstructs smooth temporal trends from noisy, incomplete data. Application to ABPM data from patients with concurrent obstructive sleep apnea and type 2 diabetes uncovers clinically relevant associations between patient characteristics and ABPM measurements, which are missed by summary-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11723v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leyuan Qian, R. Nisha Aurora, Naresh M. Punjabi, Irina Gaynanova</dc:creator>
    </item>
    <item>
      <title>Fiducial Matching: Differentially Private Inference for Categorical Data</title>
      <link>https://arxiv.org/abs/2507.11762</link>
      <description>arXiv:2507.11762v1 Announce Type: new 
Abstract: The task of statistical inference, which includes the building of confidence intervals and tests for parameters and effects of interest to a researcher, is still an open area of investigation in a differentially private (DP) setting. Indeed, in addition to the randomness due to data sampling, DP delivers another source of randomness consisting of the noise added to protect an individual's data from being disclosed to a potential attacker. As a result of this convolution of noises, in many cases it is too complicated to determine the stochastic behavior of the statistics and parameters resulting from a DP procedure. In this work, we contribute to this line of investigation by employing a simulation-based matching approach, solved through tools from the fiducial framework, which aims to replicate the data generation pipeline (including the DP step) and retrieve an approximate distribution of the estimates resulting from this pipeline. For this purpose, we focus on the analysis of categorical (nominal) data that is common in national surveys, for which sensitivity is naturally defined, and on additive privacy mechanisms. We prove the validity of the proposed approach in terms of coverage and highlight its good computational and statistical performance for different inferential tasks in simulated and applied data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11762v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ogonnaya Michael Romanus, Younes Boulaguiem, Roberto Molinari</dc:creator>
    </item>
    <item>
      <title>Bayesian multivariate models for bounded directional data</title>
      <link>https://arxiv.org/abs/2507.11784</link>
      <description>arXiv:2507.11784v1 Announce Type: new 
Abstract: In some areas of knowledge there are data representing directions restricted to a specific range of values. Consequently, it is useful to have models for describing variables defined in subsets of the k-dimensional unit sphere. This need has led to the development of models such as the multivariate projected Gamma distribution. However, the proposal of multivariate models whose marginal variables are defined only in sections of the unit circle and with a flexible dependency structure is limited. In this work, we propose constructing multivariate models where each marginal variable is a circular variable defined only in the first quadrant of the unit circle. Our approach is based on the concept of copula functions. The inferences for the proposed models rely on generating samples of the posterior joint density of all parameters involved in the models. This is achieved by applying a conditional approach that allows inferences to be made using a two-stage sampling. The proposed methodology is illustrated with both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11784v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joel Montesinos-Vazquez, Gabriel N\'u\~nez-Antonio</dc:creator>
    </item>
    <item>
      <title>A Relativity-Based Framework for Statistical Testing Guided by the Independence of Ancillary Statistics: Methodology and Nonparametric Illustrations</title>
      <link>https://arxiv.org/abs/2507.11816</link>
      <description>arXiv:2507.11816v1 Announce Type: new 
Abstract: This paper introduces a decision-theoretic framework for constructing and evaluating test statistics based on their relationship with ancillary statistics-quantities whose distributions remain fixed under the null and alternative hypotheses. Rather than focusing solely on maximizing discriminatory power, the proposed approach emphasizes reducing dependence between a test statistic and relevant ancillary structures. We show that minimizing such dependence can yield most powerful (MP) procedures. A Basu-type independence result is established, and we demonstrate that certain MP statistics also characterize the underlying data distribution. The methodology is illustrated through modifications of classical nonparametric tests, including the Shapiro-Wilk, Anderson-Darling, and Kolmogorov-Smirnov tests, as well as a test for the center of symmetry. Simulation studies highlight the power and robustness of the proposed procedures. The framework is computationally simple and offers a principled strategy for improving statistical testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11816v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Albert Vexler, Douglas Landsittel</dc:creator>
    </item>
    <item>
      <title>R2 priors for Grouped Variance Decomposition in High-dimensional Regression</title>
      <link>https://arxiv.org/abs/2507.11833</link>
      <description>arXiv:2507.11833v1 Announce Type: new 
Abstract: We introduce the Group-R2 decomposition prior, a hierarchical shrinkage prior that extends R2-based priors to structured regression settings with known groups of predictors. By decomposing the prior distribution of the coefficient of determination R2 in two stages, first across groups, then within groups, the prior enables interpretable control over model complexity and sparsity. We derive theoretical properties of the prior, including marginal distributions of coefficients, tail behavior, and connections to effective model complexity. Through simulation studies, we evaluate the conditions under which grouping improves predictive performance and parameter recovery compared to priors that do not account for groups. Our results provide practical guidance for prior specification and highlight both the strengths and limitations of incorporating grouping into R2-based shrinkage priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11833v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier Enrique Aguilar, David Kohns, Aki Vehtari, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Bias reduction method for prior event rate ratio, with application to emergency department visit rates in patients with advanced cancer</title>
      <link>https://arxiv.org/abs/2507.11861</link>
      <description>arXiv:2507.11861v1 Announce Type: new 
Abstract: Objectives: Prior event rate ratio (PERR) is a promising approach to control confounding in observational and real-world evidence research. One of its assumptions is that occurrence of outcome events does not influence later event rate, or in other words, absence of 'event dependence'. This study proposes, evaluates and illustrates a bias reduction method when this assumption is violated. Study Design and Setting: We propose the conditional frailty method for implementation of PERR in the presence of event dependence and evaluate its performance by simulation. We demonstrate the use of the method with a study of emergency department visit rate and palliative care in patients with advanced cancer in Singapore. Results: Simulations showed that, in the presence of negative (positive) event dependence, the crude PERR estimate of treatment effect was biased towards (away from) the null value. The proposed method successfully reduced the bias, with median of absolute level of relative bias at about 5%. Dynamic random-intercept modelling revealed positive event dependence in emergency department visits among patients with advanced cancer. While conventional time-to-event regression analysis with covariate adjustment estimated higher rate of emergency department visits among palliative care recipients (HR=3.61, P&lt;0.001), crude PERR estimate and the proposed PERR estimate were 1.45 (P=0.22) and 1.22 (P=0.57), respectively. Conclusions: The proposed bias reduction method mitigates the impact of violation of the PERR assumption of absence of event dependence. It allows broader application of the PERR approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11861v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangmei Ma, Chetna Malhotra, Eric Andrew Finkelstein, Yin Bun Cheung</dc:creator>
    </item>
    <item>
      <title>Regularized k-POD: Sparse k-means clustering for high-dimensional missing data</title>
      <link>https://arxiv.org/abs/2507.11884</link>
      <description>arXiv:2507.11884v1 Announce Type: new 
Abstract: The classical k-means clustering, based on distances computed from all data features, cannot be directly applied to incomplete data with missing values. A natural extension of k-means to missing data, namely k-POD, uses only the observed entries for clustering and is both computationally efficient and flexible. However, for high-dimensional missing data including features irrelevant to the underlying cluster structure, the presence of such irrelevant features leads to the bias of k-POD in estimating cluster centers, thereby damaging its clustering effect. Nevertheless, the existing k-POD method performs well in low-dimensional cases, highlighting the importance of addressing the bias issue. To this end, in this paper, we propose a regularized k-POD clustering method that applies feature-wise regularization on cluster centers into the existing k-POD clustering. Such a penalty on cluster centers enables us to effectively reduce the bias of k-POD for high-dimensional missing data. To the best of our knowledge, our method is the first to mitigate bias in k-means-type clustering for high-dimensional missing data, while retaining the computational efficiency and flexibility. Simulation results verify that the proposed method effectively reduces bias and improves clustering performance. Applications to real-world single-cell RNA sequencing data further show the utility of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11884v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Guan, Yoshikazu Terada</dc:creator>
    </item>
    <item>
      <title>Bootstrap prediction intervals for the age distribution of life-table death counts</title>
      <link>https://arxiv.org/abs/2507.11946</link>
      <description>arXiv:2507.11946v1 Announce Type: new 
Abstract: We introduce a nonparametric bootstrap procedure based on a dynamic factor model to construct pointwise prediction intervals for period life-table death counts. The age distribution of death counts is an example of constrained data, which are nonnegative and have a constrained integral. A centered log-ratio transformation is used to remove the constraints. With a time series of unconstrained data, we introduce our bootstrap method to construct prediction intervals, thereby quantifying forecast uncertainty. The bootstrap method utilizes a dynamic factor model to capture both nonstationary and stationary patterns through a two-stage functional principal component analysis. To capture parameter uncertainty, the estimated principal component scores and model residuals are sampled with replacement. Using the age- and sex-specific life-table deaths for Australia and the United Kingdom, we study the empirical coverage probabilities and compare them with the nominal ones. The bootstrap method has superior interval forecast accuracy, especially for the one-step-ahead forecast horizon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11946v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Overcoming Standardization: Revealing Hidden Age Patterns of Suicide with Spatiotemporal Models</title>
      <link>https://arxiv.org/abs/2507.12033</link>
      <description>arXiv:2507.12033v1 Announce Type: new 
Abstract: Indirect standardization is widely used in disease mapping to control for confounding, but relies on restrictive assumptions that may bias estimates if violated. Using data on suicide-related emergency calls, this study highlights such limitations and proposes age-structured hierarchical Bayesian models as an alternative. These models incorporate space-time, space-age, and time-age interactions, allowing for more accurate estimation without strong assumptions. The results show improved model fit, especially when including age effects. The best model reveals a rising temporal trend (2017--2022), a nonlinear age pattern, and stronger risk increases among younger individuals compared to older ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12033v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>J. Mart\'in-Pozuelo (Department of Statistics and Operations Research. University of Valencia), A. L\'opez-Qu\'ilez (Department of Statistics and Operations Research. University of Valencia), X. Barber (Joint Research Unit UMH-FISABIO), M. Marco (Department of Social Psychology, University of Valencia)</dc:creator>
    </item>
    <item>
      <title>Assessing the Impact of Covariate Distribution and Positivity Violation on Weighting-Based Indirect Comparisons: a Simulation Study</title>
      <link>https://arxiv.org/abs/2507.12241</link>
      <description>arXiv:2507.12241v1 Announce Type: new 
Abstract: Population-Adjusted Indirect Comparisons (PAICs) are used to estimate treatment effects when direct comparisons are infeasible and individual patient data (IPD) are only available for one trial. Among PAIC methods, Matching-Adjusted Indirect Comparison (MAIC) is the most widely used. However, little is known about how MAIC performs under challenging conditions such as limited covariate overlap or markedly non-normal covariate distributions.
  We conducted a Monte Carlo simulation study comparing three estimators: (i) MAIC matching first moment (MAIC-1), (ii) MAIC matching first and second moments (MAIC-2), and (iii) a benchmark method leveraging full IPD -- Propensity Score Weighting (PSW). We examined eight scenarios ranging from ideal conditions to situations with positivity violations and non-normal (including bimodal) covariate distributions. We assessed both anchored and unanchored estimators and examined the impact of adjustment model misspecification. We also applied these estimators to real-world data from the AKIKI and AKIKI-2 trials, comparing renal replacement therapy strategies in critically ill patients.
  MAIC-1 demonstrated robust performance, remaining unbiased in the presence of moderate positivity violations and non-normal covariates, while MAIC-2 and PSW appeared more sensitive to positivity violations. All methods showed substantial bias when key confounders were omitted, emphasizing the importance of correct model specification. In real-world data, a consistent trend was found with MAIC-1 showing narrower confidence intervals with positivity violation.
  Our findings support the cautious use of unanchored MAICs and highlight MAIC-1's resilience across moderate violations of assumptions. However, the method's limited flexibility underscores the need for careful use in real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12241v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnaud Serret-Larmande, J\'er\^ome Lambert, St\'ephane Gaudry, David Hajage</dc:creator>
    </item>
    <item>
      <title>Does $K$-fold CV based penalty perform variable selection or does it lead to $n^{1/2}$-consistency in Lasso?</title>
      <link>https://arxiv.org/abs/2507.12457</link>
      <description>arXiv:2507.12457v1 Announce Type: new 
Abstract: Least absolute shrinkage and selection operator or Lasso, introduced by Tibshirani (1996), is one of the widely used regularization methods in regression. It is observed that the properties of Lasso vary wildly depending on the choice of the penalty parameter. The recent results of Lahiri (2021) suggest that, depending on the nature of the penalty parameter, Lasso can either be variable selection consistent or be $n^{1/2}-$consistent. However, practitioners generally implement Lasso by choosing the penalty parameter in a data-dependent way, the most popular being the $K$-fold cross-validation. In this paper, we explore the variable selection consistency and $n^{1/2}-$consistency of Lasso when the penalty is chosen based on $K$-fold cross-validation with $K$ being fixed. We consider the fixed-dimensional heteroscedastic linear regression model and show that Lasso with $K$-fold cross-validation based penalty is $n^{1/2}-$consistent, but not variable selection consistent. We also establish the $n^{1/2}-$consistency of the $K$-fold cross-validation based penalty as an intermediate result. Additionally, as a consequence of $n^{1/2}-$consistency, we establish the validity of Bootstrap to approximate the distribution of the Lasso estimator based on $K-$fold cross-validation. We validate the Bootstrap approximation in finite samples based on a moderate simulation study. Thus, our results essentially justify the use of $K$-fold cross-validation in practice to draw inferences based on $n^{1/2}-$scaled pivotal quantities in Lasso regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12457v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mayukh Choudhury, Debraj Das</dc:creator>
    </item>
    <item>
      <title>Inference on Optimal Policy Values and Other Irregular Functionals via Smoothing</title>
      <link>https://arxiv.org/abs/2507.11780</link>
      <description>arXiv:2507.11780v1 Announce Type: cross 
Abstract: Constructing confidence intervals for the value of an optimal treatment policy is an important problem in causal inference. Insight into the optimal policy value can guide the development of reward-maximizing, individualized treatment regimes. However, because the functional that defines the optimal value is non-differentiable, standard semi-parametric approaches for performing inference fail to be directly applicable. Existing approaches for handling this non-differentiability fall roughly into two camps. In one camp are estimators based on constructing smooth approximations of the optimal value. These approaches are computationally lightweight, but typically place unrealistic parametric assumptions on outcome regressions. In another camp are approaches that directly de-bias the non-smooth objective. These approaches don't place parametric assumptions on nuisance functions, but they either require the computation of intractably-many nuisance estimates, assume unrealistic $L^\infty$ nuisance convergence rates, or make strong margin assumptions that prohibit non-response to a treatment. In this paper, we revisit the problem of constructing smooth approximations of non-differentiable functionals. By carefully controlling first-order bias and second-order remainders, we show that a softmax smoothing-based estimator can be used to estimate parameters that are specified as a maximum of scores involving nuisance components. In particular, this includes the value of the optimal treatment policy as a special case. Our estimator obtains $\sqrt{n}$ convergence rates, avoids parametric restrictions/unrealistic margin assumptions, and is often statistically efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11780v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin Whitehouse, Morgane Austern, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Newfluence: Boosting Model interpretability and Understanding in High Dimensions</title>
      <link>https://arxiv.org/abs/2507.11895</link>
      <description>arXiv:2507.11895v1 Announce Type: cross 
Abstract: The increasing complexity of machine learning (ML) and artificial intelligence (AI) models has created a pressing need for tools that help scientists, engineers, and policymakers interpret and refine model decisions and predictions. Influence functions, originating from robust statistics, have emerged as a popular approach for this purpose.
  However, the heuristic foundations of influence functions rely on low-dimensional assumptions where the number of parameters $p$ is much smaller than the number of observations $n$. In contrast, modern AI models often operate in high-dimensional regimes with large $p$, challenging these assumptions.
  In this paper, we examine the accuracy of influence functions in high-dimensional settings. Our theoretical and empirical analyses reveal that influence functions cannot reliably fulfill their intended purpose. We then introduce an alternative approximation, called Newfluence, that maintains similar computational efficiency while offering significantly improved accuracy.
  Newfluence is expected to provide more accurate insights than many existing methods for interpreting complex AI models and diagnosing their issues. Moreover, the high-dimensional framework we develop in this paper can also be applied to analyze other popular techniques, such as Shapley values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11895v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haolin Zou, Arnab Auddy, Yongchan Kwon, Kamiar Rahnama Rad, Arian Maleki</dc:creator>
    </item>
    <item>
      <title>Enhancing Signal Proportion Estimation Through Leveraging Arbitrary Covariance Structures</title>
      <link>https://arxiv.org/abs/2507.11922</link>
      <description>arXiv:2507.11922v1 Announce Type: cross 
Abstract: Accurately estimating the proportion of true signals among a large number of variables is crucial for enhancing the precision and reliability of scientific research. Traditional signal proportion estimators often assume independence among variables and specific signal sparsity conditions, limiting their applicability in real-world scenarios where such assumptions may not hold. This paper introduces a novel signal proportion estimator that leverages arbitrary covariance dependence information among variables, thereby improving performance across a wide range of sparsity levels and dependence structures. Building on previous work that provides lower confidence bounds for signal proportions, we extend this approach by incorporating the principal factor approximation procedure to account for variable dependence. Our theoretical insights offer a deeper understanding of how signal sparsity, signal intensity, and covariance dependence interact. By comparing the conditions for estimation consistency before and after dependence adjustment, we highlight the advantages of integrating dependence information across different contexts. This theoretical foundation not only validates the effectiveness of the new estimator but also guides its practical application, ensuring reliable use in diverse scenarios. Through extensive simulations, we demonstrate that our method outperforms state-of-the-art estimators in both estimation accuracy and the detection of weaker signals that might otherwise go undetected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11922v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingtian Bai, Xinge Jessie Jeng</dc:creator>
    </item>
    <item>
      <title>Data Synchronization at High Frequencies</title>
      <link>https://arxiv.org/abs/2507.12220</link>
      <description>arXiv:2507.12220v1 Announce Type: cross 
Abstract: Asynchronous trading in high-frequency financial markets introduces significant biases into econometric analysis, distorting risk estimates and leading to suboptimal portfolio decisions. Existing synchronization methods, such as the previous-tick approach, suffer from information loss and create artificial price staleness. We introduce a novel framework that recasts the data synchronization challenge as a constrained matrix completion problem. Our approach recovers the potential matrix of high-frequency price increments by minimizing its nuclear norm -- capturing the underlying low-rank factor structure -- subject to a large-scale linear system derived from observed, asynchronous price changes. Theoretically, we prove the existence and uniqueness of our estimator and establish its convergence rate. A key theoretical insight is that our method accurately and robustly leverages information from both frequently and infrequently traded assets, overcoming a critical difficulty of efficiency loss in traditional methods. Empirically, using extensive simulations and a large panel of S&amp;P 500 stocks, we demonstrate that our method substantially outperforms established benchmarks. It not only achieves significantly lower synchronization errors, but also corrects the bias in systematic risk estimates (i.e., eigenvalues) and the estimate of betas caused by stale prices. Crucially, portfolios constructed using our synchronized data yield consistently and economically significant higher out-of-sample Sharpe ratios. Our framework provides a powerful tool for uncovering the true dynamics of asset prices, with direct implications for high-frequency risk management, algorithmic trading, and econometric inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12220v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinbing Kong, Cheng Liu, Bin Wu</dc:creator>
    </item>
    <item>
      <title>Fast Variational Bayes for Large Spatial Data</title>
      <link>https://arxiv.org/abs/2507.12251</link>
      <description>arXiv:2507.12251v1 Announce Type: cross 
Abstract: Recent variational Bayes methods for geospatial regression, proposed as an alternative to computationally expensive Markov chain Monte Carlo (MCMC) sampling, have leveraged Nearest Neighbor Gaussian processes (NNGP) to achieve scalability. Yet, these variational methods remain inferior in accuracy and speed compared to spNNGP, the state-of-the-art MCMC-based software for NNGP. We introduce spVarBayes, a suite of fast variational Bayesian approaches for large-scale geospatial data analysis using NNGP. Our contributions are primarily computational. We replace auto-differentiation with a combination of calculus of variations, closed-form gradient updates, and linear response corrections for improved variance estimation. We also accommodate covariates (fixed effects) in the model and offer inference on the variance parameters. Simulation experiments demonstrate that we achieve comparable accuracy to spNNGP but with reduced computational costs, and considerably outperform existing variational inference methods in terms of both accuracy and speed. Analysis of a large forest canopy height dataset illustrates the practical implementation of proposed methods and shows that the inference results are consistent with those obtained from the MCMC approach. The proposed methods are implemented in publicly available Github R-package spVarBayes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12251v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiafang Song, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>A Framework for Nonstationary Gaussian Processes with Neural Network Parameters</title>
      <link>https://arxiv.org/abs/2507.12262</link>
      <description>arXiv:2507.12262v1 Announce Type: cross 
Abstract: Gaussian processes have become a popular tool for nonparametric regression because of their flexibility and uncertainty quantification. However, they often use stationary kernels, which limit the expressiveness of the model and may be unsuitable for many datasets. We propose a framework that uses nonstationary kernels whose parameters vary across the feature space, modeling these parameters as the output of a neural network that takes the features as input. The neural network and Gaussian process are trained jointly using the chain rule to calculate derivatives. Our method clearly describes the behavior of the nonstationary parameters and is compatible with approximation methods for scaling to large datasets. It is flexible and easily adapts to different nonstationary kernels without needing to redesign the optimization procedure. Our methods are implemented with the GPyTorch library and can be readily modified. We test a nonstationary variance and noise variant of our method on several machine learning datasets and find that it achieves better accuracy and log-score than both a stationary model and a hierarchical model approximated with variational inference. Similar results are observed for a model with only nonstationary variance. We also demonstrate our approach's ability to recover the nonstationary parameters of a spatial dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12262v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zachary James, Joseph Guinness</dc:creator>
    </item>
    <item>
      <title>Forecasting sub-population mortality using credibility theory</title>
      <link>https://arxiv.org/abs/2507.12330</link>
      <description>arXiv:2507.12330v1 Announce Type: cross 
Abstract: The focus of the present paper is to forecast mortality rates for small sub-populations that are parts of a larger super-population. In this setting the assumption is that it is possible to produce reliable forecasts for the super-population, but the sub-populations may be too small or lack sufficient history to produce reliable forecasts if modelled separately. This setup is aligned with the ideas that underpin credibility theory, and in the present paper the classical credibility theory approach is extended to be able to handle the situation where future mortality rates are driven by a latent stochastic process, as is the case for, e.g., Lee-Carter type models.
  This results in sub-population credibility predictors that are weighted averages of expected future super-population mortality rates and expected future sub-population specific mortality rates. Due to the predictor's simple structure it is possible to derive an explicit expression for the mean squared error of prediction. Moreover, the proposed credibility modelling approach does not depend on the specific form of the super-population model, making it broadly applicable regardless of the chosen forecasting model for the super-population.
  The performance of the suggested sub-population credibility predictor is illustrated on simulated population data. These illustrations highlight how the credibility predictor serves as a compromise between only using a super-population model, and only using a potentially unreliable sub-population specific model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12330v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Lindholm, Gabriele Pittarello</dc:creator>
    </item>
    <item>
      <title>A general Bayesian approach to design adaptive clinical trials with time-to-event outcomes</title>
      <link>https://arxiv.org/abs/2303.00901</link>
      <description>arXiv:2303.00901v2 Announce Type: replace 
Abstract: Clinical trials are an integral component of medical research. Trials require careful design to, for example, maintain the safety of participants, use resources efficiently and allow clinically meaningful conclusions to be drawn. Adaptive clinical trials (i.e. trials that can be altered based on evidence that has accrued) are often more efficient, informative and ethical than standard or non-adaptive trials because they require fewer participants, target more promising treatments, and can stop early with sufficient evidence of effectiveness or harm. The design of adaptive trials requires the pre-specification of adaptions that are permissible throughout the conduct of the trial. Proposed adaptive designs are then usually evaluated through simulation which provides indicative metrics of performance (e.g. statistical power and type-1 error) under different scenarios. Trial simulation requires assumptions about the data generating process to be specified but correctly specifying these in practice can be difficult, particularly for new and emerging diseases. To address this, we propose an approach to design adaptive clinical trials without needing to specify the complete data generating process. To facilitate this, we consider a general Bayesian framework where inference about the treatment effect on a time-to-event outcome can be performed via the partial likelihood. As a consequence, the proposed approach to evaluate trial designs is robust to the specific form of the baseline hazard function. The benefits of this approach are demonstrated through the redesign of a recent clinical trial to evaluate whether a third dose of a vaccine provides improved protection against gastroenteritis in Australian Indigenous infants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.00901v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James M. McGree, Antony M. Overstall, Mark Jones, Robert K. Mahar</dc:creator>
    </item>
    <item>
      <title>Addressing Outcome Reporting Bias in Meta-analysis: A Selection Model Perspective</title>
      <link>https://arxiv.org/abs/2408.05747</link>
      <description>arXiv:2408.05747v2 Announce Type: replace 
Abstract: Outcome Reporting Bias (ORB) poses significant threats to the validity of meta-analytic findings. It occurs when researchers selectively report outcomes based on the significance or direction of results, potentially leading to distorted treatment effect estimates. Despite its critical implications, ORB remains an under-recognized issue, with few comprehensive adjustment methods available. The goal of this research is to investigate ORB-adjustment techniques through a selection model lens, thereby extending some of the existing methodological approaches available in the literature. To gain a better insight into the effects of ORB in meta-analysis of clinical trials, specifically in the presence of heterogeneity, and to assess the effectiveness of ORB-adjustment techniques, we apply the methodology to real clinical data affected by ORB and conduct a simulation study focusing on treatment effect estimation with a secondary interest in heterogeneity quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05747v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandra Gaia Saracini, Leonhard Held</dc:creator>
    </item>
    <item>
      <title>Interval Estimation of Coefficients in Penalized Regression Models of Insurance Data</title>
      <link>https://arxiv.org/abs/2410.01008</link>
      <description>arXiv:2410.01008v5 Announce Type: replace 
Abstract: The Tweedie exponential dispersion family is a popular choice among many to model insurance losses that consist of zero-inflated semicontinuous data. In such data, it is often important to obtain credibility (inference) of the most important features that describe the endogenous variables. Post-selection inference is the standard procedure in statistics to obtain confidence intervals of model parameters after performing a feature extraction procedure. For a linear model, the lasso estimate often has non-negligible estimation bias for large coefficients corresponding to exogenous variables. To have valid inference on those coefficients, it is necessary to correct the bias of the lasso estimate. Traditional statistical methods, such as hypothesis testing or standard confidence interval construction might lead to incorrect conclusions during post-selection, as they are generally too optimistic. Here we discuss a few methodologies for constructing confidence intervals of the coefficients after feature selection in the Generalized Linear Model (GLM) family with application to insurance data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01008v5</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Vol 04: Data Science and Statistical Modeling in Business: Towards Operational and Business Excellence, 2025</arxiv:journal_reference>
      <dc:creator>Alokesh Manna, Zijian Huang, Dipak K. Dey, Yuwen Gu, Robin He</dc:creator>
    </item>
    <item>
      <title>Just Trial Once: Ongoing Causal Validation of Machine Learning Models</title>
      <link>https://arxiv.org/abs/2502.09467</link>
      <description>arXiv:2502.09467v2 Announce Type: replace 
Abstract: Machine learning (ML) models are increasingly used as decision-support tools in high-risk domains. Evaluating the causal impact of deploying such models can be done with a randomized controlled trial (RCT) that randomizes users to ML vs. control groups and assesses the effect on relevant outcomes. However, ML models are inevitably updated over time, and we often lack evidence for the causal impact of these updates. While the causal effect could be repeatedly validated with ongoing RCTs, such experiments are expensive and time-consuming to run. In this work, we present an alternative solution: using only data from a prior RCT, we give conditions under which the causal impact of a new ML model can be precisely bounded or estimated, even if it was not included in the RCT. Our assumptions incorporate two realistic constraints: ML predictions are often deterministic, and their impacts depend on user trust in the model. Based on our analysis, we give recommendations for trial designs that maximize our ability to assess future versions of an ML model. Our hope is that our trial design recommendations will save practitioners time and resources while allowing for quicker deployments of updates to ML models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09467v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob M. Chen, Michael Oberst</dc:creator>
    </item>
    <item>
      <title>An Improved Satterthwaite Effective Degrees of Freedom Correction for Weighted Syntheses of Variance</title>
      <link>https://arxiv.org/abs/2503.22080</link>
      <description>arXiv:2503.22080v5 Announce Type: replace 
Abstract: This article presents an improved approximation for the effective degrees of freedom in the Satterthwaite (1941, 1946) method which estimates the distribution of a weighted combination of variance components The standard Satterthwaite approximation assumes a scaled chisquare distribution for the composite variance estimator but is known to be biased downward when component degrees of freedom are small. Building on recent work by von Davier (2025), we propose an adjusted estimator that corrects this bias by modifying both the numerator and denominator of the traditional formula. The new approximation incorporates a weighted average of component degrees of freedom and a scaling factor that ensures consistency as the number of components or their degrees of freedom increases. We demonstrate the utility of this adjustment in practical settings, including Rubin's (1987) total variance estimation in multiple imputations, where weighted variance combinations are common. The proposed estimator generalizes and further improves von Davier's (2025) unweighted case and more accurately approximates synthetic variance estimators with arbitrary weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22080v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matthias von Davier</dc:creator>
    </item>
    <item>
      <title>Deep Generative Modeling with Spatial and Network Images: An Explainable AI (XAI) Approach</title>
      <link>https://arxiv.org/abs/2505.12743</link>
      <description>arXiv:2505.12743v2 Announce Type: replace 
Abstract: This article addresses the challenge of modeling the amplitude of spatially indexed low frequency fluctuations (ALFF) in resting state functional MRI as a function of cortical structural features and a multi-task coactivation network in the Adolescent Brain Cognitive Development (ABCD) Study. It proposes a generative model that integrates effects of spatially-varying inputs and a network-valued input using deep neural networks to capture complex non-linear and spatial associations with the output. The method models spatial smoothness, accounts for subject heterogeneity and complex associations between network and spatial images at different scales, enables accurate inference of each images effect on the output image, and allows prediction with uncertainty quantification via Monte Carlo dropout, contributing to one of the first Explainable AI (XAI) frameworks for heterogeneous imaging data. The model is highly scalable to high-resolution data without the heavy pre-processing or summarization often required by Bayesian methods. Empirical results demonstrate its strong performance compared to existing statistical and deep learning methods. We applied the XAI model to the ABCD data which revealed associations between cortical features and ALFF throughout the entire brain. Our model performed comparably to existing methods in predictive accuracy but provided superior uncertainty quantification and faster computation, demonstrating its effectiveness for large-scale neuroimaging analysis. Open-source software in Python for XAI is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12743v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeseul Jeon, Rajarshi Guhaniyogi, Aaron Scheffler</dc:creator>
    </item>
    <item>
      <title>Modeling the uncertainty on the covariance matrix for probabilistic forecast reconciliation</title>
      <link>https://arxiv.org/abs/2506.19554</link>
      <description>arXiv:2506.19554v2 Announce Type: replace 
Abstract: In forecast reconciliation, the covariance matrix of the base forecasts errors plays a crucial role. Typically, this matrix is estimated, and then treated as known. In contrast, we propose a Bayesian reconciliation model that accounts for the uncertainty in the estimation of the covariance matrix. This leads to a reconciled predictive distribution that follows a multivariate t-distribution, obtained in closed-form, rather than a multivariate Gaussian. We evaluate our method on three tourism-related datasets, including a new publicly available dataset. Empirical results show that our approach consistently improves prediction intervals compared to Gaussian reconciliation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19554v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Carrara, Dario Azzimonti, Giorgio Corani, Lorenzo Zambon</dc:creator>
    </item>
    <item>
      <title>Detection of evolutionary shifts in variance under an Ornsten-Uhlenbeck model</title>
      <link>https://arxiv.org/abs/2312.17480</link>
      <description>arXiv:2312.17480v3 Announce Type: replace-cross 
Abstract: Sudden changes in environmental conditions can lead to evolutionary shifts not only in the optimal trait value, but also in the diffusion variance under the Ornstein-Uhlenbeck (OU) model. While several methods have been developed to detect shifts in optimal values, few explicitly account for concurrent shifts in both evolutionary variance and diffusion variance. We use a multi-optima and multi-variance OU model to describe trait evolution with shifts in both optimal value and diffusion variance and analyze how covariance between species is affected when shifts in variance occur along the phylogeny. We propose a new method that simultaneously detects shifts in both variance and optimal values by formulating the problem as a variable selection task using an L1-penalized loss function. Our method is implemented in the R package ShiVa (Detection of evolutionary shifts in variance). Through simulations, we compare ShiVa with existing methods that can automatically detect evolutionary shifts under the OU model (l1ou, PhylogeneticEM, and PCMFit). Our method demonstrates improved predictive ability and significantly reduces false positives in detecting optimal value shifts when variance shifts are present. When only shifts in optimal value occur, our method performs comparably to existing approaches. We apply ShiVa to empirical data on floral diameter in Euphorbiaceae and buccal morphology in Centrarchidae sunfishes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17480v3</guid>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wensha Zhang, Lam Si Tung Ho, Toby Kenney</dc:creator>
    </item>
    <item>
      <title>On the optimality of coin-betting for mean estimation</title>
      <link>https://arxiv.org/abs/2412.02640</link>
      <description>arXiv:2412.02640v3 Announce Type: replace-cross 
Abstract: Confidence sequences are sequences of confidence sets that adapt to incoming data while maintaining validity. Recent advances have introduced an algorithmic formulation for constructing some of the tightest confidence sequences for the mean of bounded real random variables. These approaches use a coin-betting framework, where a player sequentially bets on differences between potential mean values and observed data. This work discusses the optimality of such coin-betting formulation among algorithmic frameworks building on e-variables methods to test and estimate the mean of bounded random variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02640v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugenio Clerico</dc:creator>
    </item>
    <item>
      <title>Generalized Venn and Venn-Abers Calibration with Applications in Conformal Prediction</title>
      <link>https://arxiv.org/abs/2502.05676</link>
      <description>arXiv:2502.05676v3 Announce Type: replace-cross 
Abstract: Ensuring model calibration is critical for reliable prediction, yet popular distribution-free methods such as histogram binning and isotonic regression offer only asymptotic guarantees. We introduce a unified framework for Venn and Venn-Abers calibration that extends Vovk's approach beyond binary classification to a broad class of prediction problems defined by generic loss functions. Our method transforms any perfectly in-sample calibrated predictor into a set-valued predictor that, in finite samples, outputs at least one marginally calibrated point prediction. These set predictions shrink asymptotically and converge to a single conditionally calibrated prediction, capturing epistemic uncertainty. We further propose Venn multicalibration, a new approach for achieving finite-sample calibration across subpopulations. For quantile loss, our framework recovers group-conditional and multicalibrated conformal prediction as special cases and yields novel prediction intervals with quantile-conditional coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05676v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Ahmed Alaa</dc:creator>
    </item>
  </channel>
</rss>
