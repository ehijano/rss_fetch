<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Jan 2026 02:40:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Posterior concentration in spatio-temporal Hawkes processes</title>
      <link>https://arxiv.org/abs/2601.03719</link>
      <description>arXiv:2601.03719v1 Announce Type: new 
Abstract: We develop a Bayesian nonparametric framework for inference in spatio-temporal Hawkes processes, extending existing theoretical results beyond the purely temporal setting. Our framework encompasses modelling both the background and triggering components of the Hawkes process through Gaussian Processes priors. Under appropriate smoothness and regularity assumptions on the true parameter and the nonparametric prior family, we derive explicit posterior contraction rates for the conditional intensity function and the model's parameter, in the asymptotic regime of repeatedly observed and independent sequences. Our analysis generalizes known contraction results for purely temporal Hawkes processes to the spatio-temporal setting, which allows to jointly model self-excitation across time and space in event data. These results provide, to our knowledge, the first theoretical guarantees for Bayesian nonparametric methods in spatio-temporal point data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03719v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xenia Miscouridou, Deborah Sulem</dc:creator>
    </item>
    <item>
      <title>The Feldman-H\'ajek Dichotomy for Countable Gaussian Mixtures and their Asymptotic Separability in High Dimensions</title>
      <link>https://arxiv.org/abs/2601.03911</link>
      <description>arXiv:2601.03911v1 Announce Type: new 
Abstract: This paper establishes the theoretical foundations for the asymptotic separability of Gaussian Mixture Models (GMMs) in high dimensions by extending the classical Feldman-H\'ajek theorem. We first prove that a countable mixture of Gaussian measures is a well-defined probability measure. Our primary result, the Gaussian Mixture Dichotomy Theorem, demonstrates that the mutual singularity of individual Gaussian components is a sufficient condition for the mutual singularity of the resulting mixtures. We provide a rigorous proof and further discuss the ``Mixed Case,'' where the presence of even a single equivalent pair of components leads to partial absolute continuity via the Lebesgue decomposition, thereby defining the theoretical limits of perfect classification in infinite-dimensional spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03911v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Umberto Michelucci</dc:creator>
    </item>
    <item>
      <title>Adaptive thresholding for wavelet-based nonparametric heteroskedastic variance estimation on the sphere</title>
      <link>https://arxiv.org/abs/2601.03920</link>
      <description>arXiv:2601.03920v1 Announce Type: new 
Abstract: This paper investigates the nonparametric estimation of a heteroskedastic variance function on the sphere in a regression framework, assuming the variance belongs to a Besov regularity class. A needlet-based estimator is proposed, combining multiresolution analysis with hard thresholding. The method exploits the spatial and spectral localization of needlets to adapt to unknown smoothness and is shown to attain minimax-optimal convergence rates over Besov spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03920v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claudio Durastanti, Radomyra Shevchenko</dc:creator>
    </item>
    <item>
      <title>Progressive Bayesian Confidence Architectures for Cold-Start Personal Health Analytics: Formalizing Early Insight Through Posterior Contraction and Risk-Aware Interpretation</title>
      <link>https://arxiv.org/abs/2601.03299</link>
      <description>arXiv:2601.03299v1 Announce Type: cross 
Abstract: Personal health analytics systems face a persistent cold-start dilemma: users expect meaningful insights early in data collection, while conventional statistical inference requires data volumes that often exceed engagement horizons. Existing approaches either delay inference until fixed statistical thresholds are met -- leading to user disengagement -- or surface heuristic insights without formal uncertainty quantification, risking false confidence. We propose a progressive Bayesian confidence architecture that formalizes early-stage inference through phased interpretation of posterior uncertainty. Drawing on Bayesian updating and epistemic strategies from financial risk modeling under sparse observations, we map posterior contraction to interpretable tiers of insight, ranging from exploratory directional evidence to robust associative inference. We demonstrate the framework's performance through controlled experimentation with synthetic N-of-1 health data, showing that calibrated early insights can be generated within 5--7 days while maintaining explicit epistemic humility. Compared to fixed-threshold baselines requiring 30+ days of data, the proposed approach yields earlier directional signals (mean: 5.3 vs 31.7 days, p&lt;0.001) while controlling false discovery rates below 6% (5.9% at day 30) despite 26-day earlier detection, compared to 0% FDR for fixed-threshold baselines that delay insights by 30 days. In addition, we show strong uncertainty calibration (76% credible interval coverage for ground-truth correlations at day 90). This work contributes a methodological framework for uncertainty-aware early inference in personalized health analytics that bridges the gap between user engagement requirements and statistical rigor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03299v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richik Chakraborty</dc:creator>
    </item>
    <item>
      <title>Sharp concentration inequality for the sum of random variables</title>
      <link>https://arxiv.org/abs/2601.03518</link>
      <description>arXiv:2601.03518v1 Announce Type: cross 
Abstract: We present a universal concentration bound for sums of random variables under arbitrary dependence, and we prove it is asymptotically optimal for every fixed common marginal law. The concentration bound is a direct - yet previously unnoticed - consequence of the subadditivity of expected shortfall, a property well known to financial statisticians. The sharpness result is a significant contribution relying on the construction of worst-case dependency profiles between identically distributed random variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03518v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cosme Louart, Sicheng Tan</dc:creator>
    </item>
    <item>
      <title>Diaconis-Ylvisaker prior penalized likelihood for $p/n \to \kappa \in (0,1)$ logistic regression</title>
      <link>https://arxiv.org/abs/2311.07419</link>
      <description>arXiv:2311.07419v3 Announce Type: replace 
Abstract: We characterise the behavior of the maximum Diaconis--Ylvisaker prior penalized likelihood estimator in high-dimensional logistic regression, where the number of covariates is a fraction $\kappa \in (0,1)$ of the number of observations $n$, as $n \to \infty$. We construct a rescaled estimator with zero asymptotic aggregate bias and define adjusted $Z$-statistics and rescaled penalized likelihood ratio statistics that exhibit the typical null asymptotic distributions, when the covariates are independent multivariate normal with an arbitrary covariance matrix and the linear predictor has asymptotic variance $\gamma^2$. While the maximum likelihood estimate asymptotically exists only for a narrow range of $(\kappa, \gamma)$ values, the maximum Diaconis--Ylvisaker prior penalized likelihood estimate always exists and can be computed directly using standard maximum likelihood routines. Thus, our asymptotic results extend to $(\kappa, \gamma)$ values where the maximum likelihood framework breaks down, with no additional implementation or computational cost. We study the estimator's shrinkage properties, compare the proposed estimation and inference procedures with alternatives that also accommodate proportional asymptotics, and formulate a conjecture -- supported by strong empirical evidence -- that extends our results when the model includes an intercept parameter. Finally, we propose estimation methods for all unknown constants involved in our procedures and demonstrate the theoretical advances through extensive simulation studies and the analysis of digit recognition data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07419v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Sterzinger, Ioannis Kosmidis</dc:creator>
    </item>
    <item>
      <title>The causal interpretation of acceleration factors</title>
      <link>https://arxiv.org/abs/2409.01983</link>
      <description>arXiv:2409.01983v2 Announce Type: replace-cross 
Abstract: In studies of time-to-event outcomes with unmeasured heterogeneity, the hazard ratio for treatment is known to have a complex causal interpretation. Accelerated failure time (AFT) models, which assess the effect on the survival time ratio scale, are often suggested as a better alternative because they model a parameter with direct causal interpretation while allowing straightforward adjustment for measured confounders. In this work, we formalize the causal interpretation of the acceleration factor in AFT models using structural causal models and data under independent censoring. We prove that the acceleration factor is a valid causal effect measure, even in the presence of frailty and treatment effect heterogeneity. Through simulations, we show that the acceleration factor better captures the causal effect than the hazard ratio when both AFT and conditional proportional hazards models apply. Additionally, we extend the interpretation to systems with time-dependent acceleration factors, illustrating the impossibility of distinguishing between a time-varying homogeneous effect and unmeasured effect heterogeneity. While the causal interpretation of acceleration factors is promising, we caution practitioners about potential challenges for the interpretation in the presence of effect heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01983v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mari Brathovde, Hein Putter, Morten Valberg, Richard A. J. Post</dc:creator>
    </item>
    <item>
      <title>Causal Invariance Learning via Efficient Nonconvex Optimization</title>
      <link>https://arxiv.org/abs/2412.11850</link>
      <description>arXiv:2412.11850v3 Announce Type: replace-cross 
Abstract: Identifying the causal relationship among variables from observational data is an important yet challenging task. This work focuses on identifying the direct causes of an outcome and estimating their magnitude, i.e., learning the causal outcome model. Data from multiple environments provide valuable opportunities to uncover causality by exploiting the invariance principle that the causal outcome model holds across heterogeneous environments. Based on the invariance principle, we propose the Negative Weighted Distributionally Robust Optimization (NegDRO) framework to learn an invariant prediction model. NegDRO minimizes the worst-case combination of risks across multiple environments and enforces invariance by allowing potential negative weights. Under the additive interventions regime, we establish three major contributions: (i) On the statistical side, we provide sufficient and nearly necessary identification conditions under which the invariant prediction model coincides with the causal outcome model; (ii) On the optimization side, despite the nonconvexity of NegDRO, we establish its benign optimization landscape, where all stationary points lie close to the true causal outcome model; (iii) On the computational side, we develop a gradient-based algorithm that provably converges to the causal outcome model, with non-asymptotic convergence rates in both sample size and gradient-descent iterations. In particular, our method avoids exhaustive combinatorial searches over exponentially many subsets of covariates found in the literature, ensuring scalability even when the dimension of the covariates is large. To our knowledge, this is the first causal invariance learning method that finds the approximate global optimality for a nonconvex optimization problem efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11850v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Wang, Yifan Hu, Peter B\"uhlmann, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>EW D-optimal Designs for Experiments with Mixed Factors</title>
      <link>https://arxiv.org/abs/2505.00629</link>
      <description>arXiv:2505.00629v3 Announce Type: replace-cross 
Abstract: We characterize EW D-optimal designs as robust designs against unknown parameter values for experiments under a general parametric model with discrete and continuous factors. When a pilot study is available, we recommend sample-based EW D-optimal designs for subsequent experiments. Otherwise, we recommend EW D-optimal designs under a prior distribution for model parameters. We propose an EW ForLion algorithm for finding EW D-optimal designs with mixed factors, and justify that the designs found by our algorithm are EW D-optimal. To facilitate potential users in practice, we also develop a rounding algorithm that converts an approximate design with mixed factors to exact designs with prespecified grid points and the total number of experimental units. By applying our algorithms for real experiments under multinomial logistic models or generalized linear models, we show that our designs are highly efficient with respect to locally D-optimal designs and more robust against parameter value misspecifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00629v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siting Lin, Yifei Huang, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Learning Confidence Ellipsoids and Applications to Robust Subspace Recovery</title>
      <link>https://arxiv.org/abs/2512.16875</link>
      <description>arXiv:2512.16875v3 Announce Type: replace-cross 
Abstract: We study the problem of finding confidence ellipsoids for an arbitrary distribution in high dimensions. Given samples from a distribution $\mathcal{D}$ and a confidence parameter $\alpha$, the goal is to find the smallest volume ellipsoid $E$ which has probability mass $\Pr_{\mathcal{D}}[E] \ge 1-\alpha$. Ellipsoids are a highly expressive class of confidence sets as they can capture correlations in the distribution, and can approximate any convex set. This problem has been studied in many different communities. In statistics, this is the classic minimum volume estimator introduced by Rousseeuw as a robust non-parametric estimator of location and scatter. However in high dimensions, it becomes NP-hard to obtain any non-trivial approximation factor in volume when the condition number $\beta$ of the ellipsoid (ratio of the largest to the smallest axis length) goes to $\infty$. This motivates the focus of our paper: can we efficiently find confidence ellipsoids with volume approximation guarantees when compared to ellipsoids of bounded condition number $\beta$?
  Our main result is a polynomial time algorithm that finds an ellipsoid $E$ whose volume is within a $O(\beta)^{\gamma d}$ multiplicative factor of the volume of best $\beta$-conditioned ellipsoid while covering at least $1-O(\alpha/\gamma)$ probability mass for any $\gamma \in (0,1)$. We complement this with a computational hardness result that shows that such a dependence seems necessary up to constants in the exponent. The algorithm and analysis uses the rich primal-dual structure of the minimum volume enclosing ellipsoid and the geometric Brascamp-Lieb inequality. As a consequence, we obtain the first polynomial time algorithm with approximation guarantees on worst-case instances of the robust subspace recovery problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16875v3</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 08 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Gao, Liren Shan, Vaidehi Srinivas, Aravindan Vijayaraghavan</dc:creator>
    </item>
  </channel>
</rss>
