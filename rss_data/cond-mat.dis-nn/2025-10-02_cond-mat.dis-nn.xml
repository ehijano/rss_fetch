<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Oct 2025 01:47:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A universal compression theory: Lottery ticket hypothesis and superpolynomial scaling laws</title>
      <link>https://arxiv.org/abs/2510.00504</link>
      <description>arXiv:2510.00504v1 Announce Type: cross 
Abstract: When training large-scale models, the performance typically scales with the number of parameters and the dataset size according to a slow power law. A fundamental theoretical and practical question is whether comparable performance can be achieved with significantly smaller models and substantially less data. In this work, we provide a positive and constructive answer. We prove that a generic permutation-invariant function of $d$ objects can be asymptotically compressed into a function of $\operatorname{polylog} d$ objects with vanishing error. This theorem yields two key implications: (Ia) a large neural network can be compressed to polylogarithmic width while preserving its learning dynamics; (Ib) a large dataset can be compressed to polylogarithmic size while leaving the loss landscape of the corresponding model unchanged. (Ia) directly establishes a proof of the \textit{dynamical} lottery ticket hypothesis, which states that any ordinary network can be strongly compressed such that the learning dynamics and result remain unchanged. (Ib) shows that a neural scaling law of the form $L\sim d^{-\alpha}$ can be boosted to an arbitrarily fast power law decay, and ultimately to $\exp(-\alpha' \sqrt[m]{d})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00504v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong-Yi Wang, Di Luo, Tomaso Poggio, Isaac L. Chuang, Liu Ziyin</dc:creator>
    </item>
    <item>
      <title>Spin glasses and the Parisi formula</title>
      <link>https://arxiv.org/abs/2510.01054</link>
      <description>arXiv:2510.01054v1 Announce Type: cross 
Abstract: Spin glasses are models of statistical mechanics in which a large number of simple elements interact with one another in a disordered fashion. One of the fundamental results of the theory is the Parisi formula, which identifies the limit of the free energy of a large class of such models. Yet many interesting models remain out of reach of the classical theory, and direct generalizations of the Parisi formula yield invalid predictions. I will report here on some partial progress towards the resolution of this problem, which also brings a new perspective on classical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01054v1</guid>
      <category>math.PR</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-Christophe Mourrat</dc:creator>
    </item>
    <item>
      <title>Theory of Scaling Laws for In-Context Regression: Depth, Width, Context and Time</title>
      <link>https://arxiv.org/abs/2510.01098</link>
      <description>arXiv:2510.01098v1 Announce Type: cross 
Abstract: We study in-context learning (ICL) of linear regression in a deep linear self-attention model, characterizing how performance depends on various computational and statistical resources (width, depth, number of training steps, batch size and data per context). In a joint limit where data dimension, context length, and residual stream width scale proportionally, we analyze the limiting asymptotics for three ICL settings: (1) isotropic covariates and tasks (ISO), (2) fixed and structured covariance (FS), and (3) where covariances are randomly rotated and structured (RRS). For ISO and FS settings, we find that depth only aids ICL performance if context length is limited. Alternatively, in the RRS setting where covariances change across contexts, increasing the depth leads to significant improvements in ICL, even at infinite context length. This provides a new solvable toy model of neural scaling laws which depends on both width and depth of a transformer and predicts an optimal transformer shape as a function of compute. This toy model enables computation of exact asymptotics for the risk as well as derivation of powerlaws under source/capacity conditions for the ICL tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01098v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blake Bordelon, Mary I. Letey, Cengiz Pehlevan</dc:creator>
    </item>
    <item>
      <title>Depinning of KPZ Interfaces in Fractional Brownian Landscapes</title>
      <link>https://arxiv.org/abs/2510.01103</link>
      <description>arXiv:2510.01103v1 Announce Type: cross 
Abstract: We explore the critical dynamics of driven interfaces propagating through a two dimensional disordered medium with long range spatial correlations, modeled using fractional Brownian motion. Departing from conventional models with uncorrelated disorder, we introduce quenched noise fields characterized by a tunable Hurst exponent H, allowing systematic control over the spatial structure of the background medium. The interface evolution is governed by a quenched Kardar Parisi Zhang equation modified to account for correlated disorder, namely QKPZ. Through analytical scaling analysis, we uncover how the presence of long-range correlations reshapes the depinning transition, alters the critical force Fc, and gives rise to a family of critical exponents that depend continuously on H. Our findings reveal a rich interplay between disorder correlations and the non-linearity term in QKPZ, leading to a breakdown of conventional universality and the emergence of nontrivial scaling behaviors. The exponents are found to change by H in the anticorrelation regime, while they are nearly constant in the correlation regime, suggesting a super-universal behavior for the latter. By a comparison with the quenched Edwards Wilkinson model, we study the effect of the non linearity term in the QKPZ model. This work provides new insights into the physics of driven systems in complex environments and paves the way for understanding transport in correlated disordered media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01103v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neda Valizadeh, Morteza Nattagh Najafi</dc:creator>
    </item>
    <item>
      <title>Non-linear inhibitory responses enhance performance in collective decision-making</title>
      <link>https://arxiv.org/abs/2407.20927</link>
      <description>arXiv:2407.20927v2 Announce Type: replace 
Abstract: The precise modulation of activity through inhibitory signals ensures that both insect colonies and neural circuits operate efficiently and adaptively, highlighting the fundamental importance of inhibition in biological systems. Modulatory signals are produced in various contexts and are known for subtly shifting the probability of receiver behaviors based on response thresholds. Here we propose a non-linear function to introduce inhibitory responsiveness in collective decision-making inspired by honeybee house-hunting. We show that, compared with usual linear functions, non-linear responses enhance final consensus and reduce deliberation time. This improvement comes at the cost of reduced accuracy in identifying the best option. Nonetheless, for value-based tasks, the benefits of faster consensus and enhanced decision-making might outweigh this drawback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20927v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>nlin.AO</category>
      <category>physics.app-ph</category>
      <category>physics.bio-ph</category>
      <pubDate>Thu, 02 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s42005-025-02046-9</arxiv:DOI>
      <arxiv:journal_reference>Commun Phys 8, 119 (2025)</arxiv:journal_reference>
      <dc:creator>David March-Pons, Romualdo Pastor-Satorras, M. Carmen Miguel</dc:creator>
    </item>
  </channel>
</rss>
