<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 May 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Novel Compound AI Model for 6G Networks in 3D Continuum</title>
      <link>https://arxiv.org/abs/2505.15821</link>
      <description>arXiv:2505.15821v1 Announce Type: new 
Abstract: The 3D continuum presents a complex environment that spans the terrestrial, aerial and space domains, with 6Gnetworks serving as a key enabling technology. Current AI approaches for network management rely on monolithic models that fail to capture cross-domain interactions, lack adaptability,and demand prohibitive computational resources. This paper presents a formal model of Compound AI systems, introducing a novel tripartite framework that decomposes complex tasks into specialized, interoperable modules. The proposed modular architecture provides essential capabilities to address the unique challenges of 6G networks in the 3D continuum, where heterogeneous components require coordinated, yet distributed, intelligence. This approach introduces a fundamental trade-off between model and system performance, which must be carefully addressed. Furthermore, we identify key challenges faced by Compound AI systems within 6G networks operating in the 3D continuum, including cross-domain resource orchestration, adaptation to dynamic topologies, and the maintenance of consistent AI service quality across heterogeneous environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15821v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milos Gravara, Andrija Stanisic, Stefan Nastic</dc:creator>
    </item>
    <item>
      <title>Decoupling the Device and Identity in Cellular Networks with vSIM</title>
      <link>https://arxiv.org/abs/2505.15827</link>
      <description>arXiv:2505.15827v1 Announce Type: new 
Abstract: Cellular networks are now fundamental infrastructure, powering not just smartphones for daily communication and commerce, but also enabling the expansion of IoT and edge computing through last-mile connectivity. At the core of this infrastructure is the SIM card, which provides essential network authentication and subscriber identification through subscriber cryptographic key and profile information. More recently, the SIM card has evolved from a separate pluggable card, to a card integrated into the board (i.e., soldered onto the board with the same electrical interface) (eSIM), to one that is integrated into the System on Chip (iSIM). However, a fundamental limitation persists across SIM evolution: subscriber identity remains coupled to hardware. eSIM and iSIM technologies, despite enabling remote provisioning, still bind digital identities to specific hardware elements. This makes it complex to support emerging use cases like moving a phone number to a cloud AI service or transferring credentials between different devices while maintaining cellular connectivity. Furthermore, although eSIM and iSIM support multiple profiles (multiple phone numbers or carrier profiles on a single device), all profiles still link back to the same hardware identity. For users seeking to maintain privacy through identity rotation or separation (like having different numbers for different purposes), they are limited by the hardware-bound nature of the security architecture. In this paper, we seek to decouple identity from the device, enhancing privacy and flexibility compared to various SIM designs. By breaking this coupling, we enable scenarios like real identity rotation, integration with virtual assistants, or temporary use of backup phones while maintaining consistent cellular connectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15827v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shirin Ebadi, Zach Moolman, Eric Keller, Tamara Lehman</dc:creator>
    </item>
    <item>
      <title>Generative AI-Aided QoE Maximization for RIS-Assisted Digital Twin Interaction</title>
      <link>https://arxiv.org/abs/2505.15828</link>
      <description>arXiv:2505.15828v1 Announce Type: new 
Abstract: In this paper, we investigate a quality of experience (QoE)-aware resource allocation problem for reconfigurable intelligent surface (RIS)-assisted digital twin (DT) interaction with uncertain evolution. In the considered system, mobile users are expected to interact with a DT model maintained on a DT server that is deployed on a base station, via effective uplink and downlink channels assisted by an RIS. Our goal is to maximize the sum of all mobile users' joint subjective and objective QoE in DT interactions across various DT scenes, by jointly optimizing phase shift matrix, receive/transmit beamforming matrix, rendering resolution configuration and computing resource allocation. While solving this problem is challenging mainly due to the uncertain evolution of the DT model, which leads to multiple scene-specific problems, and require us to constantly re-solve each of them whenever DT model evolves.
  To this end, leveraging the dynamic optimization capabilities of decision transformers and the generalization strengths of generative artificial intelligence (GAI), we propose a novel GAI-aided approach, called the prompt-guided decision transformer integrated with zero-forcing optimization (PG-ZFO). Simulations are conducted to evaluate the proposed PG-ZFO, demonstrating its effectiveness and superiority over counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15828v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayuan Chen, Yuxiang Li, Changyan Yi, Shimin Gong</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Optimization for Digital Twin Service Provisioning over Edge Computing</title>
      <link>https://arxiv.org/abs/2505.15829</link>
      <description>arXiv:2505.15829v1 Announce Type: new 
Abstract: Digital Twin (DT) is a transformative technology poised to revolutionize a wide range of applications. This advancement has led to the emergence of digital twin as a service (DTaaS), enabling users to interact with DT models that accurately reflect the real-time status of their physical counterparts. Quality of DTaaS primarily depends on the freshness of DT data, which can be quantified by the age of information (AoI). The reliance on remote cloud servers solely for DTaaS provisioning presents significant challenges for latency-sensitive applications with strict AoI demands. Edge computing, as a promising paradigm, is expected to enable the AoI-aware provision of real-time DTaaS for users. In this paper, we study the joint optimization of DT model deployment and DT model selection for DTaaS provisioning over edge computing, with the objective of maximizing the quality of DTaaS. To address the uncertainties of DT interactions imposed on DTaaS provisioning, we propose a novel distributionally robust optimization (DRO)-based approach, called Wasserstein DRO (WDRO), where we first reformulate the original problem to a robust optimization problem, with the objective of maximizing the quality of DTaaS under the unforeseen extreme request conditions. Then, we leverage multi-level dual transformations based on Wasserstein distance to derive a robust solution. Simulations are conducted to evaluate the performance of the proposed WDRO, and the results demonstrate its superiority over counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15829v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxiang Li, Jiayuan Chen, Changyan Yi</dc:creator>
    </item>
    <item>
      <title>Characterization of Using Hybrid Beamforming in mmWave Virtual Reality</title>
      <link>https://arxiv.org/abs/2505.15830</link>
      <description>arXiv:2505.15830v1 Announce Type: new 
Abstract: Wireless Virtual Reality (VR) is increasingly in demand in Wireless LANs (WLANs). In this paper, a utility function for resource management in wireless VR is proposed. Maximizing the sum rate metric in transmitting VR audio or videos is an important factor for ascertaining low latency in obtaining QoS requirement of users in VR, so forth mmWave frequency band in WLAN technology should be used. This frequency band is presented in IEEE 802.11ad/ay. Resource access method in IEEE 802.11ay standard is MultiUser MIMO (MU-MIMO) with OFDM modulation. Operating at mmWave frequency band is equal to use massive number of antenna to enhance the received power in (Line of Sight) LoS direction by inducing sever propagation with small wavelength. Also for reducing the complexity of hardware in mmWave technology, designers should select some number of connected phase shifters to each antenna element by hybrid beamforming method. Processing delay, transmission delay and queue delay should be considered in acquiring QoS metric in terms of utility function. The optimal closed form expression of the multi-attribute utility function is based on these delays that are calculated by downlink and uplink rates in assistant with hybrid beamforming. Trends of transmission delay and multi-attribute utility function in various Es/N0 values and different scenarios are analyzed. Based on these results, 95.4% accuracy in comparison with ns3 in uplink and downlink channel modeling in IEEE 802.11ay standard's indoor environment has been reported. Also, it is shown that min channel gain consideration can cause reduction in the value of the utility function and incursion in transmission delay in VR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15830v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nasim Alikhani, Abbas Mohammadi</dc:creator>
    </item>
    <item>
      <title>Transforming Decoder-Only Transformers for Accurate WiFi-Telemetry Based Indoor Localization</title>
      <link>https://arxiv.org/abs/2505.15835</link>
      <description>arXiv:2505.15835v1 Announce Type: new 
Abstract: Wireless Fidelity (WiFi) based indoor positioning is a widely researched area for determining the position of devices within a wireless network. Accurate indoor location has numerous applications, such as asset tracking and indoor navigation. Despite advances in WiFi localization techniques -- in particular approaches that leverage WiFi telemetry -- their adoption in practice remains limited due to several factors including environmental changes that cause signal fading, multipath effects, interference, which, in turn, impact positioning accuracy. In addition, telemetry data differs depending on the WiFi device vendor, offering distinct features and formats; use case requirements can also vary widely. Currently, there is no unified model to handle all these variations effectively. In this paper, we present WiFiGPT, a Generative Pretrained Transformer (GPT) based system that is able to handle these variations while achieving high localization accuracy. Our experiments with WiFiGPT demonstrate that GPTs, in particular Large Language Models (LLMs), can effectively capture subtle spatial patterns in noisy wireless telemetry, making them reliable regressors. Compared to existing state-of-the-art methods, our method matches and often surpasses conventional approaches for multiple types of telemetry. Achieving sub-meter accuracy for RSSI and FTM and centimeter-level precision for CSI demonstrates the potential of LLM-based localisation to outperform specialized techniques, all without handcrafted signal processing or calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15835v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nayan Sanjay Bhatia, Katia Obraczka</dc:creator>
    </item>
    <item>
      <title>Optimizing Resource Allocation for QoS and Stability in Dynamic VLC-NOMA Networks via MARL</title>
      <link>https://arxiv.org/abs/2505.15841</link>
      <description>arXiv:2505.15841v1 Announce Type: new 
Abstract: Visible Light Communication (VLC) combined with Non-Orthogonal Multiple Access (NOMA) offers a promising solution for dense indoor wireless networks. Yet, managing resources effectively is challenged by VLC network dynamic conditions involving user mobility and light dimming. In addition to satisfying Quality of Service (QoS) and network stability requirements. Traditional resource allocation methods and simpler RL approaches struggle to jointly optimize QoS and stability under the dynamic conditions of mobile VLC-NOMA networks. This paper presents MARL frameworks tailored to perform complex joint optimization of resource allocation (NOMA power, user scheduling) and network stability (interference, handovers), considering heterogeneous QoS, user mobility, and dimming in VLC-NOMA systems. Our MARL frameworks capture dynamic channel conditions and diverse user QoS , enabling effective joint optimization. In these frameworks, VLC access points (APs) act as intelligent agents, learning to allocate power and schedule users to satisfy diverse requirements while maintaining network stability by managing interference and minimizing disruptive handovers. We conduct a comparative analysis of two key MARL paradigms: 1) Centralized Training with Decentralized Execution (CTDE) and 2) Centralized Training with Centralized Execution (CTCE). Comprehensive simulations validate the effectiveness of both tailored MARL frameworks and demonstrate an ability to handle complex optimization. The results show key trade-offs, as the CTDE approach achieved approximately 16\% higher for High priority (HP) user QoS satisfaction, while the CTCE approach yielded nearly 7 dB higher average SINR and 12\% lower ping-pong handover ratio, offering valuable insights into the performance differences between these paradigms in complex VLC-NOMA network scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15841v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aubida A. Al-Hameed, Safwan Hafeedh Younus, Mohamad A. Ahmed, Abdullah Baz</dc:creator>
    </item>
    <item>
      <title>Graph Neural Networks Based Anomalous RSSI Detection</title>
      <link>https://arxiv.org/abs/2505.15847</link>
      <description>arXiv:2505.15847v1 Announce Type: new 
Abstract: In today's world, modern infrastructures are being equipped with information and communication technologies to create large IoT networks.
  It is essential to monitor these networks to ensure smooth operations by detecting and correcting link failures or abnormal network behaviour proactively, which can otherwise cause interruptions in business operations.
  This paper presents a novel method for detecting anomalies in wireless links using graph neural networks. The proposed approach involves converting time series data into graphs and training a new graph neural network architecture based on graph attention networks that successfully detects anomalies at the level of individual measurements of the time series data. The model provides competitive results compared to the state of the art while being computationally more efficient with ~171 times fewer trainable parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15847v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/BalkanCom58402.2023.10167910</arxiv:DOI>
      <arxiv:journal_reference>2023 International Balkan Conference on Communications and Networking (BalkanCom)</arxiv:journal_reference>
      <dc:creator>Bla\v{z} Bertalani\v{c}, Matej Vnu\v{c}ec, Carolina Fortuna</dc:creator>
    </item>
    <item>
      <title>Sionna Research Kit: A GPU-Accelerated Research Platform for AI-RAN</title>
      <link>https://arxiv.org/abs/2505.15848</link>
      <description>arXiv:2505.15848v1 Announce Type: new 
Abstract: We introduce the NVIDIA Sionna Research Kit, a GPU-accelerated research platform for developing and testing AI/ML algorithms in 5G NR cellular networks. Powered by the NVIDIA Jetson AGX Orin, the platform leverages accelerated computing to deliver high throughput and real-time signal processing, while offering the flexibility of a software-defined stack. Built on OpenAirInterface (OAI), it unlocks a broad range of research opportunities. These include developing 5G NR and ORAN compliant algorithms, collecting real-world data for AI/ML training, and rapidly deploying innovative solutions in a very affordable testbed. Additionally, AI/ML hardware acceleration promotes the exploration of use cases in edge computing and AI radio access networks (AI-RAN). To demonstrate the capabilities, we deploy a real-time neural receiver - trained with NVIDIA Sionna and using the NVIDIA TensorRT library for inference - in a 5G NR cellular network using commercial user equipment. The code examples will be made publicly available, enabling researchers to adopt and extend the platform for their own projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15848v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastian Cammerer, Guillermo Marcus, Tobias Zirr, Fay\c{c}al A\"it Aoudia, Lorenzo Maggi, Jakob Hoydis, Alexander Keller</dc:creator>
    </item>
    <item>
      <title>Integration of TinyML and LargeML: A Survey of 6G and Beyond</title>
      <link>https://arxiv.org/abs/2505.15854</link>
      <description>arXiv:2505.15854v1 Announce Type: new 
Abstract: The transition from 5G networks to 6G highlights a significant demand for machine learning (ML). Deep learning models, in particular, have seen wide application in mobile networking and communications to support advanced services in emerging wireless environments, such as smart healthcare, smart grids, autonomous vehicles, aerial platforms, digital twins, and the metaverse. The rapid expansion of Internet-of-Things (IoT) devices, many with limited computational capabilities, has accelerated the development of tiny machine learning (TinyML) and resource-efficient ML approaches for cost-effective services. However, the deployment of large-scale machine learning (LargeML) solutions require major computing resources and complex management strategies to support extensive IoT services and ML-generated content applications. Consequently, the integration of TinyML and LargeML is projected as a promising approach for future seamless connectivity and efficient resource management.
  Although the integration of TinyML and LargeML shows abundant potential, several challenges persist, including performance optimization, practical deployment strategies, effective resource management, and security considerations. In this survey, we review and analyze the latest research aimed at enabling the integration of TinyML and LargeML models for the realization of smart services and applications in future 6G networks and beyond. The paper concludes by outlining critical challenges and identifying future research directions for the holistic integration of TinyML and LargeML in next-generation wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15854v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thai-Hoc Vu, Ngo Hoang Tu, Thien Huynh-The, Kyungchun Lee, Sunghwan Kim, Miroslav Voznak, Quoc-Viet Pham</dc:creator>
    </item>
    <item>
      <title>A Hierarchical Optimization Framework Using Deep Reinforcement Learning for Task-Driven Bandwidth Allocation in 5G Teleoperation</title>
      <link>https://arxiv.org/abs/2505.15977</link>
      <description>arXiv:2505.15977v1 Announce Type: new 
Abstract: The evolution of 5G wireless technology has revolutionized connectivity, enabling a diverse range of applications. Among these are critical use cases such as real time teleoperation, which demands ultra reliable low latency communications (URLLC) to ensure precise and uninterrupted control, and enhanced mobile broadband (eMBB) services, which cater to data-intensive applications requiring high throughput and bandwidth. In our scenario, there are two queues, one for eMBB users and one for URLLC users. In teleoperation tasks, control commands are received in the URLLC queue, where communication delays occur. The dynamic index (DI) controls the service rate, affecting the telerobotic (URLLC) queue. A separate queue models eMBB data traffic. Both queues are managed through network slicing and application delay constraints, leading to a unified Lagrangian-based Lyapunov optimization for efficient resource allocation. We propose a DRL based hierarchical optimization framework that consists of two levels. At the first level, network optimization dynamically allocates resources for eMBB and URLLC users using a Lagrangian functional and an actor critic network to balance competing objectives. At the second level, control optimization finetunes the best gains for robots, ensuring stability and responsiveness in network conditions. This hierarchical approach enhances both communication and control processes, ensuring efficient resource utilization and optimized performance across the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15977v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Narges Golmohammadi, Madan Mohan Rayguru, Sabur Baidya</dc:creator>
    </item>
    <item>
      <title>SONIC: Cost-Effective Web Access for Developing Countries</title>
      <link>https://arxiv.org/abs/2505.16519</link>
      <description>arXiv:2505.16519v1 Announce Type: new 
Abstract: Over 2.6 billion people remain without access to the Internet in 2025. This phenomenon is especially pronounced in developing regions, where cost and infrastructure limitations are major barriers to connectivity. In response, we design SONIC, a low-cost, scalable data delivery system that builds on existing infrastructures: FM radio for downlink broadcasting, and SMS for personalized uplink. SONIC is motivated by the widespread availability of FM radio and SMS infrastructure in developing regions, along with embedded FM radio tuners in affordable mobile phones. SONIC offers several innovations to effectively transmit Web content over sound over FM radio, in a reliable and compressed form. For example, we transmit pre-rendered webpages and leverage pixel interpolation to recover errors at the receiver. We further modify Android to offer a simpler deployment pipeline, supporting a wide range of devices. We deployed SONIC at an FM radio station in Cameroon for six weeks with 30 participants. Our results demonstrate a sustained downlink throughput of 10 kbps, less than 20% loss for a majority of transmissions with signal strength above -90 dbM, and a strong user engagement across both Web browsing and ChatGPT interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16519v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayush Pandey, Rohail Asim, Jean Louis K. E. Fendji, Talal Rahwan, Matteo Varvello, Yasir Zaki</dc:creator>
    </item>
    <item>
      <title>LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols</title>
      <link>https://arxiv.org/abs/2505.16821</link>
      <description>arXiv:2505.16821v1 Announce Type: new 
Abstract: Integrating large AI models (LAMs) into 6G mobile networks promises to redefine protocol design and control-plane intelligence by enabling autonomous, cognitive network operations. While industry concepts, such as ETSI's Experiential Networked Intelligence (ENI), envision LAM-driven agents for adaptive network slicing and intent-based management, practical implementations still face challenges in protocol literacy and real-world deployment. This paper presents an end-to-end demonstration of a LAM that generates standards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as part of control-plane procedures inside a gNB. We treat RRC messaging as a domain-specific language and fine-tune a decoder-only transformer model (LLaMA class) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages linearized to retain their ASN.1 syntactic structure before standard byte-pair encoding tokenization. This enables combinatorial generalization over RRC protocol states while minimizing training overhead. On 30k field-test request-response pairs, our 8 B model achieves a median cosine similarity of 0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a zero-shot LLaMA-3 8B baseline -- indicating substantially improved structural and semantic RRC fidelity. Overall, our results show that LAMs, when augmented with Radio Access Network (RAN)-specific reasoning, can directly orchestrate control-plane procedures, representing a stepping stone toward the AI-native air-interface paradigm. Beyond RRC emulation, this work lays the groundwork for future AI-native wireless standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16821v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziming liu, Bryan Liu, Alvaro Valcarce, Xiaoli Chu</dc:creator>
    </item>
    <item>
      <title>Graph Attention Network for Optimal User Association in Wireless Networks</title>
      <link>https://arxiv.org/abs/2505.16347</link>
      <description>arXiv:2505.16347v1 Announce Type: cross 
Abstract: With increased 5G deployments, network densification is higher than ever to support the exponentially high throughput requirements. However, this has meant a significant increase in energy consumption, leading to higher operational expenditure (OpEx) for network operators creating an acute need for improvements in network energy savings (NES). A key determinant of operational efficacy in cellular networks is the user association (UA) policy, as it affects critical aspects like spectral efficiency, load balancing etc. and therefore impacts the overall energy consumption of the network directly. Furthermore, with cellular network topologies lending themselves well to graphical abstractions, use of graphs in network optimization has gained significant prominence. In this work, we propose and analyze a graphical abstraction based optimization for UA in cellular networks to improve NES by determining when energy saving features like cell switch off can be activated. A comparison with legacy approaches establishes the superiority of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16347v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javad Mirzaei, Jeebak Mitra, Gwenael Poitau</dc:creator>
    </item>
    <item>
      <title>Smaller, Smarter, Closer: The Edge of Collaborative Generative AI</title>
      <link>https://arxiv.org/abs/2505.16499</link>
      <description>arXiv:2505.16499v1 Announce Type: cross 
Abstract: The rapid adoption of generative AI (GenAI), particularly Large Language Models (LLMs), has exposed critical limitations of cloud-centric deployments, including latency, cost, and privacy concerns. Meanwhile, Small Language Models (SLMs) are emerging as viable alternatives for resource-constrained edge environments, though they often lack the capabilities of their larger counterparts. This article explores the potential of collaborative inference systems that leverage both edge and cloud resources to address these challenges. By presenting distinct cooperation strategies alongside practical design principles and experimental insights, we offer actionable guidance for deploying GenAI across the computing continuum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16499v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Morabito, SiYoung Jang</dc:creator>
    </item>
    <item>
      <title>Recursive Offloading for LLM Serving in Multi-tier Networks</title>
      <link>https://arxiv.org/abs/2505.16502</link>
      <description>arXiv:2505.16502v1 Announce Type: cross 
Abstract: Heterogeneous device-edge-cloud computing infrastructures have become widely adopted in telecommunication operators and Wide Area Networks (WANs), offering multi-tier computational support for emerging intelligent services. With the rapid proliferation of Large Language Model (LLM) services, efficiently coordinating inference tasks and reducing communication overhead within these multi-tier network architectures becomes a critical deployment challenge. Existing LLM serving paradigms exhibit significant limitations: on-device deployment supports only lightweight LLMs due to hardware constraints, while cloud-centric deployment suffers from resource congestion and considerable prompt communication overhead caused by frequent service requests during peak periods. Although the model-cascading-based inference strategy adapts better to multi-tier networks, its reliance on fine-grained, manually adjusted thresholds makes it less responsive to dynamic network conditions and varying task complexities. To address these challenges, we propose RecServe, a recursive offloading framework tailored for LLM serving in multi-tier networks. RecServe integrates a task-specific hierarchical confidence evaluation mechanism that guides offloading decisions based on inferred task complexity in progressively scaled LLMs across device, edge, and cloud tiers. To further enable intelligent task routing across tiers, RecServe employs a sliding-window-based dynamic offloading strategy with quantile interpolation, enabling real-time tracking of historical confidence distributions and adaptive offloading threshold adjustments. Experiments on eight datasets demonstrate that RecServe outperforms CasServe in both service quality and communication efficiency, and reduces the communication burden by over 50\% compared to centralized cloud-based serving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16502v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyuan Wu, Sheng Sun, Yuwei Wang, Min Liu, Bo Gao, Jinda Lu, Zheming Yang, Tian Wen</dc:creator>
    </item>
    <item>
      <title>Edge-First Language Model Inference: Models, Metrics, and Tradeoffs</title>
      <link>https://arxiv.org/abs/2505.16508</link>
      <description>arXiv:2505.16508v1 Announce Type: cross 
Abstract: The widespread adoption of Language Models (LMs) across industries is driving interest in deploying these services across the computing continuum, from the cloud to the network edge. This shift aims to reduce costs, lower latency, and improve reliability and privacy. Small Language Models (SLMs), enabled by advances in model compression, are central to this shift, offering a path to on-device inference on resource-constrained edge platforms. This work examines the interplay between edge and cloud deployments, starting from detailed benchmarking of SLM capabilities on single edge devices, and extending to distributed edge clusters. We identify scenarios where edge inference offers comparable performance with lower costs, and others where cloud fallback becomes essential due to limits in scalability or model capacity. Rather than proposing a one-size-fits-all solution, we present platform-level comparisons and design insights for building efficient, adaptive LM inference systems across heterogeneous environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16508v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>SiYoung Jang, Roberto Morabito</dc:creator>
    </item>
    <item>
      <title>ReinWiFi: Application-Layer QoS Optimization of WiFi Networks with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2405.03526</link>
      <description>arXiv:2405.03526v2 Announce Type: replace 
Abstract: The enhanced distributed channel access (EDCA) mechanism is used in current wireless fidelity (WiFi) networks to support priority requirements of heterogeneous applications. However, the EDCA mechanism can not adapt to particular quality-of-service (QoS) objective, network topology, and interference level. In this paper, a novel reinforcement-learning-based scheduling framework is proposed and implemented to optimize the application-layer quality-of-service (QoS) of a WiFi network with commercial adapters and unknown interference. Particularly, application-layer tasks of file delivery and delay-sensitive communication are jointly scheduled by adjusting the contention window sizes and application-layer throughput limitation, such that the throughput of the former and the round trip time of the latter can be optimized. Due to the unknown interference and vendor-dependent implementation of the WiFi adapters, the relation between the scheduling policy and the system QoS is unknown. Hence, a reinforcement learning method is proposed, in which a novel Q-network is trained to map from the historical scheduling parameters and QoS observations to the current scheduling action. It is demonstrated on a testbed that the proposed framework can achieve a significantly better performance than the EDCA mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03526v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qianren Li, Bojie Lv, Yuncong Hong, Rui Wang</dc:creator>
    </item>
    <item>
      <title>Analysis of Channel Uncertainty in Trusted Wireless Services via Repeated Interactions</title>
      <link>https://arxiv.org/abs/2406.18204</link>
      <description>arXiv:2406.18204v3 Announce Type: replace 
Abstract: The coexistence of heterogeneous sub-networks in 6G poses new security and trust concerns and thus calls for a perimeterless-security model. Blockchain radio access network (B-RAN) provides a trust-building approach via repeated interactions rather than relying on pre-established trust or central authentication. Such a trust-building process naturally supports dynamic trusted services across various service providers (SP) without the need for perimeter-based authentications; however, it remains vulnerable to environmental and system unreliability such as wireless channel uncertainty. In this study, we investigate channel unreliability in the trust-building framework based on repeated interactions for secure wireless services. We derive specific requirements for achieving cooperation between SPs and clients via a repeated game model and illustrate the implications of channel unreliability on sustaining trusted wireless services. We consider the framework design and optimization to guarantee SP-client cooperation, given the worst channel condition and/or the least cooperation willingness. Furthermore, we explore the maximum cooperation area to enhance service resilience and reveal the trade-off relationship between transmission efficiency, security integrity, and cooperative margin. Finally, we present simulations to demonstrate the system performance over fading channels and verify our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18204v3</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingwen Chen, Xintong Ling, Weihang Cao, Jiaheng Wang, Zhi Ding</dc:creator>
    </item>
    <item>
      <title>A Primer on AP Power Save in Wi-Fi 8: Overview, Analysis, and Open Challenges</title>
      <link>https://arxiv.org/abs/2411.17424</link>
      <description>arXiv:2411.17424v2 Announce Type: replace 
Abstract: Wi-Fi facilitates the Internet connectivity of billions of devices worldwide, making it an indispensable technology for modern life. Wi-Fi networks are becoming significantly denser, making energy consumption and its effects on operational costs and environmental sustainability crucial considerations. Wi-Fi has already introduced several mechanisms to enhance the energy efficiency of non-Access Point (non-AP) stations (STAs). However, the reduction of energy consumption of APs has never been a priority. Always-on APs operating at their highest capabilities consume significant power, which affects the energy costs of the infrastructure owner, aggravates the environmental impact, and decreases the lifetime of battery-powered APs. IEEE 802.11bn, which will be the basis of Wi-Fi 8, makes a big leap forward by introducing the AP Power Save (PS) framework. In this article, we describe and analyze the main proposals discussed in the IEEE 802.11bn Task Group (TGbn), such as Scheduled Power Save, (Semi-)Dynamic Power Save, and Cross-Link Power Save. We also consider other proposals that are being discussed in TGbn, namely the integration of Wake-up Radios (WuRs) and STA offloading. We then showcase the potential benefits of AP PS in several scenarios, including a deployment of 470 real APs in a university campus. Our numerical analysis reveals that AP power consumption can be decreased on average by up to 28 percent, with further improvement potential. Finally, we outline the open challenges that need to be addressed to optimally integrate AP PS in Wi-Fi and ensure its compatibility with legacy devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17424v2</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/MCOM.004.2400486.</arxiv:DOI>
      <dc:creator>Roger Sanchez-Vital, Andrey Belogaev, Carles Gomez, Jeroen Famaey, Eduard Garcia-Villegas</dc:creator>
    </item>
    <item>
      <title>Network Intrusion Datasets: A Survey, Limitations, and Recommendations</title>
      <link>https://arxiv.org/abs/2502.06688</link>
      <description>arXiv:2502.06688v3 Announce Type: replace-cross 
Abstract: Data-driven cyberthreat detection has become a crucial defense technique in modern cybersecurity. Network defense, supported by Network Intrusion Detection Systems (NIDSs), has also increasingly adopted data-driven approaches, leading to greater reliance on data. Despite the importance of data, its scarcity has long been recognized as a major obstacle in NIDS research. In response, the community has published many new datasets recently. However, many of them remain largely unknown and unanalyzed, leaving researchers uncertain about their suitability for specific use cases.
  In this paper, we aim to address this knowledge gap by performing a systematic literature review (SLR) of 89 public datasets for NIDS research. Each dataset is comparatively analyzed across 13 key properties, and its potential applications are outlined. Beyond the review, we also discuss domain-specific challenges and common data limitations to facilitate a critical view on data quality. To aid in data selection, we conduct a dataset popularity analysis in contemporary state-of-the-art NIDS research. Furthermore, the paper presents best practices for dataset selection, generation, and usage. By providing a comprehensive overview of the domain and its data, this work aims to guide future research toward improving data quality and the robustness of NIDS solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06688v3</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cose.2025.104510</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Security 156 (2025) 104510</arxiv:journal_reference>
      <dc:creator>Patrik Goldschmidt, Daniela Chud\'a</dc:creator>
    </item>
    <item>
      <title>Blank Space: Adaptive Causal Coding for Streaming Communications Over Multi-Hop Networks</title>
      <link>https://arxiv.org/abs/2502.11984</link>
      <description>arXiv:2502.11984v3 Announce Type: replace-cross 
Abstract: In this work, we introduce Blank Space AC-RLNC (BS), a novel Adaptive and Causal Network Coding (AC-RLNC) solution designed to mitigate the triplet trade-off between throughput-delay-efficiency in multi-hop networks. BS leverages the network's physical limitations considering the bottleneck from each node to the destination. In particular, BS introduces a light-computational re-encoding algorithm, called Network AC-RLNC (NET), implemented independently at intermediate nodes. NET adaptively adjusts the Forward Error Correction (FEC) rates and schedules idle periods. It incorporates two distinct suspension mechanisms: 1) Blank Space Period, accounting for the forward-channels bottleneck, and 2) No-New No-FEC approach, based on data availability. The experimental results achieve significant improvements in resource efficiency, demonstrating a 20% reduction in channel usage compared to baseline RLNC solutions. Notably, these efficiency gains are achieved while maintaining competitive throughput and delay performance, ensuring improved resource utilization does not compromise network performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11984v3</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Fri, 23 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adina Waxman, Shai Ginzach, Aviel Glam, Alejandro Cohen</dc:creator>
    </item>
  </channel>
</rss>
